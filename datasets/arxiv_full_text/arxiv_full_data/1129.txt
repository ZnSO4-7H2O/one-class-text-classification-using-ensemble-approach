{"title": "Single-trial P300 Classification using PCA with LDA, QDA and Neural  Networks", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "The P300 event-related potential (ERP), evoked in scalp-recorded electroencephalography (EEG) by external stimuli, has proven to be a reliable response for controlling a BCI. The P300 component of an event related potential is thus widely used in brain-computer interfaces to translate the subjects' intent by mere thoughts into commands to control artificial devices. The main challenge in the classification of P300 trials in electroencephalographic (EEG) data is the low signal-to-noise ratio (SNR) of the P300 response. To overcome the low SNR of individual trials, it is common practice to average together many consecutive trials, which effectively diminishes the random noise. Unfortunately, when more repeated trials are required for applications such as the P300 speller, the communication rate is greatly reduced. This has resulted in a need for better methods to improve single-trial classification accuracy of P300 response. In this work, we use Principal Component Analysis (PCA) as a preprocessing method and use Linear Discriminant Analysis (LDA)and neural networks for classification. The results show that a combination of PCA with these methods provided as high as 13\\% accuracy gain for single-trial classification while using only 3 to 4 principal components.", "text": "event-related potential evoked scalp-recorded electroencephalography external stimuli proven reliable response controlling bci. component event related potential thus widely used brain-computer interfaces translate subjects’ intent mere thoughts commands control artiﬁcial devices. main challenge classiﬁcation trials electroencephalographic data signal-to-noise ratio response. overcome individual trials common practice average together many consecutive trials eﬀectively diminishes random noise. unfortunately repeated trials required applications speller communication rate greatly reduced. resulted need better methods improve single-trial classiﬁcation accuracy response. work principal component analysis preprocessing method linear discriminant analysis neural networks classiﬁcation. results show combination methods provided high accuracy gain single-trial classiﬁcation using principal components. various neurological diseases disrupt neuromuscular channels brain communicates external world. certain cases like hemorrhage anterior brain stem degenerative neuromuscular diseases like amyotrophic lateral scleriosis patients suﬀer total motor paralysis results condition known locked-in syndrome wherein patient awake fully aware cannot communicate outside world complete paralysis. \"locked-in\" patients need assistive technology needs muscular activity whatsoever. brain-computer interface device uses brain signals provide direct nonmuscular communication channel brain outside world idea underlying bcis measure electric magnetic physical manifestations brain activity translate commands computer devices patients locked-in syndromethe event-related potential evoked scalprecorded electroencephalography external stimuli proven reliable response controlling study present comparison classiﬁcation methods classify signal based presence component. types bcis broadly classiﬁed categories—those external stimulus don’t ﬁrst method external stimuli cause changes neurophysiologic signals called event-related potentials used identify user’s response stimuli presented. second method users generate certain detectable patterns neurophysiologic signals concentrating speciﬁc mental task. example imagination hand movement used modify activity motor cortex recording activity brain electroencephalogram method choice fast responsivity covariation cognitive processes although invasive methods electrocorticography signals using implanted electrodes accurate non-invasive methods attractive ease patients; non-invasive methods also shown comparable implanted electrodes used appropriate machine learning algorithms. non-invasive recordings done electrodes placed directly scalp using international system work data obtained non-invasive electroencephalographic recordings. moreover experiments data using subset electrodes already found meaningful classiﬁcation smaller number electrodes also better practical reasons lower cost higher usability target patients. electrodes used work ’f’’f’’c’’c’’p’’p’’o’’o’. characterized positive peak stimulus onset elicited subjects encounter rarely occurring expected stimulus among presented stimuli. subjects assigned task assigning category stimuli series stimuli types types occurs rarely seen rare stimuli. experimental paradigm based extensive research called ’oddball’ paradigm rarely occurring stimuli ’oddballs’. farwell donchin utilized characteristic design called speller’. ﬁrst kind lets user type letter time using signals captured electrode needs muscular movement. research following work improving setup many variations setup emerged. deﬁnitions speller paradigm vary literature made clear main challenges classiﬁcation signal-to-noise ratio recorded scalp contains noise ongoing electrical activity brain hard separate resulting noisy signal. problem usually overcome averaging together many subsequent trials cancels noise makes detection possible. approach averaging comes cost reduced communication rate. farwell donchin reported pioneering work needed averaging trials achieve accuracy communication rate bits characters minute. although pioneering work establish feasibility based speller communication rate painfully slow practical use. lead much work last decades focused improving accuracy communication speed p-based like speller. focus areas research development algorithms reduce number trials required achieve reliable pbased bci. amply demonstrated averaging trials stabilizes amplitude removing noise challenge also seen improving single trial accuracy. last decades seen research classiﬁcation eventual achieving single-trial based reliable literature survey research show single classiﬁcation method state art. krusienski report results comparison diﬀerent classiﬁer algorithms shows stepwise linear discriminant analysis support vector machines perform well compared classiﬁers. using swlda classiﬁcation method krusienski achieved least accuracy participants. three participants performed accuracy averaging trials. sellers donchin achieved comparable average results healthy patients using swlda. serby used matched-ﬁltering independent component analysis achieve communication rate symbols/min accuracy averaged roughly trials. detection made real-time online testing subjects average communication rate achieved symbols/min accuracy averaged roughly trials. lines survey submissions competition competition shows several diﬀerent approaches like peak-picking methods able achieve accuracy using averaged trials competition data. short list shows many diﬀerent approaches work well also particular method clearly better others. challenges classiﬁcation namely subject-dependence even session-to-session variation responses subject. recent review ﬁeld concludes work still needed create truly reliable speller. work principal component analysis preprocessing method linear discriminant analysis quadratic discriminant analysis neural networks classiﬁcation. shown work well classiﬁcation author compared various blind source separation methods preprocessing methods classiﬁcation seen generally worked better methods like independent component analysis maximum noise fraction classiﬁcation. work builds upon work done cashero tests classiﬁcation methods support vector machines work well conjunction classiﬁcation. among classiﬁcation methods choice also driven success method variants like stepwise classiﬁcation. although swlda shown work well higher accuracy expensive method high processing time therefore considered suitable online speller system considering that preferred consider lightweight provides good performance generally classiﬁcation. chosen compare with whether linear nonlinear method works better classiﬁcation. choice neural networks method study classiﬁcation primarily driven hypothesis good choice classiﬁcation. studies employing neural networks classiﬁcation many shown promise. little work done using combination classiﬁcation. provide ﬂexibility terms ’derived’ data created based ﬂexibility number hidden units could useful method noisy data like especially provides good source components. another method shown work well classiﬁcation. don’t include study fact study using already covered keep focus eﬀect classiﬁcation methods tested single trial accuracies. paper laid follows. section develops mathematical background algorithms used experiments work. classiﬁcation methods neural networks optimization algorithm used developed section. section describes datasets used details steps used experiments. section explains experiments performed learned along results experiments. section concludes providing summary ﬁndings identiﬁes avenues future work. belong class classiﬁcation methods model discriminant functions class classify given data sample class largest value discriminant function. model posterior probabilities purpose deﬁning discriminant functions. representing data sample variable class label variable dimension data sample equivalently represents number features predictors sample classes. thus need class posteriors given suppose class-conditional equation linear equation gives decision rule lda—if ratio positive sample classiﬁed belonging class class negative. value zero implies decision boundary linear it’s obvious linear discriminant functions also called generative models make assumption gaussian distribution data base classiﬁcation samples assumption. parameters gaussian distributions estimated using training data. class priors calculated based number samples class present training data neural networks primarily employed machine learning nonlinear regression classiﬁcation method. many variations ﬂavors like recurrent multilayer perceptron etc. single hidden layer work. basic neural sometimes called single hidden layer back-propagation network layer perceptron two-stage regression classiﬁcation model. consists input layer hidden layer output layer. results multilogit model produces positive estimates one. treating outcomes probabilities corresponding class negative log-likelihood objective function minimize. negative log-likelihood objective function deﬁned sigmoid activation function log-likelihood error function neural network model works linear logistic regression model hidden units parameters estimated maximum likelihood. using nonlinear transformation becomes non-linear model inputs another interesting aspect model number hidden units varied adjust non-linearity model. hidden units model becomes simple linear logistic regression model input data. work linear non-linear versions neural network model call respectively. error function minimized variety approaches. standard approaches gradient descent called back-propagation neural network setting. conjugate gradient method called scaled conjugate gradient successful faster optimization problem work work. principal component analysis technique widely used pattern recognition machine learning dimensionality reduction feature extraction. commonly used derivations pca—one maximizes variance data minimizes projection error. develop maximum variance formulation. given data samples goal project data onto space dimensionality maximizing variance projected data. ﬁrst direction projection variance data projected given means must eigenvector eigenvalue also turns measure variance. turns direction projection results maximum variance projected data called ﬁrst ’principal component’. subsequent directions found inductively follows. given principal components next principal component found solving following constrained optimization problem rn×n rd×d rn×d. decomposition columns right singular vectors eigenvectors provide required ’principal components’. additionally column vectors ordered variance produce data projected onto ﬁrst column ﬁrst principal component second second work thus derive principal components experiments. section describes datasets used study also methods recording equipment used. data representation important part signal processing method also describe data representation used study. novel using channel-subtrials also deﬁned explained. four subjects study. data subjects recorded laboratory computer science department colorado state university data recorded using g.tec g.gammasys system -electrode electrodes located subset electrodes found meaningful classiﬁcation small subsets electrodes also easier practical easy-to-use bci. subject able-bodied participant study subject subject complete spinal cord injury. level cervical injury results signiﬁcant loss function biceps shoulders. data subject collected laboratory data subject recorded home. three sessions data collection performed subjects subjects count three target letters session various non-target letters randomly ﬂashed screen. data collection done ’odd-ball’ paradigm probability occurrence target letter session data sampled inter-stimulus-interval second. session consisted recording target non-target subtrials data electrodes. data subjects taken competition experiments based speller proposed farwell donchin subjects presented matrix characters. subject’s task focus attention characters word prescribed investigator rows columns successively randomly intensiﬁed rate .hz. objective contest predict correct character provided character selection ’epochs’ consisted sequences—each sequence consisting ﬂashings— rows columns. data collected bandpass ﬁltered .-hz digitized intensiﬁcation row/column matrix blank row/column intensiﬁcations block randomized blocks sets intensiﬁcations repeated times character epoch character epoch followed period time matrix original data four subjects comes continuous entire recording. data bandpass-ﬁltered data normalized zero mean unit variance. figures show second window data bandpass ﬁltering. bandpass ﬁltering done using butterworth bandpass ﬁlter data sliced separate target non-target subtrials channel. dataset thus reshaped matrix representing channel-subtrial datapoints time series subjects subjects channelsubtrials used classiﬁcation therefore consist one-second long windows stimulus onset extracted continuous signal data segment. approach data representation involves considering time series eight channels separate subtrial purpose initial classiﬁcation. means subtrial usual sense consists eight diﬀerent time series channel split eight diﬀerent subtrials data matrix. done view response present eight channels considered although varies degree phase diﬀerence preprocessing method able common sources responses diﬀerent channels. call channel subtrials. training algorithm done results test collected channel trial results aggregated arrive classiﬁcation ’overall’ subtrial. aggregation done using voting method. data generally unbalanced positive negative examples every trial contains many negative examples positive ones accordance ’odd-ball’ paradigm. usually tends bias classiﬁer favor negative examples. equal number subtrials target non-target used experiments classiﬁcation performance four classiﬁers—lda nlr—was recorded data i.e. data transformed pca. this feature selection done original features used. classiﬁcation performance tested performing data. procedures experiments—one forward selection principal components across components forward selection done selected number components magnitude singular values. start experiments testing four methods data i.e. data without transformation four subjects. would serve benchmark experiments follow using pca. experiments follow following common approach followed. datasets randomly partitioned training test sets ratio consider channel-subtrials separate subtrials training classiﬁcation voting data partitioned ensure place complete channel-subtrials belonging subtrial together either training test sets. training partitioned create validation set. accuracies validation used choose number hidden units. pilot studies four subjects range number hidden units number features tested validation set. observed validation accuracies generally best number hidden units equal number features dataset. throughout experiments number hidden units dataset taken number features used classiﬁcation. accuracies obtained data four subjects shown table first thing note work subjects problem covariance matrices became singular gave runtime error. accuracies left blank data. problem sample covariance matrices singular occurs sample size less number features happens case using data. problem usually encountered case averaging sample covariance matrices classes resulted average covariance matrix non-singular. subsequent experiments small subset features always used ensured problem encountered. it’s neither also works best subjects sub. versions linear nonlinear work well. works best sub. across subjects generalize linear nonlinear methods clear winner. performance linear version superior methods. best accuracies obtained subjects near expected accuracy random classiﬁer single trials except gets high accuracy single trials. next step experiments feature extraction. start experiments getting principal components training data using svd. plot projection training data onto ﬁrst components four datasets. looking projections choose visual inspection—for turned projection data onto -dimensional subspace shown figure method improved best accuracy accuracies decreased almost methods. although accuracy gains impressive method neither scientiﬁc desirable reliance human intervention visual judgment. moreover limit number make process choosing thorough automate process method ’forward selection’ four algorithms. forward selection done using -fold cross-validation choose give best accuracy validation set. thus added iteration. finally minimal gives best accuracy validation chosen. method expensive pick test validation accuracies till components chosen. also ensures data sample size less dimensionality doesn’t give singular covariance matrices case qda. method thus iterates iteration picks best iteration. results method shown table results using forward selection principal components found encouraging expected. challenge lies selecting number components based validation accuracies. method chooses number components give best validation accuracy doesn’t necessarily work test data. reason extreme noise data validation accuracies don’t generalize well test data. would pick components might modeling noise validation obviously throw classiﬁer test data. also over-ﬁtting training data case non-linear methods seen nlr. addition problems matter complexity runtime. expensive method iteration choosing component iterate remaining components. considering pitfalls method need diﬀerent method selecting good subset pcs. looking back results using chosen visually components modifying forward selection algorithm select components empirically chosen number based magnitude singular values. amounts considering ordered already magnitude singular values. visually chosen components shown good results subjects looking results thus obtained observed using works best. results using restricted shown table results obtained using restricted show improvement accuracy four subjects compared regular method gives overall best accuracies sub. also accuracy better methods except visual selection discounted that’s practical method online bci. discounting visual selection method best accuracies three subjects four using method restricted addition method comes greatly reduced data dimensionality. selected high method needed components achieve much experiments shown supported literature success method classiﬁcation depends subjects. gave exceedingly good results without best accuracy methods also gave second best accuracy hand linear version nn—the —that gave best accuracy. alone visually chosen gave best accuracy. overall it’s clear help improving accuracy classiﬁcation single trials subjects sub. best results except case came much reduced dimensionality data—the dimensionality maximum subjects. feature selection increased classiﬁcation accuracy also reduced execution time algorithms resulting dimensionality reduction. among wider distinction linear nonlinear methods results split. best accuracies using linear best results nlr—both nonlinear methods. reason seems diﬀerent data acquisition methods used datasets. data collected data collected short resulted overlapping amplitude happens stimulus induced signal also explains grand average signal similar sub. overlapping signals many stimuli seems like reason makes datasets diﬃcult linearly separable. another observation method treating channel-subtrial separate subtrial purpose classiﬁcation works well hypothesis able extract relevant source components channel-subtrial dataset also validated results. also captures variance across channels component classiﬁer would ignore component won’t give signiﬁcant discriminatory value purpose classiﬁcation. variance pronounced case sub. work many avenues future work. discussed earlier classiﬁcation depends subjects would useful experiments data subjects well results generalize across subjects. subjects classiﬁcation methods especially variants fisher’s linear discriminant stepwise linear discriminant analysis regularized discriminant analysis tried results could improved. another interesting thing would compare method channel-subtrial based data methods data representation spatio-temporal frequency domains. also methods used work studied depth. example it’s well known require certain minimum number data samples optimal performance. would interesting much could performance improve number samples used training classiﬁers used work. could also multiple hidden layers would work classiﬁcation.", "year": 2017}