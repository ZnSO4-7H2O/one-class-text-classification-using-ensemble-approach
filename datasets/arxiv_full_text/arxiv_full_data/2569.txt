{"title": "Kronecker Determinantal Point Processes", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Determinantal Point Processes (DPPs) are probabilistic models over all subsets a ground set of $N$ items. They have recently gained prominence in several applications that rely on \"diverse\" subsets. However, their applicability to large problems is still limited due to the $\\mathcal O(N^3)$ complexity of core tasks such as sampling and learning. We enable efficient sampling and learning for DPPs by introducing KronDPP, a DPP model whose kernel matrix decomposes as a tensor product of multiple smaller kernel matrices. This decomposition immediately enables fast exact sampling. But contrary to what one may expect, leveraging the Kronecker product structure for speeding up DPP learning turns out to be more difficult. We overcome this challenge, and derive batch and stochastic optimization algorithms for efficiently learning the parameters of a KronDPP.", "text": "determinantal point processes probabilistic models subsets ground items. recently gained prominence several applications rely diverse subsets. however applicability large problems still limited complexity core tasks sampling learning. enable efﬁcient sampling learning dpps introducing krondpp model whose kernel matrix decomposes tensor product multiple smaller kernel matrices. decomposition immediately enables fast exact sampling. contrary expect leveraging kronecker product structure speeding learning turns difﬁcult. overcome challenge derive batch stochastic optimization algorithms efﬁciently learning parameters krondpp. introduction determinantal point processes discrete probability models subsets ground items. provide elegant model assign probabilities exponentially large sample permitting tractable sampling marginalization. often used provide models balance diversity quality characteristics valuable numerous problems machine learning related areas antecedents dpps statistical mechanics since seminal work made inroads machine learning. applied variety problems document video summarization sensor placement recommender systems object retrieval recently used compress fully-connected layers neural networks provide optimal sampling procedures nyström method general study properties also garnered signiﬁcant amount interest e.g. however despite elegance tractability widespread adoption dpps impeded cost basic tasks sampling learning cost motivated string recent works approximate sampling methods mcmc samplers core-set based samplers task learning data received less attention; methods cost iteration clearly unacceptable realistic settings. burden partially ameliorated restrict learning low-rank dpps though expense unable sample subsets larger chosen rank. considerations motivate introduce krondpp model uses kronecker product kernels. result krondpp enables learn large sized kernels also permitting efﬁcient sampling. kronecker products scale matrix models popular effective idea several machine-learning settings efﬁcient execution dpps turns surprisingly challenging. make discussion concrete recall basic facts now. suppose ground items discrete probability measure parametrized positive deﬁnite matrix drawn measure satisﬁes therefore consider class kernels whose structure makes easy compute determinants able scale dpps. alternative approach towards scalability restrict size subsets done k-dpp using rank-k kernels approaches still require preprocessing exact sampling; another caveat limit model assigning zero probabilities sets cardinality greater contrast krondpp uses kernel matrix form subkernel smaller positive deﬁnite matrix. decomposition advantages signiﬁcantly lowers number parameters required specify enables fast sampling learning. ease exposition describe speciﬁc details krondpp become clear analysis typically special cases sufﬁce obtain low-complexity sampling learning algorithms. contributions. main contribution krondpp model along efﬁcient algorithms sampling learning kronecker factored kernel. speciﬁcally inspired algorithm develop krk-picard block-coordinate ascent procedure generates sequence kronecker factored estimates kernel ensuring monotonic progress objective function. importantly show implement krk-picard time implemented batch method time space implemented stochastic method. alluded above unlike many uses kronecker models krondpp admit trivial scaling largely extensive dependence dpps arbitrary submatrices kernel. interesting theoretical nugget arises analysis combinatorial problem call subset clustering problem whose solution lead speedups algorithms. begin recalling basic properties kronecker products needed analysis; omit proofs well-known results brevity. kronecker product rp×q rr×s matrices deﬁned block matrix denote block aijb valid pair extend notation non-kronecker product matrices indicate submatrix size position proposition matrices sizes well-deﬁned. then section consider difﬁcult task krondpps learning kronecker product kernel matrix observed subsets using deﬁnition maximum-likelihood learning kernel results optimization problem moreover iteration guaranteed monotonically increase log-likelihood beneﬁts accrue cost iteration furthermore direct application cannot guarantee kronecker structure required krondpp. obtain efﬁcient algorithm optimize beyond nonconvexity kronecker structure imposes another constraint. ﬁrst rewrite function re-arrange terms write easy concave short argument shows convex appeal convex-concave procedure shows updating solving guaranteed monotonically increase krondpp idea apply easily constraint function concave convex. indeed linear concave also concave; similarly seen concave convex. hence generalizing arguments block-coordinate setting updating moreover solutions positive deﬁnite. proof. details somewhat technical hence given appendix know since partial trace operators positive follows solutions also positive deﬁnite. experimentally provide faster convergence although monotonicity log-likelihood longer guaranteed. found experimentally range admissible larger picard decreases grows larger. arguments easily generalize multiblock case. thus learning writing matrix position zeros elsewhere update updates transparent whether kronecker product saves computation. particular clear whether updates implemented faster show next section implement updates efﬁciently. theorem obtain algorithm operates alternatingly subkernel). important note speedup algorithm obtained performing stochastic updates i.e. instead computing full gradient log-likelihood perform updates using subset step instead iterating entire training set; uses stochastic gradient uil− indeed leveraging properties kronecker product updates obtained without computing l∆l. result non-trivial components must considered separately computational efﬁciency. proof provided app. however seems considering subkernels lead speed-ups. non-stochastic updates time space stochastic updates time space. marked improvement runs space time time algorithm also provides faster stochastic updates however wonder learning sub-kernels alternating updates loglikelihood converges sub-optimal limit. next section discusses jointly update show appendix solutions exist computed ﬁrst singular value ij=. note however case guaranteed increase log-likelihood. pseudocode related algorithm given appendix analysis similar proof thm. shows updates obtained determining existence partition size variant np-hard subset-union knapsack problem knapsacks value item equal solution sukp value knapsacks equivalent solution however approximate partition also simply constructed greedy algorithm. sampling sampling exactly full kernel costs size sampled subset. bulk computation lies initial eigendecomposition orthonormalizations cost although eigendecomposition need happen many iterations sampling exact sampling nonetheless intractable practice large also resort mcmc sampling; instance sampler considered results hold k-dpps suggest mcmc sampler possibly take time full dpps impractical. nevertheless develops faster mcmc samplers also able proﬁt kronecker product structure offered krondpp. experimental results order validate learning algorithm compared krk-picard joint-picard picard iteration multiple real synthetic datasets. synthetic tests three algorithms used learn synthetic data drawn true kernel. subkernels initialized coefﬁcients drawn uniformly picard initialized figures training data generated sampling subsets true kernel sizes uniformly distributed evaluate krk-picard matrices large memory large drew samples kernel rank learned kernel stochastically likelihood drastically improves steps shown figures krk-picard converges signiﬁcantly faster picard especially large values however although joint-picard also increases log-likelihood iteration converges much slower high standard deviation whereas standard deviations picard krk-picard barely noticeable. reasons drop comparison joint-picard subsequent experiments. compared krk-picard picard baby registry dataset also used evaluate learning algorithms dataset contains categories baby-related products obtained amazon. learned kernels largest categories case picard sufﬁciently efﬁcient prefered krk-picard; comparison serves evaluate quality ﬁnal kernel estimates. initial marginal kernel sampled wishart distribution degrees freedom identity covariance matrix scaled picard krk-picard chosen minimizing convergence determined objective change dipped threshold iteration takes longer picard iteration increases likelihood more δpic δkrk ﬁnal log-likelihoods shown table step-sizes largest possible values i.e. apic akrk table shows krk-picard obtains comparable albeit slightly worse log-likelihoods picard conﬁrms tractable better modeling capability full kernels make preferable krondpps. finally evaluate krk-picard large matrices real-world data train data genes dataset dataset consists genes represented features corresponding distance gene hubs biogrid gene interaction network. figure shows performance algorithms. synthetic experiments krkpicard converges much faster; stochastic updates increase performance even more shown fig. average runtimes speed-up given table krk-picard runs almost order magnitude faster picard stochastic updates orders magnitude faster providing slightly larger initial increases log-likelihood. conclusion future work introduced krondpps variant dpps kernels structured kronecker product smaller matrices showed typical operations dpps sampling learning kernel data made efﬁcient krondpps previously untractable ground sizes. carefully leveraging properties kronecker product derived lowcomplexity algorithm learn kernel data guarantees positive iterates monotonic increase log-likelihood runs time. algorithm provides even signiﬁcant speed-ups memory gains stochastic case requiring time discussing learning kernel showed cannot updated simultaneously cccp-style iteration since convex however shown geodesically convex riemannian manifold positive deﬁnite matrices suggests deriving iteration would take advantage intrinsic geometry problem viable line future work. krondpps also enable fast sampling operations using subkernels using three sub-kernels; allows exact sampling comparable even better costs previous algorithms approximate sampling. however improve computational efﬁciency subset size becomes limiting cost sampling learning. necessary line future work allow truly scalable dpps thus overcome computational bottleneck.", "year": 2016}