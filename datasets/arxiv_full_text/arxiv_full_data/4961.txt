{"title": "Bayesian Unification of Gradient and Bandit-based Learning for  Accelerated Global Optimisation", "tag": ["cs.AI", "cs.LG"], "abstract": "Bandit based optimisation has a remarkable advantage over gradient based approaches due to their global perspective, which eliminates the danger of getting stuck at local optima. However, for continuous optimisation problems or problems with a large number of actions, bandit based approaches can be hindered by slow learning. Gradient based approaches, on the other hand, navigate quickly in high-dimensional continuous spaces through local optimisation, following the gradient in fine grained steps. Yet, apart from being susceptible to local optima, these schemes are less suited for online learning due to their reliance on extensive trial-and-error before the optimum can be identified. In this paper, we propose a Bayesian approach that unifies the above two paradigms in one single framework, with the aim of combining their advantages. At the heart of our approach we find a stochastic linear approximation of the function to be optimised, where both the gradient and values of the function are explicitly captured. This allows us to learn from both noisy function and gradient observations, and predict these properties across the action space to support optimisation. We further propose an accompanying bandit driven exploration scheme that uses Bayesian credible bounds to trade off exploration against exploitation. Our empirical results demonstrate that by unifying bandit and gradient based learning, one obtains consistently improved performance across a wide spectrum of problem environments. Furthermore, even when gradient feedback is unavailable, the flexibility of our model, including gradient prediction, still allows us outperform competing approaches, although with a smaller margin. Due to the pervasiveness of bandit based optimisation, our scheme opens up for improved performance both in meta-optimisation and in applications where gradient related information is readily available.", "text": "bandit based optimisation schemes remarkable advantage gradient based approaches global perspective eliminates danger getting stuck local optima. however continuous optimisation problems problems large number actions bandit based approaches hindered slow learning. gradient based approaches hand navigate quickly high-dimensional continuous spaces local optimisation following gradient grained steps. however apart susceptible local optima schemes also less suited online learning reliance extensive trial-and-error optimum identiﬁed. contrast bandit algorithms seek identify optimal action steps possible. paper propose bayesian approach uniﬁes distinct paradigms single framework combining advantages. heart approach stochastic linear approximation function optimised gradient values function explicitly captured. model allows learn noisy function gradient observations well predicting properties across action space support optimisation. propose accompanying bandit driven exploration scheme uses bayesian credible bounds trade exploration exploitation. empirical results demonstrate unifying bandit gradient based learning obtains consistently improved performance across wide spectrum problem environments. furthermore even gradient feedback unavailable ﬂexibility model including gradient prediction still allows outperform competing approaches although smaller margin. pervasiveness bandit based optimisation scheme opens improved performance meta-optimisation applications gradient related information readily available. multi-armed bandit problem classical optimisation problem captures trade exploitation exploration reinforcement learning. problem consists agent sequentially pulls multiple arms attached gambling machine pull resulting scalar reward. reward randomly drawn unknown distribution unique arm. purpose quickly possible identify highest expected reward goal directed trial-and-error. bandit based optimisation schemes tremendous advantage gradient based approaches global perspective eliminates danger getting stuck local optima. however continuous optimisation problems problems large number arms bandit based approaches hindered inability generalise across arms independence assumption leads slow learning expected reward function must inferred independently arm. gradient based approaches hand navigate quickly high-dimensional continuous spaces local optimisation following gradient small steps. local optimisation however makes class schemes susceptible local optima. further less suited on-line learning reliance extensive trial error small parameter adjustments step. contrast bandit algorithms designed on-line operation aiming converge optimal trials possible. figure gaussian process squared exponential covariance function line marks original function black line marks function predicted noisy observations along credible interval. deal continuous large action spaces several bandit based approaches recently proposed capture interaction among actions. class schemes referred global multi-armed bandit schemes models expected rewards arms linear functions global parameter another family techniques attacks large action spaces tree based searching x-armed bandits ﬁnding global maxima expected reward function locally lipschitz finally gaussian processes applied smoothing interpolation forming foundation bandit based exploration exploitation continuous action spaces gaussian process based approaches particularly attractive provide bayesian estimate expected reward functions including credible intervals illustrated fig. brief dedicated kernel functions capture smoothness function dynamics encoded covariance matrix. however illustrated ﬁgure scheme blind towards gradient underlying reward function merely tracing line observations tending towards prior mean without input paper propose radically approach stochastic optimisation global perspective multi-armed bandits combined gradient based local optimisation eﬀect signiﬁcantly accelerating learning. brevity approach provides bayesian uniﬁcation gradient banditbased learning contributions summarised follows function gradient. model supports learning noisy function values well gradient related observations. further unobserved function values gradient information predicted across action space support goal-directed exploration exploitation reward function. section provide details bayesian approach unifying gradient bandit-based learning. introduce grid based linear approximation reward function explicitly relates functiongradient values modelled stochastic variables address noisy observations relationships. cover accompanying optimisation strategies based bayesian credibility bounds well thompson sampling section provide empirical results demonstrating superiority scheme wide range settings. conclude paper section providing pointers research. function gradient. shown fig. model stochastic variables respectively. stochastic variables normally distributed corresponding unknown means variations seen ﬁgure relationship variables deﬁned recursively according aforementioned linear approximation scheme ∇fi− furthermore model dynamics unknown gradient relating neighbouring stochastic gradient variables ∇fi− change rate stochastically governed i.i.d. gaussian noise using factor graph based computation approach model eﬃciently calculate posterior joint marginal distributions variables given noisy information function gradient values rather broad perspective currently competing strategies ﬁnding global optimum bandit setting thompson sampling based upper conﬁdence bounds. thompson sampling tends provide better performance ucb-based approaches empirical investigations however known over-explore. ucb-like approaches hand provide deterministic goal-directed path towards global optimum ﬁnding optimum probability arbitrarily close unity. thompson sampling hand always converges global optimum proposed bayesian technique solving bandit like problems akin thompson sampling principle leading novel schemes handling multi-armed non-stationary bandit problems empirical results demonstrated advantages techniques established performers. furthermore provided theoretical results stating original technique instantaneously self-correcting converges pulling optimal probability close unity desired. later testimony renewed importance thompson sampling principle modern bayesian look multi-armed bandit problem also taken promising avenue solving multi-armed bandit problem involves methods consider estimation conﬁdence intervals wherein scheme estimates conﬁdence interval reward probability optimistic reward probability estimate identiﬁed arm. optimistic reward probability estimate greedily selected authors analysed several conﬁdence interval based algorithms. algorithms also provide logarithmically increasing regret ucb-tuned variant well-known algorithm outperforming well \u0001n-greedy strategy. brief ucb-tuned following sample mean variance rewards obtained total number pulls number times pulled. thus quantity added sample average speciﬁc steadily reduced pulled corresponding uncertainty reward probability reduced. result always selecting highest optimistic reward estimate ucb-tuned gradually shifts exploration exploitation. providing bayesian estimate function optimised bug-b model supports thompson sampling ucb-based optimization. however explored below obtained best persection evaluate bug-b scheme comparing currently best performing approaches. based comparison reference algorithms quite straightforward also relate bug-b performance results performance similar algorithms. conducted numerous experiments using various functions generating artiﬁcial data varying degrees observation noise. full range empirical results show trend however report performance representative subset experiment conﬁgurations involving uni-modal multi-modal functions varying degrees noise resolutions. performance measured terms regret diﬀerence rewards expected successive rounds would obtained always selecting optimal point. experiment conﬁgurations ensemble independent replications diﬀerent random number streams performed minimize variance reported results. order investigate performance schemes broad spectrum environments test schemes using three diﬀerent representative functions sloped single maxima peaked multiple local maxima particularly similar global maxima. investigate performance varying degrees noise introduced i.i.d. gaussian observation noise employing diverse range noise levels regret measure non-trivial provide clariﬁcation here. brief regret seen diﬀerence rewards expected successive pulls would obtained pulling optimal arm. exemplify assume reward yields value penalty associated value implies expected utility pulling thus optimal regret plays would become table reports average regret multiple functions diﬀerent number time steps. exempliﬁed fig. pursuing strategy bug-b needs observations capture underlying function allowing quickly zoom promising input value regions. eﬀect seen table bug-b performing signiﬁcantly better competing state-of-the-art schemes. also notice bug-b outperforms gaussian process based approach even receiving feedback gradient function. could explained ability bug-b infer gradient information indirectly means noisy function value observations. ﬁndings conﬁrmed plots fig. showing bug-b provides superior performance every time step.the gaussian process based approach better bug-b without gradient feedback time step bug-b gradients slightly better remainder time steps. table summarises performance diverse range noise levels bug-b consistently superior approach across noise levels without feedback gradient. interestingly gradient descent improves performance range noise levels. explained table summarises computational performance. brevity model structure bug-g lends eﬃcient computation exploiting model structure local computation. leads linear increase computation time respect number observations opposed much computationally expensive gaussian process based approach experiments gradient functions pre-calculated making gradient descent computationally extremely eﬃcient. paper proposed novel approach global optimisation bandit based gradient based learning combined. bayesian model bug-b uniﬁes paradigms integrated model. heart model stochastic linear approximation function optimised. here gradient function values explicitly related. allows learn noisy function gradient observations well predicting properties across action space support optimisation. proposed accompanying bandit driven exploration scheme bayesian credibility bounds trade exploration exploitation. empirical results demonstrated unifying bandit gradient based learning obtains consistently improved performance across wide spectrum environments. furthermore even gradient feedback unavailable ﬂexibility model including gradient prediction allows still outperform competing approaches although smaller margin. pervasiveness bandit based optimisation scheme opens improved performance meta-optimisation applications gradient information readily available. future work propose pioneering results expanded number directions. first bug-b needs generalised cover multi-dimensional functions. additionally formal regret bounds asymptotic properties needs established. finally would interesting investigate bug-b leveraged novel application areas meta-learning neural networks.", "year": 2017}