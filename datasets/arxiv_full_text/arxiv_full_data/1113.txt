{"title": "Smooth Neighbors on Teacher Graphs for Semi-supervised Learning", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "The paper proposes an inductive semi-supervised learning method, called Smooth Neighbors on Teacher Graphs (SNTG). At each iteration during training, a graph is dynamically constructed based on predictions of the teacher model, i.e., the implicit self-ensemble of models. Then the graph serves as a similarity measure with respect to which the representations of \"similar\" neighboring points are learned to be smooth on the low dimensional manifold. We achieve state-of-the-art results on semi-supervised learning benchmarks. The error rates are 9.89%, 3.99% for CIFAR-10 with 4000 labels, SVHN with 500 labels, respectively. In particular, the improvements are significant when the labels are scarce. For non-augmented MNIST with only 20 labels, the error rate is reduced from previous 4.81% to 1.36%. Our method is also effective under noisy supervision and shows robustness to incorrect labels.", "text": "paper proposes inductive semi-supervised learning method called smooth neighbors teacher graphs iteration training graph dynamically constructed based predictions teacher model i.e. implicit self-ensemble models. graph serves similarity measure respect representations similar neighboring points learned smooth dimensional manifold. achieve state-of-the-art results semi-supervised learning benchmarks. error rates cifar- labels svhn labels respectively. particular improvements signiﬁcant labels scarce. non-augmented mnist labels error rate reduced previous method also effective noisy supervision shows robustness incorrect labels. collecting fully labeled dataset often expensive time-consuming semi-supervised learning developed learn good classiﬁers small labeled data large amount unlabeled data success relies smoothness assumption data points close likely label. special case named cluster density separation assumption states decision boundary density regions crossing high density regions based assumptions many methods developed. example graph-based methods deﬁne similarity data points graph enforce smoothness predictions respect graph structure. recently great advances deep neural networks representation learning remarkable results achieved among works perturbation-based discriminative learning demonstrated great promise. speciﬁcally methods encourage predictions robust random local variations inputs. example self-ensembling methods model temporal ensembling mean teacher deﬁne consistency loss encourage smooth predictions data different epochs different perturbations. instead isotropic smoothing virtual adversarial training smooths output distribution local adversarial perturbations data point. approaches fuse inputs coherent clusters smoothing mapping function locally. however consider perturbations around single data point ignoring connections data points therefore fully utilizing information unlabeled data structure clusters manifolds. extreme situation happen function smooth vicinity unlabeled point smooth vacancy among them. artifact could avoided unlabeled data structure taken consideration. known data points similar tend form clusters therefore connections similar data points help fusing become tighter effective. paper present smooth neighbors teacher graphs semi-supervised learning method considers neighboring structure induce smoothness data manifold local clusters. deﬁning teacher graph encode neighborhood relationships model encourages invariance perturbations added data point neighbors. difference traditional graph-based methods lies graph construction. previous work usually constructs graph advance using prior knowledge manual labeling graph remains ﬁxed following training process instead construct similarity graph using predictions given implicit self-ensemble models dynamically changing model evolves. particular previous methods deﬁne graph based distance input space typically low-level method graph construction leverages full advantages dnns extracting high-level information. since dnns good generalization property layers inputs low-dimensional feature space given learned neighborhood relationships enforce features similar neighbors graph dissimilar non-neighbors. ﬁnally propose doubly stochastic sampling algorithm reduce computational cost large mini-batch sizes. method easily incorporated existing works dnns including generative discriminative approaches. comparing various strong competitors show sntg achieves state-of-the-art results benchmark datasets. problem settings consider semi-supervised classiﬁcation task training consists examples labels others unlabeled. labeled i=l+ unlabeled observation corresponding {xi}n label learn function parameterized solving generic optimization problem pre-deﬁned loss function like cross-entropy loss represents predicted distribution since small portion training data labeled regularization term important leverage unlabeled data. here non-negative regularization parameter controls strongly regularization penalized. called teacher model parameters random perturbations student model parameters perturbations perturbations include input noise network dropout. teacher model deﬁned implicit ensemble previous student models expected give better predictions student model. seen training targets unlabeled inputs student model supposed predict consistently several ways deﬁne teacher model model shares parameters i.e. model evaluates network twice different realizations i.i.d perturbations every iteration minimizes mean squared error. observe that case optimizing objective equivalent minimizing variance prediction. details appendix temporal ensembling temporal ensembling maintains exponentially moving average previous predictions training stage ensemble output deﬁned prediction given current student model training epoch pre-deﬁned momentum. target given epoch debias correction divided factor since target obtained temporal ensembling based network needs evaluated training. mean teacher instead averaging previous predictions every epoch mean teacher improves target quality averaging parameters every iteration generally also framework self-ensembling sense enforcing consistent predictions perturbations. resembles model distinguishes distance metric types perturbation. seen teacher model adversarial perturbation treated student model need follow teacher. perturbation-based methods understood indirectly exploiting low-density separation assumption points near decision boundaries prone alter predictions perturbations thus large consistency losses. explicitly penalized consistency loss keep unlabeled data away decision boundaries density regions concentrated high density regions. perturbation-based methods regularize output smooth near single point locally ignoring cluster structure proven useful graph-based ssl. address issue presenting method sntg enforces neighboring points smooth stronger regularization imposing smoothness single unlabeled point. following formalize approach answering questions deﬁne graph neighbors? induce smoothness neighboring points using graph? building graph teacher model existing graph-based methods depend distance metric input space instead distance space target predictions. approach data points belonging class regarded neighbors. however issue many data points unlabeled know true labels. address problem constructing dynamic graph using targets generated teacher model every iteration becomes better training continues. self-ensembling good choice constructing graph ensemble predictions expected accurate unlabeled data outputs current classiﬁer. used targets student model training shown previous section. inspired that graph constructed teacher model guide student model move correct directions. comparison dynamic teacher graph ﬁxed graph could found section formally target prediction given teacher deﬁned previous section. k-th component denote hard target prediction argmaxk vector indicating probability example class build graph follows choices include computing divergence soft predictions either labeled unlabeled data ensemble predictions treated target labels. labeled data true labels instead i.e. replace marginal difference empirically since supervised loss makes predictions labeled data almost true labels. given graph clarify regularize neighbors smoothness. generally classiﬁer dnns decomposed mapping input space penultimate layer output layer usually parameterized fully-connected layer softmax. hierarchical nature dnns seen low-dimensional feature mapped input space smooth coherent space. feature space expected linearly separable shown common practice following linear classiﬁer sufﬁces. terms approximating semantic similarity instances euclidean distance suitable represents class probabilities. hence natural choice graph guide low-dimensional representations unlabeled data making distinguishable among classes. given similarity matrix sparse graph deﬁne sntg loss choice quite ﬂexible related unsupervised feature learning clustering. traditional choices include multidimensional scaling isomap laplacian eigenmaps utilize contrastive siamese networks since able learn invariant mapping low-dimensional space perform well metric learning face veriﬁcation speciﬁcally loss deﬁned follows pre-deﬁned margin euclidean distance. margin loss constrain neighboring points consistent embeddings. consequently neighbors encouraged consistent predictions non-neighbors pushed apart minimum distance visualizations found section interpretation proposed sntg works well classiﬁer teacher graph facilitate other. graph constructed teacher leads better abstract representations low-dimensional space aids student model give accurate predictions. turn improved student model contributes better teacher model provide accurate targets giving graph encoding learned neighborhood relationships precisely. another perspective method explores manifold assumption classiﬁcation underlies loss i.e. points class encouraged concentrate together sub-manifolds. perturbation-based methods keep decision boundaries away unlabeled data point method encourages unlabeled data points form tighter clusters leading decision boundaries locate clusters. note early work embednn bears similar merit approach. also jointly learns embedding classiﬁer using unlabeled data. however several differences making sntg unique. first inspired model sntg aims induce smoothness using neighbors perturbation-based methods embednn motivated using embedding auxiliary task help supervised tasks. second embednn uses ﬁxed graph deﬁned knearest-neighbor based distance method takes step forward. mentioned section pixel-level distance reﬂect semantic neighborhood relationship well hand graph built embednn ﬁxed graph cannot leverage knowledge distilled classiﬁer thus cannot improved more sntg jointly learns classiﬁer teacher graph stated above. furthermore graph method computed faster embednn much lower dimensional sub-sampling technique introduced next. overall objective components. ﬁrst standard cross-entropy loss labeled second regularization term encourages smoothness predictions single point well neighboring points alg. presents pseudo-code. model belongs end-to-end dnns train using stochastic gradient descent reduce variance sgd-based optimization methods usually adopt mini-batch training. follow common practice construct sub-graph random mini-batch estimate mini-batch size need compute data pairs size total. although step fast computation related overall computational cost iteration slow large reduce computational cost doubly stochastic sampled data pairs construct compute still unbiased estimation speciﬁc iteration sample mini-batch sub-sample data pairs following ramp-up learning rate regularization beginning ramp-down function annealing learning rate end. appendix details. using unlabeled data regularize classiﬁcation problem long rich history. self-training methods iteratively current classiﬁer label unlabeled ones high conﬁdence transductive svms implement cluster assumption keeping unlabeled data away decision boundaries. entropy minimization strong regularization term commonly used minimizes conditional entropy ensure image assigned class high probability avoid class overlap. many traditional graph-based methods often optimize supervised loss labeled data graph laplacian regularization term label propagation pushes label information labeled instance neighbors using predeﬁned distance metric. emphasize work differs traditional methods construction utilization graph described section perturbation regularization exploring smoothness assumption also closely related methods. manifold tangent classiﬁer regularizes predictions smooth respect manifold underlying data distribution. trains contrastive auto-encoders learn manifold enforces outputs insensitive local perturbations along lowdimensional manifold. pseudo-ensemble model ladder network evaluate dnns figure comparison model sntg synthetic datasets. labeled data marked black cross. different colors denote different classes. decision boundaries shown classiﬁers without perturbations enforce consistency predictions makes model robust random perturbations. apart discriminative approaches another line generative models efforts learn input distribution believed share information conditional distribution traditional models gaussian mixtures maximize joint loglikelihood labeled unlabeled data using algorithm. also implements cluster assumption given cluster belongs class. modern deep generative models variational auto-encoder makes scalable employing variational methods combined dnns generative adversarial networks generate samples optimizing adversarial game discriminator generator samples generated viewed another kind data augmentation tell decision boundary lie. example samples generated density regions training data rare based density separation assumption. alternatively pseudo samples could generated high density regions keep away decision boundary thus improve robustness classiﬁer section presents quantitative qualitative results demonstrate effectiveness sntg semi-supervised classiﬁcation comparing various strong competitors. synthetic datasets ﬁrst test well-known moons\" four spins\" synthetic datasets respectively. dataset includes data points label ratio neural networks three hidden layers size leaky relu suggested catgan appendix details. results visualized fig. note model strong baseline failures. speciﬁcally fig. small blob data misclassiﬁed green fig. tail green spin misclassiﬁed red. prediction model supposed smooth enough areas data points blobs. however model still fails identify them. method sntg right side classiﬁcations correct effective utilization neighboring points’ structure. compared fig. decision boundaries fig. also align better spins. experiments demonstrate effectiveness sntg. provide results widely adopted benchmarks mnist svhn cifar-. following common practice randomly sample labels mnist svhn cifar- respectively. test fewer labels non-augmented mnist well svhn cifar- standard data augmentation. results averaged runs different seeds data splits. main results presented tables method surpasses previous state-of-the-arts large margin. network architecture hyper-parameters baselines i.e. perturbationbased methods described section sntg loss needs three extra hyper-parameters table error rates benchmark datasets without augmentation averaged runs. cifar models ladder network catgan improved triple goodsemibadgan model π+sntg vat+ent vat+ent+sntg regularization parameter margin number sub-sampled pairs tune details experimental setup found appendix fair comparison also report best implementation settings covered note best results achieved additional entropy minimization regularization term leading much stronger baseline. evaluate effectiveness sntg best setting vat+ent observe improvement e.g. cifar- without augmentation respectively. fact observed could also improve performance self-ensembling methods added along sntg. keep results clear focus efﬁcacy sntg illustrate results here. moreover reports mean error omits standard deviation another important indicator robustness algorithms. additionally report values along mean errors runs. shown tables sntg applied fully supervised setting method reduces error rates compared self-ensembling methods e.g. cifar- model. indicates supervised learning also beneﬁts additional smoothness learned invariant feature space method. fewer labels. notably shown tables labels scarce e.g. mnist labels svhn labels cifar- labels improvements even signiﬁcant. since labeled data accounts small part adding strong regularizer sntg case helps model learn faster exploring unlabeled data structure. ablation study. reported results based adding sntg loss baselines overall objective already included consistency loss shown alg. quantify effectiveness method table presents evaluation π+sntg compared ablated versions. error rate model uses however using alone yields lower error rate therefore considering neighbors proves strong regularization comparable even favorable complementary. table ablation test cifar- labels without augmentation. denotes supervised loss consistency loss sntg loss. ls+rc equals model ls+rc+rs equals π+sntg. table error rates svhn translation augmentation averaged runs. labels labels model supervised-only model π+sntg tempens tempens+sntg mean teacher mean teacher+sntg vat+ent vat+ent+sntg convergence. potential concern method convergence since information graph likely inaccurate beginning training. however observe divergent case experiments. empirically teacher model usually little better student model training. therefore teacher graph guide student correct direction. furthermore ramp-up important convergence described previous work using ramp-up weighting mechanism supervised loss dominates learning earlier training. training continues student model conﬁdence information given teacher model i.e. target predictions graph gradually contributes learning process. fig. shows model converges well. approach graph dynamically built using outputs teacher model. compare ﬁxed graph based setting cifar- using labels without augmentation. since small portion labels observed training data conduct experiments constructing graph based ﬁxed teacher i.e. predictions pre-trained model training data. classiﬁcation error rate model test using predictions training data construct ﬁxed graph error rate using algorithm training dynamically built graphs scratch π+sntg achieves superior result error rate. three experiments share hyper-parameter settings. fig. shows model outperforms model model ﬁxed graph. reason performance lies using ﬁxed graph like pre-training using dynamic graphs like joint-training. dynamic graph becomes better using information extracted teacher model beneﬁt turn. however ﬁxed graph receive feedbacks model training information based pre-trained model prior knowledge. empirical results support analysis dynamic graph superior ﬁxed graph. sntg beneﬁt unlabeled data also learn noisy supervision. extra experiments supervised svhn show tolerance incorrect labels. true labels training replaced random labels following fig. shows tempens+sntg retains accuracy even labels noisy tempens alone obtains sntg regularization improves robustness generalization performance model. ﬁnally visualize embeddings algorithm model test data settings implemented using tensorboard tensorflow fig. shows representations projected dimension using tsne learned representations model concentrated within clusters potentially easier separate different classes. visualization also consistent assumption analysis. developed sntg semi-supervised learning builds graphs using teacher model enforces smoothness neighbor points method simple effective. empirically outperforms baselines signiﬁcant margin achieves state-of-the-art results several datasets. byproduct also learn invariant mapping dimensional manifold. sntg handle extreme cases fewer labels noisy labels. recently feature matching improved performed well usually generates images strange patterns. interesting ﬁnding method also helps generate realistic images thus improves performance indicates objective generator beneﬁts smooth feature space learned sntg. future work promising theoretical analysis method explore combination generative models.", "year": 2017}