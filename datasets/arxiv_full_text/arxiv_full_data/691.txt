{"title": "Graying the black box: Understanding DQNs", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "In recent years there is a growing interest in using deep representations for reinforcement learning. In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter. Moreover, we propose a new model, the Semi Aggregated Markov Decision Process (SAMDP), and an algorithm that learns it automatically. The SAMDP model allows us to identify spatio-temporal abstractions directly from features and may be used as a sub-goal detector in future work. Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success. Moreover, we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize deep neural networks in reinforcement learning.", "text": "recent years growing interest using deep representations reinforcement learning. paper present methodology tools analyze deep q-networks non-blind matter. moreover propose model semi aggregated markov decision process algorithm learns automatically. samdp model allows identify spatio-temporal abstractions directly features used sub-goal detector future work. using tools reveal features learned dqns aggregate state space hierarchical fashion explaining success. moreover able understand describe policies learned dqns three different atari games suggest ways interpret debug optimize deep neural networks reinforcement learning. reinforcement learning paradigm agent autonomously learns experience order maximize reward signal. learning control agents directly high-dimensional inputs like vision speech long-standing problem known curse dimensionality. countless solutions problem offered including linear function approximators hierarchical representations state aggregation options methods rely upon engineering problem-speciﬁc state representations hence reducing agent’s ﬂexibility making learning tedious. therefore growing interest using nonlinear function approximators general require less domain speciﬁc knowledge e.g. td-gammon unfortunately methods known unstable even diverge used represent action-value function paper done support intel collaborative research institute computational intelligence part ’why deep learning works looking inside deep learning’ icri-ci paper bundle. results paper originally published deep q-network algorithm increased training stability introducing target network using experience replay success demonstrated arcade learning environment challenging framework composed dozens atari games used evaluate general competency achieved dramatically better results earlier approaches showing robust ability learn representations. using deep learning seems promising free lunch. training deep neural network complex two-levels optimization process. inner level hypothesis class learn using gradient descent method. optimization outer level addresses choice network architecture setting hyper-parameters even choice optimization algorithm. optimization inner level analytic high degree optimization outer level currently practitioners tackle problem either trial error methodology exhaustively searching space possible conﬁgurations. moreover deep reinforcement learning also need choose model environment markov decision process i.e. choose discount factor amount history frames represent state choose various algorithms architectures issue representation i.e. allowing agent locally generalize policy across similar states. unfortunately spatially similar states often induce different control rules contrast machine learning problems enjoy local generalization. fact many cases optimal policy smooth function state thus using linear approximation optimal. example atari game seaquest whether diver collected controls outcome submarine surface action therefore optimal policy. another cause control discontinuities given problem states similar representations fact terms number state transitions required reach other. observation also explain lack pooling layers architectures different methods focus temporal structure policy also proposed. methods decompose learning task simpler subtasks using graph partitioning path processing mechanisms given good temporal representation states varying levels convergence guarantees promised main observation work learning internal model domain without explicitly trained discover internal model identifying spatio-temporal abstractions directly learned representation different ways manually clustering state space using hand crafted features semi aggregated markov decision process approximation true learned automatically data. show offer interpretation learned policies understanding show dqns learning temporal abstractions state space hierarchical state aggregation options. temporal abstractions known community mostly manual tools tackle curse dimensionality; however observe ﬁnding abstractions automatically. thus believe analysis explains success reinforcement learning research perspective. debugging propose debugging methods help reduce hyper parameters grid search drl. examples given game modeling termination initial states score over-ﬁtting. goal agents maximize expected total reward learning optimal policy time agent observes state selects action receives reward following agents decision observes next state consider inﬁnite horizon problems cumulative return discounted factor return γt−trt termination step. action-value function measures expected return choosing action state following policy afterwards. optimal action-value obeys fundamental recursion known bellman equation notice off-line algorithm meaning tuples {stat collected agents experience stored later used training. reward clipped range guarantee stability training dqns multiple domains different reward scales. algorithm maintains separate q-networks parameters second parameters θtarget updated every ﬁxed number iterations. order capture game dynamics algorithm represents state sequence history goal behind visualization dnns give better understanding inscrutable black boxes. approaches study type computation performed layer group others explain function computed individual neuron dataset-centric approaches display images data cause high activations individual units. example deconvolution method highlights areas particular image responsible ﬁring neural unit. network-centric approaches investigate network directly without data dataset e.g. synthesized images cause high activations particular units. works used input gradient images cause strong activations since research mostly focused linear function approximations policy gradient methods less familiar visualization techniques attempts understand structure learned agent. suggested saliency maps analyzed pixels affect network predictions most. using method compared standard dueling network architecture. learned embedded markov processes visualized dimensions. analysis based state transition distribution focus distances features learned dqn. t-sne non-linear dimensionality reduction method used mostly visualizing high dimensional data. technique easy optimize proven outperform linear dimensionality reduction methods non-linear embedding methods isomap several research ﬁelds including machine learning benchmarks hyper-spectral remote sensing data t-sne reduces tendency crowd points together center employing heavy-tailed student-t distribution dimensional space. known particularly good creating single reveals structure many different scales particularly important high-dimensional data lies several lowdimensional manifolds. enables visualize different sub-manifolds learned network interpret meaning. abstraction important tool enables agent focus less lower level details task solving task hand. many real-world domains modeled using form abstraction. semi aggregated models important examples extend standard formulation using temporal spatial abstractions respectively. smdp. extends action space allows agent plan skills skill deﬁned triplet deﬁnes states skill initiated intra-skill policy termination probabilities determining skill stop executing. typically either function state time planning skills performed learning state value choosing skill. formally smdp deﬁned ﬁve-tuple states skills smdp transition matrix discount factor smdp reward deﬁned analyzing policies using smdp model shortens planning horizon simpliﬁes analysis. however drawbacks approach. first must consider high complexity state space second smdp model requires identify skills challenging problem easy solution amdp. another approach analyze policy using spatial abstractions state space. reason believe groups states share common attributes similar policy value function possible state aggregation state aggregation wellstudied problem typically involves identifying clusters states amdp clusters replaces states aggregated states potentially advantageous transition probability matrix reward signal policy dimensions decreased e.g. amdp modeling approach drawbacks. first action space modiﬁed therefore planning horizon still intractable. second amdps necessarily markovian figure presents visualization tool. state represented point t-sne color points manually using global features game speciﬁc hand crafted features clicking data point displays corresponding game image saliency possible move states along trajectory using buttons. feature extraction collect statistics state values estimates generation time termination signal reward played action. also extract handcrafted features directly emulator frames player position enemy position number lives features color ﬁlter points t-sne map. ﬁltered images reveal insightful patterns cannot seen otherwise. experience hand-crafted features powerful however drawback using require manual work. ﬁgures heat color t-sne apply t-sne algorithm directly collected neural activations similar input consists game states features since data relatively large pre-processed using principal component analysis dimensionality used barnes-hut t-sne approximation saliency maps generate saliency maps computing jacobian network respect input presenting input image maps helps understand pixels image affect value prediction network most. analysis using features able understand common attributes given cluster moreover analyzing dynamics clusters identify hierarchical aggregation state space. deﬁne clusters clear entrance termination areas options interpret agent policy there. options able derive rules initiation termination figure left illustration state aggregation skills. primitive actions cause transitions states skills induce transitions samdp states right modeling approaches analyzing policies. paper propose model combines advantages smdp amdp approaches denote samdp. samdp modeling aggregation deﬁnes states skills allowing analysis spatio-temporal abstractions however samdps still necessarily markovian. summarize different modeling approaches figure rest section devoted explaining stages building samdp model feature selection state aggregation skill identiﬁcation inference model selection. feature selection. deﬁne mapping states features mapping function features higher level abstractions feature representation signiﬁcant effect quality resulting samdp model visa versa good model point good feature representation. aggregation spatio-temporal clustering. goal aggregation mapping feature space amdp state space typically clustering algorithms assume data drawn i.i.d distribution. however problem data generated therefore violates assumption. alleviate problem using different approaches. first decouple clustering step samdp model. done creating ensemble clustering candidates building samdp model stage explain non-analytic outer optimization loop choose candidates based spatio-temporal evaluation criteria. second introduce novel extension celebrated k-means algorithm enforces temporal coherency along trajectories. vanilla k-means algorithm cluster mean assigned point cluster closest mean point chosen modiﬁed step follows dimensions correspond single point expanded dimensions point assigned cluster neighbors along trajectory also close thus enforcing temporal coherency. also experienced clustering methods spectral clustering hierarchical agglomerative clustering entropy minimization skill identiﬁcation. deﬁne samdp skill uniquely single initiation state single termination state formally time agent enters amdp state state chooses skill according samdp policy follows skill policy time steps reaches state st+k deﬁne skill length a-priori skill policy infer skill length data. skill policies model deﬁne explicitly observe later model successfully identiﬁes skills localized time space. inference. given samdp states skills infer skill length samdp reward samdp probability transition matrix observations. skill length inferred skill averaging number states visited since entering samdp state leaving samdp state skill reward inferred similarly using equation inference samdp transition matrices puzzling since probability seeing next samdp state depends dynamics agent policy state space. turn discuss infer matrices observing transitions state space. goal infer quantities samdp transition probability matriij measures probability moving state given skill chosen. matrices deﬁned uniquely deﬁnition skills deterministic probability matrices. probability moving state given skill chosen quantity involves according agent samdp policy samdp transition probability matrices agent policy. however since samdp transition probability matrices deterministic equivalent agent policy samdp state space. therefore inferring transitions samdp states directly infer agent’s samdp policy. given deterministic environment agent nearly deterministic policy intuitive assume would observe nearly deterministic samdp policy. however sources increase stochasticity samdp policy stochasticity accumulated along skill trajectories. aggregation process approximation. given samdp state contain real state therefore skill. performing inference case simply observe stochastic policy chooses skills random. thus likely infer stochastic samdp transition matrix although samdp transition probability matrices deterministic even environment deterministic policy near deterministic. stage explain choose different samdp model candidates. advantages choosing multiple samdps. first different hyper parameter tune. examples number samdp states window size clustering algorithm. second randomness aggregation step. hence clustering multiple times picking best result potentially yield better models. therefore developed evaluation criteria allow select best model motivated hallak follow occams razor principle simplest model best explains data. value mean square error measures consistency model observations. estimator given stands samdp value function given policy vsam given vsam measured samdp policy. inertia minµj∈c. inertia measures variance inside clusters encourages spatial coherency. motivated ncut spectral clustering deﬁne intensity factor fraction out/in cluster transitions. however deﬁne edges states connected along trajectory give equal weights minimizing intensity factor encourages longer duration skills. entropy deﬁned samdp probability transition matrix follows pij}. entropy encourages clusters less skills i.e. clusters section demonstrate method gridword domain well three atari games pacman seaquest breakout domain provide short description domain optimal policy detail hand-crafted features designed domain analyze dqn’s policy using manual approach samdp model derive conclusions. analyze cross-domain observations representation initial terminal states inﬂuence score pixels atari. ﬁnish additional experiments samdp model. show consistent suggest novel shared autonomy application. figure state-action diagrams gridworld problem. diagram relate individual states primitive actions. smdp diagram edge colors represent different skills. amdp diagram clusters formed using spatial aggregation original state. samdp diagram clusters found transforming state space. intra-cluster transitions used explain skills inter-cluster transitions loyaly explain governing policy. gridworld description. agent placed origin goal reach green ball return. hand-crafted features. state given coordinates indicates whether agent reached ball not. policy trained skills following mankowitz analysis. visualize learned policy using maze coordinates different modeling approaches. graph consists large number states. also difﬁcult understand skills agent using. smdp graph number states remain high however coloring edges skills helps understand agent’s behaviour. unfortunately producing graph seldom possible rarely receive information skills. hand abstracting state space done easily using state aggregation. however amdp graph clusters aligned played skills routes leading ball overlap. samdp. following state representation maze width. transformation ﬂips translates states routes ball disentangled clusters aligned skills. understanding behaviour agent possible examining inter-cluster intracluster transitions observation gridworld domain correct representation hirerchy discovered samdp model. next experiments show capable learn representation automatically. setup atari data collection. record neural activations last hidden layer value estimations game states t-sne applied neural activations data non-linear dimensionality reduction method particularly good creating single reveals structure many different scales. result compact well-separated representation easy fast cluster. samdp algorithm details. feature selection. t-sne coordinates value coordinate coordinate normalized zero mean unit variance. also experimented conﬁgurations using activations without t-sne different normalization. however found conﬁguration fast gives nice results. samdp clustering skill identiﬁcation follow section additional details. samdp inference. approximations inference stage found work well overlook transitions small skill length truncate transitions less probability. samdp model selection. perform grid search parameters number clusters window size found models larger cumbersome analyze. select best model following ﬁrst sort models four evaluation criteria best worst. then iteratively intersect p-preﬁx sets starting -preﬁx. stop intersection non-empty choose conﬁguration intersection. also measure p-value chosen model. null hypothesis choose samdp model constructed random clusters. tested random samdp models none scored better chosen model breakout description. breakout layer bricks lines screen. ball travels across screen bouncing side walls. brick ball bounces away brick destroyed player receives reward. player loses turn ball touches bottom screen. prevent happening player movable paddle bounce ball upward keeping play. highest score achievable player done eliminating exactly screens bricks worth points each. good strategy breakout leading ball bricks digging tunnel side bricks block. enables agent achieve high reward safe losing ball. introducing discount factor breakout’s strategy becomes even favorable since achieves high immediate reward. analysis. figure presents t-sne breakout. agent learns hierarchical policy carve tunnel left side screen keep ball bricks long possible. clusters agent carving left tunnel. agent enters clusters exit tunnel carved identify clusters landmark option. clusters separated ball position direction. cluster ball figure breakout tunnel digging option. left states agent visits entered option clusters ﬁnishes carve left tunnel marked red. right dynamics displayed arrows t-sne map. option termination zone marked black annotation corresponds carving left tunnel. transitions clusters clusters pass singular point. heading toward tunnel cluster ball right side cluster ball bounces back tunnel hitting bricks. fewer bricks remain tunnel value gradually rising till tunnel carved value maximal tunnel carved option terminated agent moves clusters differentiated ball position regard bricks cluster ball bricks them. clusters represent termination initial states respectively cluster created emulator allows ball pass tunnel without completely carving agent learned represent incident cluster assigned high-value estimates observation interesting since indicates agent learning representation based game dynamics pixels. coloring t-sne time identify policy downsides. states remaining bricks visited multiple times along trajectory states ball bounces without hitting bricks causes ﬁxed reﬂection pattern indicating agent stuck local optima. discuss inability agent perform well second screen game section samdp. figure presents resulted samdp game breakout. first observe algorithm managed identify policy hierarchy automatically clusters consistent manual clusters. also observe transitions clusters sparse implies entropy samdp. finally inspecting mean image cluster reveals insights nature skills hiding within uncovers policy hierarchy. figure samdp visualization breakout. state represented t-sne coordinates colored value estimate samdp states visualize mean state mean t-sne coordinate. edge samdp states represent skill numbers edges correspond inferred samdp policy. description. seaquest player’s goal retrieve many treasure-divers dodging blasting enemy subs killer sharks oxygen runs out. game begins enemy worth points rescued diver worth every time agent surface divers killing enemy increased points maximum moreover agent awarded extra bonus based remaining oxygen level. however surfaces less divers oxygen ﬁlls bonus surfaces none loses life. dqn’s performance seaquest inferior human experts makes seaquest hard shooting enemies rewarded immediately rescuing divers rewarded divers collected rescued level. moreover bonus points collecting divers diminished reward clipping. sparse delayed reward signal requires much longer planning harder learn. hand-crafted features. extract features player position direction movement oxygen level number enemies number available divers collect number available divers number lives. analysis. figure shows t-sne divided different clusters. notice main partitions t-sne clusters oxygen level amount collected divers also identiﬁed partitions clusters refuelling clusters thus indicating clear option termination structure. figure salincy maps states available diver. left diver noticed saliency misunderstood enemy shot center diver enemy noticed network. right diver unnoticed network. agent distinguishes states collected diver figure implies agent understand concept collecting diver sometimes treats enemy. moreover clusters share similar value estimate highly correlated amount remaining oxygen number present enemies. however states available collected diver raise value estimates states close refueling. moreover agent never explores bottom screen collects divers. result agent’s policy kill many enemies possible avoiding shot. hasn’t collected diver agent follows sub-optimal policy ascends near level oxygen decreases. there continues shoot enemies collecting divers. however also learned surface entirely without diver. agent collects diver moves blue shaded clusters identify refuel option. noticed option initiated different zones based oxygen level singular termination cluster diver taken high level oxygen enters option northern part cluster otherwise enter point along direction option cluster agent keeps following normal shooting policy. eventually agent reaches critical level oxygen ascends level. agent jumps fueling cluster fueling cluster identiﬁed rainbow appearance level oxygen increasing rapidly. however refuel option learned perfectly. area another refuel cluster there agent exploit oxygen ascending description. pacman agent navigates maze chased ghosts. agent positively rewarded collecting bricks. episode ends predator catches agent agent collects bricks. also bonus bricks corner maze. bonus bricks provide larger reward importantly make ghosts vulnerable short period time cannot kill agent. occasionally bonus appears short time providing high reward collected. hand-crafted features. extract features player position direction movement number left bricks minimal distance player predators number lives ghost mode indicates predators vulnerable bonus feature indicate highly valued appears. analysis. figure shows t-sne colored value function figure shows tsne colored number left bricks examples states cluster. clusters well partitioned number remaining bricks value estimates. moreover examining states cluster clusters share speciﬁc bricks pattern agent location. figure also learn agent collects bonus bricks speciﬁc times order. thus conclude agent learned location-based policy focused collecting bonus bricks avoiding ghosts. agent starts cluster heading bottom-left bonus brick collecting cluster collected moves cluster heading top-right bonus brick collects cluster agent moving clusters taking bottom-right top-left bonus bricks respectively. also identiﬁed cluster termination clusters cluster hiding cluster agent hides ghosts top-left corner. effects reward-clipping clearly seen case bonus box. cluster comprises states visible bonus box. however states assigned lower value. samdp. figure presents resulted samdp game pacman. sparsity transitions clusters indicates agent spatio-temporally located meaning spends long times deﬁned areas state space suggested figure algorithm requires speciﬁc treatment initial terminal states however clear check treatment works well. therefore show t-sne maps different games focus termination initial states figure terminal states mapped successfully singular zone however initial states also mapped singular zones assigned wrong value predictions. following observations suggest investigate different ways model initial states i.e. replicating frame instead feeding zeros test figure terminal initial states. terminal states mapped singular zone. bottom initial states mapped singular zones t-sne dynamics representation. atari games include multiple repetitions game. agent ﬁnished ﬁrst screen presented another distinguished score accumulated ﬁrst screen. therefore agent might encounter problems generalizing screen over-ﬁts score pixels. breakout example current state architectures achieves game score around points maximal available points suggesting agent somehow learning generalize level. investigated effect score pixels network predictions. figure shows saliency maps different games supporting claim basing estimates using pixels. suggest investigate this example suggest train agent receive pixels input. model evaluation. perform three simulations evaluate samdp model. first measure vmse criteria deﬁned equation since true value function unknown approximate averaging value estimates cluster sions trajectory reward. given trajectory measure empirical distribution choosing greedy policy state cumulative reward finally present correlation measures state corri corr positive correlation indicates following greedy policy leads high reward. indeed states observe positive correlation supporting consistency model. third evaluation close spirit second one. partition data train test. evaluate greedy policy based train create transition matrices using bottom rewarded trajectories respectively test set. measure correlation greedy policy transition matrices different values clearly seen correlation greedy policy trajectories higher correlation bottom trajectories. eject button motivation experiment stems idea shared autonomy domains errors permitted performance must high possible. idea shared autonomy allow operator intervene decision loop critical times. example known commercial ﬂights auto-pilot returns control human pilots. following experiment show samdp model help identify agent’s behavior deteriorates. setup. evaluate agent create trajectory data evaluate features state divide data train test build samdp model train data. split train data bottom rewarded trajectories re-infer model parameters separately project test data samdp model becomes likely agent transitions coming rather eject average trajectory reward entire test un-ejected trajectories subset. measure performance gain breakout seaquest pacman respectively. eject experiment indicates samdp model used robustify given policy identifying agent going perform well return control human operator agent. work showed features learned state space different submanifolds each different features present. analyzing dynamics clusters able identify hierarchical structures. particular able identify options deﬁned initial termination rules. state aggregation gives agent ability learn speciﬁc policies different regions thus giving explanation success agents. ability automatically detect sub-goals skills also used future component hierarchical algorithm another possibility aggregation mechanism order allocate learning resources clusters better example using prioritized experience replay similar neuro-science reverse engineering methods like fmri reveal structure brain activity demonstrated describe agent’s policy simple logic rules processing network’s neural activity. believe interpretation policies learned agents particular importance itself. first help debugging process giving designer qualitative understanding learned policies. second growing interest applying solutions real-world problem autonomous driving medicine. believe reach goal gain greater conﬁdence agent learning. lastly understanding learned agents help designers develop better algorithms suggesting solutions address policy downsides. conclude deep network performing well hard understand cause even harder ways improve particularly lack tools needed analyze agent learned therefore left black testing. paper showed gray black understand better dqns work well practice suggested methodology interpret learned policies. research supported part european communitys seventh framework programme grant agreement intel collaborative research institute computational intelligence", "year": 2016}