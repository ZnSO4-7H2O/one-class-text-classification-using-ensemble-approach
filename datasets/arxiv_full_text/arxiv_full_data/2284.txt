{"title": "Extreme Learning Machine with Local Connections", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "This paper is concerned with the sparsification of the input-hidden weights of ELM (Extreme Learning Machine). For ordinary feedforward neural networks, the sparsification is usually done by introducing certain regularization technique into the learning process of the network. But this strategy can not be applied for ELM, since the input-hidden weights of ELM are supposed to be randomly chosen rather than to be learned. To this end, we propose a modified ELM, called ELM-LC (ELM with local connections), which is designed for the sparsification of the input-hidden weights as follows: The hidden nodes and the input nodes are divided respectively into several corresponding groups, and an input node group is fully connected with its corresponding hidden node group, but is not connected with any other hidden node group. As in the usual ELM, the hidden-input weights are randomly given, and the hidden-output weights are obtained through a least square learning. In the numerical simulations on some benchmark problems, the new ELM-CL behaves better than the traditional ELM.", "text": "abstract—this paper concerned sparsiﬁcation input-hidden weights ordinary feedforward neural networks sparsiﬁcation usually done introducing certain regularization technique learning process network. strategy applied since input-hidden weights supposed randomly chosen rather learned. propose modiﬁed called elm-lc designed sparsiﬁcation input-hidden weights follows hidden nodes input nodes divided respectively several corresponding groups input node group fully connected corresponding hidden node group connected hidden node group. usual hiddeninput weights randomly given hidden-output weights obtained least square learning. numerical simulations benchmark problems elm-cl behaves better traditional elm. widely used many ﬁelds outstanding approximation capability popular learning method fnns back-propagation algorithm essentially composed numerous gradient descent searching steps. drawbacks gradientbased learning method slow convergence. extreme learning machine proposed speed convergence randomly choosing rather iteratively learning weights input hidden layers. shown highly efﬁcient easy perform guarantees convergence number hidden nodes required greater equal number training samples usually quite large practice. although necessary condition convergence indeed number hidden nodes quite large. least greater number input nodes following intuitive argument instance data lower dimensional space easier classiﬁed randomly mapped higher dimensional space. therefore many input-hidden weights number input nodes big. related work local connection mention local receptive ﬁelds based elm-lrf designed data sets images important examples high dimensional data sets. elementary visual feature usually lies different positions image datum. remarkably successful neural network dealing kind data sets convolutional neural network convolutional hidden nodes introduced extracting locating elementary visual features different places input image. combines technologies local receptive ﬁelds visual cortex shared weights pooling etc.. like usual feedforward neural networks also trained method needs huge computational capability tune parameters. overcome difﬁculty elm-lrf proposed recently combining network structure learning strategy elm. efﬁciency elm-lrf proved empirically applications generalizations paper concern another class input data comparatively high dimension like image data etc. component input data represents speciﬁc attribute. call hdni data. trying build neural network work hdni data technologies visual cortex shared weights pooling etc. longer applicable. idea local receptive ﬁelds remains useful. deal hdni data propose local connections input hidden nodes divided corresponding groups connected group-group manner. elaborate network structure elm-lc simple example. suppose input dimension nine output dimension illustrated fig. divide nine input nodes three groups group contains three input nodes. group input nodes fully connected corresponding hidden node group containing four hidden nodes connected hidden node groups. then hidden nodes simply fully connected output node. pair input-hidden groups works like input hidden layers small network. hidden node group contains nodes corresponding input node group. small networks collaborate hidden-output weights ﬁnal output. usual input-hidden weights idea elm-lc might illustrated intuitively famous indian fable blind elephant blind feels elephant touching separate part draws conclusion elephant like. fable tells take part whole. hand gentlemen work together synthesizing understandings different parts elephant likely reach complete picture elephant. concept local connection method means blind touches part rather whole elephant lighten work load. comparison task elm-lrf locate elephant picture elephant appear different places picture. task elm-lc identify elephant recognizing synthesizing different parts expectation sparse robust network elm-lc hdni data. numerical simulations carried benchmark problems showing elm-cl behaves better traditional hdni data. remaining part paper organized follows. description algorithm elm-lc presented section supporting numerical simulations provided section iii. conclusions drawn section kind single-hidden-layer feedforward neural network proposed huang idea randomly generate input-hidden weights biases instead tuning iteratively speed learning process intensively transform original nonlinear problem linear problem. choose number hidden nodes data perform different numbers hidden nodes choose number hidden nodes achieves smallest training error. corresponding elm-lc number hidden nodes. d-dimensional vector; function; standard gaussian noise added model better test generalization performance independent noise level regression functions used simulations deﬁned respectively follows. function input nodes hidden nodes input hidden nodes respectively equally divided groups elm-lc. similarly function input nodes hidden nodes divided groups elm-lc. facebook data input nodes hidden nodes divided groups elm-lc. training test errors shown tables mean maximal minimal values errors training test samples presented. errors elm-lc smaller standard three data sets. parameters randomly generated rather iteratively learned. thus original nonlinear system transformed linear system approximately solved usual leastsquare method hidden node number group number large enough satisfy condition grouping below. grouping. divide input nodes roughly equally groups similarly divide hidden nodes corresponding groups hidden node group contains least node corresponding input group. random input-hidden weights. fully connect input node group corresponding hidden node group using randomly chosen weights input node group connection hidden node group. computing hidden-output weights. perform least square learning compute hidden-output weights usual elm. output. output weights. remark. give remark number inputhidden weights. elm-lc input nodes hidden nodes. obviously number inputhidden weights elm-lc input nodes hidden nodes divided groups. nodes equally divided number input-hidden weights elm-lc artiﬁcial data sets data facebook used numerical simulation regression problems. classiﬁcation problems three data sets forest types biodegradation ionosphere. usual sigmoid function used activation function hidden layer nodes. proposed elm-lc compared standard elm. algorithm trials data set. ﬁrst data classiﬁcation problem forest types data datum attributes indicating certain characteristics forest types. data divided four classes second data biodegradation data molecular descriptors classes ready biodegradable ready biodegradable third ionosphere data attributes classes ‘good’ returns ‘bad’ returns. detailed information data sets given table forest types data input nodes hidden nodes input hidden nodes respectively equally divided groups elm-lc. similarly biodegradation data input nodes hidden nodes roughly equally divided groups; ionosphere data input nodes hidden nodes equally divided groups. results training test accuracies shown tables elm-lc achieves better training test accuracies usual three data sets. table seen number input-hidden weights elm-lc much smaller elm. paper propose local connections input hidden nodes divided corresponding groups input node group fully connected corresponding hidden node group connected hidden node groups. hence elm-lc sparse input-hidden weights compared usual fully connected elm. usual fully connected input-hidden weights randomly chosen rather iteratively learned hidden-output weights obtained least square learning. elm-lc designed called hdni data comparatively high dimensional input data like image data etc. component input data represents speciﬁc attribute. numerical simulations artiﬁcial data sets four real world data sets show elm-lc achieves better learning test accuracies usual fully connected number hidden nodes. singh sandhu kumar comment volume prediction using neural networks decision trees ieee uksim-amss international conference computer modelling simulation johnson tateishi using geographically-weighted variables image classiﬁcation remote sens. lett. vol. ./... mansouri ringsted ballabio todeschini consonni quantitative structurecactivity relationship models ready biodegradability chemicals chem inf. model. vol. ./ci. kavukcuoglu sermanet y.l. boureau gregor mathieu lecun learning convolutional feature hierarchies visual recognition advances neural information processing systems pﬁster simonyan charles zisserman deep convolutional neural networks efﬁcient pose estimation gesture videos asian conference computer vision classiﬁcation hyperspectral remote sensing image using hierarchical local-receptive-ﬁeld-based extreme learning machine ieee geosci. remote sens. lett. vol. ./lgrs...", "year": 2018}