{"title": "Probabilistic Reasoning via Deep Learning: Neural Association Models", "tag": ["cs.AI", "cs.LG", "cs.NE"], "abstract": "In this paper, we propose a new deep learning approach, called neural association model (NAM), for probabilistic reasoning in artificial intelligence. We propose to use neural networks to model association between any two events in a domain. Neural networks take one event as input and compute a conditional probability of the other event to model how likely these two events are to be associated. The actual meaning of the conditional probabilities varies between applications and depends on how the models are trained. In this work, as two case studies, we have investigated two NAM structures, namely deep neural networks (DNN) and relation-modulated neural nets (RMNN), on several probabilistic reasoning tasks in AI, including recognizing textual entailment, triple classification in multi-relational knowledge bases and commonsense reasoning. Experimental results on several popular datasets derived from WordNet, FreeBase and ConceptNet have all demonstrated that both DNNs and RMNNs perform equally well and they can significantly outperform the conventional methods available for these reasoning tasks. Moreover, compared with DNNs, RMNNs are superior in knowledge transfer, where a pre-trained model can be quickly extended to an unseen relation after observing only a few training samples. To further prove the effectiveness of the proposed models, in this work, we have applied NAMs to solving challenging Winograd Schema (WS) problems. Experiments conducted on a set of WS problems prove that the proposed models have the potential for commonsense reasoning.", "text": "national engineering laboratory speech language information processing department electrical engineering computer science york university canada probabilities associated events calculated posterior probabilities according bayes theorem possible events modeled pre-deﬁned graph structure. however methods quickly become intractable practical tasks number possible events usually large. recent years distributed representations discrete language units continuous vector space gained signiﬁcant popularity along development neural networks main beneﬁt embedding continuous space smoothness property helps capture semantic relatedness discrete events potentially generalizable unseen events. similar ideas knowledge graph embedding proposed represent knowledge bases low-dimensional continuous space using smoothed representation possible reason relations among various entities. however human-like reasoning remains extremely challenging problem partially requires effective encoding world knowledge using powerful models. existing quite sparse even recently created large-scale yago nell freebase capture fraction world knowledge. order take advantage sparse knowledge bases state-of-the-art approaches knowledge graph embedding usually adopt simple linear models rescal transe neural tensor networks although deep learning techniques achieve great progresses many domains e.g. speech image progress commonsense reasoning seems slow. paper propose deep neural networks called neural association model commonsense reasoning. different existing linear models proposed model uses multilayer nonlinear activations deep neural nets model association conditional probabilities possible events. proposed framework symbolic events represented low-dimensional continuous space need explicitly specify dependency structure among events required bayesian networks. paper propose deep learning approach called neural association model probabilistic reasoning artiﬁcial intelligence. propose neural networks model association events domain. neural networks take event input compute conditional probability event model likely events associated. actual meaning conditional probabilities varies applications depends models trained. work case studies investigated structures namely deep neural networks relation-modulated neural nets several probabilistic reasoning tasks including recognizing textual entailment triple classiﬁcation multi-relational knowledge bases commonsense reasoning. experimental results several popular datasets derived wordnet freebase conceptnet demonstrated dnns rmnns perform equally well signiﬁcantly outperform conventional methods available reasoning tasks. moreover compared dnns rmnns superior knowledge transfer pre-trained model quickly extended unseen relation observing training samples. prove effectiveness proposed models work applied nams solving challenging winograd schema problems. experiments conducted problems prove proposed models potential commonsense reasoning. reasoning important topic artiﬁcial intelligence attracted considerable attention research effort past decades besides traditional logic reasoning probabilistic reasoning studied antypical genre order handle knowledge uncertainty reasoning based probability theory probabilistic reasoning used predict conditional probability event given another event state-of-the-art methods probabilistic reasoning include bayesian networks markov logic networks graphical models taking bayesian networks example conditional deep neural networks used model association between events taking event input compute conditional probability another event. computed conditional probability association generalized model various reasoning problems entailment inference relational learning causation modelling work study model structures nam. ﬁrst model standard deep neural networks second model uses special structure called relation modulated neural nets experiments several probabilistic reasoning tasks including recognizing textual entailment triple classiﬁcation multi-relational commonsense reasoning demonstrated dnns rmnns outperform conventional methods. moreover rmnn model shown effective knowledge transfer learning pre-trained model quickly extended relation observing training samples. furthermore also apply proposed models challenging commonsense reasoning problems i.e. recently proposed winograd schemas problems viewed alternative turing test support model training propose straightforward method collect associated cause-effect pairs large unstructured texts. pair extraction procedure starts constructing vocabulary thousands common verbs adjectives. based extracted pairs paper extends models solve winograd schema problems achieves accuracy causeeffect examples. undoubtedly realize commonsense reasoning still much work done many problems solved. detailed discussions would given paper. paper aims model association relationships between events using neural network methods. make clear main work ﬁrst describe characteristics events possible association relationships between events. based analysis event association present motivation proposed neural association models. commonsense reasoning main characteristics events following massive natural situations number events massive means association space model large. sparse events occur dialy life sparse. challenging task ideally capture similarities different events. time association events appears everywhere. consider single event play basketball example shown figure single event would associate many events. person plays basketball would game. meanwhile would injured cases. person could make money playing basketball well. moreover know person plays basketball coached regular game. typical associations events. however need recognize task modeling event association identical performing classiﬁcation. classiﬁcation typically event feature space pre-deﬁned ﬁnite categories classes. event association need compute association probability arbitrary events sample possibly inﬁnite set. mapping relationships event association would many-to-many; e.g. playing basketball could support make money someone makes stock trading could make money well. speciﬁcally association relationships events include causeeffect spatial temporal paper treats general relation considering sparseness useful kbs. paper believe modeling association relationships events fundamental work commonsense reasoning. could model event associations well ability solve many commonsense reasoning problems. considering main characteristics discrete event event association reasons given describing motivation. advantage distributed representation methods representing discrete events continuous vector space provides good capture similarities discrete events. time paper takes account distributed representation neural network methods data-hungry. artiﬁcial intelligence research mining large sizes useful data model learning always challenging. following section paper presents preliminary work data collection corresponding experiments made solving commonsense reasoning problems. next word takes values. causal reasoning represent cause effect respectively. example eating cheesy cakes being happy indicates likely cause binary event model nodes model different effects e.g. growing fat. moreover softmax nodes model multi-valued event e.g. happiness similarly knowledge triple classiﬁcation multi-relation data given triple consists head entity relation binary event indicating whether tail entity true false. finally applications recognizing lexical textual entailment deﬁned premise hypothesis. generally nams used model inﬁnite number events point continuous space represents possible event. work simplicity consider nams ﬁnite number binary events formulation easily extended general cases. compared traditional methods like bayesian networks nams employ neural nets universal approximator directly model individual pairwise event association probabilities without relying explicit dependency structure. therefore nams end-to-end learned purely training samples without strong human prior knowledge potentially scalable real-world tasks. learning nams assume observed examples denoted training normally includes positive negative samples. denote positive samples negative samples independence assumption statistical relational learning likelihood function model expressed follows denotes logistic score function derived numerically computes conditional probability details given later paper. stochastic gradient descent methods used maximize likelihood function leading maximum likelihood estimation nams. following case studies consider structures ﬁnite number output nodes model pair events ﬁnite number binary. ﬁrst model typical associates antecedent event input consequent event output. present anmodel structure called relation-modulated neural nets suitable multi-relational data. paper propose nonlinear model namely neural association model probabilistic reasoning. main goal neural nets model association probability events domain i.e. conditioning possible events domain projected continuous space without specifying explicit dependency structure among them. following ﬁrst introduce neural association models general modeling framework probabilistic reasoning. next describe particular structures modeling typical multi-relational data. figure shows general framework associating events general framework events ﬁrst projected low-dimension continuous space. deep neural networks multi-layer nonlinearity used model likely events associated. neural networks take embedding event input compute conditional probability event event binary models sigmoid node compute takes multiple mutually exclusive values softmax nodes need multiple embeddings nams explicitly specify different events actually related; mutually exclusive contained intersected. nams used separately compute conditional probabilities pair events task. actual physical meaning conditional probabilities varies applications depends models trained. table lists possible applications. firstly represent head entity phrase tail tity phrase embedding vectors similarly relation also represented low-dimensional vector call relation code hereafter. secondly combine embeddings head entity relation feed layer input. consists rectiﬁed linear hidden layers input ck]. feedforward process relation-modulated neural networks particularly multi-relation data following idea propose so-called relationmodulated neural nets shown figure rmnn uses operations dnns project entities relations low-dimensional continuous space. shown figure connect knowledgespeciﬁc relation code hidden layers network. shown later structure superior knowledge transfer learning tasks. therefore layer rmnns instead using linear activation signal computed previous layer relation code follows rmnn models particularly suitable knowledge transfer learning pre-trained model quickly extended relation observing samples relation. case estimate relation code based available samples keeping whole network unchanged. small size relation code reliably estimated small number samples. furthermore model performance original relations affected since model original relation codes changed transfer learning. section evaluate proposed models various reasoning tasks. ﬁrst describe experimental setup report results several reasoning tasks including textual entailment recognition triple classiﬁcation multi-relational commonsense reasoning knowledge transfer learning. word vectors word vectors initialized pre-trained skip-gram word embedding model trained large english wikipedia corpus. dimensions word embeddings experiments; dimensions relation codes relation codes randomly initialized; network structures relu nonlinear activation function network parameters initialized according meanwhile since number training examples probabilistic reasoning tasks relatively small adopt dropout approach training process avoid over-ﬁtting problem; learning process nams need negative samples automatically generated randomly perturbing positive triples {)|e d+}. task provided development tune best training hyperparameters. example tested number hidden layers among initial learning rate among dropout rate among finally select best setting based performance development ﬁnal model structure uses hidden layers learning rate dropout rate respectively experiments. model training learning rate halved performances development decreases. dnns rmnns trained using stochastic gradient descend algorithm. notice models converge quickly epochs. recognizing textual entailment understanding entailment contradiction fundamental language understanding. conduct experiments popular recognizing textual entailment task aims recognize entailment relationship pair english sentences. experiment snli dataset conduct -class experiments instances labelled entailment converted contradiction experiments. snli dataset contains hundreds thousands training examples useful training model. since data include multirelational data investigate structure task. ﬁnal result along baseline performance provided listed table model achieves considerable improvements various traditional methods. indicates better model entailment relationship natural language representing sentences continuous space conducting probabilistic reasoning deep neural networks. triple classiﬁcation multi-relational section evaluate proposed models popular knowledge triple classiﬁcation datasets namely predict whether triple relations hold based training facts database. dataset contains unique entities involving different relations total dataset covers relations entities. table summarizes statistics datasets. goal knowledge triple classiﬁcation predict whether given triple correct not. ﬁrst training data learn models. afterwards development tune global threshold make binary decision triple classiﬁed true otherwise false. ﬁnal accuracy calculated based many triplets test classiﬁed correctly. experimental results datasets given table compare models methods reported datasets. results clearly show methods achieve comparable performance triple classiﬁcation tasks yield consistent improvement existing methods. particular rmnn model yields absolute improvements popular neural tensor networks respectively. rmnn models much smaller number parameters scale well number relation types increases. example rmnn models millions parameters millions. although rescal transe models millions parameters size goes quickly tasks thousands relation types. addition training time rmnn much shorter transe since models converge much faster. example obtained least times speedup hereafter. building ﬁrst select facts conceptnet related typical commonsense relations e.g. usedfor capableof. then randomly divide extracted facts three sets train test. finally order create test classiﬁcation randomly switch entities correct triples total ×test triples statistics given table dataset designed answering commonsense questions like camel capable journeying across desert? proposed models answer question calculating association probability {camel capable journey across desert. paper compare methods popular method data overall results given table methods outperform task rmnn models obtain similar performance. furthermore show classiﬁcation accuracy relations rmnn figure show accuracy rmnn varies among different relations notice commonsense relations harder others rmnn overtakes almost relations. knowledge transfer learning knowledge transfer various domains characteristic feature crucial cornerstone human learning. section evaluate proposed models knowledge transfer learning scenario adapt pretrained model unseen relation training samples relation. randomly select relation e.g. causesdesire experiment. relation contains training samples test samples. experiments relations train baseline models transfer learning freeze parameters including weights entity representations learn relation code causesdesire given samples. last learned relation code used classify samples causesdesire test set. obviously transfer learning affect model performance original relations models changed. figure shows results knowledge transfer learning relation causesdesire increase training samples gradually. result shows rmnn performs much better experiment signiﬁcantly improve rmnn relation total training samples causesdesire. demonstrates structure connect relation code hidden layers leads effective learning relation codes relatively small number training samples. relation code. results shown figure strategy obviously improve performance relation especially training samples. however expected performance original relations deteriorates. improves performance relation training samples however performance remaining original relations drops dramatically again rmnn shows advantage transfer learning setting accuracy relation increases accuracy original relations drop slightly figure transfer learning results updating network parameters. left ﬁgure shows results relation right ﬁgure shows results original relations. previous experiments sections tasks already contained manually constructed training data however many cases want realize ﬂexible commonsense reasoning real world conditions obtaining training data also challenging. specifically since proposed neural association model typical deep learning technique lack training data would make difﬁcult train robust model. therefore paper make efforts mine useful data model training. ﬁrst step working collecting cause-effect relationships common words phrases. believe type knowledge would component modeling association relationships discrete events. section describes idea automatic cause-effect pair collection well data collection results. ﬁrst introduce common vocabulary created query generation. that detailed algorithm cause-effect pair collection presented. finally following section present data collection results. common vocabulary query generation avoid data sparsity problem start work constructing vocabulary common words. current investigations construct vocabulary contains verbs adjectives. shown table vocabulary includes verb words verb phrases adjective words. procedure constructing vocabulary straightforward. ﬁrst extract words phrases wordnet conducting part-of-speech tagging large corpus occurrence frequencies words phrases scanning tagged corpus. finally sort words phrases frequency select results. query generation based common vocabulary generate search queries pairing words currently focus extracting association relationships verbs adjectives. even small vocabulary search space large work deﬁne several patterns word phrase based popular semantic dimensions positive-negative activepassive using verbs arrest example contains patterns i.e. therefore query formed arrest would contain possible dimensions shown figure task mining cause-effect relationships words phrases becomes task getting number occurrences possible links. automatic cause-effect pair collection based created queries section present procedures extracting cause-effect pairs large unstructured texts. overall system framework shown figure query searching goal query searching possible sentences contain input queries. since number queries large structure queries hashmap conduct string matching during text scanning. detail searching program starts conducting lemmatizing part-of-speech tagging dependency parsing source corpus. scan corpus begining end. dealing sentence matched words using hashmap. strategy help reduce search complexity linear size corpus proved efﬁcient experiments. subject-object matching based dependency parsing results phrase query would check whether phrase associated least subject object corresponding sentence not. time record whether phrase positive negative active passive. moreover helping decide cause-effect relationships would check whether phrase linked connective words not. typical connective words used work ﬁnally extract cause-effect pairs design simple subject-object matching rule similar work phrases query share subject relationship between straightforward; subject phrase object phrase need apply passive pattern phrase related object. subject-object matching idea similar work proposed using query example. sentence arrested robbed obtain dependency parsing result shown figure verb arrest share subject pattern arrest passive occurrence corresponding association link i.e. link pattern pattern arrest data collection results table shows corpora used collecting causeeffect pairs corresponding data collection results. extract approximately pairs different corpora. based experiments described previous sections could conclude neural association model potential effective commonsense reasoning. evaluate effectiveness proposed neural association model paper conduct experiments solving complex winograd schema challenge problems winograd schema commonsense reasoning task proposed recent years treated alternative turing test task would interesting whether neural network methods suitable solving problem. section describes progress made attempting meet winograd schema challenge. making clear main task winograd schema ﬁrstly introduce high level. afterwards introduce system framework well corresponding modules proposed automatically solve winograd schema problems. finally experiments discussions human annotated causeeffect dataset discussion presented. winograd schema winograd schema evaluates system’s commonsense reasoning ability based traditional difﬁcult natural language processing task coreference resolution winograd schema problems carefully designed task cannot easily solved without commonsense knowledge. fact even solution traditional coreference resolution problems relies semantics world knowledge describing detail copy words small reading comprehension test involving single binary question. examples trophy would brown suitcase pronoun possessive adjective used sentence reference parties also right sort second party. case males he/him/his; females she/her/her inanimate object it/it/its groups they/them/their. question involves determining referent pronoun possessive adjective. answer always ﬁrst party mentioned sentence answer second party. solving problems easy since required commonsense knowledge quite difﬁcult collect. following sections going describe work solving winograd schema problems neural network methods. system framework paper propose commonsense knowledge required many winograd schema problems could formulized association relationships discrete events. using sentence joan made sure thank susan help given example commonsense knowledge receives help thank gives help him. believe modeling association event receive help thank give help thank make decision comparing association probability models well trained inequality following idea propose utilize data constructed previous section extend models solving problems. design frameworks training models. transmat-nam design apply four linear transformation matrices i.e. matrices transforming cause event effect event. model causeeffect association relationship cause effect events. training models based conﬁgurations straightforward. network parameters including relation vectors linear transformation matrices learned standard stochastic gradient descend algorithm. experiments section introduce current experiments solving winograd schema problems. ﬁrst select cause-effect dataset constructed standard dataset. subsequently experimental setup described detail. presenting experimental results discussions would made section. cause-effect dataset based //www.cs.nyu.edu/faculty/davise/papers/ winogradschemas/ws.html labelled causeeffect problems among available questions experiments. table shows typical examples. problem label three verb phrases corresponding parities pronoun. labelled phrases also record corresponding patterns word respectively. using word lift example generate lift active positive pattern lift active negative pattern lifted passive positive pattern lifted passive negative pattern. example sentence couldn’t lift weak identify weak lift lifted resspectively. commonsense somebody weak would likely effect lift rather lifted. main work solving problem calculate association probability phrases. experimental setup setup causeeffect task similar settings previous tasks. representing phrases neural association models bag-of-word approach composing phrases pre-trained word vectors. since vocabulary experiment contains common verbs adjectives out-of-vocabulary words phrases. based method phrase would useless words contains oov. paper remove testing samples useless phrases results testing cause-effect samples. network settings embedding size schema texts couldn’t lift weak couldn’t lift heavy worm. tasty worm. hungry mary tucked daughter anne could sleep mary tucked daughter anne could work threw schoolbag reached stairs threw schoolbag reached bottom stairs jackson greatly inﬂuenced arnold though lived centuries earlier jackson greatly inﬂuenced arnold though lived centuries later dimension relation vectors hidden layers models hidden layer sizes learning rate experiments. time better control model training learning rates learning embedding matrices relation vectors negative sampling important model training task. transmat-nam system generate negative samples randomly selecting different patterns respect pattern effect event positive samples. example positive training sample hungry causes generate negative samples like hungry causes hungry causes relationvecnam system negative sampling method much straightforward i.e. randomly select different effect event whole vocabulary. example shown here possible negative sample would hungry causes happy hungry causes talk results experimental results shown table results proposed models achieve accuracy cause-effect dataset constructed winograd schemas. speciﬁcally relationvec-nam system performs slightly better transmat-nam system. testing results performs well testing examples. instance call phone scenario proposed generates corresponding association probabilities follows. paul tried call george phone wasn’t sucavailable. available? paul george answer george testing examples model answer questions correctly calculating association probabilities. probability probability inequality relationships association probabilities reasonable commonsense. examples yelled kevin upset. upset? kevin answer kevin example also conveys commonsense knowledge daily life. know somebody upset would likely yell people. meanwhile also likely would comforted people. paper proposed neural association models probabilistic reasoning. neural networks model association probabilities events domain. work investigated model structures namely rmnn nams. experimental results several reasoning tasks shown dnns rmnns outperform existing methods. paper also reports preliminary results nams knowledge transfer learning. found proposed rmnn model quickly adapted relation without sacriﬁcing performance original relations. proving effectiveness models apply solve complex commonsense reasoning problems i.e. winograd schemas support model training task propose straightforward method collect associative phrase pairs text corpora. experiments conducted winograd schema problems indicated neural association model solve problems successfully. however still long ﬁnally achieving automatic commonsense reasoning. want thank prof. gary marcus york university useful comments commonsense reasoning. also want thank prof. ernest davis leora morgenstern charles ortiz wonderful organizations making ﬁrst winograd schema challenge happen. paper supported part science technology development anhui province china fundamental research funds central universities strategic priority research program chinese academy sciences", "year": 2016}