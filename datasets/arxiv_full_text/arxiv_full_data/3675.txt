{"title": "Learning loopy graphical models with latent variables: Efficient methods  and guarantees", "tag": ["stat.ML", "cs.AI", "cs.LG", "math.ST", "stat.TH"], "abstract": "The problem of structure estimation in graphical models with latent variables is considered. We characterize conditions for tractable graph estimation and develop efficient methods with provable guarantees. We consider models where the underlying Markov graph is locally tree-like, and the model is in the regime of correlation decay. For the special case of the Ising model, the number of samples $n$ required for structural consistency of our method scales as $n=\\Omega(\\theta_{\\min}^{-\\delta\\eta(\\eta+1)-2}\\log p)$, where p is the number of variables, $\\theta_{\\min}$ is the minimum edge potential, $\\delta$ is the depth (i.e., distance from a hidden node to the nearest observed nodes), and $\\eta$ is a parameter which depends on the bounds on node and edge potentials in the Ising model. Necessary conditions for structural consistency under any algorithm are derived and our method nearly matches the lower bound on sample requirements. Further, the proposed method is practical to implement and provides flexibility to control the number of latent variables and the cycle lengths in the output graph.", "text": "problem structure estimation graphical models latent variables considered. characterize conditions tractable graph estimation develop eﬃcient methods provable guarantees. consider models underlying markov graph locally tree-like model regime correlation decay. special case ising model number samples required structural consistency method scales number variables θmin minimum edge potential depth parameter depends bounds node edge potentials ising model. necessary conditions structural consistency algorithm derived method nearly matches lower bound sample requirements. further proposed method practical implement provides ﬂexibility control number latent variables cycle lengths output graph. introduction. learning latent variable models observed samples involves mainly tasks discovering relationships observed hidden variables estimating strength relationships. simplest latent variable models so-called latent class model n¨aive bayes model observed variables conditionally independent given state latent factor. extension models latent tree models many hidden variables forming tree hierarchy. latent tree models eﬀective modeling data variety domains evolutionary process gave rise present-day species bio-informatics electronic reprint original article published institute mathematical statistics annals statistics vol. reprint diﬀers original pagination typographic detail. ﬁnancial topic modeling modeling contextual information object recognition computer vision prior works learning latent tree models demonstrate latent tree models learned eﬃciently high dimensions. words number samples required consistent learning much smaller number variables hand. moreover inference latent tree models computationally tractable means simple algorithms belief propagation. despite advantages assumption tree structure restrictive. instance analysis relationships topics words latent tree model posits words generated single topic while reality common words across topics. loopy graphical models able capture relationships retaining many advantages latent tree models. relaxing tree assumption leads nontrivial challenges general learning models np-hard even latent variables developing methods learning fully observed models area active research paper consider structure estimation latent graphical models markov locally tree-like graphs meaning local neighborhoods graph contain cycles. learning graphs many nontrivial challenges parameters regimes models learned consistently eﬃciently? practical learning algorithms? learning guarantees loopy models comparable latent trees? learning depend various graph attributes node degrees girth graph provide answers questions paper. approach contributions. consider learning latent graphical markov models locally tree-like graphs regime correlation decay. regime long-range correlations local statistics converge tree limit. implication correlation decay immediately clear employ available latent tree methods learn local subgraphs consistently long contain cycles. however nontrivial challenge remains merge estimated local subgraphs obtain overall graph estimate? speciﬁcally merging involves matching latent nodes across diﬀerent latent tree estimates clear performed eﬃcient manner. employ diﬀerent philosophy building locally tree-like graphs latent variables. decouple process introducing cycles latent variables output model. initialize loopy graph consisting observed variables iteratively latent variables local probably approximately correct model learning page general discrete models. simplify conditions ising model node binary random variable obtain better intuitions. establish structural consistency number samples required scale number observed variables θmin minimum edge potential depth parameter depends minimum maximum node edge potentials ising model hidden variables sample complexity strengthened matches best known sample complexity learning fully-observed ising models also establish necessary conditions algorithm recover graph structure establish samples necessary structural consistency ∆min minimum degree fraction observed nodes. comparable requirement proposed method uniform node sampling given ∆max maximum degree graph. thus method competitive respect lower bound learning. proposed method number attractive features practical implementation method amenable parallelization makes eﬃcient large datasets. method provides ﬂexibility control length cycles number latent variables introduced output model. method incorporate penalty scores bayesian information criterion trade-oﬀ model complexity ﬁdelity. moreover controlling cycle lengths output model obtain models good inference accuracy simple algorithms loopy belief propagation preliminary experiments newsgroup dataset suggests method discover intuitive relationships eﬃciently also compares well popular latent dirichlet allocation terms topic coherence perplexity. related work. classical latent class models consists multivariate distributions single latent variable observed variables conditionally independent state latent variable hierarchical latent class models generalize models allowing multiple latent variables. however proposed learning algorithms based greedy local search high-dimensional space computationally expensive. moreover algorithms theoretical guarantees. similar shortcomings also hold expectationmaximization based approaches learning latent trees studied extensively before mainly context phylogenetics. thorough overview. eﬃcient algorithms provable performance guarantees available proposed method paper inspired works high-dimensional graphical model selection recent. approaches mainly classiﬁed groups local approaches based convex optimization general agreement success methods related presence correlation decay model work makes connection explicit relates extent correlation decay learning eﬃciency latent models large girth graphs. analogous study eﬀect correlation decay learning fully observed models presented paper ﬁrst work provide provable guarantees learning discrete latent models loopy graphs high dimensions chandrasekharan consider learning latent gaussian graphical models using convex relaxation method. however method cannot easily extended discrete models. moreover incoherence conditions required success convex methods hard interpret verify general. contrast conditions success transparent based presence correlation decay model. bresler considers graphical model selection hidden variables proposes learning markov graph marginal distribution replacing cliques estimated graphs hidden variables. sample complexity results provided method performs poorly high dimensions since aims estimate dense graphs. graphical models. graphical model family multivariate distributions markov accordance particular undirected graph page distribution belonging model class random variable taking value associated node graph. consider discrete graphical models ﬁnite set. edges captures conditional independence relations among random variables. random variables cliques number variables conﬁgurations corresponding clique quantity known log-partition function serves normalize probability distribution. functions known potential functions correspond canonical parameters exponential family. consider multivariate distribution belonging class latent graphical models subset nodes latent hidden. denote hidden nodes denote observed nodes. goal discover presence hidden variables learn unknown graph structure given i.i.d. samples observed variables denote number observed nodes denote total number nodes. consider family graphs bound girth length shortest cycle graph. many graph constructions lead bound girth. example bipartite ramanujan graph page random cayley graphs bounds girth. recently eﬃcient algorithms proposed generate large girth graphs eﬃciently although girth-constrained graphs locally tree-like general global structure makes hard instances learning. speciﬁcally girthconstrained graphs large tree-width known graph average degree least ∆avg girth least tree width ω⌊/⌋) thus learning nontrivial graphical markov models girth-constrained graphs even latent variables large tree width local convergence tree limit. work establishes tractable learning graphical model converges locally tree limit. sufﬁcient condition existence limits regime correlation decay refers property long-range correlations model regime also known uniqueness regime since assumption marginal distribution node asymptotically independent conﬁguration growing boundary. tailor deﬁnition correlation decay node neighborhoods provide deﬁnition below. given graph distribution markov subset pxa|g denote marginal distribution variables subgraph pxa|f denote marginal distribution obtained setting potentials edges zero. thus pxa|f markov graph denote closed neighborhood node sets dist mini∈aj∈a dist denote minimum graph distance. denote nodes within graph distance node denote boundary nodes exactly distance node denote induced subgraph distributions denote norm. words total variation distance marginal distribution distribution markov corresponding distribution markov subgraph decays function graph distance boundary. implies class functions eﬀect graph conﬁguration beyond hops node decaying eﬀect local marginal distributions. distinguish terms graph distance information distances. former refers number edges shortest path connecting nodes graph latter refers quantity background latent tree models. ﬁrst recap results latent tree models subsequently extended general latent graphical models. well known tree-structured markov distributions tree special form factorization given comparing general distributions note tree distributions directly parameterized terms pairwise marginal distributions edges. similarly markov distribution described rooted directed markov model said nonsingular conditional distributions satisfy det)| nonsingular markov model undirected tree directed counterpart equivalent note nonsingularity equivalent positivity markov tree models. particular ising models trees bounded node edge potentials nonsingular. positivity positive probability global conﬁguration node states implies conditional probability node given neighbors cannot degenerate. latent tree models phylogenetic tree models tree-structured graphical models subset nodes hidden latent. goal paper leverage techniques developed learning latent tree models analyze general class latent graphical models. learning latent tree models. learning structure latent tree models extensively studied topic. majority structure learning methods rely presence additive tree metric. additive tree metric obtained considering pairwise marginal distributions tree structured joint distribution. instance mossel considers following metric discrete distributions satisfying nonsingular condition nonsingularity assumption det| distance metric simpliﬁes special distributions example symmetric ising models given negative logarithm correlation node pair consideration quartet-based methods. popular class learning methods based construction quartets splits various procedures merge inferred quartets. quartet structure four observed nodes shown figure recap classical quartet test operating additive tree metric. path structure refers conﬁguration paths given nodes. well known quartets uniquely characterize latent tree. shown subset quartets termed representative quartets suﬃces uniquely characterize latent tree. representative quartets consists quartet edge latent tree shortest distances observed nodes. declares siblings introduces hidden variables. later iterations sibling relationships hidden variables inferred quartets involving children. finally weak edges merged tree algorithm quartet test using distance estimates {bd}ij∈v conﬁdence bound input distance estimates observed nodes {bd}ij∈v conﬁdence bound denote max. output. later modiﬁed version recursive grouping method routine algorithm estimating locally tree-like graphs. neighboring nodes merged based threshold section details. chow–liu grouping. alternative method known chow–liu grouping proposed although theoretical results clgrouping similar earlier results experiments synthetic real data sets revealed signiﬁcant improvement earlier methods terms likelihood scores number hidden variables added. clgrouping method always maintains candidate tree structure progressively adds hidden nodes local neighborhoods. initial tree structure minimum spanning tree observed nodes respect tree metric. method considers neighborhood sets constructs local subtrees local reconstruction property clgrouping makes especially attractive reconstructing girthconstrained graphs. overview algorithm. describe algorithm term localclgrouping structure estimation latent graphical markov models graphs long cycles. algorithm leverages chow– grouping algorithm developed latent tree models described previous section. main intuition learning girth-constrained algorithm test using distance estimates {bd}ij∈v conﬁdence bound threshold merging nodes. input distance estimates observed nodes {bd}ij∈v conﬁdence bound threshold denote initialize quartet graph based reconstructing local parts graph acyclic piecing together. however approach many challenges. first clear local acyclic pieces learned eﬃciently since requires presence additive tree metric. addressed considering models satisfying correlation decay second diﬃcult challenge involves merging reconstructed local latent trees provable guarantees introduction unlabeled latent nodes diﬀerent pieces. circumvent challenge leveraging chow–liu grouping algorithm merging diﬀerent pieces introducing latent nodes. algorithm localclgrouping graph estimation using distance estimatesbdn {bd}ij∈v conﬁdence bound threshold input distance estimates observed nodes {bd}ij∈v conﬁdence bound threshold bound distances used local reconstruction. denote minimum spanning tree based edge weightsbdn. given graph leaf denote nodes graph represents recursive grouping method distance estimates conﬁdence bound threshold mst;bdn). initialize bgbg leaf output observed node nodes considered minimum spanning tree constructed. graph estimate using latent tree algorithm reconstruction note running time polynomial long polynomial time algorithms employed local latent tree reconstruction. proposed method eﬃcient practical implementation divide conquer feature local latent tree-building operations parallelized obtain speedups. real datasets trade-oﬀ model complexity ﬁdelity typically enforced optimizing scores bayesian information criterion criteria easily enforced greedy local search iteration method limits number hidden variables added method. experiments section found method quick implement real synthetic datasets. subsequently establish correctness proposed method natural conditions. require parameter determines node needs chosen function depth girth graph. practice parameter provides ﬂexibility tuning length cycles added graph estimate. large enough obtain latent tree small graph estimate contain many short cycles experiments evaluate performance method diﬀerent values tuning parameters previously discussed context learning latent trees page leverage results here. details section simple example single cycle. demonstrate steps proposed method consider simple case single cycle length nodes cycle hidden hidden node observed leaves shown figure cycle length suﬃciently large information distances local neighborhoods approximately additive depicted figure moreover figure denote observed node closest hidden node terms information distance. minimum spanning tree four nodes zoomed corresponds chain shown figure similarly diﬀerent local neighborhoods observed nodes surrogate relationships similar local msts simple chains merging gives rise graph figure local neighborhood selected merged graph shown figure discover local latent tree structure based information distances shown figure since approximately additive. similarly diﬀerent neighborhoods selected local latent trees discovered distances nearby hidden nodes computed. recover latent cycle graph figure end. results ising models. ﬁrst limit providing asymptotic guarantees ising model extend results nonasymptotic guarantees general discrete distributions. similarly assume bounded node potentials. deﬁne certain quantities depend edge potential bounds. given distribution belonging class ising models edge potentials {θij} node potentials {φi} consider attractive counterpart edge potentials {|θij|} node potentials {|φi|}. maxi∈v atanh) expectation respect distribution denote distribution belonging class ising models nodes edge potential node potentials learning guarantees depend dmin dmax satisfying girth depth depth characterizes close latent nodes observed nodes graph hidden node four observed nodes form shortest quartet middle nodes consider largest graph distance quartet. depth worst-case distance hidden nodes. require following trade-oﬀ girth depth natural assumption minimum degree hidden nodes identiﬁability imposed latent tree models note latent nodes degree lower marginalized obtain equivalent representation observed statistics. relates certain distance bounds bounds edge potentials. intuitively dmin dmax bounds information distances given local tree approximation loopy model precise deﬁnition given note e−dmax e−dmin uses bounds edge potentials impose correlation decay model. natural sample requirement graph estimation algorithm depends weakest edge characterized minimum edge potential θmin. further maximum edge potential θmax characterizes presence/absence long-range correlations model. moreover prescribes extent correlation decay strong enough compared weakest edge model. conditions similar imposed graphical model selection regime correlation decay hidden variables instance upper bound imposed edge potentials limit eﬀect long paths local conditional independence tests. provides trade-oﬀ girth depth intuitively depth needs smaller girth avoid encountering cycles process graph reconstruction. recall parameter algorithm determines neighborhood local msts built ﬁrst step. chosen roughly larger depth order hidden nodes discovered. upper bound ensures distortion additive metric large. parameters latent tree learning routines chosen appropriately depending dmin dmax. section guarantees ising models. establish proposed method correctly estimates graph structure ising model high dimensions. recall depth θmin minimum edge potential dmax dmin learning ising models locally tree-like graphs sample complexity dependent minimum edge potential θmin depth method eﬃcient high dimensions since sample requirement logarithmic number nodes comparison fully observed models special case nodes observed graph locally tree-like strengthen results method establish sample complexity graph estimation matches best known sample complexity learning fully observed ising models sample complexity result holds modiﬁed version localclgrouping threshold applied information distances node local msts formed before. threshold chosen dmax graph estimate obtained union local msts local latent tree routines implemented case. prove improved sample complexity special case matches best known bounds. comparison learning latent trees method extension latent tree methods learning locally tree-like graphs. sample complexity method matches sample requirements learning general latent tree models thus establish learning locally treelike graphs akin learning latent trees regime correlation decay. extension general discrete models. extend results general discrete models provide nonasymptotic sample requirement guarantees success proposed method. local tree approximation. ﬁrst deﬁne notion local tree metric dtree computed limiting model acyclic neighborhood subgraphs respective node pairs. given graph tree ⌊g/⌋ denote induced subgraph girth graph. recall denotes nodes within graph distance cycles encountered thus induced subgraph tree acyclic. recall pxij denotes pairwise marginal distribution between induced distribution markov graph pxij tree denote pairwise marginal distribution induced considering subgraph tree denote natural assumption minimum degree hidden nodes identiﬁability also needed latent trees. assumption states every edge bounded distances local tree approximations. recall special case ising models expressed bounds edge potentials. assumption correlation decay imposes constraint rate function terms girth graph distance threshold used proposed method distance bounds dmin dmax depth implies require depth satisﬁes intuitively constraint implies relatively small compared girth graph large enough every hidden node discovered. enables proposed algorithm correct reconstruct latent trees locally. conﬁdence bound constraint based concentration bounds empirical distances. threshold merging nodes ensures spurious hidden nodes added. conditions inherited latent tree algorithms. provide guarantees reconstructing latent graphical models girth-constrained graphs. conditions success imposed girth graph relatively mild. require girth roughly larger depth correlation decay function suﬃciently strong thus learning girth-constrained graphs akin learning latent tree models wide range conditions. notable additional condition required learning girth-constrained graphs contrast latent trees requirement correlation decay however note suﬃcient condition necessary learnability. instance result establishes pairwise statistics converge locally tree limit attractive ising models strictly positive node potentials without additional constraints parameters. results analysis hold scenarios since require local convergence tree metric. results applicable discrete models extended gaussian models using notion walk-summability place correlation decay according negative logarithm correlation coeﬃcient distance metric; results also extended general linear models multivariate gaussian models gaussian mixtures along lines ﬁrst prove correctness localclgrouping tree limit {d}ij∈v show sample-based consistency. latter based concentration bounds along lines analysis latent tree models additional distortion introduced presence loopy graph. brieﬂy describe proof establishing correctness localclgrouping algorithm dtree girth-constrained graphs. intuitively distances correspond tree metric graph distance dist girth. since localclgrouping infers latent trees locally avoids running cycles thus correctly reconstructs local latent trees. initialization step localclgrouping corresponds correct merge local latent trees assumptions parameter correctness localclgrouping established. guarantees uniform sampling. given guarantees graph reconstruction given arbitrary observed nodes graph. specialize results case uniform sampling nodes provide learning guarantees. analysis provides intuitions relationship fraction sampled nodes resulting learning performance. consider ensemble graphs nodes girth least minimum degree ∆min maximum degree ∆max. denote uniform sampling probability selecting observed nodes. following result depth deﬁne constant necessary conditions graph estimation. provided suﬃcient conditions recovering latent graphical markov models girthconstrained graphs. provide necessary conditions number samples required algorithm reconstruct graph. graph matching graphs common labeled node unlabeled node sets without loss generality number isolated nodes resulting adjacency matrices. edit distance words edit distance minimum number entries diﬀerent permutation unlabeled nodes. context labeled nodes correspond observed nodes unlabeled nodes correspond latent nodes provide necessary conditions graph reconstruction certain edit distance. samples required structural consistency. thus logarithmic number samples necessary polynomial number samples necessary reconstruction. recall ising models uniform sampling observed nodes best-case sample complexity localclgrouping scales section present experimental results real synthetic data. evaluate performance terms perplexity predictive perplexity topic coherence used frequently topic modeling. addition also study trade-oﬀ model complexity data ﬁtting bayesian information criterion experiments conducted using -newsgroup data monthly stock returns companies synthetic data. datasets software code results available http//newport.eecs.uci.edu/anandkumar. ﬁxed latent node degree diﬀerent girths node potentials kept zero edge potentials chosen randomly range ensures model remains regime correlation decay since critical potential atanh newsgroup data. employ latent graphical models topic modeling modeling relationships various words co-occurring documents. hidden variable model thought representing topic topics words document drawn jointly graphical model. latent tree graphical model topics words constrained form tree loopy models relax assumption. consider binary samples keywords selected newsgroup data. binary sample indicates appearance given words posting. samples divided equal groups training test sets learning testing purposes. data. also employ latent graphical models ﬁnancial modeling particular estimating dependencies stock trends diﬀerent companies. data consists monthly stock returns companies listed index experiments dataset allows demonstrate performance algorithm data using gaussian graphical model. gaussian model simplifying assumption reveals interesting relationships companies. note sophisticated kernel models indeed used place gaussian approximation example allows trade-oﬀ model complexity data ﬁtting. addition obtain better generalization avoiding overﬁtting. note proposed method deals structure estimation expectation maximization parameter estimation. newsgroup data compare proposed method model. threshold selection method. recall parameter method controls size neighborhoods local msts constructed ﬁrst step method. earlier presented ranges recovery loopy structure theoretically guaranteed however practice range unknown since model parameters unknown learner also since ground truth respect real datasets. here present intuitive criterion selecting threshold based score. choose range threshold thereby disallowing disconnected components output graph. note choose rmax output latent tree. experiments choose value rmax reference tree model compare outcomes. newsgroup dataset rmin rmax therefore choose experiments newsgroup data. monthly stock returns data rmin rmax choose tuning parameters previously discussed context learning latent trees page leverage results here. number test samples number observed variables thus perplexity monotonically decreasing test likelihood lower perplexity indicates better generalization performance. along lines also evaluate predictive perplexity subset word occurrences xtest observed test data performance predicting rest words evaluated. experiments randomly select half words test samples. model dflda number observed variables number hidden variables model parameterized topic probability matrix -length dirichlet prior. thus perplexity monotonically decreasing score lower perplexity indicates better trade-oﬀ model complexity data ﬁtting. however likelihood score tractable exact evaluation general graphical models since involve partition function. employ loopy belief propagation evaluate them. note exact tree model approximate loopy models. along lines predictive perplexity also consider regularized version represents top- words associated topic number word pairs topic used normalization. found scores good measure human evaluated topic coherence computed using external corpus. also observed using related external corpus gives high pmi. hence experiments choose corpus containing news articles articles bag-of-words dataset. dataset vocabulary words separate articles models words topic selected based topic probability vector. latent graphical models criterion information distances learned model select nearest words topic. experimental results. results synthetic data. observe method outputs graphs similar number latent variables ground truth chosen close bound rmax deﬁned hand lower values lead cycles hidden variables output graph. normalized scores loopy graphs improve number samples shown figure expected since data becomes less noisy samples. figure shows overall improvement normalized score increasing number samples diﬀerent thresholds figure shows variation normalized scores graphs learned using thresholds girth observe normalized score decreases lowest threshold output graph shows signiﬁcant increase latent nodes edges resulting overﬁtting higher thresholds better bic. however threshold results tree model degrades since cycles longer present. graph structure newsgroup data. employ method learn graph structures diﬀerent thresholds newsgroup data controls length cycles. shown figure obtain latent tree obtain loopy models. ﬁrst long cycle appears shown figure combination short long cycles. models cycles eﬀective discovering intuitive relationships. instance comparison proposed method diﬀerent thresholds diﬀerent number topics newsgroup data. deﬁnition perplexity predictive perplexity based test likelihood scores latent tree link computer software missing tree constraint discovered moreover common words across diﬀerent topics tend connect local subgraphs. instance word program used context space program computer programs. similarly word earth used context religion space exploration. perplexity topic coherence newsgroup data. table present results method modeling newsgroup data. model vary number hidden variables contrast method designed optimize number hidden variables need input. note method competitive terms predictive perplexity topic coherence. topic coherence method optimal graph single long cycle short cycles. intuitively model able discover relationships words latent tree unable hand topic coherence degraded suggests adding many cycles counterproductive. however model performs better terms predictive perplexity indicating able evidence observed words prediction test data. moreover latent graphical models outperform models terms predictive perplexity. topic words selected topics given method model given tables number edges hidden variables remain fairly constant large range thresholds. speciﬁcally obtain graph structure another general trend observed improvement score threshold decreases till certain point. graphs learned using shown figures interesting connections companies emerge. latent tree structure figure captures many relationships. particular index node high degree since captures overall trend various companies. companies similar sectors divisions grouped together. instance retail stores target walmart home depot grouped together. however additional relationships emerge threshold decreased cycles added. observe ﬁrst cycle added connects various companies suggests strong interdependencies inﬂuence index. addition cycles emerge threshold decreased further. instance figure cycle connecting aviation company boeing honeywell aviation industry also additionally chemical industry connects companies chemicals. thus newsgroup data companies multiple categories lead cycles underlying graph. edge density threshold study edge density initialization step method function threshold newsgroup stock data. recall initialization step involves building loopy graph observed variables edge density step indicative number cycles added ultimate latent model. observe graphs become denser reduced rmax. however small number edges decreases since nodes sparser neighborhoods. trend seen figures show variation newsgroup stock data. newsgroup data graph density peaks also achieves highest predictive perplexity; table thus direct relationship edge density corresponding predictive perplexity learned model. intuitively number edges increases prediction node involves evidence. however threshold reduced further graphs become less denser also corresponding degradation predictive perplexity. experiments conﬁrm eﬀectiveness approach discovering hidden topics line theoretical guarantees established earlier paper. analysis reveals large class loopy graphical models latent variables learned eﬃciently diﬀerent domains. paper considered latent graphical models markov girth-constrained graphs proposed novel approach structure estimation. established correctness method model regime correlation decay also derived learning guarantees. compared guarantees methods graphical model selection latent variables. ﬁndings reveal latent variables much complexity learning process certain models regimes even number hidden variables large. ﬁndings push realm tractable latent models learning. acknowledgments. authors thank mossel detailed discussions beginning regarding problem formulation modeling algorithmic approaches padhraic smyth david newman evaluation measures topic models. authors also thank editor tony anonymous reviewers whose comments substantially improved paper. abridged version work appears proceedings nips bayati montanari saberi generating random graphs large girth. proceedings twentieth annual acm-siam symposium discrete algorithms siam philadelphia bogdanov mossel vadhan complexity distinguishing markov random ﬁelds. approximation randomization combinatorial optimization. lecture notes computer science springer berlin. bresler mossel reconstruction markov random ﬁelds samples observations algorithms. approximation randomization combinatorial optimization. lecture notes computer science springer berlin. chandrasekaran parrilo willsky latent variable graphical model selection convex optimization. available arxiv.. chandrasekaran parrilo willsky latent variable graphical model selection convex optimization. ann. statist. chen zhang wang eﬃcient model evaluation search based approach latent structure discovery. european workshop probabilistic graphical models. karger srebro learning markov networks maximum bounded tree-width graphs. proceedings twelfth annual acm-siam symposium discrete algorithms siam philadelphia", "year": 2012}