{"title": "Unsupervised Neural Machine Translation", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "In spite of the recent success of neural machine translation (NMT) in standard benchmarks, the lack of large parallel corpora poses a major practical problem for many language pairs. There have been several proposals to alleviate this issue with, for instance, triangulation and semi-supervised learning techniques, but they still require a strong cross-lingual signal. In this work, we completely remove the need of parallel data and propose a novel method to train an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Our model builds upon the recent work on unsupervised embedding mappings, and consists of a slightly modified attentional encoder-decoder model that can be trained on monolingual corpora alone using a combination of denoising and backtranslation. Despite the simplicity of the approach, our system obtains 15.56 and 10.21 BLEU points in WMT 2014 French-to-English and German-to-English translation. The model can also profit from small parallel corpora, and attains 21.81 and 15.24 points when combined with 100,000 parallel sentences, respectively. Our implementation is released as an open source project.", "text": "spite recent success neural machine translation standard benchmarks lack large parallel corpora poses major practical problem many language pairs. several proposals alleviate issue with instance triangulation semi-supervised learning techniques still require strong cross-lingual signal. work completely remove need parallel data propose novel method train system completely unsupervised manner relying nothing monolingual corpora. model builds upon recent work unsupervised embedding mappings consists slightly modiﬁed attentional encoder-decoder model trained monolingual corpora alone using combination denoising backtranslation. despite simplicity approach system obtains bleu points french english german english translation. model also proﬁt small parallel corpora attains points combined parallel sentences respectively. implementation released open source project. neural machine translation recently become dominant paradigm machine translation opposed traditional statistical machine translation systems trained end-to-end take advantage continuous representations greatly alleviate sparsity problem make much larger contexts thus mitigating locality problem. thanks this reported signiﬁcantly improve automatic metrics human evaluation nevertheless reasons described above requires large parallel corpus effective known fail training data enough unfortunately lack large parallel corpora practical problem vast majority language pairs including low-resource languages well many combinations major languages several authors recently tried address problem using pivoting triangulation techniques well semi-supervised approaches methods still require strong cross-lingual signal. work eliminate need cross-lingual information propose novel method train systems completely unsupervised manner relying solely monolingual corpora. approach builds upon recent work unsupervised cross-lingual embeddings thanks shared encoder translation directions uses ﬁxed cross-lingual embeddings entire system trained monolingual data reconstruct input. order learn useful structural information noise form random token swaps introduced input. addition denoising also incorporate backtranslation figure architecture proposed system. sentence language system trained alternating steps denoising optimizes probability encoding noised version sentence shared encoder reconstructing decoder on-the-ﬂy backtranslation translates sentence inference mode optimizes probability encoding translated sentence shared encoder recovering original sentence decoder. training alternates sentences analogous steps latter. spite simplicity approach experiments show proposed system reach bleu points french english bleu points german english standard translation task using nothing monolingual training data. moreover show combining method small parallel corpus improve results obtaining bleu points parallel sentences respectively. manual analysis conﬁrms effectiveness proposed approach revealing system learning non-trivial translation relations beyond word-by-word substitution. remaining paper organized follows. section analyzes related work. section describes proposed method. experimental settings discussed section section presents discusses obtained results. section concludes paper. ﬁrst discuss unsupervised cross-lingual embeddings basis proposal section section addresses statistical decipherment smt-inspired approach build machine translation system unsupervised manner. finally section presents previous work training systems different low-resource scenarios. methods learning cross-lingual word embeddings rely bilingual signal document level typically form parallel corpora closer scenario embedding mapping methods independently train embeddings different languages using monolingual corpora learn linear transformation maps shared space based bilingual dictionary dictionary used earlier work typically contains thousands entries artetxe propose simple self-learning extension gives comparable results automatically generated list numerals used shortcut considerable body work statistical decipherment techniques induce machine translation model monolingual data follows noisy-channel model used concretely treat source language ciphertext model process ciphertext generated two-stage process involving generation original english sequence probabilistic replacement words english generative process modeled using standard n-gram language model channel model parameters estimated using either expectation maximization bayesian inference. approach shown beneﬁt incorporation syntactic knowledge languages involved line proposal word embeddings also shown bring signiﬁcant improvements statistical decipherment machine translation several proposals exploit resources direct parallel corpora train systems. scenario often considered languages little parallel data well connected third language basic approach scenario independently translate source language pivot language pivot language target language. however shown advanced models like teacher-student framework bring considerable improvements basic baseline line johnson show multilingual extension standard architecture performs reasonably well even language pairs direct data given training. addition that several attempts exploit monolingual corpora combination scarce parallel corpora. simple effective approach create synthetic parallel corpus backtranslating monolingual corpus target language time currey showed training system directly copy target language text also helpful complementary backtranslation. finally ramachandran pre-train encoder decoder language modeling. best knowledge ambitious scenario model trained monolingual corpora alone never explored date made important contribution direction. concretely method trains agents translate opposite directions make teach reinforcement learning process. promising approach still requires parallel corpus considerable size warm start whereas work parallel data all. section describes proposed unsupervised approach. section ﬁrst presents architecture proposed system section describes method train unsupervised manner. shown figure proposed system follows fairly standard encoder-decoder architecture attention mechanism concretely two-layer bidirectional encoder another two-layer decoder. rnns cells hidden units dimensionality embeddings attention mechanism global attention method proposed luong general alignment function. however three important aspects shared encoder. system makes encoder shared languages involved similarly johnson instance exact encoder would used french english. universal encoder aimed produce language independent representation input text decoder transform corresponding language. fixed embeddings encoder. systems randomly initialize embeddings update training pre-trained cross-lingual embeddings encoder kept ﬁxed training. encoder given language independent word-level representations needs learn compose build representations larger phrases. discussed section several unsupervised methods train cross-lingual embeddings monolingual corpora perfectly feasible scenario. note that even embeddings crosslingual separate vocabularies language. word chair exists french english would different vector language although would common space. systems typically trained predict translations parallel corpus supervised training procedure infeasible scenario access monolingual corpora. however thanks architectural modiﬁcations proposed above able train entire system unsupervised manner using following strategies denoising. thanks shared encoder exploiting dual structure machine translation proposed system directly trained reconstruct input. concretely whole system optimized take input sentence given language encode using shared encoder reconstruct original sentence using decoder language. given pre-trained cross-lingual embeddings shared encoder encoder learn compose embeddings languages language-independent fashion decoder learn decompose representation corresponding language. inference time simply replace decoder target language generates translation input text language-independent representation given encoder. nevertheless ideal behavior severely compromised fact resulting training procedure essentially trivial copying task. such optimal solution task would need capture real knowledge languages involved would many degenerated solutions blindly copy elements input sequence. case system would best make literal word-by-word substitutions used translate language another inference time. order avoid degenerated solutions make encoder truly learn compositionality input words language independent manner propose introduce random noise input sentences. idea exploit underlying principle denoising autoencoders system trained reconstruct original version corrupted input sentence purpose alter word order input sentence making random swaps between contiguous words. concretely sequence elements make random swaps kind. system needs learn internal structure languages involved able recover correct word order. time discouraging system rely much word order input sequence better account actual word order divergences across languages. training procedure seen instance contrastive estimation spite denoising strategy training procedure still copying task synthetic alterations that importantly involves single language time without considering ﬁnal goal translating languages. order train system true translation setting without violating constraint using nothing monolingual corpora propose adapt backtranslation approach proposed sennrich scenario. concretely given input sentence language system inference mode greedy decoding translate language obtain pseudo-parallel sentence pair train system predict original sentence synthetic translation. note that contrary standard backtranslation uses independent model backtranslate entire corpus time take advantage dual structure proposed architecture backtranslate mini-batch on-the-ﬂy using model trained itself. training progresses model improves produce better synthetic sentence pairs backtranslation serve improve model following iterations. training alternate different training objectives mini-batch mini-batch. given languages iteration would perform mini-batch denoising another mini-batch on-the-ﬂy backtranslation another moreover assuming access small parallel corpus system also trained semi-supervised fashion combining steps directly predicting translations parallel corpus standard nmt. make experiments comparable previous work using french-english germanenglish datasets shared task. following common practice systems evaluated newstest using tokenized bleu scores computed multi-bleu.perl script. training data test proposed system three different settings semi-supervised assume that addition monolingual corpora also access small in-domain parallel corpus. scenario great practical interest might often parallel data could potentially beneﬁt insufﬁcient train full traditional system. purpose used monolingual data unsupervised settings together either random sentence pairs news commentary parallel corpus. supervised traditional scenario access large parallel corpus. focus work setting provide approximate upper-bound proposed system. purpose used combination parallel corpora provided comprise europarl common crawl news commentary language pairs plus gigaword corpus frenchenglish. direct comparison semi-supervised scenario also separate experiments using subsets news commentary alone. note that faithful target scenario make parallel data language pairs development tuning purposes. instead used spanish-english data preliminary experiments also decided hyperparameters without rigorous exploration. corpus preprocessing perform tokenization truecasing using standard moses tools. apply byte pair encoding proposed sennrich using implementation provided authors. learning done monolingual corpus language independently using operations. known effective overcome rare word problem standard less clear would perform challenging unsupervised scenario might difﬁcult learn translation relations subword units. reason also experiments word level unsupervised scenario limiting vocabulary frequent tokens replacing rest special token <unk>. accelerate training discarding sentences elements given proposed system uses pre-trained cross-lingual embeddings encoder described section monolingual corpora described independently train embeddings language using wordvec concretely skip-gram model negative samples context window words dimensions sub-sampling training iterations. public implementation method proposed artetxe embeddings shared space using recommended conﬁguration numeral-based initialization. addition component proposed system resulting embeddings also used build simple baseline system translates sentence word-by-word replacing word nearest neighbor language leaving out-of-vocabularies unchanged. training proposed system done using procedure described section cross-entropy loss function batch size sentences. unsupervised systems using denoising alone well combination denoising backtranslation order better analyze contribution latter. adam optimizer learning rate training dropout regularization drop probability given restrict parallel data development purposes perform ﬁxed number iterations train variant. using pytorch implementation training system took days single titan full unsupervised variant. although observed system fully converged number iterations preliminary experiments decide stop training point order accelerate experimentation hardware constraints. described section greedy decoding training time backtranslation actual inference test time done using beam-search beam size following common practice length coverage penalty might improve reported results. bleu scores obtained tested variants reported table seen proposed unsupervised system obtains strong results considering trained nothing monolingual corpora reaching bleu points french-english bleu points german-english depending variant direction much stronger baseline system word-by-word substitution improvements least cases shows proposed system able beyond literal translations effectively learning context information account internal structure languages. results also show backtranslation essential proposed system work properly. fact denoising technique alone baseline improvements table bleu scores newstest. unsupervised systems trained news crawl monolingual corpus semi-supervised systems trained news crawl monolingual corpus subset news commentary parallel corpus supervised systems trained either subsets full parallel corpus gnmt report best single model scores seen introducing backtranslation test perplexities also conﬁrm this instance proposed system denoising alone obtains per-word perplexity french english whereas backtranslation achieves much lower perplexity emphasize however proposed training procedure would work using backtranslation alone without denoising initial translations would meaningless sentences produced random model encouraging system completely ignore input sentence simply learn language model target language. thus conclude denoising backtranslation play essential role training denoising forces system capture broad word-level equivalences backtranslation encourages learn subtle relations increasingly natural setting. role subword translation observe slightly beneﬁcial german target language detrimental french target language practically equivalent english target language might surprising considering wordlevel system handle out-of-vocabularies always fails translate rare words. closer look however observe that manages correctly translate rare words also introduces errors. particular sometimes happens subword unit rare word gets preﬁxed properly translated word yielding translations like sevagency moreover observe little help translating infrequent named entities. instance observed system translated tymoshenko ebferchenko standard would easily learn copy kind named entities using relations much challenging model unsupervised learning procedure. believe better handling rare words particular named entities numerals could improve results future. addition that results semi-supervised system show proposed model greatly beneﬁt small parallel corpus. note semi-supervised systems differ full unsupervised system either parallel sentences news crawl training alternates denoising backtranslation additionally maximizing translation probability parallel sentences described section seen parallel sentences alone bring improvement bleu points sentences bring improvement points. results much better comparable system trained parallel data showing potential interest approach beyond strictly unsupervised scenario. fact semisupervised system trained parallel sentences even surpasses comparable system trained full parallel corpus cases presumably domain monolingual parallel corpora uses matches test set. source lieu l’a´eroport international angeles. cette controverse croissante autour l’agence provoqu´e beaucoup sp´eculations selon lesquelles l’incident soir r´esultat d’une cyberetait op´eration cibl´ee. nombre total morts octobre plus ´elev´e depuis avril quand personnes avaient ´et´e tu´ees. l’exception l’op´era province reste parent pauvre culture france. supervised system remarkable comparable model uses proposed architecture trains predict translations corresponding parallel corpus obtains poor results compared state note comparable system equivalent semi-supervised system except monolingual corpora consequently denoising backtranslation. such comparable differs standard shared encoder ﬁxed embeddings input corruption relatively poor results comparable model suggest additional constraints system introduced enable unsupervised learning also factor limiting potential performance believe system could improved future progressively relaxing constraints training. instance using ﬁxed cross-lingual embeddings encoder necessary early stages training forces encoder common word representation languages might also limit ultimately learn process. reason could start progressively update weights encoder embeddings training progresses. similarly could also decouple shared encoder independent encoders point training progressively reduce noise level. time note perform rigorous hyperparameter exploration favored efﬁciency performance experimental design hardware constraints. such think considerable margin improve results using larger models longer training times incorporating several well-known techniques order better understand behavior proposed system manually analyzed translations french english present illustrative examples table analysis shows proposed system able produce high-quality translations adequately modeling non-trivial translation relations. instance ﬁrst example translates expression lieu occurred going beyond literal word-by-word substitution. time correctly translates l’a´eroport international angeles angeles international airport properly modeling structural differences languages. shown second example system also capable producing high-quality translations considerably longer complex sentences. nevertheless analysis also points proposed system limitations perhaps surprisingly translation quality often lags behind standard supervised system. particular observe proposed model difﬁculties preserve concrete details source sentences. instance third example april properly translated octobre mistranslated clearly point adequacy issues also understandable given unsupervised nature system remarkable system managed least replace month another month number another close number. believe incorporating character level information might help mitigate issues could instance favor october translation octobre instead selected may. finally also cases ﬂuency adequacy problems severely hinders understanding original message proposed translation. instance last example system preserves keywords original sentence would difﬁcult correctly guess meaning looking translation. concordance quantitative analysis suggests still room improvement opening research avenues future. work propose novel method train system completely unsupervised manner. build upon existing work unsupervised cross-lingual embeddings incorporate modiﬁed attentional encoder-decoder model. using shared encoder ﬁxed cross-lingual embeddings able train system monolingual corpora alone combining denoising backtranslation. experiments show effectiveness proposal obtaining signiﬁcant improvements bleu score baseline system performs word-by-word substitution standard french-english german-english benchmarks. manual analysis conﬁrms quality proposed system showing able model complex cross-lingual relations produce high-quality translations. moreover show combining method small parallel corpus bring improvements showing potential interest beyond strictly unsupervised scenario. work opens exciting opportunities future research analysis reveals that spite solid results still considerable room improvement. particular observe performance comparable supervised system considerably state suggests architectural modiﬁcations introduced proposal also limiting potential performance. reason would like explore progressively relaxing constraints training discussed section additionally would like incorporate character level information model believe could helpful address adequacy issues observed manual analysis finally would like explore neighborhood functions denoising analyze effect relation typological divergences different language pairs. research partially supported google faculty award spanish mineco basque government upv/ehu nvidia grant program. mikel artetxe enjoys doctoral grant spanish mecd. kyunghyun thanks support ebay tencent facebook google nvidia cifar partly supported samsung advanced institute technology references mikel artetxe gorka labaka eneko agirre. learning principled bilingual mappings proceedings conword embeddings preserving monolingual invariance. ference empirical methods natural language processing austin texas mikel artetxe gorka labaka eneko agirre. learning bilingual word embeddings bilingual data. proceedings annual meeting association computational linguistics vancouver canada july association computational linguistics. http//aclweb.org/anthology/p-. dzmitry bahdanau kyunghyun yoshua bengio. neural machine translation jointly learning align translate. proceedings international conference learning representations chen yang yong cheng victor o.k. teacher-student framework zeroproceedings annual meeting asresource neural machine translation. sociation computational linguistics vancouver canada july association computational linguistics. http//aclweb.org/ anthology/p-. kyunghyun bart merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder–decoder statistical machine translation. proceedings conference empirical methods natural language processing doha qatar october association computational linguistics. http//www.aclweb.org/anthology/ anna currey antonio valerio miceli barone kenneth heaﬁeld. copied monolingual data proceedings second conference improves low-resource neural machine translation. machine translation copenhagen denmark september association computational linguistics. http//www.aclweb.org/anthology/w-. andrew quoc semi-supervised sequence learning. advances neural information processing systems http//papers.nips.cc/ paper/-semi-supervised-sequence-learning.pdf. qing kevin knight. large scale decipherment out-of-domain machine translation. proceedings joint conference empirical methods natural language processing computational natural language learning jeju island korea july association computational linguistics. http//www.aclweb.org/anthology/ qing kevin knight. dependency-based decipherment resource-limited machine translation. proceedings conference empirical methods natural language processing seattle washington october association computational linguistics. http//www.aclweb.org/anthology/d-. qing ashish vaswani kevin knight chris dyer. unifying bayesian inference vector space models improved decipherment. proceedings annual meeting association computational linguistics international joint conference natural language processing beijing china july association computational linguistics. http//www.aclweb.org/anthology/ orhan firat kyunghyun yoshua bengio. multi-way multilingual neural machine translation shared attention mechanism. proceedings conference north american chapter association computational linguistics human language technologies diego california june association computational linguistics. http//www.aclweb.org/anthology/n-. orhan firat baskaran sankaran yaser al-onaizan fatos yarman vural kyunghyun cho. zero-resource translation multi-lingual neural machine translation. proceedings conference empirical methods natural language processing austin texas november association computational linguistics. https//aclweb. org/anthology/d-. felix hill kyunghyun anna korhonen. learning distributed representations sentences proceedings conference north american chapter unlabelled data. association computational linguistics human language technologies diego california june association computational linguistics. http //www.aclweb.org/anthology/n-. melvin johnson mike schuster quoc maxim krikun yonghui zhifeng chen nikhil thorat fernand cgas martin wattenberg greg corrado macduff hughes jeffrey dean. google’s multilingual neural machine translation system enabling zero-shot translation. transactions association computational linguistics issn https//transacl.org/ojs/index.php/tacl/article/view/. philipp koehn rebecca knowles. challenges neural machine translation. proceedings first workshop neural machine translation vancouver august association computational linguistics. http//www.aclweb.org/anthology/ angeliki lazaridou georgiana dinu marco baroni. hubness pollution delving cross-space mapping zero-shot learning. proceedings annual meeting association computational linguistics international joint conference natural language processing beijing china july association computational linguistics. http//www.aclweb.org/anthology/ jason kyunghyun thomas hofmann. fully character-level neural machine translation without explicit segmentation. transactions association computational linguistics issn https//transacl.org/ojs/index.php/ tacl/article/view/. thang luong hieu pham christopher manning. bilingual word representations monolingual quality mind. proceedings workshop vector space modeling natural language processing denver colorado june association computational linguistics. http//www.aclweb.org/anthology/w-. thang luong hieu pham christopher manning. effective approaches attention-based proceedings conference empirical methods neural machine translation. natural language processing lisbon portugal september association computational linguistics. http//aclweb.org/anthology/d-. antonio valerio miceli barone. towards cross-lingual distributed representations without parallel text trained adversarial autoencoders. proceedings workshop representation learning berlin germany august association computational linguistics. http//anthology.aclweb.org/w-. prajit ramachandran peter quoc unsupervised pretraining sequence sequence learning. proceedings conference empirical methods natural language processing copenhagen denmark september association computational linguistics. https//www.aclweb.org/anthology/d-. sujith ravi kevin knight. deciphering foreign language. proceedings annual meeting association computational linguistics human language technologies portland oregon june association computational linguistics. http//www.aclweb.org/anthology/p-. rico sennrich barry haddow alexandra birch. improving neural machine translation modproceedings annual meeting association monolingual data. computational linguistics berlin germany august association computational linguistics. http//www.aclweb.org/anthology/ rico sennrich barry haddow alexandra birch. neural machine translation rare words subword units. proceedings annual meeting association computational linguistics berlin germany august association computational linguistics. http//www.aclweb.org/anthology/ noah smith jason eisner. contrastive estimation training log-linear models unlabeled data. proceedings annual meeting association computational linguistics arbor michigan june association computational linguistics. ./.. http//www.aclweb.org/anthology/ samuel smith david turban steven hamblin nils hammerla. ofﬂine bilingual word vectors orthogonal transformations inverted softmax. international conference learning representations pascal vincent hugo larochelle isabelle lajoie yoshua bengio pierre-antoine manzagol. stacked denoising autoencoders learning useful representations deep network local denoising criterion. journal machine learning research yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey jeff klingner apurva shah melvin johnson xiaobing lukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes jeffrey dean. google’s neural machine translation system bridging human machine translation. arxiv preprint arxiv. http//arxiv.org/abs/.. meng zhang yang huanbo luan maosong sun. adversarial training unsuperproceedings annual meeting assovised bilingual lexicon induction. ciation computational linguistics vancouver canada july association computational linguistics. http//aclweb.org/ anthology/p-.", "year": 2017}