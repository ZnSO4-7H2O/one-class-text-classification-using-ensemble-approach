{"title": "Multi-Task Learning of Keyphrase Boundary Classification", "tag": ["cs.CL", "cs.AI", "stat.ML"], "abstract": "Keyphrase boundary classification (KBC) is the task of detecting keyphrases in scientific articles and labelling them with respect to predefined types. Although important in practice, this task is so far underexplored, partly due to the lack of labelled data. To overcome this, we explore several auxiliary tasks, including semantic super-sense tagging and identification of multi-word expressions, and cast the task as a multi-task learning problem with deep recurrent neural networks. Our multi-task models perform significantly better than previous state of the art approaches on two scientific KBC datasets, particularly for long keyphrases.", "text": "keyphrase boundary classiﬁcation task detecting keyphrases scientiﬁc articles labelling respect predeﬁned types. although important practice task underexplored partly lack labelled data. overcome this explore several auxiliary tasks including semantic super-sense tagging identiﬁcation multi-word expressions cast task multi-task learning problem deep recurrent neural networks. multi-task models perform signiﬁcantly better previous state approaches scientiﬁc datasets particularly long keyphrases. scientiﬁc keyphrase boundary classiﬁcation task consists determining keyphrase boundaries labelling keyphrases types according predeﬁned schema. motivated need efﬁciently search scientiﬁc literature summarised keyphrases. several companies working keyphrase-based recommender systems scientiﬁc literature search interfaces scientiﬁc articles decorate graphs nodes keyphrases. keyphrases must dynamically retrieved articles important scientiﬁc concepts emerge daily basis recent concepts typically ones interest scientists. available recently typical approaches therefore rely hand-crafted gazetteers reduce task extracting list keyphrases document instead identifying mentions keyphrases sentences. related common tasks named entity recognition identiﬁcation multi-word expressions neural sequence labelling methods shown useful order overcome small data problem study using widely available data tasks related exploit synergies deep multi-task learning setup. multi-task learning become popular within natural language processing machine learning last years; particular hard parameter sharing hidden layers deep learning models. approach multi-task learning three advantages signiﬁcantly reduces rademacher complexity i.e. risk over-ﬁtting spaceefﬁcient reducing number parameters easy implement. paper shows hard parameter sharing used improve gazetteer-free keyphrase boundary classiﬁcation models exploiting different syntactically semantically annotated corpora well readily available data hyperlinks. contributions study widely underexplored though practice important task scientiﬁc keyphrase boundary classiﬁcation small amount training data available. overcome identifying good auxiliary tasks cast multi-task learning problem. evaluate models across manually annotated corpora scientiﬁc artisentence occurs rd-tec corpus. here interpolation methods loglinear linear interpolation annotated technical keyphrases performance keyphrase related measurements oracle keyphrase labelled miscellaneous. below interested predicting boundaries types keyphrases. multi-task learning approach learning generalisation improved taking advantage inductive bias training signals related tasks. abundant labelled data available auxiliary task little data target task multi-task learning form semi-supervised learning combined distant supervision signal. inducing model sparse target task data lead overﬁtting random noise data relying auxiliary data helps model generalise making easier abstract away noise well leveraging marginal distribution auxiliary input data. representation learning perspective auxiliary tasks used induce representations beneﬁcial target task. caruana also suggests auxiliary task help focus attention induction target task model. finally multi-task learning cast regulariser studies show reductions rademacher complexity multi-task architectures single-task architectures here follow probably common approach multi-task learning known hard parameter sharing. introduced caruana context deep neural networks hidden layers shared among tasks. assume different training contains pairs input-output input quences task dependent. step training process choose random task followed random training instance tagger predict labels suffer loss respect true labels update model parameters. parameters trained jointly sentence i.e. cross-entropy loss sentence employed. task associated independent classiﬁcation function tasks share hidden layers. note experiments consider auxiliary task time. experimental setup perform experiments keyphrase boundary identiﬁcation keyphrase boundary identiﬁcation classiﬁcation metrics measured token-level precision recall micro-average results across keyphrase types. types deﬁned datasets studied. auxiliary tasks experiment auxiliary tasks syntactic chunking using annotations extracted english penn treefollowing søgaard goldberg bank frame target annotations framenet hyperlink prediction using dataset spitkovsky identiﬁcation multi-word expressions using streusle corpus semantic super-sense tagging using semcor dataset following johannsen train models main task auxiliary task time. note datasets auxiliary tasks annotated keyphrase boundary identiﬁcation classiﬁcation labels. datasets semeval rd-tec dataset semeval dataset annotated three keyphrase types rd-tec dataset seven. former test development portion dataset test released yet. randomly split rd-tec computer science physics natural language processing material science number keyphrases proportion singleton keyphrases proportion single-word mentions proportion mentions word length proportion mentions word length proportion mentions word length training test reserving testing. dataset characteristics summarised table important observation semeval dataset contains signiﬁcantly higher proportion long keyphrases dataset. models networks three-layer bi-directional lstms pre-trained senna embeddings. multi-task networks follow training procedure outlined section dimensionality embeddings follow søgaard goldberg using dimensionality hidden layers. dropout input train architectures momentum initial learning rate momentum epochs. baselines baselines finkel lample order compare lexicalised state-of-the-art neural method. implementations released authors re-train models data. results semeval task corpus presented table rd-tec corpus table semeval corpus labelled multi-task learning models outperform examples previous work well singletask bilstm baseline margin. semeval corpus error reduction best labelled model stanford tagger lexicalised finkel model shows surprisingly competitive performance rd-tec corpus points behind best performing labelled model best-performing results lample unlabelled model. hand lower finkel baseline. might model large parameters model state transitions poses difﬁculty small training datasets. overall multi-task models show bigger improvements baselines semeval corpus models achieve better results rd-tec. statistics shown table help explain this. noticeably semeval dataset contains signiﬁcantly higher proportion long keyphrases dataset. interestingly rd-tec contains large proportion keyphrases appear training signiﬁcantly fewer keyphrases keyphrase type seem impact results much high proportion long keyphrases. models struggle semantically vague broad keyphrases long keyphrases especially containing clauses multi-task models generally outperform bilstm baseline long phrases able recognise long keyphrases correctly part reason multi-task models outperform baselines especially semeval dataset contains many long keyphrases. multi-task learning hard sharing hidden layers introduced caruana popularised collobert several variants introduced including hard sharing selected layers sharing parts layers søgaard goldberg show hard parameter sharing effective regulariser also heterogeneous tasks ones considered here. hard parameter sharing studied several tasks including super tagging text normalisation neural machine translation super-sense tagging sharing information achieved extending lstms external memory shared across tasks instance multi-task learning optimise supervised training objective jointly unsupervised training objective shown natural language generation auto-encoding different sequence labelling tasks language modelling. boundary classiﬁcation similar named entity recognition though arguably harder. deep neural networks applied collobert lample successful methods rely conditional random ﬁelds thereby modelling probability output label conditioned label previous time step. lample currently state-of-the-art stack crfs recurrent neural networks. leave exploring models combination multi-task learning future work. keyphrase detection methods speciﬁc scientiﬁc domain often keyphrase gazetteers features exploit citation graphs however previous methods relied corpora annotated type-level identiﬁcation mention-level identiﬁcation applications rely extracting keyphrases unfortunate consequence previous work ignores acronyms short-hand forms referring methods metrics etc. further relying gazetteers makes overﬁtting likely obtaining lower scores out-of-gazetteer keyphrases. present state keyphrase boundary classiﬁcation using data related auxiliary tasks; particular super-sense tagging identiﬁcation multi-word expressions. deep multi-task learning improves signiﬁcantly previous approaches error reductions mostly better identiﬁcation labelling long keyphrases. future work want explore alternative multi-task learning regimes hard parameter sharing experiment additional auxiliary tasks. auxiliary tasks considered standard tasks hyperlink prediction aside. tasks directly relevant predicting layout calls papers scientiﬁc conferences predicting hashtags tweets scientists since data sources contain scientiﬁc keyphrases.", "year": 2017}