{"title": "KGAN: How to Break The Minimax Game in GAN", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Generative Adversarial Networks (GANs) were intuitively and attractively explained under the perspective of game theory, wherein two involving parties are a discriminator and a generator. In this game, the task of the discriminator is to discriminate the real and generated (i.e., fake) data, whilst the task of the generator is to generate the fake data that maximally confuses the discriminator. In this paper, we propose a new viewpoint for GANs, which is termed as the minimizing general loss viewpoint. This viewpoint shows a connection between the general loss of a classification problem regarding a convex loss function and a f-divergence between the true and fake data distributions. Mathematically, we proposed a setting for the classification problem of the true and fake data, wherein we can prove that the general loss of this classification problem is exactly the negative f-divergence for a certain convex function f. This allows us to interpret the problem of learning the generator for dismissing the f-divergence between the true and fake data distributions as that of maximizing the general loss which is equivalent to the min-max problem in GAN if the Logistic loss is used in the classification problem. However, this viewpoint strengthens GANs in two ways. First, it allows us to employ any convex loss function for the discriminator. Second, it suggests that rather than limiting ourselves in NN-based discriminators, we can alternatively utilize other powerful families. Bearing this viewpoint, we then propose using the kernel-based family for discriminators. This family has two appealing features: i) a powerful capacity in classifying non-linear nature data and ii) being convex in the feature space. Using the convexity of this family, we can further develop Fenchel duality to equivalently transform the max-min problem to the max-max dual problem.", "text": "generative adversarial networks intuitively attractively explained perspective game theory wherein involving parties discriminator generator. game task discriminator discriminate real generated data whilst task generator generate fake data maximally confuses discriminator. paper propose viewpoint gans termed minimizing general loss viewpoint. viewpoint shows connection general loss classiﬁcation problem regarding convex loss function -divergence true fake data distributions. mathematically proposed setting classiﬁcation problem true fake data wherein prove general loss classiﬁcation problem exactly negative -divergence certain convex function allows interpret problem learning generator dismissing -divergence true fake data distributions maximizing general loss equivalent min-max problem logistic loss used classiﬁcation problem. however viewpoint strengthens gans ways. first allows employ convex loss function discriminator. second suggests rather limiting nn-based discriminators alternatively utilize powerful families. bearing viewpoint propose using kernel-based family discriminators. family appealing features powerful capacity classifying non-linear nature data convex feature space. using convexity family develop fenchel duality equivalently transform max-min problem max-max dual problem. generative model refers model capable generating observable samples abiding given data distribution mimicking data samples drawn unknown distribution. helps increase ability represent manipulate high-dimensional probability distributions; generative models incorporated reinforcement learning several ways; iii) generative models trained missing data provide predictions inputs missing data works generative model categorized according taxonomy shown left branch taxonomic tree explicit figure density node speciﬁes models come explicit model density function maximum likelihood inference straight-forward explicit objective function. tractability precision inference totally dependent choice density family. family must chosen well-presented true data distribution whilst maintaining inference tractable. explicit density node leftmost tractable density node deﬁnes models whose explicit density functions computationally tractable. well-known models umbrella include fully visible belief nets pixelrnn nonlinear real contrast tractable density node approximate density node points models explicit density function computationally intractable. remedy address intractability approximate true density function using either variation method markov chain generative models trained without model assumption. implicit models pointed umbrella implicit density node. models umbrella based drawing samples pmodel formulate markov chain transition operator must performed several times obtain sample model another existing state-of-the-art model umbrella generative adversarial network actually introduced novel powerful thinking wherein generative model viewed mini-max game consisting players discriminator attempts discriminate true data samples generated samples whilst generator tries generate samples mimic true data samples maximally challenge discriminator. theory behind shows model converges nash equilibrium point resulting generated distribution minimizes jensen-shannon divergence true data distribution seminal really opened line thinking oﬀers foundation variety works however mini-max ﬂavor training really challenging. beside even perfectly train nature jensen-shannon divergence minimization still encounter model collapse issue paper ﬁrst propose view another viewpoint termed minimizing general loss viewpoint. intuitively since hand formulas true generated data distributions elegantly invoke strong discriminator implicitly justify distributions are. concretely distributions away task discriminator much easier small resulting loss; contrast moving closer task discriminator becomes harder increasingly resulting loss. eventually distributions completely mixed resulting loss best discriminator maximized hence come maxmin problem inner minimization ﬁnding optimal discriminator given generator outer maximization ﬁnding optimal generator maximally makes optimal discriminator confusing. mathematically prove given convex data negative -divergence true data fake data distributions certain convex function follows maximize general loss minimize -divergence involving distributions. viewpoint explains practice many loss functions training still gaining good-quality generated samples. furthermore proposed viewpoint also reveals freely employ suﬃcient capacity family discriminators instead limiting nn-based family. bearing observation propose using kernel-based discriminators classifying real fake data. kernel-based family powerful capacity linear convex feature space allows apply fenchel duality equivalently transform max-min problem max-max dual problem. section present related background used work. depart introduction fenchel conjugate well-known notation convex analysis followed introduction fourier random feature used approximate shift-invariance positive semi-deﬁnite kernel. mapping implicitly deﬁned inner product evaluated kernel construct explicit representation idea approximate symmetric positive semi-deﬁnite kernel ﬁnite-dimensional feature mathematical tool behind approximation bochner’s theorem states every shiftinvariant p.s.d kernel represented inverse fourier transform proper distribution below e−iu⊤ωdu employ gaussian kernel exp− u⊤σu parameterized covariance matrix rd×d. choice substituting yields closedform probability distribution network based generator induce generated distribution p.d.f coinciding data distribution realized minimizing jensen-shanon divergence equivalently obtained solving following mini-max optimization problem game theory perspective viewed game players discriminator generator discriminator tries discriminate generated data real data generator attempts make discriminator confusing gradually generating fake data break real data. diagram shown figure since formulation still able generate data distribution regarded implicit density estimation method. mysterious remedy gans employ strong discriminator implicitly justify divergence clarify point rewrite optimization problem follows according optimization problem given generator need train discriminator minimizes general logistic loss data domain including real fake data. using general loss implicitly estimate are. particular general loss small moving closer general loss increases. following section strengthen proving fact substitute logistic loss decreasing convex loss wherein optimization problem equivalently interpreted minimizing certain symmetric -divergence addition challenging obstacle solving optimization problem break mini-max ﬂavor. existing address problem alternately updating discriminator generator cannot accurately solve mini-max problem rendered solutions might accumulatively diverge optimal one. figure diagram generative adversarial networks. generator produces generated samples challenge discriminator discriminator tries diﬀerentiate generated true data. section theoretically show connection problem discriminating real fake data problem minimizing distance start section introduction setting classiﬁcation problem followed proving general loss classiﬁcation problem certain loss function max-min problem also keeps spirit discriminator attempts classify real fake data generator tries makes discriminator confusing. sake simpliﬁcation replace respectively though mathematical soundness lightly loosen. particular need tackle max-min problem given p.s.d symmetric shift-invariant kernel feature consider reproducing kernel hilbert space kernel discriminator family. therefore discriminator parameterized vector using random feature kernel whose random feature hence enforce discriminator family rkhs approximate kernel. discriminator parameterized vector following advantage max-min problem employing powerful family discriminators linear rkhs opens door employ fenchel duality elegantly transform max-min problem max-max problem much easier tame. moreover max-min problem explained using linear models rkhs enforce transformation push-forward distributions maxmin problem maxψ minw alternatively solve max-max problem maxψ maxuv allows update variables simultaneously. inequality becomes equality inequality equality. section point suﬃcient conditions equality. introduce regulizers used kgan. ﬁrst regulizer mainly consists empirical loss training like optimization problem whilst second really adds regularization quantity empirical loss. paper proposed viewpoint gans termed minimizing general loss viewpoint points connection general loss classiﬁcation problem regarding convex loss function certain -divergence true fake data distributions. particular proposed setting classiﬁcation problem true fake data wherein prove general loss classiﬁcation problem exactly negative -divergence certain convex function enables convert problem learning generator minimizing -divergence true fake data distributions maximizing general loss. viewpoint extends loss function used discriminators convex loss function suggests kernel-based discriminators. family appealing features powerful capacity classifying non-linear nature data convex feature space enables application fenchel duality equivalently transform max-min problem max-max dual problem. since rank{e linearly independent vectors inside without loss generality assume combining also linearly fact singular matrix gain base hence independent. represented linear combination base means", "year": 2017}