{"title": "Complex Structure Leads to Overfitting: A Structure Regularization  Decoding Method for Natural Language Processing", "tag": ["cs.LG", "cs.AI", "cs.CL"], "abstract": "Recent systems on structured prediction focus on increasing the level of structural dependencies within the model. However, our study suggests that complex structures entail high overfitting risks. To control the structure-based overfitting, we propose to conduct structure regularization decoding (SR decoding). The decoding of the complex structure model is regularized by the additionally trained simple structure model. We theoretically analyze the quantitative relations between the structural complexity and the overfitting risk. The analysis shows that complex structure models are prone to the structure-based overfitting. Empirical evaluations show that the proposed method improves the performance of the complex structure models by reducing the structure-based overfitting. On the sequence labeling tasks, the proposed method substantially improves the performance of the complex neural network models. The maximum F1 error rate reduction is 36.4% for the third-order model. The proposed method also works for the parsing task. The maximum UAS improvement is 5.5% for the tri-sibling model. The results are competitive with or better than the state-of-the-art results.", "text": "abstract recent systems structured prediction focus increasing level structural dependencies within model. however study suggests complex structures entail high overﬁtting risks. control structure-based overﬁtting propose conduct structure regularization decoding decoding complex structure model regularized additionally trained simple structure model. theoretically analyze quantitative relations structural complexity overﬁtting risk. analysis shows complex structure models prone structure-based overﬁtting. empirical evaluations show proposed method improves performance complex structure models reducing structurebased overﬁtting. sequence labeling tasks proposed method substantially improves performance complex neural network models. maximum error rate reduction third-order model. proposed method also works parsing task. maximum improvement tri-sibling model. results competitive better state-of-the-art results. structured prediction models often used solve structure dependent problems wide range application domains including natural language processing bioinformatics speech recognition computer vision. solve structure dependent problems many structured prediction methods developed. among representative models conditional random ﬁelds deep neural networks structured perceptron models. order capture structural information accurately recent studies emphasize intensifying structural dependencies structured prediction applying long range dependencies among tags developing long distance features global features probabilistic perspective complex structural dependencies lead better modeling power. however case structured prediction problems. noticed recent work tries intensify structural dependencies really beneﬁt expected especially neural network models. example sequence labeling tasks natural increase complexity structural dependencies make model predict consecutive tags position. label word becomes concatenation several consecutive tags. correctly predict label model forced learn complex structural dependencies involved transition label. nonetheless experiments contradict hypothesis. increasing number tags predicted position performance model deteriorates. majority tasks tested performance decreases substantially. show results section argue over-emphasis intensive structural dependencies could misleading. study suggests complex structures actually harmful model accuracy. indeed obvious intensive structural dependencies eﬀectively incorporate structural information less obvious intensive structural dependencies drawback increasing generalization risk. increasing generalization risk means trained models tend overﬁt training data. complex structures instable training thus training likely aﬀected noise data leads overﬁtting. formally theoretical analysis reveals degree structure complexity lowers generalization ability trained models. since type overﬁtting caused structural complexity hardly solved ordinary regularization methods e.g. weight regularization methods regularization schemes used controlling weight complexity. deal problem propose simple structural complexity regularization solution based structure regularization decoding. proposed method trains complex structure model simple structure model. decoding simpler structure model used regularize complex structure model deriving model better generalization power. show theoretically empirically proposed method reduce overﬁtting risk. theory structural complexity eﬀect reducing empirical risk increasing overﬁtting risk. regularizing complex structure simple structure balance empirical risk overﬁtting risk achieved. apply proposed method multiple sequence labeling tasks parsing task. formers involve linear-chain models i.e. lstm models latter involves hierarchical models i.e. structured perceptron models. experiments demonstrate proposed method easily surpass performance simple structure model complex structure model. moreover results competitive state-of-the-art results better state-of-the-arts. best knowledge ﬁrst theoretical eﬀort quantifying relation structural complexity generalization risk structured prediction. also ﬁrst proposal structural complexity regularization regularizing decoding complex structure model simple structure model. contributions work two-fold methodology side propose general purpose structural complexity regularization framework structured prediction. show theoretically empirically proposed method eﬀectively reduce overﬁtting risk structured prediction. theory reveals quantitative relation structural complexity generalization risk. theory shows structure-based overﬁtting risk increases structural complexity. regularizing structural complexity balance empirical risk overﬁtting risk maintained. proposed method regularizes decoding complex structure model simple structure model. hence structured-based overﬁtting alleviated. application side derive structure regularization decoding algorithms several important natural language processing tasks including sequence labeling tasks chunking name entity recognition parsing task i.e. joint empty category detection dependency parsing. experiments demonstrate structure regularization decoding method eﬀectively reduce overﬁtting risk complex structure models. performance proposed method easily surpasses performance simple structure model complex structure model. results competitive state-of-the-arts even better. structure paper organized following. ﬁrst introduce proposed method section then give theoretical analysis problem section experimental results presented section finally summarize related work section draw conclusions section recent work focuses intensifying structural dependencies. however improvements fail meet expectations results even worse sometimes. theoretical study shows reason although complex structure results empirical risk causes high structure-based overﬁtting risk. theoretical analysis presented section according theoretical analysis reduce overall overﬁtting risk complexity-balanced structure. however kind structure hard deﬁne practice. instead propose conduct joint decoding complex structure model simple structure model call structure regularization decoding decoding simple structure model acts regularizer balances structural complexity. structures vary tasks implementation structure regularization decoding also varies. following ﬁrst introduce general framework show speciﬁc algorithms diﬀerent structures. linear-chain structure models sequence labeling tasks hierarchical structure models joint empty category detection dependency parsing task. describing method formally ﬁrst deﬁne input output model. suppose given training example input output. here stands feature space stands space includes possible substructures regarding input position. preprocessing text input transformed sequence corresponding features output structure transformed sequence input position related tags. reason deﬁnition output structure varies tasks. simplicity ﬂexibility directly model output structure. instead regard output structure structural combination output tags position input position related substructure. diﬀerent modeling structured output i.e. diﬀerent output space lead diﬀerent complexity model. instance sequence labeling tasks could space unigram tags sequence yyyi. could also space bigram tags. bigram means output regrading input feature involves consecutive tags e.g. position position then combination bigram tags yyyi. also requires proper handling tags overlapping positions. obvious structural complexity bigram space higher unigram space. point-wise classiﬁcation donated model assigns scores possible output position simplicity denote argmaxy∈y given point-wise cost function scores predicted output based gold standard output model learned structured prediction task suppose could learn models diﬀerent structural complexity simple structure model complex structure model. given corresponding point-wise cost function proposed method ﬁrst learns models separately suppose mapping {y|y complex structure decomposed simple structures. testing prediction done structure regularization decoding models based complex structure model keep description method straight-forward possible without loss generality. however make assumptions. example general algorithm combines scores complex model simple model position addition. however combination method adjusted task simple structure model used. example model probabilistic model multiplication used instead addition. besides number models limited theory long changed accordingly. moreover joint training models aﬀordable models necessarily trained independently. examples intended demonstrate ﬂexibility structure regularization decoding algorithms. detailed implementation considered respect task model. follows show structure regularization decoding implemented typical structures natural language processing i.e. sequence labeling tasks involve linear-chain structures dependency parsing task involves hierarchical structures. focus diﬀerences considerations deriving structure regularization decoding algorithms. needs reminded implementation structure regularization decoding method adapted kinds structures. implementation limited structures settings use. sequence labeling tasks involve linear-chain structures. sequence labeling task reasonable model label sequence maximum probability conditioned sequence observations i.e. words. given sequence observations x··· sequence labels denotes sentence length want estimate joint probability labels conditioned observations follows model preceding joint probability directly number parameters need estimated extremely large makes problem intractable. existing studies make markov assumption reduce parameters. also make order-n markov assumption. diﬀerent typical existing work decompose original joint probability localized order-n joint probabilities. multiplication localized order-n joint probabilities used approximate original joint probability. furthermore decompose localized order-n joint probability stacked probabilities order- order-n eﬃciently combine multi-order information. using diﬀerent orders markov assumptions diﬀerent structural complexity model. example markov assumption order- obtain simplest model terms structural complexity formula models bigram tags respect input. search space expands entails complex structural dependencies. learn models need estimate conditional probabilities. order make problem tractable feature mapping often introduced extract features conditions avoid large parameter space. paper blstm estimate conditional probabilities advantage feature engineering reduced minimum. moreover conditional probabilities higher order models converted joint probabilities output labels conditioned input labels. using neural networks learning n-order models conducted extending unigram labels n-gram labels. training aﬀects computational cost output layer linear size set. models trained eﬃciently aﬀordable training cost. strategy showed figure decode structure regularization need connect models diﬀerent complexity. fortunately complex model decomposed simple model another complex model. notice that figure illustration sr-decoding sequence labeling task blstm decodes third-order tags directly sr-decoding jointly decodes ﬁrst-order second-order third-order tags. preceding equation complex model predicts next label based current label input. simplest model. practice estimate also blstms computation derivation also generalized order-n case consists models predicting length- length-n label sequence position. moreover equation explicitly shows reasonable decoding simple structure model used regularize complex structure model. figure illustrates method. however considering sequence length order-n model decoding scalable computational complexity algorithm accelerate decoding prune tags position complex structure model toppossible tags simplest structure model. example output tags complex structure bigram labels i.e. available tags position complex structure model combination probable unigram tags position position before. addition complex model also pruned contains tags appearing training set. detailed algorithm pruning name scalable multi-order decoding given appendix figure example dependency parsing analysis augmented empty elements ptb. dependency structure according stanford dependency. denotes empty element. indicates expletive construction; indicates subject ﬁght i.e. located another place; indicates wh-movement. task question parsing task speciﬁcally joint empty category detection dependency parsing involves hierarchical structures. many versions transformational generative grammars e.g. government binding theory empty category concept bridging s-structure d-structure possible contribution trace movements. following linguistic insights traditional dependency analysis augmented empty elements viz. covert elements figure shows example dependency parsing analysis augmented empty elements. representations leverages hierarchical tree structures encode surface also deep syntactic information. goal empty category detection empty elements goal dependency parsing thus includes predicting dependencies among normal words also dependencies normal word empty element. paper concerned employ structural complexity regularization framework improve performance empty category augmented dependency analysis complex structured prediction problem compared regular dependency analysis. vertex consists nodes represented single integer. especially represents virtual root node others corresponded words represents unlabeled dependency relations particular analysis speciﬁcally represents dependency head dependent dependency graph thus unlabeled dependency relations root words represent empty category augmented dependency tree extend vertex deﬁne directed graph usual. deﬁne parsing model denote index possible dependencies {··· {··· dependency parse represented vector here trees compatible score evaluates event tree analysis sentence brief given sentence compute parse searching highest-scored dependency parse compatible trees scores assigned score. paper evaluate structured perceptron deﬁne score where feature-vector mapping corresponding parameter vector. general performing direct maximization infeasible. common solution used many parsing approaches introduce part-wise factorization above assumed dependency parse factored parts represents small substructure example might factored component dependencies. number dynamic programming algorithms designed ﬁrst second third fourth-order factorization. parsing joint empty category detection dependency parsing deﬁned similar way. another index {··· indicates empty node. dependency parse empty nodes represented vector similar output factorization function namely part deﬁned collection sibling tri-sibling dependencies decoding optimization problems namely resolved low-degree polynomial time respective number words contained particular decoding algorithms proposed zhang extensions algorithms introduced respectively mcdonald pereira collins perform structure regularization decoding need combine models. problem models linear involve probability easily combined together. assume assign scores parse trees without empty elements respectively. particular training data estimating sub-structures training data estimating therefore training data viewed mini-samples training data ﬁrst describe settings theoretical analysis give necessary deﬁnitions introduce proposed method proper annotations clearance analysis finally give theoretical results analyzing generalization risk regarding structure complexity based stability general idea behind theoretical analysis overﬁtting risk increases complexity structure complex structures less stable training. examples taken training impact complex structure models much severer compared simple structure models. detailed relations among factors shown analysis. section give preliminary deﬁnitions necessary analysis including learning algorithm data cost functions especially definition structural complexity. also describe properties assumptions make facilitate theoretical analysis. graph observations indexed denoted indexed sequence observations on}. term sample denote on}. example natural language processing sample correspond sentence words dependencies linear chain structures tree structures signal processing sample correspond sequence signals dependencies arbitrary structures. simplicity analysis assume samples observations analysis deﬁne structural complexity scope structural dependency. example dependency scope tags considered less complex dependency scope three tags. particular dependency scope tags considered full dependency scope highest structural complexity. sample converted indexed sequence feature vectors {xxx xxx} dimension corresponds local features extracted position/index matrix represent words denote input space position sampled structured output space structured output sampled uniﬁed denotation structured input output space. sampled uniﬁed denotation pair training data. size samples drawn i.i.d. distribution unknown. learning algorithm function function space i.e. maps training function suppose symmetric respect independent order structural dependencies among tags major diﬀerence structured prediction non-structured classiﬁcation. latter case local classiﬁcation based position expressed xxx) term {xxx xxx} represents local window. however structured prediction local classiﬁcation position depends whole input {xxx xxx} rather local window nature structural dependencies among tags assume simple real-valued structured prediction scheme class predicted position sign many structured learning models convex objective function models non-convex objective function well-known theoretical analysis non-convex cases quite diﬃcult. theoretical analysis focused convex situations hopefully provide insight diﬃcult non-convex cases. fact conduct experiments neural network models non-convex objective functions lstm. experimental results demonstrate proposed structural complexity regularization method also works non-convex situations spite diﬃculty theoretical analysis. base problem settings give deﬁnitions common weight regularization proposed structural complexity regularization. deﬁnition proposed structural complexity regularization decomposes dependency scope training samples smaller localized dependency scopes. smaller localized dependency scopes form mini-samples learning algorithms. assumed smaller localized dependency scopes overlapped. hence analysis simpliﬁed version structural complexity regularization. aware implementation constraint hard guarantee. empirical side structural complexity works well without constraint. e.g. representative regularizer gaussian regularizer called regularizer. call regularization techniques weight regularization. deﬁnition weight regularization function regularization strength structured classiﬁcation based objective function general weight regularization follows weight regularization normalizes model weights proposed structural complexity regularization method normalizes structural complexity training samples. analysis based diﬀerent dependency scope that example depending tags context considered less structural complexity depending four tags context. structural complexity regularization deﬁned make dependency scope smaller. simplify analysis suppose baseline case sample full dependency scope tags dependencies. then introduce factor sample localized dependency scope n/α. case represents reduction magnitude dependency scope. simplify analysis without losing generality assume localized dependency scopes overlap other. since dependency scope localized non-overlapping split original sample dependency scope mini-samples dependency scope n/α. want show that learning small non-overlapping dependency scope less overﬁtting risk learning large dependency scope. real-world tasks overlapping dependency scope. hence theoretical analysis simpliﬁed essential problem distilled real-world tasks. follows also directly call dependency scope sample structure complexity sample. then simpliﬁed version structural complexity regularization speciﬁcally theoretical analysis formally deﬁned follows deﬁnition structural complexity regularization function regularization strength structured classiﬁcation based objective function structural complexity regularization follows given formal deﬁnition structural complexity regularization comparing traditional weight regularization. below show structural complexity regularization improve stability learned models ﬁnally reduce overﬁtting risk learned models. generalization learning algorithm positively correlated stability learning algorithm analyze generalization proposed method instead examine stability structured prediction. here stability describes extent resulting learning function changes sample training removed. prove decomposing dependency scopes regularizing structural complexity stability learning algorithm improved. proof provided appendix here show lower structural complexity lower bound stability stable learning algorithm. proposed method improves stability regularizing structural complexity training samples. proof given appendix increasing size training results linear improvement increasing strength structural complexity regularization results quadratic improvement i.e. stability based removing mini-sample. moreover extend analysis function stability based comparing i.e. stability based removing fullsize sample. note that cases notation ambiguous. example either denote removing sample denote removing mini-sample thus case ambiguous diﬀerent index symbols indexing indexing respectively. means i.e. mini-samples derived sample zzzi removed. assume convex diﬀerentiable ρ-admissible. local feature value bounded denote function stability comparing ∀zzz |zzz| then bounded section formally describe relation generalization stability summarize relationship proposed method generalization. finally draw conclusions theoretical analysis. theorem real-valued structured classiﬁcation algorithm point-wise loss function deﬁned before. generalization risk based expected sample size deﬁned before. empirical risk based deﬁned like before. then probability least random draw training generalization risk bounded proof appendix upper bound generalization risk contains loss stability rewritten function stability. better stability leads lower bound generalization risk. theorem structured prediction objective function penalized structural complexity regularization factor weight regularization factor penalized function minimizer assume point-wise loss convex diﬀerentiable bounded assume ρ-admissible. local feature value bounded then probability least random draw training generalization risk bounded overﬁt-bound. reducing overﬁt-bound crucial reducing generalization risk bound. importantly overﬁt-bound structural complexity regularization factor always stays together weight regularization factor working together reduce overﬁt-bound. indicates structural complexity regularization important weight regularization reducing generalization risk structured prediction. first suggests structure complexity increase overﬁt-bound magnitude applying weight regularization reduce overﬁt-bound importantly applying structural complexity regularization additionally reduce overﬁt-bound magnitude means structural complexity regularization worst overﬁt-bound. also suggests increasing size training reduce overﬁt-bound square root level. regularizing complex structure simple structure balance empirical risk overﬁtting risk achieved. proposed method model complex structure simple structure used decoding. essence decoding based complex model purpose keeping empirical risk down. simple model used regularize structure output means structural complexity complex model compromised. therefore overﬁtting risk reduced. summarize proposed method decomposes dependency scopes regularizes structural complexity. leads better stability model means generalization risk lower. problem settings increasing regularization strength bring linear reduction overﬁt-bound. however simple structure cause dominating empirical risk. achieve balanced structural complexity could regularize complex structure model simple structure model. complex structure model empirical risk simple structure model structural risk. proposed method takes advantages simple structure model complex structure model. result overall overﬁtting risk reduced. conduct experiments natural language processing tasks. concerned types structures linear-chain structures e.g. word sequences hierarchical structures e.g. phrase-structure trees dependency trees. natural language processing tasks concerning linear-chain structures include text chunking english named entity recognition dutch named entity recognition. also conduct experiments natural language processing task involves hierarchical structures i.e. dependency parsing empty category detection. text chunking chunking data conll- shared task training consists sentences test consists sentences. since development data provided randomly sampled training data development tuning hyper-parameters. evaluation metric f-score. english named entity recognition english data conll- shared task four types entities recognized person location organization misc. data sentences training sentences development sentences testing. evaluation metric f-score. dutch named entity recognition d-ner dataset shared task conll-. dataset contains four types named entities person location organization misc. sentences training sentences development sentences testing. evaluation metric f-score. table comparing decoding baseline blstm models. order model increases performance blstm deteriorates time. decoding structural complexity controlled performance higher order model substantially improved. also interesting improvement larger order higher. figure comparing decoding baseline lstm models. decoding substantially improve performance complex structure model. moreover clear decoding improves order- models order- models. since lstm popular implementation recurrent neural networks highlight experiment results lstm. work bi-directional lstm implementation lstm considering better accuracy practice. blstm dimension input layer dimension hidden layer tasks. experiments blstm based adam learning method since default hyper parameters work satisfactorily tasks following kingma default hyper parameters follows tasks blstm almost diﬀerence adding regularization not. hence regularization blstm. weight matrices except bias vectors word embeddings diagonal matrices randomly initialized normal distribution. first apply proposed scalable multi-order decoding method blstm table compares scores blstm-sr blstm standard test data. order model increased baseline model worsens. exception result dutch-ner task. order table text chunking comparing previous work. shen sarkar also achieve result based noun phrase chunking. however result based phrase chunking tags predict diﬃcult. model increased model slightly improved. demonstrates that practice although complex structure models lower empirical risks structural risks dominant. proposed method easily surpasses baseline. chunking error rate reduction second-order model third-order model respectively. english-ner proposed method reduces error rate second-order model third-order model respectively. dutch-ner error rate reduction achieved respectively second-order model third-order model. clear improvement signiﬁcant. suppose reason proposed method combine low-order high order information. helps reduce overﬁtting risk. thus score improved. moreover reduction larger order higher i.e. improvement order- models better order- models. conﬁrms theoretical results higher structural complexity leads higher structural risks. also suggests proposed method alleviate structural risks keep empirical risks low. phenomenon better illustrated figure table shows results chunking compared previous work. achieve state-of-the-art all-phrase chunking. shen sarkar achieve score ours. however conduct experiments noun phrase chunking phrase chunking contains much tags np-chunking diﬃcult. decoding also achieves better results english dutch existing methods. huang employ blstm-crf model english task achieve score score lower best score. chiu nichols present hybrid blstm score model slightly outperforms method external cnns used extract word features. gillick keep best result dutch ner. however model trained corpora multilingual languages. model trained single language gets score performs worse ours. nothman reach semi-supervised approach dutch ner. model joint empty category detection dependency parsing joint empty category detection dependency parsing conduct experiments english chinese treebanks. particular english penn treebank chinese treebank used phrase-structure treebanks need convert dependency annotations. tool provided stanford corenlp process tool introduced yang process gold-standard derive features disambiguation. simplify experiments preprocess obtained dependency trees following way. statistics data cleaning shown table standard training validation test splits facilitate comparisons. accuracy measured unlabeled attachment score overt words percentage overt words correct head. also concerned prediction accuracy empty elements. evaluate performance empty nodes consider correctness empty edges. report percentage empty words right slot correct head. i-th slot sentence means position immediately i-th concrete word. sentence length slots. table lists accuracy individual models coupled diﬀerent decoding algorithms test sets. focus prediction overt words only. take account empty categories information available. however increased structural complexity aﬀects algorithms. table complex sibling factorization works worse simple sibling factorization english works better chinese. results tri-sibling factorization exactly opposite. complex tri-sibling factorization works better table uaso diﬀerent individual models test data. upper bottom blocks present results obtained sibling tri-sibling models respectively. cannot draw conclusion whether complex structure models performs better results vary structural complexity data. expected structural complexity aﬀects empirical risks structural risks. data structure diﬀerent relations empirical risks structural risks also diﬀerent. hence varied results. table uaso diﬀerent decoding models test data. decoding consistently improves complex structure models. improvements statistically signiﬁcant. simple tri-sibling factorization english works worse chinese. results explained theoretical analysis. structural complexity positively correlated overﬁtting risk negatively correlated empirical risk. task although overﬁtting risk increased using complex structure empirical risk decreased sometimes. hence results vary structural complexity data. table lists accuracy diﬀerent decoding models test sets. decoding framework eﬀective deal structure-based overﬁtting. time accuracy analysis overt words consistently improved. second-order model decoding reduces error rate english chinese. third-order model error rate reduction english chinese achieved proposed method. similar sequence labeling tasks third-order model improved more. suppose consistent improvements come ability reducing structural risk decoding algorithm. although task complex structure sometimes helpful accuracy parsing structural risk still increases. regularizing structural complexity improvements achieved decreased empirical risk brought complex structure. considerable studies structure-related regularization. argyriou apply spectral regularization modeling feature structures multi-task learning shared structure multiple tasks summarized spectral function tasks’ covariance matrix used regularize learning individual task. regularize feature structures structural large margin binary classiﬁers data points belonging class clustered subclasses features data points subclass regularized. methods focus regularization approaches many recent studies focus exploiting structured sparsity. structure sparsity studied variety non-structured classiﬁcation models structured prediction scenarios adopting mixed norm regularization group lasso posterior regularization string variations compared pieces prior work proposed method works substantially diﬀerent basis. term structure aforementioned work refers structures feature space substantially diﬀerent compared proposal regularizing structures related studies including studies sutton mccallum samdani roth piecewise/decomposed training methods study tsuruoka lookahead learning method. reduce computational cost model reducing structure involved. sutton mccallum simply structural dependencies graphic probabilistic models model eﬃciently trained. samdani roth simply output space structured decomposing structure sub-structures search space reduced training tractable. tsuruoka train localized structured perceptron local output searched stepping future instead directly using result classiﬁer. work diﬀers sutton mccallum samdani roth tsuruoka work built regularization framework arguments justiﬁcations reducing generalization risk better accuracy although eﬀect decoding space complex model reduced simple model. also theoretical results general graphical models detailed algorithm quite diﬀerent. generalization risk analysis related studies include tsuruoka bousquet elisseeﬀ shalev-shwartz non-structured classiﬁcation taskar london structured classiﬁcation. work targets theoretical analysis relations structural complexity structured classiﬁcation problems generalization risk perspective compared studies. propose structural complexity regularization framework called structure regularization decoding. proposed method train complex structure model. addition also train simple structure model. simple structure model used regularize decoding complex structure model. resulting model embodies balanced structural complexity reduces structure-based overﬁtting risk. derive structure regularization decoding algorithms linear-chain models sequence labeling tasks hierarchical models parsing tasks. theoretical analysis shows proposed method eﬀectively reduce generalization risk analysis suitable graphic models. theory higher structural complexity leads higher structure-based overﬁtting risk lower empirical risk. achieve better performance balanced structural complexity maintained. regularizing structural complexity decomposing structural dependencies also keeping original structural dependencies structurebased overﬁtting risk alleviated empirical risk kept well. experimental results demonstrate proposed method easily surpasses performance complex structure models. especially proposed method also suitable deep learning models. sequence labeling tasks proposed method substantially improves performance complex structure models maximum error rate reduction second-order models third-order models. parsing task maximum improvement chinese tri-sibling factorization achieved proposed method. results competitive even better state-of-the-art results.", "year": 2017}