{"title": "Deep Function Machines: Generalized Neural Networks for Topological  Layer Expression", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "In this paper we propose a generalization of deep neural networks called deep function machines (DFMs). DFMs act on vector spaces of arbitrary (possibly infinite) dimension and we show that a family of DFMs are invariant to the dimension of input data; that is, the parameterization of the model does not directly hinge on the quality of the input (eg. high resolution images). Using this generalization we provide a new theory of universal approximation of bounded non-linear operators between function spaces. We then suggest that DFMs provide an expressive framework for designing new neural network layer types with topological considerations in mind. Finally, we introduce a novel architecture, RippLeNet, for resolution invariant computer vision, which empirically achieves state of the art invariance.", "text": "paper propose generalization deep neural networks called deep function machines dfms vector spaces arbitrary dimension show family dfms invariant dimension input data; parameterization model directly hinge quality input using generalization provide theory universal approximation bounded non-linear operators function spaces. suggest dfms provide expressive framework designing neural network layer types topological considerations mind. finally introduce novel architecture ripplenet resolution invariant computer vision empirically achieves state invariance. recent years deep learning radically transformed majority approaches computer vision reinforcement learning generative models theoretically still lack uniﬁed description computational mechanisms made deeper models successful wider counterparts. substantial analysis shalev-shwartz raghu poole many others gives insight properties neural architectures like depth weight sharing determine expressivity architectures. however less studied properties data sample statistics geometric structure determine architectures expressive data. surprisingly latter perspective leads simple questions without answers rooted theory. example topological properties images allow convolutional layers expressivity generalizeability thereon? intuitively spatial locality translation invariance sufﬁcient justiﬁcations practice general theory suggests optimality convolutions? furthermore exist weight sharing schemes beyond convolutions fully connected layers give rise provably expressive models practice? paper concretely study data-architecture relationship develop theoretical framework creating layers architectures provable properties subject topological geometric constraints imposed data. problem resolution. motivate framework consider problem learning high resolution data. computationally machine learning deals discrete signals frequently signals sampled time continuous function. example audio inherently continuous function sampled vector r×t. even vision images generally piecewise smooth functions mapping pixel position color intensity sampled tensors rx×y×c. performing tractible machine learning resolution images audio increases almost always requires lossy preprocessing like discrete fourier analysis convolutional neural networks avoid dealing therein intutively assuming spacial locality vectors. however wonders lost various dimensionality reduction weight sharing schemes. note claim deep learning high resolution data currently intractible ineffective. problem resolution presented example topological constaints imposed type data yield architecutres desired provable properties. observation discussing large class smooth functions simplicity. although theoretic perspective graph function consists inﬁniteley many points relatively complex algebras functions described symbolic simplicity. great example polynomials space square mononomials occupies one-dimensional vector space generalize phenomena beyond basic families. thus explore results embracing assumption signal really sample continuous process utilize analytic simplicity certain smooth functions derive layer types. contribution. first extend neural networks inﬁnite dimensional domain continuous functions deﬁne deep function machines general family function approximators encapsulates continuous relaxation discrete counterpart. thereafer survey refocus past analysis neural networks inﬁnitely many nodes respect expresiveness maps represent. show dfms admit inﬁnite dimensional neural network generalizations literature also provide necessary language solve long standing questions universal approximation raised following stinchcombe framework ﬁrmly established return motivating goal provable deep learning show dfms naturally give rise neural networks provably invariant resolution input indeed dfms used generally construct architectures provable properties given topological assumptions. finally experimentally verify constructions introducing type layer wavelayers apart convolutions. order propose deep function machines must establish means neural network directly continuous functions. recall standardmcculloch pitts feed-forward neural network. deﬁnition feed-forward neural network following recurrence relation deﬁned adjacent layers suppose wish space functions another neural network. consider model number neurons every layer becomes uncountable. index neuron becomes real-valued along weight input vectors. process roughly depicted figure core idea behind derivation number nodes network becomes uncountable need apply normalizing term contribution node evaluation following layer avoid saturation. eventually process resembles lebesgue integration. formally layer neural network given deﬁnition without loss generality examine ﬁrst layer denote figure left resolution reﬁnement input signal simple functions. right illustration extension neural networks inﬁnite dimensions. note sample simple function furthermore process actually countable depicted here. arbitrary continuous input function neural network. likewise consider real-valued piecewise integrable weight function layer composed indexing variables analysis restrict indices compact sets simple function ﬁnite partition visually piecewise constant function underneath graph suppose vector sampled make simple function taking arbitray parition that simple function essentially piecewise constant intervals uniform length interval attains value component finally simple function approximating v-th weight matrix fashion also simple function. therefore particular neural layer associated lebesgue measure suppose reﬁnement returning original problem higher resolution sample closely approximates follows cooresponding reﬁned partition occupies individually m]). therefore weight contribution less measure theoretic sense. recalling theory simple functions without loss generality assume yield operator neural networks deﬁned endeavour deﬁne topologically inspired framework developing expressive layer types. powerful language abstraction describing feedforward neural network architectures computational skeletons introduced daniely recall following deﬁnition. deﬁnition computational skeleton directed asyclic graph whose non-input nodes labeled activations. daniely provides excellent account graph structures abstract many neural network architectures practice. give skeletons \"ﬂesh skin\" speak pursure suitable generalization neural networks allows intermediate mappings possibly inﬁnite dimensional topological vector spaces. dfms generalization. deﬁnition deep function machine computational skeleton indexed following properties expressive power generalization propose several operations encapsulate onns abstractions inﬁnite dimensional neural networks also almost feed-forward architectures used practice. generalized neural layers basic units theory deep function machines used construct architectures neural networks provable properties resolution invariance seek. basic case expect standard neural network. either become inﬁnite dimensional hope attain models functional mlps rossi inﬁnite layer neural networks globerson livni universal approximation properties. deﬁnition suggest several natural generalized layer families dfms follows. figure examples three different deep function machines activations ommited replaced actual type. left standard feed forward binary classiﬁer middle operator neural network. right complicated residues. operator neural networks many instantiations dfms. show universality results deep function machines noted substantial effort literature explore various embodiments inﬁnite dimensional neural networks. best authors’ knowledge dfms provide single uniﬁed view every proposed framework date. particular neal proposed ﬁrst analysis neural networks countably inﬁnite nodes showing number nodes discrete neural networks tends inﬁnity converge gaussian process prior functions. later williams provided deeper analysis limit neural networks. great deal effort placed analyzing covariance maps associated guassian processes resultant inﬁnite neural networks sigmoidal gaussian activation functions. results based mostly framework bayesian learning great deal analyses relationship non-parametric kernel methods inﬁnite networks including roux bengio seeger saul hazan jaakkola globerson livni initial work hazan jaakkola deﬁne hidden layer inﬁnite layer neural networks layers vector real value considering inﬁnitely many feature maps index variable weight function approach kernelized therefore resulted theory globerson livni aligns neural networks gaussian processes kernel methods. operatator neural networks differ signiﬁcantly freely paramaterized function require continuous function locally compact hausdorf space. additionally universal approximation theory provided inﬁnite layer networks directly cited following work roux bengio dfms encapsulate results also provide general universal approximation theory therefor. note scalar function vector valuued function dimension additionally deﬁniton easily extended function spaces ﬁnite dimensional vectorspaces using kronecker product. another variant inﬁnite dimensional neural networks hope generalize functional multilayer perceptron body work referenced aforementioned work inﬁnite layer neural networks clearly related. fundamental idea given locally compact hausdorff space exists generalization neural networks approximates arbitrary continuous bounded functionals authors show power approximation using functional analysis results stinchcombe additionally provide statistical consistency results deﬁning well deﬁned optimal parameter estimation inﬁnite dimensional case. stemming additionally initial work neal ﬁnal variant called continuous neural networks manifestations ﬁrst closely related functional perceptrons last exactly formulation inﬁntie layer nns. initially roux bengio shows universal approximation regime. overall formulation mimics multiplication weighting vector inﬁnite layer except continuous neural formulation parameterized weights. thereafter prove connections gaussian processes view deep function machines foregoing variants inﬁnite semi-inﬁnite dimensional neural networks merely instantiations different computational skeleton structures. summary uniﬁed view given table addition uniﬁcation dfms provide powerful language proving universal approximation theorems neural networks depth dimension. central theme approach approximation theories factored standard approximation theories discrete neural networks. forthcoming section principle allows prove approximation theories open questions since stinchcombe classic results cybenko yields theory n-discrete layers. f-functional layers work stinchcombe proved great generality certain topologies layer functional mlps universally approximate continuous functional following stinchcombe rossi extended results case wherein multiple o-operational layers prepended f-functional layers. show particular o-operational similarly d-defunctional layers alone dense much richer space uniformly continuous bounded operators function space. give three results increasing power decreasing transparency. theorem bounded interval continuous bijective activation function. integrable functions exists unique class o-operational layers statement theorem powerful; merely claim o-operational layers least function function. however proof yields insight weight kernels o-operational layers look like single condition imposed. therefrom conjecture prove statstically optimal initialization training o-operational layers given satifying training drawn i.i.d distribution theorem suppose bounded intervals lipλ lipκ uniformly continuous nonlinear operator every exists deep function machine layer operator networks universal remains consider d-deconvolutional layers. theorem suppose compact intervals denote analytic functions continuous basis analytic functions every exists deep function machine best knowledge ﬁrst approximation theorems nonlinear operators basis maps function spaces neural networks. proofs appendix roughly involve factorization arbitrary dfms approximation theories n-discrete layers. essentially factorization works follows. foregoing theorems want roughly approximate nonlinear therefore deﬁne operator called afﬁne projection takes functions converts piecewise constant approximations applies converts result piecewise constant approximations. since ﬁnite number pieces given input output respectively deﬁne operator called lattice sense reproduces |k|. show theorem theorem approximating discrete neural network chosing discreitzation surprisingly principle holds structure large class different piecewise constant approximations shown deep function machines express arbitrarily powerful conﬁgurations ’perceptron layer’ mappings betwen different spaces. however theoretically clear different conﬁgurations computaional skeleton particular spaces lead difference expressiveness dfms. answer questions structure return motivating example high-resolution data language deep function machines. resolution invariant neural networks input sampled continuous function o-operational layers natural extending neural networks deal directly before useful think continuous relaxation class perspective gain insight weight tensors n-discrete layers resolution increases. theorem o-operational layer integrable weight kernel parameters unique fully connected n-discrete layer parameters above. dfms therefore yield simple resolution invariance scheme neural networks. instead placing arbitrary restrictions like convolution assuming gradient descent implicitly smooth weight matrix ﬁlter take discretization smooth immediate advantage weight surfaces o-operational layers parameterized dense families whose parameters depend resolution input complexity model learnt. furthermore explore parameterizations constructing weight tensors thereby neural network topologies approximate action operator neural networks expressively topological properties data. generally restrictions weights discrete neural networks might achieved follows repeat process layer computational skeleton yield instantiate deep function machine called n-discrete instatiation consisting n-discrete layers discretizing o-operational layer resolution invariance schema sample perspective yields interpretations existing layer types creation ones. example convolutional n-discrete layers provably approximate o-operational layers weight kernels solutions ultrahyperbolic partial differential equation. figure wavelayer architecture ripplenet mnist. bracketed numbers denote number wave coefﬁcients. images following wavelayer example activations neurons training given feeding ripplenet. using theorem therefore propose following generalization convolutional layers weight kernels satisfying whose n-discrete instantiation resolution invariant. deﬁnition wavelayer n-discrete instantiation o-operational layer weight kernel form wavelayers named such kernels super position standing waves moving directions encoded offset phase additionally n-discrete convolutional layer expressed wavelayers setting direction case instead learning values learn theoretical guarantees given dfms propose series experiments test learnability expressivity resolution invariance wavelayers. ripplenet. baseline model performed grid search mnist various dfms hyperparameters arriving ripplenet architecture similar classic lenet- lecun depicted figure model n-discrete instatiation following consiststs successive wave layers tanh activations pooling. found using relu often resulted failure learning converge. changes activation shapes achieved merely specifying sample partition node dfm. trainable parameters particular magnitude internal frequencies initialized offsets proportional number waves comprising o-operational layer. likewise orientation wave initialized uniformly unit spherical shell. note wavelayers fourier networks layers activation functions. sufﬁces view wave layers simply another reparameterize weight matrix n-discrete layers therefore dimension wavelayers less normal fully connected layers. appendix. model expressive parameter reduction. theorem shown dfms sense parameterize complexity model learned without constraints imposed form data. ripplenet beneﬁts sheer number parameters reduced model expresses mapping satisfactory error without concern resolution variants. emprically verify methodology ﬁxing model architecture increasing number waves layer uniformly. results exponential marginal utility lowest error achieved epochs mnist respect number parameters model shown figure slight outperformance early lenet architectures suggest future work optimizing wavelayers might fruitful achieving state parameter reduction. resolution invariance. true resolution invariance desirable property consistency. principly consistency requires regardless resolution complexity data training time paramerization testing accuracy model vary. test consistency ripplenet ﬁxing aspects initialization save input resolution images. training rescale mnist using bicubic nearest neighbor scaling square resolutions sidelength conjuction compare resolution consistency fully connected convolutional architectures. models number free parameters ﬁrst layer increased necessity. likewise size input ﬁlters convolutional models varied. shown figure wavelayers remain invariant resolution changes multirun variance normalized convergence iterations whereas convolutional layers exhibit increase measurements resolution. paper proposed deep function machines novel framework topologically inspired layer parameterization. showed given topological assumptions dfms provide theoretical tools yield provable properties neural neural networks. used framework derive wavelayers type provably resolution invariant layer processing data sampled continuous signals images audio. derivation wavelayers additionally accompanied proposal several layer operations dfms inﬁnite and/or ﬁnite dimensional vector spaces. ﬁrst time proved theory non-linear operator functional basis approximation neural networks inﬁnite dimensions closing long standing questions since stinchcombe utilized expressive power dfms arrive novel architecture resolution invariant image processing ripplenet. future work. although we’ve layed ground work exploration theory deep function machines still many open questions theoretically empirically. drastic outperformance resolution variance ripplenet comparision traditional layer types suggests layer types dfms provable properties mind explored. furthermore deeper analysis existing global network topologies using dfms useful given expressive power. amit daniely frostig yoram singer. toward deeper understanding neural networks power initialization dual view expressivity. arxiv preprint arxiv. poole subhaneil lahiri maithreyi raghu jascha sohl-dickstein surya ganguli. exponential expressivity deep neural networks transient chaos. advances neural information processing systems provide direct comparison convolutional fully connected wavelayer operations data semicontinuity assumptions conducted several demonstrations. construct dataset functions whose input pairs gaussian bump functions centered different points interval cooresponding \"labels\" target outputs squres gaussian bump functions plus linear form whose slopes given position center desired wish learn construct actual dataset random sample centers sample input output pairs evenly spaced sub-intervals. resultant dataset list pairs input/output vectors train three different layer dfms n-discrete convolutional fully-connected wavelayers respectively. following three ﬁgures show outputs three layer types training progresses. ﬁrst three quadrants output layer type particular example datapoint shown along example’s input/target functions particular example shown chosen randomly beginning training. bottom right training error whole dataset layer type shown. tick number batches seen algorithm. initialization three layers exhibit predicted behavior despite artifacts towards boundaries intervals. convolutional layer acts mullifer smoothing input signal kernel gaussian. fully connected layer generates predicted normally distributed different output activations regard spatial locality input. finally wavelayer output exhibits behaviour predicted neal limits towards smoothed random walk input signal. training continues convolutional wavelayer outputs preserve continuity input signal approximate smoothness output signal induced relation ultrahyperbolic differential equations. since layer restricted given topology although approximates desired output signal closely norm fails achieve smoothness regularization explicity coded. important note example wavelayer output immediately surpasses accuracy convolutional output convolutional output bias units accross entire channels whereas bias units skcos therefore wavelayers impose hetrogenously accross output signals convolutional require much deeper architectures artiﬁcally generate biases. results demonstration illustrate intermediate ﬂexibility wavelayers purely fully connected convolutional architectures. satisfying differential equation convolutional wavelayer architectures regularized spatial locality wavelayers fact beyond convolution layers employ translational hetrogeneity. although purpose work demonstrate superiority either convoluitional wavelayer architectures open avenue exploration neural architecture design using dfms design layer types using topological constraints. point approximation theorem bounded interval continuous bijective activation function. integrable functions exists unique class o-operational layers theorem suppose σ-compact locally compact measurable hausdorff spaces. bounded linear operator exists o-operational layer proof. linear form evaluates arguments bounded domain bounded linear functional. riesz representation theorem unique regular borel measure show continuous. take open neighborhood weak* topology. recall weak* topology endows smallest collection open sets maps )]∗∗ continuous )]∗∗ )]∗. without loss generality norm continuous continuous continuous. particular compact subset maximal thus must construct borel regular measure decompose union inﬁnitely many compacta maximal measure. precompacts property un+. deﬁne χun\\un−µt maximal measure compact described paragraph. clearly measure since every mutually singular additionally next lebesgue-radon-nikodym theorem every function thus follows density non-linear operators theorem suppose bounded intervals lipλ lipκ uniformly continuous nonlinear operator. every exists deep function machine proof theorem given. lemma exist partitions −|k| cooresponding lattice therefore continuous. since compact interval image compact homeomorphic unit hypercube universal approximation theorem cybenko every exists deep function machine then continuity afﬁne projection maps implies exist therefore induced operator represents parameterized rn×j rj×m uniform partition components. parameterize deep function machine weight kernels prove similar theorem d-defunctional layers. theorem suppose compact intervals denote analytic functions continuous basis analytic functions every exists deep function machine veriﬁed composition direct product basis projections projection image coordinate maos factor countable collection maps approximate approximations increasing products deﬁne aforementioned increasing product ﬁlter maps π...n universal approximation theory standard discrete neural networks. n-discrete layer convienience instantiate using method proof nonlinear operator approximation theory above. jkxk. d-defunctional layer weight kernel ommit design weight kernels o-operational layer difﬁcult establish. together approximation equivalence instantiation statement theorem finally need deal basis compact bounded linear operator composition also bounded linear operator. therefore bounded linear approximation theorem o-operational layers appending achieve approximation bound theorem. completes proof. proof. general solution form second-differentiable. essentially shape stays constant position varies every exists continuous therefore applying theorem parameterized yield weight matrix order calculate dimension dfms contianing discretized o-operational layers denoted family dfms n-discrete skeletons whose per-node dimensionality exactly discretization thus dimension bounded however tuned estimate possible essential. suppose designing deep architecture wishes keep dimension whilst increasing per-node activation dimensionality. practice optimization higher dimensions easier dimensional parameterization embedded therein. example hyperdimensional computing sparse coding convolutional neural networks naturally neccessitate high dimensional hidden spaces beneﬁt regularized capacity. since dimensionality discretization depend original dimensionality space capacity depends directly \"complexity\" family weight surfaces endowed. would therefore convenient answer following question formally. problem. family weight surfaces. induce family discretized o-operational layers {n}w∈w denotes discretization. cdim? although work directly attack problem solution leads another dimension layer architecture design beyond topological constraints. practice would able choose give satisfactory generalizability condition learning problem. theoretical guarantees given dfms implementation feedforward error backpropagation algorithms context essential next step. consider operator neural networks polynomial kernels. aforementioned case nodes non-seperable kernels cannot give guarntees following section. therefore standard auto-differentiation set-up sufﬁce dfms example wave layers. feedforward propagation straight forward relies memoizing operators using separability weight polynomials. essentially integration need occur yield coefﬁcients power functions. algorithm feed-forward propagation function numerically integrable seperated theorem operator neural network consecutive layers given numerically integrable continuous riemann integrable input function numerically integrable. therefore numerically integrable. purpose constructing algorithm evaluation integral deﬁnition given important note previous proof requires riemann integrable. hence satisfying conditions follows every integrable inductively. integrable follows numerical integrability numerically integrable. completes proof. common many non-convex problems discretized neural networks stochastic gradient descent method developed using continuous analogue error backpropagation. deﬁne loss function follows. deﬁnition operator neural network dataset δn)} error given deﬁned ability simplify derivative output layer greatly reduces computational time error backpropagation. becomes function deﬁned interval integration next iterated integral. theorem gradient error function evaluated numerically. proof. recall composed numerically evaluated arbitrary every component show numerically evaluable hence numerically evaluated. given arbitrary examine particular partial derivative case arbitrary induct iterated integral. order reverse order integration must ensure iterated integral integrand contains variables guaranteed integration region. examine this propose following recurrence relation gradient. repeated iteration method seen inner integral moved outside iterated integral iteration yields following full reversal notational simplicity recall observing reversal yield following recurrence relation {bs}. bare mind still correspond following relation uses deﬁnition cases otherwise deﬁned. note bl−n logic. need show bl−n integrable. hence induct {bs} proposition numerically integrable also constant. consider base case every every function integrand composed functions form must numerically integrable clearly suppose bs+t numerically integrable constant. then trivially also numerically integrable contents integrand hence proposition implies holds lastly must show numerically integrable. induction must numerically integrable. hence contents integrand must also numerically integrable real. result", "year": 2016}