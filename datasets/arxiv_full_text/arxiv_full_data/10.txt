{"title": "pix2code: Generating Code from a Graphical User Interface Screenshot", "tag": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.NE", "68T45", "I.2.1; I.2.10; I.2.2; I.2.6"], "abstract": "Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites, and mobile applications. In this paper, we show that deep learning methods can be leveraged to train a model end-to-end to automatically generate code from a single input image with over 77% of accuracy for three different platforms (i.e. iOS, Android and web-based technologies).", "text": "transforming graphical user interface screenshot created designer computer code typical task conducted developer order build customized software websites mobile applications. paper show deep learning methods leveraged train model end-to-end automatically generate code single input image accuracy three different platforms process implementing client-side software based graphical user interface mockup created designer responsibility developers. implementing code however time-consuming prevent developers dedicating majority time implementing actual functionality logic software building. moreover computer languages used implement guis speciﬁc target runtime system; thus resulting tedious repetitive work software built expected multiple platforms using native technologies. paper describe model trained end-to-end stochastic gradient descent simultaneously learns model sequences spatio-temporal visual features generate variable-length strings tokens single image input. ﬁrst contribution pixcode novel approach based convolutional recurrent neural networks allowing generation computer tokens single screenshot input. engineered feature extraction pipeline expert heuristics designed process input data; model learns pixel values input image alone. experiments demonstrate effectiveness method generating computer code various platforms without need change speciﬁc tuning model. fact pixcode used support different target languages simply trained different dataset. video demonstrating system available online. second contribution release synthesized datasets consisting screenshots associated source code three different platforms. datasets pixcode implemention publicly available foster future research. automatic generation programs using machine learning techniques relatively ﬁeld research program synthesis human-readable format addressed recently. recent example deepcoder system able generate computer programs leveraging statistical predictions augment traditional search techniques. another work gaunt generation source code enabled learning relationships input-output examples differentiable interpreters. furthermore ling recently demonstrated program synthesis mixed natural language structured program speciﬁcation input. important note methods rely domain speciﬁc languages computer languages designed specialized domain typically restrictive full-featured computer languages. using dsls thus limit complexity programming language needs modeled reduce size search space. although generation computer programs active research ﬁeld suggested breakthroughs program generation visual inputs still nearly unexplored research area. closest related work method developed nguyen reverse-engineer android user interfaces screenshots. however method relies entirely engineered heuristics requiring expert knowledge domain implemented successfully. paper best knowledge ﬁrst work attempting address problem user interface code generation visual inputs leveraging machine learning learn latent variables instead engineering complex heuristics. order exploit graphical nature input borrow methods computer vision literature. fact important number research addressed problem image captioning impressive results; showing deep neural networks able learn latent variables describing objects image relationships corresponding variable-length textual descriptions. methods rely main components. first convolutional neural network performing unsupervised feature learning mapping input image learned representation. second recurrent neural network performing language modeling textual description associated input picture. approaches advantage differentiable end-to-end thus allowing gradient descent optimization. figure overview pixcode model architecture. training image encoded cnn-based vision model; context encoded language model consisting stack lstm layers. resulting feature vectors concatenated second stack lstm layers acting decoder. finally softmax layer used sample token time; output size softmax layer corresponding vocabulary size. given image sequence tokens model differentiable thus optimized end-to-end gradient descent predict next token sequence. sampling input context updated prediction contain last predicted token. resulting sequence tokens compiled desired target language using traditional compiler design techniques. task generating computer code written given programming language screenshot compared task generating english textual descriptions scene photography. scenarios want produce variable-length strings tokens pixel values. thus divide problem three sub-problems. first computer vision problem understanding given scene inferring objects present identities positions poses second language modeling problem understanding text generating syntactically semantically correct samples. finally last challenge solutions previous sub-problems exploiting latent variables inferred scene understanding generate corresponding textual descriptions objects represented variables. cnns currently method choice solve wide range vision problems thanks topology allowing learn rich latent representations images trained used perform unsupervised feature learning mapping input image learned ﬁxed-length vector; thus acting encoder shown figure input images initially re-sized pixels pixel values normalized cnn. pre-processing performed. encode input image ﬁxed-size output vector exclusively used small receptive ﬁelds convolved stride used simonyan zisserman vggnet operations applied twice down-sample max-pooling. width ﬁrst convolutional layer followed layer width ﬁnally width fully connected layers size applying rectiﬁed linear unit activation complete vision model. designed simple lightweight describe guis illustrated figure work interested layout different graphical components relationships; thus actual textual value labels ignored. additionally reducing size search space simplicity also reduces size vocabulary result language model perform token-level language modeling discrete input using one-hot encoded vectors; eliminating need word embedding techniques wordvec result costly computations. programming languages markup languages element declared opening token; children elements instructions contained within block closing token usually needed interpreter compiler. scenario number children elements contained parent element variable important model long-term dependencies able close block opened. traditional architectures suffer vanishing exploding gradients preventing able model relationships data points spread time series hochreiter schmidhuber proposed long short-term memory neural architecture order address problem different lstm gate outputs computed follows matrices weights input vector time previously produced output vector previously produced cell state’s output biases activation functions sigmoid hyperbolic tangent respectively. cell state learns memorize information using recursive connection done traditional cells. input gate used control error inputs cell state avoid input weight conﬂicts occur traditional weight used storing certain inputs ignoring others. output gate controls error outputs cell state prevent output weight conﬂicts happen standard weight used retrieving information retrieving others. lstm memory block thus decide write information decide read information used lstm variant proposed gers schmidhuber forget gate reset memory help network model continuous sequences. model trained supervised learning manner feeding image contextual sequence tokens inputs; token target label. shown figure cnn-based vision model encodes input image vectorial representation input token encoded lstm-based language model intermediary representation allowing model focus certain tokens less others ﬁrst language model implemented stack lstm layers cells each. vision-encoded vector language-encoded vector concatenated single feature vector second lstm-based model decoding representations learned vision model language model. decoder thus learns model relationship objects present input image associated tokens present code. decoder implemented stack lstm layers cells each. architecture expressed mathematically follows architecture allows whole pixcode model optimized end-to-end gradient descent predict token time seen image well preceding tokens sequence. discrete nature output allows reduce task classiﬁcation problem. output layer model number cells vocabulary size; thus generating probability distribution candidate tokens time step allowing softmax layer perform multi-class classiﬁcation. length sequences used training important model long-term dependencies; example able close block code opened. conducting empirical experiments input ﬁles used training segmented sliding window size words unroll recurrent neural network steps. found satisfactory trade-off long-term dependencies learning computational cost. every token input model therefore input image contextual sequence tokens. context used training updated time step sliding window input image reused samples associated gui. special tokens used respectively preﬁx sufﬁx ﬁles similarly method used karpathy fei-fei training performed computing partial derivatives loss respect network weights calculated backpropagation minimize multiclass loss expected token predicted token. model optimized end-to-end hence loss minimized regard parameters including layers cnn-based vision model layers lstm-based models. training rmsprop algorithm gave best results learning rate clipping output gradient range cope numerical instability prevent overﬁtting dropout regularization applied vision model max-pooling operation fully-connected layer. lstm-based models dropout applied non-recurrent connections model trained mini-batches image-sequence pairs. generate code feed image contextual sequence tokens tokens initially empty last token sequence special token. predicted token used update next sequence contextual tokens. process repeated token generated model. generated token sequence compiled traditional compilation methods desired target language. access consequent datasets typical bottleneck training deep neural networks. best knowledge dataset consisting screenshots source code available time paper written. consequence synthesized data resulting three datasets described table column synthesizable refers maximum number unique conﬁguration synthesized using stochastic user interface generator. columns instances refers number synthesized pairs. columns samples refers number distinct image-sequence pairs. fact training sampling done token time feeding model image sequence tokens obtained sliding window ﬁxed size total number training samples thus depends total number tokens written ﬁles size sliding window. stochastic user interface generator designed synthesize guis written compiled desired target language rendered. using data synthesis also allows demonstrate capability model generate computer code three different platforms. model around parameters optimize experiments performed model speciﬁc tuning; training datasets differ shown figure code generation performed greedy search beam search tokens maximize classiﬁcation probability. evaluate quality generated output classiﬁcation error computed sampled token averaged whole test dataset. length difference generated expected token sequences also counted error. results seen table figures show samples consisting input guis output guis generated trained pixcode model. important remember actual textual value labels ignored data synthesis algorithm compiler assign randomly generated text labels. despite occasional problems select right color right style speciﬁc elements difﬁculties modelling guis consisting long lists graphical components model generally able learn layout satisfying manner preserve hierarchical structure graphical elements. paper presented pixcode novel method generate computer code given single image input. work demonstrates potential system automate process implementing guis scratched surface possible. model consists relatively parameters trained relatively small dataset. quality generated code could drastically improved training bigger model signiﬁcantly data extended number epochs. implementing now-standard attention mechanism could improve quality generated code. using one-hot encoding provide useful information relationships tokens since method simply assigns arbitrary vectorial representation token. therefore pre-training language model learn vectorial representations would allow relationships tokens inferred result alleviate semantical error generated code. furthermore one-hot encoding scale vocabulary thus restrict number symbols support. generative adversarial networks gans shown extremely powerful generating images sequences applying techniques problem generating computer code input image unexplored research area. gans could potentially used standalone method generate code could used combination pixcode model ﬁne-tune results. major drawback deep neural networks need training data resulting model generalize well unseen examples. signiﬁcant advantages method described paper need human-labelled data. fact network model relationships graphical components associated tokens simply trained image-sequence pairs. although used data synthesis paper partly demonstrate capability method generate code various platforms; data synthesis might needed wants focus web-based guis. fact could imagine crawling world wide collect dataset html/css code associated screenshots rendered websites. considering large number pages already available online fact websites created every could theoretically supply virtually unlimited amount donahue anne hendricks guadarrama rohrbach venugopalan saenko darrell. long-term recurrent convolutional networks visual recognition description. proceedings ieee conference computer vision pattern recognition pages goodfellow pouget-abadie mirza warde-farley ozair courville bengio. generative adversarial nets. advances neural information processing systems karpathy fei-fei. deep visual-semantic alignments generating image descriptions. proceedings ieee conference computer vision pattern recognition pages mikolov sutskever chen corrado dean. distributed representations words phrases compositionality. advances neural information processing systems pages nguyen csallner. reverse engineering mobile application user interfaces remaui automated software engineering ieee/acm international conference pages ieee sermanet eigen zhang mathieu fergus lecun. overfeat integrated recognition localization detection using convolutional networks. arxiv preprint arxiv.", "year": 2017}