{"title": "Inverse Classification for Comparison-based Interpretability in Machine  Learning", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "In the context of post-hoc interpretability, this paper addresses the task of explaining the prediction of a classifier, considering the case where no information is available, neither on the classifier itself, nor on the processed data (neither the training nor the test data). It proposes an instance-based approach whose principle consists in determining the minimal changes needed to alter a prediction: given a data point whose classification must be explained, the proposed method consists in identifying a close neighbour classified differently, where the closeness definition integrates a sparsity constraint. This principle is implemented using observation generation in the Growing Spheres algorithm. Experimental results on two datasets illustrate the relevance of the proposed approach that can be used to gain knowledge about the classifier.", "text": "context post-hoc interpretability paper addresses task explaining prediction classiﬁer considering case information available neither classiﬁer itself processed data proposes instance-based approach whose principle consists determining minimal changes needed alter prediction given data point whose classiﬁcation must explained proposed method consists identifying close neighbour classiﬁed differently closeness deﬁnition integrates sparsity constraint. principle implemented using observation generation growing spheres algorithm. experimental results datasets illustrate relevance proposed approach used gain knowledge classiﬁer. bringing transparency machine learning models nowadays crucial task. however complexity today’s bestperforming models well subjectivity lack consensus notion interpretability make difﬁcult address. past years multiple approaches proposed bring interpretability machine learning relying intuitions ’interpretable’ means kind explanations would help user understand model predictions. existing categorizations interpretability usually distinguish approaches mainly characteristics explanations given classiﬁer interpreted in-model interpretability relies modifying learning process make simpler approaches consist building simpler model replace original classiﬁer contrary post-hoc interpretability focuses building explainer system using results classiﬁer explained. work propose post-hoc approach aims explaining single prediction model comparison. particular given classiﬁer observation interpreted focus ﬁnding closest possible observation belonging different class. explaining particular examples shown cognitive teaching sciences facilitate learning process user especially relevant cases classifer decision explain complex interpretability approaches canprovide meaningful explanations. another motivation approach lies fact many applications machine learning today information original classiﬁer existing data made available end-user making modeldata-agnostic intepretability approaches essential. address issues propose growing spheres generative approach locally explores input space classiﬁer decision boundary. speciﬁty relying existing data observation interpreted minimal change needed alter associated prediction. paper organized follows ﬁrst present existing approaches post-hoc interpretability relate proposed paper. then describe proposed comparison-based approach well formalization motivations. describe growing spheres algorithm. finally illustrate method real-world applications analyze used gain information classiﬁer. post-hoc interpretability approaches explaining behavior classiﬁer around particular observations user understand associated predictions generally disregarding actual learning process model might post-hoc interpretability results received interest recently especially black-box models deep neural networks ensemble models used classiﬁcation despite complexity. section brieﬂy reviews main existing approaches depending hypotheses made available inputs forms explanations take. axes discussion obviously overlap provide good framework motivations approach. available inputs consider case physician using diagnostic tool. natural speculate information machine learning model used make disease predictions neither idea patients used train raises question knowledge end-user hence inputs post-hoc explainer use. several approaches rely speciﬁcally knowledge algorithm used make predictions taking advantage classiﬁer structure generate explanations however cases information prediction model available highlights necessity having model-agnostic interpretability methods explain predictions without making hypotheses classiﬁer approaches sometimes called sensitivity analyzes generally analyze classiﬁer locally reacts small perturbations. instance baehrens approximate classiﬁer parzen windows calculate local gradient model understand features locally impact class change. forms explanations beyond differences regarding inputs variety existing methods also comes lack consensus regarding deﬁnition fortiori formalization notion interpretability. depending task performed classiﬁer needs end-user explaining result take multiple forms. interpretability approaches hence rely following assumptions design explanations explanations accurate representation feature importances binary rules visualizations instance give different insights predictions without knowledge classiﬁer. lime approach linearily approximates local decision boundary classiﬁer calculates linear coefﬁcients approximation give local feature importances hendricks identify class-discriminative properties justify predictions generate sentences explain image classiﬁcation. paper consider case instance-based approaches bring interpretability comparing observation relevant neighbors approaches observations explanations bring transparency prediction black-box classiﬁer. motivations instance-based approaches lies fact cases objectives mentioned contradictory cannot reached satisfying way. complex situations ﬁnding examples easier accurate describe classiﬁer behavior trying force speciﬁc inappropriate explanation representation would result incomplete useless misleading explanations user. illustration baehrens discuss approach based parzen windows succeed well providing explanations individual predictions boundaries training data giving explanation vectors actually pointing opposite direction decision boundary. comparison observations class would probably make sense case give useful insights. existing instance-based approaches moreover often rely prior knowledge machine learning model train dataset labelled instances. instance kabra identify train observations highest direct inﬂuence single prediction. disposing knowledge classiﬁer data asset existing methods create explanations desire. however democratization machine learning implies nowadays cases end-user explainer system access knowledge making approaches unrealistic. context need comparison-based interpretability tool rely prior knowledge including existing data constitutes main motivations work. highly subjective nature interpretability machine learning sometimes looks cognitive sciences justiﬁcation building explanations although mentioned previously cited instance-based approaches must underlined learning examples also possesses strong justiﬁcation cognitive teaching sciences instance watson show experiences generated examples help students ’see’ abstract concepts trouble understanding formal explanations. driven cognitive justiﬁcation need tool used available information scarce propose instance-based approach relying comparison observation interpreted relevant neighbors. principle proposed approach order interprete prediction comparison propose focus ﬁnding observation belonging class answer question ’considering observation classiﬁer minimal change need apply order change prediction observation?’. problem similar inverse classiﬁcation apply interpretability. explaining change prediction help user understand model considers locally important. however compared feature importances often built kind statistical robustness approach claim bring causal knowledge. contrary gives local insights disregarding global behavior model thus differs interpretability approaches. instance ribeiro evaluate method lime looking faithful global model local explainer however despite providing causal information proposed approach provides exact values needed change prediction class also helpful user. furthermore important note primary goal give insights classiﬁer reality approximating. approach thus aims understanding prediction regardless whether classiﬁer right wrong whether observations generated explanations absurd. characteristic shared adversarial machine learning relates approach since aims ’fooling’ classiﬁer generating close variations original data order change predictions. adversarial examples rely exploiting weaknesses classiﬁers sensitivity unknown data usually generated using knowledge classiﬁer approach propose also relies generating observations might realistic without knowledge classiﬁer whatsoever purpose interpretability. finding closest ennemy simpliﬁcation purposes propose formalization proposed approach binary classiﬁcation. however applied multiclass classiﬁcation. consider problem classiﬁer maps input space dimension output space suppose information available classiﬁer. suppose features scaled range. observation interpreted associated prediction. goal proposed instance-based approach explain observation ﬁnal form explanation difference vector particular focus ﬁnding observation belonging different class i.e. simpliﬁcation purposes call ally observation belonging class classiﬁer ennemy classiﬁed class. recalling objective mentioned earlier ﬁnal explanation looking accurate representation classiﬁer doing. decide transform problem minimization problem deﬁning function cost moving observation ennemy difﬁculty deﬁning cost function comes fact despite classiﬁer designed learn optimize speciﬁc loss function considered blackbox hypothesis compells choose different metric. ated vector sparsity ||.|| euclidean norm. looking strumbelj choose norm vector component cost function measure proximity however recalling objective need make sure cost function guarantees ﬁnal explanation easily read user. regard consider human users intuitively explanations small dimension simpler. hence decide integrate vector sparsity measured norm another component cost function combine norm weighted average. cost function discontinuous hypotheses made solving problem difﬁcult. hence choose solve sequentially components cost function using growing spheres two-step heuristic approach approximates solution problem. order solve problem deﬁned equation proposed approach growing spheres uses instance generation without relying existing data. thus considering observation interprete ignore direction closest classiﬁer boundary might context greedy approach closest ennemy explore input space generating instances possible direction until decision boundary classiﬁer crossed thus minimizing l-component metric. step detailed next part generation. algorithm growing spheres generation require binary classiﬁer require observation interpreted require hyperparameters ensure ennemy generate uniformly looking maximize sparsity vector respect this consider naive heuristic based idea smallest coordinates might less relevant locally regarding classiﬁer decision boundary thus ﬁrst ones ignored. existing approaches rely surveys evaluation asking users questions measure extent help user performing ﬁnal task order assess kind explanation quality. however creating reproducible research machine figure illustration growing spheres circle represents observation interprete plus signs observations generated growing spheres white plus ﬁnal ennemy used generate explanations. generation generation step growing spheres detailed algorithm main idea generate observations feature space l-spherical layers around ennemy found. positive numbers deﬁne -spherical layer around generate uniformly subspaces yphl algorithm generates observations uniformly distributed surface unit sphere. draw u-distributed values rescale distances generated observations result obtain observations uniformly distributed case initial generation step already contains ennemies need make sure algorithm miss closest decision boundary. done updating value initial radius repeating initial step ennemy found intial ball context present illustrative examples proposed approach applied news image classiﬁcation. particular analyze explanations given growing spheres help user gain knowledge problem identify weaknesses classiﬁer. additionally check explanations easily read user measuring sparsity explanations found. application news popularity prediction apply method explain predictions random forest algorithm news popularity dataset given numerical features created online news articles website mashable task predict wether said articles shared times not. features instance encode information format content articles number words title measure content subjectivity popularity keywords used. split dataset train random forest classiﬁer data. grid search look best hyperparameters test rest data deﬁne cost function hyperparameters algorithm illustrative example apply growing spheres random observations test instance consider case article entitled ’the white house looking good coders’ article predicted popular explanation vector given growing spheres prediction non-null coordinates found table among articles referenced article least popular would need shares order change prediction classiﬁer. additionally keywords used article associated several articles using them. keyword popular articles would need shares order change prediction. words article would predicted popular references keywords uses popular themselves. opposite presented table features would need reduced article entitled intern’ magazine expands dialogue unpaid work experience’ predicted popular change class. additionally feature ’text subjectivity score’ would need reduced indicating slightly objective point view author would lead article predicted popular. sparsity evaluation order check whether proposed approach fulﬁlls goal ﬁnding explanations easily understood user evaluate global sparsity explanations generated problem. measure sparsity number non-zero coordinates explanation vector e∗||. figure shows smoothed cumulative distribution value test data points. observe maximum value whole test dataset meaning observation test dataset needs change coordinates less order cross decision boundary. moreover need move directions less features only. shows proposed method indeed achieves sparsity order make explanations readable. important note mean need features explain observations since nothing guarantees different explanations features. applications digit classiﬁcation another application approach understanding model behaves order improve mnist handwritten digits database apply growing spheres binary classiﬁcation problem recognizing digits ﬁltered dataset contains instances features support vector machine classiﬁer kernel parameter human observer would still probably identify center digit noised version original instead thus despite achieving high accuracy learned bottom-left pixels important turn reciprocally classiﬁer still fails understand actual concepts making digits recognizable human. also check sparsity approach whole test again method seems generating sparse explanations since test dataset predictions interpreted explanations features proposed post-hoc interpretability approach provides explanations single prediction comparison associated observation closest ennemy. particular introduced cost function taking account sparsity explanations described implementation growing spheres answers problem information classiﬁer existing data. showed approach provides insights classiﬁer applications. ﬁrst growing spheres allowed gain meaningful information features locally relevant news popularity prediction. second application highlighted strengths weaknesses support vector machine used digclassiﬁcation illustrating concepts learned classiﬁer. furthermore also checked explanations provided proposed approach indeed sparse. beside collaborating experts industrial domains explanations validation outlooks work include focusing constraints imposed growing spheres algorithm. numerous real-world applications ﬁnal goal user would useless explanations using speciﬁc features. instance business analyst using model predicting whether speciﬁc customer going make purchase would ideally explanation based features leverage. context forbidding algorithm generate explanations speciﬁc areas input space using speciﬁc features promising direction future work.", "year": 2017}