{"title": "Permutohedral Lattice CNNs", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "This paper presents a convolutional layer that is able to process sparse input features. As an example, for image recognition problems this allows an efficient filtering of signals that do not lie on a dense grid (like pixel position), but of more general features (such as color values). The presented algorithm makes use of the permutohedral lattice data structure. The permutohedral lattice was introduced to efficiently implement a bilateral filter, a commonly used image processing operation. Its use allows for a generalization of the convolution type found in current (spatial) convolutional network architectures.", "text": "martin kiefel varun jampani peter gehler planck institute intelligent systems t¨ubingen germany {martin.kiefel varun.jampani peter.gehler}tuebingen.mpg.de paper presents convolutional layer able process sparse input features. example image recognition problems allows efﬁcient ﬁltering signals dense grid general features presented algorithm makes permutohedral lattice data structure. permutohedral lattice introduced efﬁciently implement bilateral ﬁlter commonly used image processing operation. allows generalization convolution type found current convolutional network architectures. domain image recognition convolutional layer today almost exclusively associated spatial convolution image domain. work take signal theoretic viewpoint convolutional operation present algorithm allows process also sparse input data. work inspired special data structures bilateral ﬁlters generalizes convolutional architectures. although approach presented general following scenarios instructive. consider training time access full resolution images train classiﬁer. test time random number pixels test image available. words sample signal differently training test time. traditional would require preprocessing step example subsets pixels dense grid image. view change required dense grid access pixels image. integration domain change. example sparsity deal pixels whose values features position. similarly color information used deﬁne ﬁltering operation well. devise convolution domain respecting color location information view image processing community edge-aware ﬁlter ﬁlter adaptive color and/or gradient image. values regular dense grid therefore direct expansion spatial convolution applicable. approach falls line view encoding invariants possible encode knowledge invariants data looking data. encoded spatial convolution prior knowledge translation invariance. encode roation invariance similarity color space? view take simply convolutions different domains. grid based convolution cannot easily used work sparse data permutohedral lattice provides right space allows efﬁcient implementations. therefore runtime comparable ones spatial convolutions depending size invariants include simply used replacement traditional layers. propose convolution operation d-dimensional input space entirely works lattice. input data tuple feature locations corresponding signal values figure permutohedral convolution consists three steps ﬁrst samples splatted onto lattice convolution operates lattice considering margin neighbors node ﬁnally result convolution transformed back output samples. importantly assume feature locations sampled regular grid example position value. input signal regular structure so-called permutohedral lattice. convolution operates constructed lattice result mapped back output space. hence entire operation consists three stages splat convolution slice strategy already used implement fast gaussian ﬁltering generalize arbitrary convolutions. permutohedral lattice result projection onto plane deﬁned orthogonal vector rd+. dimensional plane embedded rd+. lattice points tessellate subspace regular cells. given point embedding space efﬁcient enclosing simplex projection onto plane. represent sparse points sparse simplex corners lattice. importantly number corners grow exponentially dimension would axis-align simplex representation. continue describe different parts permutohedral convolution. splat slice operations take role interpolation different signal representations. input samples belong cell adjacent lattice point summed weighted barycentric coordinates calculate value pi∈c bijvi. splatting operation. barycentric coordinates depend corner point feature location reverse operation slice ﬁnds output value using barycentric coordinates inside lattice simplex sums corner points convolution performed permutohedral lattice. uses convolution kernel wnlj. convolution kernel problem speciﬁc domain compute restricted neighboring lattice points bilateral ﬁlters gaussian ﬁlter learn kernel values using back-propagation. size neighborhood takes similar role ﬁlter size grid-based cnn. transitional convolutional kernel considers sampled points either side parameters. comparable ﬁlter permutohedral lattice neighborhood elements. permutohedral convolution used building block architecture. omit derivation gradients ﬁlter elements respect output layer space constraints. discuss possible application scenarios. first mentioned free change sampling input signal lattice-based convolution. choice sampling problem speciﬁc. missing measurements domain speciﬁc sampling techniques gather information highly discriminant areas possible scenarios. furthermore show experiments method robust cases train-time sampling test-time sampling coincide. second proposed method provides tool encode additional data invariances principled way. common technique include domain knowledge artiﬁcially augment training table classiﬁcation accuracy mnist. compare lenet implementation part caffe network ﬁrst layer replaced permutohedral convolution layer trained original image resolution three pcnn models trained randomly subsampled images additional bilinear interpolation layer samples input signal spatial grid model. psnr results denoising task using bsds dataset feature mapping invariant respect transformation signal case belongs translations possible invariant feature convolution window function wvdt. idea applied general case calculating mean help window function wvdm permutohedral convolution encode invariances like rotation translation. approximating integral ﬁnite using lattice points integration samples arrive plattice approximate look-up lattice point location. consider case rotation translation invariance. intuitively stack rotated versions input images onto dimensional space dimensions location sample dimension rotation image. grid-based convolution would work rotated image points might coincide grid anymore. filtering permutohedral space naturally respects augmented feature space. investigate performance ﬂexibility proposed method sets experiments. ﬁrst setup compares permutohedral convolution spatial convolution combined bilinear interpolation. second part adds denoising experiment show modelling strength permutohedral convolution. natural spatial convolution combined interpolation compares permutohedral convolutional neural network proposed convolution particularly advantageous cases samples addressed higher dimensional space. nevertheless bilinear interpolation prior spatial convolution used dense -dimensional positional features. take reference implementation lenet part caffe project mnist dataset starting point following experiments. permutohedral convolutional layer also implemented framework. ﬁrst compare lenet terms test-time accuracy substituting ﬁrst convolutional layers permutohedral layer leave rest identical. table shows similar performance achieved seems model ﬂexibility lost. network trained according training parameters reference implementation. next randomly sample continuous points input image interpolated values signal continuous positions features. interestingly train models different amount sub-sampling test time. permutohedral representation robust respect sparse input signal. table shows experiments different signal degradation levels. sampling strategies common original input space pixels densely covered. hence bilinear interpolation prior ﬁrst convolution allows compare original lenet architecture. baseline model performs similar pcnn. strengths proposed method depend regular grid sampling tranditional convolution operators. highlight feature following denoising experiment change sampling space sparse -dimensional. higher dimensional space renders bilinear interpolation spatial convolution in-feasible high number corners hyper-cubical tessellation space. compare proposed permutohedral convolution illustrative denoising experiment spatial convolution. bilateral ﬁltering algorithmic use-cases permutohedral lattice input space features contain coordinates data sample color information image; hence -dimensional vector color images -dimensional vector gray-scale images. contrast direct application bilateral convolution noisy input ﬁlter bilateral layer pcnn trained. experiments compare performance pcnn common images bsds dataset image transformed gray-scale taking mean across channels noise artiﬁcially added samples gaussian distribution baseline network uses spatial convolution kernel size predicts scalar gray-scale value pixel layer trained ﬁxed learning rate momentum weight weight decay train set. second architecture convolution performed permutohedral lattice include pixel’s gray value additional feature generalized operation neighborhood size ﬁlter weights initialized gaussian blur either applied directly noisy input trained train minimize euclidean distance clean image learning rate cross-validate scaling input space image reuse setting experiments operate permutohedral lattice. third architecture combines spatial permutohedral convolutions summation similarly trained tested. evaluate psnr utility averaged images test slightly better performance bilateral network trained ﬁlters comparison bilateral ﬁlter linear ﬁlter table convolutional operations combined improve performance suggest complementary strengths. admittedly setup rather simple validates generalized ﬁltering advantage. future plan investigate pcnn architecture computer vision problems e.g. semantic segmentation modeling domain knowledge like rotation scale invariance. paper presents generalization convolutional operation sparse input signals. envision many consequences work. consider signals naturally represented measurements instead images like scan readings. permutohedral lattice ﬁltering avoids pre-processing assembling operation dense image possible work measured sparse signal directly. another promising ﬁlter encode scale invariance typically encoded presenting multiple scaled versions image several branches network. convolution presented deﬁned continuous range image scales without ﬁnite subselection. summary technique allows encode prior knowledge observed signal deﬁne domain convolution. typical spatial ﬁlter cnns particular type prior knowledge generalize sparse signals. arbelez pablo maire michael fowlkes charless malik jitendra. contour detection hierarchical image segmentation. ieee trans. pattern anal. mach. intell. aurich volker weule j¨org. non-linear gaussian ﬁlters performing edge preserving diffusion. mustererkennung dagm-symposium bielefeld september proceedings yangqing shelhamer evan donahue jeff karayev sergey long jonathan girshick ross guadarrama sergio darrell trevor. caffe convolutional architecture fast feature embedding. arxiv preprint arxiv. tomasi carlo roberto manduchi. bilateral ﬁltering gray color images. proceedings sixth international conference computer vision iccv washington ieee computer society.", "year": 2014}