{"title": "Learning Deep Networks from Noisy Labels with Dropout Regularization", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Large datasets often have unreliable labels-such as those obtained from Amazon's Mechanical Turk or social media platforms-and classifiers trained on mislabeled datasets often exhibit poor performance. We present a simple, effective technique for accounting for label noise when training deep neural networks. We augment a standard deep network with a softmax layer that models the label noise statistics. Then, we train the deep network and noise model jointly via end-to-end stochastic gradient descent on the (perhaps mislabeled) dataset. The augmented model is overdetermined, so in order to encourage the learning of a non-trivial noise model, we apply dropout regularization to the weights of the noise model during training. Numerical experiments on noisy versions of the CIFAR-10 and MNIST datasets show that the proposed dropout technique outperforms state-of-the-art methods.", "text": "abstract—large datasets often unreliable labels—such obtained amazon’s mechanical turk social media platforms—and classiﬁers trained mislabeled datasets often exhibit poor performance. present simple effective technique accounting label noise training deep neural networks. augment standard deep network softmax layer models label noise statistics. then train deep network noise model jointly endto-end stochastic gradient descent dataset. augmented model overdetermined order encourage learning non-trivial noise model apply dropout regularization weights noise model training. numerical experiments noisy versions cifar- mnist datasets show proposed dropout technique outperforms state-of-the-art methods. previous decade witnessed swift advances performance deep neural networks supervised image classiﬁcation recognition. state-of-the-art performance requires large datasets hand-labeled images comprising imagenet dataset large datasets suffer noise images themselves also associated labels. researchers often resort non-expert sources amazon’s mechanical turk tags social networking sites label massive datasets resulting unreliable labels. furthermore distinction class labels always precise even experts disagree correct label image. regardless source resulting noise drastically degrade learning performance learning noisy labels studied previously extensively. techniques training support vector machines k-nearest neighbor classiﬁers logistic regression models label noise presented further gives sample complexity bounds presence label noise. papers consider deep learning noisy labels. early work studied symmetric label noise neural networks. binary classiﬁcation label noise studied techniques multi-class learning general label noise models presented. approach adds extra linear layer intended model label noise conventional convolutional neural network architecture. similar vein work uses selfpaper present simple effective approach learning deep neural networks datasets corrupted label ﬂips. augment arbitrary deep architecture softmax layer characterizes pairwise label probabilities. learn jointly parameters deep network noise model simultaneously using standard stochastic gradient descent. ensure network learns accurate noise model—instead ﬁtting deep network noisy labels erroneously—we apply aggressive dropout regularization added softmax layer. encourages network learn pessimistic noise model denoises corrupted labels learning. training disconnect noise model resulting deep network classify test images. approach computationally fast completely parallelizable easily implemented existing machine learning libraries section demonstrate state-of-the-art performance dropout-regularized noise model noisy versions cifar- mnist datasets. nearly cases proposed method outperforms existing approaches learning label noise models even high rates label noise. many cases dropout even often outperforms genie-aided model noise statistics known priori. investigate properties learned noise model ﬁnding dropout-regularized model overestimates label probabilities. hypothesize pessimistic model improves performance encouraging deep network cluster images naturally confronted conﬂicting image labels. lump base model parameters—processing layer weights biases etc.—into single parameter vector further output vector ﬁnal layer base model. deﬁne usual softmax function motivate approach describe ﬁrst method presented suppose momentarily true noise distribution characterized known. augment base model linear noise model weight matrix equal depicted figure architecture express estimate distribution noisy class label returns element vector. then test sample classiﬁed according output base model i.e. noise model known perfectly might expect approach gives best possible performance. provide excellent performance section show even better performance possible cases. assume probabilistic model label noise noisy label depends true label image suppose noisy labels i.i.d. conditioned true labels. independent given true labels j|yj) image pairs represent conditional noise model column-stochastic matrix rc×c simulations synthesize noisy labels. standard datasets cifar- mnist noise distribution create noisy labels drawing i.i.d. distribution speciﬁed training samples. perturb labels test samples. columns drawn uniformly unit simplex i.e. vectors nonnegative elements one. matrix constant single instantiation noisy training call non-uniform noise model. objective learn deep network noisy training accurately classiﬁes cleanly-labeled images. approach take standard deep network— call base model—and augment noise model accounts label noise. then base noise models learned jointly stochastic gradient descent. noise model role training—as noise model effectively denoises labels backpropagation making possible learn accurate base model. training noise model disconnected test images classiﬁed using base model output. standard deep networks base model. ﬁrst deep convolutional network. three processing layers rectiﬁed linear units maxaverage-pool operations layers. hyperparameters similar used popular alexnet architecture described second model standard deep neural network three rectiﬁed linear processing layers architecture offers major advantages. first matrix unconstrained optimization. softmax layer implicitly normalizes resulting conditional probabilities need normalize force entries nonnegative. simpliﬁes optimization process eliminating normalization step described above. second congruent dropout regularization apply output base model prevent base model learning noisy labels directly. dropout well-established technique preventing overﬁtting deep learning regularizes learning introducing binary multiplicative noise training. gradient step base model outputs multiplied random variables drawn i.i.d bernoulli distribution bern. thins network effectively sampling different network gradient step. entries drawn i.i.d. bernoulli distribution bern represents hadamard product. choose different vector mini-batch i.e. step training set. using crossentropy loss resulting loss function suggested estimate noise probabilities simultaneously learning base model parameters challenge convolutional networks sufﬁciently expressive models base model noisy labels directly learn trivial noise model. prevent this authors regularization term penalizes trace estimate encourages diffuse noise model estimate permits base model learn denoised labels. associated loss function matrix trace regularization parameter chosen cross-validation. minimizing ltrace must take care project estimate onto space stochastic matrices every iteration else correspond meaningful model label noise. propose augment base model different noise architecture. depicted figure softmax layer square weight matrix rc×c unconstrained. interpret output softmax layer denoted probability distribution noisy label results effective conditional probability distribution noisy label conditioned elementary vector. architecture without loss generality. softmax function invertible one-to-one relationship noise distributions induced induced p/ct indicated corrupt labels cifar- training according leave test labels uncorrupted. reference achieves classiﬁcation error trained noise-free dataset. state classiﬁcation accuracy test table baseline present results base model noisy labels treated true labels model parameters chosen minimize standard loss function also present results true noise model known linear noise layer weights appended base model model parameters chosen minimize loss function next present results proposed softmax architecture ﬁrst without regularization proposed dropout regularization finally compare results presented linear layer added label noise model learned jointly base model parameters according trace-penalized loss function emphasize results come signiﬁcant caveats. noise level network architecture used authors used non-uniform noise model replicate paper. therefore results roughly comparable strictly identical noise scenario. cases proposed dropout method gives best performance—even better true noise model supposes known priori. case noise true noise model outperform dropout. note even without dropout regularization proposed softmax noise model gives satisfactory performance consistently outperforming base model. one-to-one relationship softmax linear noise models might expect performance similar. understand figure plot true noise model alongside equivalent noise matrices learned proposed dropout scheme. learned models correct form— approximately uniform diagonally dominant—but also pessimistic underestimating probability correct noise label percent. indeed average diagonal value learned noise matrices noise respectively. suggests learn noisy labels better denoising model pessimistic. notion topic future investigation. next state results non-uniform noise model using cnn. corrupt labels cifar- training according indicated compare proposed dropout scheme base model true noise model traceregularized scheme emphasize error rates taken directly similar identical noise model. omit results unregularized softmax scheme. observing conditional distribution instantiation multiplicative noise zeros fraction elements forcing associated probabilities baseline uniform value. forces learning action remaining probabilities encourages non-trivial noise model. bernoulli parameter determines sparsity instantiation. simulations .—which corresponds aggressively sparse model—works best. usual dropout procedure involves averaging together different models classifying samples reducing learned weights. setting unnecessary. noise model serves intermediate step denoising noisy labels train accurate base model. noise model disconnected test time averaging performed. section demonstrate performance proposed method. state results datasets noise models base models training model architecture publicly-available matlab toolbox mathconvnet changing size input units keep model hyperparameters constant. training architecture used relus layer. case present results label noise probabilities i.e. label noise corrupts training samples. mentioned earlier dropout rate simulations. train end-to-end using stochastic gradient descent batch size training mnist dataset perform early stopping ceasing iterations loss function begins increase. emphasize loss function depend true labels choosing stop require knowledge uncorrupted dataset. matlab code simulations available cifar- dataset subset tiny images dataset cifar- consists training images test images belongs object categories equally represented training test sets. image dimension latter dimension reﬂects three color channels images. first state results uniform noise model using cnn. choose training images test images. version dataset included matconvnet original black-and-white images normalized grayscale dimension reference achieves classiﬁcation error trained uncorrupted training set. first present results learning model parameters mnist training corrupted uniform noise. usual take deﬁned compare proposed dropout method base true noise models. scenario prior work compare. fig. true learned non-uniform noise distributions. ﬁrst shows elements true noise matrix non-uniform noise model noise levels. second shows noise model learned proposed dropout method. table state results experiment time drawn according non-uniform noise model similar cifar- case relative performance dropout worse. slightly under-performs relative true noise model performs substantially worse factors ﬁrst dropout scheme learns non-uniform noise models poorly seen above mnist dataset cluster naturally cifar- dataset. compare dropout performance mnist previous work also state results three-layer described mentioned above network rectiﬁed linear units layer. less sophisticated worse performance overall. trained uncorrupted mnist training achieves classiﬁcation error. before corrupt labels noise drawn according addition true noise base models compare proposed dropout scheme presented bootstrapping scheme used denoise corrupted labels training. similar before proposed dropout scheme outperforms every scheme including true noise model except noise level. however dropout signiﬁcantly outperforms bootstrapping regimes; noise dropout performs even better bootstrap noise. similar results obtain non-uniform noise shown table again dropout worse relative performance difﬁculty learning non-uniform noise model signiﬁcant noise level. plot true learned noise model noise level figure similar before learned model pessimistic closer uniform distribution true model. hypothesize drastic effect mnist digits cluster naturally cifar images. preparing manuscript became aware recently-published approach uses alexnet convolutional neural network pretrained noise-free version ilsvrc dataset. then different noisy training ﬁne-tunes last layer using auxiliary image regularization function optimized alternating direction method multipliers regularization encourages model identify discard incorrectly-labeled images. approach somewhat different setting— proposed simple effective method learning deep network training data whose labels corrupted noise. augmented standard deep network softmax layer models label noise. learn classiﬁer noise model jointly applied dropout regularization weights ﬁnal softmax layer. cifar- mnist datasets approach achieves state-of-the-art performance cases outperforms models label noise statistics known priori. consistent feature approach learns noise model overestimates probability label ﬂip. interpret result deep network encouraged learn cluster data—rather classify it—to greater extent would expect noise statistics. words better deep networks cluster ambiguously-labeled data risk learning noisy labels. details phenomenon—including noise model ideal training accurate network—is topic future research. deng dong socher l.-j. fei-fei imagenet large-scale hierarchical image database computer vision pattern recognition cvpr ieee conference ieee s´aez galar luengo herrera analyzing presence noise multi-class problems alleviating inﬂuence onevs-one decomposition knowledge information systems vol. particular rely pretrained whereas results reported herein suppose end-to-end network must trained noisy labels—so cannot give direct comparison method theirs. however reports classiﬁcation error rate noise mnist larsen nonboe hintz-madsen hansen design robust neural network classiﬁers acoustics speech signal processing proceedings ieee international conference vol. shelhamer donahue karayev long girshick guadarrama darrell caffe convolutional architecture fast feature embedding proceedings international conference multimedia. srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overﬁtting journal machine learning research vol. torralba fergus freeman million tiny images large data nonparametric object scene recognition pattern analysis machine intelligence ieee transactions vol.", "year": 2017}