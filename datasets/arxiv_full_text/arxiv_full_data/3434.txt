{"title": "It Takes (Only) Two: Adversarial Generator-Encoder Networks", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We present a new autoencoder-type architecture that is trainable in an unsupervised mode, sustains both generation and inference, and has the quality of conditional and unconditional samples boosted by adversarial learning. Unlike previous hybrids of autoencoders and adversarial networks, the adversarial game in our approach is set up directly between the encoder and the generator, and no external mappings are trained in the process of learning. The game objective compares the divergences of each of the real and the generated data distributions with the prior distribution in the latent space. We show that direct generator-vs-encoder game leads to a tight coupling of the two components, resulting in samples and reconstructions of a comparable quality to some recently-proposed more complex architectures.", "text": "present autoencoder-type architecture trainable unsupervised mode sustains generation inference quality conditional unconditional samples boosted adversarial learning. unlike previous hybrids autoencoders adversarial networks adversarial game approach directly encoder generator external mappings trained process learning. game objective compares divergences real generated data distributions prior distribution latent space. show direct generator-vs-encoder game leads tight coupling components resulting samples reconstructions comparable quality recently-proposed complex architectures. deep auto encoders vaes deep generative adversarial networks popular approaches generative learning. methods complementary strengths weaknesses. vaes learn bidirectional mapping complex data distribution much simpler prior distribution allowing generation inference; contrary original formulation learns unidirectional mapping allows sampling data distribution. hand gans complex loss functions compared simplistic data-ﬁtting losses usually generate realistic samples. several recent works looked hybrid approaches support principled sampling inference like producing samples quality comparable gans. typically achieved training jointly adversarial discriminators whose purpose improve alignment distributions latent space data space joint latent-data space alternatively method starts learning unidirectional learns corresponding inverse mapping post-hoc. compounding autoencoding adversarial discrimination improve gans vaes cost added complexity. particular systems involves least three deep mappings encoder decoder/generator discriminator. work show unnecessary advantages autoencoders adversarial training combined without increasing complexity model. figure model components generator encoder learning process adjusts parameters order align simple prior distribution latent space data distribution done adversarial training updates generator minimize divergence updates encoder minimize divergence maximize divergence demonstrate adversarial learning gives rise high-quality generators result close match real distribution generated distribution learning also incorporate reconstruction losses ensure encoder-generator acts autoencoder order propose architecture called adversarial generator-encoder network contains feed-forward mappings encoder generator operating opposite directions. vaes generator maps simple prior distribution latent space data space encoder used move real generated data samples latent space. manner encoder induces latent distributions corresponding respectively encoded real data encoded generated data. learning process considers divergence distributions original prior distribution. advantages approach. first simplicity prior distribution computing divergence latent data distributions reduces calculation simple statistics small batches images. second unlike gan-like approaches real generated distributions never compared directly thus bypassing need discriminator networks used gans. instead adversarial signal comes learning encoder increase divergence latent distribution generated data prior works generator tries decrease divergence optionally training include reconstruction losses typical aes. approach evaluated number standard image datasets show quality generated samples comparable gans quality reconstructions comparable better complex adversarially-learned inference approach training faster. evaluate approach conditional setting show successfully tackle colorization problem known difﬁcult gan-based approaches. ﬁndings summarized section related work. apart above-mentioned approaches networks related several recent gan-based systems. thus related improved gans proposed batch-level information order prevent mode collapse. divergences within training also computed batch-level statistics. another avenue improving stability gans replacement classifying discriminator regression-based energy-based gans wasserstein gans statistics seen special form regression. encoder architecture seen discriminator computing single number similarly done section introduces adversarial generator-encoder networks. composed parametric mappings encoder learnable parameters maps data space latent space generator learnable parameters runs opposite direction. shorthand notation denote distribution random variable reference distribution chosen easy sample turns allow sample unconditionally ﬁrst sampling feed-forward evaluation exactly done gans. experiments pick latent space m-dimensional sphere latent distribution uniform distribution sphere uniform. also conducted experiments unit gaussian distribution euclidean space obtained results comparable quality. goal learning align real data distribution generated distribution establishing correspondence data latent samples real data distribution empirical represented large number data samples ...xn}. learning amounts tuning parameter optimize criterion discussed section criterion based adversarial game whose saddle points correspond networks align real generated data distribution criterion augmented additional terms encourage reciprocity encoder generator details training procedure given section approach aligning distributions deﬁne adversarial game based ratio probabilities ratio estimated repeatedly ﬁtting binary classiﬁer distinguishes samples obtained real generated data distributions. here propose alternative adversarial setup advantages respect gan’s including avoiding generator collapse goal generate distribution data space close true data distribution however direct matching distributions high-dimensional data space done challenging. propose instead move comparison simpler latent space. done introducing divergence measure distributions deﬁned latent space require divergence non-negative zero distributions identical encoder function maps distributions deﬁned data space corresponding distributions latent space. below show design adversarial criterion minimizing divergence latent space induces distributions align data space well. theoretical analysis below assume encoders decoders span class measurable mappings corresponding spaces. assumption often referred non-parametric limit justiﬁed universality neural networks make assumption exists least perfect generator matches data distribution i.e. start considering simple game objective deﬁned following theorem shows perfect generators form saddle points game saddle points game based perfect generators. theorem pair forms saddle point game generator matches data distribution i.e. proofs following theorems given supplementary material. game sufﬁcient aligning distributions data space ﬁnding saddle points difﬁcult need comparing empirical distributions avoid issue introducing intermediate reference distribution comparing distributions instead resulting game importantly still induces alignment real generated distributions data space theorem pair saddle point game generator matches data distribution i.e. conversely generator matches data distribution pair saddle point important beneﬁt formulation that selected suitable manner simple compute divergence empirical distributions convenience particular choose coincide canonical distribution substituting objective loss extended include reconstruction terms improve quality result. also optimized using stochastic approximations described section given distribution data space encoder divergence interpreted extracting statistics hence game though comparing certain statistics real generated data distributions. similarly gans statistics ﬁxed evolve learning. also note that even away saddle point minimization ming ﬁxed tend collapse many reasonable choice divergence fact collapsed distribution would inevitably lead high value ﬁrst term thus unlike gans approach optimize generator ﬁxed adversary till convergence obtain non-degenerate solution. hand maximization maxe ﬁxed lead score divergences. previous section demonstrated ﬁnding saddle point sufﬁcient align real generated data distributions thus generate realistically-looking data samples. time necessarily imply mappings reciprocal. reciprocity however desirable wishes reconstruct samples codes section introduce losses encourage encoder generator reciprocal. reciprocity measured either latent space data space resulting loss functions based reconstruction errors e.g. losses thus encourage reciprocity mappings. note also traditional pixelwise loss used within natural question whether helpful minimize losses time whether considering sufﬁcient. answer given following statement theorem distributions aligned mapping then almost certainly i.e. mappings invert almost everywhere supports furthermore aligned i.e. recall theorem establishes solution game aligns distributions data space. theorem shows augmented latent space loss objective sufﬁcient ensure reciprocity. based theoretical analysis derived previous subsections suggest approach joint training generator encoder within networks. case training learning process network game iterative updates parameters driven optimization different objectives. general optimization process combines maximin game functional optimization reciprocity losses figure samples reconstructions tiny imagenet dataset svhn dataset results datasets shown columns show real examples even columns show reconstructions. qualitatively method seems obtain accurate reconstructions especially tiny imagenet dataset samples similar visual quality. denote value encoder generator parameters moment optimization user-deﬁned parameter. note objectives include reconstruction losses. speciﬁcally generator objective includes latent space reconstruction loss. experiments found omission reconstruction loss important avoid possible blurring generator outputs characteristic autoencoders. similarly gans perform several steps toward optimum iteration thus alternating generator encoder updates. maximizing difference ∆)z) optimization process focuses maximization mismatch real data distribution distribution samples generator g¯θ. informally speaking optimization forces encoder mapping aligns real data distribution target distribution mapping non-real away uniform distribution sphere goal encoder would uniformly spread real data sphere cramping much synthesized data possible together assuring non-uniformity distribution differences distributions thus ampliﬁed optimization process force optimization process focus speciﬁcally removing differences. since misalignment measured projecting distributions latent space maximization misalignment makes encoder compute features distinguish distributions. also evaluated networks conditional setting. here tackle problem image colorization hard gans. setting condition generator encoder gray-scale image. taken together experiments demonstrate versatility approach. network architectures experiments generator encoder networks similar structure generator discriminator dcgan turn discriminator encoder modiﬁed output m-dimensional vector replaced ﬁnal sigmoid layer normalization layer projects points onto sphere. divergence measure need measure divergence empirical distribution prior distribution latent space used following measure. given samples m-dimensional sphere gaussian normal distribution diagonal covariance matrix embedding m-dimensional space compute kl-divergence gaussian unit gaussian means standard deviations ﬁtted gaussians along various dimensions. since uniform distribution sphere entail lowest possible divergence unit gaussian embedding space among distributions unit sphere divergence measure valid analysis above. also tried measure divergence non-parametrically using kozachenko-leonenko estimator initial experiments versions worked equally well used simpler parametric estimator presented experiments. hyper-parameters adam optimizer learning rate perform generator updates encoder update datasets. dataset tried picked best one. ended using datasets. dimensionality latent space manually according complexity dataset. thus used celeba svhn datasets complex datasets tiny imagenet cifar-. results evaluate unconditional networks several standard datasets treating system natural reference comparison results either reproduced author’s code copied figure present results challenging tiny imagenet dataset svhn alongside real data samples also show reconstructions obtained comparison. inspection reveals ﬁdelity considerably lower tiny imagenet dataset. figure compare reconstructions celeba images obtained network overall ﬁdelity visual quality reconstructions roughly comparable better ali. furthermore takes notoriously longer time converge method reconstructions epochs training look considerably worse reconstructions epochs thus attesting beneﬁts simpler two-component system. next evaluate method quantitatively. model trained cifar- dataset compute inception score score higher score score state-of-the-art higher still qualitative results cifar- datasets shown supplementary material. also computed likelihood mnist dataset using method latent space size using authours source code. ali’s score age’s score model also superior scores respectively evaluated figure reconstruction quality comparison method ﬁrst column shows examples test celeba dataset. columns reconstructions different methods presented column method three four vae. train model epochs compare trained number epochs importantly epoch method takes times less time ali. fair comparison also present results trained till convergence. finally similarly investigated whether learned features useful discriminative tasks. reproduced evaluation pipeline svhn dataset obtained error rate unsupervised feature learning protocol model result moment unclear networks underperform task. recently several gan-based systems achieved impressive results conditional setting latent space augmented replaced second data space corresponding different modality arguably conditional setting bi-directionality lacking conventional gans needed. fact allowing switch back-and-forth data space latent space bi-directionality allows powerful neural image editing interfaces here demonstrate networks perform well conditional setting. show that picked image colorization problem known hard gans. best knowledge idea applying gans colorization task seems natural successful gan-based colorization results presented compare authors’ implementation pixpix system. also aware several unsuccessful efforts gans colorization. colorization work images color space treat color channels image data sample lightness channel image input encoder generator effectively conditioning encoder generator thus different latent variables result different colorizations grayscale image latent space experiments taken three-dimensional. particular architecture generator takes input image augments variables expanded constant maps spatial dimensions applies resnet type architecture computes encoder architecture convolutional network maps concatenation latent space. divergence measure unconditional experiments computed figure pane shows colorizations input grayscale image using conditional networks pixpix added noise maps networks produce diverse colorizations hard obtain using pixpix. show result color transfer using conditional network. color scheme ﬁrst image transferred onto second image. unconditionally perform colorization experiments stanford cars dataset training images models since cars inherently ambiguous colors hence colorization particularly prone regression-to-mean effect. images downsampled present colorization results figure crucially generator often able produce plausible diverse colorizations different latent vector inputs. wanted enable pixpix gan-based system produce diverse colorizations augmented input generator architecture three constant-valued maps however found system effectively learns ignore input augmentation diversity colorizations demonstrate meaningfulness latent space learned conditional training also demonstrate color transfer examples latent vector obtained encoding image used colorize grayscale image i.e. introduced approach simultaneous learning generation inference networks. demonstrated learning adversarial game generation inference different type objective traditional approaches. particular objective game considers divergences distributions rather discrimination level individual samples. consequence approach require training discriminator network enjoys relatively quick convergence. demonstrate range standard datasets generators obtained approach provides high-quality samples reconstructions real data samples passed subsequently encoder generator better ﬁdelity also shown approach able generate plausible diverse colorizations possible gan-based system references martín arjovsky soumith chintala léon bottou. wasserstein gan. proc. iclr yoshua bengio. learning deep architectures foundations trends machine goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. proc. nips pages yuval netzer wang adam coates alessandro bissacco andrew reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning danilo jimenez rezende shakir mohamed daan wierstra. stochastic backpropagation approximate inference deep generative models. arxiv preprint arxiv. olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei-fei imagenet large scale visual recognition challenge. international journal computer vision salimans goodfellow wojciech zaremba vicki cheung alec radford chen. improved techniques training gans. advances neural information processing systems pages proofs distributions deﬁned data latent spaces correspondingly. assume such exists invertible almost everywhere function transforms latent distribution data assumption weak since every atomless distributions invertible function exists. detailed discussion topic please refer since choice simply setting gaussian distribution uniform sphere good enough. lemma distributions deﬁned space. distributions equal holds measurable function proof. obvious measurable function measurable show assume converse exists function corresponding preimage contradicts previous assumption. lemma different nash equilibria game ming maxe saddle point proof. first note consider conclude saddle point since maximum minimum using lemma saddle point maxe possible immediately follows lemma lemma i.e. e−)}) mapping holds note every optimal encoder distributions example collapses points distribualigned optimal generator pation generators bernoulli) rameter such theorem distributions aligned mapping figure compare cifar- samples dcgan samples generated using ablated model trained without reconstruction terms using distribution alignment only. model trained reconstruction terms still able produce diverse samples also allows inference figure comparison reconstruction quality cifar- dataset. ﬁgures real examples shown columns reconstructions shown even columns. real examples come test never observed model training. figure experiments except data space reconstruction loss objective generator. ﬁgure demonstrates degradation occurring term added. columns correspond real images even reconstructions.", "year": 2017}