{"title": "Randomized Robust Subspace Recovery for High Dimensional Data Matrices", "tag": ["stat.ML", "cs.CV"], "abstract": "This paper explores and analyzes two randomized designs for robust Principal Component Analysis (PCA) employing low-dimensional data sketching. In one design, a data sketch is constructed using random column sampling followed by low dimensional embedding, while in the other, sketching is based on random column and row sampling. Both designs are shown to bring about substantial savings in complexity and memory requirements for robust subspace learning over conventional approaches that use the full scale data. A characterization of the sample and computational complexity of both designs is derived in the context of two distinct outlier models, namely, sparse and independent outlier models. The proposed randomized approach can provably recover the correct subspace with computational and sample complexity that are almost independent of the size of the data. The results of the mathematical analysis are confirmed through numerical simulations using both synthetic and real data.", "text": "rank matrix whose columns lowdimensional subspace matrix called outlier matrix models data corruption. main models data corruption fact incomparable part considered literature namely element-wise columnwise corruption. former model element-wise sparse matrix arbitrary support whose entries arbitrarily large magnitudes model columns affected non-zero elements given arbitrary support pattern. column-wise model portion columns non-zero nonzero columns column space thus portion columns so-called inliers unaffected paper focuses column-wise outlier model according following data model. data model given data matrix satisﬁes following conditions. matrix expressed rank matrix non-zero columns. non-zero columns column space hence index non-zero columns rn×r orthonormal basis column space then problem robust received considerable attention recent years however state-ofthe-art robust estimators matrix decomposition techniques mostly unscalable limits usefulness data applications. instance many existing approaches rely iterative algorithms involve computing singular value decomposition data matrix iteration computationally prohibitive highdimensional settings. motivates work paper. abstract—this paper explores analyzes randomized designs robust principal component analysis employing low-dimensional data sketching. design data sketch constructed using random column sampling followed lowdimensional embedding other sketching based random column sampling. designs shown bring substantial savings complexity memory requirements robust subspace learning conventional approaches full scale data. characterization sample computational complexity designs derived context distinct outlier models namely sparse independent outlier models. proposed randomized approach provably recover correct subspace computational sample complexity almost independent size data. results mathematical analysis conﬁrmed numerical simulations using synthetic real data. index terms—low rank matrix robust randomized algorithm subspace learning data outlier detection sketching column/row sampling random embedding principal component analysis routinely used reduce dimensionality ﬁnding linear projections high-dimensional data lower dimensional subspaces. linear models highly pertinent broad range data analysis problems including computer vision image processing machine learning bioinformatics given data matrix rn×n ﬁnds rn×r orthonormal basis rdimensional subspace denotes identity matrix frobenius norm. useful data intrinsic dimension notoriously sensitive outliers sense solution arbitrarily deviate true underlying subspace small portion data contained low-dimensional subspace. outliers prevail much real data large body research focused developing robust algorithms unduly affected presence outliers. corrupted data expressed portion data outliers. second model sparse outlier model assumed column-sparse i.e. small portion given data columns outliers assumption made linear dependence outlying columns. outlier models prove randomized approach using either designs recover correct subspace high probability technical contributions paper listed below. best knowledge used analyzed ﬁrst time robust column-wise corruption. prove recover correct subspace using roughly random linear data observations. complexity subspace recovery roughly shown ﬁrst time sufﬁcient number random linear data observations correct subspace recovery roughly proposed randomized approach based linear independence outlier columns novel. take advantage random column sampling substantially reduce number outlying columns. thus unlike conventional approaches need columns identify outliers need check data points. table summarizes derived order sufﬁcient number linear random data observations randomized designs outlier models. earliest approaches robust relied robust estimation data covariance matrix s-estimators minimum volume ellipsoid stahel-donoho estimator however approaches applicable highdimensional settings computational complexity memory requirements aware scalable algorithms implementing methods explicit performance guarantees. another popular approach replaces frobenius norm norms enhance robustness outliers instance approach uses norm relaxation commonly used sparse vector estimation yielding robustness outliers replaces -norm -norm promote column sparse solutions. recently idea using robust norm revisited therein non-convex constraint relaxed larger convex exact subspace recovery guaranteed certain conditions. nevertheless approaches directly applicable large matrices high-dimensional data settings. example iterative absolute values entries. norm deﬁned -norm column n-dimensional space vector standard basis. given vector denotes p-norm. linear subspaces said independent dimension intersection equal zero. presented algorithms analysis make following deﬁnitions. deﬁnition space matrix rank non-zero columns said incoherent parameters motivated aforementioned limitation existing approaches data settings elaborated related work section paper explores analyzes randomized approach robust using low-dimensional data sketching. randomized designs considered. ﬁrst design random embedding design wherein random subset data columns selected embedded random low-dimensional subspace. second randomized design random row-sampling design random subset data columns sampled select random subset rows sampled columns. unlike conventional robust algorithms full-scale data robust subspace recovery applied reduced data sketch. consider distinct popular models outlier matrix. ﬁrst model independent outlier model assumed small subset non-zero columns linearly dependent. model allows remarkable interesting approach outlier detection recently proposed based idea outliers typically follow dimensional structures. hence outliers cannot form linearly dependent set. unlike existing approaches approach recover correct subspace even remarkable portion data outliers albeit computational complexity roughly also number samples scales linearly data dimension quite restraining high dimensional settings. paper propose randomized algorithms distinct outlier models. ﬁrst algorithm randomized approach exploits linear independence outlying columns shown randomized algorithm recover correct subspace sample complexity almost independent size data also imposes less stringent constraints distribution outliers compared prior work independent outlier model. second algorithm presented section iii-b deals sparse column outlier model using convex rank minimization reduced data sketches. robust using convex rank minimization ﬁrst analyzed shown optimal point yields exact subspace correct outliers identiﬁcation provided sufﬁciently column-sparse. columnsparsity main requirement i.e. small fraction columns non-zero. computational complexity roughly iteration entire data needs saved working memory prohibitive data applications. paper show complexity subspace recovery reduces substantially less high dimensional data using randomized approach applies reduced data sketches. rank component low-dimensional structure element-wise sparse column-wise sparse models. low-dimensional structures motivated usage randomized algorithms robust using small sketches constructed random linear measurements however majority algorithms focused robust element-wise outlier model instance randomized methods proposed recover small subsets columns rows randomized approach shown reduce complexity iteration. assumption subset outlying columns cardinality less linearly independent. algorithm repeatedly samples data points linearly dependent found upon columns depend linearly ones selected outliers. since number samples scales linearly data dimension algorithm requires iterations average quite restraining high dimensions especially remarkable portion data ouliers. another limitation emerges assumption subset inliers least columns spans column space true general especially real world data often exhibits clustering structures. work considers column-sparse outlier model. data ﬁrst embedded random low-dimensional subspace subset columns compressed data selected. convex program used locate outlying columns compressed data. analysis provided requires roughly random linear observations exact outlier detection. paper show required number sampled columns dimension subspace random embedding almost independent size data required number random linear measurements shown roughly section propose algorithms distinct models outlier matrix. ﬁrst model independent outlier model assumed small subset outliers linearly dependent. corresponding algorithm easy implement recover correct subspace even data outliers. second model concerns scenario column-sparse allows outliers linearly dependent. algorithms consider randomized designs utilizing random embedding using random sampling. provide full analysis sample complexity algorithms based randomized designs. randomized algorithms provably retrieve correct subspace computational sample complexity almost independent size section present algorithms insights underlying proposed approach along statement main theorems. step-by-step analysis deferred sections outliers linearly independent stated next. assumption subset non-zero columns cardinality equal spans q-dimensional subspace independent column space algorithm randomized robust based outlier linear independence randomized designs input data matrix rn×n data sketching column sampling matrix rn×m samples columns randomly columns standard basis vectors. thus rn×m compression matrix rm×n drawn distribution satisfying matrix projects sampled columns random m-dimensional subspace rows rm×n subset standard basis. matrix samples rows sampled columns rm×m subspace learning sampled outlier columns detection deﬁne problem identify outlying columns minimum value non-zero column outlier). subspace learning construct columns corresponding linearly independent inlier columns spanning subspace inlier columns algorithm randomized robust based outlier matrix column-sparsity randomized designs input data matrix rn×n data sketching perform steps algorithm subspace learning sampled outlier columns detection obtain solution non-zero columns indicate location outlying columns. subspace learning construct columns corresponding linearly independent inliers spanning subspace inlier columns symbols. difference step uses random embedding uses sampling. insight suppose columns sampled randomly span column space whp. direct access assume number sampled data columns large enough number inliers least number outliers less whp. section shown sufﬁcient values upper-bound small scale linearly inlier must span columns contains least inliers. contrast outlier would span columns since selected outliers linearly dependent. basis locating outlying columns algorithm solves low-dimensional outlier identiﬁcation problem projecting sampled data lowerdimensional subspace. speciﬁcally form compressed minimum zero column concluded column inlier otherwise identiﬁed outlier. outlying columns detected estimate dimension subspace spanned inliers estimated dimension equal deﬁne matrix independent inlier columns formed columns corresponding thus outlying columns independent inliers correctly located would basis column space many applications also interested locating outlying columns. spans column space easily identify non-zero columns non-zero columns outlier detection intended alternative course data sketching would start compression followed column sampling. particularly useful distributed network setting agent sends compressed version data vector central processor opposed centralizing entire data. such central unit would work random subset columns sampled form subspace learning applied learn column space denotes obtained orthonormal basis column space non-zero columns identiﬁed non-zero columns designs. algorithm differs algorithm subspace learning step since assume outliers linearly independent. instead subspace learning relies column-sparsity convex algorithm used subspace learning step. implies different requirement compression step stated follows. requirement data sketching ensure that rank equal non-zero columns column space worth noting randomized approach substantially reduces complexity applied directly complexity would iteration randomized approach show complexity subspace learning step almost independent size data. following theorems establish performance guarantees algorithm randomized designs. theorem suppose follows data model matrix drawn distribution satisfying columns sampled randomly replacement. small algorithm recovers exact subspace correctly identiﬁes non-zero columns probability least theorem suppose follows data model columns sampled randomly replacement rows sampled randomly without replacement. addition assumed non-zero elements sampled independently zero-mean normal distribution. following theorem counterpart theorem rrd. paper analysis assume non-zero entries sampled zero-mean normal distribution. theorem suppose follows data model columns sampled randomly replacement repeated columns removed rows sampled randomly without replacement non-zero elements sampled independently zero-mean normal distribution. ﬁxed small follow equal constant numbers algorithm yields exact subspace identiﬁes non-zero columns correctly probability least remark practice number outliers smaller suppose constant number. according almost sufﬁcient choose therefore sufﬁcient number randomly sampled columns i.e. scales linearly number sampled outliers thus sufﬁcient value algorithm almost sufﬁcient choose therefore addition applications columns sampled random sampled columns contain least randomly sampled inliers. advantage column sampling randomized approach two-fold. first complexity substantially reduced since need process small subset data. second number outliers signiﬁcantly smaller total number outliers turn relaxes requirement spark considerably. clarify robust algorithms built linear independence assumption outlier columns require every subset outliers cardinality less independent. contrast algorithm requires independence signiﬁcantly smaller subsets selected outliers. following lemma establishes upper-bound number selected outliers. lemma suppose columns given data matrix sampled uniformly random replacement. section establish sufﬁcient conditions satisfy requirement suppose contains outlying columns. thus given assumption rank equal requirement clearly satisﬁed rank equal rank following lemmas provide sufﬁcient conditions randomized designs. lemma suppose contains outlying columns assume matrix satisfying distributional property thus algorithm according sufﬁcient value roughly rrd. addition permissible number outliers scales linearly i.e. restricted sublinear sparsity regime. section provide setp-by-step analysis algorithm proofs main theorems lemmas intermediate results deferred appendix. first establish sufﬁcient condition number sampled columns guarantee inlier lies span inliers based number sampled columns readily obtain upper bound number outlying columns then derive sufﬁcient condition satisfy requirement randomized approach column space learned small random subset columns therefore ﬁrst ensure selected inliers let’s assume span column space initially rn×n given. suppose compact rr×r. following lemma establishes sufﬁcient condition random subset columns rank matrix span column space. based lemma inliers span column space inlier lies span rest inliers number inliers least suppose sample βαn/n data columns randomly following lemma provides sufﬁcient condition ensure number selected inliers exceeds lemma suppose contains outlying columns rank rank component equal non-zero elements sampled independently zero-mean normal distribution rows randomly sampled rows similar analysis algorithm section make lemma derive sufﬁcient condition ensure rank equal rank number selected outliers also bounded similar way. analysis established yields exact outlier identiﬁcation sufﬁciently column-sparse. sufﬁciently sparse also column-sparse matrix representative whp. thus need ensure data sketch sufﬁcient information. following lemmas establish sufﬁcient conditions compression step satisfy requirement rrd. lemma suppose contains outlying columns assume matrix satisfying distributional property versus complexity analysis compression step computational complexity start data sketching column sampling start data sketching compression step incurs computational complexity rrd. hence favorable data reduced computational complexity. however concerning sample complexity random embedding generally effective data sketching tool since random projection matrix coherent data. clarify consider extreme scenario rows non-zero. scenario needs sample less entire rows ensure rank equal i.e. equal contrast projecting data random subspace dimension equal almost sufﬁcient ensure rank equal i.e. nearly sufﬁcient. another example consider matrix generated concatenating columns matrices assume uiqi uiqi elements sampled independently normal distribution. parameter equal thus rank equal whp. accordingly rows union lowdimensional subspaces distribution rows space highly non-uniform. fig. shows rank versus rows distributed uniformly random space thus rows sampled uniformly random enough span space need sample almost rows random span space. hand embedding data random subspace dimension almost sufﬁcient preserve rank even randomized approach consists three steps data sketching subspace recovery outlier detection. data sketching step computational complexity data sketching starts column sampling starts compression. step little impact actual run-time algorithms involves basic matrix multiplication operation data embedding. data sketching incurs computational complexity rrd. complexity subspace recovery roughly algorithms respectively. outlier detection step complexity subspace learning outlier detection dominate run-time algorithms randomized approach brings substantial speedups comparison approaches full-scale data. given sufﬁcient values almost independent size data hence randomized approach evades solving high-dimensional optimization problem. contrast solving example complexity iteration. table compares time algorithm corresponding non-randomized approach outlier detection. example randomized approach remarkably faster non-randomized approach. equal optimal point error proportional noise level. parameter chosen based noise level. modiﬁed version used algorithm account presence noise. recall algorithm built idea outliers cannot constructed from well-approximated linear combinations columns presence noise need ensure outlier cannot obtained linear combinations columns φns. outlier lies span columns coefﬁcients linear combinations columns would fairly large given columns small euclidean norm. thus make algorithm robust noise constraint follows section present numerical experiments study requirements performance randomized approach. numerical results conﬁrm sample complexity randomized methods almost independent size data. first investigate different scenarios using synthetic data. then performance requirements randomized algorithms examined real data. section rank matrix generated rn×r rn×r. product urvt elements sampled independently standard normal distribution. columns non-zero independently probability thus expected value number outliers columns non-zero entries sampled independently phase transition plots show probability correct subspace recovery pairs white designates exact subspace recovery black indicates incorrect recovery. experiments presented section data matrix except simulation fig. fig. shows phase transition algorithm different values increased required values increase need samples ensure selected columns span column space well higher dimension embedding subspace given column space higher dimension. fig. shows similar plot rrd. since section columns/rows distributed uniformly random column/row-space yield similar performance. such remaining scenarios section provide phase transitions fig. illustrates phase transition algorithm different values increasing minimal effect required number sampled columns depends linearly therefore increased increases interesting observe number sampled columns increased required also increases. fact number sampled outlier columns increases sample columns. subsequently selected outliers span subspace higher dimension wherefore need random subspace higher dimension embedding sampled columns ensure rank equal rank algorithm phase transition plots algorithm shown fig. different values left plot algorithm yield correct output whp. middle plot rank increased thus required values increase. right plot algorithm cannot yield correct subspace recovery since requires column-sparse fig. shows phase transition algorithm data matrices different dimensions. although size data increased required values remain unchanged conﬁrming analysis revealed sample complexity proposed approach almost independent size data. simulation since columns/rows distributed randomly column space space small incoherence parameters thus factors dominating sample complexity section study requirements randomized approach real data motion tracking segmentation. data generated extracting tracking points throughout frames data rank matrix motion data points union low-dimensional subspaces. scenarios hopkins data matrix rank roughly equal outlying data points. thus ﬁnal data fig. phase transition algorithm showing probability correct outlier identiﬁcation. greater algorithm yields exact outlier detection whp. vectorized images high dimensional data vectors. thus construct dimensional subspaces substantial reductions computational complexity memory requirements achieved compression operation randomized approach. experiment face images extended yale face database inlier data points. fig. displays random subset faces. database consists face images human subjects images subject low-dimensional subspace according investigations dimension face images roughly equal randomly sample images caltech database outlying data points. fig. displays randomly chosen images caltech database. deﬁne basis subspace faces thus measuring dimension span value observe compression operation preserves essential information since dimension rank rank component proportional norm components outlying data points column space rank component. fig. shows dimension values versus random embedding random sampling. although dimension data vectors shown random linear measurements data vectors nearly sufﬁcient preserve rank outlying component proof lemma since random sampling replacement number outliers matrix follows binomial distribution independent experiments success probability equal whp. matrix projection columns onto complement column space lemma follows requirements satisﬁed. ﬁrst part make following lemma lemma suppose rows sampled uniformly random matrix rank matrices column space. thus satisﬁes rank equal rank probability least prove second part. assume ﬁrst part satisﬁed i.e. rank orthonormal matrix elements matrix zero-mean independent normal random variables equal variance. order show rank equal make following lemma lemma matrix whose entries independent standard normal variables. every probability least σmin σmax minimum maximum singular values proof lemma prove lemma lemma make following result i=li denote union linear subspaces lemma dimension ﬁxed suppose matrix satisfying distributional property according lemma satisﬁes holds vectors column space probability least holds span straightforward show rank equal rank proof lemma suppose contains outlying data points. assume {∪ti}k represents union linear subspaces subspace spanned non-zero column according data model subspace -dimensional subspace since column space suppose stable embedding union subspaces {∪ti}k then dimension subspaces {ti}k changed embedding operation. accordingly columns column space note thus according lemma rank equal rank non-zero columns column space probability least proof lemma since rank equal column space. suppose contains outlying columns. break proof steps. ﬁrst step shown probability least accordingly matrix full rank matrix probability least addition study space coherency matrix since used derive guarantee projection standard basis onto space written projection matrix onto column space ﬁrst inequality follows fact {sei}m second inequality follows cauchy-schwarz inequality third inequality follows guarantee suppose true. number outliers lemma provides sufﬁcient condition guarantee requirements satisﬁed. guarantee suppose satisﬁed. first review theoretical result provided supports performance convex algorithm lemma suppose follows data model deﬁne optimal point proof theorem algorithm recovers exact subspace inliers span column space inlier lies span inlier columns contains outlying columns rank equal lemma lemma establish sufﬁcient condition guarantee whp. given assumption rank equal lemma provides sufﬁcient condition ensure rank equal rank i.e. guaranteed whp. addition lemma provides upper-bound number sampled outliers. therefore according lemma lemma lemma lemma satisﬁed algorithm recovers correct subspace probability least non-zero columns column space probability least thus subspace learned correctly identiﬁes outlying columns correctly probability least proof theorem proof theorem similar proof theorem need make lemma guarantee whp. therefore according lemma lemma lemma lemma requirements theorem satisﬁed algorithm recovers correct subspace probability least addition similar analysis provided proof lemma non-zero columns column space probability least thus subspace learned correctly identiﬁes outlying columns correctly probability least proof theorem order guarantee algorithm recovers exact subspace ensure columns span column space requirement satisﬁed. optimization problem yields correct decomposition i.e. column space equal column space non-zero columns locations. guarantee sufﬁces show according proof lemma kanade robust norm factorization presence outliers missing data alternative convex programming ieee computer society conference computer vision pattern recognition vol. huber robust statistics. springer fischler bolles random sample consensus paradigm model ﬁtting applications image analysis automated cartography communications vol. soltanolkotabi candes geometric analysis subspace clustering outliers annals statistics dasgupta gupta elementary proof theorem johnson lindenstrauss random structures algorithms vol. davenport boufounos wakin baraniuk signal processing compressive measurements ieee journal selected topics signal processing vol. baraniuk davenport devore wakin simple proof restricted isometry property random matrices constructive approximation vol. cand`es romberg robust uncertainty principles exact signal reconstruction highly incomplete frequency information ieee trans. inf. theory vol. k.-c. kriegman acquiring linear subspaces face recognition variable lighting ieee transactions pattern analysis machine intelligence vol. fei-fei fergus perona learning generative visual models training examples incremental bayesian approach tested object categories computer vision image understanding vol. therefore requirements theorem satisﬁed algorithm extracts exact subspace least addition according probability analysis provided proof lemma satisﬁes requirement theorem columns column space non-zero columns column space i.e. exact subspace retrieved identiﬁes outlying columns correctly whp. proof theorem proof theorem similar proof theorem lemma establish sufﬁcient condition guarantee addition according analysis proof lemma satisﬁes requirement theorem requirement satisﬁed also non-zero columns column space i.e. identiﬁes outlying columns correctly case exact subspace recovery. chen caramanis sanghavi robust matrix completion corrupted columns preprint arxiv. rahmani atia analysis randomized robust high dimensional data signal processing signal processing education workshop ieee. ding zhou -pca rotational invariant norm principal component analysis robust subspace factorization proceedings international conference machine learning.", "year": 2015}