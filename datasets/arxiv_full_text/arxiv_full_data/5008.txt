{"title": "Action Schema Networks: Generalised Policies with Deep Learning", "tag": ["cs.AI", "cs.LG"], "abstract": "In this paper, we introduce the Action Schema Network (ASNet): a neural network architecture for learning generalised policies for probabilistic planning problems. By mimicking the relational structure of planning problems, ASNets are able to adopt a weight-sharing scheme which allows the network to be applied to any problem from a given planning domain. This allows the cost of training the network to be amortised over all problems in that domain. Further, we propose a training method which balances exploration and supervised training on small problems to produce a policy which remains robust when evaluated on larger problems. In experiments, we show that ASNet's learning capability allows it to significantly outperform traditional non-learning planners in several challenging domains.", "text": "figure successive convolutions grow receptive ﬁeld neuron higher layers; analogously asnet neuron particular action proposition sees larger portion current state higher layers. sutskever hinton learning play video games paper brings gains deep learning planning proposing neural network architecture asnet specialised structure planning problems much convolutional neural networks specialised structure images. basic idea illustrated figure rather operating virtual graph pixels edges deﬁned adjacency relationships asnet operates graph actions propositions edges deﬁned relations form action affects proposition proposition inﬂuences outcome action structure allows asnet trained problem given planning domain applied other different problems without re-training. make three contributions. neural network architecture probabilistic planning automatically generalises problem given planning domain. representation allows weight sharing among actions modules belonging action schema among proposition modules associated predicate. representation augmented input features paper introduce action schema network neural network architecture learning generalised policies probabilistic planning problems. mimicking relational structure planning problems asnets able adopt weight sharing scheme allows network applied problem given planning domain. allows cost training network amortised problems domain. further propose training method balances exploration supervised training small problems produce policy remains robust evaluated larger problems. experiments show asnet’s learning capability allows signiﬁcantly outperform traditional non-learning planners several challenging domains. automated planning task ﬁnding sequence actions achieve goal within user-supplied model environment. past four decades wealth research machine learning automated planning motivated part belief essential ingredients intelligence—planning learning—ought strengthen nevertheless dominant paradigm among state-of-the-art classical probabilistic planners still based heuristic state space search. domain-independent heuristics used purpose capable exploiting common structures planning problems learn experience. planners deterministic learning tracks international planning competition often machine learning conﬁgure portfolios small fraction planners make meaningful learning produce domain-speciﬁc heuristics control knowledge planners transfer knowledge problems domain similarly underrepresented probabilistic track competition. parallel developments planning we’ve seen resurgence interest neural nets driven largely success problems like image recognition association advancement artiﬁcial intelligence rights reserved. domain-independent planning heuristics. training method balances exploration supervision existing planners. experiments show strategy sufﬁcient learn effective generalised policies. code models work available online. work considers probabilistic planning problems represented stochastic shortest path problems formally tuple ﬁnite states ﬁnite actions transition function cost function goal states initial state. state agent chooses action enabled actions incurring cost causing transition another state probability solution policy probability action applied state optimal policy policy minimises total expected cost reaching assume goal reachable probability ﬁxed-cost penalty incurred every time dead reached factored compact representation tuple ﬁnite binary propositions state space binary strings size |p|. thus state value assignment propositions partial state value assignment subset propositions; partial state consistent partial state value assignments contained short). goal represented partial state action consists precondition represented partial state effects represented partial state probability distribution effects actions applicable state e∈eﬀ a|s=res result changing value propositions make consistent effect lifted compactly represents factored ssps sharing structure. formally lifted tuple ﬁnite predicates ﬁnite action schemas. predicate grounded i.e. instantiated tuple names representing objects yields factored proposition. similarly action schema instantiated tuple names yields factored action. probabilistic planning domain deﬁnition language standard language describe lifted factored ssps ppddl splits description general domain speciﬁc problem. domain gives predicates action https//github.com/qxcv/asnets factored ssps sometimes support conditional effects negative disjunctive preconditions goals. simplify notation. however asnet easily extended support constructs. schemas cost function specifying lifted ssp. problem additionally gives objects initial state goal describing speciﬁc whose propositions actions obtained grounding domain predicates action schemas using objects instance domain description might specify predicate action schema walk problem description might specify objects home work. grounding using objects would produce propositions well ground actions walk walk. observe different factored ssps obtained changing problem part ppddl description reusing domain. next section show take advantage action schema reuse learn policies applied factored obtained instantiating domain. neural networks expensive train would like amortise cost many problems learning generalised policy applied problem given domain. asnet proposes novel domain-specialised structure uses learnt weights regardless shape problem. weight sharing scheme asnet’s ability generalise different problems drawn domain even problems different goals different numbers actions propositions. network structure high level asnet composed alternating action layers proposition layers action layers composed single action module ground action proposition layers likewise composed single proposition module ground proposition; choice structure inspired alternating action proposition layers graphplan hidden units layer connect nearby hidden units next layer action modules layer asnet connect directly related proposition modules next layer vice versa. last layer asnet always action layer module deﬁning action selection probability thus allowing asnet scale problems different numbers actions. simplicity also assume ﬁrst layer always action layer. action module details. consider action module action layer. module takes input feature vector produces hidden representation learnt weight matrix modw learnt bias vector nonlinearity intermediate representation size size inputs action module. feature vector serves input action module constructed enumerating propositions related action concatenating hidden representations. formally proposition related action denoted appears effect concatenation representations related propositions produces vector hidden representation produced proposition module proposition preceding proposition layer. constituent hidden represen= tations dimension notion propositional relatedness ensures that ground actions problem instances action schema ppddl domain inputs structure. note determine propositions related given ground action retrieving corresponding action schema enumerating predicates appear precondition effects action schema instantiating predicates parameters used instantiate apply procedure obtain lists related propositions respectively propositions predicate appear position deﬁnitions structural similarity asnet’s generalisation abilities. layer pair ground actions instantiated action schema s—that weight matrix hence modules actions appear layer correspond action schema weights modules appear different layers correspond different schemas learn different weights. although different problems instantiated ppddl domain different numbers ground actions ground actions still derived same ﬁxed schemas domain apply action module weights problem domain. modules construction subtly different output module action ﬁnal layer single number representing probability selecting action current state learnt policy rather vector-valued hidden representation. guarantee disabled actions never selected ensure action probabilities normalised pass outputs masked softmax activation ensures training sample actions evaluation select action highest probability. action modules ﬁrst layer asnet passed input vector composed features derived current state rather hidden representations related propositions. speciﬁcally modules ﬁrst layer given binary vector indicating truth values related propositions whether propositions appear goal. practice helpful concatenate propositional features heuristic features described section proposition module details. proposition modules appear intermediate layers asnet otherwise similar action modules. speciﬁcally proposition module proposition proposition layer network compute hidden representation rdh×dl before biases module. ﬁrst predicate pred proposition enumerate action schemas reference pred precondition effect. deﬁne feature vector denotes action schema ground action pool pooling function combines several dh-dimensional feature vectors single dh-dimensional one. hence pooled vectors concatenated becomes paper dimensionality assume pool performs pooling proposition module pool outputs many action modules pooling could potentially obscure useful information. issue could overcome sophisticated pooling mechanism pooling posed major problem experiments section even large probabilistic blocks world instances proposition modules must pool thousands inputs. pooling operations essential ensure proposition modules corresponding predicate structure. unlike action modules corresponding action schema proposition modules corresponding predicate different number inputs depending initial state number objects problem sufﬁce concatenate inputs. example consider single-vehicle logistics problem location vehicle tracked propositions form vehicle moved actions form move. location incoming road outgoing roads related move action location incoming roads outgoing roads related move actions road. problem unique planning similar trick employed network architectures graphs vertices varying in-degree exploration supervised learning. repeatedly apply procedure performance ptrain ceases improve ﬁxed time limit reached. note strategy intended learn weights asnet— module connectivity learnt rather obtained grounded representation using notion relatedness described earlier. exploration phase training epoch repeatedly asnet policy initial state problem ptrain collecting states visited along sampled trajectories. trajectory terminates reaches goal exceeds ﬁxed limit length reaches dead end. addition visited state compute optimal policy rooted produce states constitute π∗’s policy envelope—that states visits nonzero probability. trajectories drawn asnet policy policy envelopes optimal policy added state memory saving states visited optimal policy ensures always contains states along promising trajectories reachable hand saving trajectories exploration policy ensures asnet able improve states visits often even optimal goal trajectory. training phase small subsets states repeatedly sampled random produce minibatches training asnet. objective minimised minibatch cross-entropy classiﬁcation loss label expected cost choosing action following optimal policy thereafter minimal among enabled actions; otherwise encourages network imitate optimal policy. sampled batch compute gradient update weights direction decreases adam predicate. speciﬁcally proposition layer propositions pred pred corresponding weights together weight sharing scheme action modules enables learn single weights heuristic features expressiveness limitation asnet ﬁxed receptive ﬁeld network; words longest chain related actions propositions reason about. instance suppose locations arranged line previous logistics example. agent move move action makes false true. propositions thus related chain move actions length hence proposition module proposition layer affected propositions locations moves away. deeper networks reason longer chains actions asnet’s depth necessarily limits reasoning power chains actions arbitrarily long. compensate receptive ﬁeld limitation supplying network features obtained using domainindependent planning heuristics. paper derive features disjunctive action landmarks produced lm-cut features derived different heuristics could employed way. disjunctive action landmark actions least action must applied along optimal path goal deterministic delete-relaxed version planning problem. landmarks necessarily capture useful actions practice providing information landmarks often sufﬁcient compensate network depth limitations. indicates whether sole action least lm-cut landmark action landmark actions appear landmark represents related propositions currently true. encodes related portions goal state true partial state deﬁning goal. training exploration supervision learn asnet weights choosing small training problems ptrain alternating guided exploration build state memory supervised learning ensure network chooses good actions states algorithm describes single epoch efﬁcient train asnets using unguided policy gradient reinforcement learning unfortunately found policy gradient noisy inefﬁcient train deep networks nontrivial problems; practice cost computing optimal policy small training problems pays enabling sample-efﬁcient supervised learning instead reinforcement learning. experiments investigate question whether suboptimal policies still sufﬁcient supervised training asnets. past work generalised policy learning employed learnt policies control knowledge search algorithms part compensate ﬂaws policy. example yoon fern givan suggest employing policy rollout limited discrepancy search avoid occasional action recommended policy. could asnet similarly interested ability learn reliable policy own. hence evaluation always choose action maximises noted above different exploration process employed training instead sample experimental setup compare asnet three heuristic-search-based probabilistic planners lrtdp ilao* ssipp domain-independent heuristics considered three planners—lm-cut additive heuristic hadd —resulting baselines. evaluation enforce time cutoff baselines asnets well memory cutoff. since lrtdp ilao* optimal planners execute convergence problem using different random seeds. notice that hadd lrtdp ilao* might converge suboptimal solution. execution lrtdp ilao* converge given time/memory cutoff consider planner having failed reach goal. ssipp used replanner problem trained time cutevaluated; procedure repeated times problem using different random seeds. training phase ssipp consists simulating trajectory process ssipp improves lower bound optimal solution. consecutive trajectories reach goal training ssipp evaluated regardless training time left. baselines report average running time problem. domain train single asnet evaluate problem times different random seeds. hyperparmeters asnet kept ﬁxed across domains three action layers proposition layers network hidden representation size internal action proposition module nonlinearity optimiser conﬁgured learning rate batch size hard limit hours placed training. also applied regularisation coefﬁcient weights dropout outputs layer except last epoch training alternated rounds exploration shared equally among training problems batches network optimisation sampled trajectory lengths training evaluation. lrtdp lm-cut heuristic used computing optimal policies training dead-end penalty also repeated procedure lrtdp using hadd compare effects using optimal suboptimal policies training. further report well asnet performs guided hadd given lm-cutderived heuristic features described section asnets report average training time plus time solve problem highlight pays spend one-off cost training asnet domain. asnets trained evaluated virtual machine equipped memory processor clocked .ghz. training evaluation asnet restricted single dedicated processor core resources otherwise shared. baseline planners cluster processors clocked .ghz planner used single core. cosanostra pizza deliverator cosanostra pizza safely transport pizza shop waiting customer return shop. series toll booths customer booth either spend time step paying operator save step driving without paying. however don’t operator drop boom pass booth back shop crushing probability. optimal policy operators travelling customer ensure safe return return trip revisit booth. problem size number toll booths shop customer. asnets trained sizes tested sizes probabilistic blocks world extension wellknown deterministic blocks world domain robotic move blocks table goal conﬁguration. actions pick block block another fail probability failure causes target block drop onto table meaning must picked placed again. randomly generate three different problems number blocks considered testing. asnet trained randomly generated problems size training problems total. figure comparison planner running times evaluation domains. refers time used training asnet runs used optimal policies training used potentially suboptimal policies runs heuristic input features. table right shows selected problems coverage average solution cost best asnet baseline. triangle tire world cosanostra pizza probabilistic blocks world. running times averaged three problems size. asnet occludes asnet asnet also occluded absent entirely optimal planner used generate training data could solve training problems time. problem consists locations arranged triangle connections adjacent locations. objective move vehicle corner triangle another. however move chance producing tire must replaced next visited location. vehicle thus requires sequence moves locations replacement tires available. tires arranged reliable policy travels longest path goal along outside edge triangle. task made challenging scaling number locations. little thi´ebaux problem size locations. sizes training test sizes onward. results figure shows time taken train evaluate asnet using optimal suboptimal policies training data. addition shows coverage average solution cost goal reached selected problems best asnet best baseline. following summary results worth using asnet? asnets obtained coverage triangle tire world problems asnets heuristic input features similarly obtained perfect coverage cosanostra. contrast baselines failed scale larger problems. shows asnet well-suited problems local knowledge environment help avoid common traps instance cosanostra agent must learn toll booth operators carrying pizza otherwise; triangle tire world agent must learn sense follow outer edge triangle. could asnets learn tricks average solution cost obtained asnets cosanostra triangle tire world close optimal baselines suggesting optimal solution found. single pattern solve problems. even deterministic version blocks world generalised policy requires planner learn recursive property whether block goal position asnet appears successfully learnt trained suboptimal teacher given landmarks input surpassed baselines coverage moreover average solution cost asnet similar optimal baselines times less ssipp baseline best coverage. asnet policy typically obtained mean solution cost somewhere strategies presented slaney thi´ebaux suboptimal still better unstacking rebuilding towers scratch. note asnet could obtain policy within allotted time trained optimal teacher. heuristic features necessary? cases asnet’s performance improved omitting lm-cut heuristic input features. instance triangle tire world asnet took much time asnet solve problems size much time solve problems size despite executing policies near-identical average cost. notice difference cannot seen figure training time much larger time solve test instance. interestingly asnet able obtain coverage probabilistic blocks world problems figure despite receiving landmark inputs. gain stronger assurance learnt robust policy tested instances blocks asnet could solve additional test instances. contrast asnet given landmarks input—reliably solved test problems extended thus showing heuristic inputs necessary express essential recursive properties like whether block goal position. heuristic inputs also appear necessary cosanostra asnet could achieve full coverage test set. suspect asnet without heuristic inputs cannot determine direction leads pizza shop direction leads customer middle long chain toll booths. suboptimal training policies affect asnet? results suggest suboptimal policies sufﬁcient train asnet demonstrated three domains. intuitively suboptimal policies training ought beneﬁcial time would spent computing optimal policy instead used epochs exploration supervised learning. someevident cosanostra—where suboptimal training policy allows slightly faster convergence—but clear probabilistic blocks world asnet converge within chosen time limit inadmissible policy. training fewer problems allowed network converge within time limit yield robust policy suggesting suboptimal teacher sometimes necessity. asnet performing ﬁxed-depth lookahead search? seen comparing ssipp asnet. ssipp solves ﬁxed-depth sub-problems unable scale well asnets using equivalent depth parametrisation. triangle tire world particularly interesting ssipp outperform baselines quickly ﬁnding dead ends avoiding them. however unlike asnet ssipp unable generalize solution sub-problem next needs solve scratch. generalised policies topic interest planning earliest work area expressed policies decision lists insufﬁciently expressive directly capture recursive properties thus required user-deﬁned support predicates. later planners partially lifted restriction expressing learnt rules concept language taxonomic syntax capture properties directly work employed features domain-independent heuristics capture recursive properties lm-cut landmarks. srivastava also proposed substantially different generalised planning strategy provides strong guarantees plan termination goal attainment albeit restricted class deterministic problems. unlike decision lists relational decision trees employed past work model’s input features ﬁxed training fall prey rule utility problem further model trained minimise differentiable loss could modiﬁed policy gradient reinforcement learning without changing model. approach neural networks used learn policies probabilistic planning problems. factored policy gradient planner trains multi-layer perceptron reinforcement learning solve factored cannot generalise across problems must thus trained anew evaluation problem. concurrent work groshev propose generalising reactive policies heuristics applying visual representation problem demonstrate effective learnt heuristic sokoban. however approach requires user deﬁne appropriate visual encoding states whereas asnets able work directly ppddl description. integration planning neural networks also investigated context deep reinforcement learning. instance value iteration networks learn formulate solve probabilistic planning problem within larger deep neural network. vin’s internal model allow learn robust policies would possible ordinary feedforward neural networks. contrast vins asnets intended learn reactive policies known planning problems operate factored problem representations instead explicit representations like used vins. similar vein kansky present model-based technique known schema networks schema network learn transition model environment decomposed entities entities’ interactions initially unknown. entity–relation structure schema networks reminiscent action–proposition structure asnet; however relations asnet modules obtained grounding whereas schema networks learn entities related scratch. vins schema networks tend yield agents generalise well across class similar environments. however unlike vins asnets— learn policies directly—schema networks learn model environment planning model must performed separately. extension convolutional networks graph structures received signiﬁcant attention recently networks often helpful invariances fewer parameters learn fully connected networks. applications include reasoning spatio-temporal relationships variable numbers entities molecular ﬁngerprinting visual question answering reasoning knowledge graphs best knowledge paper ﬁrst technique successfully solves factored representations automated planning problems. introduced asnet neural network architecture able learn generalised policies probabilistic planning problems. much cnns generalise images arbitrary size performing repeated local operations asnet generalise different problems domain performing convolution-like operations representations actions propositions related another. problems propositions related long chains actions asnet’s modelling capacity limited depth possible avoid limitation supplying network heuristic input features thereby allowing network solve range problems. considered supervised learning generalised policies asnet architecture could principle used learn heuristics embeddings trained reinforcement learning. asnet requires model actions affect portion state could also used settings beyond ssps mdps imprecise probabilities mdps set-valued transitions hope future work able explore alternatives asnets enrich planning capabilities deep learning.", "year": 2017}