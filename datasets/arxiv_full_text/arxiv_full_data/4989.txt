{"title": "RAIL: Risk-Averse Imitation Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "Imitation learning algorithms learn viable policies by imitating an expert's behavior when reward signals are not available. Generative Adversarial Imitation Learning (GAIL) is a state-of-the-art algorithm for learning policies when the expert's behavior is available as a fixed set of trajectories. We evaluate in terms of the expert's cost function and observe that the distribution of trajectory-costs is often more heavy-tailed for GAIL-agents than the expert at a number of benchmark continuous-control tasks. Thus, high-cost trajectories, corresponding to tail-end events of catastrophic failure, are more likely to be encountered by the GAIL-agents than the expert. This makes the reliability of GAIL-agents questionable when it comes to deployment in risk-sensitive applications like robotic surgery and autonomous driving. In this work, we aim to minimize the occurrence of tail-end events by minimizing tail risk within the GAIL framework. We quantify tail risk by the Conditional-Value-at-Risk (CVaR) of trajectories and develop the Risk-Averse Imitation Learning (RAIL) algorithm. We observe that the policies learned with RAIL show lower tail-end risk than those of vanilla GAIL. Thus the proposed RAIL algorithm appears as a potent alternative to GAIL for improved reliability in risk-sensitive applications.", "text": "imitation learning algorithms learn viable policies imitating expert’s behavior reward signals available. generative adversarial imitation learning state-of-the-art algorithm learning policies expert’s behavior available ﬁxed trajectories. evaluate terms expert’s cost function observe distribution trajectory-costs often heavy-tailed gail-agents expert number benchmark continuous-control tasks. thus high-cost trajectories corresponding tail-end events catastrophic failure likely encountered gailagents expert. makes reliability gail-agents questionable comes deployment risk-sensitive applications like robotic surgery autonomous driving. work minimize occurrence tail-end events minimizing tail risk within gail framework. quantify tail risk conditional-value-at-risk trajectories develop risk-averse imitation learning algorithm. observe policies learned rail show lower tail-end risk vanilla gail. thus proposed rail algorithm appears potent alternative gail improved reliability risk-sensitive applications. reinforcement learning used learn effective policy choosing actions order achieve speciﬁed goal environment. goal communicated agent scalar cost agent learns policy minimizes expected total cost incurred trajectory. algorithms along efﬁcient function approximators like deep neural networks achieved human-level beyond human-level performance many challenging planning tasks like continuous-control game-playing classical cost function handcrafted based heuristic assumptions goal environment. challenging real-world applications also prone subjectivity induced bias. imitation learning learning demonstration addresses challenge providing methods learning policies imitation expert’s behavior without need handcrafted cost function. paper study reliability existing imitation learning algorithms comes learning solely ﬁxed trajectories demonstrated expert interaction agent expert training. figure histograms costs trajectories generated expert gail agents high-dimensional continuous control tasks hopper-v humanoid-v openai gym. inset diagrams show zoomed-in views tails distributions observe gail agents produce tails heavier expert indicating gail prone generating high-cost trajectories. imitation learning algorithms fall broad categories. ﬁrst category known behavioral cloning uses supervised learning policy function state-action pairs expert-demonstrated trajectories. despite simplicity behavioral cloning fails work well limited amount data available. algorithms assume observations i.i.d. learn single time-step decisions. whereas sequential decision making problems predicted actions affect future observations i.i.d. assumption violated. result algorithms suffer problem compounding error covariate shift approaches ameliorate issue compounding error like smile searn suffer instability practical applications dagger aggrevate require agent query expert training allowed setting learning ﬁxed expert demonstrations. another drawback behavioral cloning allow agent explore alternate policies achieving objective might efﬁcient sense expert cared for. second category algorithms known inverse reinforcement learning abbeel attempts uncover underlying reward function expert trying maximize expert-demonstrated trajectories. reward function succinctly encodes expert’s behavior used agent learn policy algorithm. method learning policies known apprenticeship learning algorithms reward functions prioritize entire trajectories others. unlike behavioral cloning single time-step decisions hence suffer issue compounding error. however algorithms indirect learn reward function explains expert behavior tell learner directly learning actionable policy left algorithms. moreover algorithms computationally expensive scalability issues large environments levine koltun recently proposed generative adversarial imitation learning algorithm presents novel mathematical framework agent learns directly extracting policy expert-demonstrated trajectories obtained following irl. authors show unlike behavioral cloning method prone issue compounding error also scalable large environments. currently gail provides state-of-the-art performance several benchmark control tasks including table risk sensitivity integral human learning risksensitive decision-making problems context mdps investigated various ﬁelds e.g. ﬁnance operations research machine learning robotics give comprehensive overview different risk-sensitive algorithms. fall broad categories. ﬁrst category includes methods constrain agent safe states exploration second modiﬁes optimality criterion agent embed term minimizing risk. studies risk-minimization rather scarce imitation learning literature. take inspiration studies like modeling risk human decision-making conservatively approximate expert’s risk preferences ﬁnding outer approximation risk envelope. much literature imitation learning developed average-case performance center overlooking tail-end events. work take inclusive direct approach minimizing tail risk gail-learned policies test time irrespective expert’s risk preferences. order evaluate worst-case risk deploying gail-learned policies studied distributions trajectory-costs gail agents experts different control tasks observed distributions gail heavy-tailed expert tail corresponds occurrences high trajectory-costs. order quantify tail risk conditional-value-at-risk deﬁned expected cost given level conﬁdence popular coherent tail risk measure. heavier tail higher value observe value much higher gail experts tasks suggests gail agents encounter high-cost trajectories often experts. since high trajectory-costs correspond events catastrophic failure gail agents reliable risk-sensitive applications. work explicitly minimize expected worst-case risk given conﬁdence bound along gail objective learned policies reliable gail deployed still preserving average performance gail. developed policy gradient actor-critic algorithms mean-cv optimization learning policies classic setting. however algorithms directly applicable setting learning policy expert-demonstrated trajectories. take inspiration work make following contributions rest paper organized follows. section builds mathematical foundation paper introducing essential concepts imitation learning. section deﬁnes relevant riskmeasures describes proposed risk-averse imitation learning algorithm. section speciﬁes experimental setup section outlines evaluation metrics. finally section presents results experiments comparing rail gail followed discussion section concludes paper scope future work. mathematical background consider markov decision process denotes possible states denotes possible actions agent take state transition function that probability distribution next states given current state action cost function generates real number feedback every state-action pair gives initial state distribution temporal discount factor. policy function gives probability distribution actions given state denote trajectory length obtained following policy deﬁne expectation function deﬁned respect policy follows apprenticeship learning apprenticeship learning inverse reinforcement learning algorithms ﬁrst estimate expert’s reward function using optimal policy recovered reward function using mathematically problem described where denotes expert-policy. denotes cost function. denote hypothesis classes policy cost functions. denotes entropy policy term provides causal-entropy regularization helps making policy optimization algorithm unbiased factors expected reward. proposed generative adversarial imitation learning packs step process irlψ single optimization problem special considerations scalability large environments. name fact objective function optimized using generative adversarial network framework. following objective function gail here agent’s policy acts generator state-action pairs. discriminative binary classiﬁer form known discriminator given state-action pair predicts likelihood generated generator. two-player adversarial game started wherein generator tries generate pairs closely match expert discriminator tries correctly classify pairs expert agent. convergence agent’s actions resemble expert given state. generator discriminator assigned parameterized models respectively. training algorithm alternates gradient ascent step respect discriminator parameters policy-gradient descent step respect generator parameters following example multi-layer perceptrons model generator discriminator. section develop mathematical formulation proposed risk-averse imitation learning algorithm. introduce measure tail risk apply gail-framework minimize tail risk learned policies. portfolio-risk optimization literature tail risk form portfolio risk arises possibility investment moving three standard deviations away mean greater shown normal distribution tail risk corresponds events small probability occurring. distribution market returns heavy-tailed tail risk high probability small investment move beyond three standard deviations. conditional-value-at-risk conservative measure tail risk unlike measures like variance value risk applied distribution returns normal. mathematically random variable. denote probability value. value-at-risk respect conﬁdence level denoted deﬁned minimum value probability exceed note order-preserving maximization respect equation equivalent maximization respect equation constant controls amount weightage given optimization relative original gail objective. equation comprises objective function proposed risk-averse imitation learning algorithm. algorithm gives pseudo-code. appendix derives expressions gradients term hα)ν) respect namely risk-neutral case equal mean trajectory costs hence rail gail. adam algorithm gradient ascent discriminator trust region policy optimization policy gradient descent generator. term trained batch gradient descent compare tail risk policies learned gail rail continuous control tasks listed table environments simulated using mujoco physics simulator environments come packed true\" reward function openai trained neural network policies using trust region policy optimization reward functions achieve state-of-the-art performance made pre-trained models publicly available environments part repository used policies generate expert trajectories work gail fair comparison policies generate expert trajectories experiments. table gives number expert trajectories sampled environment. numbers correspond best results reported again following model generator discriminator value function generator) multi-layer perceptrons following architecture observationdim tanh tanh outdim means fully connected layer nodes tanh represents hyperbolic-tangent activation function hidden layers observationdim stands dimensionality observed feature space outdim equal discriminator value function networks equal twice dimensionality action space policy network. example case humanoid-v observationdim outdim policy network. value coefﬁcient given table coarse hyperparameter search. hyperparameters corresponding gail component algorithm identical used repository experiments. value term lone parameter trained batch gradient descent learning rate section deﬁne metrics evaluate efﬁcacy rail reducing tail risk gail learned policies. given agent policy roll trajectories {ξi}n estimate deﬁned section denotes value table hyperparameters rail experiments various continuous control tasks openai gym. fair comparison number training iterations expert trajectories used figure convergence mean trajectory-cost training. faded curves corresponds original value mean trajectory-cost varies highly successive iterations. data smoothened moving average ﬁlter window size demonstrate prevalent behavior plotted solid curves. rail converges almost fast gail continuous-control tasks times even faster. trajectory-cost remains probability gives expected value cost arα. intuitively gives average value cost worst cases total probability lower value metrics lower tail risk. order compare tail risk agent respect expert deﬁne percentage relativev follows table comparison expert gail rail terms tail risk metrics ar.. scores calculated samples trajectories. smaller values method outperforms gail continuous control tasks also outperforms expert many cases. experimental results discussion section present discuss results comparison gail rail. expert’s performance used benchmark. tables present values evaluation metrics different continuous-control tasks. estimate metrics sampled trajectories following interesting observations make rail obtains superior performance gail tail risk measures without increasing sample complexity. shows rail superior choice gail imitation learning risk-sensitive applications. applicability rail limited environments distribution trajectory-cost heavy-tailed gail. showed distribution risk variable normal constant given mean standard deviation thus absence heavy tail minimization trajectory cost aids learning better policies contributing minimization mean standard deviation trajectory cost. results reacher-v corroborate claims. although histogram show heavy tail mean converges tail risk scores improved indicates distribution trajectory-costs condensed around mean gail. thus rail instead gail matter whether distribution trajectory costs heavy-tailed gail not. figure shows variation mean trajectory cost training iterations gail rail. observe rail converges almost fast gail continuous-control tasks discussion times even faster. success rail learning viable policy humanoid-v suggests rail scalable large environments. scalability salient features gail. rail preserves scalability gail showing lower tail risk. rail agents show lesser tail risk gail agents training completed. however still requires agent real world sample trajectories training. rule environmental interaction training make agent simulator learning expert’s real-world demonstrations. setting changes third person imitation learning rail formulation easily ported framework evaluate paper. conclusion paper presents rail algorithm incorporates optimization within original gail algorithm minimize tail risk thus improve reliability learned policies. report signiﬁcant improvement gail number evaluation metrics continuous-control tasks. thus proposed algorithm viable step direction learning low-risk policies imitation learning complex environments especially risk-sensitive applications like robotic surgery autonomous driving. plan test rail ﬁelded robotic applications future. acknowledgments authors would like thank apoorv vyas intel labs sapana chaudhary madras helpful discussions. anirban santara’s travel supported google india google india fellowship award. references pieter abbeel andrew apprenticeship learning inverse reinforcement learning. proceedings twenty-ﬁrst international conference machine learning page pieter abbeel adam coates morgan quigley andrew application reinforcement learning aerobatic helicopter ﬂight. advances neural information processing systems pages brenna argall sonia chernova manuela veloso brett browning. survey robot learning demonstration. robotics autonomous systems issn http//dx.doi.org/./j.robot.... http//www.sciencedirect. com/science/article/pii/s. mariusz bojarski davide testa daniel dworakowski bernhard firner beat flepp prasoon goyal lawrence jackel mathew monfort muller jiakai zhang learning self-driving cars. arxiv preprint arxiv. mariusz bojarski philip yeres anna choromanska krzysztof choromanski bernhard firner lawrence jackel muller. explaining deep neural network trained end-to-end learning steers car. arxiv preprint arxiv. goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems pages timothy lillicrap jonathan hunt alexander pritzel nicolas heess erez yuval tassa david silver daan wierstra. continuous control deep reinforcement learning. corr abs/. http//arxiv.org/abs/.. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature arne nagengast daniel braun daniel wolpert. risk-sensitive optimal feedback control accounts sensorimotor behavior uncertainty. plos computational biology yael jeffrey edlund peter dayan john o’doherty. neural prediction errors reveal risk-sensitive reinforcement-learning process human brain. journal neuroscience aravind rajeswaran sarvjeet ghotra sergey levine balaraman ravindran. epopt learning robust neural network policies using model ensembles. international conference learning representations stéphane ross drew bagnell. efﬁcient reductions imitation learning. proceedings thirteenth international conference artiﬁcial intelligence statistics pages stéphane ross geoffrey gordon drew bagnell. reduction imitation learning structured prediction no-regret online learning. international conference artiﬁcial intelligence statistics pages david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature r.s. sutton a.g. barto. reinforcement learning introduction. bradford book. bradford book isbn https//books.google.co.in/books?id= cafribfxyc. emanuel todorov erez yuval tassa. mujoco physics engine model-based control. intelligent robots systems ieee/rsj international conference pages ieee calculation gradients cvar term section derive expressions gradients cvar term equation w.r.t. denote derivations inspired shown chow ghavamzadeh figure histogram costs trajectories generated gail-learned policy reacher-v. distribution shows heavy tail. table figure observe rail performs well gail even cases distribution trajectory costs heavy-tailed.", "year": 2017}