{"title": "Communication-Free Parallel Supervised Topic Models", "tag": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "abstract": "Embarrassingly (communication-free) parallel Markov chain Monte Carlo (MCMC) methods are commonly used in learning graphical models. However, MCMC cannot be directly applied in learning topic models because of the quasi-ergodicity problem caused by multimodal distribution of topics. In this paper, we develop an embarrassingly parallel MCMC algorithm for sLDA. Our algorithm works by switching the order of sampled topics combination and labeling variable prediction in sLDA, in which it overcomes the quasi-ergodicity problem because high-dimension topics that follow a multimodal distribution are projected into one-dimension document labels that follow a unimodal distribution. Our empirical experiments confirm that the out-of-sample prediction performance using our embarrassingly parallel algorithm is comparable to non-parallel sLDA while the computation time is significantly reduced.", "text": "showed signiﬁcantly improve computation speed. basic idea independently separate mcmc sampler generate local sample sub-dataset combine local sample sets estimate desired global posterior distribution. however existing algorithms limitations learning graphical models multimodal posteriors quasi-ergodicity problem. means although theoretically sampler positive probability switching local mode another sampler stuck local mode numerically. local posteriors converge different local modes differ signiﬁcantly global posterior representation achieved ﬁnal combination stage highly inaccurate. posterior topics multimodal permutation topics forms mode. hence existing parallel technique cannot directly applied slda. paper develop parallel algorithm slda maintains speed advantage embarrassingly parallel computing time bypassing problematic quasi-ergodicity problem. idea since main objective slda prediction rather mere categorization necessary estimate global posterior document topics. allows switch order document topics combination labeling variable prediction thus overcome quasi-ergodicity problem cause multimodal posterior slda. speciﬁcally instead ﬁrstly combining sampled topics generated parallel samplers form sample global posterior distribution secondly making predictions based estimated global posterior ﬁrst make predictions using sub-posteriors based sub-dataset combine local predictions make global prediction. making prediction ﬁrst project topics follow multimodal distribution one-dimensional document label follow unimodal distribution. hence overcome quasi-ergodicity problem combination stage. slda propose different combination rules combine local i.e. simple average predictions global predictions weighted average. simple average ﬁrst apply mcmc parallel sample topics words make predictions test dataset using locally parallel markov chain monte carlo methods commonly used learning graphical models. however mcmc cannot directly applied learning topic models quasi-ergodicity problem caused multimodal distribution topics. paper develop embarrassingly parallel mcmc algorithm slda. algorithm works switching order sampled topics combination labeling variable prediction slda overcomes quasi-ergodicity problem high-dimension topics follow multimodal distribution projected one-dimension document labels follow unimodal distribution. empirical experiments conﬁrm out-of-sample prediction performance using embarrassingly parallel algorithm comparable non-parallel slda computation time signiﬁcantly reduced. topic modelling important widely used machine learning tools. successfully applied various types problems including text analysis image processing speech recognition more. topic models useful learning hierarchical structures data. example applied text documents latent dirichlet allocation generative probabilistic topic model collections text corpora. application document modeled ﬁnite mixture underlying topics topic modeled distribution words. supervised latent dirichlet allocation used infer latent topics predictive document labeling variables. example slda used predict star ratings using movie reviews predict e-commerce sales using customer reviews along exponential growth data especially text data images slda often used applications dealing data. largescale learning slda generally slow difﬁcult. natural speed computation resort parallel computing. paper designing efﬁcient parallel algorithm large-scale learning slda. communication-free parallel algorithm commonly used machine learning minimizes communication costs resulted learning synchronization. recent research proposed parallel markov chain monte carlo algorithm little communication cost estimated slda model called local predictions. that take simple arithmetic average local predictions global predictions weighted average difference simple average combine local predictions global predictions. instead taking simple arithmetic average take weighted average local predictions. weight local predictions equal either inverse corresponding training dataset mean square errors prediction accuracy corresponding training dataset. conduct empirical experiments evaluate parallel algorithm slda. ﬁrst experiment textual documents corporate annual reports predict labeling variable earnings share. second experiment textual documents imdb movie reviews predict labeling variable sentiment scores based imdb ratings. empirical performance conﬁrms test mean square error prediction accuracy using embarrassingly parallel algorithm comparable single machine slda computation time signiﬁcantly reduced. rest paper organized follows. section reviews recent literatures parallel mcmc slda. section introduces embarrassingly parallel mcmc method slda. section shows empirical results section concludes. paper related recent literatures parallel mcmc. presented parallel mcmc algorithm requires little communication still maintains asymptotic correctness. idea ﬁrst partition dataset multiple sub-datasets perform mcmc sampling parallel using sub-dataset machine ﬁnally combine sub-samples construct full samples. gaussian models assumed local posterior allows explicit product densities ﬁnal combination stage. using similar idea makes step representing posterior weierstrass transform. analytically show approximation error weierstrass sampler bounded tuning parameters provide suggestions choice values. simulation result conﬁrms computation efﬁciency. improve ﬁnal combination stage presence outliers using median posterior speciﬁcally proposed bayesian inference approach scalable robust corruption data allows avoid extensive communication among machines running independent mcmc chains split subsets. novelty proposed different approximation full data posterior subset based evaluation geometric median subset posterior distributions. paper also focuses posterior combination stage order overcome quasi-ergodicity problem. paper also related another stream literature lda. since introduced widely used text mining machine learning. supervised latent dirichlet allocation introduced infer latent topics predictive response. compared slda appropriate prediction rather mere categorization. parameter estimation approximated maximum-likelihood estimation carried using variational expectation-maximization procedure handle intractable posterior expectations. slda model proposed accommodate various types response. however parameter estimation using expectation-maximization algorithm susceptible problems involving local maxima slow convergence rate. develop mcmc method using collapsed gibbs sampling. method widely used also adapted learning slda order increase computational efﬁciency learning large-scale slda natural idea resort parallel techniques. however posteriors multimodal parallel sampling problematic quasi-ergodicity problem. paper overcome quasi-ergodicity problem embarrassingly parallel slda focusing prediction rather topic categorization. iii. embarrassingly parallel mcmc slda start introducing embarrassingly parallel mcmc method illustrating quasi-ergodicity problem learning multimodal graphical models slda. introduce single machine nonparallel mcmc method supervised topic models base paralleled mcmc method. that propose embarrassingly paralleled mcmc method slda overcomes quasi-ergodicity problem. recent research shown models certain properties embarrassingly parallel mcmc guarantee asymptotic property theoretically perform well empirically. basic idea embarrassingly parallel mcmc independently gibbs sampler parallel sub-dataset combine local sub-samples global posterior distribution following basic idea naive application embarrassingly parallel mcmc slda includes following four steps divide training dataset several subsets. mcmc parallel sample textual topics however presence multimodal posteriors often true models large number latent variables embarrassingly parallel mcmc fail quasi-ergodicity. result global posterior representation achieved sample combination stage highly inaccurate. intuition quasi-ergodicity problem shown comparison unimodal posteriors illustrated figure multimodal posteriors illustrated figure figure true posterior unimodal distribution. parallel computing uses three machines machine generates sub-samples whose empirical distribution close true posterior. therefore empirical distribution combined samples constitutes valid estimation true posterior. figure true posterior three-modal distribution. parallel computing also uses three machines. multimodal posteriors likely samples generated different machines converge different modes especially high-dimensional space. example shown figure machines turn sample leftmost mode third machine samples rightmost mode. single machine converging three modes catches peak true posterior thus considered consistent posterior. however problematic naively combine posterior samples generated three machines converging different modes. case combined posterior might signiﬁcant peak wide region covers peak nadir true posterior. resulting empirical posterior would signiﬁcantly different true posterior. quasi-ergodicity problem seems difﬁcult avoid objective learn categorization topics. however quasi-ergodicity problem bypassed slda main objective make predictions based topics rather learning topic categorization. proposed solution switch order sub-sample instead ﬁrst combining sub-samples generated running mcmc sub-dataset constitute samples full training dataset using combined sample make predictions document label ﬁrst make predictions based sub-samples combine predictions using proposed combination rules simple average weighted average. intuition illustrated figure switching order subset combination prediction help bypass quasi-ergodicity problem. making predictions ﬁrst project high-dimensional topics follow multimodal distribution one-dimensional document labels follow unimodal distribution. describe embarrassingly parallel mcmc algorithm slda ﬁrst review basic notations background existing single machine non-parallel mcmc method slda. note label discrete model logit probability label normally distributed. corresponding mcmc method slda achieved slightly adjusting procedures described above. illustrating procedure unparalleled mcmc algorithm slda move procedure embarrassingly parallel mcmc algorithm slda. benchmark ﬁrst introduce naive combination intuitively naively follows embarrassingly parallel mcmc combining subposterior rather sub-predictions. discussed above naive combination quasiergodicity problem multimodal posterior lda. propose embarrassingly parallel mcmc slda overcome quasi-ergodicity problem leveraging fact main objective slda prediction rather gibbs sampling method ﬁrst introduced modiﬁed method slda. paper follow posterior inference prediction using mcmc supervised topic modeling. data divided training data posterior inference testing data prediction. posterior inference alternative steps posterior inference using stochastic ﬁrst step gibbs sampling assign topic token wdn. second step optimize regression parameters section evaluate algorithms comparing embarrassingly parallel mcmc algorithms slda benchmarks non-parallel mcmc algorithm slda embarrassingly parallel mcmc algorithm slda combining sub-posterior rather sub-predictions include non-parallel mcmc algorithm slda benchmark compare prediction accuracy computation time embarrassingly parallel mcmc algorithms slda. include naive combination another benchmark empirically show naively combining sub-samples constitute valid sample posterior based whole sample topic models thus results prediction accuracy. conduct groups experiments using different data sets. ﬁrst group experiments uses documents management discussion analysis report predict earnings share corresponding ﬁrmyear. second group experiments uses text movie reviews predict sentiment movie reviews. united states securities exchange commission mandates public ﬁrms yearly corporate reports known form report typically includes corporate organization history information consolidated statements well management discussion analysis analysis focus text information md&a section managers often express subjective opinions ﬁrm’s operational performance. textual information believed highly correlated variables measure performance investigated lots work ﬁnancial economics document labeling variable earnings share therefore goal predict based management discussions using slda model. ﬁrst downloaded reports year extracted md&a section tokenized text tagged tokens using stanford log-linear part-of-speech tagger generated adjective-noun phrases based taggedtokens. phrases appear infrequently included phrases appear least total number ﬁrms. ﬁnalized dataset contains ﬁrms phrases. histogram earnings share shown figure distribution earnings share close normal distribution implying satisfy normal assumption document label variable assumption slda mere categorization. prediction slda necessary generate global posterior topics. method ﬁrst prediction using sub-posterior based sub-dataset combine local predictions global predictions instead ﬁrst combining subposterior global posterior prediction using global posterior suggested naive combination. formal procedure embarrassingly parallel algorithm described follows specify combine local predictions global predictions. propose combination methods simple average weighted average. ﬁrst steps embarrassingly parallel procedure simple average weighted average. methods different third step different combination functions. simple average making separate predictions test data using topics sampled parallel mcmc based sub-training data take arithmetic average local predictions global predictions weighted average weighted average making predictions based topics sampled parallel mcmc based subset take weighted average local predictions global predictions. example weight assigned local predictions equal inverse training mean square error slda learned based corresponding subset experiment movie reviews data consists imdb movie reviews labels. movie reviews specially selected sentiment analysis. label sentiment movie review represented binary sentiment score. sentiment score imdb rating smaller imdb rating higher goal predict sentiment score based text movie reviews using slda model. experiment management discussion analysis randomly draw observations training rest observations test set. test parallel algorithms dual-core computer threads thus parallel computing randomly divide training subset observations each. speed prediction accuracy four algorithms mentioned above nonparallel naive combination simple average weighted average comparing computational time test prediction mean square error average results runs shown figure experiment movie reviews labeled training containing reviews experiment label test verify accuracy. randomly draw observations training predict sentiment score rest observations test set. test parallel algorithms dual-core computer threads thus parallel computing randomly divide training subset observations each. speed prediction accuracy four algorithms mentioned above nonparallel naive combination simple average weighted average comparing computational time test prediction accuracy. average results runs shown figure results discussion shown figure figure naive combination simple average faster non-parallel approach weighted average much slower three. naive combination fastest computational advantage communication-free parallel mcmc. simple average weighted average also beneﬁt computational advantage parallel mcmc. whereas algorithms need make predictions test dataset based local posteriors using training dataset means four predictions based four different training dataset generated test data prediction necessary made naive combination. additional predictions required simple average weighted average weaken speed advantage parallel computing thus simple average weighted average naive application embarrassingly parallel mcmc suffers quasi-ergodicity problem simple combination sampled topics generated parallel samplers training dataset constitute valid samples global posterior distribution. quasiergodicity problem bypassed switching order subset combination prediction meaning ﬁrstly making predictions using local sub-samples combining local predictions form global prediction. order switching allowed slda main objective prediction rather topic categorization. based framework propose communicationfree parallel mcmc algorithms slda simple average weighted average. simple average making predictions topic posteriors training dataset generate global predictions taking arithmetic average local predictions. weighted average making local predictions simple average generate global predictions taking weighted average local predictions weights inverse training mses training accuracy empirical results suggest simple average signiﬁcantly reduce computation time without sacriﬁcing prediction accuracy. v.-a. nguyen boyd-graber resnik sometimes average best importance averaging prediction using mcmc inference topic modeling empirical methods natural language processing kogan levin routledge sagi smith predicting risk ﬁnancial reports regression proceedings human language technologies annual conference north american chapter association computational linguistics. association computational linguistics slower naive combination. compared nonparallel simple average still much faster. drawback weighted average also needs make predictions training dataset order derive weights step costly training dataset large even slower non-parallel experiment. test dataset naive combination much larger three algorithms. similarly figure shows test dataset prediction accuracy naive combination lower three algorithms. results conﬁrm existence quasiergodicity problem models. figure also shows test dataset simple average weighted average close non-parallel benchmark. figure shows test dataset prediction accuracy simple average weighted average slightly higher non-parallel benchmark. results imply quasiergodicity problem overcome proposed algorithms. therefore proposed parallel algorithms indeed bypass quasi-ergodicity problem slda. considering computation time prediction accuracy simple average seems efﬁcient approach. speeds computation also retains prediction accuracy. computational drawback simple average need predict test dataset parallel machine. implies simple average save time compared non-parallel large training dataset small test dataset. toutanova klein manning singer featurerich part-of-speech tagging cyclic dependency network proceedings conference north american chapter association computational linguistics human language technology-volume association computational linguistics", "year": 2017}