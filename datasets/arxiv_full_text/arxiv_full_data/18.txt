{"title": "Beyond Temporal Pooling: Recurrence and Temporal Convolutions for  Gesture Recognition in Video", "tag": ["cs.CV", "cs.AI", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Recent studies have demonstrated the power of recurrent neural networks for machine translation, image captioning and speech recognition. For the task of capturing temporal structure in video, however, there still remain numerous open research questions. Current research suggests using a simple temporal feature pooling strategy to take into account the temporal aspect of video. We demonstrate that this method is not sufficient for gesture recognition, where temporal information is more discriminative compared to general video classification tasks. We explore deep architectures for gesture recognition in video and propose a new end-to-end trainable neural network architecture incorporating temporal convolutions and bidirectional recurrence. Our main contributions are twofold; first, we show that recurrence is crucial for this task; second, we show that adding temporal convolutions leads to significant improvements. We evaluate the different approaches on the Montalbano gesture recognition dataset, where we achieve state-of-the-art results.", "text": "recent studies demonstrated power recurrent neural networks machine translation image captioning speech recognition. task capturing temporal structure video however still remain numerous open research questions. current research suggests using simple temporal feature pooling strategy take account temporal aspect video. demonstrate method suﬃcient gesture recognition temporal information discriminative compared general video classiﬁcation tasks. explore deep architectures gesture recognition video propose end-to-end trainable neural network architecture incorporating temporal convolutions bidirectional recurrence. main contributions twofold; ﬁrst show recurrence crucial task; second show adding temporal convolutions leads signiﬁcant improvements. evaluate diﬀerent approaches montalbano gesture recognition dataset achieve state-of-the-art results. gesture recognition core components thriving research ﬁeld humancomputer interaction. recognition distinct hand motions becoming increasingly important enables smart interactions electronic devices. furthermore gesture identiﬁcation video seen ﬁrst step towards sign language recognition even subtle diﬀerences motion play important role. examples complicate identiﬁcation gestures changes background lighting varying environment variations performance speed gestures diﬀerent clothes worn performers diﬀerent positioning relative camera. moreover regular hand motion out-of-vocabulary gestures confused target gestures. convolutional neural networks facto standard approach computer vision. cnns ability learn complex hierarchies increasing levels abstraction end-to-end trainable. success huge impact vision based applications like image classiﬁcation object detection human pose estimation many more. video seen ordered collection images. classifying video frame frame bound ignore motion characteristics integration temporal information. depending task hand aggregating spatial features produced temporal pooling viable strategy we’ll show paper however method limited gesture recognition. apart collection frames video also seen time series. successful models time series classiﬁcation recurrent neural networks either standard cells long short-term memory cells ability learn dynamic temporal dependencies allowed researchers achieve breakthrough results e.g. speech recognition machine translation image captioning feeding video recurrent models need incorporate form spatial spatiotemporal feature extraction. motivates concept combining cnns rnns. cnns unparalleled spatial feature extraction capabilities adding recurrence ensures modeling feature evolution time. general video classiﬁcation datasets like ucf- sports-m hmdb- temporal aspect less importance compared gesture recognition dataset. example appearance violin almost certainly suggests target class playing violin class involves violin. model need capture motion information particular example. said categories modeling motion another always beneﬁcial. case gesture recognition however motion plays critical role. many gestures deﬁned spatial hand and/or placement also motion pattern. work explore variety end-to-end trainable deep networks video classiﬁcation applied frame-wise gesture recognition montalbano dataset introduced chalearn challenge study ways capturing temporal structure videos. ﬁrst method involves temporal convolutions enable learning motion features. second method introduces recurrence networks allows modeling temporal dynamics plays essential role gesture recognition. extensive evaluation cnns general video classiﬁcation provided karpathy using sports-m dataset. compare diﬀerent frame fusion methods baseline single-frame architecture conclude best fusion strategy modestly improves accuracy baseline. work extended show lstms achieve improvements temporal feature pooling scheme ucf- dataset human action classiﬁcation marginal improvements sports-m dataset. reason single-frame temporal pooling architectures important baseline models. represent motion spatially estimating displacement vectors pixel. core component two-stream architecture described simonyan zisserman used human pose estimation global video descriptor learning video captioning experimented optical greater computational preprocessing complexity models implicitly learn infer motion features end-to-end fashion chose engineer them. neverova present extended overview winning solution chalearn gesture recognition challenge achieve state-of-the-art score montalbano dataset. propose multi-modal ‘moddrop’ network operating three temporal scales ensemble method merge features diﬀerent scales. also developed training strategy moddrop makes network’s predictions robust missing corrupted channels. constituent parts architectures used work diﬀerent purposes. learning motion features three-dimensional convolution layers studied taylor classify short clips human actions dataset. baccouche proposed including two-step scheme model temporal evolution learned features lstm. finally combination used speech recognition image captioning video narration section brieﬂy describe diﬀerent architectures investigate gesture recognition video. overview models depicted figure note close attention comparability network structures. number units fully connected layers number cells recurrent models optimized based validation results network individually. hyper-parameters mentioned section section optimized temporal pooling architecture. result improvements baseline models caused architectural diﬀerences rather better optimization hyper-parameters preprocessing. single-frame single-frame architecture worked well general video classiﬁcation ﬁtting solution frame-wise gesture recognition setting. nevertheless give indication much static regions. shorthand notation full architecture follows denotes convolutional layer feature maps max-pooling layer fully connected layer units softmax classiﬁer. deploy leaky rectiﬁed linear units every layer. activation function deﬁned temporal feature pooling figure overview single-frame architecture. network spanning multiple video frames. model bidirectional recurrence. adding temporal convolutions three-dimensional max-pooling architecture added temporal convolutions bidirectional recurrence. temporal feature pooling second baseline model exploits temporal feature pooling strategy. suggested position temporal pooling layer right ﬁrst fully connected layer illustrated figure layer performs either mean-pooling max-pooling across video frames. structure cnn-component identical single-frame model. network able collect spatial features given time window. however order temporal events lost nature pooling across frames. core idea rnns create internal memory learn temporal dynamics sequential data. issue conventional recurrent networks states built previous time steps. gesture however generally becomes recognizable time steps frame-wise nature problem requires predictions diﬀerent cell types widespread standard cells lstm cells cell types compared work. finally output predictions computed softmax classiﬁer takes forward backward hidden states input ﬁnal architectures extends layers temporal convolutions enables extraction hierarchies motion features thus capturing temporal information ﬁrst layer instead depending higher layers form spatiotemporal features. performing three-dimensional convolutions approach achieve this. however leads signiﬁcant increase number parameters every layer making method prone overﬁtting. therefore decide factorize operation two-dimensional spatial convolutions one-dimensional temporal convolutions. leads fewer parameters optionally nonlinearity decides activate operations. include bias another nonlinearity spatial convolution step maintain comparability architectures. diﬀerent architectures proposed using layer. ﬁrst model replace convolutional layers single-frame spatiotemporal layer deﬁned above. furthermore apply three-dimensional max-pooling reduce spatial well temporal dimensions introducing slight translational invariance time. note architecture implies sliding window approach frame-wise classiﬁcation computationally intensive. second model illustrated figure time dimensionality retained throughout network. means carry spatial maxpooling. able stack bidirectional lstm cells responding high-level temporal dependencies. also incidentally resolves need sliding window approach implement frame-wise video classiﬁcation. chalearn looking people challenge consists three tracks human pose recovery human action/interaction recognition gesture recognition. dataset accompanying gesture recognition challenge called montalbano dataset used throughout work. dataset multi-modal gestures captured microsoft kinect depth sensor. sequences single user recorded front camera performing natural communicative italian gestures. data contains rgb-d image sequence skeletal pose stream provided microsoft kinect api. gesture vocabulary contains italian cultural/anthropological signs. gestures segmented means sequences typically contain several gestures. gesture performances appear randomly within sequence without prearranged rest pose. moreover several unannotated out-of-vocabulary gestures present. include varying clothes positions backgrounds lighting. training contains gestures test contains class imbalance negligible. starting ending frames gesture annotated well gesture class label. speed training crop part images containing user rescale pixels using skeleton information however show section even achieve good results crop images leave depth information. train models scratch end-to-end fashion backpropagating time recurrent architectures. network parameters optimized minimizing cross-entropy loss function using mini-batch gradient descent adam update rule found adam works great practice especially experimenting diﬀerent layer types model. models trained recurrent networks described section video ﬁles montalbano dataset contain approximately minutes footage consisting multiple gestures. recurrent models trained random fragments frames produce predictions every frame. summarize data sample channels frames each number cells model based validation results. lstm cells small improvement units settled rnns standard cells used units. location gestures within long sequences given. gesture generally frames long. small fraction gesture located beginning considered frames model enough information label frames correctly. allow buildup forward backward direction evaluation; feed frames keep middle evaluation. non-recurrent networks single-frame trained frame frame non-recurrent networks trained number frames optimized speciﬁc architecture. best number frames mean-pool across determined validation scores tested values case max-pooling pooling frames gives better outcomes. also pretraining cnns frame-by-frame ﬁne-tuning temporal max-pooling gave slightly improved results. observed improvements however using technique temporal mean-pooling. architecture added temporal convolutions three-dimensional max-pooling showed optimal results considering surrounding frames. targets non-recurrent networks labels associated centermost frame input video fragment. evaluate models using sliding window single-frame steps. regularization data-augmentation employed many diﬀerent methods regularize deep networks. data augmentation signiﬁcant impact generalization. trained models used augmentation parameters pixel translations vertical direction horizontal rotation degrees shearing degrees image scaling factors temporal scaling factors. intervals sample random value video fragment apply transformations online using cpu. dropout used inputs every fully connected layer. furthermore using leaky relus instead conventional relus factorizing three-dimensional convolutions spatial temporal convolutions also reduce overﬁtting. table comparison results diﬀerent architectures montalbano gesture recognition dataset. jaccard index indicates mean overlap binary predictions binary ground truth across gesture categories. also compute precision recall scores gesture class report mean score across classes. *the error rate based majority voted frame-wise predictions isolated gesture fragments. follow chalearn challenge score measure performance architectures. compare previous work montalbano dataset. competition score based jaccard index deﬁned follows binary ground truth gesture category sequence denoted binary vector whereas denotes binary predictions. jaccard index seen overlap rate bsn. compute ﬁnal score mean jaccard index among categories sequences computed overview results diﬀerent architectures shown table predictions single-frame baseline achieve jaccard index expected motion features extracted. observe signiﬁcant improvement temporal feature pooling furthermore mean-pooling performs better max-pooling. adding temporal convolutions three-dimensional max-pooling improves jaccard index three last entries table recurrent networks. surprisingly rnns acting high-level spatial features surpassing learning hierarchies motion features diﬀerence performance types cells small considered equally capable type problem temporal dependencies long-ranged. finally combining temporal convolution architecture using lstm cells improves score even table montalbano gesture recognition dataset results compared previous work. crop cropping speciﬁc areas video using skeletal information. depth usage depth-maps. pose usage skeletal stream features. note even depth images still achieve better results. table compare results previous work. best model outperforms method neverova consider rgb-d pixels input features remove depth information perform preprocessing rescaling images still achieve better results moreover even achieve better results without need depth images pose information illustrate diﬀerences output predictions diﬀerent architectures show randomly selected sequence figure single-frame trouble classifying gestures temporal pooling signiﬁcantly accurate. however latter still diﬃculties boundaries. adding temporal convolutions shows improved results output contains jagged predictions. seems disappear introducing recurrence. output bidirectional matches target labels strikingly well. figure show adding temporal convolutions enables neural networks capture motion information. user standing still units feature inactive feature network without temporal convolutions active units. user moving feature shows strong activations movement locations. suggests model learned extract motion features. showed paper adding bidirectional recurrence temporal convolutions improves frame-wise gesture recognition video signiﬁcantly. observed rnns responding high-level spatial features perform much better single-frame temporal pooling architectures without need take account temporal aspect lower layers network. however adding temporal convolutions layers architecture figure output probabilities shown sequence fragment test set. dashed line represents silences. non-recurrent models make mistakes diﬃculties making hard decisions gesture starts ends unable smooth predictions time. adding recurrence enables deep networks learn behavior manual annotators great accuracy. notable impact performance able learn hierarchies motion features unlike rnns. standard cells lstm cells appear equally strong problem. furthermore observed rnns outperform non-recurrent networks able predict beginning ending frames gestures great accuracy whereas models show uncertainty boundaries. future would like build upon work research domain sign language recognition. even challenging gesture recognition. vocabulary larger diﬀerences ﬁnger positions hand movements subtle signs context dependent part language. sign language related written spoken language complicates annotation translation. moreover signers communicate simultaneously facial manual body expressions. means sign language video cannot translated speech recognition transcribe audio written sentences. would like thank nvidia corporation donation used research. research leading results received funding agency innovation science technology flanders figure motion features ﬁgure illustrates eﬀect integrating temporal convolutions. depicted spatial feature active -layer-deep feature extracted architecture without temporal convolutions. spatiotemporal feature extracted model temporal convolutions. strong activations spatiotemporal feature maps moving indicate learned motion features.", "year": 2015}