{"title": "Robustness from structure: Inference with hierarchical spiking networks  on analog neuromorphic hardware", "tag": ["q-bio.NC", "cs.NE", "stat.ML"], "abstract": "How spiking networks are able to perform probabilistic inference is an intriguing question, not only for understanding information processing in the brain, but also for transferring these computational principles to neuromorphic silicon circuits. A number of computationally powerful spiking network models have been proposed, but most of them have only been tested, under ideal conditions, in software simulations. Any implementation in an analog, physical system, be it in vivo or in silico, will generally lead to distorted dynamics due to the physical properties of the underlying substrate. In this paper, we discuss several such distortive effects that are difficult or impossible to remove by classical calibration routines or parameter training. We then argue that hierarchical networks of leaky integrate-and-fire neurons can offer the required robustness for physical implementation and demonstrate this with both software simulations and emulation on an accelerated analog neuromorphic device.", "text": "abstract—how spiking networks able perform probabilistic inference intriguing question understanding information processing brain also transferring computational principles neuromorphic silicon circuits. number computationally powerful spiking network models proposed tested ideal conditions software simulations. implementation analog physical system vivo silico generally lead distorted dynamics physical properties underlying substrate. paper discuss several distortive effects difﬁcult impossible remove classical calibration routines parameter training. argue hierarchical networks leaky integrate-and-ﬁre neurons offer required robustness physical implementation demonstrate software simulations emulation accelerated analog neuromorphic device. past decades research neural networks undergone interesting branching process. hand machine learning community gradually increased interest originally brain-inspired neural networks. efforts crowned impressive recent success however obtained price strayed away biologically plausible dynamics. hand modern computational neuroscience pushing ever complex biologically realistic simulations hope uncover biological details information processing brain today communities investigating network models little common other. neuromorphic community master increasingly difﬁcult balancing act. core neuromorphic approach aims mimic various features neocortex silico. example essentially ubiquitous feature neuromorphic devices built emulate spiking neurons however core argument building devices hope unlock brain’s computational power moving beyond neumann computing paradigms. consequently driving question neuromorphic community might formulated follows possible relevant applications spiking neural networks proﬁt typical advantages physical implementation inherent parallelism high speed power consumption? ﬁndings discussed article suggest promising path towards ﬁnding answer. issue even pronounced case analog hardware since imposes additional constraints stem physics systems themselves. opposed digital systems neumann neuromorphic beneﬁt essentially perfect precision control analog systems deal inherent imperfections. imperfections concern hand equations motion network components must obey physics substrate therefore provide approximation target dynamics. hand degree precision parameters equations tuned certainly depends hardware design always fundamentally limited ﬁxed-pattern variations temporal noise. imperfections network dynamics parameters necessarily distort behavior emulated networks usually impairs performance degree question parameter control essential one. constitutes perennial challenge analog neuromorphic system design operation therefore often addressed literature thorough discussion parameter calibration in-the-loop training analog circuits found represents complement present study. present manuscript mainly concerned distortions network dynamics imposed physics emulation device cannot directly addressed e.g. calibration. begin identifying bayesian spiking network model valuable computational properties able learn probabilistic model input data subsequently used generative discriminative model feature difﬁcult achieve even abstract neural networks here focus discriminative properties. model general susceptible hardwareorder achieve similar dynamics neurons equivalent ﬁring regime needs established. sampling framework neuron receives kinds spiking input information-encoding input neurons network diffuse background input represents source stochasticity modeled poisson sources. input spike trains generate types current onto membrane denote noise respectively here membrane capacitance leak conductance potential external current determines bias general noisy neurons logistic activation function required eqn. shown high-conductance state activation function well approximated logistic function scaled parameters represents noise-free membrane potential neuron. equivalence abstract model enables neuron sample correctly conditional distribution translation boltzmann parameters eqn. conductancebased domain achieved using following rules mean free membrane potential effective membrane time constant high-conductance state gsyn total synaptic conductance synaptic time constant erev synaptic reversal potential. allows direct mapping abstract networks neurons sample accurately target distribution important note networks merely complicated replicas classical machine learning approaches. addition able emulate computational power traditional boltzmann machines spiking networks also harness certain biological mechanisms extend functionality. example shown endowed short-term synaptic plasticity lifbased become good generative models learned datasets time maintaining high classiﬁcation performance presented individual data samples induced distortions discuss detail sec. iii. particular example characterize effects spikey chip neuromorphic system emulation back-end. despite model’s ostensible lack robustness argue that endowed particular hierarchical connectivity structure becomes robust studied hardware-induced distortions. substantiate conjecture software simulations hardware emulations particular without training compensate parameter noise hardware show network loses relatively small fraction initial performance running spikey. represents knowledge ﬁrst scalable implementation hierarchical probabilistic network accelerated analog neuromorphic hardware. continued technological advances large-scale processing enabled recent resurgence artiﬁcial neural networks. already envisioned decades theoretical models brain-like architectures neural networks routinely outperform rival models pattern recognition tasks here focus particular neural network model spiking variant boltzmann machine shown compatible biologically plausible hardware-implementable spiking neurons brieﬂy describe structure dynamics stochastic networks leaky integrate-and-ﬁre neurons discuss potential problems arise implementation analog hardware. neural sampling framework population neurons represents binary random vector refractory state neuron following spike time chosen represent -state associated random variable fig. sampling neurons distortions induced implementation physical substrate. exemplary membrane potentials sampling network neurons. neuron associated random variable equal neuron refractory. schematic recurrent network neurons symmetric synaptic weight matrix bias potential vector approximates boltzmann machine parameters given eqns exemplary state distribution -neuron network sampled distribution target distribution evolution kullback-leibler divergence time. multiple runs different random seeds marked different colors. synaptic transmission delays change temporal correlations states different neurons. example consider neurons connected large excitatory weights without delays network samples correctly target distribution relatively large delays sampled distribution becomes completely different wrongly sampled mixed states marked direct consequence synaptic transmission delays. imperfect high-conductance state leads deviation neuronal activation function ideal logistic shape. modiﬁes sampled distribution reducing probability neurons spike especially positive biases. refractory times synaptic time constants coupled ensure average interaction neurons refractoriness correct amplitude given eqn. spike-to-spike variability refractory times disrupts equivalence effectively modifying interaction strength plausible network methods exist compatible hebbian learning. wake-sleep algorithm particular requires synapse access activity prepostsynaptic neuron learning rule tries adapt activity zmodel network dreaming phase evolves freely activity zdata awake state constrained data i.e. units clamped particular values. despite simplicity learning algorithm used achieve high classiﬁcation rates various machine learning datasets distribution network samples uniquely determined neuro-synaptic dynamics parameters. deviation model speciﬁcation alter sampled distribution general restrict network’s ability perform correct inference learned sample space. mapping model imperfect physical substrate therefore straightforward. article study three types distortions network dynamics caused mapping analog silicon substrate. first consider spike transmission delays. since using point neurons describe delays synaptic delays. many analog neuromorphic devices including later mixed-signal systems meaning spikes transmitted digitally. consequently digitization transport digital data conversion back analog domain synapses contribute synaptic delays. delays short terms wall-clock time become particularly critical accelerated systems. systems neuronal synaptic dynamics deﬁne characteristic time scale network evolves orders magnitude smaller biology potentially entering range synaptic transmission delays regardless exact nature network performing neural sampling order neuron able calculate correct conditional distribution information gathered neuron incoming psps must coincide true state corresponding presynaptic neurons required eqn. temporal coincidence disrupted delays thereby distort sampled distribution exempliﬁed fig. second neuromorphic systems controllable neuron synapse parameters conﬁgured within certain range resolution. example consider membrane time constant neuron. become arbitrarily large. limit reaction speed neurons impair functionality entire network. neurons large slows saturation activation function thereby distorts logistic shape required sampling boltzmann distributions. third temporal noise also affect computation. depending particular in-silico implementation analog system subject degree temporal noise electronic signals including directly inﬂuence neuro-synaptic dynamics. particular case largest temporal noise component affects refractory times τref. since relevant neuron synaptic circuits physically disconnected chip spike-to-spike variation τref independent synaptic time constant considered ﬁxed. consequently state neuron coincide anymore information transmits psps postsynaptic partners leading distortion sampled probability distribution conceptually similar manner synaptic delays study spikey single-chip system physical emulation substrate mixed-signal device combines analog components modeling membrane synapse dynamics digital circuitry spike-based communication. fig. shows photo device along sketch neuron circuit illustrates origin three distortive effects discussed above. overlay fig. shows spike emitted neuron travels communication buses synapse driver generates voltage ramp synapse array inside synapse voltage ramp converted approximately exponential signal added total synaptic conductance neuron circuit. sequence processing stages causes effective synaptic delays seen fig. neuron schematic fig. explains cause non-logistic activation functions noisy refractory times. reversal potentials connected membrane conductances saturate certain amplitude. maximum total conductance deﬁnes minimum achievable effective membrane time constant limits gain activation function seen fig. duration refractory time determined monoﬂop controlled current iref /τref. order offset least extent effect delays require long refractory times i.e. small currents cause transistors monoﬂop leave saturation operate sub-threshold regime. transition accompanied increase relative amplitude temporal noise increases variability τref. represents primary cause large spiketo-spike ﬂuctuations refractory time seen fig. identiﬁed origins critical distortions physical implementations sampling next turn central question study possible recover computational capabilities sampling networks fig. characterization measurements employed neuromorphic system. neuromorphic spikey chip overlaid sketch neural network components bottom simpliﬁed schematic single neuron. left synaptic delays measured recurrently connecting neuron inhibitory synapse. bottom left relatively sharp onset inhibitory allows precise measurement despite temporal noise membrane potential. right synaptic delay distribution neurons single synapse driver. activation functions spikey neurons compared nearly ideal logistic activation function achieved high-conductance state exemplary activation functions drawn thicker lines blue activation functions belong neurons short long respectively. refractory times measured choosing suprathreshold leak potential neurons subtracting reset-to-threshold ﬁrst passage time interspike interval. bottom relative spike-to-spike variability refractory time mean refractory time spikey neurons. hierarchical networks robustness hardwarefig. induced distortions. structure studied network. exemplary training sample obtained mnist dataset resolution reduction binarization. bottom exemplary training samples classes. inﬂuence simulated hardware-induced distortions classiﬁcation performance network error bars represent standard deviation multiple runs different random seeds. green performance training data. blue performance test data. brown mean value standard deviation respective parameter measured spikey general framework sampling impose restrictions network topology apart requirement zero-diagonal symmetric synaptic weight matrix however imposing restrictions connectivity practical use. building network able learn generalize data rather natural hierarchization consists subdividing network layer representing visible data hidden layers recognize common features data samples ﬁnal classiﬁcation layer assigns sample particular category label. indeed guiding principle behind hierarchical neural networks multilayer perceptrons deep convolutional nets case removal lateral connections within layer proven particularly beneﬁcial learning resulting networks socalled restricted boltzmann machines emulated networks appropriate parameters described sec. synaptic delays noisy refractory times similar effects sampled distribution. however nature information lif-based rbms expected counter simultaneously. presented unambiguous input data mean ﬁring rates visible neurons ﬁxed; application example encode grayscale values pixels input image gk/. regime spike transmission delays effect visible layer essentially operates rate-based mode time shifts matter. operating mode refractory noise also averaged out. transmission delays noisy refractory times remain critical interaction hidden label neurons. however interaction comparatively weak layer architecture real-world scenarios label space typically much smaller dimensionality input space. hidden neuron therefore receives input many visible neurons label neurons. therefore even though visible-to-hidden synaptic weights approximately large hidden label layer summed input visible layer completely dominant virtue sheer numbers. therefore hidden layer mostly driven input layer distorted interaction hidden label neurons likely become insigniﬁcant. ﬁnite membrane time constant hand affect neurons layers neglected. however effect countered least extent nature sampled distribution well-trained networks. wake-sleep training effect carving deep troughs network’s energy landscape energy minima correspond particular patterns network layers local attractors state space. thus deviations sampled distribution small attractor landscape change signiﬁcantly. consequently visible layer clamped input data layers still likely fall corresponding attractor state thus conserving classiﬁcation performance. tested predictions series software simulations. trained -layer lif-based reduced version mnist dataset ensure compatibility spikey chip network size restricted visible hidden label neurons. pixel images belonging digit classes produced ﬁrst reducing mnist digit resolution followed binarization pixel values class training test consisted randomly chosen images. training consisted layer-by-layer pre-training wakesleep-style algorithm deliberately refrained ﬁne-tuning weights with e.g. backpropagation fig. study direct-to-hardware mapping hierarchical network fig. classiﬁcation performance. black software simulation distortion-free network purple software simulation network distortion mechanisms present simultaneously amplitudes variances measured spikey green hardware emulation hidden layer software evaluation label layer. blue/red emulation hidden layer repeated single neurons followed software evaluation label layer. neurons marked well conﬁgurable therefore performed chance level. exemplary spike trains subset neurons network hidden layer running hardware. spike trains belonging neurons marked red. chip biases implemented high-frequency regular spike trains connected hidden neurons arbitrary synaptic kernel scaled weights average effect regular spike train membrane potential neuron proportional therefore controlled within imposed -bit precision appropriately conﬁguring note spike trains also need routed across chip circumvent synaptic delays. three hardware-induced distortion mechanisms discussed above. expected neither synaptic transmission delays variability refractory times affected performance network signiﬁcantly. surprisingly large range membrane time constants classiﬁcation rate remained almost unaffected. activation function became signiﬁcantly distorted large attractor landscape change signiﬁcantly enough cause decay classiﬁcation rate overall within parameter ranges spikey chip network remained weakly affected studied mechanisms. however since later step network mapped hardware without training needed also consider effect discretized synaptic weights. default synaptic weights spikey chip controllable -bit precision important note pose fundamental problem networks type; effects weight discretization countered appropriate in-the-loop training discussed e.g. here take effect account preparation hardware experiments sec. fig. shows effect weight discretization network’s classiﬁcation performance. spikey chip performance decay lies approximately note effect signiﬁcantly larger effects caused distortion mechanisms. combined simulation distortive effects used provide reference later emulation spikey. effects simulated amplitudes corresponding values measured spikey ideal undistorted case network classiﬁcation performance layer implemented software. spikes produced hidden layer label neurons simulated nest this essentially broke hidden→label→hidden feedback loop argued sec. signiﬁcantly affect classiﬁcation performance network. furthermore allowed detailed investigation quality single neurons chip discussed below. label assigned network input image determined label neuron produced spikes clamping period. within error margins corresponds well reference software simulations slightly better average classiﬁcation attributed explicit selection hidden neurons. indeed result conﬁrms robustness network model also highlights robustness various hardware-induced distortions explicitly account parameter noise crosstalk furthermore implementation surprisingly robust even towards neurons strongly deviant ﬁring characteristics discussed below. network model hidden neurons laterally interconnected. furthermore label layer simulated software also label-mediated lateral interaction hidden neurons. therefore possible emulate entire network spikey neuron time. alternative emulation setup sequence emulation runs containing single hardware neuron performed. hardware neuron conﬁgured represent hidden neuron receiving corresponding input spike train. output spike trains runs aggregated label layer before. experiment repeated subset selected hardware neurons results plotted thin bars fig. overall performance neuron quantiﬁes quality task hand. main reason differences neurons shape activation function seen fig. neurons perform poorly activation function shallow thus strongly skewing sampled distribution. extreme steep activation function also detrimental resolution synaptic weights permit arbitrarily ﬁne-grained tuning effective weights biases. note particular neurons perform chance level however existence neurons appear strong effect classiﬁcation performance network whole important challenges analog neuromorphic computing design neural network architectures robust hardware-induced distortions network dynamics parameters. paper argued hierarchical spiking sampling networks emulating restricted boltzmann machines inherently resistant distortions. studied three speciﬁc distortion mechanisms general strongly disruptive ongoing computation sampling networks synaptic transmission delays variability refractory times saturating membrane conductances. despite apparent sensitivity shown hierarchical topology shapes information makes largely resilient effects. results obtained software simulations also conﬁrmed experiments accelerated analog neuromorphic device. furthermore addition robust studied distortion mechanisms network model also displayed encouraging degree resilience hardware-induced effects quantiﬁed systematically studied ones. choice neuron model made hand analytical tractability importantly fact model represents common denominator many spiking neuron models appropriate parameter choices models achieve dynamics close required sampling. furthermore model represents de-facto standard neuromorphic engineering observed properties networks encourage theoretical experimental investigation. here studied relative performance losses used relatively small network small dataset. software simulations show larger-scale versions networks enable efﬁcient powerful inference complex data spaces interesting whether large networks remain robust hardware-induced distortions smaller instantiations studied here. largescale accelerated analog devices already place able accomodate experiments. proposed architecture rather conservative clamping schedule biological second image currently achieved acceleration factor will example enable classiﬁcation full mnist dataset within wall-clock time. although used hierarchical networks classiﬁcation also used generative models perform example pattern completion. nature energy landscape networks suggests generative properties could also robust hardwareinduced distortions. since clear image corresponds deep mode energy landscape small distortions sampled distribution unlikely strongly disrupt generative properties network. probabilistic switching different modes ambiguous input present facilitated short-term plasticity shown paper deliberately refrained training hardware-emulated networks. however expected training hardware loop signiﬁcantly improve classiﬁcation. idea behind in-theloop training iteratively alternate forward pass hardware emulated network activity recorded backward pass software network parameters updated e.g. error backpropagation. recent studies demonstrated software analog neuromorphic hardware parameter updates need precise approximately follow gradient likelihood function. furthermore accelerated systems currently development also implement powerful on-chip learning solutions. architectures enable accelerated classiﬁcation even importantly accelerated learning network parameters. ﬁrst authors contributed equally work. designed performed experiments software spikey chip. designed software environment performing simulations. authors contributed writing paper. would like thank thomas pfeil johannes bill technical support spikey chip well luziwei leng providing model parameters. thank andreas baumbach johannes bill dominik dold carola fischer vitali karasenko akos kungl walter senn invaluable contribution ﬁnal manuscript. research supported grant silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam mastering game deep neural networks tree search nature vol. pfeil grübl jeltsch müller müller petrovici schmuker brüderle schemmel meier networks universal neuromorphic computing substrate frontiers neuroscience vol. schemmel brüderle grübl hock meier millner wafer-scale neuromorphic hardware system large-scale neural modeling proceedings ieee international symposium circuits systems benjamin mcquinn choudhary chandrasekaran j.-m. bussat alvarez-icaza neurogrid mixedanalog-digital multichip system large-scale neural simulations proceedings ieee vol. petrovici vogginger müller breitwieser lundqvist muller ehrlich destexhe lansner schüffny schemmel meier characterization compensation networklevel anomalies mixed-signal neuromorphic modeling platforms plos vol. indiveri linares-barranco hamilton schaik etienne-cummings delbruck s.-c. dudek häﬂiger renaud schemmel neuromorphic silicon neuron circuits frontiers neuroscience vol. sheik stefanini neftci chicca indiveri systematic conﬁguration automatic tuning neuromorphic systems ieee international symposium circuits systems bill schuch brüderle schemmel maass meier compensating inhomogeneities neuromorphic vlsi devices short-term synaptic plasticity front. comp. neurosci. vol. schmitt klähn bellec grübl güttler hartel hartmann husmann husmann jeltsch karasenko neuromorphic hardware loop training deep spiking network brainscales wafer-scale system proceedings international joint conference artiﬁcial neural networks leng petrovici martel bytschok breitwieser bill schemmel meier spiking neural networks superior generative discriminative models cosyne abstracts salt lake city february buesing bill nessler maass neural dynamics sampling model stochastic computation recurrent networks spiking neurons plos computational biology vol. petrovici bytschok bill schemmel meier high-conductance state enables neural sampling networks neurons neuroscience vol. suppl rumelhart hinton r.j. learning internal representations error propagation parallel distributed processing explorations microstructures cognition vol. salakhutdinov hinton deep boltzmann machines international conference artiﬁcial intelligence statistics haykin comprehensive foundation neural networks vol. esser merolla arthur cassidy appuswamy convolutional networks fast energy-efﬁcient neuromorphic computing proceedings national academy sciences vol. friedmann schemmel grübl hartel hock meier demonstrating hybrid learning ﬂexible neuromorphic hardware system ieee transactions biomedical circuits systems schemmel kriener müller meier accelerated analog neuromorphic hardware system emulating nmda calciumbased non-linear dendrites proceedings international joint conference artiﬁcial neural networks", "year": 2017}