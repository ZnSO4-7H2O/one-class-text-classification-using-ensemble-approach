{"title": "A Flow Model of Neural Networks", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "Based on a natural connection between ResNet and transport equation or its characteristic equation, we propose a continuous flow model for both ResNet and plain net. Through this continuous model, a ResNet can be explicitly constructed as a refinement of a plain net. The flow model provides an alternative perspective to understand phenomena in deep neural networks, such as why it is necessary and sufficient to use 2-layer blocks in ResNets, why deeper is better, and why ResNets are even deeper, and so on. It also opens a gate to bring in more tools from the huge area of differential equations.", "text": "based natural connection resnet transport equation characteristic equation propose continuous model resnet plain net. continuous model resnet explicitly constructed reﬁnement plain net. model provides alternative perspective understand phenomena deep neural networks necessary suﬃcient -layer blocks resnets deeper better resnets even deeper also opens gate bring tools huge area diﬀerential equations. deep neural networks proven impressively successful certain supervised learning tasks successively maps datasets feature space simple output functions suﬃcient achieve high performance. although single layer simple transformation composition many layers represent complicated functions. guided philosophy supported powerful computers massive amount data deeper deeper neural networks invented remarkable event record imagenet competition using resnets layers. going deeper believed helpful. however mechanism many mysteries ‘black box’ still exploration. contributions. short note construct models neural networks. restricted answering speciﬁc questions neural networks build framework connects neural networks diﬀerential equations. bridge could bring perspective methods could applied understand solve learning problems. ∗most part work submitted arxiv separated notes august september respectively. latter announced technical reasons. combination previous notes. observed resnet discretization characteristic equation transport equation. conversely transport equation regarded continuous model resnet. physics transport equations models describing dynamics quantities transported continuous ﬂows. hence call continuous model model. models immediately available explain phenomena neural networks. example naturally supports belief power depth neural networks. also relates plain nets resnets explicitly. connection used explain super depth resnets. besides explains it’s necessary -layer blocks resnets relu activations related works. consider solve supervised semi-supervised learning problems pdes point cloud data. propose alternative methods initializing training resnets. recently noted ones observed connection neural networks diﬀerential equations. proposes study resnet dynamical system. based that consider training algorithm optimal control point view. chang presents empirical study training resnet dynamical system. however papers focus resnets. haven’t seen paper considering plain nets similar point view. structure note follows. section start transport equation characteristic equation resnet. section build continuous model plain done linear activation respectively glued section model plain discretized resnet. considering relationship neural networks models comments summarized section discretizing euler’s method naturally leads resnet. order make following approximations reasonable assume change regular enough. especially assume solution exist regular enough. ﬁrst glance appears simply deﬁning transport velocity natural. actually reasonable. inner parameters used specify location space data. controls assigned velocity vector. activation non-negative even restricted often case outer parameters necessary adjust direction magnitude transport velocity. inner parameters outer parameters necessary ingredients transport velocity ﬁeld. course symmetric outer parameters necessary purpose. resnet obtained special. firstly time step residual term made suﬃciently small comparing leading term necessary condition resnet modeled transport equation. secondly parameters resnet changes slowly block block. speciﬁcally parameters positions adjacent resnet blocks close other assumed discretizations continuous functions time. example activation multiplication weight matrix bias vector. however non-residual term deﬁnes ﬁnite transformation xk−. naturally interpreted velocity makes diﬃcult modeled transport equation directly. section construct continuous done linear nonlinear activation respectively. later used construct resnet-approximation plain preparation deﬁne time scaling function required mainly considered object weight matrix without loss generality assume embedded space suﬃciently high dimension square matrix rank written exponential form done. unfortunately generally possible. consider full size singular value decomposition rank ensure taken proper rotations even include mirror reﬂection invariant subspace. since rotations ﬁnite angles expressed exponential angular velocity matrices summary nonlinear plain layer modeled successively ﬂows takes units time move takes unit time move technical completeness let’s glue ﬂows together. deﬁne previously construct transport velocity ﬁeld typical single layer plain net. let’s construct velocity ﬁeld whole network. consider terminal value problem linear transport equation transport velocity ﬁeld deﬁned gluing diﬀerent layers. detail follows. {tk}l uniform partition small enough. notice time scaled units time equivalent units time notice transport velocity ﬁeld means smooth thus seen transport equation continuous model plain net. given plain construct transport equation using parameters activations. section shown resnets modeled continuous ﬂows. section shown plain nets also modeled continuous ﬂows. it’s natural consider connection types neural networks continuous models. section show re-discretizing model obtained plain resnet approximation plain net. speciﬁcally layer plain approximated several resnet blocks. option leave whole map. option discretize continuous model section second option needs applying euler’s method odes corresponds linear map. let’s discretize ﬁrst equation example. {τr}l uniform partition small enough. denote xk−. euler’s method following let’s focus nonlinear part. activation solved following way. recall takes unit time move clarity notations still range time {τr}l uniform partition small enough. denote solve euler method iteratively thus relu activation approximation plain nets resnets quite trivial. explicit expression nonlinear consider linearization near according deﬁnition jacobian approximation contains -layer resnet block followed nonresidual linear shown figure whole activation composed several iterations approximation together linear single k-th layer plain approximated composition linear maps -layer resnet blocks. figure alternatively also successors instead whole linear figure figure single layer plain approximated composition linear multi-layer resnet. dashed green represents structure figure dotted orange ellipse approximate activation layer plain net. little confusing multi-layer resnet still contains several activations bother replace activation multi-layer structure containing activations? answer follows. roles activations original plain resnet diﬀerent. plain activation causes nonlinear distortion layers poses geometric constraint layer ﬂow. eﬀect signiﬁcant immediate. resnet above however activation causes nonlinear distortion transport velocity ﬁeld poses diﬀerential constraint layer ﬂow. eﬀect become signiﬁcant accumulation. another confusing thing continuously change parameters layer layer. since neural networks discretizing continuous it’s natural guess parameters networks varies slowly layer layer. however careful idea. generally true nonlinear networks. nonlinear plain seen continuous transport velocity ﬁeld simply like nonlinear resnet shown figure situation subtle. approximate activation layer original plain several basic structures resnet used. structure basic structures same parameters corresponding positions varies slowly. means changes changes slowly sense parameters resnet changes continuously. naively parameters layer layer continuity. section respectively transport equation characteristic equation continuous model resnets plain nets. correspondence neural network model natural even obvious resnets. summarized table illustrated figure inspired connection neural networks transport equations well studied methods area diﬀerential equations might help understand neural networks solve related problems. examples language transport equation inner parameters used specify location space data. tells network assign velocity vector. outer parameters used adjust magnitude direction velocity vector speciﬁed location. outer parameters necessary relu asymmetric. correspondence provides deep good neural networks. perspective transport equation order transform terminal value function initial value function transport velocity ﬁeld needs complicated. make discretization converge control error necessary small time step size many iterations. allows discretization regular step makes small progress. neural networks transformation provided layer also limited. needs layers accomplish required deformation datasets. practice resnets usually signiﬁcantly deeper plain nets. considering connections model reason quite transparent. hand plain equivalent model constructed section hand model discretized iterative resnet described section combining facts resnet reﬁnement original plain net. naturally deeper plain net. although resnets deep many authors shown training resnets easier plain nets comparable depth. diﬀerential equation point view resnets deform dataset incremental much regular plain nets. solving pdes people often dissipative terms increase regularity solutions. terms neural networks means randomness feedforward process. idea close dropout technique already known resnet corresponds method characteristics transport equations. methods solve pdes might lead alternative equivalent architectures neural networks. training neural networks could considered solving inverse problem transport equation. means initial value terminal value given. task time-dependent velocity ﬁeld transports initial value terminal value. course solution inverse problem highly nonunique. uncountably many velocity ﬁelds job. thus inverse problem usually formulated optimization problem constrained transport equation well initial terminal conditions. many methods solve problems. could modiﬁed train neural networks possible question continuous model dimension matching problem. practice ﬂexibility choose diﬀerent dimensions diﬀerent layers. continuous model seems diﬃcult since main concern paper theoretical it’s serious problem actually dimension matching problem already exists resnets. restriction shortcuts used dimensions matched. otherwise extra projection matrices needed. note adopted simple assumption dataset embedded space suﬃciently high dimension beginning. ambient dimension doesn’t change time. order approximate necessary reduction intrinsic dimension dataset time used compressing ﬂows. course theoretical approach ineﬃcient practice. alternative approach glue diﬀerent models diﬀerent dimensions.", "year": 2017}