{"title": "Latent Fisher Discriminant Analysis", "tag": ["cs.LG", "cs.CV", "stat.ML", "I.2.10"], "abstract": "Linear Discriminant Analysis (LDA) is a well-known method for dimensionality reduction and classification. Previous studies have also extended the binary-class case into multi-classes. However, many applications, such as object detection and keyframe extraction cannot provide consistent instance-label pairs, while LDA requires labels on instance level for training. Thus it cannot be directly applied for semi-supervised classification problem. In this paper, we overcome this limitation and propose a latent variable Fisher discriminant analysis model. We relax the instance-level labeling into bag-level, is a kind of semi-supervised (video-level labels of event type are required for semantic frame extraction) and incorporates a data-driven prior over the latent variables. Hence, our method combines the latent variable inference and dimension reduction in an unified bayesian framework. We test our method on MUSK and Corel data sets and yield competitive results compared to the baseline approach. We also demonstrate its capacity on the challenging TRECVID MED11 dataset for semantic keyframe extraction and conduct a human-factors ranking-based experimental evaluation, which clearly demonstrates our proposed method consistently extracts more semantically meaningful keyframes than challenging baselines.", "text": "linear discriminant analysis well-known method dimensionality reduction classiﬁcation. previous studies also extended binary-class case multi-classes. however many applications object detection keyframe extraction cannot provide consistent instance-label pairs requires labels instance level training. thus cannot directly applied semi-supervised classiﬁcation problem. paper overcome limitation propose latent variable fisher discriminant analysis model. relax instance-level labeling bag-level kind semi-supervised incorporates data-driven prior latent variables. hence method combines latent variable inference dimension reduction uniﬁed bayesian framework. test method musk corel data sets yield competitive results compared baseline approach. also demonstrate capacity challenging trecvid dataset semantic keyframe extraction conduct human-factors ranking-based experimental evaluation clearly demonstrates proposed method consistently extracts semantically meaningful keyframes challenging baselines. linear discriminant analysis powerful tool dimensionality reduction classiﬁcation projects high-dimensional data low-dimensional space data achieves maximum class separability basic idea classical known fisher linear discriminant analysis obtain projection matrix minimizing within-class distance maximizing between-class distance simultaneously yield maximum class discrimination. proved analytically optimal transformation readily computed solving generalized eigenvalue problem order deal multi-class scenarios easily extended binary-case generally used subspace dimensions multi-class problems number classes training dataset. effectiveness computational efﬁciency applied successfully many applications face recognition microarray gene expression data analysis. moreover shown compare favorably supervised dimensionality reduction methods experiments however expects instance/label pairs surprisingly prohibitive especially large training data. last decades semi-supervised methods proposed utilize unlabeled data classiﬁcation regression tasks situations limited labeled data transductive co-training correspondently reasonable extend supervised semisupervised method many approaches proposed. methods based transductive learning. words still need instance/label pairs. however many real applications require bag-level labeling object detection event detection paper propose latent fisher discriminant analysis model generalizes fisher model model inspired mi-svm latent multiple instance learning problems hand recently applications image video analysis require kind bag-level label. moreover using latent variable model kind problem shows great improvement object detection hand requirement instance/label pairs training data surprisingly prohibitive especially large training data. bag-level labeling methods good solution problem. mi-svm latent kind discriminative model maximizing posterior probability. model unify discriminative nature fisher linear discriminant data driven gaussian mixture prior training data bayesian framework. combining terms model infer latent variables projection matrix alternative convergence. demonstrate capability musk corel data sets classiﬁcation trecvid dataset keyframe extraction video events linear discriminant analysis popular method dimension reduction classiﬁcation. searches projection matrix simultaneously maximizes between-class dissimilarity minimizes within-class dissimilarity increase class separability typically classiﬁcation applications. attracted increasing amount attention many applications effectiveness computational efﬁciency. belhumeur proposed pca+lda face recognition. chen projects data null space within-class scatter matrix maximizes between-class scatter space deal situation size training data smaller dimensionality feature space. combines ideas above maximizes between-class scatter matrix range space null space within-class scatter matrix separately integrates parts together ﬁnal transformation. also two-stage method divided steps ﬁrst project data range space between-class scatter matrix apply space. deal non-linear scenarios kernel approach applied easily so-called kernel trick extend kernel version called kernel discriminant analysis project data points nonlinearly. recently sparsity induced also proposed however many real-world applications provide labels bag-level object detection event detection. classical supervised learning method requires training dataset consisting instance label pairs construct classiﬁer predict outputs/labels novel inputs. however directly casting semi-supervised method challenging multi-class problems. thus last decades semi-supervised methods become topic. main trend extend supervised semi-supervised method attempts utilize unlabeled data classiﬁcation regression tasks situations limited labeled data. propose novel method called semisupervised discriminant analysis makes labeled unlabeled samples. labeled data points used maximize separability different classes unlabeled data points used estimate intrinsic geometric structure data. propose semi-supervised dimensionality reduction method preserves global structure unlabeled samples addition separating labeled samples different classes other. semi-supervised methods model geometric relationships data points form graph propagate label information labeled data points graph unlabeled data points. another trend prefers extent unsupervised senarios. example ding propose combine k-means clustering lda-km algorithm adaptive dimension reduction. algorithm k-means clustering used generate class labels utilized perform subspace selection. solution latent variable model called latent fisher discriminant analysis complements existing latent variable models popular recent vision literature making possible include latent variables fisher discriminant analysis model. unlike previous latent mi-svm model extend prior data distribution maximize joint probability inferring latent variables. hence method combines latent variable inference dimension reduction uniﬁed bayesian framework. propose lfda model including latent variables fisher discriminant analysis model. represent bags corresponding labels ln}. treated label categorical assumes values ﬁnite e.g. rd×ni means contains instances fisher’s linear discriminant analysis pursue subspace instance vector namely separate classes. words instance searches projection general decided namely suppose projection matrix latent fisher proposes minimize following ratio latent variable weighting parameter regularization term. deﬁnes possible latent values sample case class scatter matrix within class scatter matrix. however dependent categorical variable instance compute case know bag-level labels instance-level labels. minimize need solve given problem chicken problem solved alternating algorithms words solve ﬁxed vice versa alternating strategy. suppose found projection matrix corresponding subspace yn}. instead inferring latent variables instance-level latent propose latent variable inference clustering-level projected space means elements cluster label. assumption reasonable elements cluster close other. hand cluster-level inference speed learning process. extend mixture discriminative analysis model incorporating latent variables instances given class. assume class components gaussians d-dimensional continuous-valued data vector mixture weights centers class posterior probability determined latent discussed later. maximize following equations latent label assignment prior clustering distributions class posterior determined voting subspace pointwise production hadamard product. treat latent fisher discriminant analysis model takes strategy latent model extend lfda combining factors together cluster class maximizing sense considers prior distribution training dataset thus treat joint latent fisher discriminant analysis model lfda prior. nutshell propose formulate discriminative generative methods uniﬁed bayesian framework. comparatively analyze models consequently select cluster maximizes equation class relabel samples positive class rest negative subject then construct training data instance level. obviously subset difference lies every element label note invariant scale vector hence always choose denominator simply reason transform problem minimizing following constrained optimization problem identity matrix rd×d optimal multi-class consists eigenvectors β)†σb corresponding nonzero eigenvalues denotes pseudo-inverse calculated project subspace note subspace preserves labels original space. words corresponding labels element level namely general multi-class uses classify input data. compute using following strategy sample projecting subspace then choose nearest neighbors labels voting cluster class then compute following posterior probability counts fall nearest neighbor label note widely used classiﬁer subspace transformation. thus consider training data vote weight discriminative cluster every class hence discriminative cluster s.t. algorithm. summarize discussion pseudo code. simply update alternative manner accept projection matrix relabeled instances. algorithm always convenge around iterations. learned matrix {λi}c maximizing select representative discriminative frames video datasets nearest neighbor searching. project training data subspace using y=px class using gaussian mixture model partition elements subspace compute σi}; maximize center relabel elements positive cluster update construct subset labels classes; fisher linear discriminant analysis update converge break compute nearest neighbors training data calculate discriminative weight class according given infer latent variable hard assignment ﬁrst term right hand side assigns class. note pln) monotonically increasing function means maximizing posterior likelihood instance maximize hard assignment case thus updating strategy algorithm special case algorithm converge local maximum algorithm. note implement infer latent variable cluster level. words maximize figure example graphical representation class hidden variable observable input projection subspace number total training data class cluster centers determined graphical model method similar model vertical. adding graphical model handle latent variables. maximize posterior probability. moreover instead maximizing also maximize joint probability using bayes rule paper gaussian mixture model approximate prior generative model. argue maximize joint probability reasonable considers discriminative representative property video dataset. give graphical representation model fig. section perform experiments various data sets evaluate proposed techniques compare baseline methods. experiments initialize uniformly weighted projection matrix lda. musk data sets benchmark data sets used virtually previous approaches described detail landmark paper data sets musk musk consist descriptions molecules using multiple low-energy conformations. conformation represented dimensional feature vector derived surface properties. musk contains average approximately conformation molecule musk average conformations bag. corel data consists three different categories instance represented dimension features characterized color texture shape descriptors. data sets positive negative example images. latter randomly drawn pool photos animals. ﬁrst reduce dimension method. parameter setting algorithm applied classiﬁcation). averaged results -fold cross-validation runs summarized table mi-svm baseline. observe lfda jlfda outperform mi-svm musk data sets comparative performance mi-svm others. conduct experiments challenging trecvid dataset. contains events attempting board trick feeding animal landing wedding ceremony working woodworking project. events consist number human actions processes activities interacting people and/or objects different place time. moment take videos events testing remaining videos training. parameters learned representative clusters class semantic frames videos labels. evaluation semantic frames video human-factors analysis—the semantic keyframe extraction problem demands human-in-the-loop evaluation. explain human factors experiment full detail experiment setup. ultimate ﬁndings demonstrate proposed latent prior model capable extraction semantically meaningful keyframes among latent competitive baselines. video representation. videos extract hogd descriptors every frames represent videos using local features apply bag-of-words model using detected points codebook elements. benchmark methods. make benchmark method experiment. take onevs-all strategy train linear mi-svm classiﬁer using light fast linear time kind event. choose frames video margin close margin positive side. frames chosen farthest away margin refer frames closest margin refer svm. also randomly select frames video refer rand experiments. experiment setup highly motivated graduate students served subjects following human-in-the-loop experiments. novel subject annotation-task paradigm underwent training process. authors gave detailed description dataset problem including background deﬁnition purpose. order indicate representative discriminative means event authors showed videos kind event subjects make sure subjects understand semantic keyframes are. training procedure terminated subject’s performance stabilized. take pairwise ranking strategy evaluation. extract frames video different methods lfda jlfda rand) respectively. video image pairs comparison. developed interface using matlab display image pair three options compare image pair time. students taught software; trial requires give ranking left better right choose ’yes’; right better left choose ’no’. image pair same choose ’equal’. subjects informed better means better semantic keyframe. subjects installed software computers conducted image pair comparison independently. order speed annotation process interface randomly sample pairs total image pairs video also subjects random choose videos test dataset. experimental results scores image pair. sampling videos event last annotations videos. means sampling videos subjects almost cover test data table shows win-loss matrix methods counting pairwise comparison results events. shows jlfda lfda always beat three baseline methods. furthermore jlfda better lfda considers prior distribution training data help jlfda representative frames. fig. keyframes extracted jlfda. compared methods basis condorcet voting method. treat ’yes’ ’no’ ’equal’ voters method image pairwise comparison process. ’yes’ cast ballot left method; else ’no’ ballot right method; else nothing methods. fig. shows ballots method event. demonstrates method jlfda always beat methods except dataset. also compared methods based rating system. video ranked methods according ranking system. then counted number methods event. results table show method better others except results based ranking consistent condorcet ranking method fig. wedding ceremony event method consistently outperformed baseline method. believe distinct nature videos video scene context distinguishes four events hence discriminative component methods taking over able outperform fisher discriminant. effect seems likely nature events data proposed method intrinsically. table video ranked methods according ranking system. then counted number method video level event. example total videos jlfda rank ﬁrst videos rand rank ﬁrst videos. higher value better results. demonstrates method capable extracting semantically meaningful keyframes. paper presented latent fisher discriminant analysis model combines latent variable inference dimension reduction uniﬁed framework. ongoing work extend kernel trick model. test method classiﬁcation semantic keyframe extraction problem yield quite competitive results. best knowledge ﬁrst paper study extraction semantically representative discriminative keyframes—most keyframe extraction video summarization focus representation summaries rather jointly representative discriminative ones. conducted thorough ranking-based human factors experiment semantic keyframe extraction challenging trecvid data found proposed methods able consistently outperform competitive baselines.", "year": 2013}