{"title": "A Re-ranking Model for Dependency Parser with Recursive Convolutional  Neural Network", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "In this work, we address the problem to model all the nodes (words or phrases) in a dependency tree with the dense representations. We propose a recursive convolutional neural network (RCNN) architecture to capture syntactic and compositional-semantic representations of phrases and words in a dependency tree. Different with the original recursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a $k$-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets.", "text": "natural language processing tasks syntax semantics distributed representations represent words dense low-dimensional real-valued vectors help address curse dimensionality better generalization discrete representations. dependency parsing chen bansal used dense vectors represent words features found representations complementary traditional discrete feature representation. however methods focus dense representations words features. embeddings pre-trained keep unchanged training phase parsing model cannot optimized speciﬁc tasks. besides also important represent phrases dense vector dependency parsing. since dependency tree also recursive structure intuitive recursive neural network used constituent parsing however recursive neural network process binary combination suitable dependency parsing since parent node child nodes dependency tree. work address problem model nodes dependency tree dense representations. propose recursive convolutional neural network architecture capture syntactic compositional-semantic representations phrases words dependency tree. different original recursive neural network introduce convolution pooling layers model variety compositions feature maps choose informative compositions pooling layers. based rcnn discriminative model re-rank k-best list candidate dependency parsing trees. experiments show rcnn effective improve state-of-the-art dependency parsing english chinese datasets. feature-based discriminative supervised models achieved much progress dependency parsing typically millions discrete binary features generated limited size training data. however ability models restricted design features. number features could large result models complicated practical prone overﬁt training corpus data sparseness. recently many methods proposed learn various distributed representations syntax semantics levels. distributed representations extensively applied many resent level nodes dense representations dependency tree. propose recursive convolutional neural network architecture capture syntactic compositional-semantic representations phrases words. rcnn general architecture deal k-ary parsing tree therefore suitable dependency parsing. node given dependency tree ﬁrst rcnn unit model interactions children choose informative features pooling layer. thus apply rcnn unit recursively vector representation whole dependency tree. output rcnn unit used input rcnn unit parent node outputs single ﬁxed-length vector root node. figure illustrates example rcnn unit represents phrases bike continuous vectors. rcnn general architecture model distributed representations phrase sentence dependency tree. although rcnn used re-ranking dependency parser paper regarded semantic modelling text sequences handle input sequences varying length ﬁxed-length vector. parameters rcnn learned jointly tasks text classiﬁcation. rcnn unit model complicated interactions head word children. combined speciﬁc task rcnn capture useful semantic structure information convolution pooling layers. applied re-ranking model parsing rcnn improve accuracy base parser make accurate parsing decisions. experiments benchmark datasets show rcnn outperforms state-ofthe-art models. idea recursive neural networks natural language processing train deep learning model applied phrases sentences grammatical structure also regarded general structure model sentence. every node tree contexts left right children node combined classical layer. weights layer shared across nodes tree. layer computed node gives representation whole sentence. following binary tree structure assign ﬁxed-length vector word leaves tree combine word phrase pairs recursively create intermediate node vectors length eventually ﬁnal vector representing whole sentence. multiple recursive combination functions explored linear transformation matrices tensor products figure illustrates architecture rnn. binary tree represented form branching triplets triplet denotes parent node children either word non-terminal node tree. tations computed based socher introduced compositional vector grammar uses syntactically untied weights learn syntactic-semantic compositional vector representations. order compute score costa applied recursive neural networks re-rank possible phrase attachments incremental constituency parser. work ﬁrst show rnns capture enough information make correct parsing decisions. menchetti used rnns re-rank different constituency parses. results full sentence parsing re-ranked candidate trees created collins parser dependency grammar widely used syntactic structure directly reﬂects relationships among words sentence. dependency tree nodes terminal node children. therefore standard architecture suitable dependency grammar since based binary tree. section propose general architecture called recursive convolutional neural network borrows idea convolutional neural network deal k-ary tree. rcnn unit ease exposition ﬁrst describe basic unit rcnn. rcnn unit model head word children. different constituent tree dependency tree non-terminal nodes. node consists word tags. node different interaction head node. word embeddings given word dictionary word represented real-valued vector dimensionality vector space. word embeddings stacked embedding matrix rm|w|. word corresponding word embedding embed retrieved lookup table layer. matrix distance embeddings besides word embeddings also distributed vector represent relative distance head word children example shown figure relative distances bike respectively. relative distances also mapped vector dimension vector randomly initialized. distance embedding usual encode distance information neural model proven effectively several tasks. experimental results also show distance embedding gives beneﬁts traditional representation. relative distance encode structure information subtree. given whole dependency tree apply rcnn unit recursively vector representation whole sentence. output rcnn unit used input rcnn unit parent node. thus rcnn used model distributed representations phrase sentence dependency tree applied many tasks. parameters rcnn learned jointly speciﬁc tasks. rcnn unit model complicated interactions head word children. combined speciﬁc task rcnn select useful semantic structure information convolution pooling layers. constituent parsing representation non-terminal node depends children. combination relative simple correctness measured ﬁnal representation non-terminal node however dependency parsing combinations head children important measure correctness subtree. therefore score function computed hidden layers rm×n linear composition matrix depends tags concatenated representation i-th child consists head word embeddings child phrase representation distance embeddings dhci represents concatenation operation. distances dhci relative distance given sentence. then relative distances also mapped m-dimensional vectors. different constituent tree combination consider order position child dependency tree. model tags embeddings directly. since composition matrix varies different pair tags capture different syntactic combinations. example combination adjective noun different verb noun. pooling convolution dynamic depends number children transform ﬁxed length determine useful semantic structure information perform pooling operation rows. re-ranking k-best lists introduced collins charniak johnson used discriminative methods re-rank constituent parsing. dependency parsing sangati used third-order generative model re-ranking k-best lists base parser. hayashi used discriminative forest re-ranking algorithm dependency parsing. re-ranking models achieved substantial raise parsing performances. apply rcnn re-ranking model ﬁrst k-best outputs sentences train base parser. thus train rcnn discriminative optimize re-ranking strategy particular base parser. given training instance max-margin criterion train model. ﬁrst predict dependency tree highest score deﬁne structured margin loss predicted tree given correct tree measured counting number nodes incorrect span proposed tree english dataset ﬁrst evaluate performances rcnn re-ranker development set. figure shows uass different models varying base parser achieves oracle best base parser achieves oracle worst achieves rcnn achieves maximum improvement performance rcnn declines increase still higher baseline reason behind rcnn could require negative samples avoid overﬁtting large. since negative samples limited k-best outputs base parser learnt parameters could easily overﬁts training set. figure shows accuracies tags modiﬁer words largest improvements. re-ranker improve accuracies therefore indirectly result rising well-known coordinating conjunction ppattachment problems. experiments datasets empirically demonstrate effectiveness approach datasets different languages experimental evaluation compare model state-of-the-art methods using unlabeled attachment score metric ignoring punctuation. english english dataset follow standard splits penn treebank using sections training section development section test set. development test sets using automatic tagger training using four-way jackknifing similar chinese chinese dataset follow split penn chinese treeban described sections training sections development sections test set. dependencies converted using pennmalt tool head-ﬁnding rules following gold segmentation tags input. linear-time incremental parser base parser calculate -best parses cell chart. note optimize training settings base parser results slightly improved max-margin criterion train rcnn. finally mixture strategy re-rank -best parses. initialization parameters train wordvec embeddings wikipedia corpus english chinese respectively. combination matrices score vectors random initialization within parameters achieve best unlabeled attachment score development chosen ﬁnal evaluation. figure varying development set. oracle best always choosing best result k-best base parser; oracle worst always choosing worst result k-best base parser; rcnn choosing probable candidate according score rcnn; re-ranker combination rcnn base parser. model table re-ranker achieves maximum improvement test set. system performs slightly better many state-of-the-art systems zhang clark huang sagae outperforms hayashi zuidema also mixture reranking strategy. since result ranker conditioned kbest results base parser also experiment avoid limitation adding oracle k-best candidates. including oracle re-ranker achieve shown last line table ﬁnal experimental results test shown table re-ranker achieves performance test also outperforms previous state-of-theart methods. adding oracle re-ranker achieve shown last line table compared re-ranking model hayashi large number handcrafted traditional methods zhang clark huang sagae distributed representations stenetorp chen chen manning re-rankers hayashi zuidema baseline re-ranker re-ranker table accuracy english test set. baseline result base parser; re-ranker uses mixture strategy -best outputs base parser; re-ranker oracle k-best outputs base parser. performance re-ranking model affected base parser. small divergence dependency trees output list also results overﬁtting training phase. although reranker outperforms state-of-the-art methods also beneﬁt improving quality candidate results. also reported reranking works larger results worse performance. think reason oracle best increases larger oracle worst decrease larger degree. error types increase greatly. re-ranking model requires negative samples avoid overﬁtting. larger number negative samples also needs multiply increase training. however obtain negative samples k-best outputs base parser. experiments also show model achieves signiﬁcant improvements adding oracles output lists base parser. indicates model boosted better candidate results implemented combining rcnn decoding algorithm. model still unsatisfactory. chen manning improved transition-based dependency parsing representing words tags labels dense vectors modeled interactions neural network make predictions actions. methods transition-based parsing model sentence semantic vector space tasks. socher proposed compositional vectors computed dependency tree sentences images common embedding space. however major differences follows. ﬁrst summed child nodes dense vector composed subtree representation vector parent node. contrast model ﬁrst combine parent child choose informative features pooling layer. represent relative position child parent distributed representation useful convolutional layer. figure shows example dtrnn illustrates rcnn represents phrases continuous vectors. speciﬁc re-ranking model zuidema proposed generative re-ranking model inside-outside recursive neural network process trees bottom-up top-down. however iornn works generative estimates probability given tree iornn cannot fully utilize incorrect trees k-best candidate results. besides iornn treats dependency tree sequence regarded generalization simple recurrent neural network unlike iornn proposed rcnn discriminative model optimize re-ranking strategy particular base parser. another difference rcnn computes score tree recursive natural hierarchical structure natural lanproceedmaxent discriminative reranking. ings annual meeting association computational linguistics pages arbor michigan june. association computational linguistics. work address problem represent level nodes dense representations dependency tree. propose recursive convolutional neural network architecture capture syntactic compositional-semantic representations phrases words. rcnn general architecture deal k-ary parsing tree therefore rcnn suitable many tasks minimize effort feature engineering external dependency parser. although rcnn used re-ranking dependency parser paper regarded semantic modelling text sequences handle input sequences varying length ﬁxed-length vector. parameters rcnn learned jointly tasks text classiﬁcation. future research develop integrated parser combine rcnn decoding algorithm. believe integrated parser achieve better performance without limitation base parser. moreover also wish investigate ability model tasks. would like thank anonymous reviewers valuable comments. work partially funded national natural science foundation china national high technology research development program china shanghai science technology development funds shanghai leading academic discipline project references mohit bansal kevin gimpel karen livescu. tailoring continuous word representations dependency parsing. proceedings annual meeting association computational linguistics. danqi chen christopher manning. fast accurate dependency parser using neural networks. proceedings conference empirical methods natural language processing pages wenliang chen zhang zhang. feature embedding dependency parsing. proceedings coling international conference computational linguistics technical papers pages dublin ireland august. dublin city university association computational linguistics. ronan collobert jason weston l´eon bottou michael karlen koray kavukcuoglu pavel kuksa. natural language processing scratch. journal machine learning research fabrizio costa paolo frasconi vincenzo lombardo giovanni soda. towards incremental parsing natural language using recursive neural networks. applied intelligence liang huang kenji sagae. dynamic programming linear-time incremental parsing. proceedings annual meeting association computational linguistics pages association computational linguistics. richard socher cliff chris manning andrew parsing natural scenes natural language recursive neural networks. proceedings international conference machine learning pages richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment treebank. emnlp. zhang stephen clark. tale parsers investigating combining graph-based transition-based dependency proceedings parsing using beam-search. conference empirical methods natural language processing pages association computational linguistics. zhang joakim nivre. transition-based dependency parsing rich non-local features. proceedings annual meeting association computational linguistics human language technologies short papers-volume pages association computational linguistics. socher richard christopher manning andrew improving word representations global context proceedings multiple word prototypes. association computational linguistics pages jeju island korea july. association computational linguistics. phong willem zuidema. inside-outside recursive neural network proceedings model dependency parsing. conference empirical methods natural language processing pages doha qatar october. association computational linguistics. ryan mcdonald koby crammer fernando pereira. online largemargin training dependency parsers. proceedings annual meeting association computational linguistics pages fabrizio costa paolo frasconi massimiliano pontil. wide coverage natural language processing using kernel methods neural networks structured data. pattern recognition letters tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases advances neural compositionality. information processing systems. joakim nivre. incrementality deterministic dependency parsing. proceedings workshop incremental parsing bringing engineering cognition together pages association computational linguistics. nathan ratliff andrew bagnell martin zinkevich. subgradient eleventh inmethods structured prediction. ternational conference artiﬁcial intelligence statistics sangati willem generative zuidema rens bod. prore-ranking model dependency parsing. ceedings international conference parsing technologies pages association computational linguistics.", "year": 2015}