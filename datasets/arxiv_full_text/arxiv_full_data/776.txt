{"title": "Giraffe: Using Deep Reinforcement Learning to Play Chess", "tag": ["cs.AI", "cs.LG", "cs.NE"], "abstract": "This report presents Giraffe, a chess engine that uses self-play to discover all its domain-specific knowledge, with minimal hand-crafted knowledge given by the programmer. Unlike previous attempts using machine learning only to perform parameter-tuning on hand-crafted evaluation functions, Giraffe's learning system also performs automatic feature extraction and pattern recognition. The trained evaluation function performs comparably to the evaluation functions of state-of-the-art chess engines - all of which containing thousands of lines of carefully hand-crafted pattern recognizers, tuned over many years by both computer chess experts and human chess masters. Giraffe is the most successful attempt thus far at using end-to-end machine learning to play chess.", "text": "report presents giraﬀe chess engine uses self-play discover domain-speciﬁc knowledge minimal hand-crafted knowledge given programmer. unlike previous attempts using machine learning perform parametertuning hand-crafted evaluation functions giraﬀe’s learning system also performs automatic feature extraction pattern recognition. trained evaluation function performs comparably evaluation functions state-of-the-art chess engines containing thousands lines carefully hand-crafted pattern recognizers tuned many years computer chess experts human chess masters. giraﬀe successful attempt thus using end-to-end machine learning play chess. also investigated possibility using probability thresholds instead depth shape search trees. depth-based searches form backbone virtually chess engines existence today algorithm become well-established past half century. preliminary comparisons basic implementation probability-based search basic implementation depth-based search showed probability-based approach performs moderately better established approach. also evidences suggesting many successful ad-hoc add-ons depth-based searches generalized switching probability-based search. believe probability-based search fundamentally correct perform minimax. finally designed another machine learning system shape search trees within probability-based search framework. given position system estimates probability moves best move without looking ahead. system highly eﬀective actual best move within ranked moves time average approximately legal moves position. also resulted signiﬁcant increase playing strength. particular gerd isenberg maintaining writing chess programming wiki comprehensive reference everything computer chess graham banks testing giraﬀe wide variety opponents. also thankful hundreds people played thousands games giraﬀe internet chess club including grandmaster tahir vakhidov international master alejandro boﬁll alejandro montalvo alex domont ulrich schulze william hartston fide master ’renium’ ’solsjenitsyn’ ’arnav’ ’wmm’ ’doctornick’. observing games allowed discover many potential improvements giraﬀe. project would much less successful without imperial college high performance computing service provided immense computing power required project. chess game requires much creativity sophisticated reasoning thought something computers ever able frequently listed alongside activities like poetry writing painting examples tasks performed humans. writing poetry remained diﬃcult computers much success building chess-playing computers. ibm’s deep blue defeated reigning world chess champion garry kasparov standard tournament rules ﬁrst time history chess. ensuing decades computer hardware research advanced state-of-art chess-playing computers point even best humans today realistic chance defeating modern chess engine running smartphone. however interesting note computers play chess diﬀerent humans play. humans computers search ahead predict game humans much selective branches game tree explore. computers hand rely brute force explore many continuations possible even ones immediately thrown skilled human. sense humans play chess much computationally eﬃcient using garry kasparov deep blue example kasparov could searching positions second deep blue supercomputer custom chess processors searched million positions second play approximately equal strength human searching positions second strong computer searching million positions second? possible build even stronger chess computers today making computationally eﬃcient? questions project investigates. many attempts making chess engines selective without overlooking important continuations proven diﬃcult problem. diﬃcult come reliable rules decide branches explore branches prune. humans applying complex rules many exceptions. people learn rules playing observing thousands games able convert knowledge concrete rules computers work situations. instead conventional method teaching computers play chess giving hardcoded rules project attempt machine learning ﬁgure play chess self-play derive rules games. using multiple deep artiﬁcial neural networks trained temporal-diﬀerence reinforcement learning framework machine learning assist engine making decisions places thanks complex well-deﬁned nature game chess extensively-studied problem artiﬁcial intelligence. thousands papers written aspects computer chess since least hundreds chess engines written researchers professionals amateurs years chess problem follows given chess position want move legal moves position maximizes chance winning game. means also model opponent chooses moves. high level chess usually model opponent assume select moves never make mistakes. assumption valid games much weaker players where example possible moves move results better position unless opponent sees plays extremely complicated reply risky count opponents making mistakes almost chess engines implement conservative model assumes perfect opponents limit engine’s ability. game theory perspective strategy leads nash equilibrium player adopts strategy opponent better adopting strategy. alternative formulation chess problem often used practice. formulation follows given chess position want assign score corresponds chance winning side move. requirement score monotonically increasing respect chance winning. exact relationship score winning probability usually known important. scores usually centered around symmetry computational reasons. case since chess zero-sum game score side simply negated score opposite side given position. although diﬀer implementation almost chess engines existence today implement largely algorithms. based idea ﬁxed-depth minimax algorithm ﬁrst developed john neumann adapted problem chess claude shannon algorithm works theory also practice simpler games like tic-tac-toe however searches entire game tree impractical games like chess average branching factor approximately average game length plies. search tree size chess estimated hundred orders magnitudes higher computationally feasible using modern computers. therefore chess engine must decide parts game tree explore. common approach ﬁxed-depth search artiﬁcially limit ahead look sub-tree want search call static evaluation function assigns score position analyzing statically simple evaluation function simply pieces sides multiplied constant evaluation function important major areas investigation project. algorithm above branches distance root dangerous because example branch last move happens score position pretty good position since pawn. however don’t pawn defended another pawn opponent take queen next move. actually position problem cannot solved simply increasing depth limit matter deep searching always horizon. solution quiescent search. idea depth instead calling evaluate returning enter special search mode expands certain types moves call evaluate quiet relatively stable position. types moves include q-search matter debate. engine authors agree winning captures included. agree captures also included. many believe queen promotions also included. believe checks check evasions also included. believe pawn moves rank also included. tradeoﬀ made include many moves q-searches become large won’t able search many plies normal search. include moves suﬀer reduced forms horizon eﬀect. q-searches usually depth-limited instead rely tree terminating. trees always terminate since number possible captures usually limited tend decrease captures made. without introducing heuristics loss information algorithm optimized introducing window call minimax. idea true score lowerbound means caller already better move therefore doesn’t care exact value node conversely true score higher upperbound means caller also doesn’t care exact value fact higher upperbound. seem counter-intuitive ﬁrst upperbound reason chess zero-sum upperbound lowerbound switch places search deeper. optimization called pruning lower upper bounds. since horrible choices variable names refer lowerbound upperbound rest paper still refer algorithm consistency existing literature. standard minimax move ordering irrelevant nodes visited exactly once. pruning explore nodes potentially useful stop searching node soon prove result outside window. means always expand best nodes ﬁrst examine many nodes sub-optimal ordering. worst case degenerates minimax. hand proven best case best moves always searched ﬁrst reduces eﬀective branching factor square root original value would allow search twice amount time obviously impossible always optimal ordering since generate optimal ordering given position would need search all. however ensuring move ordering close optimal cases using heuristics make search eﬃcient. depth-limited minimax pruning q-search form backbone virtually existing chess engines. many optimizations done done improve search. optimizations covered paper directly relevant project. however many implemented chess engine created project. mentioned section evaluation function important part chess engine almost improvements playing strength among engines nowadays come improvements respective evaluation functions. project develop evaluation function based machine learning approach. however that take look evaluation function stockﬁsh open source chess engine currently strongest chess engine world. examining existing state-of-art evaluation function help design eﬀective feature representation machine learning eﬀort. plementation squares controlled opponent pieces lesser value counted. example bishop mobility squares controlled enemy pawns included queen mobility squares controlled bishops knights rooks included. piece type diﬀerent mobility bonuses. quite complicated function hand-coded knowledge. engines don’t evaluation functions nearly extensive diﬃcult tune high number parameters hand. many attempts using machine learning solve chess endgames. endgames usually deﬁned positions pieces left. situations much simpler average chess positions relatively simple rules optimal play often derived. attempts area genetic programming inductive learning unfortunately approaches scale much complicated positions seen midgames openings simple rules optimal play exist. typical chess evaluation function hundreds parameters must tuned together. time-consuming tuning hand attempted either tune parameters automatically using machine learning hard-coded features input machine learning systems generate ﬁnal evaluation scores. neurochess attempt. falcon project attempt tune evaluation parameters using genetic programming help mentor much complicated evaluation function. possible achieve good performance using method fact features hand-picked means systems limited creativity designers recent attempt meep project veness whose evaluation function linear combination hand-designed features weights trained using reinforcement learning. machine learning approaches require evaluate performance models. results gameplay direct measure performance models often practical large number games required statistically signiﬁcant results small changes computationally ineﬃcient perform calculations required play game result less bits information successful approach thus temporal-diﬀerence reinforcement learning system tested ability predict outcome game position ability predict evaluation near future therefore achieving temporal consistency. long positions ﬁxed score models higher predictive power would better temporal consistency approach several advantages using game results important evaluations used make moves game receive diﬀerent error signals. chess possible game lost blunder even moves optimal. system based game results moves game receive high error signal whereas temporal-diﬀerence learning blunder high error signal. signiﬁcantly improves signal-to-noise ratio error signals. famous study using temporal-diﬀerence reinforcement learning play zero-sum games probably tesauro’s backgammon program td-gammon system trained random initialization using temporal-diﬀerence reinforcement learning self-play. tdgammon attained master-level performance playing million training games greatly surpassing previous programs. td-gammon uses neural networks board input well hand-designed features board representation. search involved. given position td-gammon simply picks move results position best evaluation score. similar approach used thrun neurochess program which used chess’s search routine wins approximately games chess. author attributed disappointing performance ineﬃciency implementation neural network evaluation implemented neurochess takes orders magnitudes longer evaluating optimized linear evaluation function baxter adapted temporal-diﬀerence learning algorithm used conjunction minimax searches called tdleaf. idea since minimax ﬁnal score returned always evaluation score leaf position principal variation eﬃcient perform evaluation leaf nodes instead root nodes. enjoyed enormous success using approach knightcap program achieving fide international master strength games played internet chess server. also able easily defeat chess engines constrained depths reached knightcap proving knightcap superior evaluation function. however much slower evaluation function knightcap weaker engines time tournament conditions. authors suggested diﬃcult compensate lack depth using superior evaluation since tactics make chess highly non-smooth feature-space makes machine learning diﬃcult. although explored study also postulate without signiﬁcantly simplifying evaluation function best solve tactical myopia ﬁnding eﬀective algorithm search would potential greater improvement another main topics investigation project. beyond weight tuning hand-designed features actually learned system perform feature extraction need powerful highly non-linear universal function approximator tuned approximate complex functions like evaluation function chess. machine learning models perform well approximating functions ﬁrst second order. capable building hierarchies knowledge model complex high order features. result order approximate complex functions inputoutput mapping inherently hierarchical models would made extremely large. extremely large models require high computational power also require large training sets prevent overﬁtting high degrees freedom model. unfortunately computational power availability training examples usually limiting factors practical machine learning applications. example problems requiring hierarchical feature extraction consider problem classifying whether image contains given pixel intensities image. perform classiﬁcation directly pixels system would essentially evolve memorize images cars test input images similarity images. besides problems mentioned above models also unlikely generalize well given blue yellow cars training it’s unlikely able correctly identify green cars. eﬃcient accurate classify images hierarchical approach ﬁrst layer would identify level features like gradients corners edges. second layer output ﬁrst layer identify shapes. finally location shapes used classify objects image. humans naturally it’s something machine learning systems struggled recently. class algorithms capable performing hierarchical knowledge extraction popularized hinton al.. models extended perform deep learning vast majority current applications artiﬁcial neural networks. past years deep learning refreshed state-of-the-art results wide range problems often greatly surpassing previous results. example krizhevsky trained deep network popular imagenet competition achieving top- error rate compared previous best also used handwriting recognition deep networks approaching human performance error another example google’s deepmind recently published results training deep networks play seven diﬀerent arcade games given pixel intensities input increasing score displayed objective. networks trained able surpass performance previous game-speciﬁc seven games exceeded human expert performance three besides computer vision control tasks deep learning also successfully applied natural language processing sentiment analysis audio analysis ﬁelds knowledge inherently hierarchical. project aims create chess engine based machine learning using algorithms already proven successful past attempts tdleaf well algorithms developed project. ﬁrst part project essentially modern iteration tdleaf experiment learning evaluation functions taking advantage discoveries advances neural networks past years well much higher computational power available today. create system perform high level feature extraction automatically order constrained human creativity something practical computational power constraints. second part project develop novel probability-based search framework enable highly selective searches using neural networks provide guidance amount time spend subtree given position. approach highly speculative challenges decades-worth established research using distance-from-root primary factor deciding subtrees explore minimax searches. already well-established strategic extensions reductions pruning huge beneﬁts performance chess engines however existing heuristics making decisions mechanical quite crude must done conservatively miss tactical details. hoped project bring ﬁelds machine learning game theory together intimate ever done. chess engine giraﬀe written used basis modiﬁcations. popular techniques computer chess implemented chess engine. basic linear evaluation function takes account material piece-square tables mobility king safety plays slightly fide candidate master strength. search routine also fairly standard though aggressive heuristic pruning techniques intentionally omitted want heuristic decisions made learned systems. scratch instead using existing framework since needs extensively customized application features restricted connectivity using existing framework would save work would involve making extensive modiﬁcations unfamiliar codebase. implementation done fully vectorized using eigen high performance linear algebra library multi-threaded achieve performance comparable optimized machine learning libraries. state-of-art optimization algorithms also implemented mini-batch stochastic gradient descent nesterov’s accelerated momentum adagrad adaptive subgradient method improved version adagrad method known adadelta adagrad adadelta maintain individual learning rates parameter example nodes rarely activated higher learning rate take full advantage activations nodes often activated lower learning rate converge better minimums. important chess many patterns rarely present. experiments agreed results presented adagrad adadelta papers fastest convergence highest quality minimums adadelta. ﬁrst problem tackle evaluation function systems require already good evaluation function. mentioned section evaluation function take position input estimate probability winning players without searching forward. function called leaf nodes searches terminate search time constraint. feature representation critical part project. features enough level general enough chess knowledge still discovered learning. hand well designed board representation signiﬁcantly improve performance system making useful features easier learn. goal representation succinct possible relatively untangled smooth feature space. example principle neural nets learn rules chess ﬁgure inﬂuence sliding pieces ends ﬁrst piece along direction much eﬃcient give information part board representation. neural networks work well feature representation needs relatively smooth input space mapped output space. positions close together feature space should much possible similar evaluations. means many intuitive naive representations likely work well. example many previous attempts using neural networks chess represent positions bitmaps squares represented using binary features indicating whether piece piece types exist square not. unlikely work well -dimensional space positions close together necessarily similar evaluation. illustration figure below. case position left similar evaluation position middle bishops region board. position right diﬀerent evaluation bishop somewhere else entirely. bitmap feature representation positions distance feature space. means network hard time generalizing advantages pieces speciﬁc locations general regions board. much better representation encode position list pieces coordinates. positions close together feature space would usually similar evaluation. since feature vector needs length regardless many pieces board slot system. ﬁrst slots reserved kings followed slots queens four slots knights four slots bishops four slots rooks ﬁnally sixteen slots pawns. addition coordinates slot also encodes presence absence piece well additional information piece whether defended move along direction case promotions possible starting number pieces piece types. rare cases extra pieces encoded slots system though piece counts encoded separately scored existence evaluated board representations training neural networks predict output stockﬁsh’s evaluation function supervised fashion given million positions input board representation evaluation. positions uniformly randomly sampled databases high quality games either human grandmasters computers. include games phases situations labeled stockﬁsh. hoped board representation allows network eﬃciently learn stockﬁsh’s evaluation function ﬂexible enough model least kind chess knowledge right hopefully unknown knowledge similar format future. challenge achieve keeping board representation general possible limit ability learn things. kings slot queens slots rooks/bishops/knights slots pawns. slot normalized coordinate normalized coordinate whether piece present absent lowest valued attacker defender piece. pieces assigned slots based piece lists make bulk feature representation. allow network eﬃciently learn things like centralisation knights bishops rooks opponent’s pawn rank centralisation king games advancement pawns general. also allow network learn importance mobility diﬀerent pieces diﬀerent phases game well concepts king safety based distances diﬀerent types attacking pieces opponent’s king. turned features important features many important concepts represented linear combinations pieces’ coordinates linear combinations pieces’ coordinates thresholds. fact system performs reasonable level features. side move ﬂags added network learn concept tempo chess fact else equal it’s better moving side almost situations available move better passing always true. particular class positions known zugzwang legal moves worsen situation moving side moving side still make move passing illegal chess. value tempo also depends game phase material conﬁgurations well pawn structure. left hands neural network discover. material conﬁguration section almost entirely redundant since information derived existence ﬂags piece slots. however still useful cases promotions piece slots. piece count features least given bonus existence. also numbers allow network easily discover concepts like piece synergies material combinations aﬀect importance features example moving king towards centre often good game dangerous opening middle game opponent still many pieces board. attack defend maps allow network learn concepts related board control. diﬃcult network derive piece lists piece lists piece-centric square-centric. maps provided even though information theoretically computable features. feature side move white long castle white short castle black long castle black short castle white queens white rooks white bishops white knights white pawns black queens black rooks black bishops black knights black pawns white queen exists white queen position white rook exists white rook position white rook exists white rook position white bishop exists white bishop position white bishop exists white bishop position evaluator network -layer network hidden nodes rectiﬁed linear activation modern choice much eﬃcient traditional hyperbolic tangent logistic activation functions especially networks hidden layers. constrain output output node uses hyperbolic tangent activation. feature representation includes features diﬀerent modalities often beneﬁcial higher abstract layers. levels data diﬀerent modalities cannot mixed usefully extra connections oﬀer beneﬁt case three modalities piece-centric square-centric position-centric. ﬁrst layers features diﬀerent modalities kept separate. ﬁnal layers fully connected capture interactions high level concepts derived features diﬀerent modalities. figure illustrates network architecture limited connectivity scheme arrow means nodes ﬁrst group connected nodes second group. games also want variety that system learn play highly unequal positions example. important although don’t frequently appear real games encountered internal nodes searches time engine must also know evaluate properly. several methods used previous attempts applying machine learning chess. falcon -games database grandmaster games used position drawn game clever trick applied ensure variety changing every position white-to-move. gives positions material imbalances would typically appear grandmaster-level games knightcap online play free internet chess server used engine played games training. authors also claimed self-play possible limited variations self-play project used another approach hybrid existing approaches. first collected million positions randomly database computer games however instead using position directly introduce imbalance unusual positions randomly applying legal move position using training. positions used starting points self-play. approach satisﬁes three criteria mentioned positions slightly diﬀerent actual gameplay positions large variety positions random moves total million positions. since performing reinforcement learning positions need labeled. although td-leaf train system random initialization principle complex problem like chess random initialization would lead extremely long training times. therefore simple evaluation function containing basic material knowledge bootstrap training process. bootstrapping process takes seconds puts neural network point parameter space much closer good minimum. also possible bootstrap much accurate function chess engine’s evaluation goal project investigate much chess knowledge learned entirely self-discovery approach taken. train network need generate error signals. possible another stronger chess engine mentor generate ground-truth labels project aiming create system learn play chess external help little built-in chess knowledge possible. mentioned section popular temporal-diﬀerence reinforcement learning trying make evaluation function better predictor evaluation later time achieving temporal consistency. project modiﬁed version algorithm known td-leaf allows faster learning exploiting fact local gradient minimax simply gradient evaluation function leaf node search implementation td-leaf training iteration randomly select positions training engine play moves. results searches recorded. moves played look position score changes moves compute error starting position adding changes weighted starting position. errors scaled ﬁxed many moves away error starting position. rate. however know move actually caused inconsistency. therefore make assumption move immediately score change likely caused followed move that etc. allows evaluation function learn model patterns long term consequences prioritize patterns shorter term consequences. total errors positions obtained standard back-propagation done derive gradient loss function respect connection weights biases neural network. loss chosen common loss td-leaf generates errors many outliers would completely dominate loss. gradients used train network using stochastic gradient descent adadelta update rules adadelta recently discovered optimization technique performs better conventional gradient descent algorithms. many diﬀerent optimizations implemented project momentum nesterov’s accelerated gradient adaptive gradient adadelta found highest convergence rate converges best minimums. primary diﬀerence adadelta older techniques maintains separate learning rates weight learning rates changed iteration based direction gradients. allows weights neurons rarely activated retain higher learning rates make best limited activations allowing weights neurons often activated decrease time converge better minimums similar idea much older resilient back-propagation algorithm work stochastic setting gradients frequently changing signs. test positional understanding evaluator engine strategic test suite strategic test suite collection positions split themes positions each. group positions tests engine’s understanding strategic idea. example theme tests understanding control open ﬁles another tests understanding bishop knight’s values change relative diﬀerent situations another tests understanding centre control. unlike test suites positions deep tactical lines engine making especially suitable evaluating positional understanding engine. positions test suite list solution moves associated scores. best solutions always score solutions scores test scores engine’s moves position added score figure shows result running test periodically training progresses. materialbootstrap achieves score approximately training progresses gradually improved approximately peaks proving managed gain tremendous amount positional understanding. comparison also selection engines test suite. since want compare quality evaluation function purely basis quality outputs test giraﬀe diﬀerent time limits well able search many nodes engines. clear giraﬀe’s evaluation function least comparable positional understanding compared evaluation functions engines world remarkable evaluation functions carefully hand-designed behemoths hundreds parameters tuned manually automatically several years many worked human grandmasters. test suite likely under-estimates positional understanding giraﬀe compared engines themes tested test suite generally well-understood concepts computer chess implemented many engines since test suite famous likely least engines tuned speciﬁcally test suite. since giraﬀe discovered evaluation features self-play likely knows patterns studied humans hence included test suite. evaluation function self-play including automatic feature extraction starting minimal hand-coded knowledge achieving comparable performance state-of-the-art expert-designed evaluation functions. introduce work training neural networks move ordering time allocation describe giraﬀe performs searches. main topic investigation project signiﬁcant departure searches done conventional chess engines. provide small beneﬁt playing strength itself main beneﬁt comes fact provides much theoretically-sound foundation build machine learning system mentioned section multiple ways deﬁne search problem equivalent. case goal search predict position reached sides play theoretically-optimal game sequence moves leading position. list moves make theoretically-optimal game start position search ﬁrst move list obviously move play given assumption opponent least skilled are. apply goal recursively reach possible ending positions chess. deﬁne theoretical principal variation position sequence theoreticallyoptimal moves take position consideration game positions theoretical least one. purpose discussion assume exactly one. assumption unavoidable general know many theoretical without searching entire search space practical general. therefore redeﬁne goal search want much theoretical possible. given cases time search entire search space compute actual theoretical decide subset theoretical complete search tree search. neumann introduced minimax algorithm focused cases entire game tree explored important achievement applicable games chess game tree large completely explored. norbert’s rendition minimax ﬁrst known application incomplete minimax uses heuristic function leaves search tree used distance-to-root simple metric determining part subtree search. giraﬀe introduce deﬁning search tree boundary using probability threshold instead depth. instead searching nodes less moves root position search nodes greater chance part theoretical root position. every position inside search divide probability estimate node probability estimates children node. probability estimates children node must probability estimate parent. probability cases probability-limited search depth-limited search search approximately sub-tree. cases branching factor approximately constant throughout search node probabilities decrease uniform rate search terminate approximately uniform depth. however perform diﬀerently positions subtrees diﬀerent branching factors. extreme example check check given opponent usually evasion moves available. another example branch involves exchanging high mobility pieces instances branches exchange would much lower branching factor. depth-limited searches spend much time subtrees high branching factor even though indication branch likely going taken. extreme example consider situation figure below white option exchanging queens exchanging queens. depth-limited search would spend almost time searching branch queens still board even though could searched much deeper branch exchange using tiny fraction time it’s spending no-exchange probability-limited search hand would spend roughly equal time branches reach much higher depth queen-less branch. probabilistic perspective behaviour makes sense node depth no-exchange branch much less likely part theoretical nodes depth exchange branch simply branching factor exchange branch much lower. points corresponding score split) stronger naive implementation depthlimited search. signiﬁcant diﬀerence considering fact diﬀerence vast majority positions. comparison done advanced implementation probability-limited search depth-limited search time constraint. however advanced search optimizations used depth-limited searches also implemented probability-limited search. interestingly depth-limited searches found extending search cases checks situations single reply beneﬁcial. techniques make depth-limited search perform like probabilitylimited one. probability-limited search seems generalisation techniques. previous section explained instead searching branches equal depth search branches equal probability threshold instead. previously allocated probability evenly children that well information move then search position would evaluate legal move normalize produce probability distribution multiply probability parent produce probability child. piece type type piece square from square move square square move promotion type promotions type pawn promoting rank move compare legal moves? probability move best certain situation depends legal moves position. example four-way likely best moves position move signiﬁcantly worse four likely need considered even four moves turn poor chances good least would still better move. hand better move move would signiﬁcant probability best move matter much better ﬁrst move simply probability estimation process perfect. however chicken-and-egg situation. ranking moves compute probabilities want probabilities produce ranking? solution two-pass process. ﬁrst pass moves evaluated best move. rank moves based probabilities evaluate moves based ranking. depth-limited searches insight often implemented arbitrarily reducing search depth engine goes move list. technique known late move reduction engines even searching moves long move lists. crude technique works quite well practice. giraﬀe trying achieve civilised way. network architecture probability estimating network similar architecture evaluation network described section diﬀerence output node logistic activation instead hyperbolic tangent activation. shifts output range computational reasons also rectiﬁed linear activation hidden nodes separately-connected layers features three diﬀerent modalities. please section detailed explanation connectivity scheme. training probability estimation network also need corpus unlabeled positions. however corpus used evaluation network training distributions diﬀerent. training evaluation network using td-leaf need training positions representative root positions appear actual games. means lack positions heavy material imbalance example positions normally appear games. however positions appear often internal nodes search trees. since probability estimation network used internal nodes want training distributed way. generate training perform time-limited searches root positions training evaluation network training randomly sample positions encountered internal nodes searches. unlike case evaluation network need iterative temporal-diﬀerence learning case already trained evaluator already best move training position simply searching using static uniform-probability estimator. labeled positions generate training gradient descent combining position legal moves position labeled binary training target specifying whether move best move not. trained network turned eﬀective. table shows predictive power network. time actual best move turned predicted best move. time actual best move turned among predicted moves. table shows result diﬀerent testing positions best move winning capture ﬁltered out. even among diﬃcult quiet positions correct best move predicted time best move among predicted moves time. terms actual playing strength negative eﬀect search speed excluded giraﬀe much stronger neural network probability estimator. games diﬀerence however neural network slow used everywhere search tree. used near leaves spend time allocating probability nodes actually searching nodes. therefore used nodes nodes search them. beauty approach generality. explored project time constraint likely approach easily ported zero-sum turn-based board games achieve state-of-art performance quickly especially games decades intense research creating strong player. addition machine learning aspects project introduced tested alternative variant decades-old minimax algorithm apply probability boundaries instead depth boundaries limit search tree. showed approach least comparable quite possibly superior approach past half century. also showed formulation minimax works especially well probability-based machine learning approach. eﬃciency always major consideration switching expert system machine learning approach since expert systems usually eﬃcient evaluate generic models. especially important applications like chess engine able search nodes quickly strongly correlated playing strength. earlier attempts applying neural network chess thwarted large performance penalties. example thrun reported diﬀerence orders magnitude implementation neural-network based chess engine state chess engine time giraﬀe’s optimized implementation neural network combined much higher vector arithmetics throughput modern processors eﬀective caching allows search speed less order magnitude slower best modern chess engines thus making quite competitive many chess engines gameplay without need time handicap. enhancements giraﬀe able play level fide international master modern mainstream still long away engines today play super-grandmaster levels able defeat many lower-tier engines search order magnitude faster. original goals project create chess engine less reliant brute-force contemporaries goal certainly achieved. unlike chess engines existence today giraﬀe derives playing strength able ahead able evaluate tricky positions accurately understanding complicated positional concepts intuitive humans elusive chess engines long time. especially important opening game phases plays exceptionally well. time management important part chess playing. diﬀerent tournaments diﬀerent time controls however require players divide allocated time decide much time every move. many considerations complexity position expected remaining length game conﬁdence currently chosen move giraﬀe already caches neural network outputs. however rate cache exact positions rarely evaluated twice. however positions encountered search tree similar clever designs network connectivity possible cache intermediate activations within networks. recent discoveries neural network training caruana suggest possible compress large neural networks using mentors train smaller networks another potential speed giraﬀe. static evaluation function called evaluate position needs return accurate evaluation within bounds supplied caller. pruning works true score going outside bounds caller needs informed going outside bounds. therefore beneﬁcial able quickly estimate upperlower-bound score position since possible score range entirely window need perform expensive full evaluation. possible perform bound estimation training network outputs bounds. outputs trained predict evaluator output diﬀerent error functions. example asymmetrical cost functions penalizes positive errors much negative errors used train output lower-bound vice versa. although giraﬀe already much smaller search trees engines similar strength still many orders magnitudes larger humans achieve. causes diﬀerence accurately positions evaluated. positions evaluated accurately deeper wider searches required compensate. closing primary focus project. another reason humans’ high search eﬃciency concept position similarity. humans often decide moves eﬀectively equivalent situations avoid searching them. dramatically reduces average branching factor search trees. possible implement using machine learning neural network takes positions inputs outputs sequences numbers work signatures position. unsupervised learning used signatures gauge degrees similarity positions. however unclear networks trained. able accurately predict equivalence positions moves whether using machine learning techniques likely going lead another major milestone achieving eﬃcient searches. rating system mathematical model assigns ratings players based game results. system ensures stronger player would receive higher rating weaker player. ratings comparable within pool players. rating diﬀerence corresponds score split stronger player expecting receive points. fide scale beginner knows rules game would rating serious level tournament player would rating magnus carlsen reigning world chess champion rating best chess engines estimated ratings however enough games computers humans oﬃcial fide ratings establish accurate relationship human computer rating pools.", "year": 2015}