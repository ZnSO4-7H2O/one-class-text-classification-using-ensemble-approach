{"title": "Accelerated Primal-Dual Policy Optimization for Safe Reinforcement  Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Constrained Markov Decision Process (CMDP) is a natural framework for reinforcement learning tasks with safety constraints, where agents learn a policy that maximizes the long-term reward while satisfying the constraints on the long-term cost. A canonical approach for solving CMDPs is the primal-dual method which updates parameters in primal and dual spaces in turn. Existing methods for CMDPs only use on-policy data for dual updates, which results in sample inefficiency and slow convergence. In this paper, we propose a policy search method for CMDPs called Accelerated Primal-Dual Optimization (APDO), which incorporates an off-policy trained dual variable in the dual update procedure while updating the policy in primal space with on-policy likelihood ratio gradient. Experimental results on a simulated robot locomotion task show that APDO achieves better sample efficiency and faster convergence than state-of-the-art approaches for CMDPs.", "text": "constrained markov decision process natural framework reinforcement learning tasks safety constraints agents learn policy maximizes long-term reward satisfying constraints long-term cost. canonical approach solving cmdps primal-dual method updates parameters primal dual spaces turn. existing methods cmdps on-policy data dual updates results sample inefﬁciency slow convergence. paper propose policy search method cmdps called accelerated primal-dual optimization incorporates offpolicy trained dual variable dual update procedure updating policy primal space on-policy likelihood ratio gradient. experimental results simulated robot locomotion task show apdo achieves better sample efﬁciency faster convergence state-of-the-art approaches cmdps. reinforcement learning agents learn trial error unknown environment. majority algorithms allow agents freely explore environment exploit actions might improve reward. however actions lead high rewards usually come high risks. safety-critical environment important enforce safety algorithm natural enforce safety incorporate constraints. standard formulation safety constraints constrained markov decision process framework agents need maximize long-term reward satisfying constraints long-term cost. applications cmdps include windmill control need maximize average reward bounding long-term wear-and-tear cost critical components another important example communication network control need maximize network utility bounding long-term arrival rate long-term service rate order maintain network stability optimal policies ﬁnite cmdps known models obtained linear programming cannot scale high-dimensional continuous control tasks curse dimensionality. recently algorithms work high-dimensional cmdps based advances policy search algorithms particular constrained policy search algorithms enjoy state-of-the-art performance cmdps primal-dual optimization constrained policy optimization based lagrangian relaxation updates parameters primal dual spaces turn. speciﬁcally primal policy update uses policy gradient descent dual variable update uses dual gradient ascent. comparison differs dual update procedure dual variable obtained scratch solving carefully-designed optimization problem iteration order enforce safety constraints throughout training. besides exist methods solving cmdps approaches usually computationally intensive apply speciﬁc cmdp models domains. notable feature existing constrained policy search approaches on-policy samples ensures information used dual updates unbiased leads stable performance improvement. however on-policy dual update sampleinefﬁcient since historical samples discarded. moreover on-policy nature dual updates incremental suffer slow convergence since batch on-policy samples obtained dual update made. paper propose policy search method cmdps called accelerated primal-dual optimization incorporates off-policy trained dual variable dual update procedure updating policy primal space on-policy likelihood ratio gradient. speciﬁcally apdo similar except perform one-time adjustment dual variable nearly optimal dual variable trained off-policy data certain number iterations. one-time adjustment process incurs negligible amortized overhead long term greatly improves sample efﬁciency convergence rate exisiting methods. demonstrate effectiveness apdo simulated robot locomotion task agent must satisfy constraints motivated safety. experimental results show apdo achieves better sample efﬁciency faster convergence state-of-the-art approaches cmdps another line work considers merging on-policy off-policy policy gradient updates improve sample efﬁciency. examples approaches include q-prop etc. approaches designed unconstrained mdps applied primal policy update. contrast apdo leverages off-policy samples dual updates complementary efforts merging on-policy off-policy policy gradients. constrained markov decision process markov decision process represented tuple states actions reward function transition probability function transition probability state state given action initial state distribution. stationary policy corresponds mapping states probability distribution actions. speciﬁcally probability selecting action state stationary policies denoted paper search policy within parametrized stationary policy class write policy emphasize dependence parameter long-term discounted reward policy γtr] discount factor denotes trajectory means distribution trajectories determined policy i.e. st+∼p constrained markov decision process augmented constraints long-term discounted costs. speciﬁcally augment ordinary cost functions c··· cost function ×a×s mapping transition tuples costs. longt= γtci] corresponding limit cmdp select policy maximizes long-term reward satisfying constraints long-term costs i.e. solve unconstrained minimax problem canonical approach iterative primaldual method iteration update primal policy dual variable turn. primal-dual update procedures iteration follows perform policy gradient update αk∇θ λ))|θ=θk step size. policy gradient could on-policy likelihood ratio policy gradient trpo off-policy deterministic policy gradient perform dual update πk). existing methods cmdps differ choice dual update procedure example di)]+ step size uses simple dual gradient ascent max{ projection onto dual space comparison derives dual variable solving optimization problem scratch order enforce constraints every iteration. however dual update procedures used existing methods incremental on-policy samples resulting sample inefﬁciency slow convergence optimal primal-dual solution paper propose incorporate off-policy trained dual variable dual update procedure order improve sample efﬁciency speed search optimal dual variable algorithm called accelerated primal-dual optimization described algorithm apdo similar iterations dual variable updated according simple dual gradient ascent innovation apdo one-time dual adjustment off-policy trained dual variable λoff kadj iterations off-policy trained λoff obtained running off-policy algorithm cmdps historical data stored replay buffer. provide primal-dual version ddpg algorithm supplementary material training λoff. although off-policy trained dual variable λoff could biased provides nearly optimal point tuning dual variable using on-policy data. improvement sample efﬁciency apdo fact off-policy training repeatedly exploit historical data on-policy update uses sample once; acceleration effect apdo fact off-policy training directly solves optimal dual variable ofﬂine thus avoiding slow on-policy dual update existing approaches dual update taken large batch samples obtained. note adjustment epoch kadj important parameter apdo. using small kadj avoids slow incremental dual update early dual estimate λoff could highly biased inaccurate insufﬁcient amount data. hand using larger kadj provides accurate dual estimate expense delayed adjustment. evaluate apdo state-of-the-art algorithms solving cmdps simple point-gather control task mujoco additional safety constraint used experiments implemented rllab detailed task description experiment parameters provided supplementary material. particular apdo adjustment epoch kadj additional experimental results regarding effect kadj also given supplementary material. figure shows learning curves apdo cost constraints. observed fugure apdo enforced constraints successfully limit value approximately speed did. importantly apdo generally outperforms reward performance without compromising constraint stabilization thus achieving better sample efﬁciency. example takes epochs achieve average reward satisfying safety constraint. comparison apdo takes epochs achieve point corresponds improvement sample efﬁciency task. addition fails enforce sample trajectories current policy sampled data update primal policy on-policy likelihood ratio gradient method using sampled on-policy trajectories current dual variable update dual variable dual gradient ascent safety constraint ﬁrst epochs slow convergence. using larger step size help speed convergence case over-correct response constraint violations behave conservatively. provide additional discussions choice stepsize apdo supplementary material. figure illustrates learning trajectory dual variable apdo apdo converges optimal dual variable signiﬁcantly faster pdo. particular jump\" dual variable several epochs apdo dual adjustment off-policy trained λoff. comparison adjust dual variable incrementally on-policy data. since adjustment epoch important parameter apdo important future work provide theoretical guidance setting kadj. also interesting provide theoretical justiﬁcations acceleration effects apdo. moreover observed experiments training trajectory generated apdo strives best tradeoff improving rewards enforcing cost constraints. future work incorporate safety parameter controls degree safety awareness. tuning parameter algorithm able make risk-averse actions risk-neutral actions work supported grant cns- darpa raytheon technologies contract hroo -c-. authors would also like acknowledge chengtao provided valuable feedback work.", "year": 2018}