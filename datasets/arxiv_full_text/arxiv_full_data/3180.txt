{"title": "A new framework for optimal classifier design", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "The use of alternative measures to evaluate classifier performance is gaining attention, specially for imbalanced problems. However, the use of these measures in the classifier design process is still unsolved. In this work we propose a classifier designed specifically to optimize one of these alternative measures, namely, the so-called F-measure. Nevertheless, the technique is general, and it can be used to optimize other evaluation measures. An algorithm to train the novel classifier is proposed, and the numerical scheme is tested with several databases, showing the optimality and robustness of the presented classifier.", "text": "alternative measures evaluate classiﬁer performance gaining attention specially imbalanced problems. however measures classiﬁer design process still unsolved. work propose classiﬁer designed speciﬁcally optimize alternative measures namely so-called f-measure. nevertheless technique general used optimize evaluation measures. algorithm train novel classiﬁer proposed numerical scheme tested several databases showing optimality robustness presented classiﬁer. evaluation measures crucial role classiﬁer analysis design. accuracy recall precision f-measure kappa proposed measures like informedness markedness examples diﬀerent evaluation measures. depending problem ﬁeld application measure could suitable another. behavioral sciences speciﬁcity sensitivity commonly used medical sciences analysis standard evaluation. hand information retrieval community fraud detection recall precision f-measure considered appropriate measures testing eﬀectiveness. looking best decision rule bayesian framework implies minimize overall risk taking account diﬀerent misclassiﬁcation cost equal misclassiﬁcation cost problem optimal solution maximum accuracy selecting class maximum posteriori probability. however ﬁnding decision rule looks minimum error rate maximum accuracy imbalanced domain gives solutions strongly biased favor majority class getting poor performance. problem particularly important applications instances class heavily outnumber instances class costly misclassify samples minority class. example information retrieval nontechnical losses power utilities muniz nagi mohamad medical diagnosis identifying rare events challenging issue great impact regarding many problems pattern recognition data mining. main diﬃculty ﬁnding discriminatory rules applications deal small data sets skewed data distributions overlapping classes. range classiﬁers work successfully applications etc.) poor performance context example decision tree pruning criterion usually classiﬁcation error remove branches related minority class. backpropagation neural networks expected gradient vector length proportional class size gradient vector dominated prevalent class consequently weights determined class. svms thought robust class imbalance problem since support vectors calculate region boundaries. however class problem boundaries determined prevalent class since algorithm tries largest margin minimum error. diﬀerent approach taken one-class learning example class model created based samples classes. optimality one-class svms two-class classiﬁers demonstrated important imbalanced problems. recently great eﬀort done give better solutions class imbalance problems garc´ıa references therein). approaches deal imbalanced problem idea adapt classiﬁers good accuracy balanced domains. variety ways proposed changing class distributions kolez incorporating costs decision making barandela garcia using alternative performance metrics instead accuracy learning process standard algorithms work propose diﬀerent approach problem designing classiﬁer based optimal decision rule maximizes chosen evaluation measure case f-measure speciﬁcally feature space looking classiﬁer maximizes f-measure. here given feature vector classiﬁer assigns class class address problem proposing energy minimum achieved optimal classiﬁer solve optimization problem using gradient descent inspired level-set method although analysis made f-measure could extended measures. particular case chosen measure accuracy proposed algorithm equivalent bayes approach. gorithm need change original distributions arbitrarily assign misclassiﬁcation costs appropriate decision rule severe imbalanced problems. although consensus need using suitable evaluation measures classiﬁer design best knowledge technique proposed optimizes alternative measures decision frontiers. rest paper organized follows. section optimal classiﬁer f-measure proposed numerical scheme obtain presented. experimental results shown section conclude section precision recall important measures evaluate performance given classiﬁer imbalance scenario. recall indicates true positive rate precision indicates positive predictive value. high value ensures recall precision reasonably high desirable property since indicates reasonable values true positive false positive rates. best value speciﬁc task ﬁnding classiﬁer consists deﬁning regions belongs ω+/ω− classiﬁed belonging positive/negative class. train classiﬁer maximize given performance measure must therefore regions give maximal performance measure available data set. order classiﬁer maximizes given performance measure must able express quantities terms calculated computing points training data belong regions however realization proposed algorithm estimate quantities terms probability densities positive negative classes. suppose estimates certain density functions terms functions following approximations quantities functions known task ﬁnding optimal classiﬁer consists ﬁnding regions maximize chosen measure. mentioned before choice depends particular problem application considered. paper chosen f-measure evaluation measure next subsection present algorithm determine optimal boundaries measure. however framework general quantities expressed terms functions deﬁned previous section. therefore task training classiﬁer maximizes f-measure approached ﬁnding regions minimize extent quantity given representative quantity depends extent densities available given functions deﬁned previous section represent distribution points training data. focus work task ﬁnding appropriate probability densities sake paper suppose indeed available quantity good approximation quantity calculated directly available data set. terms auxiliary function deﬁned instance signed distance boundary commonly used implementation since proven give good results. boundary regions therefore given surface satisﬁes equation deﬁnition thus expressed functional smoothed heavyside function domains integration terms task training classiﬁer consists ﬁnding function minimizes functional. must function cancels ﬁrst variation functional written terms functional derivative calculating functional derivative have steady estate reached equation satisﬁed details). since equation solved numerically principle suﬃciently regular densities allowed therefore proposed algorithm depend particulars density estimation process. introduction auxiliary function motivated level method although kind curve evolution approaches share known implementation details must taken account. instance order guarantee stability usual reinitialize function order keep distance function. relevant information terms evaluation functional partition deﬁnes. therefore possible reinitialize function signed distance function zero-level since time zero level decision frontier classiﬁer. figure evolution frontier shown initial ﬁnal certain database densities positive negative classes represented green respectively. although rigorous proof existence solution equation provided exhaustive empirical evidence zero level initialization includes intersects connected components support either densities gradient descent converges global optimum. experimental validation used four diﬀerent databases shown figure database negative class gaussian distribution positive samples ring distribution particular case samples negative class amount positive class. database multimodal distribution positive negative samples. database samples negative class samples positive class. third database horseshoe distribution samples majority class samples minority class. last database distributions database negative samples positive samples. selected databases play particular role idea consider diﬀerent scenarios imbalance balance also evaluate wide variety shapes classes distributions. experimental comparisons classical kernel density estimation technique used infer densities positive negative classes compare proposed algorithm called class tree traditional naive bayes classiﬁer. parameters algorithm chosen maximize f-measure next subsection brieﬂy explain chose algorithms considerations must taken account performance comparison. worth mentioning naive bayes performance. algorithm best accuracy expectable poorest f-measure. typical behavior classiﬁers designed minimizing classiﬁcation error problems classes highly overlapped unbalanced. illustrate point consider problem gaussian distributions negative positive classes means respectively unitary variance. number samples positive class negative class. decision problem example amounts choosing decision threshold sets frontier classes real line {τ∞}. diﬀerent values would diﬀerent values accuracy recall precision f-measure. figure shows dependencies function decision threshold. solution gives best f-measure good tradeoﬀ recall precision loss approximately accuracy compared optimal accuracy could obtained naive bayes solution getting better accuracy precision recall could solution positive class relevant also ﬁgure setting threshold away optimal f-measure point possible better value precision sacriﬁcing value recall conversely. consistent result found osvm+ker shown table slightly lower f-measure getting higher recall lower precision. figure mean values obtained executions using databases shown. standard deviations cases. explained above classes similar amounts samples separable distributions diﬀerences traditional algorithms designed imbalance problems important cases diﬀerence became signiﬁcant. conclude section present additional experiment skin segmentation data machine learning repository. skin dataset collected randomly sampling values face images various groups race groups genders obtained feret database database. total sample size samples; correspond skin samples non-skin samples. results shown figure osvm compared several values class achieves highest recall poor precision therefore obtaining f-measure approach outperforms osvm terms f-measure expected. observe previous subsection results diﬀerent databases provided showing proposed algorithm suitable imbalanced problems. even though work include results obtained hand several techniques proposed literature improve performance type algorithms unbalanced scenarios smote adaboost smootebost among others masnadi-shirazi vasconcelos l´opez garc´ıa garc´ıa references therein details). however methods post-processing techniques base classiﬁers black boxes main point section compare base classiﬁers themselves. mented tests realized) runs eﬃciently dimensions instance running much faster osvm algorithm used compare performances example using skin segmentation data. however must noted memory storage implementation depends size grid used compute decision function nevertheless eﬃcient solutions problem available instance allowing evaluate kernel density estimation evaluation points sample points raykar proposed framework classiﬁcation imbalanced problems classiﬁer design general. presented optimality conditions decision frontier maximize f-measure numerical scheme solve problem. important properties experiments consider making interesting study proposed framework. instance feasibility convenience using kernels proposed classiﬁer subject future research well combination proposed framework techniques used improve traditional classiﬁers", "year": 2013}