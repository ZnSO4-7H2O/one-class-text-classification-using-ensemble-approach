{"title": "Empirical study of PROXTONE and PROXTONE$^+$ for Fast Learning of Large  Scale Sparse Models", "tag": ["cs.LG", "cs.AI"], "abstract": "PROXTONE is a novel and fast method for optimization of large scale non-smooth convex problem \\cite{shi2015large}. In this work, we try to use PROXTONE method in solving large scale \\emph{non-smooth non-convex} problems, for example training of sparse deep neural network (sparse DNN) or sparse convolutional neural network (sparse CNN) for embedded or mobile device. PROXTONE converges much faster than first order methods, while first order method is easy in deriving and controlling the sparseness of the solutions. Thus in some applications, in order to train sparse models fast, we propose to combine the merits of both methods, that is we use PROXTONE in the first several epochs to reach the neighborhood of an optimal solution, and then use the first order method to explore the possibility of sparsity in the following training. We call such method PROXTONE plus (PROXTONE$^+$). Both PROXTONE and PROXTONE$^+$ are tested in our experiments, and which demonstrate both methods improved convergence speed twice as fast at least on diverse sparse model learning problems, and at the same time reduce the size to 0.5\\% for DNN models. The source of all the algorithms is available upon request.", "text": "iteration index sampled uniformly randomly chosen gradient ∇gik yields unbiased estimate true gradient show standard assumptions that suitably chosen decreasing step-size sequence {αk} proxsgd iterations expected sub-optimality convex objectives thus least theory fact also practice showed proxsgd slow solving problem real life applications need learn adjust fast order obtain usable model quickly. requirement results large variety approaches available accelerate convergence proxsgd methods full review immense literature would outside scope work. several recent work considered various special general cases developed algorithms enjoy linear convergence rate proxsdca miso proxsvrg proxn proxtone methods converge exponential rate value objective function except proxn achieves superlinear rates convergence solution however batch mode method. shalev-shwartz zhang’s proxsdca considered case component functions form fenchel conjugate functions computed efﬁciently. schimidt al.’s jascha al.’s considered case order solve problem linear convergent rate proposed novel fast method called proximal stochastic newton-type gradient descent compared previous methods proxtone like typical quasi-newton techniques requires adjustment hyperparameters. abstract. proxtone novel fast method optimization large scale non-smooth convex problem work proxtone method solving large scale non-smooth non-convex problems example training sparse deep neural network sparse convolutional neural network embedded mobile device. proxtone converges much faster ﬁrst order methods ﬁrst order method easy deriving controlling sparseness solutions. thus applications order train sparse models fast propose combine merits methods proxtone ﬁrst several epochs reach neighborhood optimal solution ﬁrst order method explore possibility sparsity following training. call method proxtone plus proxtone proxtone+ tested experiments demonstrate methods improved convergence speed twice fast least diverse sparse model learning problems time reduce size models. source algorithms available upon request. beneﬁted advances deep learning data accuracy dramatically improved difﬁcult pattern recognition problems vision speech currently urgent problems need solve real life especially internet applications deep learning ﬁrst always took long time adjust structures parameters obtain satisfactory deep model; second program always really deep network embedded devices mobile devices. thus fast learning sparse regularized models example regularized logistic regression regularized deep neural network regularized convolutional neural network becomes important. researchers proposed standard popular proximal stochastic gradient descent methods whose main appealing iteration cost independent making suited modern problems large. basic proxsgd method optimizing uses iterations example several hundred mini-batches order simplicity notations description distinguish between following algorithms means number mini-batches keep mind. section ﬁrst describe general procedure optimize parameter describe procedure bfgs method online hessian approximation maintained batch subfunction. followed description solving subproblem proxtone. iteration general proxtone uses regularized piecewise quadratic function approximate target loss function deep model local area around current point solution regularized quadratic model used point. component function sampled randomly gradient approximation hessian used update regularized quadratic model. procedure summarized algorithm algorithm proxtone regularized model learning input start point positive deﬁnite approximation hessian t∇gi rp∗max history∗n history gradient changes last rp∗n last rp∗n holds history; rp∗max history∗n history position changes repeat solve subproblem approximation solution work second order method proxtone promote training sparse deep models. compared conventional methods proxtone make full gradients thus needs less gradients achieve performance means converges much fast number epochs. gradient proxtone needs update hessian construct low-dimensional space solve kind lasso subproblem thus needs much time ﬁrst order methods. means ﬁnally proxtone converges slow time ﬁrst order methods. order overcome problem iteration performance less iterations solving subproblems means satisﬁed less exact steepest search directions. approximation accelerate convergence proxtone result less sparsity weights deep neural networks. empirical study found situations example training fully connected fast approximated proxtone cannot fully explore possibility sparseness weights. ﬁrst order method easy deriving accumulating sparseness iteration soft threshold operators thus propose combine ﬁrst order method proxtone training dnn. call kind methods proxtone+. experiments show proxtone proxtone+ suitable training different kind neural networks example proxtone much suitable sparse since whose almost weights shared type proxtone+ much suitable training sparse dnn. finally optimizer code reproduce ﬁgures work available upon request. outline rest study. section presents main proxtone algorithm regularized model learning states choice details implementation. section describe proxtone+ method. report experimental results section provide concluding remarks section matrices algorithm prohibitive cost storing history terms required bfgs. thus employ idea construct shared dimensional subspace makes algorithm tractable terms computational overhead memory large problems. gradients mapped limited sized shared adaptive low-dimensional space expanded meeting observation. hessian regularized quadratic model solution updated low-dimensional space. finally solution projected back original space become real optimal points. mapping projection comprised dense matrix thus sparse solution low-dimensional space result non-sparse solution original space. problem discussed solved section arguably important feature method regularized quadratic model incorporates second order information form positive deﬁnite matrix because iteration user complete freedom choice include simplest option second order information employed; provides accurate second order information much computationally expensive work with; order tradeoff accuracy complexity popular formulae updating hessian approximation bfgs formula deﬁned store certain number vector pairs used formulas. iteration computed oldest vector pair pairs replaced pair obtained step. vector pairs includes curvature information history recent iterations. indeed famous limited-memory bfgs algorithm stated formally following algorithm means gradient need several iterations computing approximated hessian forming lasso problem also needs several iterations solve. thus typically proxtone needs much time iteration ﬁrst order method. means although proxtone much fast methods number gradients epochs slower time. following section solve problem. compared conventional method proxtone achieve performance less gradients less epochs. since always needs much computation ﬁrst order method iteration thus always proxtone converges slowly ﬁrst order methods physic time. order speed proxtone solve lasso problem exactly always ’max iter algorithm result inexact solution iteration proxtone also result much faster convergence speed. speed cause problems cannot control sparseness solution. order overcome problem combine proxtone ﬁrst order method ﬁrst stage proxtone reach nearby optimal comes second stage proxsag explore possibility sparseness solution. rough idea result following proxtone+ algorithms. preliminary study large scale convex problems debug algorithm. present results numerical experiments illustrate properties proxtone method. focus sparse regularized logistic regression problem binary classiﬁcation given training examples {+−} optimal predictor solving algorithm proxtone+ regularized model learning input start point positive deﬁnite approximation hessian t∇gi number epochs perform proxtone. repeat tion results different methods plotted ﬁrst effective passes protein covertype respectively data figure test proxtone kinds hessian ﬁrst diagonal hessian constant diagonal elements hessian kind updated algorithm iterations proxtone seem achieve best all. first trained deep neural network classify digits mnist digit recognition benchmark. used similar architecture mnist dataset consists pixel greyscale images handwritten digits training test examples. network consisted input units hidden layer units second hidden layer units output units. experiment using rectiﬁed linear sigmoidal units. objective used standard softmax regression output units. theano used implement model architecture compute gradient. second trained deep convolutional network cifar- using pooling rectiﬁed linear units. cifar- dataset consists color images drawn classes split train test images. architecture used contains convolutional layers units respectively followed fully connected layer units. architecture loosely based pylearn theano used implement model. preliminary experiment used choose hyperparameter proxsag proxsgd sparse sparse respectively figure detail measurement time sparsity methods. figure show proxtone proxtone+ converge nearly twice fast state-ofthe-art methods. sparsity proxtone+ reduce size sparse training. since many share weights sparse training proxtone much suitable proxtone+ reduce size figure comparison proxtone competing optimization techniques sparse sparse cnn. bold lines indicate best performing hyperparameter optimizer. seen proxtone free chosen hyperparameter. paper make clear implementation details proxtone numerical evaluations nonconvex problems especially sparse deep learning problems. show proxtone proxtone+ make full gradients converges much faster state-of-the-art ﬁrst order methods number gradients epochs. also showed methods converges faster also time reduce size models respectively. directions current study extended. experiments show proxsag method good performance thus would meaningful also make clear theory convergence proxsag second combine randomized block coordinate method minimizing regularized convex functions huge number varialbes/coordinates. moreover trends needs data designing distributed/parallel proxtone real life applications. broader context believe current paper could serve figure comparison proxtone competing optimization techniques solving sparse regularized logistic regression datasets protein; covertype. bold lines indicate best performing hyperparameter optimizer. figure comparison proxtone+ competing optimization techniques objective functions. objective functions shown multilayer perceptron sigmoidal units trained mnist digits. value sparsity error. figure comparison proxtone competing optimization techniques objective functions. objective functions shown multilayer convolutional network rectiﬁed linear units trained cifar-. value sparsity. james bergstra olivier breuleux fr´ed´eric bastien pascal lamblin razvan pascanu guillaume desjardins joseph turian david wardefarley yoshua bengio ‘theano math expression compiler’ proceedings python scientiﬁc computing conference volume austin dimitri bertsekas ‘incremental gradient subgradient proximal methods convex optimization survey’ optimization machine learning goodfellow david warde-farley pascal lamblin vincent dumoulin mehdi mirza razvan pascanu james bergstra fr´ed´eric bastien yoshua bengio ‘pylearn machine learning research library’ arxiv preprint arxiv. goodfellow david warde-farley mehdi mirza aaron courville yoshua bengio ‘maxout networks’ arxiv preprint arxiv. geoffrey hinton deng dong george dahl abdel-rahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara sainath ‘deep neural networks acoustic modeling speech recognition shared views four research groups’ signal processing magazine ieee geoffrey hinton nitish srivastava alex krizhevsky ilya sutskever ruslan salakhutdinov ‘improving neural networks preventing co-adaptation feature detectors’ arxiv preprint arxiv. alex krizhevsky ilya sutskever geoffrey hinton ‘imagenet classiﬁcation deep convolutional neural networks’ advances neural information processing systems yann lecun l´eon bottou yoshua bengio patrick haffner ‘gradient-based learning applied document recognition’ proceedings ieee jason yuekai michael saunders ‘proximal newton-type methods convex optimization’ advances neural information processing systems julien mairal ‘optimization ﬁrst-order surrogate functions’ arxiv preprint arxiv. nesterov ‘efﬁciency coordinate descent methods huge-scale optimization problems’ siam journal optimization jorge nocedal stephen wright numerical optimization springer science business media ziqiang rujie ‘large scale optimization proximal stochastic newton-type gradient descent’ machine learning knowledge discovery databases springer jascha sohl-dickstein poole surya ganguli ‘fast large-scale optimization unifying stochastic gradient quasi-newton methods’ proceedings international conference machine learning", "year": 2016}