{"title": "On the convergence of cycle detection for navigational reinforcement  learning", "tag": ["cs.LG", "cs.AI"], "abstract": "We consider a reinforcement learning framework where agents have to navigate from start states to goal states. We prove convergence of a cycle-detection learning algorithm on a class of tasks that we call reducible. Reducible tasks have an acyclic solution. We also syntactically characterize the form of the final policy. This characterization can be used to precisely detect the convergence point in a simulation. Our result demonstrates that even simple algorithms can be successful in learning a large class of nontrivial tasks. In addition, our framework is elementary in the sense that we only use basic concepts to formally prove convergence.", "text": "consider reinforcement learning framework agents navigate start states goal states. prove convergence cycle-detection learning algorithm class tasks call reducible. reducible tasks acyclic solution. also syntactically characterize form ﬁnal policy. characterization used precisely detect convergence point simulation. result demonstrates even simple algorithms successful learning large class nontrivial tasks. addition framework elementary sense basic concepts formally prove convergence. reinforcement learning subﬁeld artiﬁcial intelligence concerned agents learn task-solving policy exploring state-action pairs observing rewards oﬀ-policy algorithms q-learning on-policy algorithms sarsa well-understood shown converge towards optimal policies quite general assumptions. algorithms updating every state-action pair estimate expected value article expressly propose eﬃcient powerful algorithm. contrast want show convergence occur already simplistic algorithms. setting result tasks agent reach goal state reward action performed. actions nondeterministic. like refer setting navigational learning. learning algorithm consider simplistic agent remember states already visited. algorithm on-policy; update rule that state revisited policy revised updated arbitrary action state. refer algorithm cycle-detection algorithm. main result algorithm converges tasks call reducible. intuitively task reducible exists policy guaranteed lead reward. also provide test convergence outside observer could apply decide convergence happened used detect convergence simulation. note ﬁnal policy allowed explore strict subset entire state space. ﬁrst motivation work understand biological organisms successful learning navigational tasks. example animals learn navigate nest foraging areas back reward could related ﬁnding food returning home. standard learning process might initially exhibit exploration eventually policy found leads animal reliably reward. context biologically plausible learning fr´emaux make following interesting observations. first navigational learning restricted physical worlds also applied abstract state spaces. second formed policy strongly depends experiences agent therefore policy necessarily optimal. elaborate observations formal framework. consider general deﬁnition tasks used represent physically-inspired tasks abstract tasks. furthermore insist ﬁnding policies generate shortest path reward satisﬁed learning policies avoid cycles. secondary motivation work contribute towards ﬁlling apparent exists ﬁeld reinforcement learning logic-based ﬁelds computer science. indeed structural level notion task used similar notion interpretation description logics notion transition system used veriﬁcation methods used establish convergence largely based techniques numerical mathematics theory optimization. give proofs convergence elementary discrete-mathematics style common logic-based ﬁelds well traditional correctness proofs algorithms standard convergence proofs assume condition state-action pairs visited inﬁnitely often e.g. conditions kind known fairness conditions theory concurrent processes also convergence proof need appropriate fairness assumption eﬀect agent repeats policy-updating conﬁguration inﬁnitely often must also explore possible updates inﬁnitely often. note cycle-detection learning algorithm could remotely related biologically plausible mechanisms. models biological learning policy represented synaptic connections neurons encoding states neurons encoding actions. connections strengthened pre-before-post synaptic activity combined reward causing organism remember action preferences encountered states. organism would initially policy frequently leads cycles task still unlearn policy follows. consider pair state preferred action policy. noise neuron participating encoding action could become activated state eﬀectively occurs. possibly post-before-pre synaptic activity leads long-term-depression i.e. connections weakened. synapses weakening eﬀect aided longer time window long-term-depression compared long-term-potentiation reward would remain absent longer periods cycles without reward noise could gradually unlearn action preferences states. absence preferences noise could generate random actions states. unlearning phase followed random action proposals would resemble cycle-detection algorithm. outline article organized follows. discuss related work section formalize important concepts section present prove results section discuss examples simulations section conclude section previous work reinforcement learning algorithms focused learning policy eﬃciently using polynomial number steps terms certain input parameters task also line work reinforcement learning necessarily aimed towards eﬃciently bounding learning time. case convergence learning process happens limit visiting task states inﬁnitely often. notable examples temporal-diﬀerence learning q-learning temporal-diﬀerence learning become attractive foundation biological learning models previous works numerical reinforcement learning optimal policies related optimal value functions optimal policy gives highest reward long run. motivated design numerical learning techniques. corresponding proof techniques always clearly illuminate properties task state space interplay particular learning algorithm. framework introduced article hope shed light properties task state space particular paths could formed graph structure task. although graph-oriented framework diﬀerent viewpoint compared standard numerical reinforcement learning believe theorem showing convergence always occurs reducible tasks proof contribute making fascinating idea reinforcement learning easily accessible wider audience. convergence result similar intent previous results showing numerical learning algorithms converge probability one. neuron activated noise state occurs refractoriness could prevent state subsequently activating resulting absence postsynaptic spike fails elicit long-term-potentiation i.e. connections strengthened. mentioned weakening eﬀect compensated. study physical abstract state spaces. example physical state space consider navigation task simulated mouse swim hidden platform rest resting corresponds reward; state contains coordinate. example abstract state space consider acrobatic swinging task reward given double pendulum reaches certain height; space abstract state contains angles angular velocities i.e. four dimensions. conceptually matter many dimensions state space agent always seeking paths graph structure task. idea ﬁnding paths task state space also explored bonet geﬀner framework based depth-ﬁrst search. framework global perspective learning operations access multiple states simultaneously overall search strongly embedded recursive algorithm backtracking. algorithm acts local perspective single agent state observed time. remarked sutton barto repeated theme reinforcement learning update policy agent visits states. theme also strongly present current article visited state policy always remembers lastly tried action state. ﬁnal convergence studied article eventually choose actions anymore encountered states. notion reducibility discussed article related principles dynamic programming upon large part reinforcement learning literature based indeed reducibility defer responsibility obtaining reward given state successor states chosen action. resembles dynamic programming reward prediction values given state estimated looking reward prediction values successor states. settings standard numerical reinforcement learning dynamic programming ﬁnds optimal policy time worst-case polynomial number states actions. time complexity also applicable iterative reducibility procedure given section formalize tasks notion reducibility section next section operational semantics formalize interaction task cycle-detection learning algorithm. section deﬁne convergence eventual stability policy. lastly section impose certain fairness restrictions operational semantics. tasks formalize tasks nondeterministic transition systems transitions labeled immediately rewarding reward on-oﬀ ﬂag. formally task ﬁve-tuple nonempty ﬁnite sets; rewards nonempty subset q×a; function maps nonempty subset elements called respectively states start states actions. rewards tells pairs states actions give immediate reward. function describes possible successor states applying actions states. remark formalization tasks keeps graph structure models previously studied reinforcement learning; essentially compared ﬁnite markov decision processes omit transition probabilities simplify numerical reward signals boolean ﬂags. study negative feedback signals performed actions give either reward reward i.e. feedback either positive neutral. framework agent observe states exact manner commonly used assumption mention negative feedback signals partial information topics work section refer elements goals goal states. intuitively goal state action reliably gives immediate reward. task least goal state rewards always nonempty. agent could learn strategy reduce encountered states goal states perform rewarding action goal states. intuition formalized next. note reduce ﬁnite index i.e. ﬁxpoint. letting reduce. intuitively state choose reducible action come closer also single state reducible reduce. figure task example states action applications represented circles boxes respectively. start states indicated arrow without origin. goal states rewarding actions highlighted double circle double respectively. would like emphasize reducibility notion progress task transition graph determinism action application i.e. transition remains inherently nondeterministic. think reducibility onion layers state space core onion consists goal states immediate reward obtained states outer layers action leads step inner layer closer reward. traveling outer layer inner layer nondeterminism manifests unpredictability exact state reached inner layer. describe cycle-detection learning algorithm operates tasks means operational semantics describes steps taken time. ﬁrst give intuition behind cycle-detection algorithm proceed formal semantics. want formally elaborate intuition path learning. therefore necessarily design another eﬃcient learning algorithm. seems informative seek bare ingredients necessary navigational learning. would simple algorithm look like? ﬁrst candidate consider algorithm given random initial policy always follows policy execution. would exploration learning since policy always followed never modiﬁed. general policy might even lead reward agent might around cycles without obtaining reward. opposite spectrum could completely random process upon visit task state always chooses random action. agent lucky random movement state space might occasionally unreliably lead reward. sign learning either storage previously gained knowledge reward obtained. consider following in-between strategy algorithm could choose random actions detects cycle state space reaching reward. agent escape cycle might keep running around indeﬁnitely without ever reaching reward. concretely could consider cycle-detection algorithm constituted following directives cycle-detection algorithm arguably amongst simplest learning algorithms could conceive. algorithm might able gradually reﬁne policy avoid cycles causing agent eventually follow acceptable path reward. working memory containing states visited obtaining reward. working memory reset whenever reward obtained. conﬁgurations conﬁguration triple maps element function called policy. called working memory contains states already visited execution reset whenever reward obtained. refer current state conﬁguration also contains state note ﬁnite number possible conﬁgurations. learning algorithm reﬁne policy trials formalize below. intuitively contains options actions successor states chosen directly branching actions chosen otherwise must restrict attention action stored policy current state. note successor state depends chosen action. reward transitions; chain ﬁnite ends reward transition contains reward transitions. rephrase trial ﬁnite ends ﬁrst occurrence reward; reward transition trial must inﬁnite. trial occurrence conﬁguration terminal occurrence last conﬁguration trial i.e. occurrence target conﬁguration reward transition. note inﬁnite trial contains terminal conﬁgurations. condition words trial ends start next trial start state reuse policy reset working memory. resetting working memory forget states visited obtaining reward. policy essential product trial. condition saying start state used beginning inﬁnitely many trials expresses want learn whole task possible start states. remark operational semantics agent repeatedly navigates start states goal states. obtaining immediate reward goal state agent’s location always reset start state. call framework episodic note framework also used study continuing operational processes always enforce strong reset mechanism goal states back remote start states. indeed task could deﬁne start states simply states. case runs possible trials start last state reached previous trial agent trying obtain sequence rewards; still reset working memory time begin trial. equivalent deﬁnition branching conﬁgurations non-terminal positions containing intuitively eventual stability means risk anymore paired actions deﬁnitely stay connected action. note states appearing ﬁnite number times always become stable converges trials terminate remark converges note policy eventually become ﬁxed change policy branching conﬁgurations non-terminal positions. lastly formed policy called ﬁnal policy studied detail section emphasize converging never stops runs deﬁned inﬁnite; ﬁnal policy remains indeﬁnitely updated anymore. would also like emphasize converging eventually trials contain cycles reaching reward moment trial state could revisited terminal conﬁguration i.e. target conﬁguration reward transition. fairness assumptions needed give learning algorithm suﬃcient opportunities detect problems better policies intuitively choice points choice independent policy working memory states current state. intuition related markov assumption independence path assumption below formalize intuition fairness notion operational semantics section begin ﬁrst trial random start conﬁguration i.e. choose random start state random policy. next choose option ﬁrst ordinal ordered opt. subsequent occurrences conﬁguration choose option ﬁguration occurs inﬁnitely often non-terminal positions continually rotate options. naturally trials ﬁrst occurrence reward choose another start state; taking care start states inﬁnitely often. cycle-detection learning algorithm formalized section continually marks encountered states visited. trials i.e. obtaining reward state marked unvisited. algorithm encounters state already visited within trial algorithm proposes generate action intuitively state encountered trial agent might running around cycles action tried escape cycle. important avoid cycles want achieve eventual upper bound length trial i.e. upper bound time takes reach reward given start state. repeatedly trying action revisited states might eventually lead reward thereby terminate trial. learning process nondeterminism task helpful hindering nondeterminism helpful transitions choose successor states closer reward nondeterminism hindering transitions choose successor states further reward might lead cycle. still suitable tasks like reducible tasks actions randomly tried upon revisits might eventually globally form policy never trapped cycle ever outline section follows. section present suﬃcient condition tasks learnable fairness. section discuss simulator could detect convergence occurred fair run. section present necessary conditions tasks learnable fairness. intuitively task reducible might able obtain policy start state leads reward without revisiting states trial. long revisits occur keep searching acyclic states implied reducibility. imagine states near goal states i.e. near immediate reward tend quickly settle action leads reward. subsequently states farther removed immediate reward reduced states near goal states growth process propagates entire state space. intuition conﬁrmed following convergence result reducibility layers deﬁned section goals. trial show ﬁniteness thus termination show induction states occur ﬁnitely many times reducible index therefore inductive proof shows every state occurs ﬁnite number times trial hence ﬁnite. base case. goals. towards contradiction suppose occurs inﬁnitely often trial making inﬁnite. occurs inﬁnitely often non-terminal positions conﬁguration branching occurs once. deﬁnition goals action rewards. since always choose inductive step. assume states occur ﬁnitely many times li−. deﬁnition action li−. towards contradiction suppose occurs inﬁnitely often making inﬁnite. like base case trial since always choose li−. branching. continue recall part proof shown trials ﬁnite. whenever conﬁguration occurs inﬁnitely often means conﬁguration occurs inﬁnitely many trials. similarly transition occurs inﬁnitely often means transition occurs inﬁnitely many trials. base case. goals. towards contradiction suppose would become stable. means inﬁnitely many nonterminal occurrences branching conﬁgurations containing ﬁnitely many possible conﬁgurations must branching conﬁguration containing occurs inﬁnitely often non-terminal positions. trial containing transition implies last transition trial show non-terminal occurrences trial must non-branching conﬁguration. hence becomes stable; desired contradiction. consider ﬁrst trial index occurs nonterminal position. conﬁguration ﬁrst occurrence trial note trial ends assignment action trials could modiﬁed action further conﬁguration branching ﬂagged visited ﬁrst occurrence trial means conﬁguration containing occur trial non-terminal position. reasoning repeated following trials non-terminal occurrences branching conﬁgurations containing eventually becomes stable. li−. deﬁnition action li−. towards contradiction suppose become stable would contradict induction regarding terminology chain -chain chain contains non-reward transitions chain following desired form note chain starts ends occurrence revisited chain. moreover ﬁrst transition performs action above. next trial -trial trial contains -chain. principle -trial could embed diﬀerent -chain. policy i.e. action assigned index trial occurrence. assumption become stable consider ﬁrst trial index occurs branching conﬁguration non-terminal position. note trials trial trial modify action ﬁrst occurrence trial always non-branching thus perform action there. subsequence trial starting ﬁrst occurrence ending branching conﬁguration non-terminal position -chain chain starts ends ﬁrst transition performs action contains non-reward transitions ends non-terminal position. hence trial -trial. apply contains transition index trial occurrence inﬁnitely many indexes trials ﬁnite occurrence trial followed know deﬁnition implies occurrence transition trial subsequence starting occurrence ending ﬁrst subsequent branching conﬁguration non-terminal position -chain chain starts ends ﬁrst transition performs action chain contains non-reward transitions ends non-terminal position. hence trial -trial. conﬁguration containing used inﬁnitely many occurrences -trials last conﬁguration -chain. note occurs inﬁnitely often non-terminal positions since -chains contain reward transitions. denoting recall transitions non-reward transitions. note li−. chain certainly marked state visited ﬁrst occurrence causing conﬁguration branching. implies option taken ﬁrst transition since also since certainly marked state visited ﬁrst occurrence implies next since conﬁguration occurs apply. conﬁguration branching. moreover know rewards since transition reward transition including ﬁrst transition. branching conﬁguration occurs inﬁnitely often nonterminal positions. hence would become stable. induction hypothesis says become stable; desired contradiction. remark theorem trials fair reducible task eventually contain number non-terminal conﬁgurations number states; otherwise least state would never become stable. relatively good eventual upper bound trial length. however theorem provides information waiting time upper bound emerge waiting time strongly depends choices made regarding start states trials tried actions successor states would inﬁnitely many trials contain non-terminal conﬁgurations states inﬁnitely many trials revisit state non-terminal position. since ﬁnitely many states would least state inﬁnitely many trials occurs branching conﬁguration non-terminal position; state become stable deﬁnition. seek policy avoids revisits states trial important intuition implied theorem reducible tasks eventually trials follow paths without cycles state space. followed paths still inﬂuenced nondeterminism never contain cycle. also path followed trial necessarily shortest possible path reward discovery paths depends experience i.e. order actions tried learning process. experience dependence experimentally observed e.g. fr´emaux remark order states become stable fair necessarily follow order reducibility layers section general seems possible states farther removed goal states could become stable faster states nearer goal states; become stable farther removed states probably ﬁrst stable strategy goal states. simulations exactly follow inductive reasoning proof theorem could compare later section canonical policy implied reducibility figure actual ﬁnal policy figure following example illustrates necessity fairness assumption theorem although convergence result reducible tasks appears natural example reveals subtle notions like fairness assumption taken account understand learning. example consider task example also visualized figure following ease notation denote conﬁgurations triples current state; action assigned policy speciﬁc state action assigned states; visited states before. state second transition. conﬁguration therefore trial action assigned states including state alternates trials starts trial state never becomes stable assign action action state alternating fashion. refer lastly formed policy ﬁnal policy. increased understanding convergence means appears interesting something form ﬁnal policy. particular would like understand kind paths generated ﬁnal policy. additional beneﬁt recognizing form ﬁnal policy allows detect convergence point simulation. syntactically characterize ﬁnal policy theorem general verifying syntactical property ﬁnal policy requires access entire task states. subsection require tasks reducible. ﬁrst introduce parts syntactical characterization namely so-called forward backward sets states induced policy. below syntactical property says forward contained backward set. precise convergence detection possible framework model reward numerically thus numerical instability issues near convergence. convergence detection enables simulation experiments section final policy formalize ﬁnal policy. task learnable fairness. fair implies converges. deﬁne convergence-trial smallest trial index following holds trial terminates trial branching conﬁgurations non-terminal positions. implies trial policy change anymore change action assigned state state would occur branching conﬁguration nonproof. show separate parts forward backward suﬃcient necessary condition ﬁnal policy part suﬃcient condition. policy occurring trial. assume forward backward show ﬁnal policy concretely show trial starting policy conﬁgurations including terminal conﬁguration; contain branching conﬁgurations non-terminal positions. implies ﬁrst trial ending convergence-trial ﬁnal policy. base case. property trial starts policy property property know non-branching ﬁrst conﬁguration trial still empty working memory visited states. property applying induction hypothesis property namely non-branching know πi−. subsequently applying induction hypothesis property namely know property start note non-branching induction hypothesis property subsequently applying induction hypothesis property namely know moreover since −−−−→ non-reward trantransition sition. hence rewards thus ground lastly applying induction hypothesis property overall obtain ground combined ing. means state revisited qi−}. note implies applying induction hypothesis property conﬁgurations know forward show backward would imply forward backward desired contradiction. inﬁnite sequence sets deﬁned backward above. show induction overall implies backward inductive step. assume towards contradiction suppose take would immediately contradiction induction hypothesis. henceforth suppose which deﬁnition means bj−. show would give desired convergence-trial whose trial index denote deﬁnition convergence-trial trial branching conﬁgurations non-terminal positions. note particular policy longer changes trial inside option stay longer inside follow policy second step reasoning show stay arbitrarily long inside even convergence-trial causing least state occur branching conﬁguration non-terminal position trial false forward backward desired left show backward first show since forward index moreover since backward ground backward ground overall ground implies forward complete reasoning combining forward assumption following base case deﬁnition valid state used inﬁnitely many trials start state also trial moreover ﬁrst conﬁguration trial always non-terminal position trial contains least transition. inductive step. assume state occurs inﬁnitely often non-terminal positions trial fj\\fj−. implies ground applying induction hypothesis know occurs inﬁnitely often non-terminal positions trial ﬁnite number possible conﬁgurations conﬁguration containing occurs inﬁnitely often non-terminal positions trial trial convergence-trial make observations conﬁguration ﬁrst contains ﬁnal policy policy longer changes trial second non-branching branching conﬁgurations occur non-terminal positions trial take since forward know occurs inﬁnitely often non-terminal positions trial ﬁnitely many possible conﬁgurations conﬁguration containing occurs inﬁnitely often non-terminal positions trial trial policy longer changes non-branching conﬁgurations occur non-terminal positions. contains ﬁnal policy nonbranching. moreover since know step overall opt. fairness following transition occurs inﬁnitely often trial apply ground transition non-reward transition. therefore conﬁguration occurs inﬁnitely often non-terminal positions trial denoting note make similar reasoning since know step conﬁguration contains ﬁnal policy occurs trial also nonbranching occurs trial non-terminal position. therefore procedure repeated times total show existence conﬁguration occurs inﬁnitely often non-terminal positions trial conﬁguration therefore branching thus existence gives desired contradiction explained beginning part proof. section seen reducibility suﬃcient property tasks learnable fairness subsection also show necessary properties tasks learnable fairness. provides ﬁrst step towards characterizing tasks learnable fairness. thinking characterization useful allows better understand tasks learnable fairness. consider policy where state-action mappings arbitrary. consider fair whose ﬁrst trial given start state initial policy. first consider following chain deﬁne qi−}. note chain indeed valid conﬁguration non-branching therefore qi+) opt; means modify policy transition −−−−→ apply mark visited gives apply. note conﬁguration contains state consider fair whose ﬁrst trial starts chain show property show start states reducible goals. fair implies converges. converges forward backward backward reducible goals show backward reduce; implies reduce desired. goals recall theorem reducibility suﬃcient property tasks learnable fairness. note reducibility implies necessary properties proposition tasks learnable fairness. discuss suﬃcient necessary properties. task. letting reduce states reduced goals deﬁne inside unstable never reliably escape unstable escaping unstable depends nondeterministic choices regarding successor states. intuition also appeared part proof theorem proof focusing single ﬁxed action state assigned ﬁnal policy hand. following example illustrates nonempty unstable could prevent convergence. particular example illustrates necessary properties proposition suﬃcient task learnable fairness. task satisﬁes necessary properties proposition reachable states states case path goals start state reducible goals. however task learnable fairness illustrate. consider trial following form starting initial policy assigns action state ﬁrst state state next stay least consecutive times state lastly proceed state obtain reward there. revisits border state revisit precise border state trial fairness choose action avoid future entrance unstable. possibility revisit border states trial exactly missing example characterization tasks learnable fairness might class tasks satisfy necessary properties proposition additionally specify assumptions transition function ensure possibility revisits border states. illuminating role unstable sets learning interesting avenue work theorem tells reducible tasks learnable fairness theorem allows detect ﬁnal policy formed. illustrate theorems consider examples tasks reducible section section respectively. show practical eﬃciency cycle-detection learning algorithm rather illustrate theoretical insights. indeed considered examples reducible learnable fairness theorem next aided theorem experimentally measure long takes learning process convergence. section work identify aspects learning algorithm could improved become suitable practice. could apply following actions grid cell ﬁnish left right down left-up left-down right-up right-down. agrid denote containing actions. ﬁnish action non-movement action gives immediate reward applied goal cell. ﬁnish action intuitively says agent believes reached goal cell claims ﬁnished. activating ﬁnish action non-goal cell leave agent cell without reward. actions ﬁnish referred movement actions. every movement action noise environment. formalize noise oﬀset function maps movement action possible relative movements cause. example oﬀset intuitively noise adds left-rotating option right-rotating option figure grid navigation task. shaded cells goal cells ﬁnish action executed obtain reward. reducibility cells illustrated arrows arrow leaving cell represents action cell could choose come closer goal cell. note arrow displays main intended direction action principle noise oﬀsets could occur execution. example cells reducible. non-reducible cells applied movement action either function deﬁnes stay stationary cell could lead another non-reducible cell. naturally non-reducible cells removed resulting task reducible. assumption reducibility additionally imposed grid navigation tasks. figure visualize grid navigation task that illustrative purposes partially reducible. convergence-trial index ﬁrst experiment measures convergencetrial index. first discuss general setup experiment. recall section convergence-trial index fair ﬁrst trial index ﬁnal policy occurs end. given task simulate fair runs stop ﬁnal policy detected characterization theorem remember convergence-trial index. started random policy state assigned random action uniformly sampling available actions. fairness depends mechanism choosing among successor states also based uniform distribution interestingly simulate inﬁnite runs always eventually detect ﬁnal policy; stop simulation ﬁnite time ﬁnite thought preﬁx inﬁnite fair run. simulated runs convergence-trial index possibly outliers large convergence-trial index although outliers relatively number. exclude outliers list numbers considering p-quantile wanted convergence-trial index depends distance start cells goal cells. purpose considered grid tasks form corridor shown figure start cell. parameter change distance start cell patch goal cells. aspects remain ﬁxed including width corridor number location goal cells. arrows lengths simulated runs l-corridor. l-corridor separately simulated runs computed .-quantile measured convergence-trial indexes; gives empirical upper bound convergence-trial index l-corridor. figure shows quantiles plotted corridor lengths. observe longer corridors require time learn. probably application movement action cell could multiple successor cells nondeterminism. intuitively nondeterminism causes drift away straight path goal cells. policy learn suitable action cells encountered drift. corridor becomes longer cells encountered nondeterminism therefore learning process takes time learn suitable action encountered cells. figure suggests almost linear relationship corridor length empirical upper bound convergence-trial index based .quantile. section another example relationship linear. figure corridor template grid navigation tasks. single start cell marked black circle. goal cells shaded gray. parameterizable part horizontal distance start cell patch goal cells. completeness arrows represent possible policy illustrates reducibility kind task. although shown recall ﬁnish action executed goal cells. length depicted corridor distance start cell patch goal cells. figure visualization forward state backward state section ﬁnal policy simulated corridor navigation task shown figure states belonging marked action ﬁnal policy; empty grid cells part set. arrow shows main intended direction action; show left-rotating noise option right-rotating noise option. goal cells assigned ﬁnish action ﬁnal policy marked sign. trial length ﬁxed grid navigation task also wanted test trial length decreases progresses. decreasing trial length would demonstrate learning algorithm gradually reﬁning policy directly reward start states avoiding cycles state space. ﬁxed corridor length simulated ﬁrst trials runs; trial measured length number transitions. gives data matrix cell index trial index contains length trial simulated trial index computed .-quantile length measurements runs. plotting resulting empirical upper bound trial length trial index arrive figure trial length generally decreases progresses. similar experimental result also reported potjans context neurons learning grid navigation task. simulated visualize forward state backward state ﬁnal policy figure note forward state indeed included backward state set. interestingly case simulated initially learned perform ﬁnish action goal cells witnessed backward state eventually goal cells longer reached start state witnessed forward state set. figure template chain tasks. regarding notation symbol denotes actions symbol stands actions except action graphical notation tasks explained figure chain task shown figure parameter tells long chain states chain order ﬁnal state obtain reward start state worst case perform actions sequence ﬁnished arbitrary action. action applied state forward nondeterminism that pair could send arbitrary state later chain closer state also backward deterministic transitions take back start state whenever apply wrong convergence-trial index using experimental procedure grid tasks simulated runs chain lengths. chain length separately simulated runs computed .-quantile measured convergence-trial indexes. plotting resulting empirical upper bound convergence-trial index chain length arrive figure convergence-trial index rises faster linear terms chain length; contrast figure grid corridors. possible explanation forward nondeterminism chain length eﬀect state could stay connected longer action leading back start state course trial start state connected action otherwise progress could made; states could principle action. might encounter ﬁnal policy long time recognizable syntactic characterization theorem trial length using experimental procedure grid tasks ﬁxed chain length simulated ﬁrst trials runs computed .-quantile trial length explained grid corridor experiment. plotting resulting empirical upper bound trial length trial index arrive figure again ﬁgure suggests learning algorithm able gradually improve policy trials. know trial ended action assigned state otherwise state could reached. property also applies convergence-trial hence forward states forward nondeterminism along chain. recall theorem ﬁnal policy satisﬁes forward backward therefore backward also states. ﬁnal policy satisfy towards contradiction largest state number then using iterations computing backward backward backward would desired contradiction studied fascinating idea reinforcement learning non-numeric framework focus lies interaction graph structure task learning algorithm. studied graph property reducibility implies existence policy makes steady progress towards reward despite nondeterminism task. interestingly reducibility combined natural fairness assumption enables simple learning algorithm learn task. also characterized ﬁnal policy converging runs allows precise detection convergence-trial simulations. discuss avenues work. characterizing learnable tasks seen suﬃcient property necessary properties tasks learnable fairness. suﬃcient necessary properties seems strongly related unstable task perhaps possible characterize tasks learnable fairness imposing additional constraints manner unstable connected reducible states addition already identiﬁed necessary properties. time convergence related remark simulations section could theoretically provide upper bound convergence-trial index class tasks. could also make assumptions regarding probability distributions underlying random actions proposed state choice successor states applying action. given task result could probability distribution convergence-trial index total number transitions convergence. fading eligibility traces models biologically plausible learning activation pair connected neurons represented eligibility trace obtaining reward value trace applied synaptic weight neurons. realistic scenarios traces fade advantage simulation practical setup keep remembering information past states obtaining reward. current article viewed studying eligibility traces non-fading working memory cycle-detection learning algorithm eﬀectively stored trial. appears interesting working memory fade perhaps modeling working memory ﬁrst-in-ﬁrst-out queue newly entering state would remove oldest state queue size limit reached. negative feedback article path state space given start state reward good. however applications want avoid certain states. example navigation task organism might want reach nest certain starting location nest organism avoid hazardous locations like pits swamps. appears interesting formally investigate cases. information hazards could incorporated extending framework article explicit hazards tasks contains state-action pairs avoided; opposite rewards. framework could extended diﬀerentiate trials terminate reward trials terminate hazard. could said convergence eventually trials terminate reward states eventually become stable. incomplete information generalization framework studied article provides complete information learning algorithm individual state mapped action. real-world applications robot navigation agent work limited sensory information available time step. case agent ﬁrst build concepts states order diﬀerentiate them. concepts made remembering sensory information time general multiple states remain grouped together concept sensory information suﬃciently accurate diﬀerentiate states. issue also raised item future work fr´emaux initially also considered framework complete information available learning agent. ongoing work partially observable tasks e.g. building concepts related problem generalization real-world tasks might many states store policy. would useful collect states conceptual groups assign action group. appears interesting formalize incomplete information generalization extended version current framework investigate suﬃcient necessary properties convergence.", "year": 2015}