{"title": "Data Programming: Creating Large Training Sets, Quickly", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users express weak supervision strategies or domain heuristics as labeling functions, which are programs that label subsets of the data, but that are noisy and may conflict. We show that by explicitly representing this training set labeling process as a generative model, we can \"denoise\" the generated training set, and establish theoretically that we can recover the parameters of these generative models in a handful of settings. We then show how to modify a discriminative loss function to make it noise-aware, and demonstrate our method over a range of discriminative models including logistic regression and LSTMs. Experimentally, on the 2014 TAC-KBP Slot Filling challenge, we show that data programming would have led to a new winning score, and also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over a state-of-the-art LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way for non-experts to create machine learning models when training data is limited or unavailable.", "text": "large labeled training sets critical building blocks supervised learning methods enablers deep learning techniques. applications creating labeled training sets time-consuming expensive part applying machine learning. therefore propose paradigm programmatic creation training sets called data programming users express weak supervision strategies domain heuristics labeling functions programs label subsets data noisy conﬂict. show explicitly representing training labeling process generative model denoise generated training establish theoretically recover parameters generative models handful settings. show modify discriminative loss function make noise-aware demonstrate method range discriminative models including logistic regression lstms. experimentally tac-kbp slot filling challenge show data programming would winning score also show applying data programming lstm model leads tac-kbp score almost points state-of-the-art lstm baseline additionally initial user studies observed data programming easier non-experts create machine learning models training data limited unavailable. many major machine learning breakthroughs last decade catalyzed release labeled training dataset. supervised learning approaches datasets increasingly become building blocks applications throughout science industry. trend also fueled recent empirical success automated feature generation approaches notably deep learning methods long short term memory networks ameliorate burden feature engineering given large enough labeled training sets. many real-world applications however large hand-labeled training sets exist prohibitively expensive create requirements labelers experts application domain. furthermore applications’ needs often change necessitating modiﬁed training sets. help reduce cost training creation propose data programming paradigm programmatic creation modeling training datasets. data programming provides simple unifying framework weak supervision training labels noisy multiple potentially overlapping sources. data programming users encode weak supervision form labeling functions user-deﬁned programs provide label subset data collectively generate large potentially overlapping training labels. many diﬀerent weak supervision approaches expressed labeling functions strategies utilize existing knowledge bases model many individual annotator’s labels leverage combination domain-speciﬁc patterns dictionaries. this labeling functions widely varying error rates conﬂict certain data points. address this model labeling functions generative process lets automatically denoise resulting training learning accuracies labeling functions along correlation structure. turn model training optimize stochastic version loss function discriminative model desire train. show that given certain conditions labeling functions method achieves asymptotic scaling supervised learning methods scaling depends amount unlabeled data uses ﬁxed number labeling functions. data programming part motivated challenges users faced applying prior programmatic supervision approaches intended software engineering paradigm creation management training sets. example consider scenario labeling functions diﬀering quality scope overlap possibly conﬂict certain training examples; prior approaches user would decide somehow integrate signal both. data programming accomplish automatically learning model training includes labeling functions. additionally users often aware able induce dependencies labeling functions. data programming users provide dependency graph indicate example labeling functions similar ﬁxes reinforces another. describe cases learn strength dependencies generalization asymptotically identical supervised case. motivation method driven observation users often struggle selecting features models traditional development bottleneck given ﬁxed-size training sets. however initial feedback users suggests writing labeling functions framework data programming easier impact feature performance dependent training statistical characteristics model labeling function simple intuitive optimality criterion labels data correctly. motivated this explore whether traditional machine learning development process head users instead focus generating training sets large enough support automatically-generated features. summary contributions outline ﬁrst contribution data programming framework users implicitly describe rich generative model training ﬂexible general previous approaches. section ﬁrst explore simple model labeling functions conditionally independent. show certain conditions sample complexity nearly labeled case. section extend results sophisticated data programming models generalizing related results crowdsourcing section validate approach experimentally large real-world text relation extraction tasks genomics pharmacogenomics news domains show average point score improvement baseline distant supervision approach—including would competition-winning score tac-kbp slot filling competition. using lstm-generated features additionally would placed second competition achieving point score gain state-of-the-art lstm baseline additionally describe promising feedback usability study group bioinformatics users. work builds many previous approaches machine learning. distant supervision approach programmatically creating training sets. canonical example relation extraction text wherein knowledge base known relations heuristically mapped input corpus basic extensions group examples surrounding textual patterns cast problem multiple instance learning extensions model accuracy surrounding textual patterns using discriminative feature-based model generative models hierarchical topic models like approach latter methods model generative process training creation however proscribed based user input approach. also wealth examples additional heuristic patterns used label training data collected unlabeled data directly users similar manner approach without framework deal fact said labels explicitly noisy. crowdsourcing widely used various machine learning tasks particular relevance problem setting theoretical question model accuracy various experts without ground truth available classically raised context crowdsourcing recent results provide formal guarantees even absence labeled data using various approaches model capture basic model crowdsourcing setting considered equivalent independent case however addition generalizing beyond getting inputs solely human annotators also model user-supplied dependencies labelers model natural within context crowdsourcing. additionally crowdsourcing co-training classic procedure eﬀectively utilizing small amount labeled data large amount unlabeled data selecting conditionally independent views data addition needing labeled data allowing views approach allows explicit modeling dependencies views example allowing observed issues dependencies views explicitly modeled boosting well known procedure combining output many weak classiﬁers create strong classiﬁer supervised setting recently boosting-like methods proposed leverage unlabeled data addition labeled data also used constraints accuracies individual classiﬁers ensembled similar spirit approach except labeled data explicitly necessary ours richer dependency structures heuristic classiﬁers supported. general case learning noisy labels treated classical recent contexts also studied speciﬁcally context label-noise robust logistic regression consider general scenario multiple noisy labeling functions conﬂict dependencies. many applications would like machine learning face following challenges hand-labeled training data available prohibitively expensive obtain suﬃcient quantities requires expensive domain expert labelers; related external knowledge bases either unavailable insuﬃciently speciﬁc precluding traditional distant supervision co-training approach; application speciﬁcations changing model ultimately wish learn. setting would like simple scalable adaptable approach supervising model applicable problem. speciﬁcally would ideally like approach achieve expected loss high probability given inputs sort domain-expert user rather traditional hand-labeled training examples required supervised methods propose data programming paradigm programmatic creation training sets enables domain-experts rapidly train machine learning systems potential type scaling expected loss. data programming rather manually labeling example users instead describe processes points could labeled providing heuristic rules called labeling functions. remainder paper focus binary classiﬁcation task distribution object class pairs concerned minimizing logistic loss linear model given features without loss generality assume then labeling function user-deﬁned function encodes domain heuristic provides label subset objects. part data programming speciﬁcation user provides labeling functions denote vectorized form example gain intuition labeling functions describe simple text relation extraction example. figure consider task classifying co-occurring gene disease mentions either expressing causal relation not. example given sentence gene causes disease object true class construct training user writes three labeling functions external structured knowledge base used label objects relatively high accuracy equivalent traditional distant supervision rule uses purely heuristic approach label much larger number examples lower accuracy. finally hybrid labeling function leverages knowledge base heuristic. labeling function need perfect accuracy recall; rather represents pattern user wishes impart model easier encode labeling function hand-labeled examples. illustrated labeling functions based external knowledge bases libraries ontologies express heuristic patterns hybrid types; evidence existence diversity experiments labeling functions also strictly general manual annotations manual annotation always directly encoded labeling function. importantly labeling functions overlap conﬂict even dependencies users provide part data programming speciﬁcation approach provides simple framework inputs. independent labeling functions ﬁrst describe model labeling functions label independently given true label class. model labeling function probability labeling object probability labeling object correctly; simplicity also assume class probability model distribution contains labels output labeling functions predicted class. allow parameters vary speciﬁes family generative models. order expose scaling expected loss size unlabeled dataset changes assume note arbitrary constraints changed roughly consistent applied experience users tend write high-accuracy high-coverage labeling functions. ﬁrst goal learn parameters consistent observations—our unlabeled training set—using maximum likelihood estimation. particular training solve problem words maximizing probability observed labels produced training examples occur generative model experiments stochastic gradient descent solve problem; since standard technique defer analysis appendix. noise-aware empirical loss given parameter learning phase successfully found accurately describe training proceed estimate parameter minimizes expected risk linear model feature mapping given deﬁne noise-aware empirical risk ˆαˆβ regularization parameter compute noise-aware empirical risk minimizer fact prove stochastic gradient descent running guaranteed produce accurate estimates conditions describe now. first problem distribution needs accurately modeled distribution family trying learn. assumption encodes idea labeling functions arbitrarily dependent features provide suﬃcient information accurately identify class. third assume algorithm used solve bounded generalization risk parameter theorem suppose data programming solving problems using stochastic gradient descent produce suppose setup satisﬁes conditions suppose number labeling functions size input dataset large enough select simplify statement theorem give reader feel scales respect |s|. full theorem scaling parameter presented appendix. result establishes achieve expected loss parameter estimate error suﬃces labeling functions training examples asymptotic scaling exhibited methods labeled data. means data programming achieves learning rate methods labeled data requiring asymptotically less work users need specify labeling functions rather manually label examples. contrast crowdsourcing setting number workers tends inﬁnity constant dataset grows. results provide explanation experimental results suggest small number rules large unlabeled training eﬀective even complex natural language processing tasks. experience data programming found users often write labeling functions clear dependencies among them. labeling functions added system developed implicit dependency structure arises naturally amongst labeling functions modeling dependencies cases improve accuracy. describe method user specify dependency knowledge dependency graph show system produce better parameter estimates. label function dependency graph support injection dependency information model augment data programming speciﬁcation label function dependency graph directed graph labeling functions edges associated dependency type class dependencies appropriate domain. experience practitioners identiﬁed four commonly-occurring types dependencies illustrative examples similar ﬁxing reinforcing exclusive example suppose functions typically labels also labels disagree labeling actually correct. call ﬁxing dependency since ﬁxes mistakes made typically agree rather disagree would reinforcing dependency since reinforces subset labels modeling dependencies presence dependency information means longer model labels using simple bayesian network instead model distribution factor graph. standard technique lets describe family generative distributions terms known factor function unknown parameter partition function ensures distribution. next describe deﬁne using information dependency graph. additional factors representing dependencies. proceed adding additional factors model dependencies encoded dependency edge factors follows. near-duplicate dependency single factor increases prior probability labels agree. ﬁxing dependency factors −{λi encode idea labels does ﬁxes errors made factors reinforcing dependency same except finally exclusive dependency single factor −{λi learning dependencies solve maximum likelihood problem like learn parameter using results continue noise-aware empirical loss minimizer solving problem order solve problems dependent case typically invoke stochastic gradient descent using gibbs sampling sample distributions used gradient update. conditions similar section provide bound accuracy results. deﬁne conditions now. first must know parameter lies analogous assumptions made section state following analogue we’ll usually reasonably sure guess value even guess using distribution labeling functions actually sampled prove following result accuracy estimates. independent case shows need unlabeled training examples achieve error asymptotic scaling supervised learning methods. suggests computational penalty richer dependency structures less statistically eﬃcient. appendix provide details including explicit description algorithm step size used achieve result. seek experimentally validate three claims approach. ﬁrst claim data programming eﬀective paradigm building high quality machine learning systems test across three real-world relation extraction applications. second claim data programming used successfully conjunction automatic feature generation methods lstm models. finally third claim data programming intuitive productive framework domain-expert users report initial user studies. relation mention extraction tasks relation mention extraction task objects relation mention candidates pairs entity mentions unstructured text goal learn model classiﬁes candidate either true textual assertion relation not. examine news application tac-kbp slot filling challenge extract relations real-world entities articles clinical genomics application extract causal relations genetic mutations phenotypes scientiﬁc literature; pharmacogenomics application extract interactions genes also scientiﬁc literature details included appendix. application collaborators originally built system training programmatically generated ordering labeling functions sequence if-then-return statements candidate taking ﬁrst label emitted script training label. refer if-then-return approach note often required signiﬁcant domain expert development time tune experiments used labeling function sets within framework data programming. experiments evaluated blind hand-labeled evaluation set. table achieve consistent improvements average points score including would winning score tac-kbp challenge observed performance gains across applications diﬀerent labeling function sets. describe labeling function summary statistics—coverage percentage objects least label overlap percentage objects label conﬂict percentage objects conﬂicting labels—and table even scenarios small conﬂict overlap relatively less common still realize performance gains. additionally disease mention extraction task written scratch within data programming paradigm allowing developers supply dependencies basic types outlined sec. point score boost. table labeling function summary statistics sizes generated training sets relative score improvement baseline methods hand-tuned lstm-generated feature sets. automatically-generated features additionally compare hand-tuned automatically-generated features latter learned lstm recurrent neural network conventional wisdom states deep learning methods rnns prone overﬁtting biases imperfect rules used programmatic supervision. experiments however using data programming denoise labels mitigate issue report point boost precision point score improvement benchmark tac-kbp task baseline if-then-return approach. additionally comparison approach point score improvement state-of-the-art lstm approach usability study hopes user without expertise productive iterating labeling functions features. test this arranged hackathon involving handful bioinformatics researchers using open-source information extraction framework snorkel goal build disease tagging system common important challenge bioinformatics domain hackathon participants access labeled training perform feature engineering. entire eﬀort restricted iterative labeling function development setup candidates classiﬁed. eight hours created training model scored within points supervised baseline; mainly recall issue candidate extraction phase. suggests data programming promising build high quality extractors quickly. introduced data programming approach generating large labeled training sets. demonstrated approach used automatic feature generation techniques achieve high quality results. also provided anecdotal evidence methods easier domain experts use. hope explore limits approach machine learning tasks held back lack high-quality supervised datasets including domains imaging structured prediction. acknowledgements thanks theodoros rekatsinas manas joglekar henry ehrenberg jason fries percy liang deepdive ddlite users many others helpful conversations. authors acknowledge support darpa fa---; iis-; nsfccf-; ccf-; darpa fa---; iis-;onr ueb; darpa’s simplex program; oracle; nvidia; huawei; labs; sloan research fellowship; moore foundation; american family insurance; google; toshiba. views conclusions expressed material authors interpreted necessarily representing oﬃcial policies endorsements either expressed implied darpa afrl u.s. government.", "year": 2016}