{"title": "Negative Learning Rates and P-Learning", "tag": ["cs.AI", "cs.LG"], "abstract": "We present a method of training a differentiable function approximator for a regression task using negative examples. We effect this training using negative learning rates. We also show how this method can be used to perform direct policy learning in a reinforcement learning setting.", "text": "present method training differentiable function approximator regression task using negative examples. effect training using negative learning rates. also show method used perform direct policy learning reinforcement learning setting. goal regression analyses regression function function models relationship independent variables dependent variables complex relationship variables exact regression function sought. instead approximate regression function used model relationship. function approximators shown sound method ﬁnding approximate regression functions. function approximates generally trained using example input-output pairs function approximated. given input pair output function approximator compared actual output example. function approximator modiﬁed –possibly gradient descent– output matches output example. training step. function approximator usually modiﬁed vigorously output matches example output perfectly training. extreme modiﬁcation approximator tends result loss forgetting previous examples previous training overwritten strong update. also updates vigorous tend negatively affect approximator’s ability generalize unseen examples. another issue occures function approximator updated parameters model changed. parameters changed quickly overrun computer’s ability represent numbers. sometimes known model explosion prevents function approximator. solve problems updates function approximator attenuated fractional amount conceptualized learning rate. allows approximator pushed desired direction small tunable amount. approximators trained gradient descent learning rate. numerous methods developed automatically compute learning rates. many second derivative parameter change input vector output vector. would like function approximator give correct output target function access example however distance desired output function function dist∗ gives similarity measure still train would like dist∗ training signal train learning rate useful channel distance desired output function training signal. learning rate learning channel take example training assign custom learning rate example. still global learning rate train example learning rate example dist∗. train simple -layer artiﬁcial neural network reproduce sine function interval example points. network uses tanh activation function hidden units. example form drawn uniform distribution interval value learning rate factor example calculated distance desired output function normally training model assume examples drawn target distribution. example learning rate want model match examples completely. instead know training examples target function instead distance desired output dist away want model match examples exactly. want match example partially amount correlated dist∗. distance desired output dist∗ want match completely; example target distribution. distance desired output dist∗ amount want match example down. −dist∗ part formula. also assume tuned global learning rate amount want over. normalize list example learning rate factors give network shown ﬁgure seems trying match target function still pretty bad. learning rates worst examples close don’t learn anything worst examples. really match target distribution need examples given best ones. using negative learning rates. negative learning rates little unusual give good treatment. negative learning rates constrain range range example learning rates examples become worst examples learning rate factor best learning rate factor examples average distance desired output function learning rate factor learn anything them seems make sense intuitively. using squared error loss function calculate gradient backpropagated network. loss function minimum output network matches example output larger value farther away network’s output target output. examples good behavior want; training broken training increases output desired output. however examples negative behavior breaks training. negative examples want stronger training network output close presented output. reduce training farther away network output example output. inverse normal behavior positive examples. looks like solved problems method. compare ﬁgure solution found network trained using learning rates network trained directly examples drawn target function. traditionally trained comparison network exactly architecture network trained using learning rate training channel. presented example inputs number training iterations. solution found virtually identical solution found traditional training. take step back really doing. training function approximator random inputs outputs. give function approximator scalar signal corresponds good given example reinforcement learning. model environment simulate happen take particular action given state. explore model search action increases probability getting rewards. example learner playing checkers learner simulate thousands possible games choose move leads winning games. approach recently used master game method several features distinguish reinforcement learning methods. first model environment. need simulate outcome actions. method totally online. secondly need perform policy evaluation. rather unusual. instead learning environment q-values directly learn policy. q-learning learns q-values methods learns policy call p-learning. good experiences. experiences action-state-reward triplets triplet must pre-back-propagate discounted rewards. propagate backwards discounted reward experience receives reward. receive experience time replace experience’s reward γt−trt. stop reward propagation know previous states affect current state updated. might happen know state start game string games. propagate rewards across games. reward propagation construct training examples example corresponds experience time example looks like train function approximator using gradient descent using p-learning training examples. actions received lead high rewards trained high learning rates. actions received lowest rewards approximator trained avoid using negative learning rates. actions lead average rewards trained; learning rates close zero. please note actions received average rewards learning rate. safely dropped training set. environment leads many rewards close average value speed training signiﬁcantly. example agent plays many games draw winning losing equal number games simple ignore draw games. removing experiences training dataset reward experience within small value mean rewards experiences. tested p-learners simple mouse cliff games agent must move checkers board start tile goal tile avoiding hazard tiles. p-learners learn comparable policies number games q-learners function approximated neural network architecture p-learner’s policy net. paper shown negative examples train function approximates regression tasks. also shown negative learning rates effect training. also introduced method reinforcement learning p-learning directly learns good policy without learning either environment model policy evaluation.", "year": 2016}