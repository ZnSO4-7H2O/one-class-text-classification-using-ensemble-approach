{"title": "Understanding Grounded Language Learning Agents", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "Neural network-based systems can now learn to locate the referents of words and phrases in images, answer questions about visual scenes, and even execute symbolic instructions as first-person actors in partially-observable worlds. To achieve this so-called grounded language learning, models must overcome certain well-studied learning challenges that are also fundamental to infants learning their first words. While it is notable that models with no meaningful prior knowledge overcome these learning obstacles, AI researchers and practitioners currently lack a clear understanding of exactly how they do so. Here we address this question as a way of achieving a clearer general understanding of grounded language learning, both to inform future research and to improve confidence in model predictions. For maximum control and generality, we focus on a simple neural network-based language learning agent trained via policy-gradient methods to interpret synthetic linguistic instructions in a simulated 3D world. We apply experimental paradigms from developmental psychology to this agent, exploring the conditions under which established human biases and learning effects emerge. We further propose a novel way to visualise and analyse semantic representation in grounded language learning agents that yields a plausible computational account of the observed effects.", "text": "neural network-based systems learn locate referents words phrases images answer questions visual scenes even execute symbolic instructions ﬁrst-person actors partially-observable worlds. achieve so-called grounded language learning models must overcome certain well-studied learning challenges also fundamental infants learning ﬁrst words. notable models meaningful prior knowledge overcome learning obstacles researchers practitioners currently lack clear understanding exactly address question achieving clearer general understanding grounded language learning inform future research improve conﬁdence model predictions. maximum control generality focus simple neural network-based language learning agent trained policy-gradient methods interpret synthetic linguistic instructions simulated world. apply experimental paradigms developmental psychology agent exploring conditions established human biases learning effects emerge. propose novel visualise analyse semantic representation grounded language learning agents yields plausible computational account observed effects. learning challenge faced children acquiring ﬁrst words long fascinated cognitive scientists philosophers start making sense language infant must induce structure constant stream continuous visual input slowly reconcile structure consistencies available linguistic observations store knowledge memory apply inform decisions best respond. many neural network models also overcome learning task varying degrees analogous early human word learning. image classiﬁcation tasks imagenet challenge require models induce discrete semantic classes many cases aligned words unstructured pixel representations large quantities photographs visual question answering systems must reconcile images sequences symbols form natural language questions order predict lexical phrasal answers. recently situated language learning agents developed learn understand sequences linguistic symbols terms contemporaneous visual input also terms past visual input actions required execute appropriate motor response advanced agents learn execute range phrasal multi-task instructions green object room pick pencil third room right small green torch continous simulated world. solve tasks agent must execute sequences hundreds ﬁne-grained actions conditioned available sequence language symbols active visual perception surroundings. importantly knowledge acquired agents mastering tasks also permits interpretation familiar language entirely novel surroundings execution novel instructions composed combinations familiar words potential impact situated linguistic agents models grounded language learning systems vast basis human users interact situated learning applications self-driving cars domestic robotic tools. however understanding agents learn behave limited. challenges interpreting factors reasoning behind decisions predictions neural networks well known. indeed concerted body research computer vision natural language processing focused addressing uncertainty. grounded language learning agents become prevalent then understanding learning dynamics representation decision-making become increasingly important inform future research build conﬁdence users interact models. therefore establish better understanding neural network-based models grounded language learning noting parallels research neuroscience psychology aims understand human language acquisition. extending approach ritter adapt various experimental techniques initially developed experimental psychologists line typical experiments humans experimental simulations conducted highly controlled environment simulated world limited objects properties corresponding unambiguous symbolic linguistic stimuli however simplicity generality architecture form inputs model make proposed methods approach directly applicable tasks combine linguistic visual data. using methods explore training environment agent affects learning outcomes speed measure generality robustness understanding certain fundamental linguistic concepts test biases decisions takes trained. further applying layerwise attention novel tool visualising computation grounded language learning models obtain plausible computational account effects terms representation processing. principal ﬁndings canonical grounded language learning architecture following shape colour biases agent exposed shape words colour words training develop human-like propensity assume words refer shape rather colour objects. however training distribution balanced shape colour terms develops analagous bias favour colours. semantic processing representation differences agent learns words different semantic classes different speeds represents features require different degrees visual processing depth compute. experiments take place deepmind simulated world modiﬁed include language channel. agent environment receives textual instructions pencil rewarded satisfying instruction case executing movement actions allow locate pencil move space occupies. timestep episode agent receives continuous pixel tensor visual input symbolic textual instruction must execute movement action. solve tasks receive rewards agent must therefore ﬁrst learn perceive environment actively controlling sees movement head navigate surroundings meaningful sequences actions. typical simulation involves specifying certain aspects environment leaving others determined randomly. instance object identiﬁcation task might wish specify overall layout world range positions objects appear list objects appear position probability appearance rewards associated selecting object. environment engine responsible randomly instantiating episodes satisfy constraints together corresponding language instructions. even detailed speciﬁcation ﬁnite inventory objects properties instruction words tens millions unique episodes agent encounter training involving different object shapes colours patterns shades sizes and/or relative positions. respect goal understanding models grounded language learning simulated environment synthetic language useful asset straightforwardly apply methods behavioural psychologists testing agents respond precisely crafted training test stimuli. figure left schematic agent architecture. right example word learning environment common experiments paper. agent observes rotating objects language instruction must select object matches instruction. case instruction shape word confounding object colours objects selected random vary across agent’s experience word chair. maximum generality simulations involve agent combines standard modules processing sequential symbolic input visual input time step visual input encoded convolutional vision module recurrent language module encodes instruction string mixing module determines signals combined passed lstm action module simply feedforward linear layer operating concatenation output hidden state policy function computes probability distribution possible motor actions state-value function approximator computes scalar estimate agent value function optimisation. estimates expected discounted future return approximating state-value λkrt+k+ state environment time following policy reward received following action performed time represents discount parameter. note architecture simpliﬁed version proposed hermann without auxiliary learning components. weight updates computed according asynchronous advantage actor-critic algorithm conjunction rmsprop update rule training single parameter vector shared across cores offers suitable tradeoff training time loss accuracy asynchronous updates. effect considered instrumental allowing children overcome challenges early word learning human shape bias whereby infants tend presume novel words refer shape unfamiliar object rather than instance colour size texture. simulated environment permits replication original experiment landau designed demonstrate shape bias humans. training agent learns word meanings room containing objects matches instruction word confounding object using method agent taught meaning colour terms shape terms ambiguous terms target referent shape term colour similarly target referent learning colours shape. contrast ambiguous terms always correspond objects speciﬁc colour shape note also colour terms refer range space application gaussian noise prototypical codes instances objects subtly different colours. agent learns periodically measure bias means test episodes learning takes place. test episode agent receives instruction must decide objects whose shape whose colour {ca} whose shape {sa} whose colour note present example neither colour blue shape fork observed agent training. original human experiment degree shape bias agent measured agent learning propensity select preference moreover varying size sets examine effect different training regimes bias exhibited agent. figure illustrates shape/colour bias develops agents exposed three different training regimes. agent taught shape terms colour terms exhibits shape bias whereas colour bias develops agent taught colour words agent taught equal number shape colour terms develops moderate colour bias. effect provides possible explanatory account rapid acceleration word learning observed hermann precisely progressive specialisation agent’s object recognition labelling mechanisms narrows space possible referents permitting faster word learning training progresses. conclusions incorporated ritter observe shape bias convolutional networks trained imagenet challenge training set. experiments single canonical architecture exposed different training stimuli indicate cause effect training data distribution rather convolutional architecture itself. indeed ﬁndings suggest feed-forward convolutional architecture operating image pixels promotes colour rather shape bias. hand typical linguistic environment perhaps extension broad-coverage machine-learning datasets contains many instances shape categories colour categories. interpretation negated sentences tell joke offensive fundamental facet natural language understanding potentially critical artiﬁcial agents receiving instructions human users. despite communicative importance negation challenging acquire figure degrees shape bias different training regimes agent trained shape words readily presumes ambiguous words refer object shape object colour. tendency measured across combinations known confounding objects labels represented blue line. magnitude bias scale mean ‘score’ random test episodes. contrast agent trained colour words exhibits colour bias. interestingly agent trained colour shape words also exhibits colour bias. data show mean standard error across fastest-learning agents different hyperparameter settings sampled random ranges speciﬁed supplementary material explore acquisition negation grounded language learning models designed simulation which before agent placed single room required select objects matching instruction. full training words subset sampled presented agent positive negative forms disjoint subset provided positive forms test whether agent could learn generally applicable notion negation periodically measured ability interpret negated versions instructions illustrated figure agent learned follow positive negative instructions various sets instruction words however unlike linguistic operations adjective-noun modiﬁation agent exhibited difﬁculty generalising notion negation acquired context held-out items difﬁculty acute consisted colour terms split evenly indeed ability generalise negation improved size increased include shapes chance also small interesting difference generalisation negating shape terms colour terms consistent processing differences discussed detail section conjecture negation generalise easily because word corresponding extension objects agent perform perfectly training simply associating instructions form ‘not extension ···. understanding negation would generalise much worse interpretation ‘not involved identifying avoiding object type small training sets results suggest model prefers former interpretation also tendency discover latter generalisable understanding increases negated concepts exposed grows. thus appropriately broad exposure instances negation training neural networks without bespoke biases regularization learn respond effectively negated stimuli pertaining perceptible surroundings. however tailored architectures computational biases required cases agents learn from constrained sets linguistic stimuli. figure problem learning negation language learning agents agent must exposed negative instructions sufﬁciently diverse range contexts order learn useful generalisable notion negation. trained interpret positive commands involving terms negative commands involving terms agent effectively interpret negative commands involving remaining terms. exposed shape terms trained interpret negative commands involving terms agent generalises negation operation effectively still perfectly. idea learning successful simpler things studied complex things basic tenet human education. consensus early exposure simple clear linguistic input helps child language acquisition although unanimous deep learning researchers also reported faster effective learning training examples ordered metric complexity approach referred curriculum learning however robust improvements curriculum learning difﬁcult achieve context text-based learning curriculum learning ordinarily applied training text-based neural language models. recent evidence suggests importance curriculum training greater agents learning conditioned language learning linguistic inputs outputs. hermann observed curricula essential agents learning execute linguistic instructions require resolving referring expressions non-trivial action policies exploration here chose explore curriculum learning controlled grounded language learning agents. trained agent learn meaning shape words conditions. condition agent presented words sampled randomly throughout training. another condition agent presented subset words mastered point subset expanded include words. process iterated subsets size eventually words. shown figure agent followed curriculum learned notably faster overall presented directly large group words. result corroborates importance training curricula grounded language learning agents. moreover unlike effect observed hermann focused tasks requiring agent explore large maze present simulation demonstrates strong curriculum effects simply learning associate objects words. thus development core linguistic semantic knowledge situated grounded agents clearly expedited starting small easy slowly increasing language learning challenge faced agent. figure curriculum training expedites vocabulary growth agent presented stimuli sampled uniformly shape words learns slowly whose stimuli constrained two-word subset agent learns words extended -word subset -word subset strong effect ‘curriculum learning’ observed comparing average reward agents conditions learning words sampled measuring vocabulary size function training episodes many studies aiming understand linguistic processing humans uncovering connections different semantic conceptual domains distinct processing patterns representation. effects emerge behavioural methods neuroimaging neuroscientiﬁc theories memory representation learning developed account effects pursuit better understanding artiﬁcial agents similarly explore links word classes semantic domains patterns learning behaviour processing representation. knowledge could ultimately essential informing process designing architectures capable learning simple language studied here also full range abstract semantic phenomena inherent adult language. word learning speeds order words different types acquired used inform theories child language acquisition human semantic processing generally explore order word learning agent exposed randomly interleaved training instances words different classes multiple shapes. compared rates word learning conditions. controlled condition class restricted exemplars. naturalistic condition class represented different number members faithful reﬂection natural language data. conditions training stimuli sampled random uniformly word types agent naturalistic condition received approximately four times much exposure shape words colour words instance. illustrated figure clear differences speed agent learned words different classes. controlled condition ﬁrst words learned blue diagonal-striped second colour word brown learned around time shapes chair suitcase relative size terms larger smaller. category terms learned shape terms. contrast naturalistic condition variable exposure different word classes seems cause degree specialisation shape words agent learns shape words well acquires colour words. figure words different semantic classes learned different speeds controlled condition agent learns words class. realistic condition class different number members supplementary material layer-wise attention complement behavioural analysis developed method better understanding semantic processing representation agent computational level. method call layer-wise attention involves modifying agent architecture expose processing differences visual features pertinent lexical concept. standard agent distributed representation linguistic input concatenated output layer -layer convolutional visual module timestep multi-layer perceptron passed agent’s core. modify agent learn attend output different layers visual processing module conditioned linguistic input available particular moment. representation language instruction output layer visual module dimension number feature maps. layerwise attention module ﬁrst passed independent linear layers common ﬁnal dimension also length stacked single multiplied passed softmax layer yield dimensional discrete probability distribution locations represented layer visual module values applied multiplicatively representations returned pooling step concatenation. layerwise attention provides measure image locations contain important information agent choosing actions given timestep also level abstraction information useful. insight visualised applying method simonyan propagating probability mass attention distribution back onto input image. figure illustrates effect backpropagating attention probabilities corresponding layer convnet onto visual input. clear visualisations agent exposed shape words learn rely features upper-most layers visual module considering objects surroundings. contrast agent trained interpret colour terms focuses feature detectors lower layers visual module order distinguish objects interest. well established convolutional networks trained classify images also exhibit differential specialisation feature detectors layers layerwise attention provides means quantify magnitude specialisation measure importance layer respect figure representation processing differences colour shape words. ‘dashboards’ interpreting processing agent layerwise attention. large pane left dashboard shows input agent. chart bottom left shows attention distribution ‘locations’ agent’s visual representations. bars show attention locations output lowest layer convnet green bars show attention locations middle layer blue bars show attention locations layer. small windows right side illustrate attention weights propagated back superimposed greyscale copy input image described simonyan agent trained exclusively colour words relies ﬁrst second layers convnet agent trained exclusively shape words uses second upper layer visual features. schematic layerwise attention agent architecture. visualisation space word embeddings weights language module agent trained different word types illustrating words cluster naturally according semantic classes linguistic memory agent. particular linguistic stimuli. also notable conventional visualisation word embeddings input layer provides evidence word-class-speciﬁc processing illustrated figure models capable grounded language learning promise signiﬁcantly advance ways humans intelligent technology interact. study explored situated language learning agent built canonical neural-network components overcomes challenge early language learning. measured behaviour exhibited ﬁrst words simple phrases acquired tested factors speed learning explored aspects language pose particular problems presented technique layerwise attention better understanding semantic visual processing agents. application experimental paradigms cognitive psychology better understand deep neural nets proposed ritter observed convolutional architectures exhibit shape bias trained imagenet challenge data. ability control precisely training test stimuli simulated environment allowed isolate effect deriving training data indeed reach opposite conclusion architecture itself. study also goes beyond ritter exploring abstract linguistic operations studying curriculum effects dynamics word learning. further complement behavioural observations computatoinal analysis representation processing layerwise attention. control precision afforded simulated environment present study made analyses conclusions possible future understanding language learning agents develops essential verify conclusions agents trained naturalistic data. ﬁrst might involve curated sets images videos naturally-occurring text ultimately experiments robots trained communicate perceptible surroundings human interlocutors. world agents capable learning advanced linguistic behaviour would certainly challenging also even crucial understand also learn references stanislaw antol aishwarya agrawal jiasen margaret mitchell dhruv batra lawrence zitnick devi parikh. visual question answering. international conference computer vision charles beattie joel leibo denis teplyashin ward marcus wainwright heinrich k¨uttler andrew lefrancq simon green v´ıctor vald´es amir sadik julian schrittwieser keith anderson sarah york cant adam cain adrian bolton stephen gaffney helen king demis hassabis shane legg stig petersen. deepmind lab. corr abs/. http //arxiv.org/abs/.. yoshua bengio j´erˆome louradour ronan collobert jason weston. curriculum learning. proceedings annual international conference machine learning melissa bowerman soonja choi. shaping meanings language universal language-speciﬁc acquisition semantic categories. language acquisition conceptual development cambridge university press devendra singh chaplot kanthashree mysore sathyendra rama kumar pasumarthi dheeraj rajagopal ruslan salakhutdinov. gated-attention architectures task-oriented language grounding. arxiv preprint arxiv. deng dong richard socher li-jia fei-fei. imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference ieee mandy ghyselinck michael lewis marc brysbaert. acquisition cumulativefrequency hypothesis review literature multi-task investigation. acta psychologica karl moritz hermann felix hill simon green fumin wang ryan faulkner hubert soyer david szepesvari wojtek czarnecki jaderberg denis teplyashin grounded language learning simulated world. arxiv preprint arxiv. george hollich kathy hirsh-pasek roberta michnick golinkoff rebecca brand ellie brown chung elizabeth hennon camille rocroi lois bloom. breaking language barrier emergentist coalition model origins word learning. monographs society research child development alexander huth wendy heer thomas grifﬁths fr´ed´eric theunissen jack gallant. natural speech reveals semantic maps tile human cerebral cortex. nature alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems yann lecun koray kavukcuoglu cl´ement farabet. convolutional networks applications vision. circuits systems proceedings ieee international symposium ieee volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. international conference machine learning simonyan vedaldi zisserman. deep inside convolutional networks visualising image classiﬁcation models saliency maps. workshop international conference learning representations hendrik strobelt sebastian gehrmann bernd huber hanspeter pﬁster alexander rush. visual analysis hidden state dynamics recurrent neural networks. arxiv preprint arxiv. tijmen tieleman geoffrey hinton. lecture .-rmsprop divide gradient running average recent magnitude. coursera neural networks machine learning caiming xiong stephen merity richard socher. dynamic memory networks visual textual question answering. international conference machine learning huijuan kate saenko. attend answer exploring question-guided spatial attention visual question answering. european conference computer vision springer theoretical maximum number time steps agent trained. number time steps action decision number independent workers running replicas environment asynchronous updating. number time steps error backpropagated core lstm action module limit norm gradient across agent network parameters decay term rmsprop gradient averaging function epsilon term rmsprop gradient averaging function learning rate training based linear annealing applied. momentum parameter rmsprop gradient averaging function strength entropy regularisation term cost function. learning rate beginning training annealed linearly reach learning rate ﬁnish train steps. table agent hyperparameters randomly sampled order yield different replicas agents training. uniform indicates values sampled uniformly range loguniform indicates values sampled uniform distribution log-space range words chair suitcase ball balloon zebra cake cassette chair guitar hair-brush ice-lolly ladder pencil suitcase toothbrush bottle cherries fork fridge hammer knife spoon apple banana ﬂower pincer plant saxophone shoe tennis-racket tomato tree wine-glass blue brown pink yellow green cyan magenta white grey purple animals tools plants clothing containers diagonal-striped chequered spotted lighter darker larger smaller", "year": 2017}