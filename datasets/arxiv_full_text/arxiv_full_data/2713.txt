{"title": "An Information-Theoretic Optimality Principle for Deep Reinforcement  Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We methodologically address the problem of Q-value overestimation in deep reinforcement learning to handle high-dimensional state spaces efficiently. By adapting concepts from information theory, we introduce an intrinsic penalty signal encouraging reduced Q-value estimates. The resultant algorithm encompasses a wide range of learning outcomes containing deep Q-networks as a special case. Different learning outcomes can be demonstrated by tuning a Lagrange multiplier accordingly. We furthermore propose a novel scheduling scheme for this Lagrange multiplier to ensure efficient and robust learning. In experiments on Atari games, our algorithm outperforms other algorithms (e.g. deep and double deep Q-networks) in terms of both game-play performance and sample complexity.", "text": "methodologically address problem qvalue overestimation deep reinforcement learning handle high-dimensional state spaces efﬁciently. adapting concepts information theory introduce intrinsic penalty signal encouraging reduced q-value estimates. resultant algorithm encompasses wide range learning outcomes containing deep q-networks special case. different learning outcomes demonstrated tuning lagrange multiplier accordingly. furthermore propose novel scheduling scheme lagrange multiplier ensure efﬁcient robust learning. experiments atari games algorithm outperforms algorithms terms game-play performance sample complexity. reinforcement learning discipline artiﬁcial intelligence seeking optimal behavioral policies enable agents collect maximal reward interacting environment. popular algorithm q-learning operates estimating expected cumulative rewards although successful numerous applications standard q-learning suffers drawbacks. first tabular nature representing q-values readily applicable high-dimensional environments large state and/or action spaces. second initially overestimates q-values introducing bias early stages training bias unlearned training proceeds thus decreasing sample efﬁciency. particularly appealing class approximators deep neural networks learn complex relationships high-dimensional inputs low-level actions. building idea deep q-networks proposed attaining state-of-the-art results large-scale domains e.g. arcade learning environment atari games though successful dqns fail address overestimation problem therefore rather sample-inefﬁcient addressing q-value overestimation introduce intrinsic penalty signal addition instantaneous rewards. intrinsic penalty affects learned q-values eventually leading lower estimates. information theory provides principled method formalize penalty interpreting agent information-theoretic channel limited transmission rate speciﬁcally state environment interpreted channel input action channel output agent’s reward quality information transmission interestingly setting limits transmission rate reﬂect limits information resources agent spend deviate given reference policy. instantaneous deviation agent’s current policy reference policy directly results intrinsic penalty subtracted reward signal. information-theoretic approaches already designed tabular setting readily apply high-dimensional environments require parametric function approximators. since interested improving sample complexity high-dimensional state spaces contribute adapting information-theoretic concepts phrase novel optimization objective learning q-values deep parametric function approximators. resultant algorithm encompasses wide range learning outcomes demonstrated tuning lagrange multiplier. show dqns arise special case proposed approach. contribute introducing dynamic scheduling scheme adapting magnitude intrinsic penalization based temporal bellman error evolution. ultimately allows outperform methods double soft q-learning large margins terms game score sample complexity atari domain. time approach leads decreased q-value estimates conﬁrming hypothesis overestimation leads poor performance practice. finally show performance increase adopting dueling architecture short contributions paper reinforcement learning agent state chooses action sampled behavioral policy πbehave πbehave resulting choice transition successor state unknown state transition model reward quantiﬁes instantaneous performance. subsequent interactions environment goal agent optimize behave maximizes expected cumulative return γtrt] denoting time clearly learn optimal behavioral policy agent reason long term consequences instantaneous actions. q-learning famous algorithm estimates effects using state-action value pairs quantify performance policy. q-learning updates conducted online interaction environment using optimistic overestimation upon careful investigation equation comes recognize q-learning updates introduce bias learning process caused overestimation optimal cumulative rewards speciﬁcally usage maximum operator assumes current guesses q-values reﬂect optimal cumulative rewards. course assumption violated especially early learning process relatively small number updates performed. correlative effect estimations different state-action pairs mistakes tend propagate rapidly q-table unlearned course training. though optimistic bias eventually unlearned convergence speed q-learning highly dependent quality initial q-values. problem optimistic overestimation gets worse high-dimensional state spaces images arcade learning environment. mentioned earlier highdimensional representations handled generalizing tabular q-learning parametric function approximators e.g. deep neural networks learning commences ﬁtting weights approximators using stochastic gradients minimize here expectation refers samples drawn replay memory storing state transitions denotes earlier stage training. intuitively minimization objective equation resembles similarities used tabular setting. again value estimates updated based information introducing max-operator bias. although dqns generalize well wide range input states unaware aforementioned overestimation problem however compared tabular setting problem even severe lack convergence guarantees optimal q-values using parametric approximators inability explore whole state-action space. hence number environmental interactions needed unlearn optimistic bias become prohibitively expensive. potential solution optimistic overestimation qlearning intrinsic penalty instantaneous rewards thus reducing q-value estimates. principled introduce penalty provided framework information theory decision-making. rationale interpret agent information-theoretic channel limited transmission rate environmental state considered channel input learning utility function usually assumed expected cumulative reward i.e. fact similar principles recently received increased attention within policy search identiﬁcation optimal cumulative reward values outlined next paragraphs. policy search information-theoretic principles similar equation categorized three classes depending choice prior policy πprior. ﬁrst class adopts ﬁxed prior remains unchanged learning process. entropy regularisation example special case within class second class makes marginal prior policy obtained averaging behavioral policy environmental states. information-theoretic intuition here encourage agent neglect reward-irrelevant information environment third class assumes adaptive prior ensure incremental improvement steps on-policy settings learning proceeds optimal cumulative reward value identiﬁcation klpenalty directly incorporated q-value estimates rather using regularization. distinct categories value identiﬁcation utilize kl-constraints different ways. ﬁrst category considers restricted class markov decision processes instantaneous rewards incorporate kl-penalty explicitly discourages deviations uncontrolled environmental dynamics. restricted mdps enable efﬁcient optimal value computation outlined second category comprises mdps intrinsic penalty signals similar equation deviations prior policy penalized. optimal values either computed generalized value iteration schemes setting similar q-learning closest work recent approaches worth mentioning apart discrete action high-dimensional state space setting tackle additional problems addressed previously. first consider dynamic adaptation trading rewards versus intrinsic penalties opposed static scheme presented second deploy robust computational approach incorporates value-based advantages ensure bounded exponentiation terms. approach also work utilising entropy reinforcement learning connects policy search optimal cuagent’s action channel output quality information transmission expressed reward utility function according shannon’s noisychannel coding theorem transmission rate upper-bounded average kullback-leibler divergence behavioral policy πbehave arbitrary reference policy support following reference policy denoted prior policy πprior. kl-divergence therefore plays role limited resource exceed maximum intuition behind information-theoretic viewpoint channel aims input output measuring quality mapping terms since transmission rate limited agent discard information little impact obtain utility-maximizing without exceeding transmission limit importantly constraint transmission rate directly translates instantaneous penalty signal leading reduced utility outlined next one-step decisionmaking problem. term πbehave πprior reﬂects instantaneous penalty. constrained optimization problem expressed concave unconstrained objective introducing lagrange multiplier objective leads wide variety learners considered generalization current methods including deep q-networks namely recover approach poses problem optimistic overestimation effectively equation estimates future cumulative rewards using prior policy seen term πprior|s)qθ− special cases recognize formulation allows variety learners steers outcomes limiting cases. note however setting values introduces instead pessimistic bias outlined tabular setting since λ-values introduce pessimistic bias large λ-values optimistic bias must λ-value between encouraging unbiased estimates. unfortunately possible compute closed form propose dynamical scheduling scheme based temporal bellman error evolution next section. note assume ﬁxed prior πprior scheduling another possibility would schedule prior action probabilities instead. latter however practically less convenient compared scheduling scalar. intrinsic penalty signal information-theoretic q-learning algorithms provide principled reducing q-value estimates hence suited addressing overestimation problem outlined earlier. although successful tabular setting algorithms readily applicable high-dimensional environments require parametric function approximators. next section adapt information-theoretic concepts high-dimensional state spaces function approximators demonstrate deep learning techniques emerge special case. reduce optimistic overestimation deep methodologically leveraging ideas informationtheory. since q-value overestimations source sample-inefﬁciency improve large-scale reinforcement learning current techniques exhibit high sample complexity introduce intrinsic penalty signal line methodology forward previous section. commencing however interesting gather insights range possible learners tuning penalty. plugging optimal behavior policy behave equation back equation yields lagrange multiplier steers magnitude penalty signal thus leads different learning outcomes. large little penalization prior policy introduced. such would expect learning outcome mostly considers maximizing utility. conﬁrmed limit robust free energy values dynamic adaptation encourages learning unbiased estimates optimal cumulative reward values. presupposing bounded also bounded limits ﬁrst term represents ordinary maximum operator vanilla deep q-learning. second term logpartition computationally stable elements non-positive exponents result log-partition non-positive subtracts portion reﬂects instantaneous information-theoretic penalty translates penalty cumulative reward values. figure q-values episodic rewards asterix road runner up’n normal dueling architectures. plot shows three pairs graphs reporting outcomes different random seeds black purple double blue information-theoretic approach clearly approach leads lower q-value estimates resulting signiﬁcantly better game play performance. ﬁxed hyperparameter undesirable course training effect intrinsic penalty remains unchanged. since overestimations severe start learning process dynamic scheduling scheme small values beginning larger values towards preferable. rationale here inversely proportional average squared loss. jsquared high average case early episodes training values favored. however jsquared average later training high values suitable learning process. therefore propose adapt running average loss targets predictions. running average javg emphasize recent history opposed samples past since parameters q-value approximator change time. achieved exponential window online update time constant referring window size running average shorthand notation equation running average allows dynamically assign javg squared loss jsquared impeding impact stability deep q-learning parametric approximator deep neural network parameters updated gradients backpropagation. prevent loss values growing large squared loss replaced absolute loss work follow similar approach huber loss jhuber instead huber loss leads robust adaptation uses absolute loss large error values instead squared one. furthermore squared loss sensitive outliers might penalize learning process unreasonable fashion presence sparse large error values. correspond expected cumulative reward taking relevant action state train network iterations iteration corresponds single interaction environment. environment interactions stored replay memory consisting elements. every fourth iteration minibatch size sampled replay memory gradient update conducted discount factor rmsprop optimizer learning rate gradient momentum squared gradient momentum minimum squared gradient rewards clipped target network updated every iterations. time constant dynamically updating hyperparameter prior policy πprior uniform. uniform prior ensures pessimistic baseline case small pessimistic baseline guarantees existence unbiased λ-conﬁgurations scheduling scheme aims detect. agent interacts environment every fourth frame skipped current action repeated skipped frames. training agent follows \u0001greedy policy initialized linearly annealed iterations ﬁnal value training \u0001-annealing start iterations. rgbimages arcade learning environment preprocessed taking pixel-wise maximum previous image. preprocessing images transformed grey scale down-sampled pixels. experiments conducted duplicate different initial random seeds. random number noop-actions beginning game episode compare approach deep q-networks double deep q-networks. addition conduct further experiments replacing network outputs dueling architecture dueling architecture leverages advantage function maxa approximate q-values generalizes learning across actions. results improved game play performance conﬁrmed experiments. training network parameters stored every iterations used ofﬂine evaluation. evaluating single network ofﬂine comprises game play episodes lasting iterations. evaluation mode agent follows \u0001-greedy policy investigate atari games. figure median normalized episodic rewards across atari games normal dueling architectures. plot compares double approach approach leads signiﬁcantly higher median game score architectures. hypothesize addressing overestimation problem results improved sample efﬁciency overall performance. arcade learning environment benchmark evaluate method. compare deep q-networks susceptible overestimations double deep q-networks alternative proposed address precise problem target. results demonstrate proposed method leads signiﬁcantly lower qvalue estimates resulting improved sample efﬁciency game play performance. also show ﬁndings remain valid recently proposed dueling architecture conduct experiments python tensorflow openai extending github project deep convolutional neural network function approximator q-values designed trained according receives input current state environment composed last four video frames. number neurons output layer number possible actions numerical values output neuron note approach also incorporated newly released rainbow framework achieves state-of-the-art results combining several independent improvements past years although focus q-value identiﬁcation work ideas similar apply well actor-critic methods like rewards function training iterations. note episodic rewards smoothed exponential window similar equation preserve clearer view. three games approach leads signiﬁcantly lower q-value estimates compared double both normal dueling architecture time leads signiﬁcant improvements game play performance signiﬁcant improvements sample efﬁciency compared double dqn. instance dins require training iterations road runner compared double dqns standard dqns using normal architecture. improvements also valid dueling setting. normalized episodic rewards enable comparison across atari games taking median normalized score games results analysis depicted figure function training iterations approach clearly outperforms double normal dueling architectures. dueling architecture yields additional performance increase combined din. quantify sample efﬁciency identify minimal number training iterations required attain maximum deep q-network performance. compute average episodic reward figure smoothed exponential window identify approach number training iterations maximum deep q-network performance attained ﬁrst. figure sample efﬁciency asterix road runner up’n normal dueling architectures. three bars game double approach clearly dins sample-efﬁcient architectures three games. figure median sample efﬁciency across atari games normal dueling architectures. plot compares double approach dins achieve signiﬁcantly better median sample efﬁciency types architecture. order assess sample efﬁciency across atari games compute median sampling efﬁciency games figure analysis conﬁrms overall improved sample complexity attained wide range tasks approach compared double dqn. figure episodic rewards asterix beamrider road runner comparing method sql. clearly results show better performance normal dueling architecture without necessity identifying optimal advance. results summarized normal dueling architecture tables respectively. cases approach achieves superior median normalized game performance compared double dqn. normal setting achieves best performance across three approaches games whereas dueling setting achieves best performance games. note conﬁrm dueling architecture combined leads performance increase games however reﬂected median performance. mentioned earlier closest work approach authors consider information theory bridge q-learning policy gradients approach goes considering dynamic adaptation course training introduces robust computation based value advantages. compare method games asterix beamrider up’n down. results depicted figure demonstrate method outperform three games signiﬁcant margins without requirement pre-specifying instance dins achieve best performance iterations road runner game. paper proposed novel method reducing sample complexity deep reinforcement learning. technique introduces intrinsic penalty signal adapting principles information theory high-dimensional state spaces. showed dqns special case proposed approach speciﬁc choice lagrange multiplier steering intrinsic penalty. finally experiments atari games demonstrated technique indeed outperforms competing approaches terms performance sample efﬁciency. results remain valid dueling architecture yielding performance boost. promising direction future work study adaptive prior policies instead ﬁxed ones line could used extend framework multi-task learning scenarios task-speciﬁc policies satisfy kl-constraint prevent deviation common prior. common prior encode behavioral policy generalizes across tasks thus enabling knowledge transfer problems shared latent structure. genewein leibfried grau-moya braun bounded rationality abstraction hierarchical decision-making information-theoretic optimality principle. frontiers robotics grau-moya leibfried genewein braun planning information-processing constraints model uncertainty markov decision processes. proceedings european conference machine learning principles practice knowledge discovery databases haarnoja zhou abbeel levine soft actorcritic off-policy maximum entropy deep reinforcement learning stochastic actor. advances neural information processing systems hessel modayil hasselt schaul ostrovski dabney horgan piot azar silver rainbow combining improvements deep reinforcement learning. proceedings aaai conference artiﬁcial intelligence mnih kavukcuoglu silver rusu veness bellemare graves riedmiller fidjeland ostrovski petersen beattie sadik antonoglou king kumaran wierstra legg hassabis human-level control deep reinforcement learning. nature mnih puigdomenech badia mirza graves lillicrap harley silver kavukcuoglu asynchronous methods deep reinforcement learning. proceedings international conference machine learning wang schaul hessel hasselt lanctot freitas dueling network architectures deep reinforcement learning. proceedings international conference machine learning peng genewein leibfried braun information-theoretic on-line update principle proceedings perception-action coupling. ieee/rsj international conference intelligent robots systems", "year": 2017}