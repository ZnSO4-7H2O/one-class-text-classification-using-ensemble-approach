{"title": "The Parameter-Less Self-Organizing Map algorithm", "tag": ["cs.NE", "cs.AI", "cs.CV"], "abstract": "The Parameter-Less Self-Organizing Map (PLSOM) is a new neural network algorithm based on the Self-Organizing Map (SOM). It eliminates the need for a learning rate and annealing schemes for learning rate and neighbourhood size. We discuss the relative performance of the PLSOM and the SOM and demonstrate some tasks in which the SOM fails but the PLSOM performs satisfactory. Finally we discuss some example applications of the PLSOM and present a proof of ordering under certain limited conditions.", "text": "parameter-less self-organizing neural network algorithm based self-organizing eliminates need learning rate annealing schemes learning rate neighbourhood size. discuss relative performance plsom demonstrate tasks fails plsom performs satisfactory. finally discuss example applications plsom present proof ordering certain limited conditions. ritter al.ritter martinetz schulten] algorithm mapping space another space. learns correct mapping independent operator supervision reward functions seen many neural network algorithms e.g. backpropagation perceptron networks. unfortunately unsupervised learning dependent annealing schemes learning rate neighbourhood size. theoretical basis determining correct type parameters annealing schemes must often determined empirically. generative topographic mapping bishop svens´en williams bishop al.bishop svens´en williams vellido al.vellido el-deredy lisboa] attempt addressing this. furthermore since annealing schemes time-dependent prevent assimilating information training complete. sometimes desirable trait tune know adaptive capabilities organic sensomotor maps inspired several attempts providing better scaling method learning rate and/or neighbourhood size well taking guesswork parameter estimation. attempt done g¨oppert rosenstiel used approximate function approximation used neighbourhood size decay parameter per-node basis. unfortunately applicable cases function approximation requires knowledge desired approximation values function thus losing major advantage som; unsupervised learning. closely related approach would plastic self organising euclidean distance input weight vector winning node used determine whether nodes map. similar growing neural algorithms fritzke] maintains plasticity. another approach self organizing adaptive neighbourhood neural network calculates neighbourhood size input space instead output space like variants. soan tracks accumulated error node scales neighbourhood function accordingly minimum maximum value like algorithms increase decrease number nodes. still leaves several parameters determined empirically user. time adaptive self-organizing shah-hosseini safabakhsh] addresses inability maintain plasticity keeping track dynamic learning rates neighbourhood sizes individual node. neighbourhood size dependent average distance weight vector winning node neighbours learning rate dependent distance weight given node input similar standard som. user still required select several training parameters without theoretical basis. auto-som haese goodhill] uses kalman ﬁlters guide weight vectors towards centre respective voronoi cells input space. automates computation learning rates neighbourhood sizes user still required initial parameters kalman ﬁlters. unfortunately computationally expensive problem increases input size number inputs training set. auto-som also needs keep track previous inputs makes continuous learning diﬃcult increases computational load compute voronoi iteration would increase computational load feasible input probability density distribution known. recent developments self-organisation include self-organizing learning array starzyk liu] noisy self-organizing neural networks reasons introduce parameter-less self-organizing fundamental diﬀerence plsom depends learning rate neighbourhood size decrease time e.g. function number iterations learning algorithm plsom calculates values based local quadratic ﬁtting error input space. allows make large adjustments response unfamiliar inputs i.e. inputs well mapped making large changes response inputs already well adjusted ﬁtting error based normalised distance input weight vector winning node input space. value computed case hence mechanism implemented without inducing noteworthy increases computational load hindering parallelised implementations campbell berglund streit]. section give details plsom algorithm section evaluate performance relative section explain observed behaviour plsom section give examples applications brief discussion aspects plsom relative non-linear mapping problems section section conclusion. mathematical proofs appendix variant examine gaussian-neighbourhood euclidean distance rectangular topology given equations algorithm brief follows input presented network time ’winning node’ i.e. node weight vector closely matches input time selected using equation referred neighbourhood function scaling function centred winning node decreasing directions euclidean distance node winning node node grid. case input/weight distance node distance calculated using distance measure euclidean distance e.g. manhattan distance link distance grid need rectangular. learning rate time neighbourhood size time equations decrease learning rate neighbourhood size respectively important point annealing scheme relies time step number actual ﬁtness network. scaling constants determined beforehand. steps repeated preset condition usually given number iterations measurement error reaches certain level. density nodes input space proportional density input samples however lead undesired results figure several variations algorithm outlined exists e.g. matlab implementation uses two-phased learning algorithm step-based neighbourhood function. fundamental idea plsom amplitude extent weight updates dependent iteration number well plsom input data. determine good calculate scaling variable used scale weight update. scaling variable deﬁned equations best understood normalised euclidean distance input vector time closest weight vector. variable large network input data poorly needs large readjustment. conversely small likely already satisfactory input large update necessary. before distance measure along grid i.e. output space winning node node currently updating. gives value decreases rate decrease determined seen figure weight update functions equations equation learning rate completely eliminated replaced thus size update dependent iteration number. variable aﬀecting weight update carried iterations scaling variable practical experiments indicate reaches maximum value ﬁrst iterations change thereafter. plsom completely eliminates selection learning rate annealing rate annealing scheme learning rate neighbourhood size inconvenience applying soms. also markedly decreases number iterations required stable ordered map. plsom also covers greater area input space leaving smaller along edges. trained matlab variant plsom identical input data number iterations. input data pseudorandom dimensional range. chosen good pseudo-random number generator readily available eliminating need store training data. since training data uniformly distributed input space perfect distribution weight vectors would evenly spaced grid narrow margin along edges input space. weight vector would evenly sized area input space. figure graph decrease uncovered space training progresses plsom matlab implementation. note quick expansion plsom consistently covers larger area variants. summarised area covered cells subtracted total area input space. resulting graph clearly shows plsom spans large part input space small number iterations maintains lead throughout simulation please note quality measure misleading situations cells overlapping typically occur ﬁrst thousand iterations. cell calculate length diagonals cell divide bigger smaller subtract thus getting number inﬁnity represents perfectly square cell. again plsom outperforms early stages simulation iterations surpass plsom. iterations diﬀerence still small however. figure figure graph absolute mean deviation cell size plsom matlab excluding edge cells. compare figure plsom outperforms matlab adaptation time accuracy needs iteration reach level ordering. average cell size idea much cells diﬀer relative size. superior plsom iterations mainly ﬂattened edge cells plsom figure ignore cells along edge picture quite diﬀerent plsom outperforms narrow margin figure illustrations section show positions weight vectors connected lines input space. trained adapt well data outside range training data even small residual learning rate left. illustrated figure presented pseudo-random uniformly distributed -dimensional data vectors range iterations. thereafter presented pseudo-random uniformly distributed -dimensional data vectors range adapted little data. addition adaptation uneven creating huge diﬀerences cell size distorting space spanned weight vectors. subject plsom changes input range diﬀerence quite dramatic; adapts correctly input range almost immediately seen figure opposite case viz. presented sequence inputs restricted small area training input space would preferable maintains original weight vector space order ’forget’ already learned data. figure demonstrates happens plsom trained pseudo-random uniformly distributed -dimensional data range iterations presented inputs conﬁned range iterations. leads increase density weight vectors input space maintains coverage entire initial input space resulting distortions along edge input space. eﬀects pronounced plsom. figure plsom ﬁrst trained inputs ranging iterations shown training iterations inputs ranging note weights higher density input space area still covered i.e. none input space left uncovered. applied plsom variants problem; mapping non-uniformly distributed input space. input space used normal distributed pseudo-random function mean standard deviation values discarded. random seed used experiments initialising weights. variant uses neighbourhood function plsom exponential annealing scheme learning rate neighbourhood size nominated ’plain som’ used comparison. seen figure severely twisted -by- node rectangular grid. size ordinary algorithm must reduced -by- traces twisting removed. altering annealing time solve problem. plsom hand performs well initial size -by- nodes ﬁlling input space figure phenomenon explained looking likelihood given input relation size weight update input result i.e. expected update given input distribution. likelihood input occurring governed gaussian probability density function. likelihood figure plsom iterations normally distributed input mean standard deviation clipped interval. correspondence weight vector density input density weaker topology preserved. compare figure also figure comparing expected update matlab algorithm plsom plsom edge nodes receives larger amount update outside area covered matlab counterpart thus making sure expansion outwards even less jerky. discretising integrated expected displacement gives vector node indicating much direction likely updated given input distribution shown figures plsom seen figure leads corner nodes algorithm expand outwards faster side nodes thus creating warping. plsom side nodes expand outward faster creating initial ’rounded’ distribution weights subsequent inputs pull corners out. also note edge nodes plsom marginally pulled inwards inputs inside weight grid since amount update depends distance input closest node distance node question contributes quicker even expansion. finally weight update functions diﬀerent algorithms give last piece explanation. consider receives input outside area currently mapping already partly annealing therefore partially ordered. figure weight density distance centre plsom. -dimensional input normal distributed mean standard deviation. observe plsom less correlation between input density weight density less variance covers larger area. also figures neighbours winning node receive small update. situation occurs plsom neighbourhood size scale include larger update neighbours winning node thus distributing update along larger number edge nodes. pointed mapping large portion input space preserving skewed distribution possible diﬀerence between length along edges length centre great preserve neighbourhood relations. faced type high-variance input distribution faced choice property sacriﬁce; neighbourhood consistency density equivalence. tries both fails. psom similar algorithms cost ending arbitrary network connections. plsom unique preserving neighbourhood relations pre-deﬁned network. comes cost poorer correspondence input density weight density seen figure application deals processing stereo sound signal presenting plsom determine direction sound source orienting microphones towards sound source. application illustrates physics binaural audition discussed nandy ben-ariehartmann] reinforcement learning early works include huang ohnishi sugiehuang al.huang ohnishi sugie] important distinguish passively determining direction sound sources active audition. active audition aims using movement listening platform pinpoint location biological systems active audition therefore intrinsically robotic problem. active audition works grouped subcategories first applications rely microphones calculate source sound e.g. rabinkin renomeron dahl french flanagan bianchi]. certainly eﬀective observe nature sound receivers suﬃcient. since microphones consume power possible point failure deﬁnite advantages frugal nature respect. secondly methods relying synergy visual auditory cues direction detection notably members humanoid group nakatani okuno kawabata kitano okuno nakadai matsui hidai lourens nakadai al.nakadai lourens okuno kitanolourens al.lourens nakadai okuno kitanonakadai al.nakadai okuno kitano nakadai al.nakadai okuno kitano]. also include neural networks even soms rucci edelman wraynakashima al.nakashima mukai ohnishi]. however known even humans born blind accurately determine direction sounds zwiers ostal cruysberg] interaction vision hearing cannot crucial component learning direction detection. implementation unique rely visual cues specialised hardware predeﬁned acoustic model. learns direction detection correct motor action unsupervised learning interaction environment. also incorporates larger number measures methods made possible som/plsom ability patterns high-dimensional data. figure illustrates procedure. pipelined approach advantage making experimenting diﬀerent processing paths much simpler. also lends approach parallelising hardware software implementations. order train plsom number samples recorded. used white noise samples locations front robot. samples recorded distance source horizontal spacing. training algorithm presents seconds random samples system training steps. sample -dimensional sample presented every sensitises node sound direction distance. latter part training done online robot responding actual sound stationary speaker front initially robot head pointed random direction training progresses robot keeps head steadily pointed towards speaker time random direction picked training continues. approach described consistently manages accuracy around comparable human acuity. precision determined keeping robot stationary registering winning node plsom. moved sound source horizontally winning node stabilised immediate neighbours initial winning cell noting much source moved. relative accuracy method demonstrated figure graphs generated using recordings white noise distance metre. plsom almost free deviation. inverse kinematics problem determining joint parameters robotic limb given position. problem interesting evaluating plsom involves mapping spaces wildly diﬀerent topologies half-torus shaped space euclidean space roughly wedge-shaped space joint parameter space. robot control depends several problems associated existing methods. jacobian pseudo-inverse lets solve problem completely computationally expensive relatively complicated unstable around singularities. jacobian transpose faster simpler particularly accurate move shortest path like pseudo-inverse. methods like cyclic coordinate descent solve unstable singularity problem proposed. also solutions problem using soms ritter martinetz schulten] lets learn joint angles number points space approximation inverse jacobian vicinity point. gives good results relatively nodes relies information stored nodes rather holistically network leading node complicated necessary. clear whether using approximated inverse jacobian lead sort instability around singularities pseudo-inverse jacobian. even method borrows heavily approach seen relation space. trained generating random joint conﬁguration updating plsom weights training complete node labelled manipulator position result applying weight vector joint parameters. allows manipulator positioned accordance following simple algorithm plsom solves problem quickly iteration necessary. input desired target position immediately approximation joint parameters achieve target. however level precision depends number nodes network. demonstration capabilities plsom method robotic programmed play chess itself. plsom nodes trained halftorus shaped area front robot covering small chessboard surrounding table training iterations completed less minutes low-end desktop even relatively nodes error well mechanical error robotic arm. robot able quickly accurately pick place chess pieces shown short video clip order assess diﬀerent methods performed simulation wherein robot moved position another. method allowed iterations complete iteration error calculated. performed test plsom nodes plsom nodes plsom nodes. repeated experiment diﬀerent target positions averaged error. result displayed figure graph plsom’s accuracy related number nodes. noted training time roughly number nodes increasing accuracy computationally expensive. accuracy slightly better reported ritter martinetz schulten] i.e. error compared error although surprising given large diﬀerence number nodes. execution speed measured averaged experiments diﬀerence execution speed node network node network typically less e.g. low-end desktop order test whether plsom handle high-dimensional clustered data selected isolet cole muthusamy fanty] data analysis. data contains suitably processed recordings speakers saying name letters alphabet twice. subdivided training containing speakers test remaining speakers. algorithms seeking solve problem trained training evaluated test set. data present -dimensional real-valued vectors elements al.cole muthusamy fanty] details. best result reported without signal processing achieved backpropagation. k-nearest neighbour methods achieve slow size training forces computation -dimensional euclidean distances classiﬁcation. classiﬁcation clustered data troublesome plsom tries approximate low-dimensional manifold input space case manifold. since plsom unsupervised direct learning eﬀort particular property input plsom tries all. isolet letter property encoded data sorts possible data present speaker speakers’ gender dialect smoker/non-smoker ideally plsom would output dimension dimension embedded manifold impractical case. therefore settled -dimensional plsom nodes. plsom trained random samples training iterations neighbourhood size node labelled input responds node responds input vote performed node labelled input responds frequently. input gets thrice many votes inputs node respond input removed. used classify test set. typically results remaining nodes utilise reference vectors k-nn classiﬁcation. thus plsom seen speeding k-nn algorithm reducing number reference vectors. however come price accuracy achieves accuracy indicated above plsom emphasises maintaining topology modelling input density distribution. many cases disadvantage example highly clustered data. cases however exactly needed useful mapping input space. happens input space nonlinearly mapped space inputs selected. best illustrated example robot navigates square area measuring angles between three uniquely identiﬁed beacons giving -dimensional vector one-to-one relationship location robot space. unfortunately density topology embedded manifold quite diﬀerent input space causing large number samples fall small areas -dimensional input space curvature embedded riemannian manifold high. causes corner corner figure density input space correctly represented topology embedded manifold not. course corrected selecting samples carefully case example since know bidirectional mapping real application would known. would therefore beneﬁcial approach largely ignores distribution input space instead emphasises topology embedded manifold. plsom higher degree figure addressed several problems popular algorithm proposed algorithm solves these; plsom provides simpliﬁcation overall application process since eliminates problems ﬁnding suitable learning rate annealing schemes. plsom also reduces training time. flexibility ordering facilitated shown plsom successfully applied wide range problems. furthermore plsom able handle input probability distributions lead failure albeit comes cost lower correspondence input distribution weight density. plsom converge sense learning rate allowed reach eﬀect achieved simply performing weight updates number inputs. achieved without inducing signiﬁcant computation time increase memory overhead. finally shown plsom guaranteed achieve ordering certain conditions. section present proof guaranteed ordering special case plsom. start establishing lemmas necessary proof examine proof ﬁnally speculate implications proof. proofs assume that implies closer input hence receive larger scaling neighbourhood function. following ordered denote nodes monotonously increasing decreasing means either true. lemma weights node cannot overshoot input i.e. weights node cannot move side input result input. proof assume input node weight amount update equal ǫhci before. since clear |∆wi| wi|. also |∆wi| lemma exists input turn ordered input dimension output dimension unordered one. proof proof contradiction. assume weights ordered wn+. prove input move past move past making unordered update leaves cases wn+. since according lemma node reach node overshoot follows nodes must side weight update before. therefore cannot become unordered. result ordered map. proof since nodes distance input winning node automatically ﬁrst again since nodes distance input amount node updated determined solely lattice distance winning node. therefore move most little less even less resulting ordered map. lemma -dimensional plsom nodes always reach ordered state given suﬃciently large number uniformly distributed inputs. proof proof computer-assisted give outline procedure calculating short proof follows calculate scalar ﬁeld expressing much closer weights attractor point ordered space update positive values indicate weights moved away attractor. given upper bound gradient expected update sample point expected update must point towards attractor vicinity sample point. holds sample points holds whole subspace unordered weights. weights seen coordinates -dimensional space possible conﬁgurations unit cube. subspace spanned constraints denoted figure subspace represents -node unordered unordered states inversions mirrors state figure ordered fulﬁls following conﬁgurations ordered subspace ﬁlls volume drawn figure proof proper introduce concept expected update vector given weight conﬁguration input probability density function compute distance direction node likely move given every point subspace associated expected update position input represented likely position uniformly distributed random input introduce point denoted ordered subspace unit cube attractor dynamic system words expected weight updates bring weight vectors closer attractor hence closer ordered subspace ||.|| l-norm manhattan distance. l-norm chosen produces simpler expression l-norm. found empirically close exact location important proof. equation deﬁnes -dimensional scalar ﬁeld order prove negativity compute upper bound length gradient estimated upper bound must check point figure evolution weight positions -node plsom initialised diﬃcult position. neighbourhood size minimum neighbourhood size simulate happen conﬁguration appears late training force value choosing simpler l-norm. necessary calculations easily performed low-end desktop computer less hours. since distance weight position attractor steadily diminishing follows weight position will given enough consecutive inputs come close enough attractor reach ordered space. whether proof extensible networks nodes dimensional input point uncertain image sequence certainly suggests possibility. kohonen mentions proof cottrell fort pag`escottrell fort]) ordering simpliﬁed based probability ordering input happening inﬁnite number inputs. proof essence relies fact sequence inputs become ordered generates suﬃciently large number inputs probability encountering ordering sequence inputs approach proof presented establishes conﬁguration expected update direction ordering single input. also shows existence ordered attractor dynamical system without satisfy robbins-monro condition. conjecture -dimensional plsom immediate neighbours winning node updated seen chain -node networks subnetwork guaranteed become ordered therefore whole network become ordered. similar proof given albeit authors conﬁdent enough authors would like thank frederic maire smart devices queensland university technology gordon wyeth division complex intelligent systems university queensland valuable input.", "year": 2007}