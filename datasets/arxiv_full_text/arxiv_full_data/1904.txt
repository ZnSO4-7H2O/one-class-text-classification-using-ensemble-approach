{"title": "Generation and Comprehension of Unambiguous Object Descriptions", "tag": ["cs.CV", "cs.CL", "cs.LG", "cs.RO", "I.2.6; I.2.7; I.2.10"], "abstract": "We propose a method that can generate an unambiguous description (known as a referring expression) of a specific object or region in an image, and which can also comprehend or interpret such an expression to infer which object is being described. We show that our method outperforms previous methods that generate descriptions of objects without taking into account other potentially ambiguous objects in the scene. Our model is inspired by recent successes of deep learning methods for image captioning, but while image captioning is difficult to evaluate, our task allows for easy objective evaluation. We also present a new large-scale dataset for referring expressions, based on MS-COCO. We have released the dataset and a toolbox for visualization and evaluation, see https://github.com/mjhucla/Google_Refexp_toolbox", "text": "figure illustration generation comprehension system. left system given image region interest; describes touching head unambiguous right system given image expression candidate regions selects region corresponds expression. discriminative nature task referring expressions tend detailed image captions. finally easier collect training data cover space reasonable referring expressions given object whole image. consider problems description generation must generate text expression uniquely pinpoints highlighted object/region image description comprehension must automatically select object given text expression refers object prior work literature focused exclusively description generation golland consider generation comprehension process real world images. paper jointly model tasks description generation comprehension using state-of-the-art deep learning approaches handle real images text. specifically model based upon recently developed methods combine convolutional neural networks recurrent neural networks demonstrate model outperforms baseline generates referring propose method generate unambiguous description speciﬁc object region image also comprehend interpret expression infer object described. show method outperforms previous methods generate descriptions objects without taking account potentially ambiguous objects scene. model inspired recent successes deep learning methods image captioning image captioning difﬁcult evaluate task allows easy objective evaluation. also present large-scale dataset referring expressions based mscoco. released dataset toolbox visualization evaluation https//github.com/ mjhucla/google_refexp_toolbox. introduction recent interest generating text descriptions images however fundamentally problem image captioning subjective ill-posed. many valid ways describe given image automatic captioning methods thus notoriously difﬁcult evaluate. particular decide sentence better description image another? paper focus special case text generation given images goal generate unambiguous text description applies exactly object region image. description known referring expression approach major advantage generic image captioning since well-deﬁned performance metric referring expression considered good uniquely describes relevant object region within context listener comprehend description recover location original object. addition expressions without regard listener must comprehend expression. also show model trained semi-supervised fashion automatically generating descriptions image regions. able generate comprehend object descriptions critical number applications natural language interfaces controlling robot interacting photo editing software addition good test performing research area vision language systems existence useful objective performance measure. summarize main contributions follows. first present large scale dataset referring expressions. second evaluate existing image captioning methods perform referring expression task. third develop method joint generation comprehension outperforms current methods. related work referring expressions. referring expression generation classic problem important issues include understanding types attributes people typically describe visual objects usage higher-order relationships phenomena underspeciﬁcation also related speaker variance context plays critical role several ways first speaker must differentiate target object collection alternatives must thus reason object differs context. second perception listener also valuable. particular golland recently proposed game theoretic formulation referring expression problem showing speakers optimally respect explicit listener model naturally adhere gricean maxims communication previous work authors focused small datasets computer generated objects connected text generation systems real vision systems. however recent interest understanding referring expressions context complex real world images humans tend generate longer phrases kazemzadeh ﬁrst collect large scale dataset referring expressions complex real world photos. dataset. however beyond expression generation jointly learn generation comprehension models. prior works explicitly enumerate attribute categories size color manually list possible visual phrases deep learningbased models able learn directly generate surface expressions images without ﬁrst convert formal object/attribute representation. concurrently propose cnn-rnn based method similar baseline model achieve state-ofthe-art results referit dataset discriminative training strategy proposed full model. investigate task generating dense descriptions image. descriptions required unambiguous. image captioning. methods inspired long line inquiry joint models images text primarily vision learning communities modeling perspective approach closest recent works applying rnns cnns problem domain main approach papers represent image content using hidden activations feed input trained generate sequence words. papers image captioning focused describing full image without spatial localization. however aware exceptions. propose attention model able associate words spatial regions within image; however still focus full image captioning task. propose model aligning words short phrases within sentences bounding boxes; train model generate short snippets given features bounding box. model similar baseline model described section however show approach good full model takes account potentially confusing regions image. visual question answering. referring expressions related task particular referring expression comprehension turned task speaker asks question where image red? system must return bounding however philosophical practical differences tasks. referring expression communication problem speaker ﬁnding optimal communicate listener whereas work typically focuses answering questions without regard listener’s state mind. additionally since questions tend open ended evaluating answers hard general image captioning whereas figure sample images google refexp dataset. green indicate object descriptions refer since dataset based coco access original annotations object mask category. objects hard describe e.g. third image ﬁrst need distinguish reﬂection mirror. kept otherwise discarded re-annotated another worker. repeated description generation veriﬁcation tasks mechanical turk iteratively three times. selected expressions. object average expressions image average expressions. dataset denoted google refexp dataset samples shown figure released dataset toolbox visualization evaluation https//github.com/ mjhucla/google_refexp_toolbox. collecting dataset learned tamara berg independently applied referit game mscoco dataset generate expressions objects images. kindly shared data brevity call google refexp dataset g-ref unc-ref-coco unc-ref. report results datasets paper. however differences collection methodologies found descriptions overlapped datasets exhibit signiﬁcant qualitative differences descriptions unc-ref dataset tending concise contain less ﬂowery language descriptions. speciﬁcally average lengths expressions dataset uncref respectively. size word dictionaries dataset unc-ref respectively. figure visual comparisons. according personal communication authors unc-ref dataset instruction reward rule unc-ref encourages annotators give concise description limited time g-ref dataset encourage annotators give rich natural descriptions. leads different styles annotations. largest existing referring expressions dataset know referit dataset collected contains expressions referring distinct objects photographs natural scenes. images dataset segmented annotated expansion imageclef iapr dataset drawbacks dataset however images sometimes contain object given class allowing speakers short descriptions without risking ambiguity imageclef dataset focuses mostly stuff rather things image selected objects between instances object type within image bounding boxes occupy least image area. resulted selecting objects images. constructed mechanical turk task presented object image worker whose task generate unique text description object. used second task different worker presented image description asked click inside object referred selected point inside original object’s segmentation mask considered description valid box) must generate referring expression target object. formally task compute argmaxsp sentence region image. since rnns represent generate word time generate sentence symbol. computing globally probable sentence hard beam search approximately probable sentences similar standard image captioning task except input region instead full image. main difference train model generate descriptions distinguish input region candidate regions. description comprehension task given full image referring expression asked localize object referred within image returning bounding box. approach would train model directly predict bounding location given referring expression however paper adopt simpler ranking-based approach. particular ﬁrst generate region proposals system rank probability. select region using argmaxr∈cp where bayes’ rule test time multibox method generate objects proposals. generates large number class agnostic bounding boxes. classify ms-coco categories discard scores. resulting post-classiﬁcation boxes proposal upper bound performance also ground truth bounding boxes objects image. cases label object interest ranking proposals. implies equally likely choose region describe. approximately true virtue constructed dataset. however real applications region saliency taken account. baseline model similar image captioning models represent image followed lstm generate text main difference augment representation whole image representation region interest addition location information. figure illustration baseline model. detail vggnet pretrained imagenet dataset last dimensional layer vggnet used representation object region. addition compute features whole image serve context. experiments ﬁne-tuned weights last layer ﬁxed layers. feed region keep aspect ratio region ﬁxed scale resolution padding margins mean pixel value gives -dimensional feature vector region image. follows region using dimensional vector lstms -dimensional word-embedding space -dimensional hidden state vector. adopt commonly used vanilla lstm structure feed visual representation input lstm time step. maximum likelihood training training data consists observed triplets image denotes region within denotes referring expression train baseline model minimize negative probability referring expressions given respective region image main intuition behind training want consider whether listener would interpret sentence unambiguously. penalizing model thinks referring expression target object could also plausibly generated object within image. thus given training sample train model outputs high maintaining whenever note stands contrast maximum likelihood objective function equation directly maximizes without considering objects image. several ways select region proposals could true object bounding boxes tends waste time objects visually easy discriminate target object alternative select true object bounding boxes belonging objects class target object; confusable finally multibox proposals test time select ones predicted object labels compare different methods section random negatives step data given image memory. optimize equation must replicate network region shown figure resulting trained model exactly number parameters trained model optimization regularization strategy section thus difference objective function. objective call max-margin intuitively captures similar effect softmax counterpart show section yields similar results practice. however since maxmargin objective compares regions network must replicated twice. consequently less memory used sentence allowing sentences loaded minibatch turn helps stabilizing gradient. semi-supervised training figure illustration train full model using softmax loss function. target region incorrect regions. weights lstms cnns shared parameters examples training set. ordinary stochastic gradient decent batch size initial learning rate halved every iterations. gradient norms clipped maximum value combat overﬁtting regularize using dropout ratio word-embedding output layers lstm. baseline method train model maximize common cnn-lstm based image captioning models. however strategy directly generates expression based target object calls reﬂex speaker strategy) drawback fail generate discriminative sentences. example consider figure generate description girl highlighted green bounding generating word pink useful since distinguishes girl girl right. propose modiﬁed training objective described below. discriminative training table measure precision unc-ref validation data. different training model. columns show performance ground truth multibox proposals ground truth generated descriptions. thus columns descriptions evaluate performance comprehension system columns descriptions evaluate performance generation system. comprehension task easy evaluate simply compute intersection union ratio true predicted bounding box. exceeds call detection true positive otherwise false positive average score images. generation task difﬁcult evaluate generated description image description using metrics cider bleu meteor however metrics unreliable account semantic meaning. rely instead human evaluation done recent image captioning competition particular asked amazon mechanical turk workers compare automatically generated object description human generated object description presented image object interest. workers know sentences human generated computer generated simply judge sentence better description equally good. addition human evaluation scale evaluate entire system passing automatically generated descriptions comprehension system verifying correctly decoded original object interest. end-to-end test automatic much reliable standard image captioning metrics. comparing different training methods section compare different ways training model maximum likelihood training max-margin loss easy ground truth negatives max-margin loss hard ground truth negatives max-margin loss hard multibox negatives softmax/mmi loss hard multibox negatives method consider using either ground truth multibox proposals test time. figure ilustration semi-supervised training process. text details. descriptions thus ubiquitously available. main intuition bounding useful even without accompanying description because allows penalize model training generates sentence cannot decode correctly recover higher whenever semi-supervised setting consider small dataset dbb+txt images bounding boxes descriptions together larger dataset images bounding boxes without descriptions. dbb+txt train model compute model generate descriptions bounding boxes retrain dbb+txt dbb+auto spirit bootstrap learning. strategy suffers generated sentences reliable pollute training set. handle this train ensemble different models dbb+txt determine generated sentences dbb+auto trustworthy. particular apply model ensemble decode sentence dbb+auto keep sentence every model maps correct object; call resulting veriﬁed dataset dﬁltered. ensures generator creates referring expressions understood variety different models thus minimizing overﬁtting. figure illustration. experiments show model beneﬁts semi-supervised training. conducted experiments coco referring expression datasets mentioned section g-ref dataset unc-ref dataset. randomly chose objects validation objects testing remaining objects training evaluation metrics experiment treat unc-ref validation explore various algorithmic options hyperparameter settings mmi. ﬁxed algorithmic options hyperparameter settings experiments g-ref dataset reduces risk overﬁt hyperparameters particular dataset. results summarized table draw following conclusions models perform better generated descriptions groundtruth ones possibly generated descriptions shorter groundtruth and/or generation comprehension models share parameters even generator uses word incorrectly comprehension system still decode correctly. intuitively model might communicate better using language others. variants full model work better strong baseline using maximum likelihood training. softmax version training similar training beneﬁts hard negatives training ground truth negatives helps using ground truth proposals using multibox proposals better multibox negatives. section compare strong baseline max-margin method validation test sets g-ref unc-ref. before consider ground truth multibox proposals test time ground truth generated descriptions. results shown table training outperforms training every setting. addition end-to-end evaluation human evaluators judge generated sentence quality. particular selected objects random test showed amazon mechanical turk workers. percentage descriptions evaluated better equal human caption baseline full model respectively. shows training much better training. conduct semi-supervised training experiment separate training g-ref dataset uncref dataset parts number objects. ﬁrst part object description annotations second part object bounding boxes. table shows results semi-supervised training validation dataset unc-ref. improvement training dbb+txt using dbb+txt. also train baseline full model random train test split w.r.t. images g-ref dataset. results consistent table multibox proposals descriptions precision baseline full model test respectively. figure sample results description generation using full model strong baseline descriptions generated full model discriminative generated baseline. figure sample results description comprehension task using full model. ﬁrst second column shows original image multibox proposals. third sixth columns show results model input arbitrary description object image. bounding denotes probable object predicted model blue dashed ones denote bounding boxes within margin probable one. descriptions groundtruth ones dataset customized descriptions model typically longer discriminative baseline model. second image example baseline describes cats laying sufﬁciently unambiguous listener understand described. full model hand describes laying figure shows qualitative results full comprehension model test dataset. ﬁrst second columns show original image multibox proposals respectively. last four columns show bounding boxes selected full model response different input sentences better interpret results also show bounding boxes within margin model dashed blue bounding boxes. bounding boxes considered possible candidates scores high chosen one. general comprehension model quite well short word phrases longer descriptions. able respond correctly single word changes referring expression also correctly identiﬁes horse referent expression dark horse carrying woman whereas woman referent woman dark horse note methods average word embeddings would likely fail example. however also failure cases. example ﬁfth woman white selects woman black; model cannot handle case object present although makes reasonable guess. also ﬁfth controller woman’s hand selects woman orange juice controller since particular kind object small detect lacks enough training data. conclusions conclude leave reader simple points. first referring expressions studied decades light recent burst interest image captioning referring expressions take importance. image captioning difﬁcult evaluate referring expressions objective performance metric require semantic understanding language vision. thus success datasets contributed paper meaningful success standard image captioning metrics. second successful generating descriptions must consider listener. experiments show modeling listener must correctly decode generated description consistently outperforms model simply emits captions based region features. hope addition dataset insights spur progress joint models vision language. grateful tamara berg sharing uncref-coco dataset. also thank sergio guadarrama vivek rathod vignesh ramanathan nando freitas rahul sukthankar oriol vinyals samy bengio early discussions feedback drafts. work partly supported center brains minds machines award ccf-.", "year": 2015}