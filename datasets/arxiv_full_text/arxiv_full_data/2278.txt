{"title": "Kernel Robust Bias-Aware Prediction under Covariate Shift", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Under covariate shift, training (source) data and testing (target) data differ in input space distribution, but share the same conditional label distribution. This poses a challenging machine learning task. Robust Bias-Aware (RBA) prediction provides the conditional label distribution that is robust to the worstcase logarithmic loss for the target distribution while matching feature expectation constraints from the source distribution. However, employing RBA with insufficient feature constraints may result in high certainty predictions for much of the source data, while leaving too much uncertainty for target data predictions. To overcome this issue, we extend the representer theorem to the RBA setting, enabling minimization of regularized expected target risk by a reweighted kernel expectation under the source distribution. By applying kernel methods, we establish consistency guarantees and demonstrate better performance of the RBA classifier than competing methods on synthetically biased UCI datasets as well as datasets that have natural covariate shift.", "text": "covariate shift training data testing data differ input space distribution share conditional label distribution. poses challenging machine learning task. robust bias-aware prediction provides conditional label distribution robust worstcase logarithmic loss target distribution matching feature expectation constraints source distribution. however employing insufﬁcient feature constraints result high certainty predictions much source data leaving much uncertainty target data predictions. overcome issue extend representer theorem setting enabling minimization regularized expected target risk reweighted kernel expectation source distribution. applying kernel methods establish consistency guarantees demonstrate better performance classiﬁer competing methods synthetically biased datasets well datasets natural covariate shift. standard supervised machine learning data used evaluate generalization error classiﬁer assumed independently drawn distribution generates training samples. assumption independent identically distributed data partially violated covariate shift setting conditional label distribution shared source target data distribution input variables differs source target samples i.e. psrc differs ptrg. models trained assumptions suffer covariate shift provide overly optimistic extrapolation generalizing data intuitive traditional method address covariate shift importance weighting tries de-bias objective loss function weighting instance ratio ptrg/psrc. however importance weighting results high variance predictions also provides generalization performance guarantees strong conditions source target data distributions assumes worst case conditional label distribution requires source feature expectation matching constraints. approach provides conservative target predictions target distribution sufﬁcient statistical support source data. statistical support deﬁned choice source statistics features. classiﬁer tries make prediction certainty target distribution small possible feature matching constraints prevent fully. result less restrictive feature constraints produce less certain predictions target data resulting classiﬁer. shown figure limited features classiﬁer allocate certainty portions source distribution target distribution density small satisfy source feature expectation matching constraints leaving much uncertainty portions target distribution. hand restrictive features constraining conditional label distribution classiﬁer produces better model data gives informative predictions less target entropy logloss figure relation inspires contribution leveraging kernel methods provide higher dimensional features classiﬁer without introducing proportionate computational burden. according representer theorem minimizer regularized empirical loss reproducing kernel hilbert space represented linear combination kernel products evaluated training data. model parameters obtained estimating coefﬁcients linear combination. however robust bias-aware classiﬁcation framework objective function dual problem regularized expected logarithmic loss target data distribution. cannot computed explicitly using data labeled target samples unavailable. meanwhile distribution discrepancy evaluating risk function sampling training data prevents applying representer theorem directly. quantitative form representer theorem proposed holds continuous case minimizer distribution—rather discrete samples—is sought. minimizer regularized expected risk represented expectation probability distribution instead linear combination training data. utilize result extend representer figure performance comparison robust bias aware classiﬁer using ﬁrst-order features ﬁrst-order third-order features labeled source data samples source target distributions data drawn shown. colormap represents predicted probability intersection source distribution target distribution better predicted third-order features much uncertain using ﬁrst moment features. corresponding target logloss entropy shown. theorem prediction covariate shift setting. show minimizer regularized expected target risk represented reweighted kernel expectation source distribution. enables apply kernel methods robust bias aware classiﬁer. paper explore theoretical foundation kernel methods robust covariate shift prediction. investigate underlying effect brought kernelization establish consistency properties realized applying kernel methods prediction. demonstrate empirical advantages kernel robust bias aware classiﬁer synthetically biased benchmark datasets well datasets natural covariate shift bias. address shifts training testing input distributions known covariate shift existing methods often reweight source samples denoted ˜psrc make representative target distribution samples. theoretical argument supporting approach reweighting asymptotically optimal minimizing target distribution loss existing covariate shift research follows idea seeking unbiased estimator target risk signiﬁcant attention paid estimating density ratio ptrg/psrc strongly impacts predictive performance direct estimation methods estimate density ratio minimizing information theoretical criterion like kl-divergence matching kernel means rather estimating ratio’s numerator denominator densities separately. methods consider ratio inner parameter within model relate ratio model misspeciﬁcation. also methods speciﬁc models covariate shift mechanism additional methods also recently proposed address limitations importance weighting theoretical analyses uncovered brittleness importance weighting covariate shift analyzing statistical learning bounds cortes establish generalization bounds learning importance weighting covariate shift hold second moment sampled importance weights bounded epsrc/psrc)] bounded small number data points large importance weights dominate reweighted loss resulting high variance predictions. kernel methods employed estimating density ratio importance weighting methods; example kernel mean matching uses core idea kernel mean reproducing kernel hilbert space source data close reweighted target data optimal density ratio obtained minimizing difference. kernel methods also served bridge source target domains broader transfer learning domain adaptation problems. approaches kernel methods used project source data target data latent space distance distributions small minimized existing applications kernel methods covariate shift orthogonal approach based empirical risk minimization formulations assumption source data could somehow transformed match target distributions. differs substantially robust approach. robust bias-aware classiﬁer robust bias-aware classiﬁcation model based minimax robust estimation framework framework estimator player ﬁrst chooses conditional label distribution minimize logloss adversarial player chooses label distribution statistic-matching conditional probability maximize logloss settings known robust loss minimization equivalent dual empirical risk minimization perspective prediction modiﬁes dual robust loss minimization problem contrast existing importance weighting methods modify primal empirical risk minimization problem. distinguishing source target distributions using logarithmic loss robust optimization problem reduces maximum entropy problem deﬁnes conditional probability simplex must reside within vector-valued feature funcˆ tion evaluated input epsrc] vector expected feature values corresponds feature function approximated using source sample data practice. solving parametric form optimization problem yields provides parameter vector estimates accomplished approximating gradient using source samples rather approximating objective function plugging gradient using source samples illustrated figure feature function predictor forms constraints prevent high levels uncertainty target distribution. result extensive sets feature constraints needed appropriately constrain model provide certain predictions portions input space target data probable source distribution like intersection source target distribution figure extended representer theorem kernel methods motivated approach provide sufﬁciently restrictive constraints forces generalization source data samples target data. however inability directly apply empirical risk minimization approach complicates incorporation since kernel method applications often empirical risk minimization starting point. regularized expected target loss instead represented reweighted expectation source distribution. paves theoretical foundation applying kernel methods essentially differs traditional empirical risk minimization based methods expected target loss starting point. theorem input space output space positive deﬁnite real valued kernel corresponding reproducing kernel hilbert space training samples drawn testing samples minimizer deﬁning conditional label distribution function minimizes regularized expected loss. positive deﬁnite real valued kernel according generalized representer theorem expected risk case minimizer takes form since using gaussian kernels bandwidth using polynomial kernels order using polynomial kernels order ellipses show source target data distribution figure intersection source distribution target distribution better predicted kernel methods applied. corresponding logloss entropy evaluated target distribution shows certain informative predictions produced kernel rba. kernel parameter estimation non-kernelized model objective function deﬁned terms labeled target distribution data unavailable. however parametric model’s form bypasses difﬁculty employing kernelized minimizer order estimate parameters derive gradient kernel predictor. corollary gradient regularized expected loss obtained approximating kernel evaluations source distribution source sample kernel evaluations. corollary indicates computation gradient requires source samples. requires approximation source distribution’s expected kernel evaluations empirical evaluations sample mean. reason approximation rooted idea minimizing exact expected target loss directly kernel rba. consequently need empirical gradient approximate true gradient. however error controlled using standard ﬁnite sample bounds like hoeffding bounds corresponding error objective also bounded. contrary importance weighted empirical risk minimization methods approximate gradient approximate training objective beginning essentially different method. understanding kernel order illustrate effectiveness kernel consider datasets figure compare linear kernel different kernel types parameters figure even though kernel methods usually regarded introduce non-linearity main effect kernel expansion constraint space adversarial player player game figure kernel achieves better informative predictions intersection source target distribution true decision boundary linear one. note gaussian kernel large bandwidth obtain linear decision boundary better visualization. moreover difference target entropy logarithmic loss gradually gets smaller last three ﬁgures. corresponds property target logarithmic loss always upper bounded target entropy proven general case previous literature therefore larger number constraints imposed i.e. kernel methods applied forms restrictive constraint target entropy bound target loss tightly. note choice kernel method kernel parameters depends speciﬁc learning problem also need account overﬁtting issues practice. amount bias also plays role source constraints brought kernel methods help improve method. speciﬁcally larger bias suffer insufﬁcient constraints source sample data results larger entropy target predictions. figure convergence decision boundary robust bias aware classiﬁer using linear features samples using gaussian kernels samples samples samples noise example. ellipses show source target data distribution closely overlap. tiled line shows true decision boundary. increasing number samples universal kernels true decision boundary recovered accuracy gradually converging optimal. consistency analysis analyze theoretical properties kernel method. stated before kernel directly minimizes regularized expected target loss. start deﬁning expected target loss explicitly parameterized learned speciﬁc data point lrba psrc ptrg normalization term. theorem bounded universal kernel regularization tending zero slower kernel method parameter eptrg] resulting predictor eptrg] a.s.−−→ proof. lrba lipschitz loss follows basic form logistic loss except consists component density ratio. given theorem minimizer expected target lrba represented using source samples. implies kernel consistent w.r.t eptrg equipped universal kernel source data assuming psrc ptrg accurate according consistency properties lipschitz loss next explore whether optimal expected lrba target distribution eptrg] indicates optimal loss target distribution. corollary pair distributions psrc ptrg psrc ptrg kernel predictor satisfying conditions theorem eptrg eptrg a.s.−−→ proof. lrba proper composite loss binary multi-class cases means satisﬁes lrba−lrba bayes conditional label probability estimated label probability function constant. target expected regret bounded expected lrba regret predictor function maps conditional label probability label. ﬁrst inequality property plug-in classiﬁers jensen’s inequality second inequality directly comes deﬁnition proper loss. therefore according theorem kernel consistent w.r.t lrba conclude eptrg eptrg a.s.−−→ note employing universal kernel sufﬁcient condition consistency hold. therefore kernel methods provide larger number features without increasing computational burdens also facilitate theoretical property hold kernel rba. demonstrate true decision boundary target distribution recovered increasing number samples source target distribution fairly close figure shown ﬁrst ﬁgure decision boundary linear case tilted noise. equipped samples universal kernel decision boundary shifted align true one. time accuracy target data gets better better roughly converging optimal. property kernel corresponds corollary loss kernel converge optimal loss limit. comparison show plots logloss accuracy kernel kernel robust methods repeated experiments using increasing number samples figure dataset similar example figure noise source target distribution closely overlapped. kernel used gaussian kernel. shown error bars even though importance weighted loss converges target loss limit theory suffers larger variance sensitivity noise reality limited number samples. reason dominated data large ptrg/psrc weights like points labels right-upper corner figure noise points push decision boundary left-bottom direction order suffer less logloss. hand kernel robust robust noise keeps reducing variance improving mean logloss accuracy. inherently modest predictions robust methods produce biased target distribution also consistency property enjoys stated theorem corollary even though number samples still small limited here source target distribution close enough reﬂect convergence tendency increasing source samples. figure logloss accuracy plots sample size increases kernel kernel robust methods gaussian kernel datasets similar figure error shows conﬁdence interval sampling distribution repeated experiments. methods suffer large variance robust methods gradually reduce variance improves logloss accuracy consistently. section demonstrate advantages kernel approach datasets either synthetically biased sampling naturally biased differing characteristic noise. chose three datasets repository synthetically biased experiments based criteria contains approximately examples minimal missing values. vehicle segment sat. dataset synthetically generate separate experiments taking source samples target data samples generally following sampling procedure described huang summarize randomly sample target portion target dataset; source portion calculate sample mean sample covariance sample proportion weights generated multivariate gaussian source dataset. dimension large sample points perform ﬁrst ﬁrst several principle components obtain weights. also investigate three naturally biased covariate shift datasets. abalone variable create bias. speciﬁcally infant source samples rest target samples. note simpliﬁed -category classiﬁcation problem abalone dataset described clark also sample data points respectively source target datasets. chose data variable makes source-target separation easier reasonable allows covariate shift assumption generally hold. addition evaluate methods mnist dataset reduc binary predictive tasks differentiating versus versus biased gaussian noise mean standard deviation testing data form covariate shift i.e. noise randomly sample training testing samples repeat experiments times. shown figure comparison batch training samples testing samples. methods evaluate approach methods kernel robust bias aware classiﬁer adversarially minimizes target distribution logloss using kernel methods trained using direct gradient calculations corollary kernel logistic regression ignores covariate shift maximizes source data conditional likelihood maxθ epsrcp exp) exp)) regularization constant. kernel importance weighting method maximizes conditional target data likelihood estimated using importance weighting density ratio maxθ epsrcp linear robust bias aware prediction adversarially minimizes target distribution logloss without utilizing kernelization i.e. ﬁrst order features used trained using direct gradient calculations linear logistic regression utilizes ﬁrst order features source conditional likelihood maximization. linear importance weighting method uses ﬁrst order features maximize reweighted source likelihood. model selection kernelized method employ polynomial kernel order choose regularization parameter -fold cross validation importance weighted cross validation apply traditional cross validation kernel apply iwcv importance weighting methods robust methods. note traditional cross validation process correct anymore covariate shift setting covariate shift assumption source marginal data distribution different target distribution though iwcv originally designed importance weighting methods proven unbiased loss function. apply perform model tuning robust methods even though error estimate variance could large. logistic regression density estimation discriminative density estimation method leverages logistic regression classiﬁer estimating density ratios. according bayes rule psrc ptrg where second ratio computed ratio number target source examples ﬁrst obtained training classiﬁer source data labeled class target data another class. similar ideas also appears recent literature resulting density ratio method also closely controlled amount regularization. also choose regularization weight cross validation. method table perform paired t-test among pair methods. indicate methods best performance bold along methods statistically indistinguishable best shown table average first logloss kernel robust robust bounded uniform distribution baselines methods arbitrary worse bias large like vehicle. aligns properties robust methods bias large density ratio becomes small results uniform predictions. indicates robust methods preferred robustness safety concern amount covariate shift large. secondly kernel robust consistently improves performance robust kernelization harm methods like sat. reason implicit assumption source features generalize target distribution hold anymore incorporating larger dimensions features could make predictions worse. kernel robust robust even though overﬁtting could still concern density ratio could adjust certainty prediction function like regularizer based data’s density training testing distribution suffer less overﬁtting. finally kernel robust improvement robust related source input distributions target input distribution. natural bias abalone comes feature variable could smaller bias synthetic data. could improvement logloss abalone smaller datasets. providing meaningful robust predictions covariate shift challenging. kernel methods avenue considering large inﬁnite feature spaces without incurring proportionate computational burden. investigated underlying theoretical foundations applying kernel methods extending generalized representer theorem makes possible represent minimizer regularized expected loss reweighted kernel expectations source distribution therefore minimize objective using gradient calculations depend source samples. addition presented implication kernel providing restrictive feature matching constraints tighter entropy bounds target loss demonstrated kernel consistent w.r.t expected target loss loss. experimentally validated advantages kernelized synthetically subsampled benchmark data naturally biased data.", "year": 2017}