{"title": "Machine Learning that Matters", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field?s energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.", "text": "much current machine learning research lost connection problems import larger world science society. perspective exist glaring limitations data sets investigate metrics employ evaluation degree results communicated back originating domains. changes needed conduct research increase impact has? present impact challenges explicitly focus ﬁeld’s energy attention discuss existing obstacles must addressed. inspire ongoing discussion focus matters. time another encounter friend spouse parent child concerned citizen upon learning work machine learning wonders what’s good for? question phrased subtly elegantly matter form gets motivational underpinnings work invest years professional lives machine learning research? diﬀerence make world large? much machine learning research inspired weighty problems biology medicine ﬁnance astronomy etc. growing area computational sustainability seeks connect advances real-world challenges environment economy society. calo project aimed integrate learning reasoning desktop assistant potentially impacting everyone uses computer machine learning eﬀecyet still observe proliferation published papers evaluate algorithms handful isolated benchmark data sets. real world experiments operate data originated real world results rarely communicated back origin. quantitative improvements performance rarely accompanied assessment whether gains matter world outside machine learning research. phenomenon occurs widespread emphasis training graduate student researchers review process submitted papers connecting advances back larger world. even rich assortment applications-driven research often fails take ﬁnal step translate results impact. many machine learning problems phrased terms objective function optimized. time question larger scope ﬁeld’s objective function? seek maximize performance isolated data sets? characterize progress meaningful measures concrete impact machine learning innovations? short position paper argues change view relationship machine learning science paper contain algorithms theorems experiments results. instead seeks stimulate creative thought research large relatively unaddressed issue underlies much machine learning ﬁeld. contributions work clear identiﬁcation description fundamental problem frequent lack connection machine learning research larger world scientiﬁc inquiry humanity suggested ﬁrst steps towards addressing issuance relevant impact challenges machine learning community identiﬁcation several obstacles machine learning section highlights aspects research conducted today limit impact larger world. goal point ﬁngers critique individuals instead initiate critical self-inspection constructive creative changes. problems trouble work common enough merit eﬀort eliminating them. argument also theory versus applications. theoretical work inspired real problems applied work can. criticisms focus instead limitations work lies between theory meaningful applications algorithmic advances accompanied empirical studies divorced true impact. increasingly papers describe algorithm follow standard evaluation template. presenting results synthetic data sets illustrate certain aspects algorithm’s behavior paper reports results collection standard data sets available archive survey non-cross-conference papers published icml reveals possible advantages using familiar data sets include enabling direct empirical comparisons methods greater ease interpreting results since data properties widely studied understood. however practice direct comparisons fail standard reproducibility. experiments vary methodology implementations reporting. interpretations almost never made. this? first meaningful interpretations hard. virtually none researchers work data sets happen also experts relevant scientiﬁc disciplines. second insidiously ﬁeld neither motivates requires interpretation. reviewers inquire classes well classiﬁed common error types were even particular data sets chosen. expectation authors report whether observed improvement performance promises real impact original domain. even authors forged collaboration qualiﬁed experts little paper space devoted interpretation require archive tremendous impact ﬁeld machine learning. legions researchers chased best iris mushroom classiﬁer. ﬂurry eﬀort seem impact ﬁelds botany mycology. scientists disciplines even need classiﬁer? publish subject journals? even agreement community role data sets serve less utility synthetic data since control process generated them fail serve real world data disassociation real world context forgotten chosen ignore data matrix numbers. further existence archive tended over-emphasize research eﬀort classiﬁcation regression problems expense problems informal discussions researchers suggest also de-emphasized need learn formulate problems deﬁne features leaving young researchers unprepared tackle problems. trend going least years. jaime carbonell editor machine learning wrote standard irvine data sets used determine percent accuracy concept classiﬁcation without regard performance larger external task change trend next years? want also problems measure performance. often abstract evaluation metric etc.) used. metrics abstract explicitly ignore remove problem-speciﬁc details usually numbers compared across domains. seemingly obvious strategy provide useful information? recognized performance obtained training model data reﬂect performance data sets drawn problem i.e. training loss underestimate test loss strategies splitting training test sets cross-validation estimate expected performance trained applied future data however metrics tell nothing impact diﬀerent performance. example accuracy iris classiﬁcation might suﬃcient botany world classify poisonous edible mushroom intend ingest perhaps accuracy required. assumption crossdomain comparability mirage created application metrics range meaning. suites experiments often summarized average accuracy across data sets. tells nothing useful generalization impact since meaning improvement diﬀerent diﬀerent data sets. related problem persistence bake-oﬀs mindless comparisons among performance algorithms reveal little sources power effects domain characteristics receiver operating characteristic curves used describe system’s behavior range threshold settings rarely accompanied discussion performance regimes relevant domain. common practice reporting area curve several drawbacks including summarizing performance possible regimes even unlikely ever used weighting false positives false negatives equally inappropriate given problem domain such insuﬃciently grounded meaningfully measure impact. methods statistics t-test commonly used support conclusion whether given performance improvement signiﬁcant not. statistical signiﬁcance function numbers; compute real-world signiﬁcance. course know this rarely inspires addition separate measure signiﬁcance. often instead t-test result serves ﬁnal punctuation experimental utterance easy oﬃce weka algorithm data downloaded web. hard identify problem machine learning oﬀer solution determine data collected select extract relevant features choose appropriate learning method select evaluation method interpret results involve domain experts publicize results relevant scientiﬁc community persuade users adopt technique truly made diﬀerence researcher might well feel fatigued daunted contemplating list activities. however necessary component research program seeks real impact world outside machine learning. ﬁeld imposes additional obstacle impact. generally speaking activities middle figure considered publishable community. innovative applications artiﬁcial intelligence conference international conference machine learning applications exceptions. international conference machine learning experimented invited applications track accepted mainstream paper icml machine learning journal machine learning research authors must demonstrate machine learning contribution often narrowly interpreted reviewers development algorithm explication novel theoretical analysis. excellent laudable advances unless equal expectation bottom figure little incentive connect advances outer world. reconnecting active research relevant real-world problems part process maturing research ﬁeld. rest world visible advances contributions larger realm science human endeavors. rather following letter machine learning reignite spirit? simply matter reporting isolated applications. needed fundamental change formulate attack evaluate machine learning research projects. ﬁrst step deﬁne select evaluation methods enable direct measurement wherever possible impact innovations. addition traditional measures performance measure dollars saved lives preserved time conserved eﬀort reduced quality living increased focusing metrics impact help motivate upstream restructuring research eﬀorts. guide select data sets structure experiments deﬁne objective functions. minimum publications report given improvement accuracy translates impact originating problem domain. reader wonder accomplished goal develop general methods apply across domains. common approach using metric domains relies unstated usually unfounded assumption possible equate improvement domain another. instead method yield proﬁt improvements year auto-tire business well avoidance unnecessary surgical interventions year demonstrated powerful wide-ranging utility. many investigations involve domain experts collaborators help deﬁne problem label data classiﬁcation regression tasks. also provide missing link performance plot signiﬁcance problem domain. help reduce number cases system perfectly solves sub-problem little interest relevant scientiﬁc community system’s performance appears good numerically insuﬃciently reliable ever adopted. could also solicit short comment papers accompany publication advance authored researchers relevant domain expertise uninvolved research. could provide independent assessment performance utility impact work. additional beneﬁt informs communities well methods work. raising awareness interest buy-in ecologists astronomers legal experts doctors etc. lead greater opportunities machine learning impact. finally consider potential impact selecting research problems tackle merely interesting challenging perspective. many people species countries square meters would impacted solution problem? level performance would constitute meaningful improvement status quo? warrick provides example work tackles three aspects. working doctors clinicians developed system detect fetal hypoxia enable emergency intervention literally saves babies brain injuries death. publishing results demonstrated ability detected fetal hypoxia cases early enough intervention acceptable false positive rate currently working clinical trials next step towards wide deployment. many examples exist. paper seeks inspire more. direct research eﬀorts articulate ambitious meaningful challenges. carbonell articulated list challenges ﬁeld increase impact instead back machine learning included impact guiding principle challenges range widely along axis. improved chess player might arguably lowest real-world impact medical diagnosis system active could impact many human lives. challenges seek capture entire process successful machine learning endeavor including performance infusion impact. diﬀer existing challenges darpa grand challenge netﬂix prize yahoo learning rank challenge focus single problem domain particular technical capability. goal inspire ﬁeld machine learning take steps needed mature valuable contributor larger world. much eﬀort often chasing goals system outperforms human task. impact challenges paper also diﬀer sort goal human-level performance gold standard. matters achieving performance suﬃcient make impact world. analogy consider sick child rural setting. neighbor runs miles fetch doctor need achieve olympic-level running speed long doctor arrives time address sick child’s needs imagine machine learning researcher motivated tackle problems widespread interest impact. obstacles success foresee? eliminating advance? jargon. issue endemic specialized research ﬁelds. vocabulary familiar diﬃcult even detect we’re using specialized term. consider handful examples feature extraction bias-variance tradeoﬀ ensemble methods cross-validation low-dimensional manifold regularization mutual information kernel methods. basic concepts within create conceptual barriers used glibly communicate others. terminology serve barrier domain experts general public even closely related ﬁelds statistics explore develop ways express ideas general terms even better terms already familiar audience. example feature extraction termed representation; notion variance instability; cross-validation also known rotation estimation outside regularization explained choosing simpler models; terms precise likely understood conversation subtleties ensue. risk. even system more less prone error human performing task relying machine feel riskier raises concerns. errors made assign culpability? level ongoing commitment system designers adjustments upgrades maintenance? concerns especially acute ﬁelds medicine spacecraft ﬁnance real-time systems exactly settings large impact possible. increased sphere impact naturally also increases associated risk must address concerns hope infuse real systems. complexity. despite proliferation toolboxes libraries ﬁeld matured point researchers areas simply apply problem choice attempts often fail lack knowledge phrase problem features search parameters etc. reason said solutions come packaged ph.d.; requires sophistication graduate student beyond successfully deploy solve real problems—and ph.d. needed maintain update system deployment. evident strategy scale goal widespread impact. simplifying maturing robustifying algorithms tools abstract activity help erode obstacle permit wider independent uses machine learning oﬀers cornucopia useful ways approach problems otherwise defy manual solution. however much current research suﬀers growing detachment real problems. many investigators withdraw private studies copy data work isolation perfect algorithmic performance. publishing results community process. successes usually communicated back original problem setting form used. opportunities real impact widespread. worlds ﬁnance politics medicine education stand beneﬁt systems analyze adapt take action. paper identiﬁes examples impact challenges several real obstacles hope inspiring lively discussion best make diﬀerence. aiming real impact increase satisfaction rest world notice recognize value adopt solutions. thank dietterich terran lane baback moghaddam david thompson three insightful anonymous reviewers suggestions paper. work performed sabbatical propulsion laboratory.", "year": 2012}