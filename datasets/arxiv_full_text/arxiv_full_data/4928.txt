{"title": "Multi-step Reinforcement Learning: A Unifying Algorithm", "tag": ["cs.AI", "cs.LG"], "abstract": "Unifying seemingly disparate algorithmic ideas to produce better performing algorithms has been a longstanding goal in reinforcement learning. As a primary example, TD($\\lambda$) elegantly unifies one-step TD prediction with Monte Carlo methods through the use of eligibility traces and the trace-decay parameter $\\lambda$. Currently, there are a multitude of algorithms that can be used to perform TD control, including Sarsa, $Q$-learning, and Expected Sarsa. These methods are often studied in the one-step case, but they can be extended across multiple time steps to achieve better performance. Each of these algorithms is seemingly distinct, and no one dominates the others for all problems. In this paper, we study a new multi-step action-value algorithm called $Q(\\sigma)$ which unifies and generalizes these existing algorithms, while subsuming them as special cases. A new parameter, $\\sigma$, is introduced to allow the degree of sampling performed by the algorithm at each step during its backup to be continuously varied, with Sarsa existing at one extreme (full sampling), and Expected Sarsa existing at the other (pure expectation). $Q(\\sigma)$ is generally applicable to both on- and off-policy learning, but in this work we focus on experiments in the on-policy case. Our results show that an intermediate value of $\\sigma$, which results in a mixture of the existing algorithms, performs better than either extreme. The mixture can also be varied dynamically which can result in even greater performance.", "text": "kristopher asis* kldeasisualberta.ca fernando hernandez-garcia* jfhernanualberta.ca zacharias holland* ghollandualberta.ca richard sutton rsuttonualberta.ca reinforcement learning artiﬁcial intelligence laboratory university alberta edmonton canada unifying seemingly disparate algorithmic ideas produce better performing algorithms longstanding goal reinforcement learning. primary example elegantly uniﬁes one-step prediction monte carlo methods eligibility traces trace-decay parameter currently multitude algorithms used perform control including sarsa q-learning expected sarsa. methods often studied one-step case extended across multiple time steps achieve better performance. algorithms seemingly distinct dominates others problems. paper study multi-step action-value algorithm called uniﬁes generalizes existing algorithms subsuming special cases. parameter introduced allow degree sampling performed algorithm step backup continuously varied sarsa existing extreme expected sarsa existing generally applicable onoff-policy learning work focus experiments on-policy case. results show intermediate value results mixture existing algorithms performs better either extreme. mixture also varied dynamically result even greater performance. temporal-difference methods important concept reinforcement learning combines ideas monte carlo dynamic programming methods. methods allow learning occur directly experience absence model environment’s dynamics like monte carlo methods also allowing estimates updated based learned estimates without waiting ﬁnal result like dynamic programming. core concepts methods provide ﬂexible framework creating variety powerful algorithms used prediction control. number control methods proposed. q-learning arguably popular considered off-policy method policy generating behaviour policy learned different. sarsa classical on-policy control method behaviour target policies same. however sarsa extended learn off-policy importance sampling expected sarsa extension sarsa that instead using action-value next state update value current state uses expectation subsequent action-values current state respect target policy. expected sarsa studied strictly on-policy method paper present general version used onoff-policy learning also subsumes q-learning. methods often described simple one-step case also extended across multiple time steps. spectrum algorithms created. exists monte carlo methods other exists one-step learning. middle spectrum intermediate methods perform better methods either extreme concept eligibility traces also applied control methods sarsa q-learning create efﬁcient learning produce better performance multi-step methods usually thought terms average many multi-step returns differing lengths often associated eligibility traces case however also natural think terms individual n-step returns associated nstep backup refer individual backups atomic backups whereas combination several atomic backups different lengths creates compound backup. existing literature clear best extend one-step expected sarsa multi-step algorithm. tree-backup algorithm originally presented method perform off-policy evaluation behaviour policy non-markov non-stationary completely unknown paper re-present tree-backup natural multi-step extension expected sarsa. instead performing updates entirely sampled transitions multi-step sarsa treebackup performs update using expected values actions transition. algorithm ﬁrst proposed sutton barto uniﬁes generalizes existing multi-step control methods. degree sampling performed algorithm controlled sampling parameter extreme exists sarsa exists tree-backup intermediate values create algorithms mixture sampling expectation. work show intermediate value outperform algorithms exist either extreme. addition show varied dynamically produce even greater performance. limit discussion atomic multi-step case without eligibility traces natural extension make compound backups avenue future research. furthermore generally applicable onoff-policy learning initial empirical study examined on-policy prediction control problems. framework agent environment interact sequence discrete time steps every time step agent receives information environment’s state possible states. agent uses information select action possible actions based behavior agent state environment agent receives reward moves another state state-transition probability agent behaves according policy probability distribution process policy iteration agent learns optimal policy maximizes expected discounted return algorithms strive maximize expected return computing value functions estimate expected future rewards terms elements environment actions agent. state-value function expected return agent state follows policy deﬁned control time focus estimating action-value function expected return agent takes action state following policy deﬁned step size parameter. update rules also known backup operations transfer information back future states current one. common visualize backup operations using backup diagrams ones depicted figure tabular solution methods used state action spaces small enough possible maintain estimates value functions array table. state space large and/or continuous approximate solution methods need used combine algorithms kind function approximation scheme. simplicity present algorithmic ideas backup length parameter greater creates atomic multi-step algorithm like one-step methods deﬁned error atomic multi-step algorithm characterized n-step return. atomic multi-step sarsa n-step return learning requires certain amount exploration behaving greedily respect estimated optimal policy often infeasible. therefore agents often trained \u0001-greedy policies agent chooses optimal action probability behaves randomly probability nevertheless learning optimal policy possible done off-policy. agent learning off-policy behaves according behavior policy learning target policy achieved using another control method expected sarsa. contrast sarsa expected sarsa behaves according behavior policy updates estimate taking expectation actions time according target policy convenience expected action-value deﬁned methods presented previous section generalized even bootstraping longer time intervals. shown decrease bias update cost increasing variance nevertheless many cases possible achieve better performance choosing value update action-values states remain i.e. qt+n qt+n−∀ update rule applicable offpolicy n-step sarsa generally useful form atomic multi-step algorithms well. paper tabular solution methods also extended function approximation thus serve also approximate solution methods. experiments study problems require tabular solution methods problem requires approximate solution method using function approximation. ﬁrst states actions sampled according behaviour policy n-step sarsa last state backed according expected action-value target policy. make n-step expected sarsa entirely off-policy importance sampling ratio term also introduced needs omit last time step. resulting update would would ρt+n− n-step return n-step expected sarsa drawback using importance sampling learn offpolicy create high variance must compensated using small step sizes; slow learning next section present method also generalization expected sarsa learn off-policy without importance sampling. shown return n-step expected sarsa calculated taking expectation actions last step backup. however possible extend idea every time step backup taking expectation every step resulting algorithm multi-step generalization expected sarsa known tree-backup characteristic backup diagram expected sarsa subsumes q-learning treebackup also thought multi-step generalization q-learning target policy greedy policy respect action-value function. tree-backup several advantages n-step expected sarsa. tree-backup capacity learning off-policy without need importance sampling. effect reducing variance speeding learning. additionally importance sampling ratio need computed behavior policy need stationary markov even known branch tree represents action main branch represents action taken time value branches value qt+n corresponding whereas value segment main branch reward corresponding time step. n-step return values branch weighted product probabilities actions leading branch multiplied corresponding power discount term. clarity easier previous sections incrementally introduced several generalizations control methods sarsa expected sarsa section present algorithm uniﬁes called sarsa generalized atomic multi-step algorithm using n-step return n-step sarsa generalizes off-policy algorithm importance sampling. contrast expected sarsa learn off-policy without need importance sampling generalizes atomic multi-step algorithms tree-backup nstep expected sarsa. algorithms presented broadly categorized families backup actions samples like sarsa; consider expectation actions backup like expected sarsa tree-backup. section introduce method unify families algorithms introducing parameter possibility unifying sarsa tree-backup ﬁrst suggested precup ﬁrst formulation presented sutton barto intuition behind based idea choice update estimate based action sampled possible future actions based expectation possible future actions. example n-step sarsa sample taken every step backup whereas tree-backup algorithm expectation taken instead. however choice sampling expectation remain constant every step backup. furthermore backup time step could based weighted average sampling expectation. order implement this parameter introduced control degree sampling step backup. thus error represented terms weighted important note every control method presented thus obtained varying sampling parameter obtain sarsa obtain expected sarsa tree-backup every step backup except last obtain n-step expected sarsa. intermediate values create entirely algorithms exist somewhere full sampling pure expectation. furthermore need remain constant throughout every episode even every time step episode continuing task. could varied dynamically function time current state measure learning progress. particular could also varied function episode number investigate experiments. potentially variety effective schemes choosing varying would subject research. section explore performance different atomic multi-step algorithms presented. first evaluate performance prediction task motivates uniﬁcation sarsa expected sarsa also introduce idea dynamically varying take advantage algorithm’s particular performance characteristics. then gridworld navigation problem show possible improve performance using intermediate dynamically varying value increasing length backup. finally investigate performance approximate solution method problem continuous state space. -state random walk -dimensional environment agent randomly transitions neighboring states. terminal state environment transitioning gives reward transitioning gives reward compare algorithms involve taking expectation based policy task formulated state actions. action deterministically transitions neighboring states agent learns on-policy equiprobable random behavior policy. differs typical random walk setups state action randomly transition either neighboring state resulting state values identical. figure -state random walk results. plot shows performance terms error value function. results average runs standard errors less best initial performance best asymptotic performance dynamic outperformed ﬁxed values learning algorithm estimate value function behavior policy. conducted experiment comparing various algorithm instances assessing different multi-step backup lengths step sizes degrees sampling. root-mean-square error estimated value function analytically computed values measured episode. instance parameter setting episodes results averaged across runs. figure shows results found representative best parameter setting instance task. sarsa better initial performance poor asymptotic performance tree-backup poor initial performance better asymptotic performance intermediate degrees sampling traded initial asymptotic performances. motivated idea dynamically decreasing towards take advantage initial performance sarsa asymptotic performance treebackup. accomplish decreased factor episode. dynamically varying outperformed ﬁxed degrees sampling. figure windy gridworld described sutton barto start goal states denoted respectively. numbers underneath column denote strength upward wind experienced agent moves column. four possible moves right left down. agent moves middle columns gridworld affected upward wind shifts resultant next state upwards number cells varies column column. figure shows layout windy gridworld along strengths wind column. agent edge world selects move would cause leave grid would pushed world wind simply replaced nearest state edge world. time step agent receives constant reward goal reached. variation windy gridworld results choosing action deterministic; called stochastic windy gridworld. layout actions wind strengths windy gridworld described above time step probability next state results picking action determined random states currently surrounding agent. conducted experiment stochastic windy gridworld consisted runs episodes evaluate performance various instances different parameter combinations. instances algorithms behaved learned according \u0001-greedy policy performance measure compared average return episodes. results summarized figure values tested choosing resulted greatest performance; higher lower values decreased performance. overall dynamic performed best close second. figure stochastic windy gridworld results. plot shows performance terms average return episodes function step size various values results selected values connected straight lines average runs. standard errors less line width. -step algorithms performed better -step equivalents dynamic performed best overall. implemented variant classical episodic task mountain described sutton barto implementation rewards actions goal remained same. however agent ever ventured past leftmost mountain would fall cliff rewarded returned random initial location valley hills. named environment mountain cliff. environments tested showed trend results. however results obtained mountain cliff pronounced suitable demonstration purposes. state space continuous approximated using tile coding function approximation. speciﬁcally used version sutton’s tile coding software tilings asymmetric offset consecutive numbers tile taking fraction feature space gives resolution approximately algorithm conducted independent runs episodes each. training done on-policy \u0001-greedy policy optimized average return episodes different values step size parameter backup length results correspond best-performing parameter combination algorithm. omit n-step expected sarsa results performance much figure mountain cliff environment. goal agent drive past without falling cliff. agent receives reward every time step falling cliff returns random initial location valley reward figure shows return episode averaged runs. smooth results computed right-centered moving average window successive episodes. observed atomic multi-step sarsa fairly similar performance. among atomic multistep methods static tree-backup best performance. nonetheless dynamic outperformed algorithms using static order gain insight nature results summarized average return episodes table standard error lower upper conﬁdence interval bounds provided validate signiﬁcance results. summaries calculated based runs. average return episodes could interpreted measure fast algorithm learn whereas average return episodes shows well algorithm capable learning. evidenced table obtained best performance ﬁrst episodes dynamic close second. however episodes dynamic managed outperform algorithms. experiments evident merit unifying space algorithms prediction tasks -state random walk varying degree sampling results trade-off initial asymptotic performance. control tasks stochastic windy gridworld intermediate degrees sampling capable achieving higher per-episode avertable summaries initial ﬁnal performance atomic multi-step algorithms mountain cliff environment. standard error lower upper conﬁdence interval bounds provided validate results. best initial performance whereas dynamic best ﬁnal performance. updating based pure expectation. results prediction control problems showed intermediate ﬁxed degree sampling outperform methods exist extremes addition presented simple dynamically adjusting outperformed ﬁxed degree sampling. presentation limited atomic multistep case without eligibility traces conducted experiments on-policy problems investigated simple method dynamically varying leaves open several avenues future research. first could extended eligibility traces compound backups. second performance could evaluated off-policy problems. third schemes dynamically varying could investigated perhaps function state recently observed rewards measure learning progress. authors thank vincent zhang harm seijen doina precup pierre-luc bacon insights discussions contributing results presented paper entire reinforcement learning artiﬁcial intelligence research group providing environment nurture support research. gratefully acknowledge funding alberta innovates technology futures google deepmind natural sciences engineering research council canada. figure mountain cliff results. plot shows performance atomic multi-step algorithm terms average return episode. right-centered moving average window successive episodes employed order smooth results. dynamic best performance among algorithms. ﬁndings also extend tasks continuous state spaces mountain cliff. evidenced results table intermediate values allow higher initial performance whereas small values allow better asymptotic performance. shown figure table dynamic able exploit beneﬁts adjusting time. moreover experiments stochastic windy gridworld task demonstrated possible improve performance choosing higher value backup length parameter comparing algorithms backup length greater noticed different families algorithm experienced different effective length. example discounting n-step sarsa’s effective backup length equal backup length however effective backup length tree-backup less depending stochasticity direct result product term thus algorithms length backup effective length equal. explore idea further could subject research.", "year": 2017}