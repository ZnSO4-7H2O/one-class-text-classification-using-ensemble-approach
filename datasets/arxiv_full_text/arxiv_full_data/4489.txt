{"title": "L1 regularization is better than L2 for learning and predicting chaotic  systems", "tag": ["cs.LG", "cs.AI"], "abstract": "Emergent behaviors are in the focus of recent research interest. It is then of considerable importance to investigate what optimizations suit the learning and prediction of chaotic systems, the putative candidates for emergence. We have compared L1 and L2 regularizations on predicting chaotic time series using linear recurrent neural networks. The internal representation and the weights of the networks were optimized in a unifying framework. Computational tests on different problems indicate considerable advantages for the L1 regularization: It had considerably better learning time and better interpolating capabilities. We shall argue that optimization viewed as a maximum likelihood estimation justifies our results, because L1 regularization fits heavy-tailed distributions -- an apparently general feature of emergent systems -- better.", "text": "emergent behaviors focus recent research interest. considerable importance investigate optimizations suit learning prediction chaotic systems putative candidates emergence. compared regularizations predicting chaotic time series using linear recurrent neural networks. internal representation weights networks optimized unifying framework. computational tests diﬀerent problems indicate considerable advantages regularization considerably better learning time better interpolating capabilities. shall argue optimization viewed maximum likelihood estimation justiﬁes results regularization heavy-tailed distributions apparently general feature emergent systems better. interested learning representing predicting emergent phenomena. general view emergent dynamics brought nature references therein) also made constructs references therein) show chaotic behavior exhibit heavy-tailed distributions. turn interested learning representing predicting chaotic processes. processes could considerable importance neurobiology. according common view brain apply chaotic systems represent control nonlinear dynamics study concerns identiﬁcation chaotic dynamics using recurrent linear neural networks particular family artiﬁcial neural networks considering anns sparsiﬁed representations considerable research interest shown reasonable success representing natural scenes underlying concept choose representation derive input using least number components compresses information eﬃciently. theoretical point view assumption seen variant occam’s razor principle references therein) provided component identical priori probabilities. recent review neurobiological relevance sparse representations numerical studies indicate creation pruning connectivity matrices anns advantageous decrease learning time increase generalization capabilities encouraging experimental studies joined representational weight sparsiﬁcations also undertaken generative networks thorough review weight pruning methods references therein. weight sparsiﬁcation connected structural risk minimization method provides measure generalization versus overﬁtting. structural risk minimization also related certain regularization methods references therein). intend demonstrate advantages sparsiﬁcation. sake simplicity shall deal linear systems. simpliﬁcations allowed make joined framework treat sparse neural activities sparse neural connectivity sets equal footing. framework constructed compare performances norms. chosen ‘battleﬁeld’ identiﬁcation chaotic time series. highly non-linear problem real challenge linear schemes. turn also good identiﬁcation achieved using linear approaches. paper organized follows basic concepts recurrent neuronal network studied introduced section mathematical formalism approximations described section iii. section reviews numerical experiments. connections information theory discussed section conclusions drawn last section. neural architectures reconstruction abilities shall considered. particular formalism concerns elman type recurrent networks belong family recurrent neural networks review rnns e.g. references therein. elman network input neurons hidden neurons output neurons. state activity neuron represented real number layers. activities input hidden output neurons indexed time quantities time instant respectively. typical artiﬁcial neurons bias term. sake notational simplicity without loss generality bias term suppressed here. bias term induced considering coordinates times. dynamics network follows consider time series assume identiﬁcation elman-type network approximation reconstructing hidden states improved hidden representation parameters elman network subject sparsiﬁcation. optimization sparsiﬁcation norm related ǫ-insensitive norm examined respect optimization quadratic norm. optimization concerns hidden representation weights matrix linear recurrent network assumption allows joined formalism shall tested diﬀerent time series. special vectors matrices certain letters denote particular vectors matrices. beyond already introduced notations denote vector matrix components equal respectively; identity matrix; matrices column indices corresponding time representing whole hidden input time series respectively; ǫ-insensitive multiplier matrix notation size shorthand denote sizes matrices. example matrix ra×b size size temporal shift denoted operator increases temporal indices produces transitions. operator also columns matrices. example matrix estimate value source subject conditions diﬀerent transformed values approximate certain values also minimize certain cost functions. formally goal goal illustrated directed graph fig. depicts costs arise. every edge represents certain transformation every cost represented node. nodes approximation nodes i.e. represent approximation costs whereas others regularization nodes represent costs associated regularization. case corresponds state sequences parameters lrnn system detailed below. fig. directed graph approximation regularization. task estimate unknown source assuming parameterized transforms i.e. certain forms transformed values approximate prescribed values estimation means quality approximation determined cumulated costs transformation minimum. here denotes regularization terms represents parameters lrnn system internal state sequence stands optimizations iterated repeatedly. procedure ensures convergence least local minimum. equations aﬃne conditions either ﬁxed lrnn parameters degenerated sense also matrix notice linear multi-terms lrnn parameters internal state respectively provided terms kept constant. thus cost function iteratively optimized alternatively minimizing cost function decreases every step upon convergence locally optimal parameter internal state sequence reached provided regularization terms managed. consider cost function made ǫ-insensitive terms support vector machines references therein). separation ǫ-insensitive norms somewhat arbitrary. norms used order demonstrate diversity enabled mathematical framework. diﬀerent combinations course possible eﬀort made better combination. shall simplify description transcribing cost functions vectorial form. noted before term irrespective ǫ-insensitive quadratic nature seen linear multi-term corresponding iteration step. thus suﬃcient consider types costs numerical studies advantages joined parameter representational sparsiﬁcation referred ‘sparse case’ explored lrnn architecture. error approximation monitored function iteration number. following test examples chosen equation standard benchmark tests. used usual parameterizations mackeyglass time series sampling time equal time series computed order runge-kutta method. shall approximate dimensional series length lrnn. diﬀerent values shall used. matrix approximates output hidden activites lrnn projection ﬁrst coordinate. observation matrix lrnn shall projection ﬁrst coordinate coordinates matrices initialized drawing independently uniform distribution ǫ-insensitivity chosen dimension hidden state optimizations sparse case compared optimizations using quadratic cost function large diversity possible mixed cost functions. eﬀort made select main contributing terms diﬀerences results. diﬀerences might change problem problem. optimization warrant locally optimal solutions. overcome problem particular experiment random initializations matrices made diﬀerent optimizations executed. averages costs computed shall depicted. approximation error function iteration number shown fig. case problem. results averaged experiments. similar results found experiments too. time series diﬀerent length tried. according ﬁgure sparsiﬁcation advantages makes convergence faster uniform approximation produced small errors quadratic norm lengths iteration number required approximate convergence time series lengths error increased considerably typical iteration number required approximate convergence increased step long time series quadratic case iterations satisfactory reach stable approximation error i.e. close neighborhood local minimum. fig. performance quadratic cost sparse case. approximation error versus iteration number length time series approximated case problem. results averaged random experiments. case quadratic cost function. sparse case. convergence speed iteration faster sparse case note diﬀerences scales. predictions optimized lrnns shown fig. time steps. sparsiﬁcation shows improved interpolating capabilities. impression reinforced fig. depicts averages randomly initialized computations alike shown fig. averaged prediction errors λappr λappr zxkr values quadratic sparse case depicted ﬁgure. respective standard deviations well best worst cases also provided. small long intervals nature cost function measures costs relative sometimes relatively large. comparison diﬀerent costs somewhat arbitrary. time series generated fir-laser henon systems lrnn trained joined parameter representational sparsiﬁcation seems exhibit factor three better approximation lrnn using quadratic norm. crude fig. predictions lrnn systems trained diﬀerent methods diﬀerent problems. problem fir-laser henon systems shown columns respectively. ‘quadratic’ prediction quadratic cost function. ‘sparse’ prediction sparse case. length approximated time series black original time series. gray approximation. detailed results fig. denotes averaging quadratic ǫ-insensitive cases received roughly respectively. comparison using nrmse favors quadratic norm advantage factor ǫ-insensitive norm. first mention convergence certain sparsiﬁcation methods questioned method presented makes norm sparsiﬁcation warrants convergence extension previous architectures consider hidden predictive model. question intriguing neuronal coding seem apply sparsiﬁcation basic strategy. neuronal ﬁring brain sparse could serve minimization energy consumption well established joint constraints sparsiﬁcation reconstruction capabilities produce receptive ﬁelds similar found primary visual cortex sophisticated architectures also apply sparsiﬁcation reproduce details receptive ﬁelds advantages beyond energy minimization? recently independent component analysis information transfer optimizing scheme related sparsiﬁcation shown fig. prediction error averaged experiments. experiments averaged randomly initialized optimized lrnn models time series examples fir-laser henon). horizontal axis time vertical axis error. solid line averaged magnitude. dashed line standard deviation. dotted line best worst cases. black results quadratic cost function. gray results sparse case. prediction error averaged order gray curves. also table appropriate thresholding corresponds well established denoising methods. denoising removal structureless high entropy portion input uncovering structures important tasks learning. subliminal statistical learning indeed strategy applied brain denoising capabilities called sparse code shrinkage enables local noise estimation local noise ﬁltering. desirable optimizes noise ﬁltering respect experienced inputs. another advantage emerges sparsiﬁed representation embedded reconstruction networks. construct produce overcomplete representations which construction exhibit graceful degradation. also overcomplete representations produce diﬀerent outputs similar inputs desirable feature categorization decision making. easy norm biases system towards sparse representation compared norm norm produces larger costs small signals depresses small signals eﬃciently. statement seems general ǫ-insensitive cost function closely related sparsiﬁcation networks diﬀerent noise models behind norms. considerations norm approximation seen log-likelihood distribution noise. norms well epsilon-insensitive norm incorporated following log-likelihood expression argument respective norms. example quadratic loss function emerges dirac’s delta function. loss recovered setting expδ. expression ǫ-insensitive loss factorized form follows normalization constant. interpretation noise aﬀecting data additive gaussian mean diﬀerent zero. variance mean noise random variables given probability distributions. diminishing range non-zero mean gaussian distributions expression approximates norm turn norm ǫ-insensitive norm correspond super-gaussian noise distributions. concerning question sparsiﬁcation weight matrices e.g. exponential forgetting pruning weights well computational means studied years. interested reader referred literature broad ﬁeld question seems particular advantage called occam’s razor principle. reverse engineering search parameters lrnn many solutions identical similar properties. could solution task. right one? least probable? answer philosopher simplest explanation makes least assumptions best probable. similar ideas emerged information theory within context kolmogorov complexity bridge built connect idea least number assumptions concepts complexity. minimum description length principle makes complexity measures data assumed family probability distributions model. best i.e. probable description minimum complexity. important connections minimum description length principle regularization theory worked literature recent review subject think weight sparsiﬁcation method decreases number parameters thus searches simpler descriptions provided precision parameters bounded. results indicate local minima better distributed and/or ǫ-insensitive norms norm within problem studied. however known problem sets quadratic cost superior absolute value easy create example quadratic cost parameters suits parameter sets exhibit gaussian distribution. turn success ǫ-insensitive norms raises question type problems proﬁt cost function family. need examine statistical properties phenomena nature. seems phenomena special distributions. distributions super-gaussian sometimes exponential sometimes heavy tails e.g. characterized power-law distribution broad range. looks typical natural forms evolving/developing systems references therein). denotes expectation value mean variable standard deviation. positive kurtosis typical super-gaussian distributions heavy tails. results shown table cases second study concerned power behavior. found zero crossings henon systems approximate power-law distributions series laser between exhibits erratic descending curve log-log scale. slope log-log smaller smaller except time series. problems studied typically heavy tailed distributions. turn and/or ǫ-insensitive norms suit problems better norm norm corresponds laplace distribution heavier tail gaussian distribution. power-law distributions however even extremes respect. heavy tailed distributions typical nature even better norms identiﬁcation emergent behaviors issue. forth joint formalism sparsiﬁcation representation structure neural networks. approximation applied recurrent neural network joined eﬀect studied. experiments conducted number benchmark problems mackey-glass parameters emission far-infrared laser henon time series. results suggest joined constraint sparsiﬁcation advantageous examples. namely optimizations faster local optima exhibited better interpolating properties sparsiﬁed case traditional quadratic norm. argued ﬁndings underlying statistics phenomena quadratic norms assume gaussian noise distributions heavy-tails better approximated norms lower degrees. joint advantages removal structureless high entropy feature representational sparsiﬁcation discovery structures sparse networks needs explored. nature works science self-organized criticality holland emergence order chaos jensen self-organized criticality johnson emergence connected lives ants brains cities software albert barabsi reviews modern physics lodding queue korn faure comptes rendus biologies gauthier phys. sanger connection science deco finnoﬀ zimmermann neural computation reed marks ieee transactions neural networks kamimura neural computation olshausen field sparse coding overcomplete basis strategy employed olshausen field nature koene takane neural computation chen haykin neural computation olshausen field current opinion neurobiology hinton artiﬁcial intelligence williams neural computation hyv¨arinen karthikesh proc. int. workshop independent component analysis blind signal separation haykin neural networks comprehensive foundation vapnik statistical learning theory evgeniou pontil poggio advances computational mathematics tsoi back neurocomputing vapnik nature statistical learning theory isbn ---. hoyer neurocomputing herbrich learning kernel classiﬁers minka matrix algebra useful statistics available citeseer.ist.psu.edu/minkaold.html mackey glass science weigend gershenfeld time series prediction forecasting future understanding past olshausen a.i. memo c.b.c.l. paper baddeley nature ballard nature neuroscience jutten herault signal processing comon signal processing bell sejnowski neural computation hyv¨arinen neural computation watanabe nanez sasaki nature girosi neural computation pontil mukherjee girosi proceedings algorithmic learning theory a.i. cover thomas elements information theory rissanen automation mackay thesis caltech department computation neural systems hinton zemel advances neural information processing systems", "year": 2004}