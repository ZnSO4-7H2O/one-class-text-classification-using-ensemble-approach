{"title": "A Probabilistic Theory of Deep Learning", "tag": ["stat.ML", "cs.CV", "cs.LG", "cs.NE"], "abstract": "A grand challenge in machine learning is the development of computational algorithms that match or outperform humans in perceptual inference tasks that are complicated by nuisance variation. For instance, visual object recognition involves the unknown object position, orientation, and scale in object recognition while speech recognition involves the unknown voice pronunciation, pitch, and speed. Recently, a new breed of deep learning algorithms have emerged for high-nuisance inference tasks that routinely yield pattern recognition systems with near- or super-human capabilities. But a fundamental question remains: Why do they work? Intuitions abound, but a coherent framework for understanding, analyzing, and synthesizing deep learning architectures has remained elusive. We answer this question by developing a new probabilistic framework for deep learning based on the Deep Rendering Model: a generative probabilistic model that explicitly captures latent nuisance variation. By relaxing the generative model to a discriminative one, we can recover two of the current leading deep learning systems, deep convolutional neural networks and random decision forests, providing insights into their successes and shortcomings, as well as a principled route to their improvement.", "text": "grand challenge machine learning development computational algorithms match outperform humans perceptual inference tasks visual object speech recognition. factor complicating tasks presence numerous nuisance variables instance unknown object position orientation scale object recognition unknown voice pronunciation pitch speed speech recognition. recently breed deep learning algorithms emerged high-nuisance inference tasks; constructed many layers alternating linear nonlinear processing units trained using large-scale algorithms massive amounts training data. recent success deep learning systems impressive routinely yield pattern recognition systems nearsuper-human capabilities fundamental question remains work? intuitions abound coherent framework understanding analyzing synthesizing deep learning architectures remained elusive. answer question developing probabilistic framework deep learning based bayesian generative probabilistic model explicitly captures variation nuisance variables. graphical structure model enables learned data using classical expectation-maximization techniques. furthermore relaxing generative model discriminative recover current leading deep learning systems deep convolutional neural networks random decision forests providing insights successes shortcomings well principled route improvement. deep rendering model capturing levels abstraction dcns probabilistic message passing networks deep rendering model message passing uniﬁcation neural networks probabilistic inference probabilistic role max-pooling algorithm shallow rendering model algorithm deep rendering model dropout training? generative discriminative classiﬁers deep rendering model deep convolutional networks relation mixture factor analyzers i-theory invariant representations inspired sensory cortex scattering transform achieving invariance wavelets learning deep architectures sparsity google facenet learning useful representations dcns renormalization theory summary distinguishing features realistic rendering models inference algorithms top-down convolutional nets top-down inference derivative-free learning dynamics learning video training labeled unlabeled data humans expert wide array complicated sensory inference tasks recognizing objects image understanding phonemes speech signal despite signiﬁcant variations position orientation scale objects pronunciation pitch volume speech. indeed main challenge many sensory perception tasks vision speech natural language processing high amount nuisance variation. nuisance variations complicate perception turn otherwise simple statistical inference problems small number variables much higher-dimensional problems. example images taken different camera viewpoints highly curved nonlinear manifold high-dimensional space intertwined manifolds myriad objects. challenge developing inference algorithm factor nuisance variation input. past decades vast literature approaches problem myriad different perspectives developed difﬁcult inference problems remained reach. recently breed machine learning algorithms emerged high-nuisance inference tasks resulting pattern recognition systems sometimes super-human capabilities so-called deep learning systems share common hallmarks. first architecturally constructed many layers alternating linear nonlinear processing units. second computationally parameters learned using large-scale algorithms massive amounts training data. examples architectures deep convolutional neural network seen great success tasks like visual object recognition localization speech recognition part-of-speech recognition random decision forests image segmentation. success deep learning systems impressive fundamental question remains work? intuitions abound explain success. explanations focus properties feature invariance selectivity developed multiple layers others credit computational power amount available training data however beyond intuitions coherent theoretical framework understanding analyzing synthesizing deep learning architectures remained elusive. paper develop theoretical framework provides insights successes shortcomings deep learning systems well principled route design improvement. framework based generative probabilistic model explicitly captures variation latent nuisance variables. rendering model explicitly models nuisance variation rendering function combines task-speciﬁc variables interest collection nuisance variables. deep rendering model extends hierarchical fashion rendering product afﬁne nuisance transformations across multiple levels abstraction. graphical structures enable inference message passing using example sum-product max-sum algorithms training expectation-maximization algorithm. element framework relaxation rm/drm generative model discriminative order optimize bias-variance tradeoff. unites subsumes current leading deep learning based systems max-sum message passing networks. conﬁguring different nuisance structures gaussian translational nuisance evolutionary additive nuisance leads directly dcns rdfs respectively. intimate connection deep learning systems provides range insights work answering several open questions. moreover provides insights deep learning fails suggests pathways improvement. important note theory methods apply wide range different inference tasks feature number task-irrelevant nuisance variables however concreteness exposition focus classiﬁcation problem underlying visual object recognition. paper organized follows. section introduces demonstrates step-by-step onto dcns. section summarizes insights provides operation performance dcns. section proceeds similar fashion derive rdfs variant models hierarchy categories. section closes paper suggesting number promising avenues research including several lead improvement deep learning system performance generality. proofs several results appear appendix. section develops generative probabilistic model explicitly captures nuisance transformations latent variables. show inference corresponds operations single layer dcn. extend deﬁning rendering model layers representing different scales levels abstraction. finally show that application discriminative relaxation inference learning correspond feedforward propagation back propagation training dcn. enables conclude dcns probabilistic message passing networks thus unifying probabilistic neural network perspectives. rendering model capturing nuisance variation visual object recognition naturally formulated statistical classiﬁcation problem. given d-pixel multi-channel image object intensity pixel channel seek infer object’s identity ﬁnite classes. terms object class interchangeably. given joint probabilistic model images objects classify particular image using maximum posteriori classiﬁer object recognition like many inference tasks complicated high amount variation nuisance variables formation ignores. advocate explicitly modeling nuisance variables encapsulating parameter nuisances. cases natural transformation endowed group structure. propose generative model images explicitly models relationship between images object subject nuisance first given auxiliary parameters deﬁne rendering function renders image. image inference problems example might photorealistic computer graphics engine particular realization image generated adding noise output renderer assume noise distribution exponential family includes large number practically useful distributions also assume noise independent identically distributed function pixel location class nuisance variables independently distributed according categorical distributions. assumptions becomes probabilistic rendering recall focus object recognition images concreteness exposition. restriction ﬁnite removed using nonparametric prior chinese restaurant independence merely convenient approximation; practice depend example humans denotes distribution exponential family parameters include mixing probabilities natural parameters sufﬁcient statistics whose mean rendered template identity matrix. generalizes gaussian na¨ıve bayes classiﬁer gaussian mixture model allowing variation image depend observed class label like gnbc unobserved nuisance label like gmm. gnbc conveniently described directed graphical model figure depicts graphical models gnbc fig. shows combined form finally since world spatially varying image contain number different objects natural break image number subimages called patches indexed spatial location thus patch deﬁned collection pixels centered single pixel general patches overlap meaning tile image image pixel belong patch. given notion pixels patches allow class nuisance variables depend pixel/patch location i.e. local image class local nuisance omit dependence clear context. notion rendering operator quite general refer function maps target variable nuisance variables pattern template example speech recognition might phoneme case represents volume pitch speed accent amplitude acoustic signal natural language processing might grammatical part-of-speech case represents syntax grammar clause phrase sentence. perform object recognition must marginalize nuisance variables consider approaches conventional unconventional. sum-product classiﬁer sums nuisance variables figure graphical depiction naive bayes classiﬁer gaussian mixture model shallow rendering model deep rendering model dependence pixel location suppressed clarity. bra-ket notation inner products last line used definition exponential family distribution. thus sp-rm computes marginal posterior distribution target variable given input image. conventional approach used probabilistic modeling latent variables. ms-rmc computes mode posterior distribution target nuisance variables given input image. equivalently computes likely global conﬁguration target nuisance variables image. intuitively effective strategy explanation justiﬁed settings rendering function deterministic nearly noise-free. approach classiﬁcation unconventional machine learning computational neuroscience literatures sum-product approach commonly used although received recent attention throughout paper assume isotropic diagonal gaussian noise simplicity treatment presented generalized distribution exponential family straightforward manner. note extension require non-linear transformation depending speciﬁc exponential family. please supplement section details. ﬁrst assumption noise added rendered template isotropically gaussian i.e. pixel noise variance independent conﬁguration then assuming image normalized yields max-sum gaussian classiﬁer demonstrate sequence operations ms-rmc coincides exactly operations involved layer image normalization linear template matching thresholding pooling. fig. explore operation detail make link precise. first image normalized. recently several different types normalization typically employed dcns local response normalization local contrast normalization however recent highly performing dcns employ different form normalization known batch-normalization come back later show derive batch normalization principled approach. implication unclear probabilistic assumption older forms normalization arise from any. second image ﬁltered noise-scaled rendered templates wcg. size templates depends size objects class values nuisance variables large objects large templates corresponding fully connected layer small objects small templates corresponding locally connected layer distribution objects depends pixel position then general need different rendered templates case locally connected layer appropriate. hand objects equally likely present locations throughout entire image assume translational invariance yields global templates used pixels corresponding convolutional layer ﬁlter sizes large relative scale image variation occurs ﬁlters overcomplete adjacent ﬁlters overlap waste computation. case appropriate strided convolution output traditional convolution down-sampled factor; saves computation without losing information. fourth recall given image pixel reside several overlapping image patches rendered parent class nuisance location thus must consider possibility collisions i.e. different parents might render pixel avoid undesirable collisions natural force rendering locally sparse i.e. must enforce renderer local neighborhood active. figure example mapping deep rendering model corresponding factor graph deep convolutional network showing transformation level hierarchy abstraction level generative model single super pixel level renders image patch level whose location speciﬁed factor graph representation model supports efﬁcient inference algorithms max-sum message passing shown here. computational network implements max-sum message passing algorithm explicitly; structure exactly matches dcn. however switching variables strong correlations crowding effect neighbors must order prevent rendering collisions. although natural realistic rendering complicates inference. thus employ approximation instead assuming state renderer completely random thus independent variables including measurements course approximation real rendering simpliﬁes inference leads directly rectiﬁed linear units show below. approximations true sparsity extensively studied known spike-and-slab sparse coding models bcga bias terms relu max{u denotes soft-thresholding operation performed rectiﬁed linear units modern dcns last line assumed prior uniform bcga independent dropped. world summarizable varying levels abstraction hierarchical bayesian models exploit fact accelerate learning. particular power abstraction allows higher levels learn concepts categories rapidly lower levels stronger inductive biases exposure data informally known blessing abstraction light beneﬁts natural extend thus giving power summarize data different levels abstraction. order illustrate concept consider example rendering image face different levels detail level specify identity face overall location pose without specifying ﬁner-scale details locations eyes type facial expression. level figure sculpture henri matisse illustrates deep rendering model sculpture leftmost panel analogous fully rendered image lowest abstraction level moving left right sculptures become progressively abstract rightmost panel reach highest abstraction level ﬁner-scale details ﬁrst three panels lost fourth nuisance parameters whereas coarser-scale details last panel preserved target certain location pose state without specifying ﬁner-scale parameters continue level adding ﬁner-scaled information unspeciﬁed level level fully speciﬁed image’s pixel intensities leaving fully rendered multi-channel image refers pixel location level another illustrative example consider back series sculptures artist henri matisse moves left right sculptures become increasingly abstract losing low-level features details preserving high-level features essential overmeaning i.e. woman back facing conversely moves right left sculptures become increasingly concrete progressively gaining ﬁner-scale details culminating rich textured rendering. formalize process progressive rendering deﬁning deep rendering model analogous matisse sculptures image generation process starts highest level abstraction random choice object class overall pose followed generation lower-level details progressive levelby-level rendering intermediate rendered images detailed information. process ﬁnally culminates fully rendered d-dimensional sets target-relevant target-irrelevant nuisance variables level respectively. rendering path deﬁned sequence root individual pixels abstract template transformations renders ﬁner-scale details moves abstract concrete. note afﬁne transformation bias term suppressed clarity. figure illustrates corresponding graphical model. before suppressed dependence pixel location level hierarchy. cast incremental form deﬁning intermediate class intuitively represents partial rendering path level then partial rendering level written afﬁne transformation shown bias term explicitly introduced noise diagonal covariance important note correspond different kinds target relevant irrelevant features different levels. example rendering faces might correspond different edge orientations different edge locations patch whereas might correspond different types different gaze directions patch generates images intermediate abstraction levels incremental rendering functions hence complete rendering function composition incremental rendering functions amounting product afﬁne transformations compared shallow factorized structure this assumes using exponential family linear sufﬁcient statistics i.e. however note family gaussian instead factor analyzer different probabilistic model. introduce noise reasons make easier connect later existing algorithms factor analyzers always take noise-free limit impose cluster well-separatedness needed. indeed rendering process deterministic nearly noise-free latter justiﬁed. formulated distinct related several hierarchical models deep mixture factor analyzers deep gaussian mixture model essentially compositions another model mixture factor analyzers highlight similarities differences models detail section inference similar inference shallow example classify images either sum-product max-sum classiﬁer. difference deep shallow yields iterated layer-by-layer updates ﬁne-to-coarse abstraction coarse-to-ﬁne abstraction case interested inferring high-level class need ﬁne-to-coarse pass consider section. importantly bottom-up pass leads directly dcns implying dcns ignore potentially useful top-down information. maybe explanation difﬁculties vision tasks occlusion clutter top-down information essential disambiguating local bottom-up hypotheses. later section describe coarse-to-ﬁne pass class top-down dcns make information. given input image max-sum classiﬁer infers likely global conﬁguration executing max-sum message passing algorithm stages ﬁne-to-coarse levels abstraction infer overall class label coarsems intermediate levels to-ﬁne levels abstraction infer latent variables mentioned above focus ﬁne-to-coarse pass. since hierarchical prior rendered templates derive ﬁne-to-coarse covariance rendered image x|m|y note signiﬁcant change respect shallow covariance longer diagonal iterative afﬁne transformations rendering must decorrelate input image order classify accurately. care inferring overall class image ﬁne-to-coarse pass sufﬁces since information relevant determining overall class integrated. high-level classiﬁcation need iterate eqs. note simpliﬁes assume sparse patch rendering section coming back dcns iteration corresponds feedforward propagation layer dcn. thus dcn’s operation probabilistic interpretation ﬁne-to-coarse inference probable global conﬁguration drm. important note fully reconstituted architecture modern yet. particular softmax regression layer typically attached network missing. means high-level class necessarily training data class labels given dataset. fact labels general distinct. interpret answer probable global conﬁguration inferred interpreted good representation input image i.e. disentangles many nuisance factors independent components interpretation becomes clear high-level class disentangled representation need training data class label disentangled representation lies penultimate layer activations given representation infer class label using simple linear classiﬁer softmax regression. explicitly softmax regression layer computes chooses likely class. softmax function θsoftmax bl+} parameters softmax regression layer. encouraged correspondence identiﬁed section step back moment reinterpret major elements dcns probabilistic light. derivation inference algorithm mathematically equivalent performing max-sum message passing factor graph representation shown fig. factor graph encodes information generative model organizes manner simpliﬁes deﬁnition execution inference algorithms inference algorithms called message passing algorithms work passing real-valued functions called messages along edges nodes. drm/dcn messages sent ﬁner coarser levels feature maps however unlike input image channels feature maps refer colors instead abstract features factor graph formulation provides powerful interpretation convolution maxpooling relu operations correspond max-sum inference drm. thus architectures layer types commonly used today’s dcns hoc; rather derived precise probabilistic assumptions entirely determine structure. thus uniﬁes perspectives neural network probabilistic inference. summary relationship perspectives given table consider role max-pooling message passing perspective. interpreted max-sum thus executing max-marginalization nuisance variables typically operation would intractable since exponentially many conﬁgurations drm’s model abstraction deep product afﬁne transformations comes rescue. enables convert otherwise intractable max-marginalization tractable sequence iterated max-marginalizations abstraction levels thus max-pooling operation implements probabilistic marginalization absolutely essential dcn’s ability factor nuisance variation. indeed since relu also cast max-pooling on/off switching variables conclude important operation dcns max-pooling. conﬂict recent claims contrary since graphical models latent variables learn parameters training data using expectation-maximization algorithm ﬁrst develop algorithm shallow section extend section table summary probabilistic neural network perspectives dcns. provides exact correspondence providing probabilistic interpretation common elements dcns relating underlying model inference algorithm learning rules. reference algorithm shallow rendering model iteration algorithm congiven dataset labeled training images cn}n sists e-step infers latent variables given observed variables parameters ˆθold last m-step followed m-step updates parameter estimates according parameters deﬁned include prior probabilities different classes nuisance variables along rendered templates pixel noise variance instead isotropic gaussian fullcovariance gaussian different exponential family distribution sufﬁcient statistics rendered template parameters would different clusters well-separated input image assigned nearest cluster hard e-step wherein care likely conﬁguration given input case responsibility image consistent likely conﬁguration; otherwise equals thus compute responsibilities using max-sum message passing according eqs. case hard algorithm reduces high-nuisance tasks algorithm shallow computationally intractable since requires recomputing responsibilities parameters possible conﬁgurations rendering tree rooted however crux factorized form rendered templates results dramatic reduction number parameters. enables efﬁciently infer probable conﬁguration exactly thus avoid need resort slower approximate sampling techniques commonly used approximate inference deep hbms exploit realization e-step. guided algorithm extend algorithm shallow previous section drm. e-step performs inference ﬁnding likely rendering tree conﬁguration given current training input m-step updates parameters layer weights biases responsibility-weighted regression output activations input activations. interpreted layer learning summarize input feature coarser-grained output feature essence abstraction. note exact spike-n-slab approximation truly sparse rendering model renderer neighborhood active described section technically approximation tree instead so-called polytree. nevertheless max-sum exact trees polytrees powerful learning rule discovered recently independently google seen approximation algorithm whereby approximated normalizing input activations respect training batch introducing scaling bias parameters according since activation normalized independently avoid costly full covariance calculation. diagonal matrix bias vector parameters introduced compensate distortions batch-normalization. light algorithm derivation clear scheme crude approximation true normalization step whose decorrelation scheme uses nuisance-dependent mean full covariance λ)†. nevertheless excellent performance google algorithm bodes well performance exact algorithm developed above. mention common regularization scheme used dcns dropout dropout training consists units dropping outputs random. seen kind noise corruption encourages learning features robust missing data prevents feature co-adaptation well dropout speciﬁc dcns; used architectures well. brevity refer reader proof dropout algorithm appendix show dropout derived algorithm. constructed correspondence dcns mapping deﬁned exact. particular note constraints weights biases reﬂections distributional assumptions underlying gaussian drm. dcns constraints weights biases free parameters. result faced training data violates drm’s underlying assumptions freedom compensate. order complete mapping create exact correspondence dcns relax parameter constraints allowing weights biases free independent parameters. however seems approach. instead theoretically motivate relaxation? turns distinction classiﬁers fundamental former known generative classiﬁer latter known discriminative classiﬁer distinction generative discriminative models bias-variance tradeoff. hand generative models strong distributional assumptions thus introduce signiﬁcant model bias order lower model variance hand discriminative models relax distributional assumptions order lower model bias thus data speak itself cost higher variance practically speaking generative model misspeciﬁed enough labeled data available discriminative model achieve better performance speciﬁc task however generative model really true data-generating distribution generative model better choice. motivated distinction types models section deﬁne method transforming call discriminative relaxation. call resulting discriminative classiﬁer discriminative counterpart generative classiﬁer. show applying procedure generative classiﬁer yields discriminative classiﬁer although focus gaussian treatment generalized exponential family distributions modiﬁcations formally deﬁne procedure preliminary deﬁnitions remarks helpful. generative classiﬁer models joint distribution input features class labels. classify inputs using bayes rule calculate picking likely label training classiﬁer known generative learning since generate synthetic features sampling joint distribution therefore generative classiﬁer learns indirect input features labels modeling joint distribution labels features. contrast discriminative classiﬁer parametrically models order estimate parameter trains dataset input-output pairs known discriminative learning since directly discriminate different labels given input feature therefore discriminative classiﬁer learns direct input features labels directly modeling conditional distribution labels given features. given deﬁnitions deﬁne discriminative relaxation procedure converting generative classiﬁer discriminative one. starting standard learning objective generative classiﬁer employ series transformations relaxations generative conditional discriminative log-likelihoods respectively. line used chain rule probability. line introduced extra parameters also introducing constraint enforces equality generative parameters line relax equality constraint allowing classiﬁer parameters differ image generation parameters line pass natural parametrization exponential family distribution natural parameters ﬁxed function conventional parameters constraint natural parameters ensures optimization lcond yields answer optimization lcond. ﬁnally line relax natural parameter constraint learning objective discriminative classiﬁer parameters free optimized. summary starting generative classiﬁer learning objective lgen complete steps arrive discriminative classiﬁer learning objective ldis. refer process discriminative relaxation generative classiﬁer resulting classiﬁer discriminative counterpart generative classiﬁer. figure illustrates discriminative relaxation procedure applied consider gaussian simply comprises mixing probabilities mixture parameters {πcg corresponding relaxed discriminative parameters weights biases ηdis {wcg bcg}. intuitively interpret discriminative relaxation brain-world transformation applied generative model. according interpretation instead world generating figure graphical depiction discriminative relaxation procedure. rendering model depicted graphically mixing probability parameters rendered template parameters λcg. brain-world transformation converts equivalent graphical model extra parameters constraints introduced. discriminatively relaxing constraints yields single-layer discriminative counterpart original generative classiﬁer rendering parameters θworld brain generates labels classiﬁer parameters ηdis ηbrain note graphical model depicted fig. equivalent fig. except relaxation parameter constraints represent discriminative relaxation. apply show discriminative relaxation drm. first apply brain-world transformation drm. resulting classiﬁer precisely deep maxout neural network discussed earlier. second impose translational invariance ﬁner scales abstraction introduce switching variables model inactive renderers. yields convolutional layers relu activation functions section third learning algorithm generative classiﬁer algorithm eqs. must modiﬁed according account discriminative relaxation. particular note discriminative e-step ﬁne-to-coarse corresponds forward propagation dcns. discriminative m-step variety choices general purpose optimization algorithm used choosing gradient descent leads classical back propagation algorithm neural network training typically modern-day dcns trained using variant back propagation called stochastic gradient descent gradients computed using mini-batch data time light developments here re-interpret discriminative counterpart generative batch algorithm ﬁers discriminative relaxation classiﬁers forward propagation corresponding inference probable conﬁguration drm. also reinterpreted learning back propagation training dcns discriminative relaxation batch learning algorithm drm. provided principled motivation passing generative discriminative counterpart showing discriminative relaxation helps alleviate model misspeciﬁcation issues increasing drm’s ﬂexibility cost slower learning requiring training data. light intimate connection drms dcns provides insights dcns work answering many open questions. importantly drms also show dcns fail improve section explore insights. factor graph formulation provides useful interpretation dcns shows convolutional max-pooling layers correspond standard message passing operations applied inside factor nodes factor graph drm. particular max-sum algorithm corresponds max-pool-conv neural network whereas sum-product algorithm corresponds mean-pool-conv neural network. generally architectures layer types used commonly successful dcns neither arbitrary hoc; rather derived precise probabilistic assumptions almost entirely determine structure. summary perspectives neural network probabilistic given table derivation inference enables understand trained dcns distill store knowledge past experiences parameters. speciﬁcally generates rendered templates product afﬁne transformations thus implying class appearance models dcns stored factorized form across multiple levels abstraction. thus explain past attempts understand dcns store memories examining ﬁlters layer fruitless exercise mentioned section typically followed softmax regression layer end. layer classiﬁes hidden representation class labels used training. section details. product ﬁlters/weights layers yield meaningful images objects. indeed fact encapsulated mathematically eqs. notably recent studies computational neuroscience also shown strong similarity representations primate visual cortex highly trained suggesting brain might also employ factorized class appearance models. also shed light another approach understanding memories proceeds searching input images maximize activity particular class unit technique call activity maximization. results activity maximization high performance trained million images shown fig. resulting images striking reveal much dcns store memories. derive closed-form expression activity-maximizing images function underlying model’s learned parameters. mathematically seek image maximizes score speciﬁc object class. using argmaxg∈g g)|i∗ image decomposed patches size pixels outside patch zero. maxg∈g operator ﬁnds probable within patch. solution activity maximization individual activity-maximizing patches implies contains multiple appearances object various poses. activity-maximizing patch pose agreement fig. images provide strong conﬁrming evidence underlying model mixture nuisance parameters expected light drm. figure results activity maximization imagenet dataset. given class activity-maximizing inputs superpositions various poses object distinct patches containing distinct poses predicted figure adapted permission authors. goal representation learning disentangle factors variation contribute image’s appearance. given formulation clear dcns discriminative classiﬁers capture factors variation latent nuisance variables such theory presented makes clear prediction supervised learning task targets lead inevitably unsupervised learning latent task nuisance variables. perspective manifold learning means architecture dcns designed learn disentangle intrinsic dimensions data manifold. order test prediction trained classify synthetically rendered images naturalistic objects cars planes. since explicitly used renderer power systematically control variation factors pose location lighting. after training probed layers trained quantify much linearly separable information exists task target latent nuisance variables figure shows trained possesses signiﬁcant information latent factors variation furthermore nuisance variables layers required disentangle factors. strong evidence depth necessary amount depth required increases complexity class models nuisance variations. light results talk training dcns traditional distinction between supervised unsupervised learning ill-deﬁned worst misleading best. evident initial formulation task target latent variable capturing nuisance parameters another derivation shows dcns discriminative classiﬁers latent variables capture nuisance variation. believe main reason noticed earlier probably latent nuisance variables hidden within max-pooling units serve dual purpose learning marginalizing latent nuisance variables. random decision forests best performing least understood classiﬁers machine learning. intuitive structure seem arise proper probabilistic model. success vast array tasks perplexing clear explanation theoretical understanding. particular quite successful real-time image video segmentation tasks prominent example pose estimation body part tracking microsoft kinect gaming system also figure manifold entanglement disentanglement illustrated -layer max-out trained classify synthetically rendered images planes naturalistic objects different poses locations depths lighting conditions. amount linearly separable information target variable increases layer depth information nuisance variables follows inverted u-shaped curve. layers increasing information correspond disentanglement manifold factoring variation independent parameters whereas layers decreasing information correspond marginalization nuisance parameters. note disentanglement latent nuisance parameters achieved progressively multiple layers without requiring network explicitly train them. complexity variation induced several layers required successful disentanglement predicted theory. great success medical image segmentation problems wherein distinguishing different organs cell types quite difﬁcult typically requires expert annotators. section show that like dcns rdfs also derived model different assumptions regarding nuisance structure. instead translational switching nuisances show additive mutation nuisance process generates hierarchy categories heart rdf. derivation start generative classiﬁer derive discriminative relaxation. such rdfs possess similar interpretation dcns cast max-sum message passing networks. decision tree classiﬁer takes input image asks series questions answer question determines branch tree follow. next node another question asked. pattern repeated leaf tree reached. leaf class posterior probability distribution used classiﬁcation. different leaves contain different class posteriors. ensemble decision tree classiﬁers classify input sent input decision tree individually decision tree outputs class posterior averaged obtain evolutionary deep rendering model hierarchy categories deﬁne evolutionary evolutionary tree categories. samples model generated starting root ancestor template randomly mutating templates. child template additive mutation parent speciﬁc mutation depend parent repeating pattern child node entire evolutionary tree templates generated. assume simplicity working gaussian e-drm leaves tree sample generated adding gaussian pixel noise. course described earlier extended handle noise distributions exponential family. mathematically here special structure additive mutation process identity matrix. before sets target-relevant targetirrelevant nuisance variables level respectively. rendering path represents template evolution deﬁned sequence root ancestor template individual pixels abstract template root ancestor note differences drn/dcn inference derivation input layer always input image iterations coarse-to-ﬁne rather ﬁne-to-coarse resulting network neural network rather deep decision tree single-layer neural networks. differences special additive structure mutational nuisances evolutionary tree process underlying generation category templates. mapping single decision tree complete; leaf label histograms missing. analogous missing softmax regression layers dcns highlevel representation class label inferred e-drm need training data class label clarity treat separate general. understand interpret inferred conﬁguration disentangled representation input wherein different factors including vary independently world. contrast dcns class labels decision tree instead inferred discrete evolutionary path variable leaf histograms note decision trees also label histograms internal order allow weights biases deﬁne decision functions internal nodes free. note exactly analogous steps section mapping gaussian dcns. forest? important since well known individual decision trees notoriously good overﬁtting data. indeed historical motivation introducing forest decision trees order prevent overﬁtting averaging many different models trained randomly drawn subset data. technique known bootstrap aggregation bagging short ﬁrst introduced breiman context decision trees completeness section review bagging thus completing mapping e-drm rdf. order derive bagging necessary following make explicit den= i.e. pendence learned inference parameters training data dependence typically suppressed work necessary bagging entails training different decision trees different subsets full training data. words mathematically perform inference follows given previously seen data unseen image classify computing posterior distribution collection switching variables indicates data points included i.e. data point included dataset dci. randomly subsampled full dataset times line approximating true marginalization possible subsets data. line perform bayesian model averaging possible values e-drm/decision tree parameters since intractable approximate estimate line overall result e-drm trained separately randomly drawn subset entire dataset ﬁnal output classiﬁer average individual classiﬁers. approach train e-drm classiﬁer maximize mutual information given training labels inferred rendering path level. note discrete random variables. mutual information-based classiﬁer plays role softmax regression layer dcns predicting class labels given good disentangled representation input order train classiﬁer update classiﬁer parameters θmic m-step solution optimization mutual information random variables entropy random variable parameters layer ﬁrst line used layerby-layer structure e-drm split mutual information calculation across levels coarse ﬁne. second line used max-sum algorithm split optimization sequence optimizations third line used information-theoretic relationship algorithm known infomax literature introduced diagonal noise covariance equivalent model. dmfa employ parameter sharing resulting exponential reduction number parameters compared collapsed shallow version models. serves strong regularizer prevent overﬁtting. despite high-level similarities several essential differences mfa-based models critical reproducing dcns. first randomness choice observation noise rendering. naturally leads inference probable conﬁguration max-sum algorithm equivalent max-pooling dcn. second drm’s afﬁne transformations multi-channel images level produce multi-channel images level structure important leads directly notion feature maps dcns. third drm’s layers vary connectivity sparse dense give rise convolutional locally connected fully connected layers resulting dcn. fourth switching variables model active renderers manifestation variables relus thus critical elements architecture arise directly aspects structure absent mfa-based models. representational invariance selectivity important ideas developed computational neuroscience community. according perspective main purpose feedforward aspects visual cortical processing ventral stream compute representation sensed image invariant irrelevant transformations sense perspective quite similar basic motivations. however approach remained qualitative explanatory power recently theory invariant representations deep architectures dubbed i-theory proposed inspired neuroscience models visual cortex ﬁrst serious attempt explaining success deep architectures formalizing intuitions invariance selectivity rigorous quantitatively precise manner. i-theory posits representation employs group averages orbits explicitly insure invariance speciﬁc types nuisance transformations. transformation must possess mathematical semi-group structure; result invariance constraint relaxed notion partial invariance built slowly multiple layers architecture. high level shares similar goals i-theory attempts capture explicitly notion nuisance transformations. however differs i-theory critical ways. first impose semi-group structure nuisance transformations. provides ﬂexibility learn representation invariant wider class nuisance transformations including non-rigid ones. second representation images advance. instead representation emerges naturally inference process. instance summax-pooling emerge probabilistic marginalization nuisance variables thus necessary proper inference. deep iterative nature also arises direct mathematical consequence drm’s rendering model comprises multiple levels abstraction. important difference theories. despite differences i-theory complementary approach several ways spends good deal energy focusing questions many templates required accurate discrimination? many samples needed learning? plan pursue questions future work. used notion target nuisance variables explain power learning selectivity invariance nuisance transformations. another theoretical approach learning selectivity invariance scattering transform consists series linear wavelet transforms interleaved nonlinear modulus-pooling wavelet coefﬁcients. goal explicitly hand-design invariance speciﬁc nuisance transformations using properties wavelet transforms. ignore modulus-pooling moment implicitly assumes images modeled linear combinations pre-determined wavelet templates. thus approach maximally strong model bias learning all. performs well tasks consistent strong model bias i.e. small datasets successful performance therefore contingent strong model bias. however challenged difﬁcult real-world tasks complex nuisance structure large datasets available. contrasts strongly approach presented machine learning community large hand-designed features outperformed learned features vast majority tasks. optimal machine learning architecture given task? question typically answered exhaustively searching many different architectures. learn optimal architecture directly data? arora provide ﬁrst theoretical results direction. order retain theoretical tractability assume simple sparse neural network generative model data. then given data design greedy learning algorithm reconstructs architecture generating neural network layer-by-layer. prove algorithm optimal certain restrictive assumptions. indeed consequence restrictions results directly apply plausible generative models natural images. however core message paper nonetheless inﬂuential development inception architecture recently achieved highest accuracy imagenet classiﬁcation benchmark sparse reconstruction approach relate drm? indeed also sparse generative model rendering image approximated sequence afﬁne transformations applied abstract high-level class template. thus potentially represented sparse neural network. another similarity approaches focus clustering highly correlated activations next coarser layer abstraction. indeed composition sparse factor analyzers higher layer really decorrelate cluster layer below quantiﬁed despite high-level similarities approaches differ signiﬁcantly overgoals results. first focus recovering architectural parameters; instead focused class architectures well-suited task factoring large amounts nuisance variation. sense goals approaches different complementary. second able derive structure dcns rdfs exactly drm. enables bring bear full power probabilistic analysis solving high-nuisance problems; moreover enable build better models representations hard tasks addressing limitations current approaches principled manner. google facenet learning useful representations dcns recently google developed face recognition architecture called facenet illustrates power learning good representations. achieves state-of-the-art accuracy face recognition clustering several public benchmarks. facenet uses architecture crucially trained classiﬁcation. instead trained optimize novel learning objective called triplet ﬁnding learns good representations general. basic idea behind representation-based learning objective encourage dcn’s latent representation embed images class close embedding images different classes away other idea similar numax algorithm words learning objective enforces well-separatedness criterion. light work connecting drms dcns next show learning objective understood perspective drm. explanation dominate given input image equivalently clusters well-separated. thus noise-free deterministic well-separated equivalent. indeed implicitly used well-separatedness criterion employed hard algorithm establish correspondence drms dcns/rdfs. given drm’s notion irrelevant transformations multiple levels abstraction interpret dcn’s action iterative coarse-graining image thus relating work another recent approach understanding deep learning draws upon analogy renormalization theory physics approach constructs exact correspondence restricted boltzmann machine block-spin renormalization iterative coarse-graining technique physics compresses conﬁguration binary random variables smaller conﬁguration less variables. goal preserve much information longer-range correlations possible integrating shorter-range ﬂuctuations. work shows analogy goes even created exact mapping latter interpreted real-space renormalization scheme. indeed drm’s main goal factor irrelevant features multiple levels detail thus bears strong resemblance core tenets renormalization theory. result believe important avenue research. features distinguish approach others literature summarized explicitly models nuisance variation across multiple levels abstraction product afﬁne transformations. factorized linear structure serves dual purposes enables exact inference serves regularizer preventing overﬁtting novel exponential reduction number parameters. critically inference performed single variable interest instead full global conﬁguration. justiﬁed low-noise settings i.e. rendering process nearly deterministic suggests intriguing possibility vision less probabilities inverting complicated rendering transformation. shown powerful generative model underlies dcns rdfs powerful vision paradigms currently employed machine learning. despite power drm/dcn/rdf limitations room improvement. broad terms limitations framework traced back fact discriminative classiﬁer whose underlying generative model known. without generative model many important tasks difﬁcult impossible including sampling model reﬁnement top-down inference faster learning model selection learning unlabeled data. generative model tasks become feasible. moreover models rendering sequence afﬁne transformations severely limits ability capture many important real-world visual phenomena including ﬁgure-ground segmentation occlusion/clutter refraction. also lacks several operations appear fundamental brain feed-back dynamics geometry. finally unable learn unlabeled data generalize examples. result dcns require enormous amounts labeled data training. limitations overcome designing deep networks based model structures message-passing inference algorithms learning rules summarized table explore solutions detail. improve dcns designing better generative models incorporating realistic assumptions rendering process latent variables cause images. assumptions include symmetries translation rotation scaling perspective non-rigid deformations rendered computer graphics multi-view geometry. order encourage intrinsic computer graphics-based representations enforce symmetries parameters learning initially could local afﬁne approximations transformations example could impose weight tying based rotations depth. nuisance transformations also interest scaling indeed scaling-based templates already state-of-the-art dcns inception architectures developed google approach already shown substantial promise. also perform intrinsic transformations directly scene representations. example could train networks depth maps subset channels input feature maps encode pixel z-depth. augmented input features help deﬁne useful higher-level features image features thereby transfer representational beneﬁts even test images provide depth information richer geometric representations learning inference algorithms modiﬁed account constraints according equations multi-view geometry another important limitation restriction static images. notion time dynamics corresponding model. result training large-scale datasets requires millions images order learn structure high-dimensional nuisance variables resulting glacial learning process. contrast learning natural videos result accelerated learning process typically nuisance variables change frame frame. property enable substantial acceleration learning inference nuisance variables changed faster accurate section details. showed section dcns implicitly infer probable global interpretation scene max-sum algorithm however potentially major component missing algorithm max-sum message passing propagates likely hypothesis higher levels abstraction optimal strategy general especially uncertainty measurements high consequently consider wider variety softer inference algorithms deﬁning temperature parameter enables smoothly interpolate max-sum sum-product algorithms well message-passing variants approximate variational bayes best knowledge notion soft novel. inference algorithm lacks form top-down inference feedback. performance tasks using low-level features suboptimal higher-level information informs low-level variables neither inference learning. solve problem using since proper generative model thus enables implement top-down message passing properly. employing steps outlined section convert top-down neural network implements bottom-up top-down passes inference max-sum message passing algorithm. kind top-down inference dramatic impact scene understanding tasks require segmentation target detection back propagation often used deep learning algorithms simplicity. shown back propagation dcns actually inefﬁcient implementation approximate algorithm whose e-step consists bottom-up inference whose m-step gradient descent step fails take advantage underlying probabilistic model contrary algorithm much faster accurate directly exploits drm’s structure. e-step incorporates bottomtop-down inference m-step fast computation sufﬁcient statistics speed-up efﬁciency substantial since generative learning typically much faster discriminative learning bias-variance tradeoff moreover em-algorithm intrinsically parallelizable although deep incorporated time dynamics auditory tasks dcns visual tasks remained predominantly static trained static inputs. latent causes natural world tend change little frameto-frame previous frames serve partial self-supervision learning dynamic version would train without external supervision large quantities video data supplement video recordings natural dynamic scenes synthetically rendered videos objects traveling along smooth trajectories enable training focus learning nuisance factors cause difﬁculty dcns purely discriminative techniques thus cannot beneﬁt unlabeled data. however armed generative model perform hybrid discriminative-generative training enables training beneﬁt labeled unlabeled data principled manner. dramatically increase power pre-training encouraging representations input disentangled factors variation. hybrid generativediscriminative learning achieved optimization novel objective function learning relies generative model discriminative relaxation. particular learning objective terms both described recall section discriminative relaxation generative model performed relaxing certain parameter constraints learning according model’s generative naturally parametrized generative conditional discriminative likelihoods. natural parameters expressed function traditional parameters training dataset labels images dc|i training dataset labels given images. although discriminative relaxation optional important achieving high performance real-world classiﬁers discriminative models less model bias therefore less sensitive model mis-speciﬁcations thus design principled training algorithms span spectrum discriminative generative thanks barberan help manuscript mayank kumar mousavi salman asif andreas tolias comments discussions. thanks karen simonyan providing activity maximization ﬁgure. special thanks pitkow whose keen insight criticisms detailed feedback work instrumental development. thanks ruchi kukreja unwavering support humor raina patel providing inspiration. gaussian rendering model classiﬁer deep dcns proposition discriminative relaxation noise-free classiﬁer single layer consisting local template matching operation followed piecewise linear activation function line take noise-free limit means hypothesis dominates others likelihood. line assume image consists multiple channels conditionally independent given global conﬁguration typically input images color channels general abstract line assume pixel noise covariance isotropic conditionally independent given global conﬁguration proportional identity matrix line deﬁned locally connected template matching operator location-dependent template matching operation. local template matching operation ﬁlters/templates {wch}c∈ch∈h lemma maxout template matching pooling operation translational nuisance variables reduces traditional convolution max-pooling operation. proof. activation single output unit finally vectorizing gives desired result maxpooldcn proposition discriminative relaxation noise-free translational nuisances random missing data single convolutional layer traditional dcn. layer consists generalized convolution operation followed relu activation function max-pooling operation. proof. model completely random missing data nuisance transformation {keep drop} keep leaves rendered image data untouched drop throws entire image rendering. thus switching variable models missing data. critically whether data missing assumed completely random thus independent task variables including measurements since missingness evidence another nuisance invoke proposition conclude discriminative relaxation noise-free random missing data also maxout-dcn specialized structure derive. primed constants independent pulled outside maxa. line primed constants also independent dropped argmaxcg. finally line assume uniform prior resulting sequence operations corresponds exactly applied single convolutional layer traditional dcn. keep drop i.e. whether current measurements available/relevant/important instead missing/irrelevant/unimportant hypothesis relu also promotes sparsity activations. last section showed mixture gaussian nuisance classiﬁers discriminative relaxation maxout section generalize result arbitrary mixture exponential family nuisance classiﬁers. example consider laplacian poisson generalizing exponential family derivations discriminative relations simplify greatly roles played familiar concepts natural parameters sufﬁcient statistics log-partition functions. furthermore importantly resulting discriminative counter parts still maxout nns. thus maxout quite robust class e-family mixtures maxout d-counterparts. theorem nuisance mixture classiﬁer exponential family. discriminative counterpart maxout proof. proof analogous proof proposition except generalize using deﬁnition exponential family distribution simply fact exponential family distributions natural canonical form described deﬁnition thus natural parameters serve generalized weights biases sufﬁcient statistic serves generalized input. note require non-linear transformation i.e. quadratic logarithmic depending speciﬁc exponential family. despite large amount labeled data available many real-world vision applications deep dcns regularization schemes still critical part training essential avoiding overﬁtting data. important scheme dropout consist training unreliable neurons synapses. unreliability modeled ‘dropout’ probability neuron synapse won’t send output receiving neuron. intuitively downstream neurons cannot rely every piece data/evidence always there thus forced develop robust features. prevents co-adaptation feature detectors undermines generalization ability. section answer question derive dropout algorithm generative modeling perspective? show answer yes. dropout derived generative model algorithm condition missing data. proof. since data missing completely random algorithm train strategy show single iteration em-algorithm corresponds full epoch dropout training note typically em-algorithm used train generative models; utilize emalgorithm novel performing discriminative relaxation m-step. generative algorithm deﬁne discriminative algorithm d-e-step equivalent usual generative e-step. given observed data current parameter estimate compute posterior latent variables missing data indicator matrix i.e. p-th feature input data available. contains latent nuisance variables important classiﬁcation task. since assume noise-free actually execute hybrid e-step hard soft hard-e step yield max-sum message passing algorithm soft e-step yield ensemble average characteristic feature dropout d-m-step start maximizing complete-data log-likelihood usual generative m-step. however near derivation employ discriminative relaxation free rigid distributional assumptions generative model instead leave much ﬂexible assumptions embodied discriminative modeling problem deﬁned conditional likelihood partition data. deﬁnition allows write invoking conditional probability symbol mh|x] maxh{pf reduced dataset ddropout ﬁnal objective function left optimize mixture exponentially-many discriminative models trained different random subset training data sharing parameters since intractable approximate sums monte carlo sampling yielding ensemble {a}. resulting optimization corresponds exactly dropout algorithm. hannun case casper catanzaro diamos elsen prenger satheesh sengupta coates deepspeech scaling end-to-end speech recognition arxiv preprint arxiv. schmid part-of-speech tagging neural networks proceedings conference computational linguistics volume ser. coling stroudsburg association computational linguistics available http//dx.doi.org/./. criminisi shotton decision forests computer vision medical image analysis ser. advances computer vision pattern recognition. springer london available https//books.google.com/books?id=fa-naeacaaj searcy bartlett inversion processing component spatialrelational information faces. journal experimental psychology. human perception performance vol. aug. dahl sainath hinton improving deep neural networks lvcsr using rectiﬁed linear units dropout acoustics speech signal processing ieee international conference hinton srivastava krizhevsky sutskever salakhutdinov improving neural networks preventing co-adaptation feature detectors arxiv preprint arxiv. wilamowski iplikci kaynak algorithm fast convergence training neural networks proceedings international joint conference neural networks vol. yamins hong cadieu solomon seibert dicarlo performance-optimized hierarchical models predict neural responses higher visual cortex proceedings national academy sciences vol. anselmi leibo rosasco mutch tacchetti poggio unsupervised learning invariant representations hierarchical architectures arxiv preprint arxiv. kumar satoor buck fast parallel expectation maximization gaussian mixture models gpus using cuda high performance computing communications hpcc’. ieee international conference graves a.-r. mohamed hinton speech recognition deep recurrent neural networks acoustics speech signal processing ieee international conference", "year": 2015}