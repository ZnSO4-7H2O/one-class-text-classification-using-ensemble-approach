{"title": "Adversarial Generation of Natural Language", "tag": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "abstract": "Generative Adversarial Networks (GANs) have gathered a lot of attention from the computer vision community, yielding impressive results for image generation. Advances in the adversarial generation of natural language from noise however are not commensurate with the progress made in generating images, and still lag far behind likelihood based methods. In this paper, we take a step towards generating natural language with a GAN objective alone. We introduce a simple baseline that addresses the discrete output space problem without relying on gradient estimators and show that it is able to achieve state-of-the-art results on a Chinese poem generation dataset. We present quantitative results on generating sentences from context-free and probabilistic context-free grammars, and qualitative language modeling results. A conditional version is also described that can generate sequences conditioned on sentence characteristics.", "text": "generative adversarial networks gathered attention computer vision community yielding impressive results image generation. advances adversarial generation natural language noise however commensurate progress made generating images still behind likelihood based methods. paper take step towards generating natural language objective alone. introduce simple baseline addresses discrete output space problem without relying gradient estimators show able achieve state-of-the-art results chinese poem generation dataset. present quantitative results generating sentences context-free probabilistic context-free grammars qualitative language modeling results. conditional version also described generate sequences conditioned sentence characteristics. deep neural networks recently enjoyed success modeling natural language typically recurrent convolutional language models trained maximize likelihood observing word character given previous observations sequence models commonly trained using technique called teacher forcing inputs network ﬁxed model trained predict next item sequence given previous observations. corresponds maximum-likelihood training models. however one-step ahead prediction training makes model prone exposure bias exposure bias occurs model trained conditioned groundtruth contexts exposed errors important consequence exposure bias generated sequences degenerate small errors accumulate. many important problems machine translation abstractive summarization trained maximum-likelihood training objective require generation extended sequences evaluated based sequence-level metrics bleu rouge possible direction towards incorporating sequence-level training objective generative adversarial networks gans yielded impressive results modeling images advances natural language generation lagged behind. progress made recently incorporating objective sequence modeling problems including natural language generation. lamb adversarial criterion match hidden state dynamics teacher forced recurrent neural network samples output distribution across multiple time steps. unlike approach lamb sequence gans maximum-likelihood augmented gans adversarial loss outputs rnn. using outputs however isn’t trivial since sampling outputs feed discriminator non-differentiable operation. result gradients cannot propagate generator discriminator. policy gradient estimate generator’s gradient present importance sampling based technique. alternatives include reinforce gumbel softmax straighthrough estimator among others. work address discrete output space problem simply forcing discriminator operate continuous valued output distributions. discriminator sees sequence probabilities every token vocabulary generator sequence -hot vectors true data distribution fig. technique identical proposed gulrajani parallel work this. paper provide complete empirical investigation approach applying gans discrete output spaces. present results using recurrent well convolutional architectures three language modeling datasets different sizes word character-level. also present quantitative results generating sentences adhere simple context-free grammar richer probabilistic context-free grammar compare method previous works objective generate natural language chinese poetry generation dataset. addition present conditional generates sentences conditioned sentiment questions. gans general framework used training generative models formulating learning process player minimax game formulated equation below. generator network tries generate samples close possible true data distribution interest ﬁxed noise distribution refer samples produced generator discriminator network trained distinguish samples true data distribution generator network trained using gradient signals sent discriminator minimizing log)). goodfellow shown that respect optimal discriminator minimax formulation gans reported notoriously hard train practice several techniques proposed alleviate complexities involved getting work including modiﬁed objective functions regularization discuss problems following subsection. nowozin show possible train gans variety f-divergence measures besides jsd. wasserstein gans minimize earth mover’s distance wasserstein distance least squared gans modiﬁes replaces loss loss. wgan-gp incorporate gradient penalty term discriminator’s loss wgan objective acts regularizer. work compare objectives context natural language generation. arjovsky bottou argue part problem training regular gans seeks minimize generator trying optimized log)) gradients receives vanish discriminator trained optimality. authors also show trying optimize practical alternative −log)) generator might suffer vanishing gradients receives unstable training signals. also important consider fact highly structured data like images language low-dimensional manifolds wassterstein gans overcome problems regular training providing softer metric compare distributions lying dimensional manifolds. contribution work identifying importance lipschitz constraint achieved clamping weights discriminator ﬁxed interval. lipschitz constraint training discriminator multiple times every generator gradient update creates strong learning signal generator. gulrajani present alternative weight clamping call gradient penalty enforce lipschitzness since model performance reported highly sensitive clamping hyperparameters. following penalty discriminator training objective gd)|| potential concern regarding strategy train discriminator distinguish sequence -hot vectors true data distribution sequence probabilities generator discriminator easily exploit sparsity -hot vectors reach optimality. however wassterstein distance lipschitz constraint gradient penalty provides good gradients even optimal discriminator isn’t problem practice. even though possible extract performance regular objective gradient penalty wgans still provide better gradients generator since discriminator doesn’t saturate often. model architecture input generator network attempt generate natural language. implementation convenience sample shape length sequence ﬁxed length dimension noise vector time step. generator transforms sequence probability distributions vocabulary size size true data distribution’s vocabulary. discriminator network provided fake samples samples true data distribution samples true distribution provided sequence -hot vectors vector serving indicator observed word sample. described section discriminator trained discriminate real fake samples generator trained fool recurrent neural networks particularly long short-term memory networks gated recurrent networks powerful models successful modeling sequential data transform sequence input vectors sequence hidden states hidden state maintains summary input until then. language models autoregressive nature since input network time depends output time however context generating sequences noise inputs pre-determined direct correspondence output time input time fundamentally changes auto-regressiveness rnn. however carry forward information output time subsequent time steps hidden states evident recurrent transition function. order incorporate explicit dependence subsequent outputs peephole connection between output probability distribution time hidden state time show lstm equations below. typical language models shared afﬁne transformation matrix wout shared across time steps projects hidden state vector vector size target vocabulary generate sequence outputs subsequently softmax function applied vector turn probability distribution vocabulary. inference output sampled softmax distribution becomes input subsequent time step. training inputs pre-determined. models perform greedy decoding always pick argmax using lstm discriminator simple binary logistic regression layer last hidden state determine probability sample generator’s data distribution real data distribution. tanh tanh element-wise sigmoid function hadamard product tanh element-wise tanh function. learn-able parameters model constitute input forget output cell states lstm respectively. convolutional models convolutional neural networks also shown promise modeling sequential data using -dimensional convolutions convolution ﬁlters convolved across time input dimensions treated channels. work explore convolutional generators discriminators residual connections gulrajani convolutional model generator discriminator. generator consists residual blocks convolutional layers each. ﬁnal convolution layer transforms output residual blocks sequence un-normalized vectors element input sequence vectors normalized using softmax function. convolutions ’same’ convolutions stride followed batchnormalization relu activation function without pooling preserve shape input. discriminator architecture identical generator ﬁnal output single output channel. likelihood based training generative language models models trained make one-step ahead predictions result possible train models relatively long sequences even initial stages training. however adversarial formulation generator encouraged generate entire sequences match true data distribution without explicit supervision step generation process. provide training signals incremental difﬁculty curriculum learning train generator produce sequences gradually increasing lengths training progresses. based methods often critiqued lacking concrete evaluation strategy however recent work uses annealed importance based technique overcome problem. context generating natural language possible come simpler approach evaluate compute likelihoods generated samples. synthesize data generating distribution compute likelihoods tractable manner. propose simple evaluation strategy evaluating adversarial methods generating natural language constructing data generating distribution p−cfg. possible determine sample belongs probability sample p−cfg using constituency parser provided productions grammar. also present simple idea estimate likelihood generated samples using randomly initialized lstm data generating distribution. viable strategy evaluate generative models language randomly initialized lstm provides little visibility complexity data distribution presents obvious increase complexity. cfgs pcfgs however provide explicit control complexity productions. also learned grammar induction large treebanks natural language data generating distribution synthetic typical language models evaluated measuring likelihood samples true data distribution model. however gans impossible measure likelihoods under model measure likelihood model’s samples true data distribution instead. simple simple publicly available contains productions. generate sets data consisting samples length another length contains samples selected random cfg. ﬁrst vocabulary tokens second tokens. evaluate models task measuring fraction generated samples satisfy rules grammar also measure diversity generated samples. generating samples noise computing fraction valid grammar using earley parsing algorithm order measure sample diversity simply count number unique samples; assumes samples orthogonal still serves proxy measure entropy. compare various generator discriminator objectives problem. penn treebank pcfg construct challenging problem simple sections subsection penn treebank induce pcfg using simple count statistics productions. train model sentences treebank restrict output vocabulary frequently occurring words. evaluate models task measuring likelihood sample using viterbi chart parser measure mostly captures grammaticality sentence still reasonable proxy sample quality. chinese poetry zhang lapata present dataset chinese poems used evaluate adversarial training methods natural language dataset consists -line poems variable number characters line. treat line poem training example lines length train/validation/test split speciﬁed bleu- bleu- measure model performance task. since obvious target generated sentence works report corpus-level bleu measures using entire test reference. language generation generate language three different datasets varying sizes complexity. dataset comprising simple english sentences henceforth refer cmu−se version penn treebank commonly used language modeling experiments google -billion word dataset perform experiments generating language word well character-level. cmu−se dataset consists sentences vocabulary words penn treebank consists sentences vocabulary words. random subset million sentences -billion word dataset constrain vocabulary frequently occurring words. curriculum learning strategy lstm models starts training sentences length word level characters increases sequence length ﬁxed number epochs based size data. convolutional methods able generate long sequences even without curriculum however found critical generating long sequences lstm. conditional generation sequences gans able leverage explicit conditioning high-level attributes data generate samples contain attributes. recent work generates sentences conditioned certain attributes language sentiment using variational autoencoders holistic attribute discriminators. paper features inherent language sentiment questions. generate sentences questions cmu−se dataset label sentences contain questions rest statements. generate sentences positive negative sentiment amazon review polarity dataset collected ﬁrst million short reviews vocabulary frequently occurring words. conditioning sentence attributes achieved concatenating single feature containing either entirely ones zeros indicate presence absence attribute output convolutional layer. conditioning done generator discriminator. experiment conditional gans using convolutional methods since methods adding conditioning information well studied architectures. models trained using back-propagation algorithm updating parameters using adam optimization method stochastic gradient descent batch sizes learning rate used lstm generator discriminators convolutional architectures learning rate noise prior lstm hidden dimensions except chinese poetry generation task table. presents quantitative results generating sentences adhere simple described section column computes accuracy model generates samples using sample generations. observe models able sequences length wgan wgan-gp objectives able generalize longer sequences length motivated wgan wgan-gp objectives subsequent experiments. gan-gp criterion appears perform reasonably well restrict experiments wgan wgan-gp criteria only. gans shown exhibit phenomenon mode dropping generator fails capture large fraction modes present data generating distribution therefore important study diversity generated samples. uniq column computes number unique samples sample generations serves rough indicator sample diversity. wgangp objective appears encourage generation table accuracy uniqueness measure samples generated different models. lstm lstm-p refers lstm model output peephole wgan-gp gan-gp refer models gradient penalty discriminator’s training objective fig. shows negative-log-likelihood generated samples using lstm architecture using wgan-gp gan-gp criteria. models used lstm generator. sequence length likelihoods evaluated every epoch samples. table. contains quantitative results chinese poetry generation dataset. results indicate straightforward strategy overcome back-propagating discrete states competitive outperforms complicated methods. contains sequences generated questions/statements. model able pick certain consistent patterns questions well expressing sentiment generating sentences. conclusion work presents straightforward effective method train gans natural language. simplicity lies forcing discriminator operate continuous values presenting sequence probability distributions generator sequence -hot vectors corresponding data true distribution. propose evaluation strategy involves learning data distribution deﬁned pcfg. lets evaluate likelihood sample belonging data generating distribution. wgan wgan-gp objectives produce realistic sentences datasets varying complexity also show possible perform conditional generation text high-level sentence features sentiment questions. future work would like explore gans domains goal-oriented dialog systems clear train-billion-word opposition growing china undergoing operation year everyone blame everyone shares miller seems converted president democrat actually best children eventual policy weak companies upheld respectively patented saga ambac. independence unit lights wrap annually morocco town registration matched citizens holl hubby timers faller work jith linter revent good homeostasis money well spent kickass cosamin time great britani lovethis. question <s>when friday convention </s> <s>how many snatched crew </s> <s>we record </s> <s>how open hall </s> table coditional generation text. shows generated samples conditionally trained amazon review polarity dataset attributes ’positive’ ’negative’. bottom samples conditioned ’question’ attribute authors would like thank ishaan gulrajani martin arjovsky guillaume lample rosemary juneki hong varsha embar advice insightful comments. grateful fonds recherche qu´ebec nature technologie ﬁnancial support. would also like acknowledge nvidia donating dgx- computer used work. demonstrate approach solve problem discrete outputs produces reasonable outputs even applied images. figure shows samples generated binarized mnist dataset used generator discriminator architecture identical wgan-gp criterion. generator’s outputs continuous samples true data distribution binarized. references martin arjovsky l´eon bottou. towards principled methods training generative adversarial networks. nips workshop adversarial training. review iclr. volume samy bengio oriol vinyals navdeep jaitly noam shazeer. scheduled sampling sequence prediction recurrent neural networks. advances neural information processing systems. pages yoshua bengio j´erˆome louradour ronan collobert jason weston. curriculum learning. proceedings annual international conference machine learning. pages eric brill. automatic grammar induction parsing free text transformation-based approach. proceedings workshop human language technology. association computational linguistics pages tong yanran ruixiang zhang devon hjelm wenjie yangqiu song yoshua bengio. maximum-likelihood augmented discrete arxiv preprint generative adversarial networks. arxiv. ciprian chelba tomas mikolov mike schuster thorsten brants phillipp koehn tony robinson. billion word benchmark measuring progress statistical language modeling. arxiv preprint arxiv. kyunghyun bart merri¨enboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. vincent dumoulin ishmael belghazi poole alex lamb martin arjovsky olivier mastropietro aaron courville. adversarially learned inference. arxiv preprint arxiv. gauthier. conditional generative adversarial nets convolutional face generation. class project stanford convolutional neural networks visual recognition winter semester goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adadvances neural information versarial nets. processing systems. pages alex graves j¨urgen schmidhuber. ofﬂine handwriting recognition multidimensional recurrent neural networks. advances neural information processing systems. pages kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recogproceedings ieee conference nition. computer vision pattern recognition. pages sergey ioffe christian szegedy. batch normalization accelerating deep network training arxiv preprint reducing internal covariate shift. arxiv. salimans goodfellow wojciech zaremba vicki cheung alec radford chen. improved techniques training gans. advances neural information processing systems. pages xiang zhang junbo zhao yann lecun. character-level convolutional networks text clasadvances neural information prosiﬁcation. cessing systems. pages klein christopher manning. parsing fast exact viterbi parse selection. proceedings conference north american chapter association computational linguistics human language technology-volume association computational linguistics pages alex lamb anirudh goyal alias parth goyal ying zhang saizheng zhang aaron courville yoshua bengio. professor forcing algorithm training recurrent networks. advances neural information processing systems. pages tomas mikolov martin karaﬁ´at lukas burget cernock`y sanjeev khudanpur. recurrent neural network based language model. interspeech. volume page vinod nair geoffrey hinton. rectiﬁed linear units improve restricted boltzmann machines. proceedings international conference machine learning pages ryota tomioka. f-gan training generative neural samplers using variational divergence minimization. advances neural information processing systems. pages kishore papineni salim roukos todd ward weijing zhu. bleu method automatic evalproceedings uation machine translation. annual meeting association computational linguistics. association computational linguistics pages alec radford luke metz soumith chintala. unsupervised representation learning deep convolutional generative adversarial networks. arxiv preprint arxiv.", "year": 2017}