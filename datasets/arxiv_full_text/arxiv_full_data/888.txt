{"title": "Depth Creates No Bad Local Minima", "tag": ["cs.LG", "cs.NE", "math.OC", "stat.ML"], "abstract": "In deep learning, \\textit{depth}, as well as \\textit{nonlinearity}, create non-convex loss surfaces. Then, does depth alone create bad local minima? In this paper, we prove that without nonlinearity, depth alone does not create bad local minima, although it induces non-convex loss surface. Using this insight, we greatly simplify a recently proposed proof to show that all of the local minima of feedforward deep linear neural networks are global minima. Our theoretical results generalize previous results with fewer assumptions, and this analysis provides a method to show similar results beyond square loss in deep linear models.", "text": "deep learning depth well nonlinearity create non-convex loss surfaces. then depth alone create local minima? paper prove without nonlinearity depth alone create local minima although induces non-convex loss surface. using insight greatly simplify recently proposed proof show local minima feedforward deep linear neural networks global minima. theoretical results generalize previous results fewer assumptions analysis provides method show similar results beyond square loss deep linear models. deep learning recently profound impact machine learning computer vision artiﬁcial intelligence communities. addition practical successes previous studies revealed several reasons deep learning successful viewpoint model classes. simpliﬁed explanation harmony great expressivity data great expressivity deep learning less bias large training dataset leads less variance. great expressivity seen aspect representation learning well whereas traditional machine learning makes features designed human users experts type prior deep learning tries learn features data well. accurately aspect model classes deep learning generalization property; despite great expressivity deep learning model classes maintain great generalization properties would distinguish deep learning possibly ﬂexible methods shallow neural networks many hidden units traditional kernel methods powerful kernel. therefore practical success deep learning seems supported great quality model classes. however great model class useful cannot good model model class training. training deep model typically framed non-convex optimization. non-convexity high dimensionality unclear whether eﬃciently train deep model. note diﬃculty comes combination non-convexity high dimensionality weight parameters. reformulate training problem several decoupled training problems small number weight parameters eﬀectively train model non-convex optimization theoretically shown bayesian optimization global optimization literatures result non-convexity high-dimensionality shown training general neural network model np-hard however hardnessresult worst case analysis would tightly capture going practice seem able eﬃciently train deep models practice. understand practical success beyond worst case analysis theoretical practical investigations training deep models recently become active research area model represents function mapping input output diﬀerent distinct settings weight space. accordingly many distinct globally optimal points many distinct points loss values weight–space symmetries would result non-convex epigraph well non-convex sublevel sets thus unclear whether depth create diﬃcult non-convex loss surface. recent work indirectly showed consequence main theoretical results depth create local minima deep linear model frobenius norm although creates potentially saddle points. paper directly prove local minima deep linear model corresponds local minima shallow model. building upon theoretical insight propose simpler proof main results recent work local minima feedforward deep linear neural networks frobenius norm global minima. power proof beyond frobenius norm long loss function satisﬁes theorem local minima deep linear model corresponds local minimum shallow model. rdi×di− weight matrix rd×m input training data target training data. min≤i≤h index corresponding smallest width. note rank analyze optimization problem also consider following optimization problem shallow linear model equivalent problem terms global minimum value note problem non-convex unless whereas problem non-convex even words deep parameterization creates non-convex loss surface even without nonlinearity. though consider frobenius loss here proof holds general cases. long loss function satisﬁes theorem local minima deep linear model corresponds local minimum shallow model. ﬁrst main result states even though deep parameterization creates non-convex loss surface create local minima. words every local minimum problem corresponds local minimum problem theorem generalizes main results fewer assumptions. following theoretical work random matrix theory recent work showed strong assumptions local minima global minima class nonlinear deep networks. furthermore recent work proved following properties class general deep linear networks arbitrary depth width objective function non-convex non-concave; local minima global minima; every critical point saddle point; saddle point hessian negative eigenvalue shallow networks hidden layer whereas saddle points exist deeper networks. theorem generalizes second statement fewer assumptions; previous papers assume data matrix distinct eigenvalues whereas assume that. order deduce proof theorem need fundamental facts linear algebra. next lemmas recall basic facts perturbation theory singular value decomposition lemma implies perturbed matrix perturbation original matrix full rank condition. formally lemma full-rank matrix singular value decomposition perturbation then exists perturbation perturbation perturbation proof small perturbation matrix lemma shows singular values change much. thus small enough ¯σi| also small remember singular values positive. letting contain singular value lemma thus lemma implies singular space perturbed matrix corresponding singular value initial matrix change much. statement lemma follows combining result diﬀerent singular values together prove theorem induction. easily show perturbation product matrices product matrix perturbation matrix. product speciﬁc matrices induction perturbation product product perturbation perturbations matrix. perturbation also product perturbations speciﬁc matrices proves statement follows lemma exists perturbation perturbation ¯sigma perturbation rank small perturbation positive singular values remain strictly positive whereby rank together assumption rank rank note hence diagonal matrix. remember perturbation thus perturbation then perturbation perturbation proves case inductive step given lemma holds case consider case ¯wk+ index deﬁned denote i-th element then exists note written product matrices thus inductive hypothesis perturbation rank exists desired matrices perturbation perturbation product equal meanwhile either matrix matrix rank rank follows rank rank thus setting apply proof case conclude exists perturbation wiwi. combined statement inductive hypothesis implies lemma whereby ﬁnish proof induction. idea proof change weight keep weights becomes convex least square problem. able perturb maintain objective value well perturbation full rank. proof theorem notational convenience local minimum local minimum svds respectively diagonal matrix ﬁrst terms strictly positive minimizing least square problem normal equation proof theorem theorem lemma show perturb ¯wp− sequence make sure perturbed weight still optimal solution rank similar strategy make sure rank proves whole theorem. then follows lemma local minimum local minimum ¯wp−. follows theorem exists close enough local minimum rank note perturbation whereby lemma exists ˆwp− perturbations ¯wp− respectively ˆwp− proof theorem ﬁrst show need consider case identity matrix diagonal matrix noticing rotation invariant frobenius norm. show local minimum must block diagonal symmetric matrix block term projection matrix space corresponding eigenvalue diagonal matrix finally show projection matrices must onto eigenspace corresponding large possible eigenvalues shows local minimum shares function value. note left hand symmetric matrix thus also symmetric matrix. meanwhile symmetric matrix whereby r-block diagonal matrix block corresponding diagonal terms therefore also r-block diagonal matrix. show local minima must global minima. local minima block diagonal matrix thus assume without loss generality square matrices symmetric rows columns change anything. thus follows symmetric matrix. remember either whereby proven that even though depth creates non-convex loss surface create local minima. based insight successfully proposed simple proof fact local minima feedforward deep linear neural networks global minima corollary. beneﬁts results limited simpliﬁcation previous proof. example results apply problems beyond square loss. consider shallow problem minimize s.t. rank deep parameterization counterpart minimize analysis shows function long satisﬁes theorem local minimum corresponds local minimum limited least square loss depth creates local minima. addition analysis directly apply matrix completion unlike previous results. show local minima symmetric matrix completion problem global high probability. authors would like thank professor robert freund professor leslie pack kaelbling generous support. also want thank cheng helpful discussions.", "year": 2017}