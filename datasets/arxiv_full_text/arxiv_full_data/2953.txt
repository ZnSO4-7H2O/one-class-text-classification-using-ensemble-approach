{"title": "Using Feature Weights to Improve Performance of Neural Networks", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "Different features have different relevance to a particular learning problem. Some features are less relevant; while some very important. Instead of selecting the most relevant features using feature selection, an algorithm can be given this knowledge of feature importance based on expert opinion or prior learning. Learning can be faster and more accurate if learners take feature importance into account. Correlation aided Neural Networks (CANN) is presented which is such an algorithm. CANN treats feature importance as the correlation coefficient between the target attribute and the features. CANN modifies normal feed-forward Neural Network to fit both correlation values and training data. Empirical evaluation shows that CANN is faster and more accurate than applying the two step approach of feature selection and then using normal learning algorithms.", "text": "different features different relevance particular learning problem. features less relevant; important. instead selecting relevant features using feature selection algorithm given knowledge feature importance based expert opinion prior learning. learning faster accurate feature importance account. correlation aided neural networks presented algorithm. cann treats feature importance correlation coefficient target attribute features. cann modifies normal feedforward neural network correlation values training data. empirical evaluation shows cann faster accurate applying step approach feature selection using normal learning algorithms. feature selection improve performance inductive machine learning algorithms. many learning problems large feature many redundant features. thus extracting useful features learning improves performance considerably however feature selection algorithms preprocessors select modify dataset discarding less relevant features. preprocessing stage machine learning algorithms treat features equal importance. however less-relevant features still contribute much totally discarding impede accuracy; reason feature selection degrades performance cases. features important learning problem. ranking based importance towards learning problems generated. fact ranking measures used many feature selection algorithms given less amount data. training data scarce fields. data mining problems data issue goal learn massive data warehouses. however learning problems made good progress ones data limited. hence high accuracy scarce data must real world machine learning. machine learning algorithms inductive require large portion data. amount data needed focus statistical learning theory however developing refined inductive algorithms solution scarcity data problem. learning theory fundamental limit much knowledge learned data. solution provide external knowledge along data. focus paper learn faster given external knowledge form feature importance weight. instead treating features equally learners treat features based importance knowledge importance learning shown perform better. field machine learning research gaining attention. iann extended multilayer perceptrons feature importance values. domain knowledge provided feature weights real value range represent importance feature. iann performed better many empirical learning algorithms. iann also required significantly less training data perform well. much important improved accuracy acquiring training data expensive domains research uses different approach iann system. present cann neural network system feature importance values learning process attain better performance. robust theoretically sound iann. iann based heuristics little theoretical justification cann based principles neural network backpropagation itself. correlation coefficient popular measure relationship random variables correlation coefficient measures variables linearly influence other. words variables strongly correlated expected change together. correlation coefficient feature target variable measure much influence feature target variable. good measure importance feature. consider feature importance correlation coefficient output variable input feature consideration. definition feature importance feature true correlation coefficient output variable 𝐼𝐼𝑘𝑘=𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐= 𝑐𝑐𝑐𝑐𝑐𝑐 𝜎𝜎𝑦𝑦𝜎𝜎𝑋𝑋𝑘𝑘 𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐= 𝑐𝑐𝑐𝑐𝑐𝑐 𝜎𝜎𝑦𝑦𝜎𝜎𝑋𝑋𝑘𝑘 =𝐸𝐸�� 𝑋𝑋𝑘𝑘−𝜇𝜇𝑋𝑋𝑘𝑘� �𝑦𝑦−𝜇𝜇𝑦𝑦�� 𝜎𝜎𝑦𝑦𝜎𝜎𝑋𝑋𝑘𝑘 𝑐𝑐𝑐𝑐𝑐𝑐=𝐸𝐸�� 𝑋𝑋𝑘𝑘−𝜇𝜇𝑋𝑋𝑘𝑘��𝑦𝑦−𝜇𝜇𝑦𝑦�� =𝐸𝐸−𝐸𝐸 𝑐𝑐𝑐𝑐𝑐𝑐=n�� 𝑋𝑋𝑘𝑘𝑖𝑖−𝑋𝑋𝑘𝑘����� �𝑦𝑦𝑖𝑖−𝑦𝑦�� 𝑋𝑋𝑘𝑘𝑦𝑦�����−𝑋𝑋𝑘𝑘����.𝑦𝑦� cann algorithm given training data correlation values features. feature importance values derived different ways. deduced experts calculated different datasets. normal multilayer perceptrons fits training dataset. goal cann algorithm learned neural network fits dataset correlation values given. thus would like minimize correlation error well. learned model training feature importance learned model output instance dataset correlation coefficient generated. feature importance target correlation value want learned model training set. iann algorithm used feature importance values changing learning rate based importance. connections input features first hidden layer nodes different learning rates scaled feature importance value. weights also initialized important features higher probability larger initial weight. heuristic being important features overall higher connection weight redundant features less influence thus less overall connection weight. heuristic shown successful experimentally; even doesn’t theoretical foundation. cann uses different design changing search objective instead variation learning rate. cann considers feature weights correlation input features output. algorithm thus tries given correlation well fitting data. thus cann adding additional constraints backpropagation. paper cann applied several real world datasets different domains. performance analyzed compared many popular learning algorithms. brief outline paper section provides theoretical background behind cann section describes actual algorithm section shows experimental results. section concludes paper. describe notation used paper. number instances number features. used refer feature importance feature represents features data learning model covariance output neural network given instance consequently also represents actual output node whenever mentioned subscript. components neural networks represented layered fashion. layer superscript always represents subscripts represent nodes successive layers. therefore 𝑤𝑤𝑖𝑖𝑖𝑖𝑘𝑘 weight node layer node layer fashion ℎ𝑖𝑖𝑛𝑛 output node layer 𝜃𝜃𝑖𝑖 bias node represents error unit learning rate. layerk units units input represents mean. notion features important learning problem others central machine learning. many research attempts provide measure importance firm saliency goal researches primarily feature selection. however instead calculating importance values. would like quantify importance something learning model. therefore 𝛥𝛥𝐸𝐸=p𝜕𝜕𝐸𝐸𝐷𝐷𝜕𝜕𝑤𝑤+ 𝜕𝜕𝐸𝐸𝐶𝐶𝜕𝜕𝑤𝑤 𝑊𝑊𝑁𝑁 𝜕𝜕𝐸𝐸𝐶𝐶𝜕𝜕𝑤𝑤𝑖𝑖𝑦𝑦𝑛𝑛=��ck−𝑐𝑐𝑐𝑐𝑐𝑐��−𝜕𝜕𝜕𝜕𝑤𝑤cov� 𝜕𝜕𝜕𝜕𝑤𝑤𝑖𝑖𝑦𝑦𝑛𝑛cov= 𝜕𝜕𝜕𝜕𝑤𝑤𝑖𝑖𝑦𝑦𝑛𝑛 n�𝑋𝑋𝑘𝑘𝑌𝑌 −𝑋𝑋𝑘𝑘���𝑛𝑛�𝑌𝑌𝑁𝑁 x�.𝑛𝑛�hjn 𝑦𝑦𝑑𝑑�n�xkhjn ��������− x�.hjn���� =𝑦𝑦𝑑𝑑� xk.hjn 𝜕𝜕𝐸𝐸𝐶𝐶𝜕𝜕𝑤𝑤=−��𝑐𝑐𝑘𝑘−𝑐𝑐𝑐𝑐𝑐𝑐�𝑦𝑦𝑑𝑑� 𝑥𝑥𝑘𝑘.ℎ𝑖𝑖𝑁𝑁 ��������− 𝑋𝑋�.ℎ𝑖𝑖𝑁𝑁 ����� training rule normal backpropagation normally derived using stochastic gradient descent whole dataset iterated make change training output layer calculates error propagated backward lower layers. error output simple backpropagation simply changed summed dataset training rule cann includes sample covariance sample mean terms must calculated entire dataset time single instance iterated. computationally costly. cannot pre-calculate mean terms depend output network instance; turn dependent configuration network. time weight updated sample mean also changes. instead calculating means every time memoization iterative improvement calculate mean. requires space order space complexity remains unchanged. multilayer perceptrons feed-forward neural networks neurons nodes connected feed forward fashion many layers. connection allowed successive layers. normally input output layer hidden layers. logistic activation function trained minimize error backward manner. algorithm known backpropagation tries minimize least squared error dataset using stochastic gradient descent find suitable weights connections nodes general form feed-forward neural network 𝑦𝑦=𝜎𝜎��𝑤𝑤𝑖𝑖𝑦𝑦𝑛𝑛 ℎ𝑖𝑖𝑛𝑛 𝜃𝜃𝑦𝑦� ℎ𝑖𝑖𝑘𝑘 =𝜎𝜎� 𝑤𝑤𝑖𝑖𝑖𝑖𝑘𝑘−ℎ𝑖𝑖𝑘𝑘− 𝜃𝜃𝑖𝑖� ℎ𝑖𝑖=𝑋𝑋𝑖𝑖 𝛥𝛥𝐸𝐸𝐷𝐷=𝜕𝜕𝐸𝐸𝐷𝐷𝜕𝜕𝑤𝑤𝑖𝑖𝑦𝑦𝑛𝑛= 𝜕𝜕𝜕𝜕𝑤𝑤𝑖𝑖𝑦𝑦𝑛𝑛 =−�𝑦𝑦𝑑𝑑ℎ𝑖𝑖𝑁𝑁 𝐻𝐻𝑒𝑒𝑐𝑐𝑒𝑒 𝜕𝜕𝜕𝜕𝑤𝑤𝑖𝑖𝑦𝑦𝑛𝑛𝑦𝑦𝑑𝑑=𝑦𝑦𝑑𝑑ℎ𝑖𝑖𝑁𝑁 target value input output network input weights connected output layer. algorithm works changing error function mlps minimize correlation error well. extend error function normal backpropagation include well. therefore instead minimizing data error backpropagation would minimize both. would also thus output value training instance changed changed mean calculated subtracting previous value adding value. reason storing element values table. instance training calculate value network replace value table value also change mean subtract-add formula described. moving average real mean eventually reach convergence several training epochs dataset. similar stochastic gradient descent approach used updating weights. tested cann datasets university california irvine repository. datasets chosen relatively higher number features also known especially difficult. data sets used described below algorithms compared include classic high performing ones. tested standard feed-forward neural network support vector machines k-nearest neighbour naïve bayes table experiment results percent accuracy order valid correlation based knowledge decided calculate correlation features using full dataset. done datasets except promoters associated expert knowledge. importance values promoters dataset derived knowledge. correlation values given cann. algorithms trained using data rest used test set. experiments averaged iterations. results shown table results show clear difference cann normal neural network. cann outperforms significantly pronounced difficult datasets arrhythmia promoters average performance algorithms lower. cann generally outperforms algorithms. except promoters spambase particular algorithm outmatches others. reason problem specifically suitable particular algorithm. even cann performs next best. performance difference iann statistically significant. iann outperforms datasets slightly. however general trend emerges. cann clearly makes better feature importance weight given. next experiment shown table feature selection used inductive algorithms cann iann used full feature set. features removed using chi-squared evaluation ranking. datasets used training rest tests. results table show using feature selection always improve performance. apart changes performance makeup almost remained unchanged. algorithms better performance dataset performance actually decreased some. uniform trend. results slightly increased cases. surpassed cann spambase dataset. overall advantage cann normal algorithms remained. cann performed best average. evident curves cann initially performs better learns faster. dataset size increases eventually closes performance gap. however trend true cases performance depended difference correlation value provided cann correlation makeup dataset. correlations dataset close true correlation cann show less performance improvement knowledge advantage; case spambase performs better cann. hand feature weights promoters derived expert domain theory. always outperforms normal mlp. research using different error function squared error comprehensive. many different seemingly exotic error functions tried success however domain knowledge instead training data limited. kbann family algorithms incorporated rule based domain theory initializing network structure weights additional constraints along tradition error function explored before. called constraint based learning. certain derivatives target function specified prior. approach explored simard provided tangentprop simard p.s. victoni lecun denker tangent prop-a specifying selected invariances adaptive network. advances neural information processing systems. mateo morgan kaufmann. additional constraint backpropagation input derivatives. knowledge-based used domain knowledge provided additional constraints support vector machines mustafa showed provide hints additional constraint proposed well performed approach incorporating feature neural network learning. performance learner shows feature importance aided learners achieve superior performance ordinary inductive irrelevant features feature selection good approach however expert knowledge domains correlation features could calculated different problem dataset well. extra knowledge could transferred cann attain higher performance. approach incorporating feature importance learners worthy development. possible future applications algorithm areas related machine learning problems solved expert knowledge available. future research areas modifications existing popular empirical learners utilize feature importance. correlation coefficient aided algorithms maybe developed algorithms support vector machines; decision tree based algorithms bayesian classifiers. current machine learning algorithms rely much training examples. incorporating domain knowledge using knowledge related problem area improvement. proposed method shows improvements methods.", "year": 2011}