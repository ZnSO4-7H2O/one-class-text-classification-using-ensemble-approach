{"title": "Robustness of classifiers to uniform $\\ell\\_p$ and Gaussian noise", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "We study the robustness of classifiers to various kinds of random noise models. In particular, we consider noise drawn uniformly from the $\\ell\\_p$ ball for $p \\in [1, \\infty]$ and Gaussian noise with an arbitrary covariance matrix. We characterize this robustness to random noise in terms of the distance to the decision boundary of the classifier. This analysis applies to linear classifiers as well as classifiers with locally approximately flat decision boundaries, a condition which is satisfied by state-of-the-art deep neural networks. The predicted robustness is verified experimentally.", "text": "arbitrary covariance matrix. characterize robustness random noise terms distance decision boundary classiﬁer. analysis applies linear classiﬁers well classiﬁers locally approximately decision boundaries condition satisﬁed state-of-the-art deep neural networks. predicted robustness veriﬁed experimentally. image classiﬁcation techniques recently witnessed major advances leading record performances challenging datasets besides reaching classiﬁcation error equally important classiﬁers deployed real-world environments correctly classify perturbed noisy samples. speciﬁcally suﬃciently small perturbation alters sample desirable estimated label classiﬁer remains unchanged. altering perturbations take various forms additive perturbations geometric transformations occlusions image data. analysis robustness classiﬁers perturbation regimes crucial unraveling fundamental vulnerabilities. example state-of-the-art image classiﬁers recently empirically shown vulnerable wellsought imperceptible additive perturbations physically plausible nuisances goal paper derive precise quantitative results robustness general classiﬁers random noise. proposed bounds predict accurately robustness classiﬁers. ﬁnally show analysis predicts high robustness classiﬁers image quantization conﬁrms previous empirical evidence. related work. robustness properties linear kernel classiﬁers studied robust optimization approaches constructing robust classiﬁers proposed recently robustness properties deep neural networks investigated. particular shows deep neural networks robust worst-case adversarial perturbations. several works followed attempted provide explanations vulnerability particular shown theoretically ratio robustness random noise robustness adversarial perturbations measured norm scales linear classiﬁers general classiﬁcation functions therefore data suﬃciently high dimensional robustness adversarial perturbations small gives explanation imperceptible nature perturbations. work generalizes broader noise regimes sparse noise quantization noise correlated gaussian noise. indeed follow similar methodology ﬁrst establish results linear case extend results nonlinear classiﬁers satisfying locally approximately decision boundary. outline. paper organized follows. section introduces framework robustness random adversarial perturbations. section presents theoretical estimates robustnesses linear classiﬁers generalized section classiﬁers locally approximately decision boundary. section details experiments showing validity bounds state-of-the-art classiﬁers exposing applications results. argmaxk denotes component goal paper analyze robustness random perturbations input. that consider arbitrary distribution interpret giving direction noise measure length minimal scaling applied required change estimated label probability least precisely random variable distributed according given deﬁne empty paper focus families choices ﬁrst family parameterized real number distribution i.e. )/p. setting distribution write rνε. observe euclidean norm invariant orthonormal basis change case i.e. depends basis chosen write signal hence dependence also holds rpε. diﬀerent choices allow span range realistic noise models. example choosing leads sparse noise vectors modeling generally normalized scale. distribution multivariate normal distribution mean covariance matrix notation setting. special case family therefore additive white gaussian noise note however family much broader model noise correlated input assumption made remainder paper goal derive bounds robustness classiﬁers random noise sampled either families. ﬁrst deﬁne quantity analysis robustness worst-case perturbations words estimated label classiﬁer equivalently distance data point decision boundary classiﬁer. often alternatively referred adversarial perturbation corresponds least noticeable perturbation adversary would apply fool classiﬁer. note that like heavily depends choice norm thus choice orthonormal basis. figure illustrates dependence perturbation. perturbations subject intense studies used derive guarantees robustness random noise. simplicity exposition state results binary classiﬁers extend results multi-class classiﬁers supplementary material. proofs also found supplementary material. figure illustration noise diﬀerent values first histogram uniformly sampled noise unit ball second example noise image. third example noisy image. note diﬀerent values result perceptually diﬀerent noise images. result demonstrates well estimated times multiplicative factor independent order multiplicative factor becomes previously shown factor depends choice classiﬁer vector dependence expected p-norm depends choice basis. dependence takes account relation choice basis write signal direction chosen classiﬁer. example classiﬁer uses ﬁrst component signal. problem eﬀectively becomes one-dimensional ﬁrst coordinate matters r∗∞∞. order proposition random direction uniformly distributed unit -sphere then result asymptotic valid random decision hyperplanes experimentally show section dependence allows propose estimate providing good approximation robustness random noise. factor determined convex combination eigenvalues precisely eigenvectors eigenvalues loss generality) factor given weighted average eigenvalues giving formal deﬁnition describe main idea behind locally approximately flat decision boundary model. model requires decision boundary locally sandwiched hyperplanes parallel tangent hyperplane. hold every point decision boundary hold closest points decision boundary data points. deﬁnition binary classiﬁer smooth decision boundary deﬁne hyperplane tangent point halfspace points side hyperplane parallel passes though point similarly halfspace points side hyperplane parallel passes though point γ-locally approximately flat point diﬀerently p-ball centered radius model assumes decision boundary locally approximated hyperplane vicinity images sampled data distribution. noted that order able deﬁne locality need distance measure thus property depends implicitly choice norm. model corresponds prior empirical evidence shown state-of-the-art deep neural network clasapproximately along random directions siﬁers decision boundaries normal two-dimensional cross-sections deﬁnition replaced hyperplane intersecting decision boundary then results conditioned model gradient replaced normal vector plane. random directions) decision boundary illustrated figure note crosssections curvature thereby providing evidence assumption holds approximately complex classiﬁers modern deep neural networks. noted model tightly related curvature condition decision boundary model however assume regularity condition decision surface nonsmooth many settings finally theorem holds normal vector replaced gradient point boundary closest noted nonlinear classiﬁers gradient plays role linear classiﬁers normal tangent decision boundary theorem figure dimensional normal cross-sections decision boundary deep network classiﬁer along random directions vicinity diﬀerent natural images caﬀenet architecture trained imagenet used. case uncorrelated basis used write signal random direction sphere) obtain result proposition provides bounds robustness random noise depend show asymptotic bounds provide robustness binary linear classiﬁer uniform random noise. assess empirically bounds robustness random noise. ﬁrst consider -class mnist digit classiﬁcation task train binary linear classiﬁer separating digits digits achieves performance test set. assess analytical results compare empirical estimate robustness uniform random noise diﬀerent values based combination expressions found theorem proposition ﬁxed estimate figure empirical robustness random uniform noise derived upper lower bounds estimate function linear classiﬁer trained mnist. given empirical robustness computed exhaustive search smallest radius ball fraction points sampled uniformly ball distance hyperplane therefore computed closed form figure illustrates empirical robustness theoretical bounds estimate respect addition providing accurate upper lower bounds range tested p-norms observe estimate provides remarkably accurate approximation robustness random noise analytical results hence correctly predict robustness behavior classiﬁer wide variety noise models therefore used predict robustness regimes. robustness multi-class deep neural network uniform random noise. consider complex classiﬁcation setting evaluate robustness vgg- deep neural network multi-class imagenet dataset natural images similarly experiment linear classiﬁer compare empirical value robustness diﬀerent values theoretical bounds theorem estimate note unlike previous case worst-case using algorithm described results shown figure observe that again estimate predicts accurately robustness deep neural network diﬀerent values hence despite high nonlinearity deep network function inputs bounds established assumption hold accurately tested values figure empirical robustness random uniform noise derived upper lower bounds estimate function vgg- classiﬁer trained imagenet. caption figure details computation rpε. choose figure experiments chosen quantization step size. according analytical results section approximate step size classiﬁer tolerate without changing label classiﬁer despite signiﬁcant distortions images caused heavy quantization finally note analytical results conﬁrm quantify earlier empirical observations highlighted high robustness classiﬁers compression mechanisms stress that normalization amount noise added images. distribution noise diﬀers noise concentrated pixels images white pixels spread white images. verify hypothesis show figure ratio images imagenet validation vgg- classiﬁer function whiteness xi≥txi. similarly previous experiments estimated approximately satisﬁes bounds although empirical ratio surpass upper bound images signiﬁcant white pixels. potentially assumption randomness direction decision boundary violated case fact white pixels appear often background images thus correlated decision boundaries classiﬁer. despite assumption satisﬁed bounds allow predict fairly accurately behavior complex deep network presence image-dependent gaussian noise. derived precise bounds robustness linear nonlinear classiﬁers random noise noise distributions uniform noise unit ball gaussian noise. quantitative results show state-of-the-art classiﬁers orders magnitude robust typical random noise worst-case perturbations typically order square root input dimension. bounds shown hold challenging settings stateof-the-art deep network used large scale multi-class dataset imagenet. analysis leveraged quantify eﬀect many disturbances classiﬁers provide robustness guarantees systems deployed real world environments. moreover analysis allows draw links diﬀerent noise regimes show eﬀect robustness adversarial perturbations noise regimes. work studied robustness respect generic norms. future work believe would interesting characterize robustness classiﬁers random perturbations using perceptual similarity metrics adapted diﬀerent modalities images speech. function encodes white pixels image are. noise model images small noise concentrated along pixels images large noise spread across pixels image. circle represents average robustness ratio images", "year": 2018}