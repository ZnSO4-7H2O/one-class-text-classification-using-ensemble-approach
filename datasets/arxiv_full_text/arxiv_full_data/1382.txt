{"title": "Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "We propose a new method for creating computationally efficient and compact convolutional neural networks (CNNs) using a novel sparse connection structure that resembles a tree root. This allows a significant reduction in computational cost and number of parameters compared to state-of-the-art deep CNNs, without compromising accuracy, by exploiting the sparsity of inter-layer filter dependencies. We validate our approach by using it to train more efficient variants of state-of-the-art CNN architectures, evaluated on the CIFAR10 and ILSVRC datasets. Our results show similar or higher accuracy than the baseline architectures with much less computation, as measured by CPU and GPU timings. For example, for ResNet 50, our model has 40% fewer parameters, 45% fewer floating point operations, and is 31% (12%) faster on a CPU (GPU). For the deeper ResNet 200 our model has 25% fewer floating point operations and 44% fewer parameters, while maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7% fewer parameters and is 21% (16%) faster on a CPU (GPU).", "text": "propose method creating computationally efﬁcient compact convolutional neural networks using novel sparse connection structure resembles tree root. allows signiﬁcant reduction computational cost number parameters compared state-of-the-art deep cnns without compromising accuracy exploiting sparsity inter-layer ﬁlter dependencies. validate approach using train efﬁcient variants state-of-the-art architectures evaluated cifar ilsvrc datasets. results show similar higher accuracy baseline architectures much less computation measured timings. example resnet model fewer parameters fewer ﬂoating point operations faster deeper resnet model fewer ﬂoating point operations fewer parameters maintaining state-of-the-art accuracy. googlenet model fewer parameters faster paper describes method creating computationally efﬁcient compact convolutional neural networks using novel sparse connection structure resembles tree root. allows signiﬁcant reduction computational cost number parameters compared state-of-the-art deep cnns without compromising accuracy. shown large proportion learned weights deep networks redundant property widely exploited make neural networks smaller computationally efﬁcient unsurprising regularization critical part training networks using large datasets without regularization deep networks susceptible over-ﬁtting. regularization achieved weight decay dropout furthermore carefully designed sparse network connection structure also regularizing effect. convolutional neural networks embody idea using sparse convolutional connection structure exploit locality natural image structure. consequence easier train. exceptions state-of-the-art cnns image recognition largely monolithic ﬁlter operating feature maps ﬁlters previous layer. interestingly stark contrast understand biological neural networks highly evolved arrangements smaller specialized networks interconnected speciﬁc ways recently learning low-rank basis ﬁlters found improve generalization reducing computational complexity model size full rank ﬁlters however work addressed spatial extents convolutional ﬁlters work show similar idea applied channel extents i.e. ﬁlter inter-connectivity using ﬁlter groups show simple alterations state-of-the-art architectures drastically reduce computational cost model size without compromising accuracy. previous work reducing computational complexity cnns focused approximating convolutional ﬁlters spatial domain either using low-rank approximations fourier transform based convolution general methods used reduced precision number representations compression previously trained models explore methods reduce computational impact large number ﬁlter channels within state-ofart networks. speciﬁcally consider decreasing number incoming connections nodes. stated fact alexnet network approximately fewer connection weights corresponding network without ﬁlter groups. reduction input channel dimension grouped convolution ﬁlters despite large difference number parameters models achieve comparable accuracy ilsvrc fact smaller grouped network gets lower top- validation error. paper builds upon ﬁndings extends state-of-the-art networks. low-dimensional embeddings. proposed method reduce dimensionality convolutional feature maps. using relatively cheap convolutional layers comprising ﬁlters size layers learn feature maps lowerdimensional spaces i.e. feature maps fewer channels. subsequent spatial ﬁlters operating lower dimensional input space require signiﬁcantly less computation. method used state networks image classiﬁcation reduce computation method complementary. googlenet. contrast much work szegedy propose architecture highly optimized computational efﬁciency. googlenet uses basic building block mixture low-dimensional embeddings heterogeneously sized spatial ﬁlters collectively ‘inception’ module. distinct forms convolutional layers inception module lowdimensional embeddings spatial googlenet keeps large expensive spatial convolutions minimum using ﬁlters using convolutions even ﬁlters. motivation convolutional ﬁlters respond localized patterns small receptive ﬁeld requiring larger receptive ﬁeld. number ﬁlters successive inception module increases slowly decreasing feature size order maintain computational performance. googlenet efﬁcient state-of-theart network ilsvrc achieving near state-of-the-art accuracy lowest computation/model size. however show even efﬁcient optimized network architecture beneﬁts method. low-rank approximations. various authors suggested approximating learned convolutional ﬁlters using tensor decomposition example jaderberg propose approximating convolutional ﬁlters trained network representations low-rank spatial channel domains. approach signiﬁcantly decreases computational complexity albeit expense small amount accuracy. paper figure filter groups. convolutional ﬁlters typically channel dimension input feature maps operate. however ﬁlter grouping independent groups ﬁlters operate fraction input feature channels reducing ﬁlter dimensions h×w×c h×w×c/g. change affect dimensions input output feature maps signiﬁcantly reduces computational complexity number model parameters. figure alexnet filter groups. model parameters top- error variants alexnet model ilsvrc image classiﬁcation dataset. models moderate numbers ﬁlter groups fewer parameters surprisingly maintain comparable error. alexnet filter groups. amongst seminal contributions made krizhevsky ‘ﬁlter groups’ convolutional layers ﬁlter groups necessitated practical need sub-divide work training large network across multiple gpus side effects somewhat surprising. speciﬁcally authors observe independent ﬁlter groups learn separation responsibility consistent different random initializations. also surprising explicitly figure learning basis filters. learning linear combination mostly small heterogeneously sized spatial ﬁlters note ﬁlters operate channels input feature map. learning basis filters approach connected ioannou showed replacing ﬁlters linear combinations ﬁlters smaller spatial extent could reduce model size computational complexity state-of-the-art cnns maintaining even increasing accuracy. however work address channel extent ﬁlters. section present main contribution work novel sparsely connected architectures resembling tree roots decrease computational complexity model size compared state-of-the-art deep networks image recognition. learning basis filter dependencies unlikely every ﬁlter deep neural network needs depend output ﬁlters previous layer. fact reducing ﬁlter co-dependence deep networks shown beneﬁt generalization. example hinton introduced dropout regularization deep networks. training network layer dropout random subset neurons excluded forward backward pass mini-batch. furthermore cogswell observe correlation covariance hidden unit activations overﬁtting. explicitly reduce covariance hidden activations train networks loss function based covariance matrix activations hidden layer. instead using modiﬁed loss regularization penalty randomized network connectivity training prevent co-adaption features take much direct approach. ﬁlter groups force network learn ﬁlters limited dependence previous layers. ﬁlters ﬁlter groups smaller figure root modules. root modules compared typical convolutional layers found resnet modern architectures. grey blocks represent feature maps layer’s ﬁlters operate colored blocks represent ﬁlters layer. reduced connectivity also reduces computational complexity model size since size ﬁlters ﬁlter groups reduced drastically evident fig. unlike methods increasing efﬁciency deep networks approximating pre-trained existing networks models trained random initialization using stochastic gradient descent. means method also speed training since merely approximating existing model’s weights accuracy existing model upper bound accuracy modiﬁed model. root module basic element network architecture root module shown fig. root module given number ﬁlter groups ﬁlter groups fewer number connections previous layer’s outputs. spatial convolutional layer followed lowdimensional embedding like conﬁguration learns linear combination basis ﬁlters implicitly representing ﬁlter full channel depth limited ﬁlter dependence. present image classiﬁcation results obtained replacing spatial convolutional layers within existing stateof-the-art network architectures root modules improving network network cifar- network network near state-of-theart network cifar- composed spatial convolutional layers large number ﬁlters interspersed pairs low-dimensional embedding layers. baseline replicated standard network architecture described used state-of-the-art training methods. trained using random cropped mirrored images pixel zero-padded mean-subtracted images also used initialization batch normalization conﬁguration whitening required reproduce validation accuracies obtained also dropout found little effect presumably batch normalization suggested ioffe szegedy assess efﬁcacy method replaced spatial convolutional layers original network root modules preserved original number ﬁlters layer subdivided groups shown table considered ﬁrst pair existing layers part root modfigure network-in-network cifar results. spatial ﬁlters grouped hierarchically. best models closest origin. standard network mean standard deviation shown different random initializations. ules. group ﬁlters ﬁrst convolutional layer since operates three-channel image space limited computational impact compared layers. results shown table fig. various network architectures. compared baseline architecture root variants achieve signiﬁcant reduction computation model size without signiﬁcant reduction accuracy. example root- architecture gives equivalent accuracy ﬂoating point operations model parameters original network approximately faster timings included §a.). results show so-called root topology gives best performance providing smallest reduction accuracy given reduction model size computational complexity. similar experiments deeper network architectures delivered similar results reported results root topologies. aligns intuition deep networks image recognition subsuming deformable parts model. assume ﬁlter responses identify parts ﬁlter dependence depth parts assembled complex concepts. residual networks state-of-the network ilsvrc. resnets computationally efﬁcient architecture based low-dimensional embeddings resnets also accurate quicker converge identity mappings. baseline used ‘resnet model resnet convolutional layers one-third spatial convolutions training augmentation aside random cropping mirroring. training used initialization scheme described modiﬁed compound layers batch normalization assess efﬁcacy method replaced spatial convolutional layers original network root modules preserved original number ﬁlters layer subdivided groups shown table considered ﬁrst existing layers subsequent spatial convolution part root modules. architectures outlined table evaluated cifar test set. block-diagonalization enforced ﬁlter group structure visible larger number ﬁlter groups. shows network learns organization ﬁlters sparsely distributed strong ﬁlter relations visible brighter pixels grouped denser block-diagonal structure leaving visibly darker low-correlated background. images explanation derivation. interesting question concerns degree grouping root modules varied function depth network. nin-like architectures described earlier might consider degree grouping decrease depth ﬁrst convolutional layer e.g. remain constant depth ﬁrst convolutional layer e.g. increase depth e.g. figure network-in-network root architecture. root- architecture compared original architecture convolutional layers. colored blocks represent ﬁlters layer. don’t show intermediate feature maps layer’s ﬁlters operate ﬁnal fully connected layer space considerations decreasing degree grouping successive root modules means network architectures somewhat resemble plant roots hence name root. tation model size without signiﬁcant reduction accuracy. example best result exceeds baseline accuracy reducing model size ﬂoating-point operations timings faster timings faster. drop accuracy however root- model reduces model size reduces ﬂoating point operations timings faster timings faster. ilsvrc provide baseline used code implementing full training augmentation achieve state-ofthe-art results. table shows results experiments top- top- error center cropped images. models trained roots comparable error baseline network fewer parameters less computation. root- model fewer flops fewer parameters resnet replicated network described szegedy exception using training augmentation aside random crops mirroring train used initialization modiﬁed compound layers batch normalization without scale bias test time evaluate center crop image. preserving original number ﬁlters layer trained networks various degrees ﬁlter grouping described table inception architecture relatively complex simplicity always number groups within groups different ﬁlter sizes despite different cardinality. networks grouped ﬁlters within ‘spatial’ convolutions ings model size measured number ﬂoating point parameters. many conﬁgurations top- accuracy remains within baseline model. highest accuracy result top- accuracy baseline model higher top- accuracy within error bounds resulting training different random initializations. maintaining accuracy network faster timings. however model lower top- accuracy baseline much higher gains computational efﬁciency fewer ﬂoating point operations fewer model parameters faster faster timings. results seem modest compared results resnet googlenet smallest fastest near state-of-the-art model ilsvrc model. believe experimentation using different cardinalities ﬁlter grouping heterogeneously-sized ﬁlter groups within inception module improve results further. experiments show method achieve signiﬁcant reduction runtimes state-ofthe-art cnns without compromising accuracy. however reductions runtime smaller might expected based theoretical predictions computational complexity believe largely consequence optimization caffe existing netshow possible achieve high throughput large batches implementing efﬁcient interface used cublas batched calls. modiﬁed caffe cublas batched calls achieved signiﬁcant speedups root-like network architectures compared vanilla caffe without cudnn e.g. speed root- modiﬁed version googlenet architecture. however optimized implementation still fast caffe cudnn presumably unrelated optimizations cudnn library. therefore suggest direct integration cublasstyle batching cudnn could improve performance ﬁlter groups signiﬁcantly. paper focused using homogeneous ﬁlter groups however optimal. heterogeneous ﬁlter groups reﬂect better ﬁlter co-dependencies found deep networks. learning combined spatial channel basis also improve efﬁciency further. explored effect using complex hierarchical arrangements ﬁlter groups cnns show imposing structured decrease degree ﬁlter grouping depth ‘root’ topology allow obtain efﬁcient variants state-of-the-art networks without compromising accuracy. method appears complementary existing methods low-dimensional embeddings used efﬁciently train deep networks methods approximate pre-trained model’s weights. validated method using create efﬁcient variants state-of-the-art network-in-network googlenet resnet architectures evaluated cifar ilsvrc datasets. results show similar accuracy baseline architecture fewer parameters much less compute network-in-network cifar model parameters original network approximately faster timings. resnet model fewer parameters faster even efﬁcient near state-of-the-art ilsvrc network googlenet model uses fewer parameters faster", "year": 2016}