{"title": "Distributed optimization of deeply nested systems", "tag": ["cs.LG", "cs.NE", "math.OC", "stat.ML"], "abstract": "In science and engineering, intelligent processing of complex signals such as images, sound or language is often performed by a parameterized hierarchy of nonlinear processing layers, sometimes biologically inspired. Hierarchical systems (or, more generally, nested systems) offer a way to generate complex mappings using simple stages. Each layer performs a different operation and achieves an ever more sophisticated representation of the input, as, for example, in an deep artificial neural network, an object recognition cascade in computer vision or a speech front-end processing. Joint estimation of the parameters of all the layers and selection of an optimal architecture is widely considered to be a difficult numerical nonconvex optimization problem, difficult to parallelize for execution in a distributed computation environment, and requiring significant human expert effort, which leads to suboptimal systems in practice. We describe a general mathematical strategy to learn the parameters and, to some extent, the architecture of nested systems, called the method of auxiliary coordinates (MAC). This replaces the original problem involving a deeply nested function with a constrained problem involving a different function in an augmented space without nesting. The constrained problem may be solved with penalty-based methods using alternating optimization over the parameters and the auxiliary coordinates. MAC has provable convergence, is easy to implement reusing existing algorithms for single layers, can be parallelized trivially and massively, applies even when parameter derivatives are not available or not desirable, and is competitive with state-of-the-art nonlinear optimizers even in the serial computation setting, often providing reasonable models within a few iterations.", "text": "science engineering intelligent processing complex signals images sound language often performed parameterized hierarchy nonlinear processing layers sometimes biologically inspired. hierarchical systems oﬀer generate complex mappings using simple stages. layer performs diﬀerent operation achieves ever sophisticated representation input example deep artiﬁcial neural network object recognition cascade computer vision speech front-end processing. joint estimation parameters layers selection optimal architecture widely considered difﬁcult numerical nonconvex optimization problem diﬃcult parallelize execution distributed computation environment requiring signiﬁcant human expert eﬀort leads suboptimal systems practice. describe general mathematical strategy learn parameters extent architecture nested systems called method auxiliary coordinates replaces original problem involving deeply nested function constrained problem involving diﬀerent function augmented space without nesting. constrained problem solved penalty-based methods using alternating optimization parameters auxiliary coordinates. provable convergence easy implement reusing existing algorithms single layers parallelized trivially massively applies even parameter derivatives available desirable competitive state-of-the-art nonlinear optimizers even serial computation setting often providing reasonable models within iterations. continued increase recent years data availability processing power enabled development practical applicability ever powerful models statistical machine learning example recognize faces speech translate natural language however physical limitations serial computation suggest scalable processing require algorithms massively parallelized proﬁt thousands inexpensive processors available cloud computing. focus hierarchical processing architectures deep neural nets originally inspired biological systems visual auditory cortex mammalian brain proven successful learning sophisticated tasks recognizing faces speech trained data. typical neural deﬁnes hierarchical feedforward parametric mapping inputs outputs. parameters learned given dataset numerically minimizing objective function. outputs hidden units layer obtained transforming previous layer’s outputs linear operation layer’s weights followed nonlinear elementwise mapping deep nonlinear neural nets universal approximators approximate target mapping arbitrary accuracy given enough units representation power shallow nets hidden units encode hierarchical distributed features useful deal complex sensory data. example trained images deep nets learn lowlevel features edges t-junctions high-level features parts decompositions. examples hierarchical processing systems cascades object recognition scene understanding computer vision phoneme classiﬁcation speech processing wrapper approaches classiﬁcation regression kinematic chains robotics architectures share fundamental design principle mathematically construct deeply nested mapping inputs outputs. ideal performance nested system arises parameters layers jointly trained minimize objective function desired task classiﬁcation error however challenging nesting produces inherently nonconvex functions. joint training usually done backpropagation algorithm recursively computes gradient respect parameter using chain rule. simply update parameters small step negative gradient direction gradient descent stochastic gradient descent feed gradient nonlinear optimization method compute better search direction possibly using second-order information process repeated convergence criterion satisﬁed. backprop variants suﬀers problem vanishing gradients gradients lower layers much smaller higher layers leads tiny steps slowly zigzagging curved valley slow convergence. problem worsens depth researchers give practice nets beyond around hidden layers exception) recently improved initialization strategies much faster computers—but really improvement optimization algorithms themselves—have renewed interest deep architectures. besides backprop parallelize layers applicable mappings diﬀerentiable respect parameters needs careful tuning learning rates. summary decades research neural optimization simple backprop-based algorithms stochastic gradient descent remain state-of-the-art particularly combined good initialization strategies addition selecting best architecture example number units layer deep number ﬁlterbanks speech front-end processing requires combinatorial search. practice approximated manual trial-and-error procedure costly eﬀort expertise required leads suboptimal solutions describe general optimization strategy deeply nested functions call method auxiliary coordinates partly alleviates vanishing gradients problem embarrassing parallelization reuse existing algorithms optimize single layers individual units. section describes section describes related work section gives experimental results illustrate deﬁniteness describe approach deep later sections show settings. consider regression problem mapping inputs outputs deep given dataset pairs typical objective function learn deep hidden layers form layer function form i.e. linear mapping followed squashing nonlinearity applies scalar function sigmoid elementwise vector argument output method applies loss functions squared error fully sparsely connected layers diﬀerent number hidden units weights shared across layers regularization terms weights basic issue deep nesting mapping traditional minimize computing gradient weights using backpropagation feeding nonlinear optimization method. seen coordinates intermediate feature space hidden unit activations intuitively eliminating equivalent nested problem prove general assumptions problems exactly minimizers problem seems complicated terms involve small subset parameters nested functions. show reduces ill-conditioning caused nesting partially decouples many variables aﬀording eﬃcient distributed optimization. deﬁnes continuous path which mild assumptions converges minimum constrained problem thus minimum original problem practice follow path loosely. objective function seen breaking functional dependences nested mapping unfolding layers. every squared term involves shallow mapping; variables equally scaled improves conditioning problem; derivatives required simpler require backpropagated gradients sometimes gradients all. w-step minimizing ﬁxed results separate minimization weights hidden unit—each single-layer single-unit problem solved existing algorithms. speciﬁcally unit nonlinear least-squares regression form thus w-step results many independent single-layer single-unit problems solved existing algorithms without extra programming cost. z-step however always form generalized proximal operator reduces complex highly-coupled problem—training deep net—to sequence simple uncoupled problems coordinated auxiliary variables large large dataset aﬀords enormous potential parallel distributed computation. wz-step operates large decoupled blocks variables decrease objective function large iteration unlike tiny decreases achieved nested function. large steps eﬀectively shortcuts -space instead tiny steps along curved valley w-space. rather algorithm method auxiliary coordinates mathematical device design optimization algorithms suited speciﬁc nested architecture provably convergent highly parallelizable reuse existing algorithms non-nested architectures. idea judicious elimination subexpressions nested function equality constraints. architecture need strictly feedforward designer need introduce auxiliary coordinates every layer spectrum auxiliary coordinates hybrids auxiliary coordinates semi-deep nets every single hidden unit auxiliary coordinate. auxiliary coordinate replace subexpression nested function methods constrained optimization used depending characteristics problem wz-steps solved number nonlinear optimization methods gradient descent newton’s method using standard techniques warm starts caching factorizations inexact steps stochastic updates using data minibatches etc. respect similar metaalgorithms expectation-maximization algorithms alternating-direction method multipliers become ubiquitous statistics machine learning optimization areas. fig. illustrates learning sigmoidal deep autoencoder architecture introducing auxiliary coordinates hidden unit layer classical backprop-based techniques stochastic gradient descent conjugate gradients need many iterations decrease error mac/qp iteration achieves large decrease particularly beginning reach pretty good network pretty fast. mac/qp’s serial performance already remarkable parallel implementation achieves linear speedup number processors stopping criterion exactly optimizing follows minima path strictly unnecessary usually performs inexact faster optimization. unlike general problem case accurate know exit optimization given since real goal minimize nested error exit value increases decreases less tolerance relative terms. further common neural training validation error measured validation set). means follow path strictly inasmuch approach nested minimum approach seen sophisticated taking descent step derived using stopping criterion maintains theoretical convergence guarantees path still ends minimum drive postprocessing step ﬁnished optimizing formulation method apply fast post-processing step reduces objective function achieves feasibility eliminates auxiliary coordinates. simply satisfy constraints setting keep weights except last layer ﬁtting dataset prove resulting weights reduce leave unchanged value another important advantage easily applicable heterogeneous architectures layer perform particular type processing specialized training algorithm exists possibly based derivatives weights example quantization layer object recognition cascade nonlinear layer radial basis function network often k-means training estimate weights. simply reusing existing training algorithm w-step layer allows learn jointly parameters entire network minimal programming eﬀort something easy possible methods. fig. illustrates learning autoencoder architecture encoder decoder networks introducing auxiliary coordinates coding layer w-step basis functions trained k-means weights remaining layers trained least-squares. before mac/qp achieves large error decrease iterations. ﬁnal advantage enables eﬃcient search parameter values given architecture architectures themselves. traditional model selection usually involves obtaining optimal parameters possible architecture evaluating architecture based criterion cross-validation bayesian information criterion picking best discrete-continuous optimization involves training exponential number models practice settles suboptimal search model selection achieved w-step model selection separately layer letting z-step coordinate layers usual way. speciﬁcally consider model selection criterion form nested objective function additive layers satisﬁed many criteria minimum description length essentially proportional number free parameters. optimizing directly involves testing deep nets choices layer w-step separates layers requires testing single-layer nets iteration. model selection tests still costly parallel need iteration. alternate running multiple iterations optimize given architecture running model-selection iteration still guarantee monotonic decrease practice observe near-optimal model often found early iterations. thus ability decouple optimizations reduces search exponential number complex problems iterated search linear number simple problems. fig. illustrates learn architecture autoencoder trying diﬀerent values number basis functions encoder decoder because early optimization mac/qp settles architecture quite smaller used result fact achieved even less time. believe ﬁrst propose formulation full generality nested function learning provably equivalent constrained problem optimized jointly space parameters auxiliary coordinates using quadratic-penalty augmented lagrangian methods. however exist several lines work related mac/qp seen giving principled setting justiﬁes previous heuristic eﬀective approaches opening door principled ways training deep nets nested systems. updating activations hidden units separately weights neural done past early work neural nets recent work learning sparse features dimensionality reduction interest using activations neural nets independent variables goes back early days neural nets learning good internal representations important learning good weights fact backpropagation presented method construct good internal representations represent important features task domain necessarily requires dealing explicitly hidden activations. thus several papers proposed objective functions weights activations intended solve nested problem achieve distributed optimization help learn good representations. algorithms typically converge converge solution nested problem developed single-hidden-layer tested small problems. recent variations similar problems nearly early work focused case single hidden layer easy enough train standard methods great advantage obtained reveal parallel processing aspects problem become truly important deep case. extracting features using overcomplete dictionaries sparsity often encouraged sometimes requires explicit penalty features considered single layer minimize nested problem work single hidden layer mentions possibility recovering backpropagation limit used construct algorithm converges nested problem optimum. recent works deep learning pretraining greedy layerwise training single pass input output layer ﬁxing weights layer sequentially without optimizing joint objective weights. heuristics used achieve good initial weights converge minimum nested problem. auxiliary variables used statistics machine learning early work factor homogeneity analysis learn dimensionality reduction mappings given dataset high-dimensional points here takes latent coordinates data point parameters estimated together reconstruction mapping maps latent points data various nonlinear versions approach exist spline single-layer neural radial basis function kernel regression gaussian process however particularly nonparametric functions error driven zero separating inﬁnitely apart methods need ad-hoc terms prevent this. dimensionality reduction unsupervised regression approach carreira-perpi˜n´an seen truncated version quadratic-penalty approach kept constant limited single-hidden-layer net. therefore resulting estimate nested mapping biased minimize nested error. summary works typically concerned single-hidden-layer architectures solve nested problem instead goal deﬁne diﬀerent problem designer explicit control net’s internal representations features auxiliary coordinates purely mathematical construct solve well-deﬁned general nested optimization problem embarrassing parallelism suitable distributed computation necessarily related learning good hidden representations. also none works realize possibility using heterogeneous architectures layer-speciﬁc algorithms learning architecture minimizing model selection criterion separates w-step. finally formulation similar spirit alternating direction method multipliers variables introduced decouple terms. however admm splits existing variable appears multiple terms objective function rather functional nesting example minx becomes minxy s.t. split non-negative non-positive parts. contrast introduces variables break nesting. admm known simple eﬀective parallelizable able achieve pretty good estimate pretty fast thanks decoupling introduced ability existing optimizers subproblems arise. also characteristics problems involving function nesting. section describes implemented wz-steps sections show learn homogeneous architecture heterogeneous architecture architecture itself respectively. cases show speedup achieved parallel implementation well. essentially approximates hessian linearizing function solves linear system search direction line search iterates. practice iterations converge high tolerance. z-step minimizing ﬁxed separates problem also nonlinear least-squares formally similar w-step enter objective function nearly symmetric additional quadratic optimization mac-constrained problem based quadratic-penalty method gaussnewton steps produces reasonable results simple implement intended particularly eﬃcient. eﬃcient optimization achieved using methods constrained optimization augmented lagrangian method instead quadratic penalty method; using eﬃcient wz-steps combining standard techniques unconstrained optimization methods l-bfgs conjugate gradients gradient descent alternating optimization others. exploring topic future research. replace loop weight vectors auxiliary coordinates parfor loop. matlab sends iteration loop diﬀerent processor. shared-memory multiprocessor machine using processors obtained results reported paper. simple matlab parallel processing toolbox quite ineﬃcient. larger speedups would achievable parallel computation models using distributed architecture dataset handwritten digit images train deep autoencoder architecture maps input image low-dimensional coding layer tries reconstruct image used mac/qp introducing auxiliary coordinates hidden unit layer. fig. shows learning curves. dimension hidden layer using initial random weights large decrease error achieved simply adjusting biases output layer network output matches mean target data algorithms attain ﬁrst iterations giving impression large error decrease followed slow subsequent decrease. focus plots subsequent diﬃcult error decreases always apply single step gradient descent random initial weights mostly adjusts biases described. resulting weights given initial weights optimization method. mac/qp runs z-step gauss-newton iteration w-step gauss-newton iterations. also small regularization ﬁrst iterations drop since tends lead better local optimum. value optimize inexact described section exiting value nested error evaluated validation increases decreases less tolerance relative terms. tolerance increase rather aggressively show learning curves classical backprop-based techniques stochastic gradient descent conjugate gradients. stochastic gradient descent several parameters carefully user ensure convergence occurs convergence fast possible. grid search minibatch size learning rate found minibatches samples learning rate best. randomly permute training beginning epoch. nonlinear conjugate gradients used polak-ribi`ere version widely regarded best carl rasmussen’s implementation minimize.m uses line search based cubic interpolation sophisticated backtracking allows steps longer found minibatch worked best. used restarts every steps. figure plots mean squared training error nested objective function time diﬀerent algorithms initialization. validation error follows closely training error plotted. markers shown every iteration every iterations every epochs change points quadratic penalty parameter indicated using ﬁlled circles beginning iteration happened. learning curve parallelized version also shown blue. fig. shows usps images reconstruction training algorithm. need many iterations decrease error mac/qp iteration achieves large decrease particularly beginning reach pretty good network pretty fast. mac/qp’s serial performance already remarkable parallel implementation achieves linear speedup number processors figure training deep autoencoder reconstruct images handwritten digits. left nested function error algorithm markers shown every iteration every iterations every epochs mac/qp incremented quadratic penalty indicated solid markers. experiments computer using single processor except parallel training curve used processors sharing memory using matlab parallel processing toolbox. right reconstruction sample training images diﬀerent methods. dataset object images train autoencoder architecture encoder decoder networks rather using gradient-based optimization subnet k-means train basis functions w-step used mac/qp introducing auxiliary coordinates coding layer. backprop-based algorithms incompatible k-means training instead compare alternating optimization. fig. shows learning curves. coil– image dataset commonly used benchmark test dimensionality reduction algorithms contains rotation sequences diﬀerent objects every degrees networks single hidden layer. ﬁrst form vector maps point image. thus complete autoencoder concatenation gaussian networks hidden layers sizes total almost million weights. usual networks applied quadratic regularization linear-layer weights small value nested problem minimize following objective function least-squares error plus quadratic regularization figure training autoencoder reconstruct images objects. left nested function error algorithm markers shown every iteration details right reconstruction sample training images diﬀerent methods. practice networks trained stages consider encoder example. first typically k-means simply ﬁxing random subset inputs. second determined obtains linear least-squares problem solving linear system. reason preferred fully nonlinear optimization centers weights achieves near-optimal nets simple noniterative procedure. type two-stage noniterative strategy obtain nonlinear networks widely applied beyond networks example support vector machines kernel slice inverse regression others. wish capitalize attractive property train deep autoencoders constructed concatenating networks. however backprop-based algorithms incompatible two-stage training procedure since derivatives optimize centers. leads following optimization methods alternating optimization approach mac. alternating optimization approach alternate following steps step train applying k-means linear system step identical w-step step train applying k-means nonlinear optimization longer appears linearly objective function nonlinearly embedded argument decoder. step signiﬁcantly slower w-step deﬁne mac-constrained problem follows. introduce auxiliary coordinates coding layer allows w-step become desired k-means plus linear system training encoder decoder separately. requires programming eﬀort; simply call existing k-means-based training algorithm encoder decoder separately. start quadratic penalty parameter increase iterations. experiment instead using random initial weights obtained initial values coordinates running nonlinear dimensionality reduction method elastic embedding gives signiﬁcantly better embeddings spectral methods; takes input matrix similarity values every pair coil images produces nonlinear projection used gaussian similarities kernel width iterations using user parameter. optimization algorithms initialized projections. fig. shows nested function error before mac/qp achieves large error decrease iterations. alternating optimization much slower. again parallel implementation mac/qp achieves large speedup nearly linear number processors fig. shows coil images reconstruction training algorithm. fig. shows initial projections ﬁnal projections manifolds improved example opening loops folded. repeat experiment autoencoder previous section learn architecture. jointly learn architecture encoder decoder trying diﬀerent values number basis functions deﬁne following objective function architectures weights figure learning architecture autoencoder dataset using mac. show total error plus model cost) point. model selection steps every iterations indicated green markers details linear weights) autoencoder thus choose numbers centers discrete consisting equispaced values range estimated result autoencoder section large number parameters thus bias. section centers network constrained equal subset input points start mac/qp optimization complex model centers every iteration optimizes mac/qp objective function model selection step every iterations. selects separately best value potentially figure shows total error model selection steps indicated green markers annotated resulting value details ﬁrst change architecture moves smaller model achieving enormous decrease objective. explained strong penalty imposes number parameters favoring simpler models. then followed minor changes architecture interleaved continuous optimization weights. ﬁnal architecture total million weights. architecture incurs larger training error previous section uses much simpler model lower value overall objective function because early optimization mac/qp settles architecture quite smaller used result fact achieved even less time. again parallel implementation trivial achieves approximately linear speedup number processors drastically facilitates runtime human eﬀort practical design estimation nonconvex nested problems jointly optimizing parameters reusing existing algorithms searching automatically architectures aﬀording massively parallel computation provably converging solution nested problem. could replace complement backpropagation-based algorithms learning nested systems serial parallel settings. particularly timely given serial computation reaching plateau cloud computing becoming commodity intelligent data processing ﬁnding mainstream devices thanks increases computational power data availability. important area application joint automatic tuning stages complex intelligent-processing system data-rich disciplines found computer vision speech distributed cloud computing environment. also opens many questions optimal introduce auxiliary coordinates given problem choice speciﬁc algorithms optimize wz-steps. first give theorem holds general assumptions. particular require functions smooth holds loss function beyond least-squares holds nested problem subject constraints. mac-constrained problem vice versa recall following deﬁnitions unconstrained minimization problem minx∈rn local minimizer exists nonempty neighborhood constrained minimization problem s.t. local minimizer exists nonempty neighborhood local minimizer nested problem then exists nonempty neighborhood call nonempty neighborhood -space. hence local minimizer mac-constrained problem then exists nonempty neighborhood note applies particular calling hence strict minimizers) exchange figure illustrates theorem. essentially nested objective function stretches along manifold deﬁned preserving minimizers maximizers. projection w-space part sits manifold recovers figure illustration equivalence nested mac-constrained problems objective function shown contour lines -space vertical lines feasible nested objective function shown blue. corresponding minima problems indicated. show ﬁrst-order necessary conditions problems stationary points. simplicity clarity exposition give proof special case proof layers follows analogously. assume functions continuous ﬁrst derivatives w.r.t. input weights. indicates jacobian w.r.t. input. simplify notation sometimes omit dependence weights; example write eqs. recover eqs. thus point constrained problem stationary point nested problem. conversely given stationary point nested problem deﬁning eqs. satisﬁes eqs. point constrained problem. hence one-to-one correspondence stationary points nested problem points mac-constrained problem. theorem follows minimizers maximizers saddle points nested problem one-to-one correspondence respective minimizers maximizers saddle points mac-constrained problem. penalty parameter given positive increasing sequence nonnegative sequence starting point method ﬁnds approximate minimizer iterate satisﬁes k∇xqk given algorithm theorem suppose besides constraint gradients linearly independent point problem points inﬁnite subsequence limk∈k limk∈k −µkci multiplier vector satisﬁes conditions problem particularize general theorems case obtain stronger theorems. theorem generally applicable optimization problems involving nested functions typically convex local minima. theorem applicable prove convergence nonconvex case. assume functions continuous ﬁrst derivatives w.r.t. input weights diﬀerentiable w.r.t. quadratic-penalty function given positive increasing sequence nonnegative sequence starting point suppose method ﬁnds approximate limk→∞ point problem lagrange multiplier vector elements constraint gradients l.i. point thus particular limit this ﬁrst compute constraint gradients. constraint cnkh znkh point layer unit deﬁne auxiliary coordinate indices layer gradient", "year": 2012}