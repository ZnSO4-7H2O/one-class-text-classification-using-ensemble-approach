{"title": "Spikes as regularizers", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "We present a confidence-based single-layer feed-forward learning algorithm SPIRAL (Spike Regularized Adaptive Learning) relying on an encoding of activation spikes. We adaptively update a weight vector relying on confidence estimates and activation offsets relative to previous activity. We regularize updates proportionally to item-level confidence and weight-specific support, loosely inspired by the observation from neurophysiology that high spike rates are sometimes accompanied by low temporal precision. Our experiments suggest that the new learning algorithm SPIRAL is more robust and less prone to overfitting than both the averaged perceptron and AROW.", "text": "present conﬁdence-based single-layer feed-forward learning algorithm spiral relying encoding activation spikes. adaptively update weight vector relying conﬁdence estimates activation offsets relative previous activity. regularize updates proportionally item-level conﬁdence weight-speciﬁc support loosely inspired observation neurophysiology high spike rates sometimes accompanied temporal precision. experiments suggest learning algorithm spiral robust less prone overﬁtting averaged perceptron arow. perceptron conceptually simple widely used discriminative linear classiﬁcation algorithm. originally motivated observations signals passed neurons brain. return perceptron model neural computation technical point view main weakness perceptron linear classiﬁer prone overﬁtting. particular type overﬁtting likely happen perceptron learning feature swamping i.e. frequent features prevent co-variant features updated leading catastrophic performance frequent features absent less frequent test time. words perceptron well passive-aggressive learning crammer parameters updated features occur rare features therefore often receive inaccurate values. several ways approach overﬁtting e.g. capping model’s supremum norm focus speciﬁc line research conﬁdence-weighted learning linear classiﬁers. conﬁdence-weighted learning explicitly estimates conﬁdence induction often maintaining gaussian distributions parameter vectors. words model parameter interpreted mean augmented covariance estimate. conﬁdence-weighted learning ﬁrst learning algorithm this crammer later introduced adaptive regularization weight vectors simpler effective alternative arow passes data item item computing margin i.e. product weight vector item updating covariance matrix standard additive fashion. weights interpreted means covariance matrix form gaussian distribution weight vectors. speciﬁcally conﬁdence xσx. smoothing constant compute learning rate adaptively neurons synchronously constant rate. neural signals spike-shaped onset increase signal followed spike decrease signal inhibition neuron returning equilibrium. simplify picture assuming spikes bell-shaped learning algorithm propose below motivated observation spike rate increases neuron ﬁres futhermore keller takahashi show increased activity lead spiking higher rates lower temporal precision. means active neurons less successful passing signals leading neuron return stable ﬁring rate. words brain performs implicit regularization exhibiting temporal precision high spike rates. prevents highly active neurons swamping co-variant less active neurons. hypothesise implementing similar mechanism learning algorithms prevent feature swamping similar fashion. finally blanco show periods increased spike rate lead smaller standard deviation synaptic weights. loosely inspired implement temporal imprecision high spike rates decreasing weight’s standard deviation. single layer feedforward model perceptron sampling gaussian spikes effect input therefore implement regularizer noise injection variance relative conﬁdence model input item means parameter values. multiply input inverse sample reﬂecting intuition highly active neurons less precise likely drop clip sample give pseudocode algorithm following conventions crammer extract binary classiﬁcation problems mnist training data points testing even ones. since algorithm parameter-free explicit parameter tuning implementation spiral experiment ﬁrst problems test robustness spiral relatively perceptron arow randomly corrupt input test time removing features. set-up inspired globerson roweis plots figure x-axis presents number features kept observe tendencies results spiral outperforms perceptron consistently features sometimes large margin; except cases perceptron better features. contrast arow less stable improves signiﬁcantly perceptron mid-range noise levels cases. perceptron almost always superior full features since relatively simple learning problem overﬁtting unlikely unless noise injected test time. compute spiral’s practical rademacher complexity ability spiral random re-labelings data. randomly label dataset times compute average error reduction random baseline. perceptron achieves error reduction random baseline average overﬁtting quite random labelling data. contrast spiral reduces errors random baseline average suggesting almost resilient overﬁtting dataset. presented simple conﬁdence-based single layer feed-forward learning algorithm spiral uses sampling gaussian spikes regularizer loosely inspired recent ﬁndings neurophysiology. spiral outperforms perceptron arow large margin noise injected test time lower rademacher complexity algorithms. wilfredo blanco catia pereira vinicius cota annie souza cesar renno-costa sharlene santos gabriella dias guerreiro adriano tort adriao neto sidarta ribeiro. synaptic homeostasis restructuring across sleep-wake cycle. plos computational biology", "year": 2016}