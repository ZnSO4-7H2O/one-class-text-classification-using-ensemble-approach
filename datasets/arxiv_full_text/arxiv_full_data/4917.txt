{"title": "On the Discrepancy Between Kleinberg's Clustering Axioms and $k$-Means  Clustering Algorithm Behavior", "tag": ["cs.LG", "cs.AI", "I.5.2; I.2.6"], "abstract": "This paper investigates the validity of Kleinberg's axioms for clustering functions with respect to the quite popular clustering algorithm called $k$-means. While Kleinberg's axioms have been discussed heavily in the past, we concentrate here on the case predominantly relevant for $k$-means algorithm, that is behavior embedded in Euclidean space. We point at some contradictions and counter intuitiveness aspects of this axiomatic set within $\\mathbb{R}^m$ that were evidently not discussed so far. Our results suggest that apparently without defining clearly what kind of clusters we expect we will not be able to construct a valid axiomatic system. In particular we look at the shape and the gaps between the clusters. Finally we demonstrate that there exist several ways to reconcile the formulation of the axioms with their intended meaning and that under this reformulation the axioms stop to be contradictory and the real-world $k$-means algorithm conforms to this axiomatic system.", "text": "paper investigates validity kleinberg’s axioms clustering functions respect quite popular clustering algorithm called k-means.we suggest reason algorithm kleinberg’s axiomatic system stems missing match informal intuitions formal formulations axioms. kleinberg’s axioms discussed heavily past concentrate case predominantly relevant k-means algorithm behavior embedded euclidean space. point contradictions counter intuitiveness aspects axiomatic within evidently discussed far. results suggest apparently withdeﬁning clearly kind clusters expect able construct valid axiomatic system. particular look shape gaps clusters. finally demonstrate exist several ways reconcile formulation axioms intended meaning reformulation axioms stop contradictory real-world k-means algorithm conforms axiomatic system. important application areas machine learning so-called cluster analysis clustering referred also unsupervised learning learning without teacher. seeks split items subsets called clusters groups similar within clusters dissimilar them. additional criteria like group balancing group size limits etc. also taken account. subsequently restrict somehow meaning terms. partition understand output process cluster analysis. partition would object objects called clusters sets original items diversity clustering methods grows exists strong pressure ﬁnding formal framework systematic overview expected properties partitions obtained. kleinberg claims good partition result reasonable method clustering formulated axioms distance-based cluster analysis need clustering method itself. postulated quite natural axioms need manipulate distances objects. however axioms proved applicable clustering algorithms rather speak properties kleinberg expects clustering functions following e.g. ackerman note invariance consistency properties assume transformation clusters. respect transformations speak invariance transform consistency transform. kleinberg demonstrated three axioms cannot once. kleinberg’s work points important issue shall ﬁrst revise expectations towards obtained partition seemingly obvious axiom apparently sound. particular stated impossibility theorem. kleinberg proved theorem above-mentioned paper. anproof found paper ambroszkiewicz koronacki along discussion kleinberg’s concepts. ackerman prove general impossibility theorem beside providing proof axioms contradictory kleinberg showed axioms pairwise. uses purpose demonstration versions well-known statistical single-linkage procedure. versions diﬀer stopping condition distance function partition distance functions γ-transformation belonging cluster belonging diﬀerent clusters distance functions. γtransformation reﬂect property reducing step performed random uniform sampling step performed speak k-means-random algorithm. according k-means++ heuristics proposed arthur vassilvitskii speak k-means++ algorithm. note attain local minimum worst. also touch incremental k-means discussed there exists whole stream research papers attempt approximate k-meansideal within reasonable error bound cleverly initiated k-means type algorithms e.g. k-means++ like stated current point algorithms rather theoretical value. veriﬁcation kleinberg’s axioms k-means diﬃcult even k-means-ideal cannot guarantee exists single minimum function. talk instead possible minimizing easily seen scale-invariant sees immediately rich also demonstrated kleinberg consistent. k-means-random k-means++ even worse hits usually local minimum there. talk random variable assuming particular probability. assumption easily seen scale-invariant sees immediately none rich k-means-ideal consistent neither realistic variants hence widely used algorithm violates practice three kleinberg’s axioms. cannot considered clustering function. perceive least counterintuitive. ben-david ackerman section raised also similar concern perspective axiomatic system accomplish. state would expect axiomatised objects kind soundness completeness. soundness mean useful clustering algorithms would axioms. completeness expresses apparent non-clustering algorithms would fail least axiom. kleinberg’s axioms explicitly address distance-based clustering algorithms fall apparently short reaching goal. paper demonstrate even narrower algorithms ones data embedded euclidean space axioms fail. exist number open questions recall observed laarhoven marchiori kleinberg’s proof impossibility theorem stops valid case graph clustering. raises immediately question validity euclidean space. note kleinberg bother embedding distance space. whether k-means kleinberg’s axioms peculiar property k-means algorithm embedded euclidean space would fail paying special attention k-means algorithm constitute restrictive limitation. k-means applied many domains natural domains data embedded clusters enclosed voronoi regions also non-linearly separable clusters manifolds spectral clustering community detection social networks demonstrated dhillon kmeans equivalent sense normalized method graph clustering turn viewed equivalent balanced newman’s modularity used community detection shown bolla crisp fuzzy versions used. therefore made eﬀort identify overcome least reasons diﬃculties connected axiomatic understanding research area cluster analysis hope guidance generalizations encompass least considerable part real-life algorithms. paper investigates k-means algorithm violates kleinberg’s axioms clustering functions. claim reason mismatch informal intuitions formal formulations axioms. claim also reconcile k-means kleinberg’s consistency requirement introduction centric consistency motion consistency neither subset superset kleinberg’s consistency rather kmeans clustering model speciﬁc adaptation general idea shrinking cluster moving cluster away. k-means apply also euclidean space k-richness granted richness near richness achievable consistency violated. show also reﬁnement consistency violated too. dictions. demonstrate practical settings application many algorithms. metric m-dimensional space number features impossible contract single cluster without moving ones consequence running risk moving clusters closer together. also show k-means version allow range change optimal clustering kleinberg’s operation applied. intuitions demonstrate reformulation axioms stop contradictory particular introduce notion centric consistency adaptation general idea shrinking cluster. relies simply moving cluster elements towards center. provide example clustering function axioms near-richness scale-invariance possesses property centric consistency clear contradictory. introduce also notion motion consistency approximate consistency property outside cluster show k-means order motion-consistent must impose requirement clusters. introduction requirement hand violates kleinberg’s non-refutability axiom berg’s axioms k-means. demonstrate assumption appropriate gaps either relax centric consistency inner cluster consistency k-richness approximation richness axiomatic systems traced back early wright proposed axioms clustering functions creating unsharp partitions similar fuzzy systems. framework every domain object attached positive real-valued weight could distributed among multiple clusters. prominent axiomatic sets later ﬁercely discussed kleinberg already stated. point view classiﬁcation imposes restrictions clustering function itself. already discussed impossibility theorem kleinberg demonstrates contradiction axioms set. however problems covered theorem. ben-david ackerman mentioned pointed problems consistency such. showed example fig. moving clusters away clusters create groups. paper repeat ﬁndings ﬁx-dimensional euclidean space beyond that. draw attention fact moving clusters away completely impossible without going dimension. furthermore show also shrinking single cluster consistent also impossible demonstrate interaction consistency transformation scaling-invariance transformation actually something contrary intuition behind consistency pulls cluster closer instead pushing away. number relaxations axioms related clustering functions proposed order overcome kleinberg’s impossibility result. recall several here based overview ackerman tutorial ben-david relaxation allows algorithms splitting data ﬁxed number clusters like k-means immediately discarded clustering algorithms given cluster allowed empty. property method said outer-consistent delivers partition increases distances elements diﬀerent clusters lets distances within clusters unchanged. still another relaxation richness proposed hopcroft kannan richness distinct points given euclidean space points algorithm input produces clusters whose centers respective points weakness lies fact points subject clustering reasonable algorithms. even k-richness still problematic issue demonstrated ackerman useful property stability clusters malicious addition data points holds balanced clusters. property method said inner-consistent delivers partition decreases distances elements cluster lets distances elements diﬀerent clusters unchanged. k-means algorithm sense inner-consistent. later discuss representation problem type consistency k-means. mention prove general clustering function simultaneously satisfy outer-consistency scaleinvariance richness general clustering function simultaneously satisfy inner-consistency scaleinvariance richness. claim also -means-ideal properties outer-consistency locality. none properties claimed satisﬁed k-means-random k-means furthest element initialization. furthermore k-richness matched k-means-random algorithm. paper point fact euclidean space even inner-consistency alone outer-consistency alone self-contradictory. also consistency alone poses problem k-means-ideal. hand show k-richness matched k-means-random algorithm still another relaxation kleinberg’s consistency called reﬁnement consistency. modiﬁcation consistency axiom replacing requirement requirement reﬁnement other. partition reﬁnement partition cluster exists cluster obviously replacement consistency requirement reﬁnement consistency breaks impossibility proof kleinberg’s axiom system. practical concern general reﬁnement consistency means transformation scaling transform clustering other. usefulness axiom hence questionable. paper show circumstances unidirectional reﬁnement consistency achieved makes much sense. zadeh ben-david propose instead order-consistency versions single-linkage algorithm classiﬁed clustering algorithm. distance functions orderings edge lengths k-means order-consistent. could also relax scale-invariance instead e.g. robustness small changes distance function result small changes partition basic problem partitions discrete term small hard deﬁne reasonably. small changes distances result major changes partitions obtained k-means algorithm. show however true. clustering function clustering clusters locality property whenever given clustered partition take subset clustering ∪c∈γ clusters yield exactly also mention works like dunn ackerman dasgupta seemingly nothing kleinberg’s axioms superﬁcial impression. papers discussing issue well separated clusters nicely separated perfectly separated point fact weakness non-refutability axiom apparent want partition rather meaningful. ackerman dasgupta handle incremental clustering algorithms. introduce incremental version k-means algorithm. clusters nicely separated deﬁned distance element element cluster lower distance element element outside cluster. authors demonstrate incremental algorithm space complexity linear discover clusters nicely separated. contrary single-link algorithm identify candidate elements among diﬀerent clusters nice clustering unique. nice clustering detected sense incremental algorithm memory linear cannot detected incremental k-means even large memory. however looking issue randomly generated sequence data memory linear suﬃces incremental kmeans probability. introduce perfect clustering property smallest distance elements distinct clusters larger distance elements cluster. demonstrate exists incremental algorithm discovering perfect clustering linear respect space. incremental kmeans fails discuss issue section ackerman ben-david propose another direction resolving problem kleinberg’s axiomatisation impossibility. instead axiomatising clustering function rather create axioms cluster quality function. number characterizations clustering functions linkage algorithms note beside kleinberg’s axioms exist impossible characterizations clustering functions. meila demonstrates can’t compare partitions manner agrees lattice partitions convexly additive bounded. general tendency researchers wanting overcome kleinberg’s contradiction weaken axioms kleinberg. contradiction removed removal relied weakening reasoning capabilities strong conclusions reached. research diﬀerent strengthening assumptions example proof k-means consistency becomes possible. susceptivity among others k-means algorithm hostile addition points data set. turns k-means stable disturbances given clusters well balanced suﬃcient gaps clusters. demonstrates data points diﬀerent clusters applies weighting functions data points. papers though explicitly addressing k-richness demonstrate problems resulting axiom. implies small clusters disintegrated hostile points practical purposes shall interested larger clusters. allows conclude poor estimates densities sparse clusters lead erroneous drawing cluster boundaries. kleinberg paper proved so-called anti-chain theorem implies scaling contraction transform clustering other. fact combined richness axiom leads directly contradiction three axioms. theorem data clustering function satisﬁes scale-invariance richness. proof. consisting elements potentially partitions {{e}{e}} according scale-invariance deﬁnition obvious contradiction. demonstrate later function matching richness axiom kleinberg necessarily exhibit richness distances conﬁned case theorem matter talk data points hence automatically validity distance granted. theorem data function produce no-split partition distance function partition distance function satisﬁes consistency scale-invariance properties. proof. show mind minee∈s maxd maxee∈s easy γ-transform maxd mind partition therefore scale-invariance hence consistency contradicts assumption mentioned pointed fact construction would possible realm graph clustering. shall then provided proof show forcing data points euclidean space invalidate construction scaling operation keeps points original euclidean space. visible kleinberg’s anti-chain theorem theorem data function produce partition consisting sets elements n}{n distance function partition consisting three sets elements n}{n distance function satisﬁes consistency scale-invariance properties. proof. take elements. richness property implies distinct distance functions clustering function form partitions resp. deﬁned theorem. invariance property derive distance function distance elements lower biggest distance invariance property derive distance function distance elements bigger consistency axiom. derive distance function elements identical distance distances element bigger distances elements derive distance function elements proof however assumptions made possibly correct require distances distances euclidean space. conﬁguration points point found equidistant ones. even guaranteed second distinct point exists property. hence construction proof initial step needed matching using consistency property pose points onto sphere points line orthogonal subspace containing passing origin sphere. course contradiction still valid euclidean space exercise shows proofs kleinberg need rewritten deal euclidean spaces. note restrict posing points onto sphere work anymore. points become identical. k-means normally operates euclidean space using so-called kernel-trick operate objects embedded space without actually ﬁnding embedding clustering space optimizing function. dive deeper paper discussion properties kernel k-means. make remark kernel k-means given exists embedding fact k-means feature space. ﬁndings related k-means would apply also feature space. weighted version kernel k-means considered tricky approximated multiplying unweighted points restriction multiplied points cluster doss seem invalidate ﬁndings. separate question course whether invert kernel function order points transformed e.g. centric consistency transform feature space. still apply kernel trick? easily embedding three-dimensional space allows imaginary coordinates table distances kept rigidly distance formula kleinberg’s non-embeddability axiom suitable clustering algorithms position points space needs anticipated. assumption euclidean embedding problem clearly solved. hand richness property denial nothing distances hence embedding euclidean space already known mentioned publications e.g. obviously violated k-means returns partitions clusters. proof. proceed constructing data required partition. consider data points arranged straight line want split clusters ﬁtting concrete partition purpose arrange clusters line non-increasing order cardinality. cluster shall occupy unit length. space clusters cluster) case application k-means algorithm lead desired partition. reasons follows case k-means-ideal right cluster partition diﬀerent containing space clusters. deﬁnition distance chosen split parts along space clusters attach left right part neighboring clusters splitting cluster number clusters falls resulting partition optimal. hence optimal k-means-ideal clustering contain spaces mean exist versions k-means distances euclidean distance. wanted demonstrate notion embedding needed want look k-means kleinberg’s axioms perspective. case k-means-random clusters seeded spaces clusters large clustering resulting seeding identical upon subsequent steps partition change more. consider case random initialization partition cluster centers cluster seeded cluster center right would nonetheless contain data element consider therefore clusters left cluster center right set. cluster formed elements closest contain therefore cluster center update step k-means-random move right position points closest therefore steps become center later process happen seeded clusters left till partition formulate argument precisely. mentioned earlier following probabilistic algorithms talk probabilistic k-richness obtainable probability independent actual clustering intended obtained. probability increased wishes scheme whenever initialization place seeds clusters spread right given clusters without seeds there ensuring cluster gets cluster center. note clusters lacking seeds left move seeds there. hence cluster sorted decreasing size order left right probability seeding upon moving cluster centers right assign cluster cluster center amounts least k/kk. computed follows favorable seeding occurs ﬁrst seed ﬁrst cluster seed cluster left. clusters sorted non-increasingly probability hitting ﬁrst cluster least results furthermore targeted clustering absolute minimum k-means-ideal hence k-means-random multiple time order achieve desired probability kneed certainty need rerun k-means-random times richness. e.g. ﬁrst seed distributed like k-means assurance unhit cluster hit. probability cluster seeding step ﬁrst proportional squared distances cluster elements closest seed assigned earlier. consider cluster far. closest cluster left least distance stress exist attempts upgrade k-means algorithm choose proper portion variance explained clustering used quality criterion. well known increase increases value criterion. optimal deemed increase stops signiﬁcant. construction could extended cover range values choose from. however full richness achievable split clusters better keeping single cluster maximum attained criterion either clustering trivial quite large number partitions excluded. however even k-richness oﬀers large number partitions choose from. kleinberg proved artiﬁcial example k-means algorithm consistent. kleinberg’s counter-example would require embedding high dimensional space non-typical k-means applications. also kmeans tends produce rather balanced clusters kleinberg’s example could deemed eccentric. transformation consisting rotating line segments around point plane spread ﬁrst coordinates towards ﬁrst coordinate axis angle axis degree. k-means yields diﬀerent partition splitting line segments happen seemingly intuitive axioms lead contradiction. need look carefully consistency axiom conjunction scale-invariance. γ-transform kleinberg claimed describing situation moving elements distinct clusters apart elements within cluster closer another. recall intuition behind clustering partition data points members cluster close another distance members diﬀerent clusters distant another distance high. intuitively obvious moving elements distinct clusters apart elements within cluster closer another make partition look better. theorem clustering algorithm conforming consistency scaling invariance axioms distance derived distance consistency transformation obtained scaling existence cannot always obtained consistency axiom transformation. distance function γ-transform implies points embedded space straight line. without restricting generality assume coordinates points space located points resp. assume want perform γ-transformation kleinberg manner data points remain move elements second i.e. closer another stay shifted least apply rescaling original interval multiply coordinates stays closer before. could made things still drastic transforming instead going case rescaling would result means drastic relocation second cluster towards ﬁrst distance clusters decreases instead increasing claimed kleinberg. surprise. transform moved elements cluster closer together apart distinct clusters rescaling disturb proportions. turned way. contradicts consistency assumption. something wrong either idea scaling γ-transformation. shall reluctant blame scaling except practical case scaling leads indiscernibility points respect measurement errors. note observe clash invariance richness. distance functions demonstrates richness clustering function conforming richness scaling scaling distance functions demonstrate richness clustering function again. scaling impair richness. consider counter-intuitiveness consistency axiom. illustrate recall ﬁrst fact large portion known clustering algorithms uses data points embedded dimensional feature space usually distance euclidean distance therein. imagine want perform γtransform single cluster partition γ-transform shall provide distances compatible situation elements single cluster change position embedding space. proof. assume cluster internal point cluster hyperplane containing points clusters side. furthermore assume clusters contain together data points untypical case. problem starts. position determined distances elements clusters increase distance would necessarily decrease distance contrary consistency requirement. hence claim γ-transform enforces either adding dimension moving aﬀected single cluster along change positions elements least clusters within embedding space. therefore vast majority algorithms meet consistency also inner consistency requirement. moving second cluster problematic? illustrate diﬃculties original kleinberg’s consistency looking application known k-means algorithm allowed cover range single value two-dimensional data visible figure example mixture data points sampled normal distributions. k-means algorithm expected separates quite well points various distributions. visible second column table fact best reducing unexplained variance. figure illustrates result γ-transform results former clustering. visually would tell clusters. look third column table convinces really best choice clustering data k-means algorithm. course contradicts kleinberg’s consistency axiom. demonstrates weakness outer-consistency concept well. theorem k-means allowed range values limited variance increase criterion choice operating ﬁx-dimensional space euclidean distance cannot conform outerconsistency axiom. ﬁnally look figure again. ignore right cluster turns cluster points surrounded points clusters. therefore possible move cluster inﬁnitely small distance without decreasing distances diﬀerent clusters. therefore theorem contradicts apparently claim k-means possesses property outer-consistency. word theorem however continuously strict conjunction euclidean distance. embedding euclidean space causes problem. already mentioned richness near-richness forces introduction reﬁnement-consistency weak concept. even allow resolution contradiction kleinberg’s framework still make suitable practical purposes. serious drawback kleinberg’s axioms richness requirement. whether possible richness partition exists always distance function clustering function return partition restrict clustering function rich more even anti-chain. consider following clustering function takes distance function takes distinct values three data points creates clusters points belong cluster otherwise belong distinct clusters. hand takes distance function exhibiting property works like k-means. obviously function rich time conﬁned theorem partition elements returned clustering function distance function satisﬁes consistency exists distance function embedded elements partition proof. show validity theorem construct appropriate distance function embedding dmax maximum distance considered elements clusters contained cluster construct ball radius equal minxy∈cix=y ball located origin coordinate system. b...i ball containing balls center c...i radius r...i. ball located surface ball center c...i− radius r...i− dmax select distinct locations elements within ball distance function deﬁne euclidean distances within constructed locations. stated ﬁrst richness easy achieve. imagine following ’clustering function. order nodes average distance nodes tights squared distance sorting achieved unsortable points cluster. create enumeration clusters onto unit line segment. take quotient lowest distance largest distance state quotient mapped line segment identiﬁes optimal clustering points. though algorithm simple principle meets axioms richness scale -invariance practical problem limitations furthermore algorithms cluster analysis constructed incremental way. useless clustering quality function designed unfriendly way. example function logical functions class member distances non-class member distances look sample data table cluster quality function invented along line exact quality value computed partitioning ﬁrst points data illustrated table turns best partition points give hint best partition points therefore possible partition needs investigated order best one. summarizing examples learnability theory points basic weaknesses richness even near-richness axioms. hand hypothesis space learning clustering sample hand exhaustive search space prohibitive theoretical clustering functions make practical sense. problem. clustering function data practically unable learn structure data space data learning capability necessary least cases either data representatives larger population distances measured measurement error both. note speak much broader aspect so-called cluster stability cluster validity pointed luxburg obvious richness axiom kleinberg needs replaced requirement space hypotheses large enough. k-means algorithm shown theorem k-richness satisﬁed k-means satisﬁes scale-invariance axiom consistency axiom needs adjusted realistic. create working deﬁnition transform follows distances cluster changed moving point along axis connecting cluster center reducing within cluster factor distances elements outside cluster kept points elements belong cluster distance strictly lower largest distance elements. belong cluster belong cluster belong cluster. consequence minimum distance elements distinct clusters largest distance elements easily seen weakened richness fulﬁlled. scale-invariance granted relativity inter-cluster distance. consistency redeﬁned obviously element outside cluster distance closest element transform smaller distance closest element transform. note shift attention. insist longer distance element cluster increased rather distance cluster whole shall increase. stronger version inner-consistency would insuﬃcient purposes. generalization euclidean space higher dimensionality seems quite obvious ties distances embed points space points belong cluster distance along dimensions lower largest distance elements along respective dimension. distance understood maximum distances along dimensions. created herewith possibility shrinking single cluster without move ones. pointed impossible kleinberg’s transform increase distances objects distinct clusters. fact intuitively want objects distant rather clusters. proposed keep cluster centroid unchanged decreasing distances cluster elements proportionally insisting distance elements closest element shrunk cluster decrease. approach pretty rigid. assumes capable embed objects euclidean space centroid meaning. algorithm consistent also reﬁnement-consistent. algorithm innerconsistent also consistent. algorithm outer-consistent also consistent. subsumptions centric-consistency. tally table k-means algorithm behaves properly transformation. figure illustrates two-fold application transform recognizable visually inspecting forth column table best choice k-means algorithm centric-consistency axiom followed. theorem k-means algorithm satisﬁes centric consistency following partition local minimum k-means partition subject centric consistency yielding also local minimum k-means. proof. k-means algorithm minimizes equation squares distances objects cluster consider moving data point cluster cluster demonstrated nl+x∗ nj−x∗ assume local optimality obviously symbol cluster quality function instead section axiomatic system scale-invariant case consistency changes opposite direction respect richness apply k-richness. transform data transform elements cluster elements ﬁgure consider partition clusters except transformed elements form cluster clusters? question move data point project orthogonally point properties k-means exclude possibilities. denote second case theorem k-means algorithm satisﬁes centric consistency following partition global minimum k-means partition subject centric consistency yielding also global minimum k-means. proof. consider ﬁrst simple case clusters optimal clustering given objects consist clusters subset shall gravity center origin coordinate sysnt denote cardinalities variances prove contradiction applying transform partition still optimal transformed data points. shall assume contrary transform optimum -means clustering partition another transforms sets turn easily veriﬁed coeﬃcient furthermore d})− quadratic polynomial positive coeﬃcient negative ends interval positive middle. contradiction. proves thesis optimal -means clustering remains optimal transformation. turn general case k-means. optimal clustering given objects consist clusters zk−. subset shall gravity center origin coordinate system. quality cardinality cluster prove contradiction applying transform partition still optimal transformed data points. shall assume contrary transform optimum k-means clustering partition zk−} another k∪zk∪···∪zk−k} transforms disjoint sets turn j=tj easily veriﬁed ﬁcient furthermore zk−}) z∗k}) quadratic polynomial positive coeﬃcient negative ends interval positive middle. contradiction. proves thesis optimal k-means clustering remains optimal transformation. note centric consistency specialization kleinberg’s consistency requirement increased distance elements diﬀerent clusters required based consistency. note also decrease distance need equal elements long gravity center relocate. also limited rotation cluster allowed for. moving clusters motion consistency stated already actually impossible move clusters increase distances elements clusters however shall possibly move away clusters whole increasing distance cluster centers overlapping cluster regions which case k-means represent voronoi-regions. property clustering method conforms motion consistency returns clustering distances cluster centers increased moving point cluster vector without leading overlapping convex regions clusters. concentrate k-means case look neighboring clusters. voronoi regions associated k-means clusters fact polyhedrons outer polyhedrons moved away rest without overlapping region. operation regions permissible without changing cluster structure? closer look issue tells not. k-means terminates neighboring clusters’ polyhedrons touch hyperplane straight line connecting centers clusters orthogonal hyperplane. causes points side hyperplane closely center one. move clusters touch along hyperplane happens points within ﬁrst cluster become closer center cluster vice versa. moving clusters generally change structure unless points actually within polyhedrons rather within paraboloids appropriate equations. moving along border hyperplane change cluster membership intrinsic cluster borders paraboloids. would happen relocate clusters allowing touching along paraboloids? problem occur again. consider problem susceptibility class membership change within plane containing cluster centers. cluster center located point plane border ﬁrst cluster characterized function shape border same properly touching point symmetry conditions easily sees touching point must point lies surface must hold. point border ﬁrst cluster center following must hold this nice trick behind claim incremental k-means identify perfectly separated clusters. clusters k-means points polyhedrons contrary assumptions borderline ﬁrst cluster touching point clusters. equation means orthogonal. property implies must deﬁnition circle centered reasoning applies touching point clusters k-means cluster would ball-shaped order allow movement clusters without elements switching cluster membership. stated however clusters even enclosed ball-shaped region need separated suﬃciently properly recognized. consider circumstances cluster radius containing elements would take elements cluster radius consider worst case center subcluster lies straight line segment connecting cluster centers. center remaining subcluster would line side second cluster center. distances centers center second cluster. relations note motion consistency axiom substitute outer-consistency impossible continuously euclidean space. underlined speak local optimum k-means. abovementioned size global k-means minimum elsewhere clustering possibly without gaps. also motion consistency transformation preserves local minimum partition applied local minima global minimum change. subsequent sections investigate rigidity transformations weakened appropriate width gaps grant properties global minimum. particular shall study well clusters need separated enough global optimum. problem k-means discrepancy theoretically optimized function k-means-ideal) actual approximation value. appears problematic even well-separated clusters. commonly assumed good initialization k-means clustering seeds diﬀerent clusters. well known circumstances k-means recover poor initialization consequence natural cluster split even well-separated data. theorem distance cluster centers least radius ball centered enclosing cluster also radius ball centered enclosing cluster cluster seeded clusters cannot loose cluster elements k-meansrandom k-means++ iterations. deﬁnition shall clusters centered enclosed balls centered radius nicely ball-separated distance least ρab. pairs clusters nicely ball separated ball radius shall perfectly ball-separated. consider plane established line parallel line projections onto plane. establish hyperplane orthogonal passing middle line segment hyperplane containing boundary clusters centered balls centered hyperplane orthogonal plane figure manifest intersecting line cross circles around projections respective balls. draw solid lines circles tangential them. line lines case cluster center jump ball. therefore show line circle suﬃcient consider symmetrical). center line segment draw point line parallel cuts circles points notice centric symmetry point transforms circles another point images points symmetry. examine circle center note lines distance line c’d’. note also absolute values direction coeﬃcients tangentials circle identical. distant lines line gets closer gets bigger ye∗a becomes smaller. properties circle increases decreasing rate ye∗a decreases increasing rate. ye∗a biggest value identical discuss point notions perfect separation introduced theorem ackerman dasgupta show incremental k-means algorithm introduced algorithm able cluster correctly data perfectly clusterable however obvious perfect-ball-separation introduced incremental k-means algorithm discover structure clusters. reason follows. perfect ball separation ensures exists enclosing ball distance points within ball lower bigger whenever ackerman’s incremental k-mean merges points points ball. upon merging resulting point lies within ball. conclude note point however incremental k-means algorithm would return cluster centers without stating whether perfect ball clustering. important know case otherwise resulting cluster centers arbitrary unfavorable conditions correspond local minimum k-means ideal all. however allowed inspect data second time diﬀerence deﬁnition well separatedness lies essentially understanding clustering partition data points fact user interested partition sample space hence also correction kleinberg’s axiomatic framework take account. turn concept nice clustering show theorem nice clustering cannot discovered incremental algorithm memory linear theorem show incremental algorithm cluster centers detect points nice clusters. incremental k-means achieve even nice convex conditions. surely concept nice-ball-clustering even restrictive nice-convex clustering. upgrade candidates algorithm behaves like k-means replace step moving bottom-up assign internal node data point children assignment internal node properly weighted average algorithm upgraded incremental k-means version fact return reﬁnement clustering. more allowed second pass data pick real cluster centers using upgrade candidates algorithm. algorithms considered section fail second pass data candidates single linkage tree assign leaf node corresponding data point moving bottom-up assign internal node indicating left right child. return points distance root discuss kleinberg axioms perfectly ball-separated clusters. clear k-means random k-means++ gets initiated initial cluster center hits diﬀerent cluster upon subsequent steps cluster centers leave clusters. gets stuck minimum necessarily global one. understand kleinberg’s phrase function returns clustering possible minima clustering functions. k-richness trivially granted restrict perfectlyball-separated clusters. performs scaling perfectly ball separated clusters remain perfectly ball separated applies moving-consistency transformation clusters remain perfectly ball separated. also centric-consistency transformation keep partition realm perfect-ball-clusterings. hence property clustering method conforms inner cluster consistency returns clustering positions distances cluster centers kept distances within cluster decreased theorem k-means restricted perfectly ball separated clusterings conforms k-richness scale-invariance motion consistency inner cluster consistency. perfect clustering also exists perfect ball clustering clusters exists clustering. regrettably inner cluster consistency transformation data perfect ball kclustering obtain data perfect ball clustering possible even impossible prior transformation. albeit nested clusters emerge. would choose largest number last least make remark even perfect-ball-clustering exists need global optimum k-means ideal possible diﬀerent cardinalities clusters. fact global optimum imperfect even perfect clustering exists. state thing. assume allow broader range values k-means. note centric consistency contrary inner cluster consistency transform perfect ball structures emerge. therefore theorem k-means ranging values assume returns perfectly/nicely ball separated clusterings largest possible conforms richness scale-invariance motion consistency centric consistency. theorem cluster centers. radius ball centered enclosing cluster also radius ball centered enclosing cluster. distance cluster centers amounts pick points cluster cluster clusters preserve balls centered radius consider plane established line parallel line projections onto plane. establish hyperplane orthogonal passing middle line segment hyperplane containing boundary clusters centered balls centered hyperplane orthogonal plane figure manifest intersecting line cross inner circles around projections respective balls. draw solid lines circles tangential them. line lines case cluster center jump ball. therefore show circle suﬃcient consider symmetrical). center line segment draw point line parallel cuts circles points notice centric symmetry point transforms circles another point cinf dine. images points symmetry. examine circle center note lines distance line c’d’. note also absolute values direction coeﬃcients tangentials circle identical. distant lines line gets closer gets bigger identical cluster clustering) gaps clusters amount least randomly picked point cluster used initial cluster center k-means. happens initial cluster center lies appropriate core ﬁrst iteration k-means clusters properly formed. however cluster centers core chance ﬁrst iteration clusters possess stranger cluster elements strangers come cores clusters. hence would interested getting cluster centers cores next iteration. worst case cluster lose oﬀ-core elements clusters obtain oﬀ-core elements. clearly core separation incremental k-means fail usually recover clustering. either well-separatedness criterion core-clustering perfect-ball-clustering nice-ball-clustering applies k-meansrandom k-means++ appropriate clusters seeded representative cluster. theorems substituting perfect core clustering apply. becomes obvious k-richness axiom make much sense. even clusters turn well separated probability hitting cluster element growing sample size prohibitively small. k-means random small clusters lower number required restarts k-means grow approximately linearly better exhaustive search least kn−k possibilities still prohibitive. would render k-means useless. respective retrial counts look signiﬁcantly better k-means++ still unacceptable. alternatively consider oﬀ-core elements noise need bounded ball. cores parts cluster enclosed balls centered cluster center distance ball centers four times radius core. case upon initialization. must surely lower core smallest cluster. rejecting share elements risk removing parts distant cluster. keep likely included seeding must keep bounded ration noise contribution cluster contribution. noise would distance cluster unfavorable case. balance contribution noise cluster minus noise ratio noise smallest cluster ration investigate circumstances possible tell without exhaustive check well separated clusters global minimum k-means. ratio largest smallest cluster cardinality plays important role. therefore k-richness fact welcome. seek highest possible central squares combined clusters would lower lowest conceivable combined sums squares around respective centers clusters variance cluster distance center subcluster center cluster vilj distance center subcluster center subcluster clj. total k-means function clusters amount proceed second case. cluster contains subcluster maximum cardinality diﬀerent cluster relation unique reindex actually contains maximum cardinality subcluster cjj. rewrite inequality call above-mentioned well-separatedness absolute clustering. sees immediately inner cluster consistency kept time terms global optimum restraint clusters. regrettably structure emerge upon consistency therefore maximal number possible absolute clusters kept. however apply centric consistency max-k-means keeps richness/invariance/motion consistency axioms. theorem k-means ranging values assume returns absolutely separated clusterings largest possible conforms globally richness scale-invariance motion consistency centric consistency. make remark theorem applies kleinberg’s consistency transformation euclidean space continuously course because possible already shown discrete manner jumping clusters transform represented superposition motion consistency transform inner cluster consistency transform. reason follows consider cluster point another cluster compute distance cluster center cluster prior kleinberg’s consistency transformation increases. consider distance |c|+ distance center hence generalize also distances clusters increase kleinberg’s consistency transformation. hence fact kleinberg’s consistency transformation represented superposition mentioned transforms. property method matches condition inner cluster proportional consistency decreasing distances within cluster factor speciﬁc cluster keeping position cluster center space returns partition. theorem k-means ranging values assume returns absolutely separated clusterings largest possible conforms globally richness scaleinvariance motion consistency inner cluster proportional consistency. note motion consistency inner cluster proportional consistency include special case outer-consistency. denied theorem general clustering function simultaneously satisfy outer-consistency scaleinvariance richness. make point remark insist inner cluster proportional consistency. reasonable assumption consistency transformation would possible partition given cluster subject consistency transformation would take advantage consistency transformation substructures would occur cluster. context k-means would mean following. consider cluster partition whole distance prior consistency transformation distance consistency transformation. consider alternative partitions denote quality function distance would expect unless trivial partition element separate cluster. hold pair partitions including following ones puts points separate clusters except single cluster puts points separate clusters except shortened respectively consistency transformation. requirement means induction whole consistency transformation would need shorten distances within factor. result justiﬁes inner cluster proportional consistency concept special case centric consistency. theorem k-means ranging values assume returns absolutely separated clusterings largest possible conforms globally richness scale-invariance unidirectional reﬁnement consistency. inspect eﬀect k-richness described cases. inequality large discrepancy maximum minimum cluster size implies clusters needs grow absolute clustering. inequality something similar time relation smallest cluster overall number elements sample play dominant role. additionally size impacted number clusters. paper contrary results former researchers reached conclusion k-means algorithm comply simultaneously kleinberg’s krichness scale-invariance consistency axioms. variant k-means comply simultaneously kleinberg’s richness scale-invariance reﬁnement consistency axioms. variant k-means even comply richness scale-invariance motion plus inner proportional consistency axioms. last axioms pretty well approximate kleinberg’s consistency without creating risk emergence structures within cluster. richness concerned already ackerman showed properties like stability malicious attacks requires balanced clusters hence k-richness counterproductive seeking stable clusterings. paper pointed number problems richness near-richness axiom itself. major ones huge space search hostile clustering criterion problems ensuring learnability concept clustering population richness scaling-invariance alone lead contradiction special case. showed also consistency axiom constitutes problem neither consistency inner-consistency outer-consistency executed continuously euclidean space limited dimension. therefore substitute inner-consistency proposed centric consistency showed k-means property centric consistency. investigating substitute outer-consistency motion consistency showed clusters necessary motion consistency k-means shape cluster counts enclosed ball k-means. therefore investigated impact behavior k-means light kleinberg’s axioms. showed perfect ball clustering local minimum k-means function perfect ball clusterings axioms invariance k-richness inner cluster consistency motion consistency hold consider variant k-means varying broad spectrum take ﬁnal clustering perfect ball clustering largest number clusters possible instead inner cluster consistency centric consistency used approximation near-richness achieved. would exclude clusters several cluster members grounds fact statistically speaking want sure probability element occurring smaller cluster times. elements cluster k-means-random k-means++ k-richness constitutes problem appropriate seeding. seeding becomes important gaps gaps prohibit recovery inappropriate seeding. investigated absolute clustering realm space perfect ball clusterings turn global minimum k-means. k-richness requirement widens gaps clusters necessary. axiomatic behavior diﬀer much perfect ball clusterings except fact transformations remain real absolute clusterings. introduction gaps draws attention important issue broader gaps clustering properties close kleinberg’s axioms. happens price violating kleinberg’s implicit assumptions clustering function always returns clustering. illustrate point incremental clustering algorithms ackerman dasgupta. prove theorems form perfect clustering exists algorithm returns question raised algorithm return clustering perfect? algorithms return something. agree approach. clustering type algorithm good looking exist algorithm state found clustering type clustering type clustering given type exist. would response ideal algorithm. worse still usable would give ﬁrst answers. investigation show post-processing kmeans would capable answer question whether found clustering nice-ball-clustering perfect-ball-clustering absolute-clustering none them. second question type clustering looking for? habit k-means stop lowest value quality function reached. clustering worse ones generated in-between e.g. perfect-ball-clustering exists become victim unbalanced cluster sizes. looking become also victim transformations kleinberg proposing. natural clusters returned k-means preferably ball-shaped centric consistency transformation motion consistency transformation kleinberg consistency transform preserve them gaps conform perfect-ball absolute separation. voronoi diagrams shapes motion consistency transformation kleinberg’s consistency transformations destructive. however connected well separated area would deemed good cluster even centric consistency transformation turn disastrous. kleinberg’s consistency transformation dissimilar statement made richness related axioms. requirement rich space hypotheses imposes heavy burden clustering algorithms. shall instead envision hypotheses spaces rich enough still learnable decision possible still hypotheses space solution. disagree extent opinion expressed axiomatic systems deal either clustering functions clustering quality function relations quality partitions. particular formulations axiomatic systems state rather equivalence relations clusterings themselves. hence must ﬁrst imagination kind clusters looking formulate axioms transformations reasonable within target class clusterings lead outside class equivalence relations clusterings makes sense within class need deﬁned outside. hence still much space research clustering axiomatization especially clariﬁcation types clusters real interest whether axiomatised way. kleinberg pointed problem good starting point. authors wish thank institute computer science polish academy sciences promoting ﬁnancing research. research done robert klopotek ﬁnanced research fellowship within project ’information technologies research interdisciplinary applications’ agreement number uda-pokl...--/-.", "year": 2017}