{"title": "Qualitatively characterizing neural network optimization problems", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve optimization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct training with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We find that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any significant obstacles.", "text": "goodfellow∗ oriol vinyals∗ andrew saxe∗∗ ∗google inc. mountain view ∗∗department electrical engineering stanford university stanford {goodfellowvinyals}google.com asaxestanford.edu training neural networks involves solving large-scale non-convex optimization problems. task long believed extremely difﬁcult fear local minima obstacles motivating variety schemes improve optimization unsupervised pretraining. however modern neural networks able achieve negligible training error complex tasks using direct training stochastic gradient descent. introduce simple analysis technique look evidence networks overcoming local optima. that fact straight path initialization solution variety state neural networks never encounter signiﬁcant obstacles. neural networks generally regarded difﬁcult optimize. objective functions must optimize order train non-convex many theoretical guarantees performance popular algorithms problems. nevertheless neural networks commonly trained successfully obtain state results many tasks. paper present variety simple experiments designed roughly characterize objective functions involved neural network training. experiments intended measure speciﬁc quantitative property objective function rather answer simple qualitative questions. neural networks enter escape series local minima? move varying speed approach pass variety saddle points? answering questions deﬁnitively difﬁcult present evidence strongly suggesting answer questions show exists linear subspace neural network training could proceed descending single smooth slope barriers. early symmetry breaking conspicuous example non-convexity. important question happens leaves well-behaved linear subspace. main text article restricted experiments peer-reviewed prior iclr appendix presents additional experiments added review process ended. experiments show cases encounter obstacles ravine shapes path never found evidence local minima saddle points slowed trajectory. suggests less exotic problems poor conditioning variance gradient estimate primary difﬁculties training neural networks. cases examine total cost function course ever acts unbiased stochastic approximations loss function. structure stochastic approximations could different global loss functions examine here remains possible neural networks difﬁcult train exotic structures individual terms total cost function noise induced sampling minibatches terms. results linear subspace experiments qualitatively seven models examined drawn variety categories including fully-connected supervised feed-forward networks variety activation functions supervised models chosen performed well competitive benchmark tasks. research needed determine whether interpret results implying never encounters exotic obstacles training neural networks implying works well encounter structures. training neural network consists ﬁnding optimal parameters initialized small random initial parameters train using stochastic gradient descent minimize reaching convergence training trajectory follows complicated high-dimensional. difﬁcult summarize trajectory meaningfully two-dimensional visualization. simple learning curves showing value objective function time convey fairly simple information. example learning curve bounces repeatedly know whether objective function highly bumpy whether rapidly changing direction noise stochastic minibatch-based estimate gradient. objective function remains constant long periods time know whether parameters stuck region oscillating around local minimum tracing around perimeter large obstacle. paper introduce simple technique qualitatively analyzing objective functions. simply evaluate series points varying values sweeps line parameter space. whether cross-section objective function along line well-behaved. objective function simple approximately convex shape along cross-section. words knew correct direction single coarse line search could good training neural network. begin investigation simplest kind neural network deterministic feed-forward fully-connected supervised network. experiments mnist dataset using dataset augmentation best result category maxout network regularized dropout adversarial training trained using momentum. appendix paper full speciﬁcation architecture training algorithm subsequent experiments. conﬁguration results average mistakes mnist test examples total. without adversarial training model also performs well mistakes. running linear interpolation experiment problem fig. subspace spanning initial parameters ﬁnal parameters well-behaved spends time exploring region bottom valley. maxout units saturate perhaps surprising optimization simple case. determine whether hard zero saturation relus two-sided saturation logistic sigmoids induce additional difﬁculties linear interpolation experiment activation functions. results presented fig. fig. again subspace spanning initial ﬁnal parameters contains difﬁcult exotic structures. figure experiments maxout mnist. row) state model adversarial training. bottom row) previous best maxout network without adversarial training. left column) linear interpolation experiment. experiment shows objective function fairly smooth within subspace spanning initial ﬁnal parameters model. apart ﬂattening near appears nearly convex subspace. chose initial direction correctly could solve problem coarse line search. right column) progress actual algorithm time. vast majority learning happens ﬁrst epochs. thereafter algorithm struggles make progress. figure linear interpolation curves fully connected networks different activation functions. left) sigmoid activation function. right) relu activation function. figure linear interpolation experiment maxout relus sigmoids mnist plotted axis comparison. plot axis scale otherwise differences bottom curve difﬁcult see. figure higher resolution linear interpolation experiments. tiling interval values zoomed-in view plot. tiling interval values whether initial symmetry breaking causes difﬁcult structures. tiling interval values behavior objective function exotic regions parameters encode fully learned intermediate concepts. show validation objective widely separated training objective would require zooming plot far. possible objection results explored coarse resolution expose local non-convex structures. therefore variety higher-resolution experiments presented fig. experiments dropout resolution high enough expose artifacts induced monte carlo approximation true dropout loss function involves dropout masks. maxout tends overﬁt mnist used without dropout used relus experiments. found increased resolution expose small difﬁcult structures. figure linear interpolation search local minima. left) interpolating different solutions show solution different local minimum within subspace. right) interpolate random point space solution local minima besides solution suggesting local minima rare. figure linear interpolation experiment convolutional maxout network cifar- dataset left) global scale curve looks well-behaved. right) zoomed near initial point shallow barrier must navigate. course multiple minima neural network optimization problems shortest path minima contain barrier higher cost. different solutions using different random seeds random number generators used initialize weights generate dropout masks select examples minibatches. local minima within subspace solution points different solutions appear correspond different choices break symmetry saddle point origin rather fundamentally different solutions varying quality. fig. performed experiments understand behavior neural network optimization supervised feedforward networks verify behavior occurs advanced networks. case convolutional networks single barrier objective function near network initialized. simply correspond network initialized large random weights. barrier reasonably wide tall. fig. details. examine behavior generative models experimented mp-dbm model useful purposes gets good performance generative model classiﬁer objective function easy evaluate secondary local minimum high error visualization trajectory reveals passed enough around anomaly avoid affect learning. figure experiments mp-dbm. left) linear interpolation experiment reveals secondary local minimum high error. right) horizonal axes plot components capture extrema throughout learning process. vertical axis plot objective function. point another epoch actual learning. plot allows pass near anomaly. finally performed linear interpolation experiment lstm regularized dropout penn treebank dataset fig. experiment difﬁcult structures showing exotic features non-convex optimization appear cause difﬁculty even recurrent models sequences. saxe advocated developing mathematical theory deep networks studying simpliﬁed mathematical models networks. deep networks formed composing alternating series learned afﬁne transformations ﬁxed non-linearities. simpliﬁed model functions compose series learned linear transformations. composition series linear transformations linear transformation mathematical model lacks expressive capacity general deep network. however weights model factored learning dynamics resemble deep network. output model linear input model non-linear function model parameters. particular ﬁtting linear regression parameters convex problem ﬁtting deep linear regression parameters non-convex problem. figure linear interpolation small random initialization point solution linear regression model depth shows qualitative features linear interpolation experiments neural networks ﬂattening objective function near saddle point origin minimum within subspace. figure left) interpolation solutions deep linear regression. though solutions connected manifold globally minimal values straight line encounters barrier higher cost. curve dimensional linear model qualitative characteristics curve high dimensional non-linear networks studied. right) interpolation random point large norm solution deep linear regression. neural network search encounter minima solution used initialize search. linear interpolation experiments carried analytically rather experimentally case deep linear regression. results strikingly similar results deep non-linear networks. speciﬁcally show problem training output using mean squared error sufﬁcient produce qualitative features neural network training linear interpolation experiments exposed. fig. fig. reason success wide variety tasks clear tasks relatively easy optimize. finding good direction high-dimensional space difﬁcult problem nearly difﬁcult navigating error surface complicated obstacles within multiple different low-dimensional subspaces. work considered neural networks perform well. possible neural networks perform well extensive hyperparameter search found problems able optimize easily hyperparameters correspond optimization problems hard. particular seems likely large neural networks easier particular task. future work characterize problems easy understand able avoid obstacles present determine training large models remains slow despite scarcity obstacles. advanced optimization algorithms could reduce computational cost deploying neural networks enabling small networks reach good performance could reduce cost training large networks reducing amount time required reach solution. would like thank j¨org bornschein eric drexler yann dauphin helpful discussions. would like thank yaroslav bulatov chung-cheng chiu greg corrado jeff dean reviews drafts work. would like thank developers theano pylearn. references bastien fr´ed´eric lamblin pascal pascanu razvan bergstra james goodfellow bergeron arnaud bouchard nicolas bengio yoshua. theano features speed improvements. deep learning unsupervised feature learning nips workshop bergstra james breuleux olivier bastien fr´ed´eric lamblin pascal pascanu razvan desjardins guillaume turian joseph warde-farley david bengio yoshua. theano math expression compiler. proceedings python scientiﬁc computing conference june oral presentation. dauphin yann pascanu razvan gulcehre caglar kyunghyun ganguli surya bengio yoshua. identifying attacking saddle point problem high-dimensional non-convex optimization. ghahramani welling cortes lawrence n.d. weinberger k.q. advances neural information processing systems curran associates inc. glorot xavier bordes antoine bengio yoshua. deep sparse rectiﬁer neural networks. jmlr w&cp proceedings fourteenth international conference artiﬁcial intelligence statistics april goodfellow warde-farley david lamblin pascal dumoulin vincent mirza mehdi pascanu razvan bergstra james bastien fr´ed´eric bengio yoshua. pylearn machine learning research library. arxiv preprint arxiv. goodfellow warde-farley david mirza mehdi courville aaron bengio yoshua. maxout networks. dasgupta sanjoy mcallester david international conference machine learning jarrett kevin kavukcuoglu koray ranzato marc’aurelio lecun yann. best multi-stage architecture object recognition? proc. international conference computer vision ieee lecun yann kavukcuoglu koray farabet cl´ement. convolutional networks applications vision. circuits systems proceedings ieee international symposium ieee srivastava nitish hinton geoffrey krizhevsky alex sutskever ilya salakhutdinov ruslan. dropout simple prevent neural networks overﬁtting. journal machine learning research experiments except sigmoid network using hyperparameters taken directly literature. fully specify here. adversarially trained maxout network model used goodfellow public conﬁguration paper describes modify previous best maxout network obtain maxout network model retrained using publicly available implementation used goodfellow code available relu network dropout model intended nearly reproduce dropout relu network described srivastava standard reference implementation provided pylearn speciﬁc available here relu network without dropout simply removed dropout preceding conﬁguration ﬁle. sigmoid network simply replaced relu non-linearities sigmoids. performs acceptably sigmoid network; gets test error rate convolutional network used best convolutional network available pylearn cifar- dataset speciﬁcally developed goodfellow reduce computational cost computing training objective order used variant without data augmentation. conﬁguration available here https//github.com/lisa-lab/pylearn/blob/master/pylearn/ scripts/papers/maxout/cifar_no_aug_valid.yaml figure plots projection along axis initialization solution versus norm residual projection random walks varying dimension. plot formed using steps. designate step solution continue plot steps order simulate neural network training trajectories continue past point early stopping validation criterion chooses solution. step made incrementing current coordinate sample gaussian distribution zero mean unit covariance. dimensionality space forces trajectories highly regular shape kind plot meaningful investigating behaves moves away subspace study paper. shown that seven different models practical interest exists straight path initialization solution along objective function decreases smoothly monotonically least resolution experiments investigated. might naturally want plot norm residual parameter value projecting parameters point time subspace identiﬁed. begins solution point chosen early stopping visits time deﬁne unit vector pointing direction primary subspace investigated line θi)u. plot coordinate within subspace horizontal axis. deﬁne second unit vector pointed direction projection subspace αu)v. words norm residual projection onto line spanning initialization solution. plot vertical axis. next section shape objective function terms coordinates interesting features vary problem another. however important understand kind plot tell much shape trajectory. tells strays primary linear subspace. high dimensional spaces shape curve convey much information. fig. demonstration plot converges simple geometric shape dimensionality random walk increases. figure show effect different learning rates momentum coefﬁcients plot projection residual norm gradient descent several different hyperparameters. case make plots comparable true solution synthetic convex problem endpoint trajectories. problem dimensional curves simple shape primary quantity distinguishing maximum norm residual. figure plot examines linear path strays training maxout network mnist. axis projection along linear path initialization solution. axis norm residual. plot uses norm also known euclidean norm. squared norm. plots residual norm projection trajectories converge similar geometric shape high dimensional spaces. fig. example several different runs problem. however still glean information kind plot looking maximum norm residual comparing maximum norm parameter vector whole. show kind plot maxout network fig. keep mind shape trajectory interesting ratio norm residual total norm parameter vector point give idea much information projection discards. plot linear subspace captures least norm parameter vector points time. natural question whether exist obstacles well-behaved linear subspace described path followed sgd. investigate introduce additional direction exploration. speciﬁcally would like explore line passing point projection primary subspace investigated far. line contains obstacles could explain exploit simple behavior within subspace. figure error surface factored linear model layers unit layer. error surface shows saddle point origin non-linear manifold global solutions. trajectory initialization solution encounters negative curvature near initial point positive curvature near ﬁnal point encounter exotic obstacles. factored linear model training loss accomplish viewing heatmap cost function. ﬁgure predicts many properties expect complicated neural network models negative curvature initialization positive curvature surrounding solution lack obstacles separating trajectory well-behaved region function connected manifold globally optimal points. case connected manifold hyperbola neural networks pattern equivalent solutions different. example take deep rectiﬁed linear network obtain equivalent network rescaling parameters. modify parameters layer multiplying bias column weight matrix preserve function input network respesents dividing weights layer case factored linear model hyperbolic shape manifold means linearly interpolating solution points reveals region high cost middle even though solutions connected manifold solutions. kind plot directly achievable problems parameters. must summarize parameters coordinate somehow. remainder section summarize parameters projection line spanning initialization solution coordinate deﬁned previous section) projection line orthogonal subspace passes point deﬁned coordinate previous section. figure canonical example deep factored linear model deep network training trained linear model mean squared error mnist. speciﬁcally multiply together matrices ﬁrst nine square dimension mnist input last columns. output thus dimensional vector. mean squared error encourages element vector value close elements zero true class plot shows negative curvature near initialization point positive curvature near solution point general lack obstacles. plot cost function cost function deep factored linear model roughly structure cost function lstm feedforward networks models show structures predicted factored linear model learning dynamics. however adversarially trained maxout network obstacle small height steep constrains narrow canyon preventing accessing subspace studied main text paper finally recall mp-dbm local minimum within primary subspace plot passed around plateau surrounding point. note visualizations signﬁcant negative curvature early part trajectory seem difﬁculty escaping saddle point near origin. possible explanation behavior model sufﬁciently small step size naturally avoids saddle points. consider trajectory function time analytical model small step size consider continuous-time gradient descent process figure visualizations feedforward networks shapes qualitatively factored linear network. show adversarially trained relu network representative sample. hessian matrix respect view shows second-order approximation time continuous-time gradient descent incorporates second-order information space hessian matrix. speciﬁcally second-order term taylor series expansion equivalent ascending gradient ||∇θj||. words ﬁrst-order term says downhill second-order term says make gradient bigger. latter term encourages exploit directions negative curvature. visualization typically used tool understanding structure neural network objective functions. mostly neural network objective functions highdimensional visualizations necessity fairly dimensional. section include control visualizations reminder need interpret low-dimensional visualization carefully. visualizations showed rich structure cost function relatively simple shape trajectory. it’s important remember visualizations showing linear subspace. instead showing multiply subspaces rotated parallel other. particular choice subspaces intended capture variation cost function side effect discards variation high-dimensional trajectory reducing trajectories semi-circles. control instead plot randomly selected linear subspace intermediate control generated plot mp-dbm axis axis random linear projection. allows true linear subspace signiﬁcant variation cost function choice ﬁrst axis also allows trajectory semi-circle. fig. figure control experiment plot random subspace intersecting solution point. subspace complicated trajectory essentially variation cost function value. visualization useful reminder visualizations presented paper designed expose variation cost function discard variation shape trajectory. directions cost function high variability trajectories vary greatly.", "year": 2014}