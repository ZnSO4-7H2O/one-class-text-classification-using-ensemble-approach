{"title": "A Deep Reinforcement Learning Chatbot", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "stat.ML", "I.5.1; I.2.7"], "abstract": "We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neural network models. By applying reinforcement learning to crowdsourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A/B testing with real-world users, where it performed significantly better than many competing systems. Due to its machine learning architecture, the system is likely to improve with additional data.", "text": "present milabot deep reinforcement learning chatbot developed montreal institute learning algorithms amazon alexa prize competition. milabot capable conversing humans popular small talk topics speech text. system consists ensemble natural language generation retrieval models including template-based models bag-of-words models sequence-to-sequence neural network latent variable neural network models. applying reinforcement learning crowdsourced data real-world user interactions system trained select appropriate response models ensemble. system evaluated testing real-world users performed signiﬁcantly better many competing systems. machine learning architecture system likely improve additional data. dialogue systems conversational agents including chatbots personal assistants voicecontrol interfaces becoming ubiquitous modern society. examples include personal assistants mobile devices technical support help telephone lines well online bots selling anything fashion clothes cosmetics legal advice self-help therapy. however building intelligent conversational agents remains major unsolved problem artiﬁcial intelligence research. amazon.com proposed international university competition goal building socialbot spoken conversational agent capable conversing coherently engagingly humans popular topics entertainment fashion politics sports technology. socialbot converses natural language speech amazon’s echo device article describes models experiments ﬁnal system developed team university montreal. main motivation participating help advance artiﬁcial intelligence research. competition provided special opportunity training testing state-of-the-art machine learning algorithms real users relatively unconstrained setting. ability experiment real users unique artiﬁcial intelligence community vast majority work consists experiments ﬁxed datasets software simulations addition computational resources technical support ﬁnancial support provided amazon helped scale system test limits state-of-the-art machine learning methods. among things support enabled crowdsource labels amazon mechanical turk maintain dedicated tesla gpus running live system. socialbot based large-scale ensemble system leveraging deep learning reinforcement learning. develop deep learning models natural language retrieval generation including recurrent neural networks sequence-to-sequence models latent variable models evaluate context competition. models combined ensemble generates candidate dialogue responses. further apply reinforcement learning including value function policy gradient methods train system select appropriate response models ensemble. particular propose novel reinforcement learning procedure based estimating markov decision process. training carried crowdsourced data interactions recorded real-world users preliminary version system. trained systems yield substantial improvements testing experiments real-world users. competition semi-ﬁnals best performing system reached average user score scale minimal number hand-crafted states rules without engaging non-conversational activities performance best system substantially better average teams competition semi-ﬁnals. further system averaged high turns dialogue also signiﬁcantly higher average teams competition semi-ﬁnals well ﬁnalist teams. improvement back-and-forth exchanges user system suggests system likely engaging system among systems competition. finally system bound improve additional data nearly system components learnable. early work dialogue systems based mainly states rules hand-crafted human experts. modern dialogue systems typically follow hybrid architecture combining hand-crafted states rules statistical machine learning algorithms complexity human language however probably never possible enumerate states rules required building socialbot capable conversing humans open-domain popular topics. contrast rule-based systems core approach built entirely statistical machine learning. believe plausible path artiﬁcially intelligent conversational agents. system architecture propose aims make assumptions possible process understanding generating natural human language. such system utilizes small number hand-crafted states rules. however every system component designed optimized using machine learning algorithms. system components trained ﬁrst independently massive datasets jointly real-world user interactions. system learn relevant states rules conducting open-domain conversations implicitly. given adequate amount examples system outperform systems based hand-crafted states rules. further system continue improve perpetuity additional data. system architecture inspired success ensemble-based machine learning systems. systems consist many independent sub-models combined intelligently together. examples ensemble systems include winner netﬂix prize utilizing hundreds machine learning models predict user movie preferences watson ﬁrst machine learning system quiz game jeopardy recently google observed substantial improvements building ensemble-based neural machine translation system system consists ensemble response models. response models take input dialogue output response natural language text. addition response models also output several scalar values indicating internal conﬁdence. explained later response models engineered generate responses diverse topics using variety strategies. throughout semi-ﬁnals carried several testing experiments evaluate different variants system score based best performing system period july august score based leaderboard averages scores variants system dialogue manager responsible combining response models together. input dialogue manager expects given dialogue history conﬁdence values automatic speech recognition system generate response dialogue manager follows three-step procedure. first uses response models generate candidate responses. second exists priority response candidate responses response returned system. example question \"what name?\" response alexa prize socialbot\" priority response. third priority responses response selected model selection policy. example model selection policy select response scoring candidate responses picking highest-scored response. overall process illustrated figure conﬁdences certain threshold system requests user repeat last utterance. otherwise system utilize conﬁdences. particular neither response models model selection policy make conﬁdences. nevertheless system perfect. therefore plausible system could improved signiﬁcantly conditioning response models model selection policy conﬁdences. response models system including retrieval-based neural networks generationbased neural networks knowledge base question answering systems template-based systems. examples candidate model responses shown table section aims summarize response models. first template-based models described. start describing template-based response models system. alicebot alicebot uses aiml templates produce response given dialogue history user utterance freely available alice kernel available www.alicebot.org. default templates generate non-priority responses conﬁgure templates related socialbot’s name location output priority responses. modify templates make consistent challenge majority templates remain unchanged. alicebot model also outputs scalar conﬁdence score. since aiml templates repeat user’s input utterance always correct sentences. therefore string-based rules determine response constitutes correct sentence. response correct sentence returns high candidate responses bowescapeplan don’t answer this. bowfactgenerator know male rabbits called bucks females does? gruquestiongenerator american family initiatorbot believe love ﬁrst sight? lstmclassiﬁermsmarco parts learning feed rabbit learning dualencoderredditnews doggie dualencoderredditpolitics who’s edger shboys? alicebot hurrah good number rabbits. elizabot elaborate bit? vhredredditmovies hmm. nope never heard vhredsubtitles would like them? helpresponse would really like talk news politics movies. candidate responses bowescapeplan don’t answer this. bowfactgenerator here’s interesting fact. international telephone dialing code antarctica gruquestiongenerator york city? initiatorbot day? lstmclassiﬁermsmarco third anthology released dualencoderredditnews whisper time bby. dualencoderredditpolitics hahaha mobile ain’t even close. wish could show looks awful sorry man. alicebot like robots movie? elizabot make feel? star wars movie movie star wars series. vhredredditmovies please. please. pleeeease. happen. vhredsubtitles what? helpresponse would really like talk news politics movies. conﬁdence otherwise returns conﬁdence score. process illustrated algorithm algorithm alicebot input dialogue history response apply aiml templates dialogue history response correct sentence response given priority elizabot similar alicebot elizabot model performs string matching select answer templates. model based famous eliza system designed mimic rogerian therefore contrast alicebot elizabot’s psychotherapist. responses personal questions meant engage user continue conversation. ellipses mark parts response sentence replaced text user’s utterance. model detects appropriate template selects corresponding response model runs template response reﬂections better format string response initiatorbot initiatorbot model acts conversation starter asks user open-ended question conversation started increase engagement user. wrote question phrases initiatorbot. examples phrases include \"what today?\" pets?\" \"what kind news stories interest most?\". special case model also start conversation stating interesting fact. case initiator phrase \"did know <fact>?\" fact replaced statement. facts used bowfactgenerator model described later. returning response initiatorbot ﬁrst checks hasn’t already triggered last turns conversation. user gives greeting initiatorbot return response priority. important observed greetings often indicate beginning conversation user particular topic would like talk about. asking question system takes initiative procedure detailed algorithm storybot storybot model outputs short ﬁction story request user. implemented model observed many users asking socialbot tell stories. storybot determines user requested story checking request word story-type word utterance response states story’s title author followed story body. example responses model follows pattern \"alright tell story <story_title> <story_body> <story_author>\" <story_title> title story <story_body> main text <story_author> name story’s author. stories scraped website www.english-for-students.com. example story grasshopper ants worked hard summer. sorted food winter. time grasshopper remained idle. winter came ants enough eat. grasshopper nothing eat. starve. went ants begged foods. ants asked return \"what summer?\" replied idled away time summer\". replied \"then must starve winter.\" moral never idle. evibot evibot response model forwards user’s utterance amazon’s question-answering web-service www.evi.com. designed primarily handle factual questions. therefore evibot returns priority response direct questions deﬁned user utterances containing wh-word otherwise returns non-priority possibly empty query direct question contains non-stop words evibot follow response. three step procedure generate response. first evibot forwards query www.evi.com containing whole user utterance returns resulting answer valid. fails evibot applies nltk’s named entity processor query subqueries named entities. subphrase contains named entity evibot forwards queries www.evi.com returns result upon valid response. finally previous steps fail evibot forwards queries every subquery without named entities returns either valid response empty response. procedure detailed algorithm algorithm evibot input dialogue history query last user utterance has-wh-words true utterance contains wh-word otherwise false has-only-stop-words true utterance stop words otherwise false has-only-stop-words has-wh-words priority has-wh-words subentities entities extracted query using nltk’s named entity processor subphrases list subphrases entities subphrase subphrases bowmovies bowmovies model template-based response model handles questions movie domain. model list entity names tags model searches user’s utterance known entities tags. entities identiﬁed string matching. done cascading order giving ﬁrst preference movie title matches actor name matches ﬁnally director name matches. tags also identiﬁed string matching. however exact string matching fails tags identiﬁcation performed word embedding similarity. entity present agent dispatch call several data sources retrieve data item selected query type. agent limited data available apis access. model’s responses follow predeﬁned templates. movie titles actor names director names extracted internet movie database movie descriptions taken google knowledge graph’s api. movie title queries directed open movie database actor director queries wikiedata used. first search actor director names done wikidata json dump. described earlier model uses word embeddings match tags. word embeddings trained using wordvec movie plot summaries actor biographies extracted imdb database input dialogue history entity entity contained last user utterance list movie titles actors directors entity entity entity contained previous user utterances movie titles actors directors entity entity movie title else entity actor name else entity director name return response vhred models system contains several vhred models sequence-to-sequence models gaussian latent variables trained variational auto-encoders models trained using procedure serban comparison vhred generative sequence-to-sequence models provided serban trained vhred models generate candidate responses follows. first model responses retrieved dataset using cosine similarity current dialogue history dialogue history dataset based bag-of-words tf-idf glove word embeddings approximation log-likelihood responses computed vhred response highest log-likelihood returned. system vhred models based datasets scraped reddit vhred model based news articles vhred model based movie subtitles vhredredditpolitics trained https//www.reddit.com/r/politics extracting vhredredditnews trained reddit https//www.reddit.com/r/news extracting vhredredditsports trained reddit https//www.reddit.com/r/sports vhredredditmovies trained reddit https//www.reddit.com/r/movies vhredwashingtonpost trained reddit https//www.reddit.com/r/politics vhredsubtitles using movie subtitles dataset subtle particular vhredredditpolitics vhredwashingtonpost different retrieval procedure. models logistic regression model score responses instead approximate log-likelihood. logistic regression model trained reddit threads candidate responses annotated amazon mechanical turk workers likert-type scale candidate responses selected reddit threads according cosine similarity w.r.t. glove word embeddings. label collection training procedure logistic regression model similar procedures described section response logistic regression model takes input vhred log-likelihood score well several input features outputs scalar-valued score. even though logistic regression model improve appropriateness responses selected reddit threads vhredredditpolitics used extremely rarely ﬁnal system suggests training model rerank responses based labeled reddit threads responses cannot help improve performance. skipthought vector models system contains skipthought vector model trained bookcorpus dataset semeval task model trained using procedure kiros called skipthoughtbooks. skipthoughtbooks ensures system complies amazon alexa prize competition rules. rule introduced early competition socialbots supposed state opinions related political religious topics. user wishes discuss topics socialbots proceed asking questions stating facts. skipthoughtbooks also handles idiosyncratic issues particular alexa platform. example many users understand purpose socialbot asked socialbot play music. case system instruct user exit socialbot application play music. skipthoughtbooks follows two-step procedure generate response. ﬁrst step compares user’s last utterance trigger phrases. match found model returns corresponding priority response. example user says \"what think donald trump?\" model return priority response \"sometimes truth stranger ﬁction.\". match found skipthought vector model’s semantic relatedness score user’s last utterance trigger phrase predeﬁned threshold user’s last utterance contains keywords relevant trigger phrase. total trigger phrases response sets. model match ﬁrst step proceeds second step. step model selects response among reddit dataset responses. before model responses retrieved using cosine similarity. model returns response highest semantic relatedness score. dual encoder models system contains dual encoder retrieval models dualencoderredditpolitics dualencoderredditnews. models composed sequence encoders encq encr single vhredwashingtonpost responses extracted based cosine similarity current dialogue news article keywords. varies depending number user comments within news articles certain cosine similarity threshold. vhredsubtitles cosine similarity computed based one-hot vectors word. trigger phrases multiple responses. case response selected random. some trigger phrases keywords. case matching based semantic relatedness. lstm recurrent layer used encode dialogue history candidate response. score candidate response computed bilinear mapping dialogue history embedding candidate response embedding lowe models trained using method proposed principle also possible early stopping based separate model trained domain similar target domain response highest score candidate responses retrieved using tf-idf cosine similarity based glove word embeddings. model dualencoderredditpolitics trained reddit https//www.reddit.com/r/politics dataset extracts responses reddit datasets. model dualencoderredditnews trained reddit https//www.reddit.com/r/news dataset extracts responses reddit datasets. bag-of-words retrieval models system contains three bag-of-words retrieval models based tf-idf glove word embeddings wordvec embeddings similar vhred models models retrieve response highest cosine similarity. bowwashingtonpost model retrieves user comments washingtonpost news articles using glove word embeddings. model bowtrump retrieves responses twitter tweets scraped donald trump’s proﬁle https//twitter.com/realdonaldtrump. model also uses glove word embeddings returns response least relevant keyword phrase found user’s utterance list trigger keywords phrases include ’donald’ ’trump’ ’potus’ ’president united states’ ’president ’hillary’ ’clinton’ ’barack’ ’obama’. model bowfactgenerator retrieves responses interesting facts including facts animals geography history. model uses wordvec word embeddings. model bowgameofthrones retrieves responses quotes scraped https//twitter.com/ thronequotes using glove word embeddings. tweets source manually inspected cleaned remove tweets quotes series. bowtrump model list trigger phrases determine model’s output relevant user’s utterance. populate list around popular character names place names family names large unique domain. also added aliases account alternative speech transcriptions named entities. phrases include ’ned stark’ ’jon snow’ ’john snow’ ’samwell tarly’ \"hodor\" \"dothraki\" bowescapeplan system contains response model called bowescapeplan returns response topic-independent generic pre-deﬁned responses \"could repeat again\" don’t know\" \"was question?\". main purpose maintain user engagement keep conversation going models unable provide meaningful responses. model uses logistic regression classiﬁer select response based higher-level features. train logistic regression classiﬁer annotated user utterances candidate response pairs appropriateness likert-type scale user utterances extracted interactions alexa users preliminary version system. candidate responses sampled random bowescapeplan’s response list. label collection training procedure logistic regression model similar procedures described section logistic regression model trained log-likelihood training early-stopping development evaluated testing set. however trained model’s performance poor. obtained pearson correlation coefﬁcient spearman’s rank correlation coefﬁcient indicates logistic regression model slightly better selecting topic-independent generic response compared selecting response uniform random. future work investigate collecting labeled data pre-training logistic regression model. query retrieves ﬁrst search snippets. retrieved snippets preprocessed stripping trailing words removing unnecessary punctuation truncating last full sentence. model uses bidirectional lstm separately last dialogue utterance snippet embedding vectors. resulting representations concatenated passed predict scalar-value indicating appropriate snippet response utterance. model trained binary classiﬁcation model microsoft marco dataset crossentropy predict relevancy snippet given user query given search query search snippet model must output search snippet relevant otherwise zero. search queries ground truth search snippets taken positive samples search snippets selected random negative samples. task model able reach prediction accuracy w.r.t. microsoft marco development set. system able search apis various search engines including google bing aifounded current model choose google search engine since qualitative inspection showed retrieved appropriate responses. system contains generative recurrent neural network language model called gruquestiongenerator generate follow-up questions word-by-word conditioned dialogue history. input model consists three components one-hot vector current word binary question label binary speaker label. model contains layers softmax output layer. model trained reddit politics reddit news conversations wherein posts labeled questions detecting question marks. optimizer adam perform early stopping checking perplexity validation generation ﬁrst condition model short question template generate rest question sampling model question label clamped one. generation procedure stops question mark detected. further length question controlled tuning temperature softmax layer. speed requirements candidate responses generated best w.r.t. log-likelihood ﬁrst words returned. generating candidate response dialogue manager uses model selection policy select response returns user. dialogue manager must select response increases satisfaction user entire dialogue. must make trade-off immediate long-term user satisfaction. example suppose user asks talk politics. dialogue manager chooses respond political joke user pleased turn. afterwards however user disappointed system’s inability debate political topics. instead dialogue manager chooses respond short news story user less pleased turn. however news story inﬂuence user follow factual questions system better adept handling. make trade-off immediate long-term user satisfaction consider selecting appropriate response sequential decision making problem. section describes approaches learn model selection policy. approaches evaluated real-world users next section. reinforcement learning framework dialogue manager agent takes actions environment order maximize rewards. time step agent observes dialogue history must choose actions taking action agent receives reward transferred next state then agent provided actions issue speciﬁc setting actions changes depending state happens candidate responses generated response models also depend dialogue history. addition response models deterministic. means candidate responses likely different every time agent encounters state contrast certain reinforcement learning problems learning play atari games actions ﬁxed given state. simplify notation number actions henceforth. action-value parametrization different approaches parametrize agent’s policy. ﬁrst approach based action-value function deﬁned parameters estimates expected return taking action given dialogue history given agent continue policy afterwards. given agent chooses action highest expected return action-value function selecting dialogue responses closely related recent work lowe noseworthy serban angelard-gontier bengio pineau model learned predict quality dialogue system response. however case conditioned dialogue context. hand model proposed lowe noseworthy serban angelard-gontier bengio pineau conditioned dialogue context human reference response. action-value function also related work learn evaluation model used train reinforcement learning agent select appropriate dialogue response strategies. stochastic policy parametrization second approach instead parameterizes policy discrete distribution actions. parameters. agent selects action sampling scoring model action-value function closely related. functions yield ranking actions; higher values imply higher expected returns. action-value function policy equivalent greedy policy simplicity parametrization therefore functions take features input process using similarity metric computed last user utterance candidate response last utterances dialogue candidate response last three user utterances dialogue candidate response last utterances dialogue candidate response stop-words removed last three user utterances dialogue candidate response stop-words removed. one-hot vector size equal number response models entry equal candidate response generated model class index part-of-speech tags candidate response estimated using maximum entropy tagger trained penn treebank corpus. sequence part-ofspeech tags mapped one-hot vector constitutes input feature. outer-product one-hot vector representing dialogue one-hot vector indicating model class non-stop-words overlap between candidate response last user utterance otherwise zero. bigram exists candidate response last user utterance otherwise zero. bigram exists candidate response last utterances dialogue context otherwise zero. named-entity exists candidate response last user utterance otherwise zero. named-entity exists candidate response last utterances dialogue context otherwise zero. candidate response consists stopwords words shorter characters otherwise zero. candidate response contains wh-word otherwise zero. last user utterance contains wh-word otherwise zero. candidate response contains intensiﬁer word otherwise zero. last user utterance contains intensiﬁer word otherwise zero. binary features candidate response contains speciﬁc word otherwise zero. candidate response contains negation word otherwise zero. candidate response contains non-stopword otherwise zero. include features based conﬁdences speech recognition system experimental reasons. speech recognition errors confounding factor experiments real-world users. speech recognition errors likely affect user satisfaction. features based speech recognition conﬁdences included policy might learn handle speech recognition errors better another policy. turn could make policy perform better w.r.t. overall user satisfaction. however would effect caused imperfect speech recognition system would reﬂect user satisfaction perfect speech recognition system. excluding features input scoring model helps minimize confounding effect.nevertheless even features excluded noted speech recognition errors still constitute substantial confounding factor later experiments. lastly reasons none response models utilize speech recognition conﬁdences. principle possible compute input features encoding dialogue context candidate response using recurrent neural networks convolutional neural networks however models known require training large corpora order achieve acceptable performance access addition need keep scoring model’s execution time otherwise slowdown response time could frustrate user lower overall user satisfaction. rules large rnns convnets amazon alexa prize competition since would require computational runtime. however future dialogue systems utilizing larger datasets consider large-scale models. section describes scoring model’s architecture. scoring model ﬁve-layered neural network. ﬁrst layer input consisting features described previous section. second layer contains hidden units computed applying linear transformation followed rectiﬁed linear activation function input layer units. third layer contains hidden units computed applying linear transformation preceding layer units. similar matrix factorization layer compresses hidden units hidden units. fourth layer contains outputs units probabilities output units computed applying linear transformation preceding layer units followed softmax transformation. layer corresponds amazon mechanical turk labels described next sub-section. ﬁfth layer ﬁnal output scalar computed applying linear transformation units third fourth layers. model illustrated figure settling architecture experimented deeper shallow models. however found deeper models shallow models performed worse. nevertheless future work explore alternative architectures. different machine learning approaches learn scoring model. described next. section describes ﬁrst approach learning scoring model based estimating action-value function using supervised learning crowdsourced labels. approach also serves initialization approaches discussed later. figure computational graph scoring model used model selection policies based action-value function stochastic policy parametrizations. model consists input layer features hidden layer hidden units hidden layer hidden units softmax layer output probabilities scalar-valued output layer. dashed arrow indicates skip connection. crowdsourcing amazon mechanical turk collect data training scoring model. follow setup similar show human evaluators dialogue along candidate responses score appropriate candidate response likert-type scale. score indicates response inappropriate make sense indicates response acceptable indicates response excellent highly appropriate. setup asks human evaluators rate overall appropriateness candidate responses. principle could choose evaluate aspects candidate responses. example could evaluate ﬂuency. however ﬂuency ratings would useful since models retrieve responses existing corpora contain mainly ﬂuent grammatically correct responses. another example could evaluate topical relevancy. however choose evaluate criteria since known difﬁcult reach high inter-annotator agreement fact well known even asking single overall rating tends produce fair agreement human evaluators disagreement annotators tends arise either dialogue context short ambiguous candidate response partially relevant acceptable. dialogues extracted interactions alexa users preliminary versions system. dialogues system priority response extracted dialogues sampled random remaining dialogues sampled random excluding identical dialogues. dialogue corresponding candidate responses created generating candidate responses response models. preprocess dialogues candidate responses masking profanities swear words stars furthermore anonymize dialogues candidate responses replacing ﬁrst names randomly selected gender-neutral names finally dialogues truncated last utterances last words. reduces cognitive load annotators. examples crowdsourcing task shown figure figure figure dialogue example shown figure ﬁctitious example. sampling random advantageous goal ensures candidate responses frequent user statements questions tend annotated turkers. increases average annotation accuracy utterances turn increases scoring model’s accuracy utterances. inspected annotations manually. observed annotators tended frequently overrate topic-independent generic responses. responses considered acceptable single turn conversation likely detrimental repeated again. particular annotators tended overrate responses generated response models alicebot elizabot vhredsubtitles bowescapeplan. responses generated models often acceptable good majority topic-independent generic sentences. therefore response models mapped labels furthermore responses consisting stop-words decreased labels level finally bowmovies response model suffered label collection period. therefore decreased labels given bowmovies responses total collected labels. split training development testing datasets consisting respectively labels each. training optimize scoring model w.r.t. log-likelihood predict layer represents label classes. formally optimize parameters input features corresponding label class model’s predicted probability given computed second last layer scoring model. ﬁrst-order gradient-descent optimizer adam experiment variety hyper-parameters select best hyper-parameter combination based log-likelihood set. ﬁrst hidden layer experiment layer sizes second hidden layer experiment layer sizes regularization model parameters except bias parameters. experiment regularization coefﬁcients unfortunately labels train last layer. therefore parameters last layer vector words assign score label poor score label poor score label acceptable score label good table scoring model evaluation amazon mechanical turk test w.r.t. pearson correlation coefﬁcient spearman’s rank correlation coefﬁcient mean squared error. table shows performance w.r.t. pearson correlation coefﬁcient spearman’s rank correlation coefﬁcient mean squared error. metrics computed linearly transforming class categories scalar output score average predictor baseline model always predicts average output score. shown supervised achieves pearson correlation coefﬁcient spearman’s rank correlation coefﬁcient signiﬁcant reduction mean squared error. indicates supervised performs signiﬁcantly better baseline. figure shows performance w.r.t. label class. addition supervised ﬁgure shows performance three baseline policies random selects response random alicebot selects alicebot response available otherwise selects response random evibot alicebot selects evibot response available otherwise selects alicebot response. policy ﬁgure shows percentage responses selected policy belonging particular label class. spectrum observe supervised point reduction compared random responses belonging \"very poor\" class. label class supervised reduction points compared alicebot evibot alicebot. spectrum observe supervised performs signiﬁcantly better three baselines w.r.t. classes \"good\" \"excellent\". particular supervised reaches responses belonging class \"excellent\". double compared three baseline policies. demonstrates supervised learned select \"good\" \"excellent\" responses avoiding \"very poor\" \"poor\" responses. overall results show supervised improves substantially baseline policies. nevertheless supervised responses belong classes \"very poor\" \"poor\". implies ample space improving supervised candidate responses ﬁrst scoring model supervised ﬁxed last output layer weights words assigned score poor responses poor responses acceptable responses it’s clear whether score correlated scores given real-world alexa users ultimately want optimize system for. section describes another approach remedies problem learning predict alexa user scores based previously recorded dialogues. learned reward function dialogue history corresponding response given system time learn linear regression model predicts corresponding return current dialogue turn model parameters. call reward model since directly models alexa user score maximize. denote observed real-valued return dialogue speciﬁcally alexa user score given dialogue it’s optional users give score; users prompted give score stopping application. although users give scores consider examples without scores. furthermore users encouraged give score range majority users give whole number scores users give decimal scores therefore treat real-valued number range learn minimizing squared error model’s prediction observed return before optimize model parameters mini-batch stochastic gradient descent select coefﬁcient smallest squared error hold-out dataset. input reward model compute features based dialogue history candidate response. training data scarce higher-level features vector indicating probability label classes candidate response computed using supervised well probability candidate response priority. candidate response priority vector zero entries except last entry corresponding priority class binary feature response contains stop-words otherwise zero. number words response square root number words response. one-hot vector indicating whether last user utterance’s dialogue request question statement contains profanity ignoring dialogues without alexa user scores introduce signiﬁcant bias reward model. particular seems likely users provide score either found system poor lack particular functions/features expected related problem arises medical statistics patients undergo treatment later outcome observed. number words response. binary feature last user utterance short contains least word indicating user confused number dialogue turns well square root logarithm number dialogue turns. total dataset training reward model dialogues. split training examples test examples. increase data efﬁciency learn ensemble model variant bagging technique create training sets shufﬂed versions original training set. shufﬂed dataset split sub-training sub-hold-out set. subhold-out sets created examples overlap sub-hold-out sets. reward model trained sub-training hyper-parameters selected sub-hold-out set. increases data efﬁciency allowing re-use sub-hold-out sets training would otherwise used. ﬁnal reward model ensemble output average underlying linear regression models. reward model obtains mean squared error spearman’s rank correlation coefﬁcient w.r.t. real alexa user test set. comparison model predicting average user score obtains mean squared error spearman’s rank correlation coefﬁcient zero. although reward model better predicting average correlation relatively low. reasons this. first amount training data small. makes difﬁcult learn relationships features alexa user scores. second alexa user scores likely high variance because inﬂuenced many different factors. score user determined single turn dialogue score user affected accuracy speech recognition module. speech recognition errors inevitably lead frustrated users. preliminary study found spearman’s rank correlation coefﬁcient speech recognition conﬁdences alexa user scores comparison correlations factors implies speech recognition performance plays important role determining user satisfaction. addition extrinsic factors likely substantial inﬂuence user scores. user scores likely depend dialogue also user’s proﬁle environment user’s expectations towards system starting conversation emotional state user training prevent overﬁtting train scoring model scratch reward model target. instead ﬁrst initialize model parameters supervised scoring model ﬁne-tune reward model outputs minimize squared error before optimize model parameters stochastic gradient descent using adam. training model depend labels training carried recorded dialogues. train several thousand recorded dialogue examples used training used hold-out set. regularization used. early stop squared error this conﬁrmed manual inspection conversation logs majority conversations several speech recognition errors. conversations excessive number speech recognition errors users’ utterances clearly showed frustration system. hold-out dataset w.r.t. alexa user scores predicted reward model. scoring model trained learned reward function call supervised learned reward. discussed earlier parametrize policy discrete probability distribution actions. parametrization allows learn policy directly recorded dialogues methods known policy gradient methods. section describes approach. off-policy reinforcement learning variant classical reinforce algorithm call off-policy reinforce. recall policy’s distribution actions parametrized softmax function applied function parameters before dialogue history dialogue time agent’s action dialogue time return dialogue number dialogues number turns dialogue further parameters stochastic policy used dialogue off-policy reinforce algorithm updates policy parameters ratio corrects discrepancy learned policy policy data collected up-weights examples high probability learned policy down-weights examples probability learned reward function. intuition behind algorithm illustrated analogy learning trial error. example high return term vector pointing direction increasing probability taking action hand example return term vector close zero vector pointing opposite direction hence decreasing probability taking action induces bias learning process also acts regularizer. reward shaping mentioned before problem off-policy reinforce algorithm presented suffers high variance algorithm uses return observed episode update policy’s action probabilities intermediate actions episode. small number examples variance gradient estimator overwhelming could easily lead agent over-estimate utility poor actions vice versa under-estimate utility good actions. remedy problem reward shaping reward time step estimated using auxiliary function purpose propose simple variant reward shaping takes account sentiment user. user responds negative sentiment assume preceding action highly inappropriate assign reward zero. given dialogue time assign reward furthermore substituting constant reward time step compute estimated number time steps episode policy. discussed later orthogonal metric based analyse evaluate policy. however estimate include number priority responses since actions agent take priority response. training initialize policy model parameters supervised train parameters w.r.t. stochastic gradient descent using adam. thousand dialogues recorded alexa users preliminary version system. examples used training used development testing. reduce risk overﬁtting train weights related second last layer using off-policy reinforce. random grid search different hyper-parameters include temperature parameter learning rate. select hyper-parameters highest expected return development set. similar supervised learned reward policy reward model training off-policy reinforce algorithm. section describes combine approaches. reward shaping learned reward model reward model compute estimate reward time step dialogue substituted training evaluation. training off-policy reinforce initialize policy model parameters supervised model train parameters w.r.t. mini-batch stochastic gradient descent using adam. dialogues split off-policy reinforce. random grid search different hyper-parameters before reduce risk overﬁtting train weights related second last layer using method. include temperature parameter learning rate select hyper-parameters highest expected return development set. case expected return computed according learned reward model. policy uses learned reward model call off-policy reinforce learned reward. approaches described advantages disadvantages. quantify differences decomposition known bias-variance trade-off. spectrum supervised policy variance trained hundreds thousands human annotations level model response. however reason supervised incurs substantial bias human annotations reﬂect real user satisfaction entire conversation. spectrum off-policy reinforce suffers high variance trained thousand dialogues corresponding user scores. make matters worse user scores affected many external factors occur granularity entire conversation. nevertheless method incurs bias directly optimizes objective metric care utilizing learned reward function supervised learned reward off-policy reinforce learned reward suffer less bias since learned reward function variance component bound higher variance. general ﬁnding optimal trade-off bias variance notoriously difﬁcult. section propose novel method trading bias variance learning policy simulations approximate markov decision process. motivation markov decision process framework modeling sequential decision making general setting model consisting discrete states discrete actions transition distribution function reward distribution function discount factor before agent aims maximize reward episode. denote time step episode length time step agent state takes action afterwards agent receives reward transitions state given model open-domain conversations dozens algorithms could apply learn agent’s policy unfortunately difﬁcult build estimate. could naively estimate recorded dialogues would require solving extremely difﬁcult problems. first would need learn transition distribution outputs next user utterance dialogue given dialogue history. problem likely difﬁcult original problem ﬁnding appropriate response user second would need learn reward distribution time step. however shown earlier difﬁcult learn predict user score entire dialogue. given data available estimating reward single turn likely also going difﬁcult. instead propose tackle problem splitting three smaller parts. figure probabilistic directed graphical model abstract discourse markov decision process. time step discrete random variable represents abstract state dialogue represents dialogue history represents action taken system represents sampled label represents sampled reward. abstract discourse markov decision process model propose learn called abstract discourse mdp. illustrated figure model follows hierarchical structure time step. time agent state discrete random variable representing abstract discourse state. variable represents high-level properties related dialogue history. deﬁne cartesian product sets. {accept reject request politics generic question personal question statement greeting goodbye other}. intention acts user’s utterance second consists sentiments types zuser sentiment {negative neutral positive}. third represent binary variable zgeneric user utterance {true false}. variable true user utterance generic topic-independent build hand-crafted deterministic classiﬁer maps dialogue history corresponding classes zdialogue zuser sentiment zgeneric user utterance. denote mapping fh→z. although consider dialogue acts sentiment generic utterances trivial expand abstract discourse state types discrete real-valued variables. given sample abstract discourse samples dialogue history ﬁnite dialogue histories particular sampled uniformly random dialogue histories last utterance mapped words dialogue history dialogue user sentiment generic property identical discrete variable purpose recorded dialogues alexa users preliminary version system. formally makes abstract discourse non-parametric model since sampling model requires access recorded dialogue histories grows time system deployed practice. useful allows continuously improve policy data becomes available. further noted small enough every possible state observed several times recorded dialogues. given sample agent chooses action according policy parameters reward sampled distribution function. case probability function parameters estimated using supervised learning labels specify reward \"very poor\" response class reward \"poor\" response class reward \"acceptable\" response class reward \"good\" response class reward \"excellent\" response class. reduce number hyperparameters expected reward instead sample variable represents appropriateness interpretation output. variable helps predict future state overall appropriateness response signiﬁcant impact user’s next utterance finally state sampled according transition distribution parameters transition distribution parametrized three independent two-layer models take input features scoring function well one-hot vector representing sampled response class one-hot vector representing dialogue last user utterance one-hot vector representing sentiment last user utterance binary variable indicating whether last user utterance generic binary variable indicating whether last user utterance contained wh-word ﬁrst predicts next dialogue second predicts next sentiment type third predicts whether next user utterance generic. dataset training mlps consists transitions used training evaluation. mlps trained maximum log-likelihood using mini-batch stochastic gradient descent. adam early-stop hold-out set. large number examples regularization used. three models obtain joint perplexity comparison baseline model always assigns average class frequency output probability obtains perplexity average means roughly possible states eliminated conditioning previous variables words previous state together agent’s action signiﬁcant effect future state zt+. means agent trained abstract discourse potential learn take account future states dialogue selecting action. contrast policies learned using supervised learning consider future dialogue states. idea modeling high-level abstraction dialogue related dialogue state tracking challenge challenge task dialogue history discrete state representing salient information dialogue. unlike dialogue state tracking challenge however variable includes limited salient information dialogue. example implementation include topical information. such partial representation dialogue history. training given abstract discourse able learn policies directly simulations. q-learning experience replay learn policy parametrized actionvalue function q-learning simple off-policy reinforcement learning algorithm shown effective training policies parametrized neural networks. experience replay memory buffer size \u0001-greedy exploration scheme experiment discount factors before parameters updated using adam. reduce risk overﬁtting train weights related ﬁnal output layer skip-connection using q-learning. training carried alternating phases. train policy episodes. then evaluate policy episodes w.r.t. average return. afterwards continue training policy another episodes. evaluation dialogue history sampled separate dialogue histories heval disjoint dialogue histories htrain used training time. ensures policy overﬁtting ﬁnite dialogue histories. hyper-parameter combination train policy episodes. select policy performs best w.r.t. average return. keep notation brief call policy q-learning amt. section carry preliminary evaluation response model selection policies. evaluation ﬁrst evaluate learned policies w.r.t. human scores test set. measure average performance real-valued scalar label \"very poor\" given score label \"poor\" given score also report standard deviations scores measure variance risk policies willing take; higher standard deviations indicate policy likely select responses result extreme labels means standard deviations report conﬁdence intervals estimated assumption scores gaussian-distributed. addition measuring performance full test also measure performance subset test neither alicebot evibot responses labeled \"good\" \"excellent\". test examples appropriate response likely come models. determining appropriate response examples likely difﬁcult. refer subset \"difﬁcult test set\". evaluate policies supervised off-policy reinforce q-learning amt. addition also evaluate heuristic policies policy selecting alicebot responses called alicebot policy selecting evibot responses possible alicebot responses otherwise called evibot alicebot. results given table results show three learned policies signiﬁcantly better w.r.t. mean score compared alicebot evibot alicebot. surprisingly difference ampliﬁed difﬁcult test set. q-learning supervised off-policy reinforce appear perform overall equally well. shows machine learning helped learn effective policies able select model responses neither alicebot evibot responses appropriate. next results show q-learning higher standard deviations policies full test difﬁcult test set. furthermore since standard deviations evaluated level single response might expect variability compound throughout entire conversation. strongly indicates q-learning risk tolerant policies. off-policy evaluation evaluate selection policies using off-policy evaluation given equation provides estimate expected alexa user score policy. described earlier equation used estimate expected number time steps episode expected alexa user score number time steps episode given table observe off-policy reinforce performs best followed q-learning supervised w.r.t. expected alexa user score. off-policy reinforce reaches major improvement second best performing model q-learning amt. however advantage taken grain salt. discussed earlier off-policy evaluation biased estimator since importance weights truncated. moreover off-policy reinforce trained speciﬁcally maximize biased estimator policies trained maximize objective functions. similarly w.r.t. expected number time steps off-policy reinforce reaches highest number time steps followed q-learning supervised amt. before take result grain salt since evaluation also biased take account priority responses. further it’s clear increasing number time steps increase user scores. nevertheless off-policy reinforce q-learning supervised appear prime candidates experiments. response model selection frequency figure shows frequency supervised off-policy reinforce q-learning select different response models. observe policy learned using off-policy reinforce tends strongly prefer alicebot responses models. alicebot responses among safest topic-dependent generic responses system suggests off-policy reinforce learned highly risk averse strategy. hand q-learning policy selects alicebot responses substantially less often off-policy reinforce supervised amt. instead q-learning tends prefer responses retrieved washington post google search results. responses semantically richer potential engage user deeply particular topic also risky suggests q-learning learned risk tolerant strategy. possible explanation difference q-learning trained using simulations. learning online simulations policy able explore actions discover high-level strategies lasting multiple time steps. particular policy allowed experiment riskier actions learn remediation fall-back strategies order handle cases risky action fails. might also explain stronger preference bowfactgenerator responses might serving fall-back strategy outputting factual statements current topic. would difﬁcult figure response model selection probabilities across response models supervised offpolicy reinforce q-learning label test dataset. conﬁdence intervals shown based wilson score interval binomial distributions. table policy evaluation using abstract discourse w.r.t. average return average reward time step average episode length reward function based supervised amt. learn off-policy reinforce since sequence actions high-level strategies sparsely observed data observed corresponding returns high variance. second observation q-learning strongest preference initiatorbot among three policies. could indicate q-learning leans towards system-initiative strategy analysis needed conﬁrm this. abstract discourse evaluation next evaluate performance policy w.r.t. simulations abstract discourse mdp. simulate episodes policy evaluate w.r.t. average return average reward time step dialogue length. addition evaluating policies described earlier also evaluate three heuristic policies policy selecting responses random called random alicebot policy evibot alicebot policy. evaluating models serve validate approximate mdp. results given table observe supervised performs best w.r.t. average return average reward time step. however comes surprise. reward function deﬁned supervised construction policy achieves highest reward time step. next observe q-learning supervised achieving average reward time step. second line comes off-policy reinforce achieving average reward time step however off-policy reinforce also achieved highest average dialogue length spectrum comes expected random policy performing worst w.r.t. metrics. comparison alicebot evibot alicebot perform better w.r.t. metrics evibot alicebot achieving best average return average reward time step three heuristic policies. validates utility abstract discourse environment training evaluating policies. overall off-policy reinforce q-learning supervised still appear best performing models preliminary evaluation. figure contingency table comparing selected response models supervised q-learning amt. cells matrix show number times supervised policy selected response model q-learning policy selected column response model. cell frequencies computed simulating episodes q-learning policy abstract discourse mdp. note models retrieving responses reddit agglomerated class reddit models. finally compare q-learning supervised w.r.t. action taken states episodes simulated abstract discourse mdp. shown figure policies diverge w.r.t. several response models. supervised would selected topic-independent generic alicebot elizabot responses q-learning often selects bowfactgenerator initiatorbot vhredwashingtonpost responses. example instances supervised selected alicebot q-learning selected bowfactgenerator. similarly supervised would preferred generic vhredsubtitle responses q-learning often selects responses bowfactgenerator initiatorbot vhredredditsports. supports previous analysis showing q-learning learned risk tolerant strategy involves response models semantically richer content. next section evaluate policies real-world users. evaluate dialogue manager policies described previous section carry testing experiments. testing experiment evaluate several policies selecting response model. alexa users start conversation system automatically assigned random policy afterwards dialogues ﬁnal scores recorded. testing allows accurately compare different dialogue manager policies keeping system factors constant contrast evaluating system performance time system continuously modiﬁed. situation often difﬁcult evaluate improvement degradation performance w.r.t. particular system modiﬁcations. however even testing experiments distribution alexa users still changes time. different types users using system depending time weekday holiday season. addition user expectations towards system change time interact socialbots competition. words must consider alexa user distribution following non-stationary stochastic process. therefore take steps reduce confounding factors correlations users. first testing experiment evaluate policies interest simultaneously. ensures approximately number users interacting policy w.r.t. time weekday. minimizes effect changes user distribution ﬁnal user scores within period. however since user distribution changes testing experiments still cannot accurately compare policy performance across testing experiments. second discard scores returning users users returning system likely inﬂuenced previous interactions system. example users previously positive experience system biased towards giving high scores next interaction. further users return system likely belong particular subpopulation users. particular group users inherently free time willing engage socialbots users. discarding returning user scores ensures evaluation biased towards subpopulation users. discarding scores returning users also ensure evaluation counts every user exactly once. finally noted ignore dialogues alexa user give score. inevitably biases evaluation since users provide score likely dissatisﬁed system expecting different functionality potential remedy dialogues evaluated third-party beyond scope experiments. ﬁrst testing experiment carried july august tested dialogue manager policies evibot alicebot supervised supervised learned reward off-policy reinforce off-policy reinforce learned reward q-learning amt. off-policy reinforce off-policy reinforce learned reward greedy variant deﬁned experiment occurred early amazon alexa prize competition. means alexa users expectations towards system further period july august overlaps summer holidays united states. means might expect children interact system seasons. policy evaluation results given table table shows average alexa user scores average dialogue length average percentage positive user utterances average percentage negative user utterances. total thousand user ratings collected discarding returning users. ratings collected semi-ﬁnals competition ratings conﬁdence intervals computed assumption alexa user scores policy drawn gaussian distribution mean variance. approximation since alexa user scores support interval transcribed human annotators. policy evaluated hundred unique alexa users. expected preliminary evaluation observe q-learning off-policy reinforce perform best among policies w.r.t. user scores. q-learning obtained average user score signiﬁcantly higher policies statistical signiﬁcance level w.r.t. one-tailed two-sample t-test. comparison average user score teams competition semi-ﬁnals interestingly off-policy reinforce achieved longest dialogues average length suggests off-policy reinforce yields highly engaging conversations. comparison semi-ﬁnals average dialogue length teams ﬁnalist teams also observe off-policy reinforce slightly higher percentage user utterances negative sentiment compared q-learning amt. potentially indicates longer dialogues also include frustrated interactions remaining policies achieved average alexa user scores heuristic policy evibot alicebot obtaining suggests policies learned select responses appropriately evibot alicebot heuristic. conclusion results indicate risk tolerant learned q-learning policy performs best among policies. shows learning policy simulations abstract discourse serve fruitful path towards developing open-domain socialbots. addition performance off-policy reinforce indicates optimizing policy directly towards alexa user scores could also potentially yield improvements. however investigation required. length analysis effort understand policies differ other carry analysis policies performance function dialogue length. although recorded limited amount data dialogues particular length analysis could help illuminate directions future experiments. table shows average alexa user scores w.r.t. four dialogue length intervals policies. estimates based alexa user ratings policy interval combination. first observe q-learning performs better policies intervals except medium-short interval further high performance long intervals would suggest q-learning performs excellent long dialogues. learned policies supervised off-policy reinforce off-policy reinforce learned reward also appear perform excellent long dialogues. hand heuristic evibot alicebot policy supervised learned reward policy appear perform poorly long dialogues surprising given overall performance. particular supervised learned reward seems performing well short dialogues. potentially indicates policy fails either maintain user engagement memorize longer-term context. however investigation required. topical speciﬁcity coherence carry analysis topical speciﬁcity coherence different policies. analysis aims quantify much policy stays topic speciﬁc content analysis carried utterance level fortunate recorded data. results shown table topic speciﬁcity measure average number noun phrases user utterance average number noun phrases system utterance. topic speciﬁc user higher would expect ﬁrst metric similarly topic speciﬁc system higher would expect second metric topic coherence measure word overlap user’s utterance system’s response well word overlap user’s utterance system’s response next turn. policy prefers stay topic higher would expect metrics shown table q-learning obtained signiﬁcantly higher scores w.r.t. word overlap metrics average number noun phrases system utterance. indicates q-learning policy highest topical coherency among policies generates topic speciﬁc responses. line previous analysis found q-learning follows highly risk tolerant strategy. next line comes supervised also appears maintain high topic speciﬁcity coherence. fact supervised obtained highest metric w.r.t. number noun phrases user utterance indicates policy encouraging user give topic speciﬁc responses. afterwards comes off-policy reinforce off-policy reinforce learned reward tend select responses signiﬁcantly less noun phrases less word overlap. also line previous analysis found off-policy reinforce follows risk averse strategy. finally heuristic policy evibot alicebot selects responses noun phrases least word overlap among policies. indicates heuristic policy might least topic coherent policy mainly selects generic topic-independent responses. initiatorbot evaluation experiment also allowed analyze outcomes different conversation starter phrases given initiatorbot. carried analysis computing average alexa user score possible phrases. found phrases related news politics travelling performed poorly across policies. hand phrases related animals movies table first testing experiment topical speciﬁcity coherence different policies. columns average number noun phrases user utterance average number noun phrases system utterance average number overlapping words user’s utterance system’s response average number overlapping words user’s utterance system’s response next turn conﬁdence intervals also shown. stop words excluded. food performed well across policies. example conversations initiatorbot asked questions related news politics average alexa user score systems mean while conversations initiatorbot asked questions animals food movies corresponding average alexa user score expected conversation topic affect user engagement however surprising particular topics preferred ones. possible explanation system perform well news politics travelling topics. however system already several response models dedicated discussing news politics sequence-to-sequence models extracting responses reddit news reddit politics models extracting responses washington post user comments bowtrump model extracting responses donald trump’s twitter proﬁle. addition evibot capable answering many factual questions news politics bowfactgenerator contains hundreds facts related news politics. such another plausible explanation users’ preferences towards topics animals movies food. likely explanation group users. inspecting conversational transcripts observed many users interacting system appeared children teenagers. would hardly come surprise user population would prefer talk animals movies foods rather news politics travels. second testing experiment carried august august tested dialogue manager policies off-policy reinforce q-learning amt. before greedy variant off-policy reinforce deﬁned experiment occurred amazon alexa prize competition semi-ﬁnals. means many alexa users already interacted socialbots competition therefore likely developed expectations towards systems. expectations likely involve conversing particular topic engaging non-conversational activities playing games). further period august august overlaps summer holidays beginning school year united states. means expect less children interact system previous testing experiment. policy evaluation results given table total eight hundred user ratings collected discarding returning users. such policy evaluated hundred unique alexa users. before ratings transcribed human annotators. observe off-policy reinforce q-learning perform better policies previous experiment. however experiment off-policy reinforce achieved average alexa user score q-learning achieved lower score nonetheless off-policy reinforce statistically signiﬁcantly better. experiment also signiﬁcant difference policies w.r.t. percentage positive negative user utterances. discussed earlier performance difference compared previous testing experiment could change user proﬁles user expectations. point time alexa users interacted socialbots teams. mean while socialbots evolving. therefore user expectations towards system likely higher now. further since summer holidays ended less children adults expected interact system. plausible adults also higher expectations towards system even likely less playful less tolerant towards mistakes. given change user proﬁles expectations risk tolerant strategy learned q-learning policy likely fare poorly compared risk averse strategy learned off-policy reinforce. third testing experiment carried august august surprising results previous testing experiment decided continue testing dialogue manager policies off-policy reinforce q-learning amt. before greedy variant off-policy reinforce deﬁned experiment occurred amazon alexa prize competition semi-ﬁnals. discussed before means likely many alexa users already developed expectations towards systems. further period august august lies entirely within beginning school year united states. means expect less children interact system previous testing experiment. policy evaluation results given table total hundred user ratings collected discarding returning users. such policy evaluated three hundred unique alexa users. unlike previous experiments semi-ﬁnals ended ratings transcribed human annotators. observe off-policy reinforce q-learning perform better policies evaluated ﬁrst experiment. however experiment off-policy reinforce achieved average alexa user score q-learning achieved higher score before neither policy statistically signiﬁcantly better other. nevertheless ﬁrst experiment q-learning achieved higher percentage positive utterances lower percentage negative utterances off-policy reinforce. experiment q-learning also obtains longest dialogues average. overall experiment indicates q-learning better policy. before difference performance compared previous testing experiments likely change user proﬁles user expectations. fact q-learning performs slightly better off-policy reinforce might explained many different causes. first despite conﬁdence intervals statistical tests presented earlier course possible previous testing experiments enough statistical power accurately discriminate whether q-learning off-policy reinforce obtains highest average user score. second possible topics users want discuss simply better handled q-learning amt. third possible adult users might weak preference toward risk averse qlearning policy still signiﬁcant amount children teenagers interacting system even though summer holidays ended. finally possible user population grown tired off-policy reinforce follows risk averse strategy responding less semantic content. dialogue manager policies q-learning off-policy reinforce demonstrated substantial improvements policies including policies learned using supervised learning heuristic policies. discussed earlier q-learning policy achieved average alexa user score substantially average score teams amazon alexa prize competition semi-ﬁnals without relying non-conversational activities. addition also achieved higher number dialogue turns average teams semi-ﬁnals average ﬁnalist teams semi-ﬁnals. policy off-policy reinforce similarly obtained high number dialogue suggesting resulting conversations engaging. results demonstrate advantages overall ensemble approach many different models generate natural language responses dialogue manager policy selects response among them. results also highlight advantages learning policy using reinforcement learning techniques. optimizing policy maximize either real-world user scores maximize rewards abstract discourse demonstrated signiﬁcant gains achieved w.r.t. real-world user scores number dialogue turns. dialogue manager architecture open-domain conversational agent utilize many different types modules modules looking information modules daily chitchat discussions modules discussing movies respect system architecture related recent general-purpose dialogue system frameworks systems abstract away individual modules black boxes sharing interface similar response models ensemble. this turn enables controlled executive component reinforcement learning much work applied reinforcement learning training improving dialogue systems. idea dialogue formulated sequential decision making problem based markov decision process appeared already goal-oriented dialogue systems line research area focused learning dialogue systems simulations using abstract dialogue states actions approaches differ based simulator created estimated whether simulator also considered agent trying optimize reward. example levin tackle problem building ﬂight booking dialogue system. estimate user simulator model counting transition probabilities dialogue states user actions used train reinforcement learning policy. setting states actions abstract discrete variables minimizes amount natural language understanding generation policy learn. another example georgila traum tackle problem learning dialogue policies negotiation games party dialogue agent reward function. setting policy effect also user simulator trained playing policies using model-free on-policy reinforcement learning. recent example build open-domain chitchat dialogue system using reinforcement learning. particular propose learn dialogue manager policy model-free off-policy reinforcement learning based simulations template-based system a.l.i.c.e. reward function learned crowdsourced annotations. shown yield substantial improvements w.r.t. overall appropriateness system response conversational depth dialogues researchers also recently started investigate learning generative neural network policies operating directing text user simulations contrast earlier work policies require deeper understanding natural language ability generate natural language. example propose train generative sequence-to-sequence neural network using maximum log-likelihood ﬁne-tune multi-objective function. multi-objective function includes among things reinforcement learning signal based self-play monte carlo rollouts using hand-crafted reward function. lewis apply model-free reinforcement learning learning system capable negotiation domain crowdsourced data. demonstrate it’s feasible learn effective policy training generative sequence-to-sequence neural network crowdsourced data policy improved using on-policy reinforcement learning self-play monte carlo rollouts. lewis self-play. self-play viable option training policies problems symmetric policy space contrast self-play unlikely effective training method case interactions highly asymmetric human users speak differently system would humans further expect different answers. lane model-free on-policy reinforcement learning improve system restaurant booking domain. training system policy employ user simulator trained real-world human-human dialogues. particular constraint system user share exact reward function demonstrate reinforcement learning used improve system policy user simulator. related vein zhao eskenazi learn end-to-end neural network system playing quiz game using off-policy reinforcement learning environment game simulator. demonstrate combining reinforcement learning dialogue state tracking labels yields superior performance. work reviewed user simulators deﬁned rule-based models parametric models combination two. cases given user simulator collected training data discarded policy learned directly simulations user simulator. contrast abstract discourse propose non-parametric approach repeatedly uses collected training data policy training. reinforcement learning also applied teaching agents communicate multi-agent environments important direction future research personalization i.e. building model user’s personality opinions interests. allow system provide better user experience adapting response models known attributes user. process implementing state machine given user retrieves relevant information attributes user database. particular user attribute missing state machine user relevant information store database. important user attribute user’s name. name found database state machine user would like called afterwards extracts name user’s response. personal name detected stored database available modules insert responses. name detection proceeds follows. first match response small collection templates name ...\" \"call ...\". part-of-speech tags resulting matches detect boundary name. avoid clipping name early wrong tags also match words list common names census data. future plan explore learning user embeddings previous interactions user since know previous experiments text information alone contains signiﬁcant amount information speaker’s identity learning embedding user allow system become personalized providing response models additional context beyond immediate dialogue history. well known speech recognition errors signiﬁcant impact user experience dialogue systems furthermore speech recognition errors likely particularly averse effect system system encourages open-ended unrestricted conversations. unlike many goal-driven rule-based systems system take control dialogue direct user respond keyword canned responses. users likely give open-ended responses system also likely suffer speech recognition errors. discussed section indeed observe negative correlation conﬁdences speech recognition system alexa user scores. moreover likely speech recognition errors stronger systematic effect policies evaluated section mitigate issues speech recognition errors plan evaluate system different policies text-based evaluation amazon mechanical turk. would also help reduce problems errors incorrect turn-taking proposed large-scale ensemble-based dialogue system framework amazon alexa prize competition. system leverages variety machine learning techniques including deep learning reinforcement learning. developed deep learning models natural language retrieval generation including recurrent neural networks sequence-to-sequence models latent variable models. addition developed novel reinforcement learning procedure evaluated existing reinforcement learning methods testing experiments real-world users. innovations enabled make substantial improvements upon baseline system. scale best performing system reached average user score minimal amount hand-crafted states rules without engaging non-conversational activities performance substantially average teams competition semi-ﬁnals furthermore system averaged high turns conversation substantially average teams average ﬁnalist teams competition semi-ﬁnals suggesting system engaging systems competition. since nearly system components trainable machine learning models system likely improve greatly interactions additional data. thank aaron courville michael noseworthy nicolas angelard-gontier ryan lowe prasanna parthasarathi peter henderson helpful advice related system architecture crowdsourcing reinforcement learning throughout alexa prize competition. thank christian droulers building graphical user interface text-based chat. thank amazon providing tesla gpus amazon services platform. titan gpus used research obtained from https//deron.meranda.us/data/. contrast socialbot system alexa semi-ﬁnals would start conversation asking user question able talk news sports politics. would like talk about?\" user expected mention keywords \"news\" \"sports\" \"politics\". type system-initiative greatly reduces number speech recognition errors easier discriminate keywords compared transcribing complete open-ended utterance. donated nvidia corporation. authors acknowledge nserc canada research chairs cifar research nuance foundation microsoft maluuba druide informatique inc. funding.", "year": 2017}