{"title": "A Forward-Backward Approach for Visualizing Information Flow in Deep  Networks", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "We introduce a new, systematic framework for visualizing information flow in deep networks. Specifically, given any trained deep convolutional network model and a given test image, our method produces a compact support in the image domain that corresponds to a (high-resolution) feature that contributes to the given explanation. Our method is both computationally efficient as well as numerically robust. We present several preliminary numerical results that support the benefits of our framework over existing methods.", "text": "introduce systematic framework visualizing information deep networks. speciﬁcally given trained deep convolutional network model given test image method produces compact support image domain corresponds feature contributes given explanation. method computationally efﬁcient well numerically robust. present several preliminary numerical results support beneﬁts framework existing methods. deep neural networks resulted widespread compelling advances variety machine learning tasks object recognition image segmentation anomaly detection machine translation synthesis. however advances often accompanied signiﬁcant reduction interpretability ability visualize information extracted various layers abstraction. contrast traditional rule-based learning methods deep networks often produce decisions seemingly hard decipher justify given test data sample even though aggregate generalizability measured respect hold-out test dataset excellent. issue unpacking black-box nature deep networks identiﬁed issue several recent works springenberg selvaraju shrikumar sundararajan work focus task object detection images. broadly algorithms interpreting action deep networks task grouped follows class-discriminative approaches class activation mappings zhou gradient-based variant selvaraju produce support original image domain approximately corresponds given object class detected image. however methods coarse produce low-resolution visualizations cannot directly applied high resolution images. hand pixel-space gradient-based methods deconvolution networks zeiler fergus guided back-propagation springenberg produce ﬁne-grained features given image. however gradient based methods suffer either signiﬁcant computational efﬁciency concerns susceptible saturation phenomena vanishing/exploding gradients both. shrikumar issue alleviated suitably using second reference input stabilize estimates. however choosing reference image qualitative challenging. finally model-agnostic approaches lime ribeiro theoretically sound applied interpreting deep convolution networks involve solving challenging optimization problems. figure overview forward-backward scheme visualizing information ﬂow. forward information model combined backward information throughout network. short paper outline systematic framework visualizing information deep convolutional networks resolves computational efﬁciency well numerical robustness issues described above. present several preliminary numerical results support beneﬁts framework existing methods. high level approach based novel forward-backward scheme operates follows. consider trained deep convolutional network model given test image model able identify existence given target class. then algorithm produces output support corresponding class predicted model manner similar pixel-space gradient methods. however contrast gradient-based approaches algorithm leverages backward information output layer input also forward information extracted various layers abstraction. figure speciﬁcally method following distinguishing characteristics propose mathematically principled approach achieve backward information within deep convolutional network leveraging ideas proposed deconvolutional networks approach zeiler fergus however approach computationally expensive since requires solving sparse recovery problem layer network limits depth network method applicable. hand approach needs simple application matrix adjoints nonlinearities convolutional layer easily implementable deep networks. propose systematic using forward information guide backward-traversal. particular forward information extract support within given layer representation best corresponds speciﬁc feature map. achieve using novel masking scheme transparently combines forward backward information ﬂows network. opposed gradient-based schemes springenberg aggregate information feature maps algorithm produces binary support estimates layer layer. sense method avoids numerical stability robustness issues arise well-known problem exploding/vanishing gradients potentially affect interpretability. particular contrast shrikumar remove need separate reference image method involves making passes network given image. describe scheme visualizing convolutional neural network. term method forward-backward interpretability given test image goal identify important regions explain prediction learned network. this propagate class-probed information back image pixel space complete network using guidance learned model weights well forward activations neuron network. approach shares several similarities deconvolutional networks approach introduced zeiler fergus however instead reconstructing lower layer feature maps higher layer activations deconvnet merely identify important regions preserved forward activations layer backward information ﬂow. suppose already trained network optimal state. forward pass image presented network activations entire network computed. explain classiﬁcation consider class indicator vector predicted class zero otherwise back-propagate information input space. input backward pass approximately invert layer iteratively ﬁltering inverses using forward activations. process repeated input layer reached. dense layers. fully-connected layer denote activation softmax activation achieved ﬁnal layer. goal traverse layers backwards. order achieve this deﬁne adjoint operation follows. adjoint softmax layer deﬁned point-wise adjoint relu activation function relu itself. overall adjoint fully connected layer foward-backward masking. backward information ﬁltered using forward activations. speciﬁcally keep entries entry-wise product respective entries threshold parameter entries zero otherwise. enables identify candidate support corresponding interpretable feature input given layer. contributing feature maps. among many backward feature maps convolutional layer keep backward information ﬂow. contribution determined total activation entire map. hence features irrelevant probed class removed. unpooling. perform adjoint pooling layer reshape obtained pooled backward pass replicate values across domain operator. then evaluate entry-wise similar analogous unpooling operation deconvnet; however approach copies value single location switches stored memory. contrast scheme allows backward feature maps unpooling overly sparse retains enough spatial information interpretation. note replication step suitable downsampling ﬁlter size stride ﬁlter sizes stride lengths replicated values averaged overlapping locations. deconvolution. deconvolution step similar deconvnet compute adjoint convolving backward activation ﬂipped ﬁlter weights corresponding ﬁlter. traverse backwards network using operations successively retain subset pixel indices input layer plausibly corresponds interpretable portion given image. display locations indices together values produced adjoint. selectivity achieved successive masking means always obtain fairly sparse support ﬁnal estimate; sparsity controlled appropriate choice threshold parameter visualize interpretations provided proposed algorithm table vgg- model pretrained imagenet dataset chollet visualizations generated predictions image. choice ﬁlters computing inverse important factor contributing interpretation obtained. value less interpretations obtained looses important features high interpretations noisy. found visualizations obtained using ﬁlters pointwise thresholding based forward function value produces output close deconvnet algorithm. thus using forward function masking inverse computed explained above achieve better output deconvnet well guided backpropagation algorithms. experiments shown table ﬁlters propagate inverse layer. also thresholding value pointwise mask forward backward values. compare method guided backpropagation algorithm. notice resulting visualizations lesser noise compared guided backpropagation algorithm. work introduce novel forward-backward approach visualizing interpretations correspond particular class. however choice ﬁlters computing interpretations become hyper-parameter ensure interpretations good. also seen ﬁlters contribute particular class might similar activations activations pertaining class. hence decoupling activations ﬁlters ramprasaath selvaraju michael cogswell abhishek ramakrishna vedantam devi parikh dhruv batra. grad-cam visual explanations deep networks gradient-based localization. https//arxiv. org/abs/. bolei zhou aditya khosla agata lapedriza aude oliva antonio torralba. learning deep features discriminative localization. proceedings ieee conference computer vision pattern recognition pages marco tulio ribeiro sameer singh carlos guestrin. trust you? explaining predictions classiﬁer. proceedings sigkdd international conference knowledge discovery data mining pages", "year": 2017}