{"title": "Tree Memory Networks for Modelling Long-term Temporal Dependencies", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "In the domain of sequence modelling, Recurrent Neural Networks (RNN) have been capable of achieving impressive results in a variety of application areas including visual question answering, part-of-speech tagging and machine translation. However this success in modelling short term dependencies has not successfully transitioned to application areas such as trajectory prediction, which require capturing both short term and long term relationships. In this paper, we propose a Tree Memory Network (TMN) for modelling long term and short term relationships in sequence-to-sequence mapping problems. The proposed network architecture is composed of an input module, controller and a memory module. In contrast to related literature, which models the memory as a sequence of historical states, we model the memory as a recursive tree structure. This structure more effectively captures temporal dependencies across both short term and long term sequences using its hierarchical structure. We demonstrate the effectiveness and flexibility of the proposed TMN in two practical problems, aircraft trajectory modelling and pedestrian trajectory modelling in a surveillance setting, and in both cases we outperform the current state-of-the-art. Furthermore, we perform an in depth analysis on the evolution of the memory module content over time and provide visual evidence on how the proposed TMN is able to map both long term and short term relationships efficiently via a hierarchical structure.", "text": "tharindu fernando image video research laboratory saivt queensland university technology australia. simon denman image video research laboratory saivt queensland university technology australia. aaron mcfadyen robotics autonomous systems queensland university technology australia. sridha sridharan image video research laboratory saivt queensland university technology australia. clinton fookes image video research laboratory saivt queensland university technology australia. domain sequence modelling recurrent neural networks capable achieving impressive results variety application areas including visual question answering part-of-speech tagging machine translation. however success modelling short term dependencies successfully transitioned application areas trajectory prediction require capturing short term long term relationships. paper propose tree memory network modelling long term short term relationships sequence-to-sequence mapping problems. proposed network architecture composed input module controller memory module. contrast related literature models memory sequence historical states model memory recursive tree structure. structure effectively captures temporal dependencies across short term long term sequences using hierarchical structure. demonstrate effectiveness ﬂexibility proposed practical problems aircraft trajectory modelling pedestrian trajectory modelling surveillance setting cases outperform current state-of-the-art. furthermore perform depth analysis evolution memory module content time provide visual evidence proposed able long term short term relationships efﬁciently hierarchisequence-to-sequence modelling vital element machine learning knowledge representation multiple application areas including machine translation trajectory prediction part-of-speech tagging problem represented predicting output sequence given input sequence predicting future element sequence time instance utilising current input model content memory previous time step represented modelling long term relationships within sequences considered challenging problems within machine learning community although many memory architectures proposed sequence-to-sequence modelling capable mapping short term relationships less successful handling long term dependencies long term relationships within data extremely useful signiﬁcantly inﬂuence accuracy predictions considering repetitive nature many processes. instance consider trafﬁc modelling problem. even though short term dependencies current weather neighbouring trafﬁc inﬂuential factors repetitive nature aircraft trajectories ﬂight schedules suggests easily deduce coherence among trajectories days seasons. certain runway similar trajectories observed short term sudden change runway means trajectories seen. long term memory means actually trajectories instead recalled earlier similar weather event operational conﬁguration. similar logic applied modelling pedestrian behaviour surveillance scenario example shown fig. current location neighbouring pedestrians inﬂuential cannot discard inﬂuence historical behaviour similar contexts events. pedestrians wandering free area trains arrive pedestrian movement appears response congestion. posses historical data similar contexts capable accurately anticipating pedestrian motion. paper interested efﬁciently aggregating long term dependencies among input data. sample scenario grand central dataset presented fig. high pedestrian highlighted area observed time stamp arrival train decreases time stamp. similar fig. observed time stamp fig. conﬁrming hypothesis future pedestrian anticipated historical data. figure pedestrian different times grand central dataset high pedestrian highlighted area observed time stamp arrival train decreases time stamp. similar subﬁgure observed time stamp conﬁrming hypothesis future pedestrian anticipated historical data. application proposed memory architecture practical problems aircraft trajectory modelling pedestrian trajectory modelling surveillance setting cases able achieve state-of-the-art results. applications demonstrate proposed approach aircraft trajectory prediction pedestrian trajectory prediction related sequence-to-sequence modelling tasks however quite distinct characteristics illustrate adaptability proposed approach. aircraft trajectories primarily function ﬂight schedule typically ﬁxed week still varies according changes weather schedule arrivals/departures etc. pedestrian trajectories however less dependent schedule inﬂuenced behaviour nearby pedestrians. would like emphasise fact even though demonstrating approach different application scenarios trajectory prediction domain varied nature problems demonstrates proposed model directly applied sequence-tosequence prediction problem modelling long term relationships necessary. possible application areas include diver behaviour modelling autonomous driving text video synthesis context aware machine translation related work within scope paper categorised memory architectures aircraft trajectory prediction approaches pedestrian trajectory prediction approaches figure neural network architecture external memory. memory used store important historical facts utilised future predictions. controller responsible issuing read write commands order take write back memory. input module used encode generate vector representation input deep learning models recurrent neural networks applied extensively many sequence-to-sequence modelling problems capable producing state-of-theart results. number approaches kumar malinowski fritz joulin mikolov also utilised termed memory modules prediction. memory stores important facts historical inputs generates future predictions based stored knowledge. sample architecture input module controller external memory shown fig. firstly input module generates vector representation input time instance controller triggers memory read operation. memory module attention process searches history outputs relevant facts. ﬁnal output generated merging memory output. finally controller triggers memory update operation memory updated weston utilised memory module improve accuracy natural language processing problems. proposed memory architecture fully extendible considering usage ofﬂine feature engineering process using bag-of-words approach. similar works joulin mikolov kaiser sutskever image caption generation malinowski fritz chen zitnick visual question answering authors extensively applied notion external memory. memory architecture episodic memory proposed kumar shown capable outperforming external memory architectures proposed terms accuracy figure comparison memory model proposed kumar proposed memory model approaches time instance input module generates representation input controller triggers memory read operation. memory module attention process output relevant facts. ﬁnal output given merging memory output. finally memory update operation updates memory fig. depicts episodic memory model proposed kumar authors model episodic memory hierarchical recurrent sequence model utilising sequential nature memory. authors propose generalised neural sequential module recurrent lstm memory cells sequence encoding memory mechanism response generation. work extended shared memory architecture neelakantan even exemplary results short term dependency modelling problems none stated architectures capable handling sequences long term relationships. score value quantiﬁes relevance content memory module time current context munkhdalai authors completely update content memory locations based main drawback using approaches neelakantan munkhdalai long term dependency modelling data sets that cases attention mechanism generate small scores examples dissimilar input. therefore approaches required maintain extremely large memory sequence similar inputs occur long time intervals. furthermore recurrent models lstms sequence becomes long output becomes biased towards recent historical states rather considering entire historical states equally. hypothetically memory module enough non-linearity sufﬁciently large database output able produced components reside memory. furthermore model capable learning modelling short term contextual effects well long term contextual effects input memory. seen dictionary learning process memory dictionary learning memory module learn produce accurate prediction different combinations inputs historical trajectories. recently recursive lstm structure proposed authors extend sequential lstm tree structures memory cell reﬂect historical memories multiple child cells multiple descendant cells recursive process. chen performed depth analysis strengths weaknesses sequential recursive structures neural language modelling task; found syntactical relationships structure logic input data best captured recursive lstm structure encoding semantics sentences sequential lstm models provide state-of-the-art results. proposed model illustrated approach eschews sequence representation favour tree-based approach motivated positive characteristics s-lstm architecture exhibits feature compression power preservation semantic relationships among data. detailed analysis architecture comparison state-of-the-art methods provided section approaches utilise probability models aircraft dynamics order generate predictions future motion. purely rely assumptions made dynamics aircraft without using historical information contributes main drawback. choi hebert leege winder researchers treated aircraft trajectory prediction machine learning problem train model using historical trajectory data together weather observations. recently ayhan samet proposed approach considered trajectories dimensional data cubes together weather parameters. initially performed time series clustering data segmentation learnt cluster. still uncertainty weather observations trajectory prediction approaches become inefﬁcient. several efforts made improve trajectory prediction better wind estimation cole rekkas delahaye hollister approaches failed achieve signiﬁcant improvement task trajectory prediction. furthermore would like emphasise fact stated approaches consider aircrafts individually without considering trafﬁc within neighbourhood completely discarding important factors volume proximity nearby trafﬁc. even though weather vital factor future predictions implicit neighbouring trafﬁc behaviour. therefore inferring notion weather neighbouring trafﬁc computationally inexpensive compared tedious interpolations ground based weather observations require considering literature human behaviour prediction social force model koppula saxena pellegrini yamaguchi variants considered well-established. approaches generate attractive repulsive forces pedestrians deﬁning optimal path different contexts respect neighbourhood. variant mixture model presented zhou approach ignores interactions among pedestrians. wang proposed topic model extended incorporate spatio-temporal dependencies hospedales emonet stated approaches hand-engineered features input prediction module considered main drawback fail account semantics scene. depending domain knowledge feature engineer handcrafted features capture abstract level semantics environment. alahi removed need hand-crafted features unsupervised feature learning approach. authors encode trajectory pedestrian scene particular time frame using lstms. hidden states neighbouring pedestrians immediately preceding time step used generating positions current time step. pointed fernando approach sufﬁcient generating reactive behaviours collision avoidance fails generate smooth trajectories long term trajectory planning. fernando extended idea incorporate entire trajectory pedestrian interest well neighbouring pedestrians. best knowledge none literature addressing human behaviour prediction considered long-term relationships among human behavioural patterns. motivated limitation intend explore utility temporal data trajectory prediction tree memory network. work motivated exemplary results achieved tree structure task discriminative dictionary learning trajectories fernando contrast mapping historic data shallow layer recurrent memory cells hierarchically memory bottom tree structure historical states represented bottom layer tree hierarchy concatenating signiﬁcant features order generate output particular time step. furthermore rather stacking individual recurrent layers utilise tree-lstm structure focusses historical information neighbours. therefore seen passing signiﬁcant features temporally adjacent neighbours upper layer. memory module consider rp×k sequence historical lstm embeddings length embedding dimension would like model memory. seen queue structure length data element within queue dimension adapt s-lstm model represent memory module proposed framework. extends general sequential structure lstms bottom tree structure composed compressed representation children nodes parent node. provides principled considering long-distance interactions memory inputs avoids current drawbacks lstm models handling lengthy sequences order preserve simplicity represent recursive-lstm structure binary tree parent node maximum child nodes; however extension model tree structure straightforward. computing output time instance extract tree conﬁguration time instance memory matrix resultant concatenating nodes tree depth. allow capture different levels abstraction exists memory network. score attention scoring function implemented multi-layer perceptron proposed memory architecture memory cell contains input gate output gate forget gates time instance node memory network updated following manner hidden vector representations cell states left right children respectively. relevant weight vectors represented appropriate super subscripts superscript represents relevant child node subscript represents relevant gate vector weight attached process also illustrated major difference proposed approach current state methods kumar neelakantan munkhdalai representation memory. approaches memory composed single layer memory units. memory update mechanism kumar neelakantan written matrix ones vectors ones. denotes outer product duplicates left vector times form matrix. contrast model memory binary tree structure updated bottom fashion utilising main drawback using approaches neelakantan munkhdalai long term dependency modelling data sets that cases attention mechanism generate small score values examples dissimilar input. therefore approaches required maintain extremely large memory sequence similar inputs occur long time intervals. furthermore recurrent models lstms sequence becomes long output becomes biased towards recent historic states rather considering entire historic states equally. contrast represent memory tree structure learn logical coherence among neighbouring memory cells different levels abstraction. present experimental results trajectory datasets aircraft trajectory database south east queensland region australia; widely utilised pedestrian trajectory database. datasets speciﬁcally chosen demonstrate capability proposed model handle varying dimensionalities temporal relationships present real world applicability. obtain trafﬁc data south east queensland region australia real position reports recorded australian advanced trafﬁc system used current trafﬁc management australia pre-processing step data transformed trajectories utilising aircraft identiﬁcation tags reported timing. trajectories less position reports removed short trajectory modelling task. ﬁnal step ﬂight trajectory resampled provide equally spaced data points. gives trajectories. aircraft input sequence trajectories represented dimensional data streams point trajectories chronologically. remaining trajectories used testing. based recommendations provided ayhan samet paglione oaks measure following three error metrics aircraft trajectory prediction experiment. trajectory prediction errors calculated observed radar track point input trajectory segment. number trajectories testing interested predicting trajectory time period pred observing trajectory aircraft obs. predicted course north trajectory time instance denoted baseline models implemented model proposed ayhan samet dynamic memory networks approach given kumar model required weather data obtained data services provided australian bureau meteorology observed wind speed direction temperature minute frequency. data interpolation parameter quantisation performed ayhan samet hyper parameters length memory module embedding dimension proposed memory module evaluated experimentally. fig. shows variation average altitude error modules solid dashed green lines respectively. proposed error converges around value plot shows error decreases ﬁrst starts increasing length memory exceeds hidden units. veriﬁes assertion naive memory models sequential lstm architectures fail model long term dependencies. gives lowest altitude error model value similar experiment evaluated optimal embedding dimension fig. shows variation average altitude error modules. modules produces smallest altitude error embedding dimension units. finally evaluated number levels memory tree memory read proposed memory module evaluation results shown fig. suggest produces optimal results. observed error reduced increasing number levels memory read operation exceeds levels. density extracted memory activation. using tree structure memory capture information hierarchical manner vital information bottom layers passed layer. therefore extracted matrix becomes dense decoding function fails discriminate vital facts performance starts degrade. table quantitative results aircraft trajectory prediction. methods forecast trajectories length frames. ﬁrst reports along track error second shows cross track error ﬁnal shows altitude error reference results table illustrate capability proposed model infer different modes trafﬁc behaviour. would like emphasise without explicit weather modelling neighbourhood modelling proposed augmented memory architecture capable learning salient aspects long term dependencies necessary modelling aircraft trajectories. another prominent factor accuracy improvement model model illustrates memory architectures kumar fail capture long term dependencies; proposed hierarchical memory architecture able successfully learn long term relationships. section perform depth analysis hidden state activations memory module illustrates proposed multi-layer architecture generates future trajectories encoding necessary information history. fig. show prediction results model model model aircraft trajectory dataset. noted model generates better predictions even highly varied nature database instance rows demonstrate model adapts takeoff landing scenarios. last fig. show failure cases. reason deviations ground truth mostly sudden turns movements. even though trajectories match ground truth proposed method still outperforms current state-of-the-art methods generates much realistic trajectories. order verify capability proposed model understand capture effect weather aircraft trajectory prediction historical trajectories conducted separate experiment model used predict trafﬁc stormy day. severe storms affected south east queensland november tested model trajectories november november allows sufﬁcient number examples initialise memory module. noted examples used training model. compare proposed model model explicitly weather information. table quantitative results aircraft trajectory prediction stormy conditions. methods forecast trajectories length frames. ﬁrst reports along track error second shows cross track error ﬁnal shows altitude error comparing table. table. though accuracy predictions slightly reduced predictions still accurate compared ayhan samet error increased dramatically indicating baseline model adapt well changing weather conditions. referring results presented fig. evident uniform nature trafﬁc stormy weather conditions well captured proposed approach. even accurate weather information model fails properly exploit data generates erroneous trajectories. model anticipates uneven nature looking recent history also mapping correlated trajectories behaved considering long term history. experiment considered months worth trajectories edinburgh informatics forum database trained model trajectories tested trajectories. models observed trajectory frames predicted trajectory next frames. experiment trajectories represented dimensional data point comparison implemented soft+hard wired attention model fernando social lstm model given alahi dynamic memory networks given kumar so-lstm model local neighbourhood size considered hyper-parameters according alahi fernando considered neighbourhood size left right front directions embedding size hidden units. models experimental settings given previous experiment. similar alahi report prediction accuracy following error metrics. predicting trajectory period pred observing trajectory number trajectories testing predicted position trajectory time instance shown table proposed model outperforms sh-atn model so-lstm model three error metrics. dataset considered quite challenging multiple source sink positions different crowd motion patterns present motion paths heavily crowded. without explicitly modelling neighbourhood motion sh-atn model so-lstm model approach still outperforming current state-of-the-art models. fig. show prediction results sh-atn model model model trajectory dataset. examples shown evident different modes human motion well captured represented proposed augmented memory module. instance fig. fig. pedestrians exhibit sudden change motion baseline models fail capture. proposed model successfully anticipated motion historical behaviour. table quantitative results pedestrian trajectory prediction. methods forecast trajectories length frames. ﬁrst represents average displacement error second shows ﬁnal displacement error third shows average non-linear displacement error section explore elements memory module activated different conditions. particular show baseline model kumar ﬁrst layer proposed approach activations based recent inputs model; last layer proposed approach activations driven input rather recent history it’s ability better capture long term dependencies. conducted investigation pedestrian dataset simpler visualise. section analyse memory activation various test cases experiment randomly selected memory cell ﬁrst layer memory analysed temporal evaluation activations hidden states particular memory cell. memory cell hidden state dimension every input sequence length time steps. smaller hidden state dimension used compared previous experiments ensure clear visibility memory activations. fig. shows results analysis process. searched test common activation patterns. ﬁrst column fig. shows memory activations highly correlated memory patterns shown green blue; remainder activations shown grey. second column shows input model predicted sequence previous trajectories memory shown third column. black white colour coded recent trajectory oldest trajectory. expect ﬁrst layer memory module generate similar activation patterns exists similar historical trajectories exists similarity historical trajectories input. fig. illustrates activations ﬁnal memory cell column descriptions identical fig. second third columns fig. provide visual evidence proposed memory module successfully learnt relationships among input trajectories. memory module enough capacity capturing long term dependencies ﬁnal layer generate similar activations similar input trajectory patterns rather completely reliant current short term context. evident similarity shown fig. observe similar activations similar inputs patterns importantly fig. visualise correlated activations baseline memory model shown fig. column labels identical fig. compare activations fig. fig. observing column evident hidden state activations dominated recent inputs memory long term dependencies little importance. noted inherent problem sequential lstm architectures therefore regardless input model memory module generating similar activations considering short term context. hence prediction error high. furthermore would like highlight similarity ﬁrst layer proposed memory module illustrate limitations baseline model clustered input trajectories particular cluster extracted highest correlated input trajectories within cluster. even though expect baseline memory module generate similar activations inputs prediction module similar generates vastly different activations. order highlight differences among memory activations randomly selected hidden units within memory illustrated temporal evaluations colour rest activations shown grey. short term history within base line memory module signiﬁcantly different example another memory module generates extremely different activations completely discarding input given. furthermore comparing fig. fig. baseline memory module generates similar activations short term history similar vastly different activations short term history different input prediction module long term dependencies little importance. proposed tree memory network model generalised architecture modelling long term short term relationships applied directly sequence-to-sequence mapping task. evaluation results demonstrated proposed memory architecture able outperform considered baselines provide visual evidence power able capture long term short term relationships efﬁcient tree structure. demonstrated approach different application scenarios trajectory prediction domain; however varied nature problems demonstrates proposed model directly applied sequence-to-sequence prediction problem modelling long term relationships necessary. future work exploring applications encoding mechanism large scale multimodal inputs videos encoded vector representation memory utilised generate sparse representation entire video sequence temporal relationships.", "year": 2017}