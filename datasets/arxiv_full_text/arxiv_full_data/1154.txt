{"title": "Filling in the details: Perceiving from low fidelity images", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Humans perceive their surroundings in great detail even though most of our visual field is reduced to low-fidelity color-deprived (e.g. dichromatic) input by the retina. In contrast, most deep learning architectures are computationally wasteful in that they consider every part of the input when performing an image processing task. Yet, the human visual system is able to perform visual reasoning despite having only a small fovea of high visual acuity. With this in mind, we wish to understand the extent to which connectionist architectures are able to learn from and reason with low acuity, distorted inputs. Specifically, we train autoencoders to generate full-detail images from low-detail \"foveations\" of those images and then measure their ability to reconstruct the full-detail images from the foveated versions. By varying the type of foveation, we can study how well the architectures can cope with various types of distortion. We find that the autoencoder compensates for lower detail by learning increasingly global feature functions. In many cases, the learnt features are suitable for reconstructing the original full-detail image. For example, we find that the networks accurately perceive color in the periphery, even when 75\\% of the input is achromatic.", "text": "humans perceive surroundings great detail even though visual ﬁeld reduced low-ﬁdelity color-deprived input retina. contrast deep learning architectures computationally wasteful consider every part input performing image processing task. human visual system able perform visual reasoning despite small fovea high visual acuity. mind wish understand extent connectionist architectures able learn reason acuity distorted inputs. speciﬁcally train autoencoders generate full-detail images low-detail foveations images measure ability reconstruct full-detail images foveated versions. varying type foveation study well architectures cope various types distortion. autoencoder compensates lower detail learning increasingly global feature functions. many cases learnt features suitable reconstructing original fulldetail image. example networks accurately perceive color periphery even input achromatic. success machine learning algorithms depends heavily upon representation input data. major appeal deep learning current dominant approaches machine vision tasks based automatically learn useful feature representations data. criticism deep architectures wastefully process every input component performing task; example input layer considers pixels every region input learning image classiﬁer making classiﬁcation decisions. contrast human visual system small fovea high resolution chromatic input allowing judiciously budget computational resources order receive additional information ﬁeld view make either covert overt shifts attention. overt shifts attention eye-movements allow bring fovea particular locations environment relevant current behavior. avoid serial nature processing demanded overt shifts attention visual system also engage covert shifts attention eyes remain ﬁxated location attention deployed different location. human retina receives million bits second exceeds computational resources available visual system assimilate given time even though perceive environment around great detail small fraction information registered visual system processed. paper asks simple question high detail input available would artiﬁcial neural networks still able capture aspects underlying distribution? question perspective fovea takes entire retina solely responsible sharp central full color vision maximum acuity. relative visual acuity diminishes rapidly eccentricity fovea result visual performance best fovea progressively worse towards periphery indeed visual cortex receiving distorted color-deprived visual input except central degrees visual ﬁeld seen figure despite receiving distorted signal perceive world color high resolution mostly unaware distortion. even confronted actual blurry distorted visual input visual system good extracting scene contents context. instance system recognize faces emotions expressed faces resolutions pixels reliably extract contents scene gist image even resolutions recently ullman shown visual system capable recognizing contents images critical feature conﬁgurations current deep learning systems cannot utilize similar tasks. mircs resemble foveations image results reveal human visual system employs features processes used current deep networks. similarly little attention given deep learning community networks deal distorted noisy inputs. draw inspiration abilities human visual system whether artiﬁcial neural network learn perceive image ﬁdelity input. case design state architectures image super resolution automatic image coloring image compression time reduce computational costs processing entire images associated deep networks. revival applying idea attention deep learning architectures work exciting lead improvements tasks ranging machine translation image captioning however many approaches—especially employ soft attention mechanism—the computational cost increased. example generating target sentence network must compute softmax every word source sentence location source image. unlike systems humans perceive sequentially directing attention relevant portions data turn enables visual system reduce computational costs paper want understand kind information gleaned low-ﬁdelity inputs. gleaned single foveal glimpse? predictive region image? present framework studying questions based generative model known autoencoder. contrast traditional de-noising autoencoders attempt reconstruct original image autoencoders even relatively simple dfae architectures able perceive color shape contrast information fail recover high-frequency information confronted extremely impoverished input. interestingly amount detail present input diminishes structure learnt features becomes increasingly global. autoencoders class unsupervised algorithms pairs bottom-up recognition network top-down generative network encoder denoted function forms compressed representation input feature vector representation code computed context work interested whether learn rich representation ﬁdelity input image output denoted function maps feature vector back input space producing reconstruction minimization reconstruction error function. good generalization means reconstruction error test examples close reconstruction error training examples. capture structure underlying data distribution prevent autoencoder learning identity function either require hidden layer lower dimensionality input regularize weights lower dimension constraint classical autoencoder higher dimension used sparse autoencoders recently denoising autoencoders shown regularize network adding noise salt-and-pepper noise input thus forcing model learn predict true image noisy representation training example. minimization carried standard gradient descent algorithms like backpropagation. commonly used forms encoder afﬁne mapping followed linearity appropriately sized parameters mentioned above shown features learnt encoder without nonlinearity subspace principal components input space however nonlinear activation sigmoid used encoder learn powerful feature detectors simple architecture simple hidden layer similar multilayer perceptron difference mlps lies output layer predicts class input whereas reconstructs start reviewing related work using distorted inputs train deep networks move describe architecture used test feature extraction downsampled images. image input. therefore denoising autoencoder learnt reconstruct clean input corrupted version. shown introducing noise input lowers classiﬁcation error benchmark classiﬁcation problems. ﬁlters produced denoising tend capture distinctive blob-like features higher level corruption input image learn less localized ﬁlters. fact bishop argued linear system training noise similar effect training regularizer weight decay. another proposal make autoencoders noise invariant rifal improved daes adding penalty term called contraction ratio learnt mapping makes features learnt robust invariant change input. spirit denoising incorporate form noise input image. however unlike noise noise generated using foveation function image. investigated whether foveations acted strong regularizer like noise thus allowing deep architectures. denoising images investigated using architectures autoencoders. presented approach remove noise corrupted inputs using sparse coding deep networks pre-trained daes. system could automatically remove complex patterns like text image addition simple patterns like pixels missing random. type noise additions investigated white gaussian noise noise image background changes. along lines post deblurring denoising using convolutional neural networks natural image denoising patterns specks dirt rain investigated mentioned above resolution images considered type noisy input. domain image super resolution used resolution images interpolated size output image pipeline restore resolution images. cascade model trained end-to-end requires optimization layer individually. similar approach dong improves using convolutional neural networks end-to-end system. behnke demonstrated difﬁcult non-linear image reconstruction resolution inputs learnt hiearchical recurrent networks. given handwritten digit image input system iteratively increase it’s resolution output. work viewed image super resolution problem network learns mapping resolution high resolution images. contrast existing approaches network end-to-end differentiable thus learns features automatically backpropagation. current approaches require manual engineering features image pre-processing interpolations. finally emphasize goal achieving state results image super resolution. rather want study deep architecture’s ability extract useful representation low-detail images showing range mapping resolution high resolution images possible. usefulness representation measured using mean squared error input reconstructed output. present framework studying extent neural networks perceive image given various types low-detail inputs. begin specifying space neural network architectures precisely deﬁning notion perceives measure. important framework general dependent speciﬁc task image classiﬁcation which example ability learn domain-speciﬁc discriminating features might make easy solve classiﬁcation problem without fully modeling structure input. undesirable unable trust classiﬁcation accuracy reliable surrogate perceiving. mind focus instead generative models input data itself speciﬁcally autoencoders ae’s hidden units analogous intermediate neurons visual system capture features structure visual input. similarly ae’s weights forge visual memories training thus analogous long-term memory. weights properly trained activations hidden units reﬂect network perceiving novel input. however since units directly interpretable indirectly measure well network perceives evaluating similarity original generated images similar images better network able perceive. formally original input image lower-detail foveated version image. version image mostly low-detail except possibly small portion high-detail example encode images vectors ﬂoats might deﬁne class foveation functions s.t. foveation function might downsample original image according eccentricity image center also removing vector components corresponding color. employ autoencoder defoveate generating high-quality output image which example finally measure similarity foveation function ﬁlters original image removing detail later make independent variables experiments study effect different types input distortion. given framework study well different architectures able cope different types foveated inputs. note much like denoising autoencoders autoencoders reconstruct original image corrupted version. however form corruption systematic foveation instead random noise. thus homage denoising autoencoders termed models defoveating autoencoders dfaes. describe exact model next section. logistic function. sigmoid ﬁnal layer conveniently allows compare pixel intensities generated image original image directly without post-process output values. experiment number hidden units per-layer well number layers. training could employ traditional mean-squared error cross-entropy loss found domain-speciﬁc loss function peak signal-to-noise ratio yielded much better training behavior. psnr generated image original input deﬁned follows figure example fully-connected defoveating autoencoder architecture single hidden layer. image foveated autoencoder mapped hidden representation ﬁnal output image generated reconstruction error measured inputs foveations. form stochastic gradient descent determines per-feature learning rate dynamically training. adagrad calculates different learning rate feature allowing efﬁciently learn weights even features rarely occur training data. learning rate initialized adjusted adagrad training. performed epochs training experiments. architecture useful studying single foveations primary focus work. however remark straightforward augment dfaes recurrent connections handle sequence foveations similar done solving classiﬁcation tasks attention first augment foveation function include locus fovea centered. second saccade function predicts locus dfae’s current hidden states ﬁnally make hidden state recurrent function putting together yields following architecture dfae handle sequence foveations given input image allowing train model realistic fashion. human visual system access high detail information must must instead forge visual memories sequence foveations. thus mimick this rather trying reconstruct original image instead reconstruct foveation time information available time similar language model trained. recall interested question whether artiﬁcial neural network perceive image foveated input images. context autoencoders hidden layers responsible representing foveated inputs network learns reasonable representation able produce higher resolution output measure similar output network original image evaluate well network perceive. experiments architecture network family described previous section vary type foveation number hidden units number layers study learnt features reconstruction accuracy. address following questions experiments study several different foveation functions many cases downampling employed part foveation function employ nearest neighbor interpolation algorithm. nearest neighbor interpolation simple sampling algorithm selects value nearest point consider values neighboring points all. interpolation algorithm generates poor quality blocky images smoothing function. picked nearest neighbor downsampling algorithm test worst case possible downsampled inputs system. foveation functions include downsampled factor fovea present entire image uniformly downsampled factor using nearest neighbor interopolation method. example factor transforms image image approximatley pixels removed. note case color images channel separately downsampled resulting color distortion. downsampling factors tested mnist cifar dataset examples. scotoma entire regions image removed create blind spot/region rest image remains original resolution. experiment location scotoma fovea small fovea high resolution rest image downsampled factor note special case equivalent downsampling entire image uniformly. used datasets experiments mnist cifar. mnist database consists handwritten digits training examples test examples. therefore class examples. cifar dataset consists color images classes. examples classes ﬂowers large natural outdoor scenes insects people vehicles etc. class examples. training consists images test consists images. trained dfaes mnist cifar dataset normalized datasets pixel values additionally zero-centered them. step corresponds local brightness contrast normalization. aside step preprocessing patch extraction whitening applied. first establish baselines context results compare -layer dfae common upsampling algorithms found image editing software. report mean squared error reconstructed image original image measure quality reconstructed images interpolation algorithm dfae. zero means algorithm dfae able reconstruct input perfect accuracy. figure shows interpolation algorithms dfae. surprisingly nearest neighbor performed worst reconstruction overall. bilinear interpolation performed best comparison upsampling algorithms tested. interpolation algorithms performed poorly reconstructed image downsampled beyond factor error rates produced interpolation algorithms mnist dataset higher natual image dataset. figure show single layer dfae outperforms standard algorithms datasets tested. figure performance standard upsampling algorithms compared single layer dfae hidden units mnist cifar grayscale cifar color input respectively. experiment foveation functions size fovea foveation functions uniformly downsample original various factors purpose experiment study well network reconstruct image high-detail input available. variables consider number hidden units layer number layers. pilot experiments showed number hidden units less downsampled input size dfaes performed poorly. surprising autoencoders cannot learn features complete state downsampled input contains impoverished features. figure show examples reconstructed images single layer dfae. images produced dfae compared upsampled reconstructions bilinear algorithm. compared figure mnist cifar images reconstruction dfae. shows original image. shows downsampled input used training followed reconstruction images bilinear algorithm dfaes. bilinear algorithm dfaes correctly extract contents downsampled input even pixels removed. compelling example even faced blank input seen figure dfae correctly predict digit however performance dfaes suffered input downsampled beyond factor even though dfae made predictions based input reconstructions incorrect. reconstructed natural images seen figure show dfae learnt smoothing centering function even though unable reconstruct high frequencies images. dfaes could predict shape objects natural images high frequency details. next looked ﬁlters features learnt single layer dfaes shown figure feature detectors correspond ﬁnal hidden layer network visualized. hidden neuron associated vector weights uses compute product input example. weight vectors ﬁlters dimensionality input allowing visualize images highlighting aspects input hidden unit sensitive. goal visualizing feature detectors examine qualitatively kind feature detectors learnt downsampled images compare learnt full-resolution input. mnist images single layer dfae learns neuron like features given original input. input downsampled forced learn stroke like features. curiously similar result observed vincent denoising autoencoder learnt global structures trained corrupted inputs. dfae also learnt increasingly global features input downsampled correspondingly. however ability learn useful features deteriorated input downsampled beyond factor instance given input downsampled factor majority features learnt superimpositions digits reﬂected images reconstructed shown figure hand ﬁlters learnt cifar images look meaningful. cases network learnt speciﬁc color gradients locally circular blobs probably enabled better reconstructing frquency shape information landscapes particularly well. since whiten images used image patches training noise modeled dfae natural images surprising. understand number hidden units layers affect performance dfaes increased breadth and/or depth dfae. figure show performance dfae improve drastically network given additional capacity breadth depth dfae error rates stabilized number hidden units increased beyond note number hidden units varied according original input size. therefore mnist images range hidden units varied hidden units similarly cifar images increasing number hidden units dfae improve performance either cifar color grayscale images. pilot tests networks upto -layers showed performance mnist cifar images improve signiﬁcantly increasing number layers. evaluated dfaes uniformly downsampled images kind input unrealistic received retina. section evaluate dfaes foveated inputs sct-r fov-r described section discussed introduction human visual system makes effective kinds foveated inputs. machine learning perspective desirable recognize classify images degraded conﬁgurations turn reduce need carefully pruned preprocessed datasets training. rationale scotoma-like regions input test whether available input contained enough information reconstruct rest image. dataset used grayscale cifar images. variable sized areas region removed original input. location removal chosen randomly four quadrants input image except condition image around center removed. since majority input images subject interest tested central region contained enough information reconstruct rest image. reconstructions figure show dfae perform well input removed dfae reconstruct landscapes reconstruct shape information symmetry demonstrating it’s ability extract frequency information. centered reconstruction process breaks dfae cannot predict input beyond given region information. ﬁlters learnt conditions look similar grayscale version figure bigger smooth blobs blacked regions input. fov-r inputs sct-r inputs chose downsampling factor regions outside fovea since previous experiments revealed dfaes cannot reconstruct inputs downsampled beyond factor. figure shows reconstructed images fov-r inputs figure show error rate reconstruction. cluster lines lower error rates show dfae performed considerably well fov-r sct-r inputs performance better dfae trained uniformly downsampled inputs result surprising given fov-r contains additional information regions outside fovea. results suggests small number foveations containing rich details might neural networks need extract contents input higher detail. well known human visual system loses chromatic sensitivity towards periphery retina. recently interest deep networks speciﬁcally convolutional neural networks learn color grayscale images learn artistic style speciﬁcally dahl’s reconstructions grayscale images numerous cases colorized images produced muted sepia colored. problem colorization inherently ill-posed treated classiﬁcation task studies. dfaes perceive color absent input? investigated question using ach-r fova-r inputs described section regions color tested color full color. figure show examples color reconstructions input types. dfae trained full color ach-r inputs make mistakes reconstructing right color seen figure example colors yellow ﬂower pink purplish-red landscape blue. input grayscale colorizations gray muted sepia toned simply incorrect case landscapes. fovea color single layer dfae reconstruct colorizations correctly. ofcourse fovea color reduced i.e. color reconstruction accuracy falls drastically. example predicts yellowish tone sunﬂower among brown leaves. critical result performance difference full colored inputs foveated color inputs small seen figure results suggest color reconstructions accurate networks ﬁgure color region image accurately opposed every region image. similar human visual system networks capable determining accurate colors periphery color information available foveation. ﬁnding paper current deep architectures capable learning useful features ﬁdelity inputs. discussed introduction human visual system uses sequential foveations gather information surroundings. foveation fraction input high resolution. studied capability deep networks learn face minimal information speciﬁcally foveated inputs. results indicate single layer dfae reconstruct ﬁdelity inputs better existing upsampling algorithms remarkably color reconstructions foveated inputs good full colored inputs. general model achieves results using shallow network small number hidden units. investigated capacity dfae terms layers number hidden units interacts foveated inputs. found small shallow networks capable learning good representation especially frequencies input. noted performance dfae qualitatively better mnist digit images natural images. firstly mnist dataset contains training examples class compared cifar dataset contains training examples class. secondly shape digits prominent mnist dataset natural images contains texture multiple objects contrast variation adding high frequency noise. noise signal ratio lower mnist dataset general helped dfae learn better representations. color information obviously important human visual system results show performance dfae improve signiﬁcantly color images seen figures color information important improving accuracy dfae colorized images foveated inputs. image consist single multiple image regions predictive contents image? paper focused single foveated region test speciﬁc region predictive rest image. future studies investigate regions image predictive. many regions exist within image? regions generalize across class images? combine regions reconstruct image? general foveated inputs enabled dfae learn best representations overall terms image contents color. gives hope learn useful feature representations full resolution input available small computational budget. future work want study models make shifts attention improve representation demand needed associated task. peter lennie. cost cortical computation. current biology kristin koch judith mclean ronen segev michael freed michael berry vijay balasubramanian pawan sinha benjamin balas yuri ostrovsky richard russell. face recognition humans nineteen results computer vision researchers know about. proceedings ieee volodymyr mnih nicolas heess alex graves koray kavukcuoglu. recurrent models visual ghahramani welling cortes n.d. lawrence k.q. weinberger editors kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhutdinov richard zemel yoshua bengio. show attend tell neural image caption generation visual attention. pascal vincent hugo larochelle yoshua bengio pierre-antoine manzagol. extracting composing robust features denoising autoencoders. proceedings international conference machine learning icml pages york acm. pascal vincent hugo larochelle isabelle lajoie yoshua bengio pierre-antoine manzagol. stacked denoising autoencoders learning useful representations deep network local denoising criterion. journal machine learning research christopher poultney sumit chopra yann efﬁcient learning sparse representations energy-based model. advances neural information processing systems pages pierre baldi kurt hornik. neural networks principal component analysis learning exam yann lecun bernhard boser john denker donnie henderson richard howard wayne hubbard lawrence jackel. backpropagation applied handwritten code recognition. neural computation salah rifai pascal vincent xavier muller xavier glorot yoshua bengio. contractive autoencoders explicit invariance feature extraction. proceedings international conference machine learning pages christian schuler harold christopher burger stefan harmeling bernhard scholkopf. machine learning approach non-blind image deconvolution. computer vision pattern recognition ieee conference pages ieee ryan dahl. automatic colorization. http//tinyclouds.org/colorize/. accessed leon gatys alexander ecker matthias bethge. neural algorithm artistic style. arxiv", "year": 2016}