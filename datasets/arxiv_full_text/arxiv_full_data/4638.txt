{"title": "Understanding Exhaustive Pattern Learning", "tag": ["cs.AI", "cs.LG", "I.2.7; I.2.6"], "abstract": "Pattern learning in an important problem in Natural Language Processing (NLP). Some exhaustive pattern learning (EPL) methods (Bod, 1992) were proved to be flawed (Johnson, 2002), while similar algorithms (Och and Ney, 2004) showed great advantages on other tasks, such as machine translation. In this article, we first formalize EPL, and then show that the probability given by an EPL model is constant-factor approximation of the probability given by an ensemble method that integrates exponential number of models obtained with various segmentations of the training data. This work for the first time provides theoretical justification for the widely used EPL algorithm in NLP, which was previously viewed as a flawed heuristic method. Better understanding of EPL may lead to improved pattern learning algorithms in future.", "text": "pattern learning important problem natural language processing exhaustive pattern learning methods proved ﬂawed johnson similar algorithms showed great advantages tasks machine translation. article ﬁrst formalize show probability given model constant-factor approximation probability given ensemble method integrates exponential number models obtained various segmentations training data. work ﬁrst time provides theoretical justiﬁcation widely used algorithm previously viewed ﬂawed heuristic method. better understanding lead improved pattern learning algorithms future. usually pattern learning crux many natural language processing problems. solved grammar induction problems. parsing learn statistical grammar respect certain linguistic formalism context free grammar dependency grammar tree substitution grammar tree adjoining grammar combinatory categorial grammar etc. machine translation learn bilingual grammar transfer string tree structure source language corresponding string tree structure target language. embarrassing many grammar induction algorithms provide stateof-the-art performance usually regarded less principled aspect statistical modeling. johnson prescher showed data oriented parsing algorithm biased inconsistent. ﬁeld almost statistical models proposed recent years rely similar heuristic methods extract translation grammars koehn chiang quirk galley shen carreras collins name them. similar heuristic methods also used many pattern learning tasks example like semantic parsing zettlemoyer collins chunking daum´e marcu implicit way. heuristic algorithms needs extract overlapping structures training data exhaustive way. therefore article call exhaustive pattern learning methods. methods intended cope uncertainty building blocks used statistical models. concerned koehn found better deﬁne translation model phrases words obvious deﬁne phrases algorithms needs collect statistics overlapping structures training data valid generative models. thus algorithms grammar induction viewed heuristic methods denero daum´e recently denero blunsom cohn blunsom cohn post gildea investigated various sampling methods grammar induction believed principled epl. however convincing empirical evidence showing methods provided better performance large-scale data sets. article show exists mathematically sound explanation approach. ﬁrst introduce likelihood function based ensemble learning marginalizes possible building block segmentations training data. then show probability given grammar constant-factor approximation ensemble method integrates exponential number models. therefore grammar induction algorithm learn model much diversity training data. explain methods provide state-of-the-art performance many pattern learning problems. rest article organized follows. ﬁrst formalize section section introduce ensemble method show approximation theorem corollaries. discuss important problems section conclude work section purpose formalizing core idea hereby introduce task called monotonic translation. analysis task extended pattern learning problems. then deﬁne segmentation training data introduce grammar later used section theoretical justiﬁcation epl. monotonic translation deﬁned follows. input string words xx...xi source language. monotonic translation string words yy...yi length target language translation short monotonic translation simpliﬁed version machine translation. word reordering insertion deletion. ignore impact word level alignment focus effort study building blocks. leave incorporation alignments future work. fact simply view alignments constraints building blocks. monotonic translation already general enough model many tasks labelling chunking. without losing generality assume training data contains single pair word strings could long. xx...xn yy...yn. source word aligned target word length word strings figure shows simple example training data. least word. monotonic translation source side target side share topology segmentation. tokens building blocks statistical model presented means parameters model deﬁned tokens instead words. segment obtain tokenized training contains pair token strings vsi. uu...u|ds| vv...v|ds| |ds| total number tokens figure shows example segmentation training data. here token pair spans words given training data segmentation unique joint probabilistic model obtained parameter model contains source token target token. since token represents string words call model string-to-string grammar gds. speciﬁcally pair tokens however given training data segmentation unknown cope problem consider possible segmentations. string distribution training data lead good estimation hidden segmentation tokens. section introduce ensemble method incorporate grammars obtained possible segmentations. segmentations generated certain prior distribution. present solution. follow widely-used heuristic method generate grammar applying various segmentations time. build heuristic grammar training data counting possible token pairs words side given parameter desirable translation rule ‘left went heuristic grammar although weight diluted noise. hope that good translation rules appear often training data distinguished noisy rules. decoding phase grammar regular grammar. xx...xi input source string. segmentation test sentence uu...uk resultant string source tokens. length string length token string |ua| translation looking given target token vector previous work structure-based calculate marginal probability sums possible target tokens generating word string concern computational complexity. obviously take advantage larger context words. however common criticism approach grammar like mathematically sound. probabilities simply heuristics clear statistical explanation. next section show mathematically sound. section ﬁrst introduce ensemble model prior distribution segmentation. show theorem approximation present corollaries conditional probabilities tree structures. training data words. arbitrary token segmentation unknown given obtain model/grammar maximum likelihood estimation. thus calculate joint probability given grammar potentially exponential number distinct segmentations here ensemble method possible segmentations. method would provide desirable coverage diversity translation rules learned training data. segmentation ﬁxed prior probability shown section thus deﬁne ensemble probability follows. interesting turns prior distribution that certain conditions limit value depends parameter prior distribution shown theorem constant hypotheses input. therefore constant-factor approximation using extent equivalent using possible grammars time ensemble method. xx...xi input source string. segmentation resultant token string uu...uk. vv...vk hypothesis translation number times string pair appears training data number times token pair appears segmented data order prove theorem assume following assumptions true pair tokens fact section rely assumption bound ratio know small positive number build upper lower bounds ratio based however assumption much easier picture assume true rest section. solve bounding |ds| values independent separating variables related appearances divide parts based appearances pairs. part contains contains appearances rest internal separating variables independent mjs. example shown figure black boxes represent appearances. concatenate fragments keep i-internal separating variables variants segmentation depending deﬁne separating variables fragments. following segmented sub-sets. here represent segmentation vectors respectively changeable separating variables number words contained number changeable variables follows binomial distribution figure ﬁxed inter-fragment separating variables represented bold italic font. previous work using conditional theorem joint distribution token pairs. probabilities ofter used example like starting theorem easily obtain following corollaries conditional probabilities. proof according deﬁnition |vj|. therefore need consider pairs |vj| |uj|. number distinct ﬁnite number since source vocabulary ﬁnite set. therefore according theorem small positive number exists positive number that proofs conditional probabilities depend special property monotonic translation; length length however true real application phrase-based translation. source target sides different segmentations. leave modeling real phrase-based translation future work. extend theorem string-to-tree grammar. first deﬁne prior distribution tree segmentation. assign bernoulli variable tree node representing probabilities separate tree node probabilities choose separate node. string–tree pair source string target tree. number words number non-terminals length input sentence |x|. thus probability appearance exactly tokenized −nj+. proofs lemmas assumption used last steps. therefore could build upper lowers bounds ratio without assumption connecting inequalities appendixes respectively. then approximate posterior probability given point estimation. thus grammar means distribution mass assigned grammar also assume independent thus method investigated article date back data oriented parsing special model uses overlapping treelets various sizes exhaustive building blocks statistical tree grammar. framework pair represent input text represent tree structure. thus would similar string-to-tree model section joint probability stands unigram probability however original estimator quite different monotonic translation model. conditional probability deﬁned obvious model monotonic translation. therefore theoretical justiﬁcation still open problem. article ﬁrst formalized exhaustive pattern learning widely used grammar induction nlp. showed using heuristic grammar equivalent using ensemble method cope uncertainty building blocks statistical models. better understanding lead improved pattern learning algorithms future. work affect research various ﬁelds natural language processing including machine translation parsing sequence classiﬁcation etc. also applied research ﬁelds outside nlp. work inspired enlightning discussion scott miller rich schwartz spyros matsoukas author technologies. reviewers conll emnlp icml jmlr helped sharpen focus work. however mistakes belong", "year": 2011}