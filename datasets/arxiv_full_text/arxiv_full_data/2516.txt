{"title": "An Empirical Evaluation of True Online TD(λ)", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "The true online TD({\\lambda}) algorithm has recently been proposed (van Seijen and Sutton, 2014) as a universal replacement for the popular TD({\\lambda}) algorithm, in temporal-difference learning and reinforcement learning. True online TD({\\lambda}) has better theoretical properties than conventional TD({\\lambda}), and the expectation is that it also results in faster learning. In this paper, we put this hypothesis to the test. Specifically, we compare the performance of true online TD({\\lambda}) with that of TD({\\lambda}) on challenging examples, random Markov reward processes, and a real-world myoelectric prosthetic arm. We use linear function approximation with tabular, binary, and non-binary features. We assess the algorithms along three dimensions: computational cost, learning speed, and ease of use. Our results confirm the strength of true online TD({\\lambda}): 1) for sparse feature vectors, the computational overhead with respect to TD({\\lambda}) is minimal; for non-sparse features the computation time is at most twice that of TD({\\lambda}), 2) across all domains/representations the learning speed of true online TD({\\lambda}) is often better, but never worse than that of TD({\\lambda}), and 3) true online TD({\\lambda}) is easier to use, because it does not require choosing between trace types, and it is generally more stable with respect to the step-size. Overall, our results suggest that true online TD({\\lambda}) should be the first choice when looking for an efficient, general-purpose TD method.", "text": "true online algorithm recently proposed universal replacement popular algorithm temporal-diﬀerence learning reinforcement learning. true online better theoretical properties conventional expectation also results faster learning. paper hypothesis test. speciﬁcally compare performance true online challenging examples random markov reward processes real-world myoelectric prosthetic arm. linear function approximation tabular binary non-binary features. assess algorithms along three dimensions computational cost learning speed ease use. results conﬁrm strength true online sparse feature vectors computational overhead respect minimal; non-sparse features computation time twice across domains/representations learning speed true online often better never worse true online easier require choosing trace types generally stable respect step-size. overall results suggest true online ﬁrst choice looking eﬃcient general-purpose method. temporal-diﬀerence learning core learning technique modern reinforcement learning main challenges reinforcement learning make predictions initially unknown environment future rewards return based currently observed feature values certain behaviour policy. learning possible learn good estimates expected return quickly bootstrapping expected-return estimates. popular algorithm combines basic learning eligibility traces speed learning. ability speed learning explained forward view states estimate time step moved toward update target known λ-return λ-parameter determines trade-oﬀ bias variance update target. trade-oﬀ large inﬂuence speed learning optimal setting true online recently proposed variation better theoretical properties. speciﬁcally maintains exact equivalence forward view times. contrast accurately approximates forward view appropriately small step-sizes. hence expected true online better improving learning speed. initial experiments suggest indeed case however signiﬁcant empirical study performed far. paper empirically compare true online wide variety domains. described -tuples form consisting states; p|s) transition probability function giving state probability transition state next step; reward function giving expected reward transition discount factor specifying future rewards weighted respect immediate reward. contain terminal states dividing sequence state transitions episodes. terminal state reached current episode ends state reset initial state. return time step discounted rewards observed time step sensitive respect parameters. especially large value combined large value easily cause divergence even simple tasks bounded rewards. reason variant often used robust respect parameters. variant assumes binary features uses diﬀerent trace-update equation uses equation update elegibility-trace vector said replacing traces; contrast default implementation based trace update said accumulating traces. paper indicate implemenations ‘replace ‘accumulate respectively. experiments compare versions. compared accumulate trace update t−φt] td-error time-step correction trace; call term simply δ-correction. reduces case δ-correction hence true online reduces regular method. algorithm shows pseudocode implements true online order discuss computational cost total number features number features non-zero value. then number basic operations time step conventional true online takes another resulting operations total. hence sparse feature vectors used computational overhead true online minimal. non-sparse feature vectors used true online require respectively. case true online roughly twice expensive conventional section compare performance true online accumulate replace first compare behaviour challenging examples random mrps ﬁnally real-world data set. experiments random mrps real-world data methods sample sequence hence lower error corresponds higher learning speed. ﬁrst experiments designed small examples challenging either accumulate replace check true online deal well problems ﬁrst one-state example challenging accumulate second two-state example challenging replace challenging part one-state example feature revisited frequently within episode; challenging part two-state example value function cannot represented exactly left graph figure shows early learning performance one-state example diﬀerent step-size values. replace true online well task. state-value converges single episode already resulting average error zero. surprising given return figure left one-state example. right two-state example. circles indicate states; squares terminal states; arrows indicate state transitions examples state-space represented single binary feature right graph figure shows error approximate convergence diﬀerent values two-state example. also error least mean squares solution shown. accumulate error equal solution gets worse smaller corresponds theory states ﬁxed point accumulate equals solution diﬀerent solution true online behaviour. surprisingly replace value eﬀect. task illustrates main weakness replace avoids divergence issues accumulate overly conservative approach. avoids divergence resetting trace time feature revisited reduces overall eﬀect trace. extreme case eﬀect removed completely happens here. overall tasks show accumulate replace weakness true online suﬀer weaknesses. course problems constructed extreme examples. practise tasks one-sided properties examples. figure left error episode averaged ﬁrst episodes one-state example right error approximate convergence two-state example considered values converged error changed less last time steps. second series experiments used randomly constructed mrps. represent random -tuple consisting number states; branching factor standard deviation reward compared performance three diﬀerent mrps small number states larger number states branching factor stochasticity reward evaluated using three diﬀerent representations tabular features binary features non-binary normalized features details representations appendix domain/representation/method combination performed scan values determine best performance combination. performance metric used mean-squared error respect solution early learning normalized error dividing error obtained relevant domain/representation combination methods reduce algorithm addition averaged independent runs. domain/representation/method used sample sequences. figure shows results comparisons domain/representation combination. lies parameter range optimized over normalized error never higher method/domain normalized error equal means setting higher either eﬀect error gets worse. either case eligibility traces eﬀective domain/representation/method combination. results conﬁrm strength true online optimal performance true online domains representations least good optimal performance replace accumulate speciﬁcally true online outperforms conventional domains/representations considered. next subsection compare methods using real-world data. experiment compare performance true online conventional real-world data-set consisting sensorimotor signals measured human control electromechanical robot arm. source data series manipulation tasks performed participant amputation presented pilarski study amputee participant used signals recorded muscles residual limb control robot multiple degrees-of-freedom interactions kind known myoelectric control consistency comparison results used source data prediction learning architecture published pilarski total signals predicted grip force motor angle signals robot’s hand. speciﬁcally target prediction discounted signal time similar return predictions possible used implementation code base pilarski data experiment consisted time steps recorded sensorimotor information sampled state space consisted tile-coded representation robot gripper’s position velocity recorded gripping force muscle contraction signals human user. standard implementation tile-coding used bins signal eight overlapping tilings single active bias unit. results state space features active given time. hashing used reduce space vector features presented learning system. signals normalized provided function approximation routine. discount factor predictions force angle results presented pilarski parameter sweeps conducted three methods. performance metric mean absolute return error time steps learning normalized dividing error figure shows performance angle well force predictions. relative performance replace accumulate depends predictive question asked. predicting robot’s grip force signal—a signal small magnitude rapid changes—replace better accumulate non-zero values. however predicting robot’s hand actuator position smoothly changing signal figure left picture experimental setup. middle normalized error predictions diﬀerent best value force predictions. right same angle predictions. compared true online conventional along three broad dimensions computational cost learning speed ease use. terms computational cost slight advantage. worst case true online twice expensive. typical case sparse features fractionally expensive memory requirements methods. terms learning speed experiments true online usually better never worse speciﬁcally true online substantially outperformed mrps myoelectricarm experiments. finally terms ease conclude true online clear advantage. ﬁrst diﬃculty conventional typically must choose types traces whereas true online choice made. second diﬃculty accumulate performance sensitive step-size parameter making hard acceptable value. overall results suggest true online ﬁrst choice looking eﬃcient general-purpose method. number states domain. tabular representation state represented unique standard-basis vector dimensions. binary representation constructed ﬁrst assigning indices states. then binary encoding index state used feature vector represent state. length feature vector determined total number states length length example feature vectors states respectively. finally non-binary normal representation state mapped -dimensional feature vector value feature drawn normal distribution zero mean unit variance. feature values state drawn normalized feature vector unit length. generated feature vectors kept ﬁxed state.", "year": 2015}