{"title": "Deep Neural Networks with Random Gaussian Weights: A Universal  Classification Strategy?", "tag": ["cs.NE", "cs.LG", "stat.ML", "62M45", "I.5.1"], "abstract": "Three important properties of a classification machinery are: (i) the system preserves the core information of the input data; (ii) the training examples convey information about unseen data; and (iii) the system is able to treat differently points from different classes. In this work we show that these fundamental properties are satisfied by the architecture of deep neural networks. We formally prove that these networks with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data. Similar points at the input of the network are likely to have a similar output. The theoretical analysis of deep networks here presented exploits tools used in the compressed sensing and dictionary learning literature, thereby making a formal connection between these important topics. The derived results allow drawing conclusions on the metric learning properties of the network and their relation to its structure, as well as providing bounds on the required size of the training set such that the training examples would represent faithfully the unseen data. The results are validated with state-of-the-art trained networks.", "text": "abstract—three important properties classiﬁcation machinery system preserves core information input data; training examples convey information unseen data; system able treat differently points different classes. work show fundamental properties satisﬁed architecture deep neural networks. formally prove networks random gaussian weights perform distance-preserving embedding data special treatment in-class out-of-class data. similar points input network likely similar output. theoretical analysis deep networks presented exploits tools used compressed sensing dictionary learning literature thereby making formal connection important topics. derived results allow drawing conclusions metric learning properties network relation structure well providing bounds required size training training examples would represent faithfully unseen data. results validated state-of-the-art trained networks. deep neural networks revolution areas machine learning audio analysis computer vision achieving state-of-the-art results numerous applications work formally study properties deep network architectures random weights applied data residing dimensional manifold. results provide insights outstanding empirically observed performance role training size training data. motivation studying networks random weights twofold. first series works empirically showed successful learning techniques based randomization. second studying system random weights rather learned deterministic ones lead better understanding system even deterministic case. example ﬁeld compressed sensing goal recover signal small number measurements study random sampling operators breakthroughs understanding number measurements required achieving stable reconstruction bounds provided case universally optimal introduction learning phase provides better reconstruction performance adapts system particular data hand ﬁeld information retrieval random projections used locality-sensitive hashing scheme capable alleviating curse dimensionality approximate nearest neighbor search high dimensions original randomized scheme seldom used practice availability data-speciﬁc metric learning algorithms provided many fruitful insights. ﬁelds phase retrieval gained signiﬁcantly study based random gaussian weights notice technique proving results deep learning assumptions random distribution showing holds general case unique work. contrary stronger recent theoretical results follow path. example arora analyzed learning autoencoders random weights range showing possible learn polynomial time restrictions depth network another example series works study optimization perspective dnn. similar fashion work study properties deep networks assumption random weights. before turn describe contribution survey previous studies formally analyzed role deep networks. hornik cybenko proved neural networks serve universal approximation measurable borel functions. however ﬁnding network weights given function shown np-hard. scattering transform– cascade wavelet transform convolutions nonlinear modulus averaging operators showed deep architecture layers resulting features made invariant increasingly complex groups transformations. study wavelet scattering transform demonstrates deeper architectures able better capture invariant properties objects scenes images anselmi showed image representations invariant transformations translations scaling considerably reduce sample complexity learning deep architecture ﬁltering pooling learn invariant representations result particularly important cases training labels scarce totally unsupervised learning regimes. mont´ufar morton showed depth allows representing restricted boltzmann machines number parameters exponentially greater number network parameters mont´ufar suggest layer divides space hyper-plane therefore deep network divides space exponential number sets unachievable single layer number parameters. bruna showed pooling stage results shift invariance authors interpret step removal phase complex signal show signal recovered pooling stage using phase retrieval methods. work also calculates lipschitz constants pooling rectiﬁed linear unit stages showing perform stable embedding data assumption ﬁlters applied network frames e.g. relu stage exist constants rm×n denotes linear operator given layer network denoting input output dimensions respectively relu operator applied element-wise. however values lipschitz constants real networks behavior function data dimension currently elude understanding. bound loose consider output linear part fully connected layer undesired behavior unique normally-distributed characteristic distribution bounded fourth moment. note addition non-linear operators relu pooling makes lipschitz constants even worse. former example teaches scaling data introduced drastically deform distances throughout layer even case close makes unclear whether possible recover input network output. work main question focus happens metric input data throughout network? focus mentioned setting assuming network random i.i.d. gaussian weights. prove preserve metric structure data propagates along layers allowing stable recovery original data features calculated network. type property often encountered literature notice however recovery input possible size network output proportional intrinsic dimension data input similarly data reconstruction small number random projections however unlike random projections preserve euclidean distances small distortion layer random weights distorts distances proportionally angles input points smaller angle input stronger shrinkage distances. therefore deeper network stronger shrinkage get. note contradict fact recover input output; even properties lighting pose location removed image resemblance original image still maintained. random projection universal sampling strategy dimensional data deep networks random weights universal system separates data according angles points general assumption large angles different classes training projection matrix adapts better preserve speciﬁc distances others training network prioritizes intra-class angles inter-class ones. relation alluded proof techniques empirically manifested observing angles euclidean distances output trained networks demonstrated later paper section rest paper organized follows section start utilizing recent theory -bit compressed sensing show layer preserves metric input data gromov-hausdorff sense small constant assumption data reside low-dimensional manifold denoted allows draw conclusions tessellation space created layer network relation operation layers local sensitive hashing also show possible retrieve input layer certain accuracy output. implies every layer preserves important information data. section proceed analyzing behavior euclidean distances angles data throughout network. section reveals important effect relu. without relu would random projections euclidean distance preservation. theory shows addition relu makes system sensitive angles points. prove networks tend decrease euclidean distances points small angle distances points large angles then section prove low-dimensional data input remain throughout entire network i.e. increase intrinsic dimension data. property used section deduce size data needed training dnn. conclude studying role training section random networks blind data labels training select discriminatively angles cause distance deformation. therefore cause distances different classes increase distances within class. demonstrate several simulations networks recently showed state-of-the-art performance challenging datasets e.g. network imagenet dataset section concludes paper summarizing main results outlining future research directions. assumption classes separated large angles common literature assumption refer feature space rather data space. course examples might found contradict assumption concentric spheres sphere represents different class. respect particular examples things said first cases rare real life signals typically exhibiting amount scale invariance absent example. second prove property discrimination based angles holds random weights conjecture section potential role training favor certain angles others select origin coordinate system respect angles calculated. illustrate section effect training compared random networks using trained achieved state-of-the-art results literature. claim suitable models clearly distinguishable angles classes random weights used classes distinguishable angles training used. sake simplicity discussion presentation clarity focus role relu operator assuming data properly aligned i.e. invariant operations rotation translation therefore need pooling operation achieve invariance. combining recent results phase retrieval problem proof techniques paper lead theory also applicable pooling operation. addition strategy possible generalize guarantees sub-gaussian distributions random convolutional ﬁlters. defer natural extensions future studies. section consider distance metrics input output single layer form mapping input vector output vector random gaussian matrix semi-truncated linear function applied element-wise. linear interval constant outside relu henceforth denoted example function sigmoid hyperbolic tangent functions satisfy property approximately. note supxy∈khg width direction illustrated fig. mean provides average widths different isotropically distributed directions leading deﬁnition gaussian mean width sparsely representable signals data approximated sparse linear combination atoms dictionary i.e. kkxk pseudo-norm counts number nonzeros vector rn×l given dictionary. model similar results shown models union subspaces dimensional manifolds. examples details refer reader show standard layer performs stable embedding data gromov-hausdorff sense i.e. δ-isometry manifolds input output data metrics induced them. function δ-isometry theorem linear operator applied layer semi-truncated linear activation function manifold input data layer. rm×n random matrix i.i.d normally distributed entries deﬁned sgn) high probability δ-isometry i.e. fig. behavior function extremities correspond angles zero respectively notice larger angle larger value addition vanishes small angles vectors. activation function transformation keeps output data sphere proof theorem sequel). therefore case normalization requirement holds small distortion throughout layers. adds motivation normalization stage output layer shown provide gain several normalization also useful shallow representations interpreted transformation making inner products data vectors coincide cosines corresponding angles. bounds provide section require normalization show operations layer rely angles data points. theorem dsn− geodesic distance hamming distance sign function applied elementwise deﬁned proof approximate injectivity follows proof theorem theorem important provides better understanding tessellation space layer creates. result stands line suggested layer network creates tessellation input data different hyper-planes imposed rows however theorem implies that. implies cell tessellation diameter i.e. fall side hyperplanes addition number hyperplanes separating points contains enough information deduce distance small distortion. perspective layer followed sign function acts locality-sensitive hashing approximately embedding input metric hamming metric. stable embedding data natural assume possible recover input layer output. indeed mahendran vedaldi demonstrate achievable whole network next result provides theoretical justiﬁcation this showing possible recover input layer output theorems applying existing results -bit compressed sensing dnn. theorem deals embedding hamming cube theorem uses fact show recover input output. indeed theorem applies individual layer cannot applied consecutively throughout network since deals embedding hamming cube. deal problem extend proof embedding sm−. instead turn focus relu prove speciﬁc results exact distortion angles euclidean distances. also include proof stable embedding network layer focused metric preservation deep networks terms gromov-hausdorff distance. section turn look euclidean distances angles change within layers. focus case relu activation function. similar analysis also applied pooling. simplicity discussion defer future study. fig. left histogram angles output layers imagenet deep network different input angle ranges. middle histogram ratio angles output layers network angles input different input angle ranges. right histogram differences angles output layers network angles input different input angles ranges. therefore euclidean distance shrinks half throughout layers network. emphasize contradiction theorem guarantees approximately isometric embedding hamming space. binarized output hamming metric approximately preserves input metric euclidean metric output distorted. smaller angle larger distortion distance output layer smaller distance turns hand shrinkage distances bounded seen following corollary theorem proof follows inequality arithmetic geometric means behavior conclude random gaussian weights preserve local structures manifold enable decreasing distances points away property much desired classiﬁcation. inﬂuence entire network angles slightly different. note starting input second layer vectors reside non-negative orthant. cosine angles translated range ﬁrst layer range subsequent second layers. particular range translated range terms angle range π/]. angles shrink approximately half ones initially small remain approximately unchanged. action network preserves order angles. generally speaking network affects angles range way. particular range angles output layer behave like wider range bounded therefore note equal zero theorems would stated distances angles preserved throughout network. seen fig. behaves approximately like larger angle larger consequently also euclidean distance output. angle close zero close vanishes smaller angle shrinks distance points larger angle them. ideally would like behavior causing points belonging class stay closer output network compared points different classes. however random networks selective sense point forms angle point class point another class distance distorted approximately equal amount. moreover separation caused network dependent setting coordinate system origin respect angles calculated. location origin dependent bias terms zero random networks studied. learned training network affecting angles cause distortions euclidean distances. demonstrate effect training section order show results sections also apply entire network layer need show gaussian mean width grow signiﬁcantly data propagate layers. instead bounding variation gaussian mean width throughout network bound change covering number i.e. smallest number ℓ-balls radius cover bound covering number dudley’s inequality proof divide proof parts. ﬁrst consider effect activation function size covering second examine effect linear transformation starting activation function center ball covering point belongs ball radius i.e. hard since semi-truncated linear activation function shrinks data therefore size covering increase linear part therefore linear operation covering ball initial radius bigger since activation function increase size covering linear operation followed activation function size covering balls increases factor therefore size covering balls radius output data bounded size covering balls radius fig. sketch distortion classes distinguishable angle obtained layer random weights. networks suitable separating type classes. note distance blue points shrinks less distance points angle latter smaller. theory captures behavior endowed pooling test angles change state-of-the-art -layers deep network trained imagenet dataset. randomly select angles input network partitioned three equally-sized groups group corresponding ranges test behavior applying eight sixteen non-linear layers. latter case corresponds part network excluding fully connected layers. denote vector output layer corresponding input vector fig. presents histogram values angles output layers three groups. shows also ratio difference angles output layers original value input network. theorem predicts ratio corresponding half ratio corresponding input angles range furthermore ratios ranges approximately same range slightly larger. line theorem claim angles range decrease approximately rate larger angles shrink slightly larger. also note according theory ratio corresponding input angles range behave average like number layers. indeed centers histograms range close values. notice similar behavior also range surprising looking fig. observe angles also turn range ratio remarkably fig. demonstrates network keeps order angles theorem suggests. notice shrinkage angles cause large angles become smaller angles originally signiﬁcantly smaller them. moreover small angles input remain small output seen fig. sketch distortion sets distinguishable angle layer random weights fig. observed distance points ‘ground-truth autoencoder’ generating data. combination data dimension bound provided prior relation data deep network lead better prediction number needed training samples. fact cannot refrain drawing analogy ﬁeld sparse representations signals combined properties system input data works improved bounds beyond na¨ıve manifold covering number references therein). following section presents combination showing empirically purpose training treat boundary points. observation likely lead signiﬁcant reduction required size training data also apply active learning training constructed adaptively. proof theorem provides insight role training. property gaussian distribution allows keep ratio angles data rotational invariance. phase random gaussian vector i.i.d. entries random vector uniform distribution. therefore prioritize direction manifold treats same leading behavior angles distances throughout described above. general points within class would small angles within points distinct classes would larger ones. holds points random gaussian weights would ideal choice network parameters. however practice rarely case important role training would select smart separating hyper-planes induced angles points different classes ‘penalized more’ angles points class. note integration formula done uniformly interval contains range directions simultaneously positive inner products learning ability pick angle maximizes/minimizes inner product based whether belong class distinct classes increase/decrease euclidean distances output layer. optimizing angles pairs points hard problem. explains random initialization good choice dnn. hard optimal conﬁguration separates classes manifold desirable start universal treats theorem generalizes results theorems used whole network exists algorithm recovers input output; whole distort euclidean distances based angels input network; angular distances smaller altered signiﬁcantly network. note however theorem apply theorem order later need also version theorem guarantees stable embedding using metric input output given layer e.g. embedding hamming cube hamming cube sm−. indeed exactly guarantee corollary implies stable embedding euclidean distances layer network. though corollary focuses particular case relu unlike theorem covers general activation functions implies stability whole network lipschitz sense even stronger stability gromov-hausdorff sense would generalization theorem implication theorem consider low-dimensional data admitting gaussian mixture model gaussians dimension k-sparse represention given dictionary atoms. covering number amount labeled samples needed training. using sudakov minoration upper bound size covering number balls radius include points demonstrated networks random gaussian weights realize stable embedding; consequently network trained using screening technique selecting best among many networks generated random weights suggested number data points needed order guarantee network represents data o/ǫ)). since proxy intrinsic data dimension seen previous sections details) bound formally predicts number training points grows exponentially intrinsic dimension data. exponential dependency pessimistic often possible achieve better bound required training sample size. indeed bound developed requires much less samples. authors study data recovery capability autoencoder assume exists fig. ratios closest inter farthest intra-class class euclidean angular distances cifar-. data point calculate euclidean distance farthest point class closest point class input output last convolutional layer. compute ratio i.e. point input farthest point class point output farthest point class calculate k¯x−¯zk kx−yk distances different classes comparing shortest euclidean angular distances. fig. differences closest inter farthest intra class euclidean angular distances cifar- data point calculate euclidean distance farthest point class closest point class input output last convolutional layer. compute difference i.e. point input farthest point class point output farthest point class calculate distances different classes comparing shortest euclidean angular distances. validate hypothesized behavior trained mnist cifar- datasets containing classes. training networks done using matconvnet toolbox mnist cifar- networks trained four layers respectively followed softmax operation. used default settings provided toolbox dataset cifar- also used horizontal mirroring ﬁlters ﬁrst layers instead improve performance. trained achieve errors mnist cifar- respectively. data point calculate euclidean angular distances farthest intra-class point closest inter-class point. compute ratio distances output last convolutional layer ones input. point input farthest point class point output farthest point class calculate k¯x−¯zk euclidean kx−yk distances angles. distances different classes comparing shortest ones. fig. presents histograms distance ratios cifar-. fig. present histograms differences euclidean angular distances i.e. also compare behavior inter intra-class distances computing ratios pairs points input respect corresponding points output. ratios presented fig. present also differences fig. present results three trained networks addition random denoted net. corresponds different amount training epochs resulting different classiﬁcation error. considering random note histograms ratios centered around ones differences around implying network preserves distances theorems predict network random weights. trained networks histograms data point pairs change slightly training. also observe trained networks behave like random counterparts keeping distance randomly picked pair points. however distort distances points class boundaries better random network sense farthest intra class distances shrunk larger factor fig. ratios inter intra class euclidean angular distances randomly selected points cifar-. calculate euclidean distances randomly selected pairs data points different classes class input output last convolutional layer. compute ratio i.e. pairs points input corresponding points output calculate k¯x−¯yk kx−yk fig. differences inter intra class euclidean angular distances randomly selected points cifar calculate euclidean distances randomly selected pairs data points different classes class input output last convolutional layer. compute difference i.e. pairs points input corresponding points output calculate ones random network closest inter class distances farther apart training. notice shrinking distances within class enlargement distances classes improves training proceeds. conﬁrms hypothesis goal training treat boundary points. similar behavior observed angles. closest angles enlarged trained network compared random one. however enlarging angles classes also causes enlargement angles within classes. notice though enlarged less ones outside class. finally observe enlargement angles seen theorems causes larger distortion euclidean distances. therefore explain enlargement distances within class means shrinking intra-class distances. similar behavior observed mnist dataset. however gaps random network trained network smaller mnist dataset contains data initially well separated. argued above manifolds random network already good choice. also compared behavior validation data imagenet dataset network provided network random weights. results presented figs. behavior similar observed case cifar- also manifested imagenet network. shown random gaussian weights perform stable embedding data drawing connection dimension features produced network still keep metric information original manifold complexity data. metric preservation property network provides formal relationship complexity input data size required training set. interestingly follow-up studies found adding metric preservation constraints training networks also leads theoretical relation complexity data number training samples. moreover constraint shown improve practice generalization error i.e. improves classiﬁcation results small number training examples available. preserving structure initial metric important vital ability distort distances order deform data euclidean distances represent faithfully similarity would like points class. proved ability inherent architecture euclidean distances input data distorted throughout networks based angles data points. results lead conclusion universal classiﬁers data based angles principal axis classes data. fig. intra class euclidean angular distances imagenet. data point calculate euclidean distance farthest point class closest point class input output last convolutional layer. compute ratio i.e. point input farthest point class point output farthest point class calculate k¯x−¯zk kx−yk distances different classes comparing shortest euclidean angular distances. angles would like work reality training reveals actual angles data. fact applications possible networks random weights ﬁrst layers separating points distinguishable angles followed trained weights deeper layers separating remaining points. practiced extreme learning machines techniques results provide possible theoretical explanation success hybrid strategy. work implies possible view stagewise metric learning process suggesting might possible replace currently used layers metric learning algorithms opening venue semi-supervised dnn. also stands line recent literature convolutional kernel methods addition observed potential main goal training network treat class boundary points keeping distances approximately same. lead active learning strategy deep learning acknowledgmentswork partially supported nsseff aro. a.b. supported authors thank reviewers manuscript suggestions greatly improved paper. fig. differences closest inter farthest intra class euclidean angular distances imagenet data point calculate euclidean distance farthest point class closest point class input output last convolutional layer. compute difference i.e. point input farthest point class point output farthest point class calculate distances different classes comparing shortest euclidean angular distances. proposition i.i.d. gaussian random vector zero mean unit variance lipschitzcontinuous function lipschitz constant every probability exceeding fig. ratios inter intra class euclidean angular distances randomly selected points imagenet. calculate euclidean distances randomly selected pairs data points different classes class input output last convolutional layer. compute ratio i.e. pairs points input corresponding points output calculate k¯x−¯yk kx−yk fig. differences inter intra class euclidean angular distances randomly selected points imagenet calculate euclidean distances randomly selected pairs data points different classes class input output last convolutional layer. compute difference i.e. pairs points input corresponding points output calculate distributed. therefore strategy would calculate expectation random variables show using bernstein’s inequality mean random variables deviate much expectation. proof theorem proof theorem consists three keys steps. ﬁrst show bound holds high probability points second pick ǫ-cover show holds pair cover. last generalizes bound point sudakov’s inequality |nǫ| cǫ−w. plugging inequality leads cǫ−w setting assumption cδ−ω term exponent negative therefore probability decays exponentially increases. bound rewrite desired result setting using triangle inequality combined proposition control fact taylor expansions cos− functions control terms related inner product positive. therefore higher probability positive inner product using fact gaussian vector uniformly distributed sphere calculate expectation following integral dependent angle upper bound |zi|. calculate fourth moments calculate easy compute calculating later similar provides insight role training. random layers ‘integrate uniformly’ interval learning picks angle maximizes/minimizes inner product based whether belong class distinct classes. pinto doukhan dicarlo high-throughput screening approach discovering good forms biologically inspired visual representation plos comput biol vol. pinto beyond simple features large-scale feature search approach unconstrained face recognition ieee international conference automatic face gesture recognition workshops march duarte-carvajalino sapiro learning sense sparse signals simultaneous sensing matrix sparsifying dictionary optimization ieee trans. imag. proc. vol. july cand`es strohmer voroninski phaselift exact stable signal recovery magnitude measurements convex programming communications pure applied mathematics vol. saxe mcclelland ganguli exact solutions nonlinear dynamics learning deep linear neural network international conference learning representations dauphin pascanu gulcehre ganguli bengio identifying attacking saddle point problem high-dimensional non-convex optimization advances neural information processing systems saligrama aperiodic sequences uniformly decaying correlations applications compressed sensing system identiﬁcation ieee trans. inf. theory vol. sept. lapanowski plan vershynin one-bit compressed sensing non-gaussian measurements linear algebra applications vol. special issue sparse approximate solution linear systems.", "year": 2015}