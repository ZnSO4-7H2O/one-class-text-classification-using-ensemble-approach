{"title": "Softmax Q-Distribution Estimation for Structured Prediction: A  Theoretical Interpretation for RAML", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "Reward augmented maximum likelihood (RAML), a simple and effective learning framework to directly optimize towards the reward function in structured prediction tasks, has led to a number of impressive empirical successes. RAML incorporates task-specific reward by performing maximum-likelihood updates on candidate outputs sampled according to an exponentiated payoff distribution, which gives higher probabilities to candidates that are close to the reference output. While RAML is notable for its simplicity, efficiency, and its impressive empirical successes, the theoretical properties of RAML, especially the behavior of the exponentiated payoff distribution, has not been examined thoroughly. In this work, we introduce softmax Q-distribution estimation, a novel theoretical interpretation of RAML, which reveals the relation between RAML and Bayesian decision theory. The softmax Q-distribution can be regarded as a smooth approximation of the Bayes decision boundary, and the Bayes decision rule is achieved by decoding with this Q-distribution. We further show that RAML is equivalent to approximately estimating the softmax Q-distribution, with the temperature $\\tau$ controlling approximation error. We perform two experiments, one on synthetic data of multi-class classification and one on real data of image captioning, to demonstrate the relationship between RAML and the proposed softmax Q-distribution estimation method, verifying our theoretical analysis. Additional experiments on three structured prediction tasks with rewards defined on sequential (named entity recognition), tree-based (dependency parsing) and irregular (machine translation) structures show notable improvements over maximum likelihood baselines.", "text": "xuezhe pengcheng jingzhou graham neubig eduard hovy language technologies institute carnegie mellon university {xuezhem pcyin liujingzhou gneubig hovy}cs.cmu.edu reward augmented maximum likelihood simple effective learning framework directly optimize towards reward function structured prediction tasks number impressive empirical successes. raml incorporates task-speciﬁc reward performing maximum-likelihood updates candidate outputs sampled according exponentiated payoff distribution gives higher probabilities candidates close reference output. raml notable simplicity efﬁciency impressive empirical successes theoretical properties raml especially behavior exponentiated payoff distribution examined thoroughly. work introduce softmax q-distribution estimation novel theoretical interpretation raml reveals relation raml bayesian decision theory. softmax q-distribution regarded smooth approximation bayes decision boundary bayes decision rule achieved decoding qdistribution. show raml equivalent approximately estimating softmax q-distribution temperature controlling approximation error. perform experiments synthetic data multi-class classiﬁcation real data image captioning demonstrate relationship raml proposed softmax q-distribution estimation method verifying theoretical analysis. additional experiments three structured prediction tasks rewards deﬁned sequential tree-based irregular structures show notable improvements maximum likelihood baselines. many problems machine learning involve structured prediction i.e. predicting group outputs depend other. recent advances sequence labeling syntactic parsing machine translation beneﬁt development sophisticated discriminative models structured outputs seminal work conditional random ﬁelds large margin methods demonstrating importance joint predictions across multiple output components. principal problem structured prediction direct optimization towards task-speciﬁc metrics used evaluation token-level accuracy sequence labeling bleu score machine translation. contrast maximum likelihood estimation uses likelihood serve reasonable surrogate task-speciﬁc metric number techniques emerged incorporate task-speciﬁc rewards optimization. among methods reward augmented maximum likelihood stood simplicity effectiveness leading state-ofthe-art performance several structured prediction tasks machine translation image captioning instead maximizing log-likelihood ground-truth output raml attempts maximize expected log-likelihood possible candidate outputs w.r.t. exponentiated payoff distribution deﬁned normalized exponentiated reward. incorporating task-speciﬁc reward payoff distribution raml combines computational efﬁciency conceptual advantages reinforcement learning algorithms optimize expected reward simple raml appears empirical success piqued interest analyzing justifying raml theoretical empirical perspectives. pioneering work norouzi showed raml optimize divergence exponentiated payoff distribution model distribution opposite directions. moreover applied log-linear model raml also shown equivalent softmax-margin training method nachum applied payoff distribution improve exploration properties policy gradient model-free reinforcement learning. despite efforts theoretical properties raml especially interpretation behavior exponentiated payoff distribution largely remained under-studied first raml attempts match model distribution heuristically designed exponentiated payoff distribution whose behavior largely remained under-appreciated resulting non-intuitive asymptotic property. second direct theoretical proof showing raml deliver prediction function better third attempt made improve raml algorithmic practical perspectives. paper attempt resolve above-mentioned under-studied problems providing theoretical interpretation raml. contributions three-fold theoretically introduce framework softmax q-distribution estimation able interpret role payoff distribution plays raml speciﬁcally softmax q-distribution serves smooth approximation bayes decision boundary. comparing payoff distribution softmax q-distribution show raml approximately estimates softmax q-distribution therefore approximating bayes decision rule. hence theoretical results provide explanation distribution raml asymptotically models prediction function provided raml outperforms provided algorithmically propose softmax q-distribution maximum likelihood improves raml achieving exact bayes decision boundary asymptotically. experimentally experiment using synthetic data multi-class classiﬁcation using real data image captioning verify theoretical analysis showing sqdml consistently good better raml task-speciﬁc metrics desire optimize. additionally three structured prediction tasks natural language processing rewards deﬁned sequential tree-based complex irregular structures deepen empirical analysis norouzi showing raml consistently leads improved performance task-speciﬁc metrics yields better exact match accuracy throughout uppercase letters random variables lowercase letters realizations corresponding random variables. input desired structured output e.g. machine translation french english sentences resp. assume possible outputs ﬁnite. instance machine translation english sentences maximum length. denotes task-speciﬁc reward function evaluates predicted output ground-truth denote true distribution data i.e. training samples usually i.i.d. samples denote parametric statistical model indexed parameter parameter space. widely used parametric models conditional log-linear models deep neural networks parametric statistical model learned given input model inference performed ﬁnding output achieving highest conditional probability indicator function. attempts learn conditional model distribution close conditional empirical distribution possible pˆθml theoretically certain regularity conditions asymptotically pˆθml converges true distribution since converges proposed norouzi raml incorporates task-speciﬁc rewards re-weighting log-likelihood possible candidate output proportionally exponentiated scaled reward empirical distribution discussed norouzi globally optimal solution raml achieved learned model distribution matches exponentiated ﬁxed payoff distribution i.e. pˆθraml value open problems raml identify three open issues theoretical interpretation raml distributions deﬁned output space though pˆθraml former conditioned input latter conditioned output appears serve ground-truth sampled data distribution makes behavior raml attempting match unintuitive; supposing training data exist training instances input different outputs i.e. targets making unclear pˆθraml iii) rigorous theoretical distribution pˆθraml yields better prediction function generating evidence showing generating pˆθraml pˆθml best knowledge attempt made theoretically address problems. main goal work theoretically analyze properties raml hope eventually better understand answering questions improve proposing training framework. next section introduce softmax q-distribution estimation framework facilitating later analysis. goal theoretically interpreting raml mind section present softmax q-distribution estimation framework. ﬁrst provide background bayesian decision theory softmax approximation deterministic distributions then propose softmax q-distribution establish framework estimating softmax q-distribution training data called softmax q-distribution maximum likelihood analyze sqdml central linking raml softmax q-distribution estimation. bayesian decision theory fundamental statistical approach problem pattern classiﬁcation quantiﬁes trade-offs various classiﬁcation decisions using probabilities rewards accompany decisions. based notations setup denote possible prediction functions input output space i.e. then expected reward prediction function reward function accompanied structured prediction task. bayesian decision theory states global maximum i.e. optimal expected prediction reward achieved prediction function so-called bayes decision rule called conditional reward. thus bayes decision rule states maximize overall reward compute conditional reward output select output maximized. importantly reward function indicator function i.e. bayes decision rule reduces speciﬁc instantiation called bayes classiﬁer true conditional distribution data deﬁned attempts learn true distribution thus optimal case decoding produces bayes classiﬁer distribution learned i.e. pˆθml general bayes decision rule rest section derive theoretical proof showing decoding distribution learned raml i.e. pˆθraml approximately achieves illustrating raml yields prediction function improved performance towards optimized reward function aimed providing smooth approximation bayes decision boundary determined bayes decision rule ﬁrst describe widely used approximation deterministic distributions using softmax function. denote class functions assume ﬁnite. then deﬁne random variable argmaxk∈k input random variable. obviously deterministic given i.e. ready propose softmax q-distribution central revealing relationship raml bayes decision rule. ﬁrst deﬁne random variable argmaxy∈y then deterministic given according deﬁne softmax q-distribution approximate conditional distribution given making predictions according softmax q-distribution equivalent bayes decision rule would like construct statistical model directly model softmax q-distribution similarly models true data distribution call framework softmax q-distribution maximum likelihood framework modelagnostic probabilistic model used conditional log-linear models deep neural networks directly applied modeling softmax q-distribution. suppose parametric statistical model model softmax q-distribution. order learn optimal parameters training data intuitive well-motivated objective function kl-divergence empirical conditional distribution denoted model distribution directly leaves problem deﬁning empirical conditional distribution deﬁning ﬁrst note deﬁned empirical distribution asymptotically converges true q-distribution learned model distribution ideally pˆθsqdml achieves bayes decision rule straightforward deﬁne empirical distribution unfortunately empirical distribution efﬁcient compute since expectation term inside exponential function leads seek approximation softmax q-distribution corresponding empirical distribution. propose following distribution approximate softmax q-distribution deﬁned exponentiated payoff distribution raml equation states raml approximation proposed sqdml approximating interestingly mostly practice input unique training data i.e. s.t. resulting ˆθsqdml ˆθraml. states estimated distribution pˆθsqdml exactly input unique training data since empirical distributions estimated training data same. provided theoretical interpretation raml establishing relationship raml sqdml. section answer questions raml raised using interpretation analyze level approximation softmax q-distribution proving upper bound approximation error. let’s ﬁrst interpretation answer three questions regarding raml first instead optimizing divergence artiﬁcially designed exponentiated payoff distribution model distribution raml formulation approximately matches model distribution softmax q-distribution second based interpretation asymptotically raml learns distribution converges therefore approximately converges softmax q-distribution. third mentioned generating softmax q-distribution produces bayes decision rule theoretically outperforms prediction function w.r.t. expected reward. necessary mention raml sqdml trying learn distributions decoding delivers bayes decision rule. directions also achieve bayes decision rule minimum bayes risk decoding attempts estimate bayes decision rule directly computing expectation w.r.t data distribution learned training data. discussion concentrated theoretical interpretation analysis raml without concerns well approximates characterize approximating error proving upper bound divergence them theorem given input output random variable data distribution suppose reward function bounded softmax q-distribution approximation deﬁned assume then theorem observe level approximation mainly depends factors upper bound reward function temperature parameter practice often less equal metrics like accuracy bleu applied. noted that extreme becomes larger approximation error tends zero. time however softmax q-distribution becomes closer uniform distribution unif providing less information prediction. thus practice necessary consider trade-off approximation error predictive power. extreme close zero possible? suitable assumptions data distribution characterize approximating error using divergence theorem suppose reward function bounded constant. suppose additionally that like subgaussian every satisﬁes exponential tail bound w.r.t. exists unique every section performed sets experiments verity theoretical analysis relation sqdml raml. discussed raml sqdml deliver predictions input unique data. thus order compare sqdml raml ﬁrst experiments designed data sets unique synthetic data cost-sensitive multi-class classiﬁcation mscoco benchmark dataset image captioning. conﬁrm advantages raml thus necessity better theoretical understanding performed second experiments three structured prediction tasks nlp. cases sqdml reduces raml input unique three data sets. first perform experiments synthetic data cost-sensitive multi-class classiﬁcation designed demonstrate raml learns distribution approximately producing bayes decision rule asymptotically prediction function delivered sqdml. synthetic data -class classiﬁcation task deﬁne four base points class build data multiple references. thus total number training instances million. validation test data independently generate million pairs respectively. model used feed-forward neural networks hidden layers units. optimization performed mini-batch stochastic gradient descent learning rate momentum model trained epochs apply early stopping based performance validation sets. reward function designed distinguish four classes. correct predictions speciﬁc reward values assigned four classes wrong predictions rewards always zero i.e. figure depicts effect varying temperature parameter model performance ranging step ﬁxed report mean performance repetitions. figure shows averaged rewards obtained function validation test datasets raml sqdml respectively. figure increases performance sqdml raml keeps decreasing indicting raml achieves better approximation sqdml. evidence verities statement theorem approximating error raml sqdml decreases continues grow. results figure raise question larger necessarily yield better performance raml? illustrate effect model performance raml sqdml perform experiments wide range step also repeat experiment times. results shown figure model performance however kept growing increasing discussed softmax q-distribution becomes closer uniform distribution becomes larger making less expressive prediction. thus applying raml practice considerations regarding trade-off approximating error predictive power model needed. details results analysis conducted experiments provided appendix second show optimizing toward proposed sqdml objective yields better predictions raml real-world structured prediction tasks evaluate mscoco image captioning dataset. dataset contains images paired least manually annotated captions. follow ofﬂine evaluation setting reserve images validation testing respectively. implemented simple neural image captioning model using pre-trained vggnet encoder long short-term memory network decoder. details experimental setup appendix sake comparing sqdml raml verify theoretical analysis average reward performance measure simply deﬁning reward pairwise sentence level bleu score model’s prediction reference caption though standard benchmark metric commonly used image captioning simply deﬁned averaging pairwise rewards prediction reference captions. stochastic gradient descent optimize objectives sqdml raml however denominators softmax-q distribution sqdml payoff distribution raml contain summations intractable exponential hypotheses space therefore propose simple heuristic approach approximate denominator restricting exponential space using ﬁxed sampled targets i.e. approximating intractable hypotheses space using sampling structured prediction shown effective optimizing neural structured prediction models speciﬁcally sampled candidate constructed including ground-truth reference uniformly replacing n-gram reference randomly sampled n-gram. refer approach n-gram replacement. provide details training procedure appendix table lists results. evaluate average reward benchmark metric also tested vanilla baseline achieves average reward corpus-level bleu. sqdml raml outperform according metrics. interestingly comparing sqdml raml observe signiﬁcant improvement average reward. hypothesize fact reference captions image largely different making highly non-trivial model predicate consensus caption agrees multiple references. example randomly sampled images validation compute averaged sentence-level bleu references nevertheless case studies still found interesting examples demonstrate sqdml capable generating predictions match multiple candidates. figure gives examples. examples sqdml’s predictions match multiple references registering highest average reward. hand raml gives sub-optimal predictions terms average reward since approximation sqdml. ﬁnally since objective solely maximizing reward w.r.t single reference gives lowest average reward achieving higher maximum reward. norouzi already evaluated effectiveness raml sequence prediction tasks speech recognition machine translation using neural sequence-to-sequence models. section conﬁrm empirical success raml apply raml three structured prediction tasks including named entity recognition dependency parsing machine translation using classical feature-based log-linear models state-of-the-art attentional recurrent neural networks different norouzi edit distance uniformly used surrogate training reward learning objective approximated sampling task-speciﬁc rewards deﬁned sequential tree-based complex irregular structures speciﬁcally instead sampling apply efﬁcient dynamic programming algorithms directly compute analytical solution present analysis comparing raml showing different learning objectives raml registers better results task-speciﬁc metrics yields better exact-match accuracy. named entity recognition experimented english data conll shared task four predeﬁned types named entities person location organization misc. dataset includes training sentences validation testing. built linear model features used finkel instead using ofﬁcial score complete span predictions token-level accuracy training reward metric factorized word hence exists efﬁcient dynamic programming algorithm compute expected log-likelihood objective dependency parsing dependency parsing evaluate english penn treebanks follow standard splits using sections training section validation testing. adopt stanford basic dependencies using stanford parser v... applied data preprocessing procedure dyer adopt edge-factorized tree-structure log-linear model features used zhao unlabeled attachment score training reward also ofﬁcial evaluation metric parsing performance. similar expectation computed deﬁciently using dynamic programming since factorized edge. machine translation tested german-english machine translation task iwslt evaluation campaign widely-used benchmark evaluating optimization techniques neural sequence-to-sequence models. dataset contains training sentence pairs. follow previous works attentional neural encoder-decoder model lstm networks. size lstm hidden states similar sentence level bleu score training reward approximate learning objective using n-gram replacement evaluate using standard corpus-level bleu. results dependency parsing shown table table respectively. observed raml model obtained best results dependency parsing. beyond raml models worse baseline tasks showing practice selection temperature needed. addition rewards directly optimized training stable w.r.t. evaluation metrics illustrating practice choosing training reward correlates well evaluation metric important. table summarizes results also compare model previous works incorporating task-speciﬁc rewards optimizing neural sequence-to-sequence models approach albeit simple surprisingly outperforms previous works. speciﬁcally previous methods require pre-trained baseline initialize model raml learns scratch. suggests raml easier stable optimize compared existing approaches like bahdanau requires sampling moving model distribution suffers high variance. finally remark raml performs consistently better across temperature terms. table illustrates performance raml different metrics three tasks. observe raml outperforms directly optimized rewards task-speciﬁc evaluation metrics interestingly trend gets better results three tasks exact match accuracy reward attempts optimize line theoretical analysis raml achieve better prediction functions w.r.t. corresponding rewards optimize. work propose framework estimating softmax q-distribution training data. based theoretical analysis asymptotically prediction function learned raml approximately achieves bayes decision rule. experiments three structured prediction tasks demonstrate raml consistently outperforms baselines. dzmitry bahdanau philemon brakel kelvin anirudh goyal ryan lowe joelle pineau aaron courville yoshua bengio. actor-critic algorithm sequence prediction. proceedings iclr toulon france mauro cettolo niehues sebastian stuker luisa bentivogli marcello federico. report iwslt evaluation campaign iwslt proceedings international workshop spoken language translation xinlei chen fang tsung-yi ramakrishna vedantam saurabh gupta piotr dollár lawrence zitnick. microsoft coco captions data collection evaluation server. corr abs/. chris dyer miguel ballesteros wang ling austin matthews noah smith. transition-based dependency parsing stack long short-term memory. proceedings beijing china july jenny rose finkel trond grenager christopher manning. incorporating non-local information information extraction systems gibbs sampling. proceedings arbor michigan june mohammad norouzi samy bengio navdeep jaitly mike schuster yonghui dale schuurmans reward augmented maximum likelihood neural structured prediction. proceedings nips barcelona spain sang tjong erik fien meulder. introduction conll- shared task languageindependent named entity recognition. proceedings conll- volume edmonton canada yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey google’s neural machine translation system bridging human machine translation. arxiv preprint arxiv. figure decision boundaries different models together bayes decision rule display decision boundary decision boundaries raml achieves best performance corresponding boundaries sqdml. best performance achieved better illustrate properties raml sqdml display decision boundary learned models figure figure gives boundary bayes decision rule figure boundary that expected gives unbiased boundary incorporating information task-speciﬁc reward. figure decision boundaries raml sqdml that even small sqdml able achieve good decision boundary similar bayes decision rule boundary raml similar might suggest raml approximation sqdml might degenerates approximation error. figure decision boundaries raml sqdml large that consistently matching analysis neither raml sqdml learn reasonable prediction function. reason discussed becomes larger softmax encoder following adopt widely-used architecture vggnet image encoder. speciﬁcally last fully connected layer image representation decoder generate captions. pre-trained vggnet model keep ﬁxed training decoder. decoder lstm network decoder predicate sequence target words yt}. formally decoder uses internal hidden state time step track generation process deﬁned embedding previous word yt−. initialize memory cell decoder passing ﬁxed-length image representation afﬁne transformation layer. probability target word given training n-gram replacement discussed approximate exponentially large hypotheses space using subset sampled hypotheses formally training consists pairs images multiple references {y∗} i.e. {x{y∗}}. raml split single training instance x{y∗} multiple ones pairing i.e. y∗}. instance maximize experiments size sampled targets sqdml raml. sake efﬁciency iteration stochastic gradient descent randomly-selected hypotheses perform gradient update. sqdml raml. conﬁguration sentence-level bleu nist geometric smoothing reward. replace word types whose frequency less special <unk> token. resulting vocabulary size dimensionality word embeddings lstm hidden sates respectively. decoding beam search beam size batch size baseline larger size sqdml raml sake efﬁciency. optimize need efﬁciently compute objective gradient. next sections feature reward follow certain factorizations efﬁcient dynamic programming algorithms exist. dependency parsing represents generic dependency tree consists directed edges heads dependents edge-factorized model factorizes potential function edges model overview apply neural encoder-decoder model attention input feeding given source sentence words {xi}n conditional probability target sentence {yi}t probability computed using bi-directional lstm encoder lstm decoder encoder denote embedding i-th source word unidirectional lstms process forward backward order sequence hidden states {hi}n hi}n representation i-th word given concatenating decoder lstm used decoder predict target word time step formally decoder maintains hidden state track translation process deﬁned denotes vector concatenation embedding previous target word. initialize ﬁrst memory cell decoder using last hidden states encoding lstms cell ﬁrst hidden state decoder initialized tanh. attentional vector computed conﬁguration pre-processed dataset wiseman rush vocabulary size german english data words resp. similar bahdanau dimensionality word embeddings lstm hidden states neural network parameters uniformly initialized adam optimizer. validate perplexity development every epoch halve learning rate validation performance drops. sentence level bleu nist geometric smoothing training reward ofﬁcial multi-bleu.perl script evaluating corpus-level bleu. beam size decoding batch size baseline larger size raml sake efﬁciency. approximating learning objective using importance sampling suggested norouzi bleu training reward objective function raml could optimized using importance sampling. verify this conducted experiment using importance sampling. since cannot directly sample exponentiated payoff distribution parameterized bleu score payoff distribution negative hamming distance proposal distribution sample y∼qhm ˜qbleu|y∗; )/˜qhm|y∗; exp{bleu y∗)/τ}/ exp{hm y∗)/τ} ˜q’s denote payoff distributions without normalization terms. bleu denote sentence-level bleu score negative hamming distance respectively. sample size draw sample follow norouzi apply stratiﬁed sampling. ﬁrst sample distance sample sentence hamming distance denote number length hamming distance ground-truth deﬁned vocabulary size. table lists performance importance sampling different temperatures. best model comparable achieved n-gram replacement. however n-gram replacement much simpler implement importance sampling requires extra computation proposal distribution associated importance weights would less computationally efﬁcient. experiments highly optimized raml model achieves training speed words/sec importance sampling words/sec n-gram replacement. extra experiments using negative hamming distance training reward sake completeness also experimented using negative hamming distance reward function raml proposed norouzi results listed table best model gets corpus-level bleu score worse best results achieved optimizing directly towards bleu scores", "year": 2017}