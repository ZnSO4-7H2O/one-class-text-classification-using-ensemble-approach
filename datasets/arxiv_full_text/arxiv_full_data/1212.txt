{"title": "Deep learning is a good steganalysis tool when embedding key is reused  for different images, even if there is a cover source-mismatch", "tag": ["cs.MM", "cs.CV", "cs.LG", "cs.NE"], "abstract": "Since the BOSS competition, in 2010, most steganalysis approaches use a learning methodology involving two steps: feature extraction, such as the Rich Models (RM), for the image representation, and use of the Ensemble Classifier (EC) for the learning step. In 2015, Qian et al. have shown that the use of a deep learning approach that jointly learns and computes the features, is very promising for the steganalysis. In this paper, we follow-up the study of Qian et al., and show that, due to intrinsic joint minimization, the results obtained from a Convolutional Neural Network (CNN) or a Fully Connected Neural Network (FNN), if well parameterized, surpass the conventional use of a RM with an EC. First, numerous experiments were conducted in order to find the best \" shape \" of the CNN. Second, experiments were carried out in the clairvoyant scenario in order to compare the CNN and FNN to an RM with an EC. The results show more than 16% reduction in the classification error with our CNN or FNN. Third, experiments were also performed in a cover-source mismatch setting. The results show that the CNN and FNN are naturally robust to the mismatch problem. In Addition to the experiments, we provide discussions on the internal mechanisms of a CNN, and weave links with some previously stated ideas, in order to understand the impressive results we obtained.", "text": "lionel pibre j´er ˆome pasquet dino ienco marc chaumont universite nimes nˆımes cedex france universite montpellier umr-lirmm montpellier cedex france cnrs umr-lirmm montpellier cedex france {lionel.pibre jerome.pasquet dino.ienco marc.chaumont}lirmm.fr analyzing empirical security embedding algorithm laboratory environment select clairvoyant scenario i.e. suppose steganalyst knows algorithm payload size used steganograph good knowledge cover distribution images type used steganograph. clairvoyant scenario best classiﬁer ensemble classiﬁer classiﬁer able treat high dimensional vectors easily parallelizable smaller computational complexity svm. moreover improvements proposed order increase efﬁciency embedding probabilities order better steganalyze adaptive algorithms tuning false alarm probability treating cover-source mismatch problem best classiﬁer also ensemble classiﬁer steganalysis ﬁeld qian proposed deep learning replace traditional step approach. article qian obtained detection percentage lower obtained ensemble classiﬁer features tested algorithms hugo s-uniward bossbase database ﬁrst results encouraging since study proof concept feature vector dimension compare favorably respect deep learning. indeed dimension feature vector last convolution layer provides features whereas dimension feature vector provides article pursue study steganalysis deep learning scenario steganograph always uses embedding embedding different images simulator embedding. many this scenario recommended scenario scenario steganograph weakens security embedding algorithm. note practical error easily occur example using version s-uniward downloadable binghamton since boss competition steganalysis approaches learning methodology involving steps feature extraction rich models image representation ensemble classiﬁer learning step. qian shown deep learning approach jointly learns computes features promising steganalysis. paper follow-up study qian show scenario steganograph always uses embedding embedding simulator different images intrinsic joint minimization preservation spatial information results obtained convolutional neural network fully connected neural network well parameterized surpass conventional first numerous experiments conducted order best shape cnn. second experiments carried clairvoyant scenario order compare results show reduction classiﬁcation error fnn. third experiments also performed cover-source mismatch setting. results show naturally robust mismatch problem. addition experiments provide discussions internal mechanisms weave links previously stated ideas order understand results obtained. also discussion scenario same embedding key. ﬁrst step requires extraction features describing image. features must diverse means capture maximum information modeling image also complete means values different cover stego. best feature represent image supplied rich models layer made neurons take input values computations returns values supplied next layer. precisely inside layer computations done three successive steps convolution step application activation function pooling step note outputs layer could considered images. terminology image named feature map. fig. representing qian network ﬁrst layer generates feature maps size note means ﬁlters thus convolutions applied input image size second ﬁfth layer three steps convolution activation pooling time convolutions applied feature maps. discuss detail convolutions sub-sampling fulﬁlled next subsection. last convolution layer connected fully connected layer neuronal network. then softmax function connected outputs last layer order normalize outputs delivered network softmax function gives predicted probability belonging class knowing weighted outputs last layer. thus network delivers values output giving probability classifying ﬁrst class giving probability classifying second class classiﬁcation decision obtained returning class highest probability. convolutional neural network similar classical neuron network needs long learning time order tune unknown parameter. case qian network number unknown parameters close indeed large since convergence requires less hours programming nvidia tesla learning achieved well known back-propagation algorithm. roughly speaking back-propagation error equivalent gradient descent well-known function optimization technique. network learning thus seen optimization function lots unknown parameters well thought stochastic gradient descent. huge number parameters learn neural network needs database considerable number examples order converge. moreover database examples must diverse enough obtain good generalization network. network built different qian section review major concepts convolutional neural network. section introduce experimental settings describe shape best present steganalysis results scenarios clairvoyant cover-source mismatch. finally section discuss link network construction steganalysis research explain results. neural networks studied since ﬁfties. initially proposed model brain behavior. computer science especially artiﬁcial intelligence used years learning purposes. recently neural networks considered long learning time less efﬁcient modern classiﬁers. recently recent advances neural network ﬁeld computational power supplied gpus deep learning approaches proposed natural extension neural networks getting popular high classiﬁcation performance. deep learning networks neural networks directly take data input. image processing network directly pixels. deep learning network handles steps since many adjustments proposed improve robustness reduce computational costs. paper recall major concepts convolutional neural networks deep learning network proved efﬁciency image classiﬁcation competitions used qian steganalysis purposes. learning methodology similar classical one. image database needed with image label image given network input; case pixel value taken input many neurons. network made given number layers. layer consists neurons take input values computations returns values supplied next layer. illustration figure gives network used qian network image size ﬁrst ﬁltered high-pass ﬁlter whose kernel denoted size note preliminary step speciﬁc steganalysis problem. observed cnns converge much slower without preliminary high-pass ﬁltering. then ﬁltered image size given ﬁrst layer. qian network convolution layers. convolution. similar operation classical feature extraction process supplied rich models best knowledge spatio-frequential decomposition. figure gives example ﬁlter kernels obtained second layer efﬁcient network obtained. remember convolution layer made three steps i.e. convolution activation pooling. three consecutive steps summarized looking link feature layer previous note ﬁlter kernels bias learned modiﬁed back-propagation error. thus layer ...l} number ﬁlters layer convolution kernel made weights total number unknown parameters tuned back-propagation convolutional layer part. example qian network number parameters coming convolution layers equal ﬁlters unknown parameters coming convolution weights bias denote image given high-pass ﬁltered image; section figure denote ﬁlter layer ...l} beeing number convolutional layers ...k} beeing number ﬁlters layer also number feature outputs layer). convolution ﬁrst layer ﬁlter leads ﬁltered image denoted second layer last convolution layer convolution less classical since feature maps images) input denoted ...k}. convolution lead ﬁltered image resulting convolution layer numbered fact convolutions that number convolutions layer size feature layer number weights convolution. note complexity take account activation operation cost pooling operation cost normalization operation cost. computations classical accelerated example fast convolution fourier domain parallel computation. thus using network size input image number ﬁlters size ﬁlters number layers convolution computation cost pooling normalization included less o×i×|f high. example qian network complexity convolution part network would give less additions multiplications less mega operations. small example intel core compute gigaflops nvidia tesla compute teraflops. even looking entire network computational cost high. long learning times fact operations done database scanned many times order back-propagation process converge network. convolution convolution layer applied activation function applied value ﬁltered image function named activation function reference notion binary activation ﬁrst network neuron deﬁnition. activation function example absolute function sine function sinus gaussian function qian network relu etc... functions break linearity property resulting linear ﬁltering done convolutions. usually interesting property exploited ensemble classiﬁer majority vote also used rich models min-max features choice activation function linked classiﬁcation problem. example qian proposed unusual gaussian function. note chosen function derivable order compute back-propagation error. derivative could less computationally costly impact learning time. choice activation function thus often guided pooling operation consists computing average maximum local neighborhood. object classiﬁcation ﬁeld operation especially maximum ensures translation invariance features. introduced order reduce variance obtained convolution. qian propose average operation stego noise small. also empirically validated fact experiments. results obtained using average outperform obtained maximum operation. moreover pooling coupled sub-sampling operation order reduce size obtained feature comparison size previous layer. article qian reduction factor four feature size layer previous layer. seen classical down-sampling preliminary pass ﬁltering. useful reduce memory occupation gpu. nevertheless step similar denoising signal processing point view induces information loss. pooling step seem interesting steganalysis scenario steganograph always uses embedding embedding simulator different images indeed experimentally observed suppressing pooling operation increased classiﬁcation results return suppressing pooling step gives feature maps constant size layers leads increase computational cost increase memory consumption. experiments ﬁrst took database bossbase consisting grey-level images size coming different cameras split image four order obtain images size named database cropped bossbase database. article qian also reduced images sizes memory limitation. note applied image resizing instead image cropping. researchers free-access database required cite follows lirmmbase database built columbia dresden photex raise databases whose images come cameras bossbase database ﬁrst time used pibre pasquet ienco chaumont lirmm laboratory montpellier france june website www.lirmm.fr/~chaumont/lirmmbase.html. lionel pibre j´erˆome pasquet dino ienco marc chaumont deep learning good steganalysis tool embedding reused different images even cover source-mismatch proceedings media watermarking security forensics part is&t international symposium electronic imaging francisco california feb. pages. cameras none present bossbase images camera. database allows evaluation cover-source mismatch phenomenon. note readers different versions lirmmbase hosting website color version lirmmbasecolor images grey-level image database lirmmbasex images lirmmbasex database grey level images. images come well-known databases i.e. columbia dresden photex raise databases. note used script used bossbase order transform full resolution color images grey-level images. formed selection channel same embedding scenario. moreover paper also shows exhibits invariance property respect cover-source mismatch scenario. tested many efﬁcient network obtained consisted convolutional layers followed three layer fully connected network. figure illustrates network. input image size ﬁrst high-pass ﬁltered ﬁlter qian size ﬁltered image thus ﬁrst layer ﬁlters size applied. figure illustrates ﬁlters learning images cropped bossbase. already stated ﬁlters seem oriented band-pass ﬁlters. note that memory constraints convolution applied pixel lines columns leads ﬁltered images size note comparison qian network drastically increased number ﬁlters ﬁrst layer reduced number layers. increase height reasons network generates better results compared qian architecture. increase layer number leads loss information probably negative impact pooling step convolutions relu activation function applied function forces values positive. note gaussian activation function article qian improve results. already discussed activation breaks linearity successive convolutions applied traverse convolutional layers. note suppressed pooling step counter-productive. feature maps returned ﬁrst layer thus size finally last process done layer normalization feature maps applied ...k} value position normalization done across maps ﬁrst layer convolution ﬁlter size image size virtually leads image size selected padding option stride value implies image down-sampling gives image size experiments embeded messages using simulator s-uniward bits pixel used stego key. embedding database obtained bossbase consisted images database lirmmbase consisted images limited experiments payload size high number computations experiments cnn. tested many parameter variations. figure illustrates different values probability error tested cnn. high number parameters necessary high number iterations back-propagation process order converge. experiments number times database scanned learning database consisting images size leads learning time less efﬁcient double precision card market june i.e. nvidia tesla thus takes one-month computation order best network. reproducibility experiments main parameters given mini-batch size moment learning coefﬁcient weights bias weight decay convolutions layers fully connected network drop activated. script converts full resolution color images images resizes images smaller side pixels long crops images ﬁnally converts greyscale -bit images; script found www.lirmm.fr/ ~chaumont/lirmmbase.html. size parameter default values. feature maps size entering second layer padded order obtain feature maps size next convolutions applied explained section equation recalls particular convolution step convolutions seen research local signals feature maps computation correlations researched signals feature maps. step normalization steps probably explain robustness cover-source mismatch observed experiments. figure illustrates ﬁlters learning images cropped bossbase. relu activation employed applying convolutions ﬁlters normalization performed again. leads feature maps size each. note concatenating feature maps values leads feature vector consisting features times consists features. convolution layers fully connected three layer neural network. ﬁrst second layers consist neurons last layer neurons. operations carried ﬁrst second layers products bias additions applications relu activation function. operations performed last layer products bias additions softmax order rescale output values ﬁrst test clairvoyant scenario. scenario forward hypothesis steganalyst knows embedding algorithm good knowledge statistical distribution image database used steganograph knows relative payload size. scenario almost matches kerckhoffs principle. steganalyst knows public parameters know private parameters embedding secret key. scenario laboratory scenario used empirically assess security steganographic embedding algorithm additionally make hypothesis normalization done local response normalization layer. enables detection high-frequency features high neuron response damping responses uniformly large local neighborhood. type regularizer encourages competition activities among nearby groups neurons. cudaconvnet documentation. steganograph made error always secret embedding thus steganalyst access cover/stego images pairs stego images generated secret embedding key. note embedding done simulator using always key. tests carried cropped bossbase database consists grey-level images bits whose size embed s-uniward bits pixels. obtained images thus made images limited experiments single payload size high number computations high number experiments cnns. three steganalysis approaches evaluated. ﬁrst steganalysis done using rich models ensemble classiﬁer rich models whose dimension denote steganalysis rm+ec rich models ensemble classiﬁer. second steganalysis done using efﬁcient built denote steganalysis convolutional neural network. third steganalysis done using fully connected neural network denote steganalysis fully-connected neural network. payload size conducted tests where test learning done images randomly taken images covers stegos always paired. tests performed images randomly taken remaining images. ensemble classiﬁer decision threshold base learner adjusted minimize total detection error equal priors training set. errors minimized training used different ensemble classiﬁers. given binghamton website; case learning consists images written proposed paper features selection parallelization options; case learning consists images. cases results similar report results obtained binghamton code. rich models ensemble classiﬁer gives probability error whereas gives probability error gives probability error thus improvement using fnn. impressive improvement considering difﬁculty grab percentages probability error steganalysis. ﬁrst tentative explanation good behavior already given previous sections. generates better results rich models associated ensemble classiﬁer following reasons vii) security error lack understanding steganograph security issues related key. security embedding path always embedding simulator always uses pseudo-random number sequence generating change probabilities. steganalysis task thus easier steganalyzer sensitive spatial content also carried additional experiment conﬁrmed ﬁrst layers play strong role classiﬁcation. experiment ﬁrst consisted cutting another smaller previously learned keeping convolution layers. network nothing feature extractor; experiment features. second feature extractor used extract learning database feature vector image. third ensemble classiﬁer used feature vectors learn model. probability error computed averaged tests. observed ensemble classiﬁer learned features coming smaller allowed reduce average probability error w.r.t. smaller cnn. indicated that feature extraction interesting part fully connected classiﬁer part necessarily best classiﬁer. note smaller allowed obtain improvement compared rm+ec. would also like comment adaptive scenario adaptive steganalysis also named selection-channel-aware steganalysis using adaptive steganalysis steganalyst uses estimation modiﬁcation probability pixel additional information learning order distinguish cover stego. scenario authors report detectability improvement ensemble classiﬁer detecting s-uniward bossbase database. improvement less results comparable since images smaller increment really minor compared increment. also mention obtained results clairvoyant scenario steganograph uses different embedding. probability error result probably fact able stego pattern. result improved increasing numbers ﬁlters. experimented test titanx card digits library ﬁlters ﬁrst layer ﬁlters second neurons ﬁrst layer fully connected network second layer. probability error reduced increasing size allows improve efﬁciency network performance moment state second test gave interesting results case coversource mismatch. cover-source mismatch phenomenon occurs sources model obtained learning step differs sources used steganograph. geometrical point view explain inconsistency problem fact cloud describing images used steganalyst located place cloud describing images used steganograph. papers assessed cover-source mismatch practice nevertheless satisfactory solution currently available even though attempts understand phenomenon case subsampled cropped bossbase times thus obtaining different training sets. training built applied classiﬁcation model lirmmbase test database report average error probability trials. note cover-source mismatch present since none bossbase cameras lirmmbase. note also used script used bossbase order generate grey-level images. moreover original images used build lirmmbase uncompressed came three known databases dresden raise columbia. average probability error rm+ec given table noise) spatio-frequential decomposition role second layer detect presence particular patterns spatio-frequential bands could explain invariance image content thus invariance cover model. additionally robustness also explained fact specialized researching spatial pattern probability change strongly occurring same embedding scenario. figure illustrates probability change obtained counted many times pixel changed stego images cropped bossbase database. observe pixels never change stego images change almost certainly. observed cover-source mismatch issue seriously affected performance rm+ec classiﬁer. results close obtained random classiﬁer. return showed incredible robustness mismatch phenomenon probability error. gave similar results probability error. experiments bowsbase also conﬁrmed robust cover-source mismatch phenomenon whereas rm+ec not. bowsbase nevertheless practical database since used cameras unknown since cameras also used bossbase. surprisingly probability error lirmmbase lower cropped bossbase fewer texture images lirmmbase cropped bossbase. lirmmbase thus easier steganalyze. invariance cover-source mismatch good mismatch phenomenon longer present thus obtained steganalysis results related content complexity data-base robustness probably transformation done ﬁrst layers thus feature extractor ﬁrst layers gives features robust cover-source mismatch. features probably invariant means feature representation obtained second layer sensitive different image statistics. assuming role ﬁrst layer decompose noise signal result slightly better rm+ec better obtained cropped bossbase database ﬁlters ﬁrst layer ﬁlters second neurons ﬁrst layer fully connected network second layer probability error rm+ec. order demystify already explained learning equivalent minimization function many unknown parameters technique similar gradient descent. subsection make links previous research topic. important step process convolution detailed sections learning ﬁlter kernels done minimization classiﬁcation error using back-propagation procedure. thus simple optimization ﬁlter kernels. strategy already shown efﬁciency paper ﬁlter kernel values used computing feature vector obtained optimization downhill simplex algorithm. objective minimize probability error given ensemble classiﬁer. learning achieved convolution layers shares idea leads customized kernels well suited steganalysis purpose. looking precisely ﬁrst layer kernel seems multi-band ﬁltering recent articles spatio-frequential decomposition order compute rich models using gabor ﬁlters ﬁlters ﬁlters used deﬁne projections used computing histogram leading feature vector. looking precisely second convolution layer applies unusual convolution approach nothing similar could found recent papers dealing feature extraction histogram computation non-uniform quantization feature selection dimension reduction etc. nevertheless something important occurring second layer allows obtain features unsensitive cover-source mismatch. looking equation role second layer looks like searching patterns multiple bands. convolutions equation accumulate presence clues searched signal subands. second layer outputs maps giving clues signal presence. close discussion also last treatment layer normalization step type processing also found papers normalization done order obtain comparable output values neuron. also mention analysis activation function requiered completely understand impact. activation introduced nonlinearity also case ensemble classiﬁer majority vote rich models min-max features order conclude discussion mention papers quach studied payload location estimation problem hypothesis embedding reused. scenario different since used forensic-steganalysis. classical steganalysis steganalyst embedded message. forensic-steganalysis steganalyst often guess embedding occurred tries recover information payload size embedding location secret keys embedded message. quach studies clairvoyant scenario adds three additional hypothesis steganograph always uses images size steganograph always uses steganalyst access stego images steganalyst knows images stego. image steganalysis method order estimate covers. stego images accumulate clues payload location. quach proposition payload location estimated replacement algorithm matching algorithm replacement group-parity steganography. using random markov model quach estimates modiﬁed locations stego image test then accumulating location clues test estimate sufﬁciently well payload location payload high number images enough independently scenario independently adaptive non-adaptive steganography observe articles article security weak error made steganograph consisting reuse key. stated before error leads easier localization suspect/modiﬁed pixels spatial pattern probability change learned approach recovered papers accumulating clues obtained stego images. knowledge spatial pattern probability change becomes easier apply classical steganalysis present article forensic-steganalysis payload localization article propose pursue study cnns steganalysis. tested cnns found right parameters steganalysis domain scenario steganograph reuses secret embedding embedding simulator images. instead using deep network proposed qian experiments network height consists convolutional layers. replaced unconventional gaussian activation function classical relu activation function suppressed pooling step acting down-sampling pre-processed images applying high pass ﬁltering feeding cnn. evaluated cnns different scenarios. ﬁrst tests done clairvoyant scenario. used cropped bossbase database embedding s-uniward bpp. compared state-of-the-art approach i.e. ensemble classiﬁer features reduce classiﬁcation error three fold. second tests done cover-source mismatch scenario. used bossbase s-uniward embedding learning tests carried public lirmmbase database. cover-source mismatch fully achieved since cameras different base another. conventional method totally failed detect steganography lirmmbase since classiﬁcation error almost random classiﬁcation. conversely exhibited natural invariance cover-source mismatch classiﬁcation error", "year": 2015}