{"title": "A Probabilistic Framework for Deep Learning", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We develop a probabilistic framework for deep learning based on the Deep Rendering Mixture Model (DRMM), a new generative probabilistic model that explicitly capture variations in data due to latent task nuisance variables. We demonstrate that max-sum inference in the DRMM yields an algorithm that exactly reproduces the operations in deep convolutional neural networks (DCNs), providing a first principles derivation. Our framework provides new insights into the successes and shortcomings of DCNs as well as a principled route to their improvement. DRMM training via the Expectation-Maximization (EM) algorithm is a powerful alternative to DCN back-propagation, and initial training results are promising. Classification based on the DRMM and other variants outperforms DCNs in supervised digit classification, training 2-3x faster while achieving similar accuracy. Moreover, the DRMM is applicable to semi-supervised and unsupervised learning tasks, achieving results that are state-of-the-art in several categories on the MNIST benchmark and comparable to state of the art on the CIFAR10 benchmark.", "text": "develop probabilistic framework deep learning based deep rendering mixture model generative probabilistic model explicitly capture variations data latent task nuisance variables. demonstrate max-sum inference drmm yields algorithm exactly reproduces operations deep convolutional neural networks providing ﬁrst principles derivation. framework provides insights successes shortcomings dcns well principled route improvement. drmm training expectation-maximization algorithm powerful alternative back-propagation initial training results promising. classiﬁcation based drmm variants outperforms dcns supervised digit classiﬁcation training faster achieving similar accuracy. moreover drmm applicable semi-supervised unsupervised learning tasks achieving results state-of-the-art several categories mnist benchmark comparable state cifar benchmark. humans adept wide array complicated sensory inference tasks recognizing objects image understanding phonemes speech signal despite signiﬁcant variations position orientation scale objects pronunciation pitch volume speech. indeed main challenge many sensory perception tasks vision speech natural language processing high amount nuisance variation. nuisance variations complicate perception turning otherwise simple statistical inference problems small number variables much higher-dimensional problems. challenge developing inference algorithm factor nuisance variation input. past decades vast literature approaches problem myriad different perspectives developed difﬁcult inference problems remained reach. recently breed machine learning algorithms emerged high-nuisance inference tasks achieving super-human performance many cases. prime example architecture deep convolutional neural network seen great success tasks like visual object recognition localization speech recognition part-of-speech recognition. success deep learning systems impressive fundamental question remains work? intuitions abound explain success. explanations focus properties feature invariance selectivity developed multiple layers others credit computational power amount available training data. however beyond intuitions coherent theoretical framework understanding analyzing synthesizing deep learning architectures remained elusive. paper develop theoretical framework provides insights successes shortcomings deep learning systems well principled route design improvement. framework based generative probabilistic model explicitly captures variation latent nuisance variables. rendering mixture model explicitly models nuisance variation rendering function combines task target variables collection task nuisance variables deep rendering mixture model extends hierarchical fashion rendering product afﬁne nuisance transformations across multiple levels abstraction. graphical structures drmm enable efﬁcient inference message passing training expectation-maximization algorithm. element framework relaxation rmm/drmm generative model discriminative order optimize bias-variance tradeoff. below demonstrate computations involved joint inference relaxed drmm coincide exactly dcn. intimate connection drmm dcns provides range insights work work. theory methods apply wide range different inference tasks feature number task-irrelevant nuisance variables concreteness exposition focus classiﬁcation problem underlying visual object recognition. proofs several results appear appendix. theories deep learning. theoretical work shares similar goals several others i-theory nuisance management scattering transform simple sparse network proposed arora hierarchical generative models. drmm closely related several hierarchical models including deep mixture factor analyzers deep gaussian mixture model like models drmm attempts employ parameter sharing capture notion nuisance transformations explicitly learn selectivity/invariance promote sparsity. however features distinguish drmm approach others drmm explicitly models nuisance variation across multiple levels abstraction product afﬁne transformations. factorized linear structure serves dual purposes enables tractable inference serves regularizer prevent overﬁtting exponential reduction number parameters. critically inference performed single variable interest instead full global conﬁguration nuisance variables. justiﬁed lownoise settings. importantly derive structure dcns precisely endowing operations convolution rectiﬁed linear unit spatial max-pooling principled probabilistic interpretations. independently work soatto also focus strongly nuisance management challenge deﬁning good scene representations. however work considers max-pooling relu approximations marginalized likelihood whereas work interprets operations differently terms max-sum inference speciﬁc probabilistic generative model. work number linear regions dcns complementary sheds light complexity functions compute. approaches could combined answer questions many templates required accurate discrimination? many samples needed learning? plan pursue questions future work. semi-supervised neural networks. recent work neural networks designed semi-supervised learning seen resurgence generative-like approaches ladder networks stacked what-where autoencoders many others. network architectures augment usual task loss regularization term typically including image reconstruction error train jointly. difference drmm-based approach networks arise proper probabilistic density must resort learning bottom-up recognition top-down reconstruction weights separately cannot keep track uncertainty. although focus drmm paper deﬁne explore several interesting variantsincluding deep rendering factor model evolutionary drmm discussed detail appendix. e-drmm particularly important since max-sum inference algorithm yields decision tree type employed random decision forest classiﬁer. figure graphical model depiction shallow rendering models drmm. dependence pixel location suppressed clarity. sparse sum-over-paths formulation drmm. rendering path contributes active here template function class nuisance switching variable off} determines whether render template particular patch; sparsity prior thus encourages patch causes. noise distribution exponential family without loss generality illustrate using gaussian noise assume noise i.i.d. function pixel location class nuisance variables independently distributed according categorical distributions. finally since world spatially varying image contain number different objects natural break image number patches centered single pixel described applies patch level depend pixel/patch location omit dependence clear context. inference shallow yields layer dcn. connect computations layer deep convolutional network perform object recognition must marginalize nuisance variables maximizing log-posterior choosing likely class yields max-sum classiﬁer computes likely global conﬁguration target nuisance variables image. assuming gaussian noise added template image normalized uniformly distributed becomes moment parameters natural parameters {wcg generative parameter constraints. demonstrate sequence operations max-sum classiﬁer coincides exactly operations involved layer image normalization linear template matching thresholding pooling. first image normalized second image ﬁltered noise-scaled rendered templates wcg. assume translational invariance rendered templates yield convolutional layer third resulting activations passed pooling layer; translational nuisance taking maximum corresponds pooling dcn. fourth since switching variables latent max-marginalize classiﬁcation. leads relu operation deep rendering mixture model capturing levels abstraction marginalizing nuisance intractable modern datasets since contain conﬁgurations high-dimensional nuisance variables response extend hierarchical deep rendering mixture model factorizing number different nuisance variables different levels abstraction. drmm image generation process starts highest level abstraction random choice object class overall nuisance followed random choices lower-level details progressively rendering concrete information level-by-level process ﬁnally culminates fully rendered d-dimensional image generation drmm takes form latent variables parameters helper variables deﬁned full detail appendix drmm deep gaussian mixture model special constraints latent variables. here target-relevant nuisance variables target-irrelevant nuisance variables level rendering path deﬁned sequence root individual pixels represents sequence local template used render image nuisance transformations partially render ﬁner-scale details move abstract concrete. note suppressed clarity. fig. illustrates corresponding graphical model. before suppressed dependence pixel location level hierarchy. sum-over-paths formulation drmm. rewrite drmm generation process expanding matrix multiplications scalar products. yields interesting active paths pixel product weights along path. rendering path active exponentially many possible rendering paths exist small fraction controlled sparsity active. fig. depicts sum-over-paths formulation graphically. recursive nonnegative forms. rewrite drmm recursive form refer helper latent variables intermediate rendered templates. also deﬁne nonnegative drmm drmm extra nonnegativity constraint intermediate rendered templates latter enforced training relu operation top-down reconstruction phase inference. throughout rest paper focus nn-drmm leaving unconstrained drmm future work. brevity drop preﬁx. factor model. also deﬁne explore variant drmm top-level latent identical drmm call deep rendering factor model drfm closely related spike-and-slab sparse coding model explore training results leave exploration future work. number free parameters. compared shallow |cl| parameters |g+|dd+ parameters exponential reduction number free parameters number units layer enables efﬁcient inference learning better generalization. note assumed dense λg’s here; impose structure number parameters reduced. bottom-up inference. shallow given input image drmm classiﬁer infers likely global conﬁguration executing max-sum/product message passing algorithm stages bottom-up infer overall class label top-down infer latent variables intermediate levels first focus ﬁne-to-coarse pass since leads directly dcns. using ﬁne-to-coarse nn-drmm inference algorithm inferring likely cateogry given output feature maps layer ﬁlters/weights layer comparing iteration corresponds feedforward propagation layer dcn. thus dcn’s operation probabilistic interpretation ﬁne-to-coarse inference probable conﬁguration drmm. top-down inference. unique contribution generative model-based approach principled derivation top-down inference algorithm nn-drmm resulting algorithm amounts simple top-down reconstruction term λˆgn discriminative relaxations generative discriminative classiﬁers. constructed correspondence drmm dcns mapping complete. particular recall generative constraints weights biases. dcns constraints weights biases free parameters. result faced training data violates drmm’s underlying assumptions freedom compensate. order complete mapping drmm dcns relax parameter constraints allowing weights biases free independent parameters. refer process discriminative relaxation generative classiﬁer appendix details). learning deep rendering model expectation-maximization algorithm describe learn drmm parameters training data hard algorithm algorithm drmm e-step consists bottom-up top-down e-steps layer model. γncg responsibilities brevity absorbed drmm m-step consists m-steps layer model. per-layer m-step turn consists responsibility-weighted regression denotes solution generalized least squares regression problem predict targets predictors otherwise. several interesting useful features algorithm. first note derivative-free alternative back propagation algorithm training intuitive potentially much faster second easily parallelized layers since m-step updates layer separately moreover extended batch version iteration model simultaneously updated using separate subsets data enable training distributed easily across multiple machines. vein algorithm shares several features admmbased bregman iteration algorithm however motivation optimization perspective resulting training algorithm derived proper probabilistic density. third interpretable connections sparse coding hard algorithm gmms. sum-over-paths formulation makes particularly clear mixture components paths drmm. g-step. training results paper generalized algorithm wherein replace m-step gradient descent based g-step useful comparison backpropagation-based training ease implementation. g-step would like make remarks proper m-step algorithm saving implementation future work. flexibility extensibility. since choose different priors/types nuisances larger drmm family could useful modeling wider range inputs including scenes speech text. algorithm used train whole system end-to-end different sources/modalities labeled unlabeled data. moreover capability sample model allows probe captured drmm providing principled ways improve model. ﬁnally order properly account noise/uncertainty possible principle extend algorithm soft algorithm. leave interesting extensions future work. insights deep convnets dcns message passing networks. drmm inference algorithm equivalent performing max-sum-product message passing drmm note max-sum-product mean novel combination max-sum max-product described detail proofs appendix. factor graph encodes information generative model organizes manner simpliﬁes deﬁnition execution inference algorithms inference algorithms called message passing algorithms work passing real-valued functions called messages along edges nodes. drmm messages sent ﬁner coarser levels fact feature maps factor graph formulation provides powerful interpretation convolution max-pooling relu operations correspond max-sum/product inference drmm. thus architectures layer types commonly used today’s dcns derived precise probabilistic assumptions entirely determine structure. drmm therefore uniﬁes perspectives neural network probabilistic inference shortcomings dcns. dcns perform poorly categorizing transparent objects might explained fact transparent objects generate pixels multiple sources conﬂicting drmm sparsity prior encourages sources. dcns also fail classify slender man-made objects locality imposed locallyconnected/convolutional layers equivalently small size template drmm. result dcns fail model long-range correlations. class appearance models activity maximization. drmm enables understand trained dcns distill store knowledge past experiences parameters. speciﬁcally drmm generates rendered templates mixture products afﬁne transformations thus implying class appearance models dcns stored similar factorized-mixture form multiple levels abstraction. result product ﬁlters/weights layers yield meaningful images objects also shed light another approach understanding memories proceeds searching input images maximize activity particular class unit technique call activity maximization. results activity maximization high performance trained million images shown fig. resulting images reveal much dcns store memories. using drmm solution activity maximization class derived individual activity-maximizing patches function learned drmm parameters implies contains multiple appearances object various poses. activity-maximizing patch pose consistent fig. extensive experiments alexnet vggnet googlenet images provide strong conﬁrmational evidence underlying model mixture nuisance parameters predcted drmm. unsupervised learning latent task nuisances. goal representation learning disentangle factors variation contribute image’s appearance. given formulation drmm clear dcns discriminative classiﬁers capture factors variation latent nuisance variables such theory presented makes clear prediction supervised learning task targets lead unsupervised learning latent task nuisance variables. perspective manifold learning means architecture dcns designed learn disentangle intrinsic dimensions data manifolds. order test prediction trained classify synthetically rendered images naturalistic objects cars cats variation factors location pose lighting. training probed layers trained quantify much linearly decodable information exists task target latent nuisance variables fig. shows trained possesses signiﬁcant information latent factors variation furthermore nuisance variables layers required disentangle factors. strong evidence depth necessary amount depth required increases complexity class models nuisance variations. experimental results evaluate drmm drfm’s performance mnist dataset standard digit classiﬁcation benchmark training labeled images test labeled images. also evaluate drmm’s performance cifar dataset natural objects include training labeled images test labeled images. experiments full e-step bottom-up phase principled top-down reconstruction phase. order approximate class posterior drmm include kullback-leibler divergence term inferred posterior true prior regularizer also replace m-step algorithm algorithm g-step update model parameters gradient descent. variant known generalized algorithm refer drmm experiments done nn-drmm. conﬁgurations models corresponding dcns provided appendix supervised training. supervised training results shown table appendix. shallow -layer yields similar performance convnet conﬁguration also predicted theory generative discriminative classiﬁers training converges faster deep training results initial implementation -layer drfm algorithm converges faster conﬁguration achieving similar asymptotic test error also completeness compare supervised training -layer drmm corresponding show comparable accuracy unsupervised training. train -layer drmm unsupervised images followed end-to-end re-training whole model using labeled images. results comparison swwae model shown table drmm model outperforms swwae model scenarios convnet pl-dae wta-ae swwae dropout m+tsvm skip deep generative model laddernetwork auxiliary deep generative model catgan improvedgan drmm -layer semi-sup drmm -layer semi-sup drmm -layer semi-sup nn+kl swwae unsup-pretr unsup-pretr drmm -layer unsup-pretr semi-supervised training. semi-supervised training randomly chosen subset labeled images unlabeled images training validation set. results shown table -layer drmm -layer drmm comparisons related work. drmms performs comparably state-of-the-art models. specially -layer drmm yields best results results second best result also show training results -layer drmm cifar table appendix drmm yields comparable results cifar best semi-supervised methods. results comparisons related work appendix conclusions understanding successful deep vision architectures important improving performance solving harder tasks. paper introduced family hierarchical generative models whose inference algorithms different models reproduce deep convnets decision trees respectively. initial implementation drmm algorithm outperforms backpropagation supervised unsupervised classiﬁcation tasks achieves comparable/stateof-the-art performance several semi-supervised classiﬁcation tasks architectural hyperparameter tuning acknowledgments. thanks pitkow poole helpful discussions feedback. supported iarpa doi/ibc contract dpc. also supported ccf- afosr fa--- wnf--- n---. supported graduate reseach fellowship igert training grant references anselmi leibo rosasco mutch tacchetti poggio. magic materials theory deep hierarchical architectures learning sensory representations. cbcl technical report arora bhaskara provable bounds learning deep representations. arxiv kingma welling. auto-encoding variational bayes. arxiv preprint arxiv. kschischang frey h.-a. loeliger. factor graphs sum-product algorithm. ieee russakovsky deng krause satheesh huang karpathy khosla bernstein imagenet large scale visual recognition challenge. international journal computer vision proposition discriminative relaxation noise-free gaussian rendering mixture model classiﬁer nuisance variable single layer neural consisting local template matching operation followed piecewise linear activation function line take noise-free limit grmm means hypothesis dominates others likelihood. line assume image consists multiple channels conditionally independent given global conﬁguration typically input images color channels general abstract line assume pixel noise covariance isotropic conditionally independent given global conﬁguration proportional identity matrix line deﬁned locally connected template matching operator location-dependent template matching operation. note nuisance variables marginalized over application local template matching operation ﬁlters/templates {wcg}c∈cg∈g. lemma maxout template matching pooling operation translational nuisance variables reduces traditional convolution max-pooling operation. traditional convolution operator. finally vectorizing gives desired result maxpooldcn proposition discriminative relaxation noise-free grmm translational nuisances random missing data single convolutional layer traditional dcn. layer consists generalized convolution operation followed relu activation function max-pooling operation. proof. model completely random missing data nuisance transformation {keep drop} keep leaves rendered image data untouched drop throws entire image rendering. thus switching variable models missing data. critically whether data missing assumed completely random thus independent task variables including measurements since missingness evidence another nuisance invoke proposition conclude discriminative relaxation noise-free grmm random missing data also maxout-dcn specialized structure derive. mathematically decompose nuisance variable parts then following similar line reasoning proposition operator invoke identity max{u max{u relu real numbers we’ve deﬁned bdrop we’ve used slightly modiﬁed convolution operator deﬁned also observe primed constants independent pulled outside maxa. line primed constants also independent dropped argmaxct. finally line assume uniform prior resulting sequence operations corresponds exactly applied single convolutional layer traditional dcn. deﬁne drmm full detail. deﬁnition deep rendering mixture model deep gaussian mixture model special constraints latent variables. generation drmm takes form assuming isotropic noise taking zero-noise limit term denominator smallest zero slowly. hence responsibilities γncg approach zero except term γnc∗g∗ approach one. thus soft responsibilities become hard responsibilities zero-noise limit third fourth equalities used associativity elementwise multiplication deﬁnition relu respectively. either since maxai∈{} therefore solution optimization follows mˆau relu e-step inference top-level category theorem inference drmm according dynamic programming-based algorithm below yields signed dcns. inference algorithm bottom-up top-down pass. line employ sent next layer. notice implicitly inferred lemma line becomes vector also line diag↓) diagonal matrix diagonal matrix rows indexed columns corresponds output convolutional layer prior relu spatial max-pooling operators. note succeeded expressing optimization recursively terms level smaller sub-problem iterating procedure yields recurrence relations deﬁne dynamic programming algorithm bottom-up top-down inference drmm bottom-up e-step formally deﬁne procedure preliminary deﬁnitions remarks helpful. generative classiﬁer models joint distribution input features class labels. classify inputs using bayes rule calculate picking likely label training classiﬁer known generative learning since generate synthetic features sampling joint distribution therefore generative classiﬁer learns indirect input features labels modeling joint distribution labels features. contrast discriminative classiﬁer parametrically models trains dataset input-output pairs order estimate parameter known discriminative learning since directly discriminate different labels given input feature therefore discriminative classiﬁer learns direct input features labels directly modeling conditional distribution labels given features. given deﬁnitions deﬁne discriminative relaxation procedure converting generative classiﬁer discriminative one. starting standard learning objective generative classiﬁer employ series transformations relaxations obtain learning figure graphical depiction discriminative relaxation procedure. rendering model depicted graphically mixing probability parameters rendered template parameters λcg. intuitively interpret discriminative relaxation brain-world transformation applied generative model. according interpretation instead world generating images class labels instead imagine world generating images rendering parameters θworld brain generates labels classiﬁer parameters ηdis ηbrain brain-world transformation converts equivalent graphical model extra parameters constraints introduced. discriminatively relaxing constraints yields single-layer discriminative counterpart original generative classiﬁer generative conditional discriminative log-likelihoods respectively. line used chain rule probability. line introduced extra parameters also introducing constraint enforces equality generative parameters line relax equality constraint allowing classiﬁer parameters differ image generation parameters line pass natural parametrization exponential family distribution natural parameters ﬁxed function conventional parameters constraint natural parameters ensures optimization lcond yields answer optimization lcond. ﬁnally line relax natural parameter constraint learning objective discriminative classiﬁer parameters free optimized. graphical model depiction process shown fig. summary starting generative classiﬁer learning objective lgen complete steps arrive discriminative classiﬁer learning objective ldis. refer process discriminative relaxation generative classiﬁer resulting classiﬁer discriminative counterpart generative classiﬁer. figure results activity maximization imagenet dataset given class activity-maximizing inputs superpositions various poses object distinct patches containing distinct poses predicted figure adapted permission authors. image decomposed argmaxg∈g g)|i∗ patches size pixels outside patch zero. maxg∈g operator ﬁnds probable within patch. solution activity maximization individual activity-maximizing patches section show that like dcns random decision forests also derived drmm model. instead translational switching nuisances show additive mutation nuisance process generates hierarchy categories heart rdf. mutating templates. child template additive mutation parent speciﬁc mutation depend parent leaves tree sample generated adding gaussian pixel noise. like drmm given cat+) template image rendered here special structure additive mutation process identity matrix. rendering path represents template evolution deﬁned sequence root ancestor template individual pixels represents sequence local nuisance transformations case accumulation many additive mutations. drmm cast e-drmm incremental form deﬁning intermediate class g+)) intuitively represents partial evolutionary path level then mutation level written eqs. deﬁne decision tree. leaf label histograms decision tree plays similar role softmax regression layer dcns. applying bagging decision trees yield random decision forest table summary probabilistic neural network perspectives dcns. drmm provides probabilistic interpretation common elements dcns relating underlying model inference algorithm learning rules. experiments conﬁgurations -layer drfm similar lenet variants. also conﬁgurations -layer drmm -layer drmm similar conv-small conv-large architectures respectively. convnet -layer drmm -layer convnet -layer unsup-pretr drmm -layer unsup-pretr swwae unsup-pretr semi-sup drmm -layer semi-sup convnet tsvm pl-dae wta-ae swwae dropout swwae dropout m+tsvm skip deep generative model laddernetwork auxiliary deep generative model improvedgan catgan table comparison -layer drmm -layer drmm stacked what-where auto-encoders various regularization approaches mnist dataset. number labeled images used extra unlabeled image.", "year": 2016}