{"title": "Clipped Action Policy Gradient", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Many continuous control tasks have bounded action spaces and clip out-of-bound actions before execution. Policy gradient methods often optimize policies as if actions were not clipped. We propose clipped action policy gradient (CAPG) as an alternative policy gradient estimator that exploits the knowledge of actions being clipped to reduce the variance in estimation. We prove that CAPG is unbiased and achieves lower variance than the original estimator that ignores action bounds. Experimental results demonstrate that CAPG generally outperforms the original estimator, indicating its promise as a better policy gradient estimator for continuous control tasks.", "text": "study demonstrate improve policy gradient methods exploiting knowledge actions clipped. prove variance policy gradient estimates strictly reduced mild assumptions hold popular policy classes gaussian policies diagonal covariance matrices. proposed algorithm termed clipped action policy gradient alternative unbiased policy gradient estimator lower variance original estimator. experimental results challenging mujoco-simulated continuous control benchmark problems show capg generally outperforms original policy gradient estimator incorporated deep algorithms. consider markov decision process deﬁned tuple possible states possible actions ×a×s transition probability distribution reward function distribution initial state discount factor. probability distribution action conditioned state referred policy algorithms policy maximizes expected cumulative discounted reward initial states. many continuous control tasks bounded action spaces clip out-of-bound actions execution. policy gradient methods often optimize policies actions clipped. propose clipped action policy gradient alternative policy gradient estimator exploits knowledge actions clipped reduce variance estimation. prove capg unbiased achieves lower variance original estimator ignores action bounds. experimental results demonstrate capg generally outperforms original estimator indicating promise better policy gradient estimator continuous control tasks. reinforcement learning achieved remarkable success recent years wide range challenging tasks games robotic manipulation locomotion help deep neural networks. policy gradient methods among successful model-free approaches particularly suitable continuous control tasks i.e. environments continuous action spaces directly improve policies represent continuous distributions actions order maximize expected returns. continuous control tasks policies typically represented gaussian distributions conditioned current past observations. although gaussian policies unbounded support continuous control tasks often bounded action sets execute example controlling torques motors effective torque values physically constrained. policies unbounded support like gaussian sample average instead variance lower i.e. long unbiased i.e. below derive estimate policy gradient given explaining algorithm characterize class policies consider study. deﬁnition distribution parameter differentiable respect allows exchange deriva−∞ pθda pθda call compatible distribution. compatible distribution conditioned variable call compatible conditional distribution. derive unbiased estimator decompose expected value. function random variable indicator function takes satisﬁes condition otherwise since a≤αx α<a<βx β≤ax decomposed meanwhile following useful lemma holds compatible conditional distributions. lemma suppose compatible conditional distribution whose cumulative distribution function algorithms rely estimation referred policy gradient methods. estimation unbiased variance typically high considered crucial problem policy gradient methods. respectively. clip function deﬁned clip computed elementwise vector i.e. constant function case reward function depends actions clipping discussed section suppose estimate value unbiased lower-variance manner using sample average therefore conclude capg lower variance original estimator unbiased. general following result. lemma suppose compatible conditional distritbuion whose cumulative distribution function capg version function includes case multivariate gaussian policy diagonal covariance. going details notational simplicity. since action conditionally independent decompose variance vaψ] right-hand side estimated using sample average given estimator call clipped action policy gradient better original gradient estimator sense lower variance unbiased. obvious difference original estimator capg comes endpoints. replaced respectively. intuitively speaking since either random variable given variance decreased; fact true. capg easily incorporated existing policy gradient methods. short replace computation capg. computed using automatic differentiation library instead replace although used standard notations mdps results rely markov property. capg works unbiased low-variance policy gradient estimator non-markovian environments reinforce algorithm works environments. assumed becomes constant outside action bounds. however sometimes makes sense reward function depends out-of-bound actions even state transition dynamics e.g. penalize norm actions order prevent policy going bounds. reward function longer holds. instead recursive structure obtain given policy action bounds consider policy modeled probability distribution bounded support whose cumulative distribution function α≤a<βπθ deﬁned mixture degenerate distributions truncated version corresponding probability density function respect measure generated mixture given call distribution clipped distribution. seeing capg applied fact estimating policy gradient gaussian policies used action bounds clipped gaussian policies capg straightforward policy gradient estimator them whereas conventional estimator unnecessarily high variance. clipped distribution resembles truncated distribution multimodal even underlying distribution unimodal peaks action bounds. contrast truncated distribution always unimodal underlying distribution unimodal implies inferior representational power modeling policy. demonstrate capg works interacts aspect problems separately used continuumarmed bandit problems i.e. mdps continuous action spaces state transitions. stateindependent policies optimized policy gradients maximize action-dependent immediate rewards. action space reward function deﬁned i|ai| choosing optimal action zeros achieves maximum zero reward. policy modeled multivariate gaussian distribution diagonal covariance matrix parameterized mean vector main diagonal covariance matrix. following experimental settings used unless otherwise stated. actions scalars i.e. parameters policy initialized zero mean unit variance dimension. policy update used batch pairs. average reward batch used baseline subtracted reward. adam default hyperparameters used update parameters. quantify variance reduction achieved capg repeatedly estimated policy gradients using samples withupdating policy. figure shows mean standard deviation policy gradient estimates obtained capg varying mean variance ﬁxed policy. settings capg consistently achieved lower variance without introducing visible bias. results numerically corroborate capg’s variance reduction ability well unbiasedness. efﬁcacy capg diminished sampled actions rarely outside bounds. figure shows training curves capg four different aspects separately controlled variance initial policy mean initial policy number dimensions actions batch size. capg consistently achieved faster learning across settings. larger initial variance distant initial mean tend make visible. capg’s gain scales even dimensions implies utility challenging complex continuous control tasks. using smaller batch sizes beneﬁts capg expected smaller batch sizes affected variance obs. space action space invertedpendulum-v inverteddoublependulum-v reacher-v hopper-v halfcheetah-v swimmer-v walkerd-v ant-v humanoid-v humanoidstandup-v gradient estimation. batch size training curve capg difﬁcult distinguish noted experiments actions sampled state. practical model-free scenarios cannot sample action state. capg also easily incorporated existing deep algorithms rely policy gradients. evaluate effectiveness deep settings used following popular deep algorithms continuous control used mujoco-simulated environments implemented openai experiments widely used benchmark tasks deep algorithms names environments listed along observation action spaces table environments bounded action spaces; hence actions clipped sent environments. considered combinations {ppo trpo} {capg environments trained million timesteps. combination tried times different random seeds. since found difﬁcult obtain reasonable performance within million timesteps figure mean standard deviation policy gradient estimates obtained capg continuum-armed bandit problem varying mean variance ﬁxed policy. data points policy gradients respect estimated times using different batches pairs. capg plots mean gradients almost overlap other plots visible. figure training curves continuum-armed bandit problems four different aspects controlled left right variance initial policy mean initial policy number dimensions actions batch size. last reward every policy update sampled averaged last updates obtain smoothed curve. smoothed curves aggregated compute mean curves bootstrapped conﬁdence intervals drawn shaded areas. followed hyperparameter settings used except learning rate adam used reduced million timesteps training order obtain reasonable performance used separate neural networks hidden layers hidden units tanh nonlinearities policy state value function. policy network outputs mean multivariate gaussian distribution diagonal covariance matrix. main diagonal covariance matrix separately parameterized logarithm standard deviation dimension. table summarizes comparison capg combined trpo ppo. used areas learning curves evaluation measure since measure ﬁnal performance also learning speed stability. trpo million training timesteps capg signiﬁcantly improved aucs environments respectively. also signiﬁcantly helped training million timesteps three harder environments trpo. environments kept allevel aucs tasks although seemed slight decreases environments. figures show smoothed learning curves experiments. cases improvements small consistent e.g. trpo inverteddoublependulum-v trpo humanoidstandup-v cases large improvements achieved e.g. swimmer-v trpo humanoid-v although used hyperparameters tuned best hyperparameters different capg since affects variance policy gradient estimation. possible separate hyperparameter tuning improve performance capg. comparing results trpo affected trpo difference estimators suggesting vulnerable high variance gradient estimation. trpo likely robust variance following reasons. trpo solves constrained optimization problem every policy update change divergence close constant thus robust changes scale gradients. also adapts step size figure training curves trpo mujoco-simulated environments. average last training episodes every training episode computed linearly interpolated episodes obtain smoothed curve. smoothed curves aggregated compute mean curves bootstrapped conﬁdence intervals drawn shaded areas. figure training curves trpo three harder mujoco-simulated environments. average last training episodes every training episode computed linearly interpolated episodes obtain smoothed curve. smoothed curves aggregated compute mean curves bootstrapped conﬁdence intervals drawn shaded areas. table performance comparison capg mujoco-simulated environments. performance evaluated average area learning curve standard error million timesteps. training computed linearly interpolating returns training episodes. combination {trpo ppo} {capg environments training runs different random seeds average standard error computed. p-values also computed capg versions welch’s t-test. bold numbers indicate better counterparts signiﬁcance. variety techniques proposed reduce variance policy gradient estimation since introduction. control variate method namely subtracting baseline approximate returns widely used reduce variance avoiding introduction bias estimation relying predicted values instead sampled returns also popular despite bias often introduces approach reduces variance different common approaches. therefore easily combined existing techniques reduce variance introducing additional bias. problem using probability distributions unbounded support control problems bounded action spaces pointed proposed modeling policies beta distributions solution. reported performance improvements using beta policies across multiple continuous control environments gaussian policies still nearly dominate deep literature truncated distributions also used deal bounded action spaces prior work contrast approach allows keep using policy parameterizations typically gaussians still exploit action bounds. addition argued section capg also seen using distribution bounded support multimodal whereas beta policies truncated gaussian policies unimodal. example clipped gaussian policy easily learn choose end-values action bounds high probability moving mean toward corresponding exploiting integrated form stochastic policy gradients reduce variance also proposed directly evaluated integral whole action space analytically computed limited classes action value approximators policies. method reduce variance eliminating need monte-carlo estimation policy gradients introducing bias action value approximation. method evaluates integral outside action bounds i.e. action values constant thus unbiased. shown variance policy gradient estimation reduced exploiting fact actions clipped sent environment. unbiased lower-variance policy gradient estimator call clipped action policy gradient proposed based analysis. capg easy implement combined existing variance reduction techniques control variates value function approximations. numerically analyzed capg’s behavior simple continuum-armed bandit problems conﬁrming efﬁcacy variance reduction. incorporated existing deep algorithms capg generally achieved better performance challenging simulated control benchmark tasks indicating promise alternative standard estimator. action spaces. prior work proposed beta truncated distributions end. argued capg seen estimating policy gradient different distribution bounded support termed clipped distribution. studies needed behaviors different kinds distributions policy representations including ease optimization representational power. chou po-wei maturana daniel scherer sebastian. improving stochastic policy gradients continuous control deep reinforcement learning using beta distribution. icml dhariwal prafulla hesse christopher klimov oleg nichol alex plappert matthias radford alec schulman john sidor szymon yuhuai. openai baselines. https//github.com/openai/ baselines shixiang lillicrap timothy ghahramani zoubin turner richard sch¨olkopf bernhard levine sergey. interpolated policy gradient merging on-policy off-policy gradient estimation deep. nips heess nicolas dhruva sriram srinivasan lemmon merel josh wayne greg tassa yuval erez wang ziyu eslami riedmiller martin silver david. emergence locomotion behaviours rich environments. arxiv e-prints mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg petersen stig beattie charles sadik amir antonoglou ioannis king helen kumaran dharshan wierstra daan legg shane hassabis demis. human-level control deep reinforcement learning. nature mnih volodymyr badia adri`a puigdom`enech mirza mehdi graves alex lillicrap timothy harley silver david kavukcuoglu koray. asynchronous methods deep reinforcement learning. icml schulman john moritz philipp levine sergey jordan michael abbeel pieter. high-dimensional continuous control using generalized advantage estimation. iclr silver david huang maddison chris guez arthur sifre laurent driessche george schrittwieser julian antonoglou ioannis panneershelvam veda lanctot marc dieleman sander grewe dominik nham john kalchbrenner sutskever ilya lillicrap timothy leach madeleine kavukcuoglu koray. mastering game deep neural networks tree search. nature silver david schrittwieser julian simonyan karen antonoglou ioannis huang guez arthur hubert thomas baker lucas matthew bolton adrian chen yutian lillicrap timothy sifre laurent. mastering game without human knowledge. nature publishing group tassa yuval doron yotam muldal alistair erez yazhe diego casas budden david abdolmaleki abbas merel josh lefrancq andrew lillicrap timothy riedmiller martin. deepmind control suite. arxiv e-prints", "year": 2018}