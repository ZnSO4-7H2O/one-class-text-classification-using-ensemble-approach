{"title": "Weighted Contrastive Divergence", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Learning algorithms for energy based Boltzmann architectures that rely on gradient descent are in general computationally prohibitive, typically due to the exponential number of terms involved in computing the partition function. In this way one has to resort to approximation schemes for the evaluation of the gradient. This is the case of Restricted Boltzmann Machines (RBM) and its learning algorithm Contrastive Divergence (CD). It is well-known that CD has a number of shortcomings, and its approximation to the gradient has several drawbacks. Overcoming these defects has been the basis of much research and new algorithms have been devised, such as persistent CD. In this manuscript we propose a new algorithm that we call Weighted CD (WCD), built from small modifications of the negative phase in standard CD. However small these modifications may be, experimental work reported in this paper suggest that WCD provides a significant improvement over standard CD and persistent CD at a small additional computational cost.", "text": "case observed using train rbms worked quite well practice. fact important deep learning rbms since authors suggested multilayer deep neural network better trained layer separately pre-trained single thus training rbms stacking rbms possible designing deep learning architectures. case probabilistic potential largely overlooked. recently rbms found interesting applications solving challenging problems otherwise difﬁcult tackle contrastive divergence approximation true computationally intractable log-likelihood gradient such perfect biased even converge also variants persistent fast persistent lead steady decrease log-likelihood learning furthermore maximum log-likelihood models learned probability distribution accumulates probability mass small number states. paper propose alternative approximation gradient called weighted contrastive divergence main difference consists weighting elements involved negative phase relative probability batch. small signiﬁcant change leads probability distributions closely resemble target ones expense large additional computational cost. order illustrate points explicitly address small size problems allow exact evaluation partition function kullback-leibler divergence directly related log-likelihood data. compare target model probabilities state thus showing beneﬁts scheme proposed detailed level. also analyze real-world large size problems partition function evaluated. intractability exact calculation probabilities parzen window estimator measure quality results ﬁnding provides signiﬁcant improvement resulting model. paper organized follows. section reviews model section present basic algorithm natural extension persistent weighted experiments showing beneﬁts described presented sections abstract— learning algorithms energy based boltzmann architectures rely gradient descent general computationally prohibitive typically exponential number terms involved computing partition function. resort approximation schemes evaluation gradient. case restricted boltzmann machines learning algorithm contrastive divergence well-known number shortcomings approximation gradient several drawbacks. overcoming defects basis much research algorithms devised persistent manuscript propose algorithm call weighted built small modiﬁcations negative phase standard however small modiﬁcations experimental work reported paper suggest provides signiﬁcant improvement standard persistent small additional computational cost. restricted boltzmann machines originally conceived eighties topological simpliﬁcation boltzmann machines captured attention neural network community last decade. role building blocks multilayer learning architectures deep belief networks deep autoencoders rbms successfully applied several areas interest image classiﬁcation collaborative ﬁltering acoustic modeling mention few. able learn target probability distribution samples. rbms layers hidden another visible units intra-layer connections. property makes working rbms simpler regular bms. stochastic computation loglikelihood gradient efﬁciently evaluated since gibbs sampling performed parallel. similar universal approximators sense that given enough hidden units rbms approximate probability distribution. notice factor weights equally every example training different probability state comes repetition examples. important probabilities learned nonuniform. section describe modiﬁcation family algorithms proposed work generically call weighted contrastive divergence first point main limitations provide description algorithm ﬁnally present extension persistent version. previously mentioned exact evaluation partition function general possible. therefore difﬁcult compare behavior real world problems respect exact gradient however expected conclusions drawn small problems probabilities exactly computed extrapolated large ones. performed extensive evaluation standard problems tractable number input units experiments conclude that practice standard shows following properties many cases standard able obtain good models combination parameters able assign large enough probabilities examples training set. effect noticeable small number hidden units small. cases happens even number hidden units large seems related difﬁculty problem. good combination parameters standard able obtain somewhat good models preserving probabilities training set. however many cases tends concentrate probability mass states even cases states training receive probability. binary visible binary hidden variables respectively. hidden variables usually introduced increase expressive power model. probability distribution visible variables deﬁned marginal distribution particular form energy function allows efﬁciently compute free energy numerator e−freeenergy e−energy addition since factorize possible compute step making possible perform gibbs sampling efﬁciently however evaluation still computationally prohibitive number input hidden variables large energy function depends several parameters given data learning rbms consists adjusting maximize loglikelihood data. energy-based models derivative log-likelihood expressed term called positive phase ﬁrst second term negative phase. similar exact computation derivative log-likelihood general computationally prohibitive negative phase efﬁciently computed. fact negative phase comes derivative logarithm partition function. notice log-likelihood data kullback-leibler divergence contain information used indistinguishably intuitively weighting negative phase allows obtain better estimators real negative phase also weights differently every state assigning weight states larger boltzmann probability. conﬁrm experiments small problems modiﬁcation positive effect learning process allowing obtain models much lower values standard cdk. addition differently standard proposed approach better training probability distribution shown section moreover decreasing behavior learning. therefore weighting negative phase helps overcoming non-desired behavior standard pointed section iii-a. even though detailed study unfeasible real-world large problems approach employed section seems indicate weighted negative phase also improves statistical representativity model. easily generalize previous procedure variant standard changes reconstructions data suitable choice points leading weighted negative phase deﬁned previous deﬁnition impose condition subset gives ﬂexibility also presents additional problem selecting good candidates. principle subset could used. instance special relevant case taken persistent reconstructions data. denote corresponding weighted version weighted persistent hand spans whole space negative phase exact gradient. obviously computational cost directly proportional number elements notice that general computational overhead associated calculation negative phase small compared corresponding evaluation standard although obviously depends consequence likelihood presents non-monotonic behavior starts increasing reaches maximum starts decrease. accordingly kullback-leibler divergence starts decreasing reaches minimum increases. behaviour seen several ﬁgures section iv). since positive phase standard exactly equal positive phase exact gradient need modify contrast negative phase suffers extreme drastic approximations responsible limitations described above. reason propose modiﬁcation negative phase cdk. modiﬁcation consists weighting differently every contributing state algorithm weighted negative phase. call contrastive divergence describe following. negative phase exact gradient reads computes reconstructions data whereas exact gradient computes whole space means explores every batch tiny fraction space. good efﬁciency point view statistical side. weights every element constant whereas exact gradient weights every element model probability means gives weight every element contrast exact gradient gives importance elements larger probabilities. algorithm proposes overcome second issue. assign larger weights elements larger probabilities computing relative probabilities elements batch following perform series experiments test proposed approach. precisely compare standard together weighted counterparts wcdk wpcd. section restrict analysis small dimensional spaces exact calculations performed. goal compare different models lowest possible level avoid drawing conclusions coarse approximation usually done dealing larger problems. particular evaluate exact partition function model compute exact probabilities whole space compare probabilities data different models. also evaluate exact obtained models. tested proposed approach series data sets described following. training performed including examples probabilities. denote target distribution probabilities assigned states data set. three different schemes used establish target distributions. simplest empirical distribution sets probability state. second draws probabilities gaussian proﬁle. third model assigns different uniform probabilities separate subsets. call training space combination data target distribution associated data. ﬁrst problem denoted bars stripes consists detecting vertical horizontal lines binary images containing either both. versions problem tested containing images respectively. second problem named labeled shifter ensemble consists learning number states formed follows given initial n-bit pattern generate three states concatenating sequences n-bit pattern computed original shifting left intermediate code copying unchanged code shifting right code size states bits. again versions problem evaluated problems already explored tested parity problem consists learning whether number bits value even not. parity problem known difﬁcult learn classical neural networks easy understand difﬁcult problem learn context boltzmann machines high order model requires single weight connecting units simultaneously problem tested input variables target distributions associated problems corresponding empirical distributions. every element data probability equal number elements data set. data sets assign different probabilities formed ordered list ﬁrst variant multg assigns position list probability deﬁned second variant multd assigns three different probability values elements list belong sublists respectively. value ﬁxed imposing probabilities group denote scheme discrete. table summarizes main properties training spaces used experiments. experiments performed steps. ﬁrst selected every data suitable parameters standard standard pcd. selection performed independently every model. second step used parameters found ﬁrst step test wcdk wpcd. networks trained standard gradient ascent problems stochastic gradient ascent multg multd data sets. weights initialized gaussian distribution zero mean variance suitably selected every model. weight decay used. every network trained epochs cases epochs multg multd problems. fig. divergence learning optimal probabilities models found data set. x-axis left panel accounts number epochs/. x-axis right panel corresponds integer index labelling state training set. target probabilities shown black line. optimal combination parameters selected every every problem explained next. every conﬁguration parameters tested times different random seeds. therefore experiments every every data set. experiments selected combination parameters achieved smallest step learning process. similar model selection performed pcd. differences described following. first since observed smallest values usually obtained hidden units value tested. second learning rates values tested spanned range powers different schemes ﬁxed linearly decaying. third since momentum relevant parameter tested models momentum selecting parameters ﬁnal models obtained every data similar conditions. tested parameters selected furthermore experiment performed varying number hidden units number visible units repeated times different random seeds. procedure applied parameters wpcd parameters respectively. notice that results weighted models optimal better results achieved parameters speciﬁcally optimized. following show results models data sets described trained whole training space. table summarizes best results obtained case showing minimal obtained along learning process. seen general weighted version algorithm performs better non-weighted counterpart cases differences signiﬁcantly large. notice that explained above learning parameters wcdk wpcd even optimized. furthermore shown ﬁgures minimum variant models achieved early stage learning process afterwards degenerates. contrast evolution weighted versions much smooth minimum attained training. notice also that overall best performer leading always small values reﬂected good probabilistic models. data training space optimal models obtained compared corresponding target ones also report values training process evaluate evolution models found learning. figure shows results obtained problem. seen probabilities dramatically wrong comparison target probabilities outstanding. regarding optimal asymptotic values much better also models found much stable according small local variability curve. fig. optimal probabilities models found multg data set. target probabilities depicted black line. x-axis panels index every element data corresponding ordered list smooth previous case though optimal value found much worse found problem. again optimal much lower corresponding pointing better probabilistic model compared target distribution. performs roughly case though convergence asymptotic value much slower still keeps decreasing behavior. features reﬂected optimal probabilities reported right panel. seen probabilities much closer target ones generated also much uniform. furthermore probabilities present much larger oscillations particular several outlayers take large amount probability mass individually mentioned introduction. case however though algorithm produces almost uniform probabilities desired exact value reproduced case meaning states contained training space acquire non-zero probabilities. happen estimate achieves small value therefore well target probabilities. matter fact also happens problem model probabilities already smaller version problems show similar behavior. figure shows results toughest problem analyzed work data set. upper lower plots show results respectively. case neither able learn sensible model maybe difﬁculty problem. remarkably optimal almost identical optimal evolution along learning cases similar smooth poor. consequence resulting probability distributions almost identical none making much sense. notice probabilities found quite uniform still away real target values training space consisting half total space receives half total probability different situation found predictions shown lower plots fig. probabilities test corresponding models minimum training found multg problem. left training/test. right training/test. bottom left training/test. bottom right training/test. x-axis similar ﬁgure examples test shown. target probabilities shown black line. ﬁgure. case ﬁnds minimum degenerates afterwards case behaves previous problems monotonically decreasing approaching asymptotic value much lower one. resulting probabilities closer target ones cases remarkable better performs. resulting models similar going leads much accurate model case whereas produces highly non-uniform distribution scattered values broad range. fig. report panels results multg problem. split different scales resulting probability distributions. left panel shows right panel depicts probabilities obtained pcd. expected quality weighted version markedly better standard prediction later clearly unable reproduce target distribution. token performs better pcd. important take account ordering states multg problem relevant. three different classes built ﬁrst containing states corresponding values order second containing values order seen detects lower values larger probabilities able discern among different classes. cases estimations distribution probabilities capture main trends target probabilities. however cases ﬂuctuations around right values large comparable maybe performing little worse agreement values reported table. hand recovers nice model clearly discriminating among three groups. finally report probability distributions multd problem obtained fig. case prediction dramatically better later able assign essentially probability state able discern feature problem. contrast performs well clearly discriminates three categories problem quite uniform probability group. also performs similarly multg case. experiments described previous section focused trying obtain models minimum order probabilities training space. therefore analyzing approximation capability respective models. section focus generalization capability many cases important objective system. section show technique performs real-world high dimensional data sets exact computation probability state unfeasible. since evaluation likelihood possible employ alternative estimation quality results using parzen window estimator probabilities test obtained samples performed learned model. computational reasons algorithm estimate probabilities which addition hard validate really high dimensional spaces ones considered here. short parzen window estimator uses gaussian probability density distribution centered point sample standard deviation idea assign probability density point test equal averaged samples. uses averaged probability densities build unnormalized log-likelihood test depends samples used. speciﬁcally assign point test averaged gaussian value space used previous section leaving rest states test set. fraction values used training/test sets %/%. model minimum training saved subsequently tested test set. previous experiments compared standard weighted counterparts wpcd. parameters models already selected experiments performed complete training space. figure depicts results multg problem. essence keep structure observed states training space used learning. means always missing main trends target probability keeps fairly well. still quality worsens fraction states used training reduced expected. remarkable that cases able generalize successfully. particular notice trained complete training space performs much better bare trained whole training space. finally ﬁgure presents quantities previous ﬁgure multd problem. discussed above hard problem never able capture single feature data. contrast holds quite well even severe training space restrictions. similar results found rest models data sets training space reduced above. fig. comparison quality samples generated using parzen window procedure wpcd caltech fashion-mnist mnist ocr-letters problems. x-axis represents number samples generated model used compute ull. tested proposed approach following four data sets mnist fashion-mnist caltech silhouettes letters. mnist data well known benchmark problem corresponding grayscale images handwritten digits. fashion mnist contains grayscale images associated different clothing categories caltech silhouettes data contains binary images containing items different categories finally ocr-letters data contains samples grayscale images handwritten letters. table shows number features data number samples train wpcd cases architecture network contained variable number visible units ﬁxed number hidden units cases networks trained epochs stochastic gradient ascent batch size examples. different values learning rate tested logarithmic mesh spanning range optimal value case. momentum weight decay factors respectively. finally learning rate followed linearly decaying scheme. combination parameters also tested best values mentioned ranges. cases selection criterion highest test accuracy supervised models labels appended training examples onehot coding representation. ﬁnal models trained unsupervised optimal values found model selection described above. figure shows comparison test function number samples employed parzen window estimator four data sets analyzed. cases blue curves correspond results obtained wpcd respectively. upper left right panels show caltech fashion-mnist lower left right panels correspond mnist ocr-letters problems. seen cases wpcd produces higher values pointing better model learned model respect test set. must kept mind however real estimation log-likelihood data cases arbitrary constant sets origin scales. means relative differences estimations make sense. anyway higher scores attributed better models although possible quantify better. notice always transient regime beginning curves variation large corresponding poor statistical representation produced reduced number samples employed. however number increases curves approach stable regime predictions seem stabilize wpcd approaching better statistical representativity smaller number samples. work propose variant standard learning algorithm rbms modiﬁes negative phase gradients involved weights update rule. negative phase computed weighted average members batch weighting coefﬁcients relative model probabilities batch. cheap modiﬁcation nevertheless delivers better performance terms optimal model probabilities. tested proposed algorithm small problems exact probabilities evaluated statistical representation learned models much better obtained pcd. large problems direct measure probabilities unfeasible parzen window evaluation quality resulting models still indicates wpcd performs better alternatives. case important realize that data processed reduced smalldimensional space weighting negative phase good choice improves probability description model. furthermore weighting scheme extended useful techniques alternative parallel tempering possible future work involves sophisticated selection elements entering weighted negative phase. instance involve members training also neighboring ones that continuity reasons could also contain relevant information. small problems comparison exact gradient calculations also carried order contrast statistical averages exact calculation approximated learning scheme. smolensky information processing dynamical systems foundations harmony theory parallel distributed processing explorations microstructure cognition rumelhart mcclelland eds. press salakhutdinov mnih hinton restricted boltzmann machines collaborative filtering proceedings international conference machine learning. a.-r. mohamed dahl hinton acoustic modeling using deep belief networks ieee transactions audio speech language processing vol. fischer igel empirical analysis divergence gibbs sampling based learning algorithms restricted boltzmann machines international conference artiﬁcial neural networks vol. desjardins courville bengio vincent delalleau parallel tempering training restricted boltzmann machines international conference artiﬁcial intelligence statistics geman geman stochastic relaxation gibbs distributions bayesian restoration images ieee transactions pattern analysis machine intelligence vol. rumelhart hinton williams learning internal representations error propagation parallel distributed processing explorations microstructure cognition rumelhart mcclelland eds. press bengio lecun chapter scaling learning algorithms towards large-scale kernel machine bottou chapelle weston eds. press", "year": 2018}