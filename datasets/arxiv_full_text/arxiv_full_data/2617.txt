{"title": "Learning From Graph Neighborhoods Using LSTMs", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Many prediction problems can be phrased as inferences over local neighborhoods of graphs. The graph represents the interaction between entities, and the neighborhood of each entity contains information that allows the inferences or predictions. We present an approach for applying machine learning directly to such graph neighborhoods, yielding predicitons for graph nodes on the basis of the structure of their local neighborhood and the features of the nodes in it. Our approach allows predictions to be learned directly from examples, bypassing the step of creating and tuning an inference model or summarizing the neighborhoods via a fixed set of hand-crafted features. The approach is based on a multi-level architecture built from Long Short-Term Memory neural nets (LSTMs); the LSTMs learn how to summarize the neighborhood from data. We demonstrate the effectiveness of the proposed technique on a synthetic example and on real-world data related to crowdsourced grading, Bitcoin transactions, and Wikipedia edit reversions.", "text": "many prediction problems phrased inferences local neighborhoods graphs. graph represents interaction entities neighborhood entity contains information allows inferences predictions. present approach applying machine learning directly graph neighborhoods yielding predicitons graph nodes basis structure local neighborhood features nodes approach allows predictions learned directly examples bypassing step creating tuning inference model summarizing neighborhoods ﬁxed hand-crafted features. approach based multi-level architecture built long short-term memory neural nets lstms learn summarize neighborhood data. demonstrate effectiveness proposed technique synthetic example real-world data related crowdsourced grading bitcoin transactions wikipedia edit reversions. many prediction problems naturally phrased inference problems local neighborhood graph. consider instance crowdsourced grading. construct graph consisting items graders edges connect items users graded them labeled grade assigned. infer grade item look graph involving adjacent nodes graph known neighborhood consists people graded item grades assigned. wish sophisticated determine people good graders could look also work performed people expanding analysis outwards -neighborhood item. another example consider problem predicting bitcoin addresses spend deposited funds near future. bitcoins held addresses; addresses participate transactions send receive bitcoins. predict addresses likely spend bitcoin near future natural build graph addresses transactions consider neighborhoods address. neighborhood contains information bitcoins came from happened bitcoins interacting addresses help predict whether coins transacted soon. third example consider problem predicting user behavior wikipedia. users interact collaboratively editing articles interested predicting users work reverted. build graph users nodes interactions edges interaction occurs users edit article short succession either keeps undoes work other. -neighborhood user tell often user’s work kept reverted. again consider larger neighborhoods gather information user people interacted with trying determine whether good contributors experienced whether involved disputes forth. paper show solve problems applying machine learning using architecture based multi-level long short-term memory neural nets lstm level processing degree challenge applying machine learning graph neighborhoods lies fact many common machine learning methods neural nets support vector machines handle ﬁxed-length vectors features input. graph neighborhood variable size topology necessary summarize neighborhood ﬁxed number features learning. machine learning methods logistic regression accept potentially unbounded number inputs every input index name obvious local topology graph ﬁxed naming scheme preserves structure useful information. machine-learning methods learn sequences lstms recurrent neural nets offer power. possible traverse local neighborhood node graph order encode neighborhood sequence features complete markers denote edge traversals feed sequence lstm. experimented approach obtain useful results lstms unable learn anything useful ﬂattened presentation graph neighborhood. propose learning architecture based multiple levels lstms. architecture performs predictions target graph node time. first graph unfolded target node yielding tree target node root level neighbors level- children neighbors’ neighbors level- children forth desired depth tree node level level-d lstm sequentially information children level produces output information itself. thus exploit lstms’ ability process sequences length process trees branching factor. top-level lstm produces desired prediction target node. architecture requires training lstms tree level. lstms learn summarize neighborhood radius basis data avoiding manual task synthesizing ﬁxed features. dedicating lstm level tailor learning distance target node. instance bipartite graph arising crowdsourced grading desirable different lstms aggregating edges converging item aggregating edges converting user proach four problems. ﬁrst problem synthetic example concerning crowdsourcing yes/no labels items. three based real data previously mentioned problems aggregating crowdsourced grades predicting bitcoin spending predicting future reversions user’s edits wikipedia. four problems show ability mlsl exploit feature data leads high performance minimal feature engineering effort apriori model assumptions. making available open-source code implementing lstms mlsl along datasets https//sites.google. com/view/ml-on-structures. predicting properties nodes graph structures common problem widely studied. several existing approaches view model-based inference problem. model created parameters tuned basis information available; model used perform inference. exact probabilistic inference generally intractable techniques rely iterative approximation approaches. iterative approximations also root expectation maximization iterative parameter estimation used together gibbs sampling reliably aggregate peer grades massive on-line courses iterative modelbased approaches also used reliably crowdsourcing boolean multi-class labels works bipartite graph items workers created worker reliabilities item labels grades iteratively estimated convergence. compared models beneﬁt proposed approach require model thus avail features happen available. instance crowdsourced grading agreement among graders judge reliability also information might available time taken grade time number items previously graded user without need model features might inﬂuence grade reliability. show ability lead superior performance compared additional features available. hand machine-learning based approaches dependent availability training data model-based approaches employed even absence. nodevec enables construction feature vectors graph nodes feature vector optimally represents node’s location graph. speciﬁcally feature vector maximizes a-posteriori probability graph neighborhoods given feature vector. resulting feature vector thus summarizes node’s location graph summarize original features node neighbors. contrast techniques introduce allow feed machine learning node features entire graph neighborhood. deepwalk feature vectors graph nodes constructed performing random walks nodes applying various summarization techniques list feature vectors visited nodes. approach enables consideration variable-diameter neighborhoods contrast exploration proceeds strictly breath-ﬁrst. deepwalk construction summarizing feature vector proceeds according chosen algorithm guided backpropagation learning goal. words summarization learned overall task. contrast approach summarization itself carried lstms learned backpropagation goal. lstms proposed overcome problem vanishing gradient long sequences problems recurrent neural nets widely useful wide variety learning problems; e.g. recurrent neural nets lstms generalized multi-dimensional settings multi-level architecture proposed handle arbitrary topologies non-uniform nodes edges rather regular n-dimensional lattices cost exploring smaller neighborhoods around nodes. learning graphs reduced standard machine-learning problem summarizing information available node ﬁxed features. done instance goal link prediction consisting predicting users social network collaborate connect next graph summarization typically requires deep insight problem order design summary features. multi-level lstms propose constitute learning graph summarization. recent work looked problem summarizing large graphs feature vectors goals thus different present paper emphasis consists considering nodes together immediate neighborhoods input machine learning. consider graph vertices edges assume edge labeled vector features size vertex associated vector labels. goal learn predict vertex labels basis structure graph edge labels. setting model wide variety problems. considering edge features rather also vertex features involves loss generality interesting features associated vertices included edges leading them. goal consists predicting edge outputs rather vertex construct dual graph edges vertices learning method overview. learning strategy summarized follows. order predict label node consider tree rooted depth ﬁxed obtained unfolding graph starting traverse bottomup using sequence learners deﬁned below compute label node labels children edges nodes traversal yields output label root tree. training output compared desired output loss computed backpropagated tree. present detail steps. graph unfolding. given graph node along depth deﬁne full unfolding depth tree root constructed follows. root depth node depth children nodes depth plus depth single graph node correspond node unfolding. rename nodes unfolding distinct; nodes edges unfolding inherit labels correspondents graph. figure example graph asymmetric unfolding node depth rename nodes appear many locations distinct names instance denote copies descendants figure illustrates graph asymmetric tree unfolding node depth unfolding useful depends speciﬁcs learning problem discuss choice applications. sequence learners. proposed method learning graphs leverages sequence learners. sequence learner machine-learning algorithm accept input arbitrary-length sequences feature vectors producing single vector output. long short-term memory neural nets example sequence learners. denote sequence learner parameterized vector parameters lstms parameter vector consists lstm weights. sequence learner shape accepts sequence vectors size produces vector size output. assume sequence learner shape perform three operations loss backpropagation. loss function given ∂l/∂y output compute ∂l/∂x here ∂l/∂y vector ∂l/∂yi component component likewise ∂l/∂x vector components ∂l/∂x parameter update. loss function given ∂l/∂y output compute vector parameter updates. parameter updates instance computed gradient-descent method taking −α∂l/∂w precise method varies according structure sequence learner; e.g. given graph labeled edges above describe learning architecture perform forward step node label prediction backward step backpropagation parameter updates. term proposed architecture multi-level sequence learners mlsl short. start choosing ﬁxed depth unfolding. prediction learning performed sequence learners sequence learner responsible aggregating information children depth unfolding trees computing information parent depth sequence learner shape size edge labels edge labels computes features size sequence learner depth shape able aggregate edge labels output learners below single vector size note learners depth appear multiple times tree node depth tree. instances share parameters treated separately forward backward propagation. behavior sequence learners deﬁned parameter vectors goal learning learn values parameter vectors minimizes loss function. stress sequence learners parameter vectors depend depth tree depend root node whose label trying predict. order learn repeatedly select root nodes instance looping them probability distribution nodes construct unfoldings tv∗. perform forward backpropagation steps parameter update follows. forward propagation. forward propagation step proceeds bottom-up along figure illustrates sequence learners applied unfolding root node graph figure depth yield prediction node parameter update consider learner depth deﬁned parameters update parameters consider instances tree corresponding nodes depth ∂l/∂f instance compute parameter update ∆iw. compute overall parameter update averpreserving learner instance state. mentioned above sequence learner given depth occur several instances tree obtained unfolding graph commonly perform backpropagation parameter update though learner necessary preserve state learner forward propagation step; case instance neural nets lstms. thus even though learner instances depth deﬁned single parameter vector general necessary cache state every learner instance tree individually. training repeatedly select target node unfold graph feed unfolding multi-level lstms obtain prediction backpropagate loss updating lstms. important choice order which tree node edges children nodes lstm. edges random order shufﬂing order every training sample ﬁxed order. applications found approaches uses. implemented multi-level sequence learners basis lstm implementation performing backpropagation-though-time learning combined adadelta choice learning step report results synthetic setting three case studies based real data. code datasets found https//sites.google. com/view/ml-on-structures. figure forward propagation corresponding tree unfolding figure elements sequence learner consist features respective edges concatenated output learners below. note three instances learner depth- node unfolding. instances share parameters. ﬁgure symbol denotes concatenation feature vectors. learner aggregate sequence edge labels single label depth consider node depth children depth forward learner sequence vectors obtained concatenating feature vectors edges children feature vectors computed learners depth learner produce feature vector backward propagation. obtain vector root compute loss compute ∂l/∂y. loss backpropagated root leaves following topology tree consider node depth computed feature vector backpropagate instance learner computed loss obtaining ∂l/∂xi input vectors corresponding children children depth vector consists concatenation features graph edge features computed former require backpropagation retain portion ∂l/∂f backpropagation. imbalanced datasets apart accuracy report average recall unweighted average recall classes. suitable case classes different frequencies since highly imbalanced datasets easy inﬂate accuracy measure predicting labels frequent classes. considered common boolean crowdsourcing task users provide yes/no labels items. modeled bipartite graph items users kind nodes; edges labeled yes/no. task consists reconstructing likely labels items. generated synthetic data similar used data items true yes/no label users hidden boolean variable indicating whether truthful random. truthful users report item label random users report yes/no probability each. also called spammer-hammer user model. report results graph users items item labels balanced probability user reliable item gets votes different users. compare three algorithms expectation maximization user reliability modeled beta distribution. used informative prior initial beta distribution reﬂects proportion reliable users graph. multi-level lstm also consider case users additional observable feature correlated truthfulness. represents feature user created account week observable part standard crowdsourcing models. feature true reliable users unreliable users. denote algorithms access extra feature -lsl+ -lsl+; cannot make feature part model. intent show machinelearning approaches mlsls increase report results table additional information available superior -mlsl slightly superior -mlsl. additional feature available -mlsl+ -mlsl+ learn usefulness perform best. considered dataset containing peer grading data computer science classes. data comes online tool lets students submit homework grade other’s submissions. sumission typically reviewed students. data bipartite graph users submissions previous crowdsourcing application. users assign grades items predeﬁned range edge labeled grade additional features time student started grading submission time submitted grade. treat classiﬁcation task classes integer grades ground truth provided instructor grades available subset submissions. dataset contined labeled submissions; used training testing. compare three methods. simple average provided grades rounded closest integer. another method based expectation maximization iteratively learning accuracy users estimating grades. finally employed mlsl following features time complete review amount time review completion review deadline median grade received student assignment. output learner level size reaches peak experiment. wikipedia popular crowdsourced knowledge repository contributions people around world various languages. users occasionally contributions reverted users either quality part quarrel simply carelessness. interest predicting user whether user’s next edit reverted. note different question question whether speciﬁc edit whose features already known reverted future model user interactions wikipedia multigraph users nodes. edge represents implicit interaction users occurring creates revision immediately following revision edge labeled feature vector consisting edit distances revision immediately preceding edit distance. feature vector contains also elapsed times revisions quality measured deﬁned since english wikipedia large dataset experiment used complete dumps asturian wikipedia graph consists nodes edge obtain labels user consider state graph time days last date content available dump; leaves ample time reversions occur extra days ensuring label users correctly. train model repeatedly pick edit user construct graph neighborhood around user consisting edits preceding selected edit label user yes/no according whether selected edit reverted not. local neighborhood graph mlsl. performed training data validated remaining trained models depth validated measuring average recall f-scores labels. table shows average results depth level. observe scores reversion reversion labels high. moreover results show improvement performance increasing depth. blockchain public immutable distributed ledger bitcoin transactions recorded bitcoin coins held addresses hash values; address identiﬁers used owners anonymously hold bitcoins ownership provable public cryptography. bitcoin transaction involves source addresses destination addresses coins source addresses gathered sent various amounts destination addresses. mining data blockchain challenging anonymity addresses. data blockchain predict whether address spend funds deposited obtain dataset addresses using slice blockchain. particular consider addresses deposits happened short range blocks contain unique addresses deposits took place. looking state blockchain blocks addresses still funds sitting call hoarding addresses. goal predict addresses hoarding addresses spent funds. randomly split addresses training validation addresses. built graph addresses nodes transactions edges. edge labeled features transaction time amount funds transmitted number recipients forth total features. compared different algorithms mlsl depths outputs memory sizes learners reported results increasing maintained virtually performance increasing training time. using output memory cell providing advances performance. table shows results. using baseline poor results; score smaller class particularly low. tapping transaction history using level learner already provides good prediction average recall approaching increasing number levels enhances quality prediction digests information history transactions. increasing levels beyond lead better results dataset. results applications show mlsl provide good predictive performance wide variety problems without need devising applicationtailored models. sufﬁcient training data available mlsl graph representation problem available features achieve high performance. conclusions order processing nodes training matters. crowdsourced grading randomly shufﬂing order edges learning instance used different iterations training process superior using ﬁxed order. bitcoin hand feeding edges temporal order worked best. seems intuitive transactions happened temporal order. challenge choice learning rates various levels. gradient backpropagates across multiple levels lstms becomes progressively smaller. successfully learn needed different learning rates lstms different levels levels tend learn faster.", "year": 2016}