{"title": "Clamping Improves TRW and Mean Field Approximations", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We examine the effect of clamping variables for approximate inference in undirected graphical models with pairwise relationships and discrete variables. For any number of variable labels, we demonstrate that clamping and summing approximate sub-partition functions can lead only to a decrease in the partition function estimate for TRW, and an increase for the naive mean field method, in each case guaranteeing an improvement in the approximation and bound. We next focus on binary variables, add the Bethe approximation to consideration and examine ways to choose good variables to clamp, introducing new methods. We show the importance of identifying highly frustrated cycles, and of checking the singleton entropy of a variable. We explore the value of our methods by empirical analysis and draw lessons to guide practitioners.", "text": "examine effect clamping variables approximate inference undirected graphical models pairwise relationships discrete variables. number variable labels demonstrate clamping summing approximate sub-partition functions lead decrease partition function estimate increase naive mean ﬁeld method case guaranteeing improvement approximation bound. next focus binary variables bethe approximation consideration examine ways choose good variables clamp introducing methods. show importance identifying highly frustrated cycles checking singleton entropy variable. explore value methods empirical analysis draw lessons guide practitioners. undirected graphical models also called markov random ﬁelds powerful compact represent dependencies variables become central tool machine learning. challenge estimate normalizing partition function. example used compute probability evidence often critical component learning model. exact solution obtained junction tree method unless treewidth bounded take exponential time hence many approximate methods developed. focus three popular approaches tree-reweighted approximation na¨ıve mean ﬁeld approximation bethe approximation often implemented belief propagation case shall examine effect respective partition function estimate clamping variables possible setting combining approximate results obtained clamped sub-models. deﬁnitions. variables clamped exact solution obtained time exponential number variables. intuitively variables clamped would hope better results always case demonstrating guarantees challenging. weller jebara recently proved attractive binary pairwise model optimum bethe partition function approximation increase variable clamped. also provided example non-attractive model clamping variable leads worse approximation. nevertheless introduced heuristics identifying good variable clamp attractive mixed models demonstrated empirically approximation error sometimes signiﬁcantly reduced clamping even variable. make following contributions. show pairwise models number labels types potentials clamping improve partition function estimate decreasing increasing bounds respectively. proofs also yield insight approximate marginals returned. next examine select good sequence variables clamp. although methods weller jebara perform well choosing variable show that models methods perform poorly particularly selecting multiple variables. introduce methods strip model core search strongly frustrated cycles make approximate singleton entropy. provide empirical analysis approaches including comparison ‘greedy’ choice best variable clamp hindsight exhaustive exploration. conclude observations help guide practitioners. technique branching conditioning variables approximating remaining variables explored algorithms branch-and-cut work resolution versus search cutset conditioning discussed pearl reﬁned peot shachter method render remaining topology acyclic using belief propagation. eaton ghahramani developed further introducing conditioned belief propagation. explored feedback message passing inference gaussian models deriving strong results attractive models. bouchard zoeter discuss soft-binning split conﬁgurations subsets apply mean ﬁeld approximation without guarantees. choi darwiche examined methods approximate partition function deleting edges. consider pairwise models variables graph topology contains nodes corresponds contains edge pairwise relationship. sometimes consider multi-label models sometimes restrict attention binary models conﬁguration variables neighbors consider probability distribution e−e/z energy conﬁguration partition function quantity fundamental interest. requires summing states yield normalizx ensures denote log-partition function somereparamei∈v θixi singleton potentials edge weights wij. edge attractive edge repulsive. edges model attractive model called attractive else mixed. model write vector potentials vector marginals using standard overcomplete exponential family representation vector sufﬁcient statistics corresponding model eθ]. considering divergence standard variational methods show maxµ∈m θ·µ+h termed marginal polytope space marginal vectors consistent globally valid probability distribution conﬁgurations entropy corresponding global distribution. shall examine following popular approximate inference methods deﬁned maximizing negative free energy approximation space marginals given particular polytope. tilde symbol indicate approximate value show method subscript. maxµ∈l bethe approximation denotes standard local polytope laxation enforces pairwise consistency i.e. bethe entropy pairwise mutual information tree-reweighted approximation maxµ∈l θ·µ+ entropy approximation speciﬁed convex where tree distribution results taking tree-decomposition marginals known hence further easily shown cijiij edge counting numbers {cij thus also since also marginals subset bethe exact hence collecting relationships following sandwich results binary models supermodular potentials ruozzi proved general often strikingly accurate though settings methods signiﬁcantly better. denotes probability simplex labels pseudomarginal vectors result clamping ﬁnal equality follows maximizations independent sets variables equivalent joint maximization variables objectives. form gives ﬁrst equality theorem observe result clamping single variable precisely tighten local polytope furl. immediate corollary ther hence shown following. theorem discrete model variable considering variational perspective above note constrained maxµ∈p timization standard space method optimizes bethe trw) sub-space constrained respective negative free energy approximation maximized. variables approximate methods clamped leading possible conﬁgurations exact partition function obtained exponential time. weller jebara showed bethe approximation that attractive binary pairwise model variable clamping helps mixed model however clamping variable lead worse approximation empirically shown often help signiﬁcantly. practice locally optimum solutions clamped problems show worse performance parent. however analysis shows concern guaranteed avoided clamped optimizations initialized solution parent problem. henceforth focus binary pairwise models. shown above clamping variable summing approximate sub-partition functions always reduce error improve bounds trw. further weller jebara proved bethe approximation also true attractive models empirically often helpful mixed models. leads question choose variable sequence variables clamp. weller jebara introduced selection heuristics motivated trying break strong cycles demonstrated heuristics effective several contexts. ﬁrst describe earlier approaches. maxw simple method picks variable |wij|. maxw make poor selection choose variable centre large star conﬁguration cycle. mpower introduced complex approach attempt avoid problem considering convergent series powers modiﬁed matrix approximates weighted count around cycles. shown mpower outperforms maxw cases though many examples performance similar. note maxw mpower rely exclusively absolute value edge weights |wij| ignoring signs also ignoring singleton potentials. remainder section demonstrate earlier methods perform poorly certain circumstances introduce approaches. details selection methods provided appendix. note analysis assumes distribution trees used constant. however variable clamped edges removed graph decrease bound. this construct distribution subgraphs follows original tree less edge clamped model ∀µxi disconnected edges added make tree reduce addition tree weights reoptimized reduce bound still further. figure left model ‘lamp’ topology weller jebara right core model. obtained iteratively removing variables degree maxw applied original model often chooses clamp whereas would better selected model ﬁrst stripped core. following sudderth deﬁne core graph remains iteratively pruning nodes degree equivalently subgraph induced nodes either belong cycle path cycles. example shown figure pairwise entropy approximations expect better strip model core applying method select variable many cases previously challenging maxw quick pre-processing step enables maxw perform well expensive mpower method. frustrated cycle cycle number repulsive edges cause difﬁculties many methods inference. balanced model contain frustrated cycles. easily shown model mapped equivalent attractive model ﬂipping appropriately chosen subset variables balanced hence results attractive models readily extend broader class balanced models. pairwise approximations bethe exact models without cycles. further known bethe frustrated cycles lead trouble balanced cycles illustrated figure shows approximation error symmetric models uniform edge weights. edge weights rise bethe underestimate error bounded tends tends correct solution. strong negative weights however lead frustrated cycles bethe show rapidly increasing error without bound. note model cycle balanced even negative edge weights symmetric error either side edge weights. observation bethe perform arbitrarily badly strong frustrated cycles whereas error bounded explains later experimental results outperforms bethe motivates trying identify strong frustrated cycles. maxw mpower earlier methods weller jebara consider |wij| hence unable differentiate balanced frustrated cycles. strong frustrated cycles np-hard introduce heuristics build recent algorithm sontag used cutting plane approach tighten local polytope inference. combine ideas algorithm cycle scores based loop series method present heuristics identifying good variable clamp frustcycles seeks identify variable lying strong frustrated cycles clamped would remove cycles; strongcycles attempts also take consideration value removing strong balanced cycles. details provided appendix. previous clamping selection methods examine edge weights. however variable already singleton entropy effectively already held ﬁxed value little gained clamping hand variable high entropy strongly connected many others without frustrated cycle clamping effectively lead cascade many variables also ‘effectively clamped’ yielding signiﬁcant improvement approximation error. afterward little residual value actually clamping variables. effect illustrated comparing rows figure observe even fully connected model frustrated cycle present clamping sufﬁcient obtain almost zero error. illustration provided considering figure approximate log-partition function minus true value i.e. symmetric models variables cycle complete graph topologies uniform edge weights varied bethe trw. bottom error models methods clamping variable summing approximate sub-partition functions. observe clamping even strong positive weights lead underestimates bounded error; strong negative weights frustrated cycles lead unbounded overestimates bethe trw. clamping methods signiﬁcantly improved; frustrated cycles remaining methods almost exact. discussion figure example model earlier maxw mpower heuristics weller jebara perform poorly select variables clamp methods perform well. solid blue edges strongly attractive maxw mpower repeatedly select variables ﬁrst clamp good less picking frustrated cycle repeat clampings reap little beneﬁt. best pick model figure recognizing effect ideally would compute singleton entropies exact inference would clearly costly hence approximate inference. speciﬁcally introduce versions earlier method variable multiply respective earlier heuristic clamp score entropy choose best. approximate entropy reasons singleton marginals typically similarly good accuracy bethe often easier estimate particularly interested cases edge potentials high around variable setting bethe marginals poor pulled toward even true marginal close versions heuristics perform well multiple clampings models figure tested approaches runs various randomly generated binary pairwise models exploring clampings. used topologies shown figure following parameters used random singleton potentials attractive models mixed models exact values computed using junction tree algorithm. inference methods implemented using standard open source libdai library addition performed experiments complete graphs fewer variables similar number edges figure random -regular graphs. figure shows typical timings performance. full details results provided appendix. implemented variable selection methods speciﬁcally maxw mpower frustcycles strongcycles always ﬁrst stripping core. also tried original maxw without stripping addition used versions these total heuristics. also implemented greedy search possible clampings would perform compared heuristics implemented pseudo-greedy tried heurstics basket picked best performer. best performer determined highest solution. similarly bethe attractive models best performer meant variable figure average error plots bethe runs. plot shows particular model type size arranged model type model size within plot comparison show curves middle curves bethe bottom curves justiﬁed equation case clampings. grids toroidal random models erd¨os-renyi number variables edge probability s.t. degree match grids. models shown with attractive mixed error shown bethe mixed models best worst curves indicate best worst selection heuristics start clamp point. figure left three plots show average error bethe runs; plot shows model complete graph topology variables using edge weights drawn uniformly respective range shown. within plot comparison show curves middle curves bethe bottom curves justiﬁed equation case clampings. error shown bethe mixed models best worst curves indicate best worst selection heuristics start clamp point. shown mixed model highly connected strong edge weights much accurate bethe. believe bethe return arbitrarily high error strong frustrated cycles right shows typical plot runtime error sparse models showing average results grid mixed edge weights indicates bethe. time plots models dense edges complete graph look quite different performing signiﬁcantly better. full details results provided appendix. bethe typically dominates accuracy inference method previously observed. however mixed models become densely interconnected strong edges becomes competitive even superior bethe e.g. figure believe bethe return arbitrarily high error strong frustrated cycles figure appendix shows histogram bethe errors mixed models. clamping improves accuracy signiﬁcantly particularly models many strong edge weights. improvement greater random models ﬁxed degree; likely high degree variables present good clamp. heuristics perform well. pseudo-greedy performs almost identically true greedy except there believe part effect highly non-convex optimization true greedy effectively gets beneﬁt many random initializations. clear value probing portfolio heuristics picking best since method dominated. best single performer maxw being augmented core updates best half time appendix details including error plots zoomed around bethe results. runtime varies signiﬁcantly figure typical example. considering clamping approaches greedy takes longest time though yields little beneﬁt pseudogreedy. maxw augmented core updates fast yields best time-adjusted results. appendix timings note sometimes clamping makes subsequent optimization problems easier solve hence total time clamping occasionally lower without also signiﬁcantly accurate. branches multiple clampings parallelized clearly greedy approaches. inference method runs fastest could inﬂuenced implementation timings sensitive parameters. order converge used damping signiﬁcantly slows down though faster convergent methods. further edge weights could optimized implemented libdai. bethe used double-loop algorithm number discrete labels. guarantees difﬁcult obtain apply general bethe prior result knowledge weller jebara bethe restricted case attractive binary pairwise models. clamping leads directly useful improved upper lower bounds true partition function also helpfully optimum bethe approximation. further derivation provides surprising interpretation terms tightening local polytope relaxation theorem earlier approaches selecting variable clamp perform poorly settings. examined likely occur introduced methods based ﬁrst stripping core looking heavy cycles using singleton entropies. methods empirically yielded signiﬁcant beneﬁts. based experimental comparison across different inference approaches clamping selection methods including examining accuracy improvement time tradeoff able suggest following practical recommendations previously observed typically bethe best approach provided convergence difﬁculties arise. however perhaps surprisingly densely connected mixed models strong edges much accurate variable selection speed critical updated maxw heuristic otherwise basket approaches pick pseudo-greedy best option. bethe mixed models guide pseudo-greedy selection. many cases helpful order obtain guaranteed bounds true partition function. bethe method used bounds also useful check poor local optimum returned lauritzen spiegelhalter. local computations probabilities graphical structures application expert systems. journal royal statistical society series libdai free open source library jourdiscrete approximate inference graphical models. machine learning research august http//www.jmlr.org/papers/ volume/mooija/mooija.pdf. details methods used selecting variable clamp. additional experimental details results. additional discussion greedily selecting variable clamp. paths length product modiﬁed edge weights along cycle. compute evaluate examine diagonal terms. however overcounts cycles particular includes relatively high value terms coming paths simply neighbor back again along powers these. order discard these compute clamp score diagonal term minus diagonal term pick variable highest score. details. frustcycle goal identify least frustrated cycle composed edges high absolute weight |wij|. method builds algorithm introduced sontag strongcycles works also takes consideration balanced cycles. approaches ﬁrst examine sign edge weights rather absolute value |wij|. methods value removing cycle estimated using cycscore heuristic lemma weller uses loop series method attempt estimate extent error i.e. caused cycle. estimate positive balanced cycle negative frustrated cycle algorithm provides outline methods. strip model core simply iteratively remove variables degree remain figure typically pre-processing step applying clamp selection heuristics. removing variables care must taken keep track original variable indices remain. methods above ﬁrst strip core. algorithm frustcycles strongcycles methods select variable clamp input edge weight model parameters {wij} output clamp scores variable variable clamp described heuristics maxw mpower frustcycles strongcycles ﬁrst strip core. addition recognizing makes assumption variables independent poor edge strengths strong irrespective cycles present also maxw heuristic ﬁrst strip core total ﬁve. described heurstics performs better worse different contexts. always yields lower bound always yields upper bound ‘probe’ trying heuristics pick yields best improvement take yields take yields lower bound similarly pick heuristic delivers call meta-heuristics pseudo-greedy. addition ‘full greedy’ process clamp possible variables pick best. call full meta-approach greedy. bethe mixed models can’t know advance overunder-estimate cannot exactly thing. instead explored performance achieved picking variable gave best improvement trw; trw-mf two; these greedy-trw heuristic successful version greedy report bethe mixed models. might occasionally observe results worse clamping even theory shows global optimum improve. initializing optimization clamped problem solution parent problem removes concern though necessary practice empirically appeared sufﬁcient initialize using random seed time. bethe easy avoid issue without using expensive methods weller jebara models. grids toroidal variables degree random -regular graphs randomly generated s.t. variables still exactly degree though structure random. random erd¨os-renyi models edge probability s.t. average degree note complete graphs fewer variables much densely connected higher treewidth. ﬁrst provide plots number clamps error runs plots zoomed bethe results easier see; plots runtime error runs. note sometimes clamping makes subsequent optimization problems easier solve hence total time clamping occasionally lower without also signiﬁcantly accurate complete graph variables figure next figure show distribution signed error bethe mixed models showing bias toward overestimation suggested discussion finally figure provide plots showing performance heuristic indicates often picks variable clamp pseudo-greedy speciﬁc clamp step. interesting question whether greedily picking variable gives best error improvement repeating times optimal i.e. result error instead possible sequences clampings long. becomes computationally expensive experiments clampings. observed iterating greedy search optimal full optimization perform better slight margin models tried. figure histograms occurrences signed error bins bethe across runs mixed models. shows error dominated high particularly clamping would expected reasoning", "year": 2015}