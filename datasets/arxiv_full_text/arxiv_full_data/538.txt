{"title": "Structured Attention Networks", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.", "text": "attention networks proven effective approach embedding categorical inference within deep neural network. however many tasks want model richer structural dependencies without abandoning end-to-end training. work experiment incorporating richer structural distributions encoded using graphical models within deep networks. show structured attention networks simple extensions basic attention procedure allow extending attention beyond standard softselection approach attending partial segmentations subtrees. experiment different classes structured attention networks linearchain conditional random ﬁeld graph-based parsing model describe models practically implemented neural network layers. experiments show approach effective incorporating structural biases structured attention networks outperform baseline attention models variety synthetic real tasks tree transduction neural machine translation question answering natural language inference. models trained learn interesting unsupervised hidden representations generalize simple attention. attention networks standard part deep learning toolkit contributing impressive results neural machine translation image captioning speech recognition question answering algorithm-learning among many applications comprehensive review). approach alleviates bottleneck compressing source ﬁxed-dimensional vector equipping model variable-length memory thereby providing random access source needed. attention implemented hidden layer computes categorical distribution make soft-selection source elements. noting empirical effectiveness attention networks also observe standard attentionbased architecture directly model structural dependencies exist among source elements instead relies completely hidden layers network. might argue structural dependencies learned implicitly deep model enough data practice useful provide structural bias. modeling structural dependencies ﬁnal output layer shown important many deep learning applications notably seminal work graph transformers work many areas work consider applications require structural dependencies attention layer develop internal structured layers modeling directly. approach generalizes categorical soft-selection attention layers specifying possible structural dependencies soft manner. applications development attention function segments source input subsequences takes account latent recursive structure source sentence. approach views attention mechanism graphical model latent variables. standard attention network seen expectation annotation function respect single latent variable whose categorical distribution parameterized function source. general case specify graphical model multiple latent variables whose edges encode desired structure. computing forward attention requires performing inference obtain expectation annotation function i.e. context vector. expectation computed exponentially-sized structures hence name structured attention network. notably step process differentiable model trained end-to-end without resort deep policy gradient methods differentiability inference algorithms graphical models previously noted various researchers primarily outside area deep learning. example gormley treat entire graphical model differentiable circuit backpropagate risk variational inference minimium risk training dependency parsers. contribution combine ideas produce structured internal attention layers within deep networks noting approaches allow resulting marginals create features long differentiable way. focus classes structured attention linear-chain conditional random ﬁelds ﬁrst-order graph-based dependency parsers initial work bahdanau particularly interesting context machine translation model able implicitly learn alignment model hidden layer effectively embedding inference neural network. similar vein framework model capacity learn segmenter hidden layer parser hidden layer without ever segmented sentence parse tree. experiments apply approach difﬁcult synthetic reordering task well machine translation question answering natural language inference. models trained structured attention outperform standard attention models. analysis learned representations reveal interesting structures emerge internal layer model. code available http//github.com/harvardnlp/struct-attn. standard neural network consist series non-linear transformation layers layer produces ﬁxed-dimensional hidden representation. tasks large input spaces paradigm makes hard control interaction components. example machine translation source consists entire sentence output prediction word translated sentence. utilizing standard network leads information bottleneck hidden layer must encode entire source sentence. attention provides alternative approach. attention network maintains hidden representations scale size source. model uses internal inference step perform soft-selection representations. method allows model maintain variable-length memory shown crucially important scaling systems many tasks. formally represent sequence inputs query categorical latent variable sample space encodes desired selection among inputs. produce context based sequence query. assume access attention distribution condition inputs query context sequence deﬁned expectation ez∼p] annotation function. attention form applied type input however primarily concerned deep networks annotation function example consider case attention-based neural machine translation sequence inputs hidden states recurrent neural network running words source sentence hidden state target decoder represents source position attended translation. attention distribution simply softmax parameterized potential typically based neural network e.g. mlp). annotation function deﬁned simply return selected hidden state context vector computed using simple tasks question answering attention similar manner instance replacing source potential facts representation question. summary interpret attention mechanism taking expectation annotation function respect latent variable parameterized function attention networks simulate selection using soft model. work consider generalizing selection types attention selecting chunks segmenting inputs even attending latent subtrees. interpretation attention using soft-selection considers possible structures input exponentially many possibilities. course expectation longer computed using simple need incorporate machinery inference directly neural network. deﬁne structured attention model attention model vector discrete latent variables attention distribution deﬁned conditional random ﬁeld specifying independence structure variables. formally assume undirected graph structure vertices. parameterized clique potentials indicates subset given clique deﬁnition symmetry exp)) softmax general sense i.e. softmax) implied partition function. practice neural comes deep model structured attention also assume annotation function factors clique standard conditions conditional independence structure inference techniques graphical models used compute forwardpass expectations context suppose instead soft-selecting single input wanted explicitly model selection contiguous subsequences. could naively apply categorical attention subsequences hope model learns multi-modal distribution combine neighboring words. structured attention provides alternate approach. concretely deﬁne random vector deﬁne }xi. figure three versions latent variable attention model standard soft-selection attention network bernoulli attention network linear-chain structured attention model segmentation. input query denoted respectively. equation similar equation —both linear combination input representations scalar represents much attention focused input. however fundamentally different ways allows multiple inputs selected given query; incorporate structural dependencies across zi’s. instance model distribution linear-chain pairwise edges pairwise potential model shown figure compare model standard attention figure simple bernoulli selection method sigmoid shown figure three methods potentials neural network takes inputs. case linear-chain marginal distribution calculated efﬁciently linear-time using message-passing i.e. forward-backward algorithm. marginals allow calculate implicitly exponentially-sized structures dynamic programming. refer type attention layer segmentation attention layer. note forward-backward algorithm used parameterized pooling thought generalizing standard attention softmax. crucially generalization vector softmax forward-backward series differentiable steps compute gradients output respect input allow structured attention model trained end-to-end part deep model. approach used involved structural dependencies. popular structure natural language tasks dependency tree enforces structural bias recursive dependencies common many languages. particular dependency tree enforces word source sentence assigned exactly parent word assignments cross employing bias encourages system make soft-selection based learned syntactic dependencies without requiring linguistic annotations pipelined decision. dependency parser partially formalized graphical model following cliques latent variables indicates i-th word parent j-th word special global constraint rules conﬁgurations zij’s violate parsing constraints parameters graph-based dependency parser potentials reﬂect score selecting parent probability parse tree given sentence figure algorithms linear-chain computation forward-backward tables marginal probabilities potentials backpropagation loss gradients respect marginals denotes state space special start/stop state. backpropagation uses identity element-wise multiplication. typically forward-backward marginals performed log-space semiﬁeld r∪{±∞} binary operations logadd numerical precision. however backpropagation requires working negative values extend ﬁeld {+−} special log-space operations. binary operations applied vectors implied element-wise. signexp function deﬁned signexp exp. section table details. note case annotation function subscript produce context vector word sentence. similar types attention applied tree properties refer type attention layer syntactic attention layer. graphical models form widely used ﬁnal layer deep models. contribution argue networks added within deep networks place simple attention layers. whole model trained end-to-end. main complication utilizing approach within network need backpropagate gradients inference algorithm part structured attention network. past work demonstrated techniques necessary approach knowledge rarely employed. consider case simple linear-chain layer equation figure shows standard forward-backward algorithm computing marginals treat forward-backward algorithm neural network layer input potentials output forward pass marginals. backpropagate loss layer need compute gradient loss respect function gradient loss respect marginals forward-backward algorithm consists differentiable steps function derived using reverse-mode automatic differentiation forward-backward algorithm itself. note reverse-mode algorithm conveniently parallel structure forward version also implemented using dynamic programming. however practice cannot simply current off-the-shelf tools task. efﬁciency quite important models beneﬁts handoptimizing reverse-mode implementation still outweighs simplicity automatic differentiation. secondly numerical precision becomes major issue structured attention networks. computing forward-pass marginals important standard log-space semiﬁeld {±∞} binary operations avoid underﬂow probabilities. computing backward-pass need remain logspace also handle negative values requires extending signed log-space semiﬁeld {+−} special operations. table based eisner demonstrates handle issue figure describes backpropagation forward-backward algorithm. dependency parsing forward pass computed using inside-outside implementation eisner’s algorithm similarly backpropagation parallels inside-outside structure. forward/backward pass inside-outside algorithm described appendix experiment three instantiations structured attention networks four different tasks simple synthetic tree manipulation task using syntactic attention layer machine translation segmentation attention question answering using nstate linear-chain multi-step inference facts natural language inference syntactic tree attention. experiments intended boost state-of-the-art tasks test whether methods trained effectively end-to-end fashion yield improvements standard selection-based attention learn plausible latent structures. model architectures hyperparameters training details described appendix ﬁrst experiments look tree-transduction task. experiments synthetic data explore failure case soft-selection attention models. task learn convert random formula given preﬁx notation inﬁx notation e.g. alphabet consists symbols numbers special root symbol task used preliminary task model able learn implicit tree structure source side. model encoder-decoder model encoder deﬁned decoder lstm. appendix full model. confusingly forward case different forward-backward algorithm marginals output. however uses term actually quite related. forward-backward algorithm interpreted forward backpropagation pass partition function. eisner details full approach seen computing second-order information. interpretation central eisner figure visualization source self-attention distribution simple structured attention models tree transduction task. special root symbol. delineates distribution parents attention distribution obtained parsing marginals able capture tree structure—e.g. attention weights closing parentheses generally placed opening parentheses training uses preﬁx-inﬁx pairs maximum nesting depth pairs depth bucket. number expressions parenthesis limited test uses unseen sequences depth sequences depth. performance measured average proportion correct target tokens produced ﬁrst failure experiments using different forms self -attention embedding-only encoders. embedding source symbol; three variants source representation atten symbol embeddings themselves i.e. simple attention symbol softmaxxi calculated using soft-selection; structured attention symbol embeddings soft-parent i.e. calculated using parsing marginals obtained syntactic attention layer. none models explicit query value—the potentials come running bidirectional lstm source producing hidden vectors computing source representation attended using standard attention mechanism decoding step lstm decoder. additionally symbol embedding parameters shared parsing lstm source encoder. results table results task. note task fairly difﬁcult encoder quite simple. baseline model performs poorly information source ordering. simple attention model performs better signiﬁcantly outperformed structured model tree structure bias. hypothesize model partially reconstructing arithmetic tree. figure shows attention distribution simple/structured models source sequence indicates structured model able learn boundaries thus attention mechanisms work setup. first structured attention source obtain soft-parents symbol second standard softmax alignment attention source representations decoding. second experiments full neural machine translation model utilizing attention subsequences. encoder/decoder lstms replace standard simple attention segmentation attention layer. experiment settings translating directly unsegmented japanese characters english words translating segmented japanese words english words japanese word segmentation done using kytea toolkit data comes workshop asian translation randomly pick sentences original training japanese sentence characters english sentence words. apply length ﬁlter provided validation/test sets evaluation. vocabulary consists tokens occurred least times training corpus. segmentation attention layer two-state unary potentials j-th decoder step parameterized results results translation task test given table sigmoid attention outperforms simple attention character-toword task potentially able learn manyto-one alignments. word-to-word task opposite true simple attention outperforming sigmoid attention. structured attention outperforms models tasks although improvements word-to-word task modest unlikely statistically signiﬁcant. analysis figure shows visualization different attention mechanisms character-to-word setup. simple model generally focuses attention heavily single character. contrast sigmoid structured models able spread attention distribution contiguous subsequences. structured attention learns additional parameters smooth type attention. phi) empirically observed marginals quickly saturate. tried various strategies overcome this putting penalty unary potentials initializing pretrained sigmoid attention model simply normalizing marginals proved effective. however changes interpretation context vector expectation annotation function case. figure visualization source attention distribution simple sigmoid structured attention models ground truth sentence character-to-word translation task. manually-annotated alignments shown bottom right. delineates attention weights source sentence step decoding. sigmoid/structured attention models able learn implicit segmentation model focus multiple characters time step. third experiment question answering linear-chain attention layer inference multiple facts. babi dataset input sentences/facts paired question answer single token. many tasks model attend multiple supporting facts arrive correct answer existing approaches multiple ‘hops’ greedily attend different facts. experiment employing structured attention perform inference non-greedy way. ground truth supporting facts given dataset able assess model’s inference accuracy. baseline attention model end-to-end memory network brieﬂy describe here. appendix full model details. input embedding vectors sentences/facts query embedding. memnn random variable sentence select k-th inference step thus probability distribution given softmax. k-th context query vectors used obtain ﬁnal answer. attention mechanism k-hop memnn network therefore interpreted greedy selection length-k sequence facts structured attention n-state k-step linear-chain crf. experiment different settings unary model node potentials task task supporting facts task three supporting facts task counting task lists sets task indefinite knowledge task compound coreference task time reasoning task basic deduction task basic induction task positional reasoning task size reasoning task path finding average table answer accuracy supporting fact selection accuracy three models babi dataset. indicates number hops/inference steps used task. task contain variable number facts hence excluded fact accuracy measurement. supporting fact selection accuracy calculated taking average best runs task. binary model designed test model’s ability perform sequential reasoning. evaluating requires summing possible sequences length practical large values however rewrite pfk. experiments results version dataset questions task. since models reduce network tasks supporting fact excluded experiments. number hops task-dependent number memories limited high variance model performance train models different initializations task report test accuracy model performed best held-out validation results three different models shown table correct answer seletion memnn binary model perform similarly unary model worse indicating importance including pairwise potentials. also assess model’s ability attend correct supporting facts table since ground truth supporting facts provided query check sequence accuracy supporting facts model taking highest probability sequence argmax model checking ground truth. overall binary able recover supporting facts better memnn. improvement signiﬁcant two-fold seen task however observed many tasks sufﬁcient select last fact correctly predict answer thus higher sequence selection accuracy necessarily imply better answer accuracy example three models answer accuracy task different supporting fact accuracies. finally figure visualize output edge marginals produced binary model single question task instance model uncertain ultimately able select right sequence facts figure visualization attention distribution supporting fact sequences example question task binary model. actual question displayed bottom along correct answer ground truth supporting facts edges represent marginal probabilities nodes represent supporting facts text supporting facts shown left. three likely sequences ﬁnal experiment looks task natural language inference syntactic attention layer. model given sentences predict relationship entailment contradiction neutral. task stanford dataset model approach decomposable attention model parikh model takes matrix word embeddings input sentence performs inter-sentence attention predict answer. appendix describes full model. transduction task focus modifying input representation take account soft parents self-attention addition three baselines described tree transduction also explore additional settings hard pipeline parent selection i.e. head index xj’s parent; pretrained structured attention structured attention parsing layer pretrained epoch parsed dataset results results models shown table simple attention improves upon attention model consistent improvements observed parikh intra-sentence attention model. pipelined model hard parents also slightly improves upon baseline. structured attention outperforms models though surprisingly pretraining syntactic attention layer parse trees performs worse training scratch—it possible pretrained attention strict task. model handcrafted features lstm encoders tree-based stack-augmented parser-interpreter neural lstm word-by-word attention matching lstms decomposable attention word embeddings decomposable attention intra-sentence attention attention constituency tree nodes neural tree indexers enhanced bilstm inference model enhanced bilstm inference model ensemble attention attention hard parent simple attention structured attention pretrained structured attention table results models others stanford test set. baseline model architecture parikh performance slightly different different settings train epochs batch size using asynchronous sgd.) despite trained without ever exposed explicit parse tree syntactic attention layer learns almost plausible dependency structure. example able correctly identify main verb fighting makes mistakes determiners generally observed pattern across sentences possibly verb structure important inference task. work outlines structured attention networks incorporate graphical models generalize simple attention describes technical machinery computational techniques backpropagating models form. implement classes structured attention layers linear-chain complicated ﬁrst-order dependency parser experiments show method learn interesting structural properties improve standard models. structured attention could also learning latent labelers parsers attention tasks. noted additional complexity computing attention distribution increases run-time—for example structured attention approximately slower train simple attention neural machine translation experiments even though attention layers asymptotic run-time embedding differentiable inference deep models exciting area research. focused models admit exact inference similar technique used embed approximate inference methods. many optimization algorithms also differentiable used output layers structured prediction energy-based models incorporating internal neural network layers interesting avenue future work. thank ankur parikh vieira matt gormley andr´e martins jason eisner yoav goldberg anonymous reviewers helpful comments discussion notes code. additionally thank yasumasa miyamoto verifying japanese-english translations. references daniel andor chris alberti david weiss aliaksei severyn alessandro presta kuzman ganchev slav petrov michael collins. globally normalized transition-based neural networks. proceedings samuel bowman christopher manning christopher potts. tree-structured composition neural networks without tree-structured architectures. proceedings nips workshop cognitive computation integrating neural symbolic approaches samuel bowman gauthier abhinav rastogi raghav gupta christopher manning christopher potts. fast uniﬁed model parsing sentence understanding. proceedings ronan collobert jason weston leon bottou michael karlen koray kavukcuoglu pavel kuksa. natural language processing scratch. journal machine learning research alex graves greg wayne malcolm reynolds harley danihelka agnieszka grabskabarwinska sergio gomez colmenarejo edward grefenstette tiago ramalho john agapiou adria puigdomenech badia karl moritz hermann yori zwols georg ostrovski adam cain helen king christopher summerﬁeld phil blunsom koray kavukcuoglu demis hassabis. hybrid computing using neural network dynamic external memory. nature october karl moritz hermann tomas kocisky edward grefenstette lasse espeholt mustafa suleyman phil blunsom. teaching machines read comprehend. proceedings nips liang lingpeng kong chris dyer noah smith steve renals. segmental recurrent neural networks end-to-end speech recognition. proceedings interspeech toshiaki nakazawa manabu yaguchi kiyotaka uchimoto masao utiyama eiichiro sumita sadao kurohashi hitoshi isahara. aspec asian scientiﬁc paper excerpt corpus. nicoletta calzolari khalid choukri thierry declerck marko grobelnik bente maegaard joseph mariani asuncion moreno odijk stelios piperidis proceedings ninth international conference language resources evaluation portoro slovenia european language resources association isbn ----. john schulman nicolas heess theophane weber pieter abbeel. gradient estimation using stochastic computation graphs. advances neural information processing systems veselin stoyanov alexander ropson jason eisner. empirical risk minimization graphical model parameters given approximate inference decoding model structure. proceedings aistats jason weston antoine bordes sumit chopra alexander rush bart merri¨enboer armand joulin tomas mikolov. towards ai-complete question answering prerequisite tasks. arxiv preprint arxiv. kelvin jimma ryan kiros kyunghyun aaron courville ruslan salakhutdinov richard zemel yoshua bengio. show attend tell neural image caption generation visual attention. proceedings icml syntactic attention layer similar ﬁrst-order graph-based dependency parser kipperwasser goldberg given input sentence corresponding word vectors bidirectional lstm hidden states time step sequence source/target symbols associated embeddings simplest baseline model take source representation matrix symbol embeddings. decoder one-layer lstm hidden states combined produces hidden states input representation bilinear rl×l produce attention distribution used obtain vector combined decoder hidden state follows note case rl×l rl×l. experiments. forward/backward lstms parsing lstm also -dimensional. symbol embeddings shared encoder parsing lstms. additional training details include batch size training epochs learning rate starts decaying half epoch parameter initialization uniform distribution gradient normalization decoding done beam search simple decoder another lstm produces hidden states attention case categorical attention hidden states combined input representation bilinear rl×l distribution used obtain context vector j-th time step encoder/decoder lstms layers hidden units additional training details include batch size training epochs learning rate starts decaying half ﬁrst epoch performance improve validation; dropout probability parameter initialization uniform distribution gradient normalization generate target translations beam search evaluate multi-bleu.perl moses. baseline model implemented following architecture described sukhbaatar particular represent sequence facts associated embeddings embedding query embeddings distribution answer vocabulary. layer computed using embedding matrices adjacent weight tying scheme paper also used compute query embedding ﬁrst hop. unary binary models input fact query representations computed unary model potentials parameterized case binary discourage model selecting fact additionally θkk+ given potentials compute marginals using forward-backward algorithm used compute context vector training setup similar sukhbaatar stochastic gradient descent learning rate divided every epochs epochs reached. capacity memory limited sentences. embedding vectors size gradients renormalized norm exceeds models implement position encoding temporal encoding linear start original paper. linear start softmax function attention layer removed beginning re-inserted epochs memnn models apply log) layer epochs. model trained separately task. baseline model/setup essentially parikh premise/hypothesis corresponding input representations input representations obtained linear transformation -dimensional pretrained glove embeddings normalizing glove embeddings unit norm. pretrained embeddings remain ﬁxed linear layer trained. words pretrained vocabulary hashed gaussian embeddings mean standard deviation concatenate input representation convex combination sentence’s input representations weights determined product followed softmax mlps -layers relu units dropout probability structured/simple models ﬁrst employ bidirectional parsing lstm obtain scores θij. structured case word representation simply concatenated soft-parent word embeddings parsing lstms also initialized glove parsing layer shared sentences. forward/backward lstms parsing layer dimensional. additional training details include batch size training epochs adagrad global learning rate gradient squared initialized parameter intialization gaussian distribution mean standard deviation gradient normalization pretrained scenario pretraining done adam learning rate equal figure shows procedure obtaining parsing marginals input potentials. corresponds running inside-outside version eisner’s algorithm intermediate data structures used dynamic programming algorithm inside tables outside tables size sentence length. first dimensions encode start/end index span third dimension encodes whether root subtree left right index span. fourth dimension indicates span complete incomplete calculate marginal distribution word’s parent using algorithm. backward pass inside-outside algorithm slightly involved still takes time. figure illustrates backward procedure receives gradient loss respect marginals computes gradient loss respect potentials computations must performed signed log-space semiﬁeld handle negative values. section table details. figure forward step syntatic attention layer compute marginals using inside-outside algorithm data structures eisner assume special root symbol ﬁrst element sequence sentence length calculations performed log-space semiﬁeld logadd numerical precision. means means", "year": 2017}