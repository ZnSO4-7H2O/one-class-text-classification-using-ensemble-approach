{"title": "Less Regret via Online Conditioning", "tag": ["cs.LG", "cs.AI"], "abstract": "We analyze and evaluate an online gradient descent algorithm with adaptive per-coordinate adjustment of learning rates. Our algorithm can be thought of as an online version of batch gradient descent with a diagonal preconditioner. This approach leads to regret bounds that are stronger than those of standard online gradient descent for general online convex optimization problems. Experimentally, we show that our algorithm is competitive with state-of-the-art algorithms for large scale machine learning problems.", "text": "analyze evaluate online gradient descent algorithm adaptive per-coordinate adjustment learning rates. algorithm thought online version batch gradient descent diagonal preconditioner. approach leads regret bounds stronger standard online gradient descent general online convex optimization problems. experimentally show algorithm competitive state-of-the-art algorithms large scale machine learning problems. past years online algorithms emerged state-of-the-art techniques solving large-scale machine learning problems addition simplicity generality online algorithms natural choices problems data constantly arriving rapid adaptation imporant. compared study convex optimization batch setting study online convex optimization relatively new. light this surprising performance-improving techniques well known widely used batch setting online analogues. particular convergence rates batch setting often dramatically improved preconditioning. online convex optimization literature provides comparable method improving regret simple eﬀective form preconditioning re-parameterize loss function magnitude coordinate directions. without modiﬁcation batch algorithm gradient descent tend take excessively small steps along axes oscillate back forth along others slowing convergence. online setting rescaling cannot done front loss functions vary time known advance. result existing no-regret algorithms online convex optimization applied machine learning problems tend overﬁt data respect certain show problem overcome principled using online gradient descent adaptive per-coordinate learning rates. algorithm comes worst-case regret bounds never worse standard online gradient descent much better magnitude gradients varies greatly across coordinates extending approach give improved bounds generalized notions strong convexity bounds terms variance cost functions bounds adaptive regret experimentally show algorithm dramatically outperforms standard online gradient descent real-world problems competitive state-of-the-art algorithms online binary classiﬁcation. optimization. case vector weights weight assigned feature round round algorithm makes prediction feature vector +exp logistic regression linear regression). algorithm incurs loss function prediction label example. example logistic regression loss log) least squares linear regression loss examples shown sequence learning rates subgradient miny∈f projection operator norm. learning rates chosen appropriately online gradient descent obtains regret maxxy∈f diameter feasible maxt {kgtk} maximum norm gradients. thus average loss points selected online gradient descent good ﬁxed point feasible set. perhaps surprising performance guarantee holds well-known batch gradient descent performs poorly presence so-called ravines surfaces curve steeply directions others section give examples showing slope loss function size feasible varies widely across coordinates gradient descent incurs high regret online setting. observations motivate per-coordinate learning rates consider problem trying predict probability user click shown alongside search results particular query using generalized linear model. simplicity imagine wish predict click-through rate many diﬀerent queries. large search engine popular query occur orders magnitude often rare query. queries occur rarely necessary relatively large learning rate order associated feature weights move signiﬁcantly away zero. popular queries large learning rate cause feature weights oscillate wildly predictions made algorithm unstable. thus gradient descent global learning rate cannot simultaneously perform well common queries rare ones. rare queries numerous common ones performing poorly either category leads substantial regret. simple example suppose feasible loss function round g|x− small positive easy verify algorithm plays initially play rounds even rounds assuming thus rounds algorithm incurs total loss always playing would incur zero loss regret hand small stay close zero long data indicates larger would incur smaller loss. example suppose always. min{d gη}. ﬁrst therefore per-round regret relative comparator captures fundamental tradeoﬀ. feasible large gradients small must larger learning rate order competitive points extremes feasible set. hand feasible small gradients large must smaller learning rate order avoid possibility oscillating extremes performing poorly relative points center. relevant values general diﬀerent diﬀerent coordinates gradient descent algorithm uses learning rate coordinates doomed either underﬁt coordinates oscillate others. handle this must diﬀerent learning rates diﬀerent coordinates. furthermore magnitude gradients known advance change time must incorporate choice learning rate online fashion. exhibit class online convex optimization problems coordinate-independent learning rate forces regret grow asymptotically larger rate per-coordinate learning rate. result summarized following theorem. theorem exists family online convex optimization problems parameterized lengths gradient descent non-increasing global learning rate incurs regret least subproblem ﬁrst type lasting rounds followed subproblems second type lasting rounds. subproblem assigned coordinate. formally loss function round component gradient vector non-zero. thus running gradient descent global learning rate equivalent running separate copy gradient descent subproblem copy uses learning rate moreover overall regret simply regret second otherwise simple minimization regret ﬁrst subproblem increasing function shows lower bound holds non-increasing sequence per-round learning rates. thus proved ﬁrst part theorem. consider alternative letting learning rate coordinate vary independently. one-dimensional subproblem feasible gradients magnitude gradient descent zinkevich proved bounds regret online gradient descent building analysis improve bounds adjusting learning rates percoordinate basis. speciﬁcally obtain bounds constructing vector ﬁrst give improved regret bound gradient descent global learning rate. next subsection make improved bound order prove desired bounds regret gradient descent per-coordinate learning rate. knowing advance; regret bound algorithm never worse bound global learning rates stated theorem futhermore illustrated theorem per-coordinate bound better arbitrarily large factor magnitude gradients varies widely across coordinates. proof. zinkevich showed that long algorithm makes assume without loss generality linear therefore hypercube projection operator simply projects coordinate indepdently onto interval thus special case think coordinate solving separate online convex optimization problem approach bounding overall regret terms regret one-dimensional problems used obtain additional regret bounds improve previous work special case feasible hypercube. observation captured following lemma. lemma consider online optimization problem feasible loss functions lower bound suppose {xt} sequence points played independently coordinate feasible loss function round observation originally used zinkevich show algorithm online linear optimization used online convex optimization. used fact proof theorem analyzed linear case. simple lemma powerful ramiﬁcations. discuss several improved guarantees obtained applying known online algorithms. simplicity stating bounds assume feasible gradients loss functions componentwise upper bounded thus bound obtained running separate copies algorithm coordinate never worse original bound substantially better variance varies greatly across coordinates. weakness standard regret bounds like stated bound performance terms static optimal solution rounds. non-stationary environment desirable obtain stronger guarantees. example suppose feasible ﬁrst rounds thereafter. algorithm plays regret loss ﬁnal rounds worse played point rounds. indeed standard regret-minimizing algorithms fail adapt simple examples this. regret relative sequence. holding constant simplicity adaptive regret bound stated implies algorithm hazan seshadhri obtains number values using separate copies coordinate instead obtain provides improved performance guarantee environment stationary respect coordinates non-stationary respect others. could happen example eﬀect certain features changes time eﬀect features remains constant. ﬁrst compare performance online gradient descent recent algorithms text classiﬁcation passive-aggressive algorithm conﬁdence-weighted linear classiﬁcation latter algorithm demonstrated state-of-the-art performance large real-world problems used four sentiment classiﬁcation data sets available positive examples negative examples well scaled versions rcv.binary news.binary data sets libsvm data shuﬄed examples algorithm pass data computing loss event training rate suggested theorem aggressive practice feasible large order improve performance parameter tuning. algorithm scaled learning rate formula factor global learning rate scaled ./r. estimate diameter global learning rate formula online based number attributes seen far. found parameters worked well practice. table presents average hinge loss fraction classiﬁcation mistakes algorithm. global per-coord algorithms designed minimize hinge loss objective per-coord algorithm consistently wins. designed maximize classiﬁcation accuracy objective per-coord best algorithms. fact classiﬁcation accuracy per-coord comparable state-of-the-art binary classiﬁcation algorithm impressive given former algorithm’s generality collected data large search engine consisting random samples queries contained particular phrase example auto insurance. data million examples. transformed data online logistic regression problem feature vector impression using features based text query. target label clicked otherwise. loss function logistic loss regularization compare gradient descent using global learning rate gradient descent using per-coordinate rate given scaled formulas given sections improved performance algorithms change relative comparison. feasible table shows regret incurred algorithms various data sets. gradient descent per-coordinate learning rate consistently obtains order magnitude lower regret global diﬀerent learning rates diﬀerent coordinates investigated extensively neural network community. focus empirical performance batch setting large number algorithms developed; example algorithms designed perform well adversarial online setting many straightforward construct examples algorithm incurs high regret. recently gave algorithm choosing per-coordinate learning rates gradient descent derive asymptotic rates convergence batch setting present number positive experimental results. conﬁdence-weighted linear classiﬁcation arow similar algorithm make diﬀerent-sized adjustments diﬀerent coordinates common features updated less aggressively rare ones. unlike algorithm algorithms apply classiﬁcation problems general online convex optimization guarantees form mistake bounds rather regret bounds. concurrent work generalize results paper handle arbitrary feasible sets matrix learning rate parameters. similar theoretical results obtained independently duchi", "year": 2010}