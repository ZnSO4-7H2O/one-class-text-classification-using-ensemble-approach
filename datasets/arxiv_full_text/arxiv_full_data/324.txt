{"title": "Visual Causal Feature Learning", "tag": ["stat.ML", "cs.AI", "cs.CV", "cs.LG"], "abstract": "We provide a rigorous definition of the visual cause of a behavior that is broadly applicable to the visually driven behavior in humans, animals, neurons, robots and other perceiving systems. Our framework generalizes standard accounts of causal learning to settings in which the causal variables need to be constructed from micro-variables. We prove the Causal Coarsening Theorem, which allows us to gain causal knowledge from observational data with minimal experimental effort. The theorem provides a connection to standard inference techniques in machine learning that identify features of an image that correlate with, but may not cause, the target behavior. Finally, we propose an active learning scheme to learn a manipulator function that performs optimal manipulations on the image to automatically identify the visual cause of a target behavior. We illustrate our inference and learning algorithms in experiments based on both synthetic and real data.", "text": "provide rigorous deﬁnition visual cause behavior broadly applicable visually driven behavior humans animals neurons robots perceiving systems. framework generalizes standard accounts causal learning settings causal variables need constructed micro-variables. prove causal coarsening theorem allows gain causal knowledge observational data minimal experimental effort. theorem provides connection standard inference techniques machine learning identify features image correlate with cause target behavior. finally propose active learning scheme learn manipulator function performs optimal manipulations image automatically identify visual cause target behavior. illustrate inference learning algorithms experiments based synthetic real data. visual perception important trigger human animal behavior. visual cause behavior easy deﬁne trafﬁc light turns green quite subtle apparently increased symmetry features leads people judge faces attractive others signiﬁcant scientiﬁc economic effort focused visual causes advertising entertainment communication design medicine robotics study human animal cognition. visual causes profoundly inﬂuence daily activity understanding constitutes visual cause lacks theoretical basis. practice well-known images composed millions variables functions pixels meaning rather pixels themselves. present theoretical framework inference algorithms visual causes images. visual cause deﬁned function image pixels causal effect target behavior perceiving system interest. present three advances provide deﬁnition visual cause target behavior macro-variable constructed micro-variables make image space. visual cause distinguished macro-variables contains causal information target behavior available image. place visual cause within standard framework causal graphical models thereby contributing account construct causal variables. prove causal coarsening theorem shows observational data used learn visual cause minimal experimental effort. connects present results standard classiﬁcation tasks machine learning. describe method learn manipulator function automatically performs perceptually optimal manipulations visual causes. illustrate ideas using synthetic real-data experiments. python code implements algorithms well reproduces experimental results available online http//vision.caltech.edu/ ˜kchalupk/code.html. chose develop theory within context visual causes setting makes deﬁnitions intuitive signiﬁcant practical interest. however framework results equally well applied extract causal information aggregate microvariables manipulations possible. examples include auditory olfactory sensory stimuli; highdimensional neural recordings; market data ﬁnance; consumer data marketing. there causal feature learning theoretical practical importance. framework extends theory causal graphical models setting input data consists pixel data. contrast standard setting macro-variables statistical dataset already specify candidate causal relata causal variables setting constructed micro-variables supervene causal relations established. emphasize difference method causal feature learning methods causal feature selection latter choose best features restricted plausible macro-variable candidates. contrast framework efﬁciently searches whole space possible macro-variables constructed image. approach derives theoretical underpinnings computational mechanics supports explicitly causal interpretation incorporating possibility confounding interventions. since allow unmeasured common causes features image target behavior distinguish plain conditional probability distribution target behavior given image distribution target behavior given observed image manipulated hoel develop similar model investigate relationship causal micromacro-variables avoid distinction assuming data generated setting would manipulated distribution take distinction interventional observational distributions features causal analysis. extant literature causal learning image video data generally consider aggregation pixel variables causal macro-variables instead starts annotated pre-deﬁned features image fig. presents paradigmatic case study visual causal feature learning running example. contents image caused external nonvisual binary hidden variables contains vertical random position contains horizontal random position. target behavior caused likely whenever whenever image contains h-bar. deliberately constructed example visual cause clearly identiﬁable manipulating presence h-bar image inﬂuence distribution thus call following function causal feature visual cause presence v-bar hand causal feature. manipulating presence v-bar image effect still presence v-bar strongly correlated value presence h-bar call following function spurious correlate presence h-bars presence v-bars good individual predictors target variable cause. identifying visual cause image thus requires ability distinguish among correlates target variables actually causal even non-causal correlates correlated target. values example stand bijective correspondence values respectively keep illustration simple. general visual cause spurious correlate probabilistic functions number hidden variables share hidden causes. example identiﬁcation visual cause presence h-bar intuitively obvious model constructed easily describable visual cause. example provide theoretical account takes visual cause general case know causally relevant pixel conﬁgurations are. section provide general account visual cause related pixel data. visual cause high-level random variable function image turn deﬁned random micro-variables determine pixel values. functional relation image visual cause general surjective though principle could bijective. interested identifying information available image given behavior deﬁne manipulation. make problem approachable introduce assumptions causal relation image behavior value target behavior determined subsequently image time variable represented image. assumptions exclude possibility cause features image seen causing itself. represent target behavior. discrete space images inﬂuence target behavior following generative model describe relation images target behavior image generated ﬁnite unobserved discrete variables target behavior determined image possibly subset variables confounders image target behavior independent noise contribute target behavior marginalized omitted sake simplicity equation. noise term incorporates hidden variables inﬂuence behavior stand causal relation image. variables directly relevant problem. fig. shows generative model. model deﬁne observational partition space images groups images classes conditional probability deﬁnition observational partition w.r.t. behavior partition induced equivalence relation denote context clear. cell observational partition called observational class. standard classiﬁcation tasks machine learning observational partition associated class labels. case images belong cell observational partition assign equal predictive probability target behavior. thus knowing observational class figure case study generative model. binary hidden variables toss unbiased coins. content image depends variables follows. chosen uniformly random images containing v-bars h-bars. chosen uniformly random images containing least h-bar v-bars. chosen uniformly random images containing least v-bar h-bars. finally chosen images containing least v-bar least h-bar. distribution binary behavior depends presence h-bar value observational studies contains v-bar. however manipulation speciﬁc image introduces v-bar general change probability occurring. thus depend causally presence v-bars visual causes target behavior functional relation image pixels visual cause interpreted causal. pixels cause features image constitute them atoms table constitute table difference causal constitutive relation former requires possibility independent manipulation whereas deﬁnition cannot manipulate visual cause without manipulating image pixels. probability distribution visual cause induced probability distribution pixels image functional mapping image visual cause. since visual cause stands constitutive relation image cannot without explanation describe interventions visual cause terms standard do-operation goal deﬁne macro-variable contains causal underlying idea images considered causally equivalent respect causal effect given causal partition image space deﬁne visual cause deﬁnition visual cause target behavior random variable whose value stands bijective relation causal class visual cause thus function whose values correspond post-manipulation distributions write indicate causal class image words image visual cause takes value knowing allows predict effects visual manipulation long estimated among generative distributions form shown fig. induce given observational partition almost induce causal partition coarsening throughout article almost mean except subset lebesgue measure zero. fig. illustrates relation causal observational partition implied theorem. note measure-zero subset coarsen indeed non-empty. provide counter-examples appendix prove appendix using technique extends meek show restricting space possible distributions compatible ﬁxed observational partition puts linear constraint distribution space; requiring false puts non-trivial polynomial constraint subspace ﬁnally follows theorem holds almost distributions agree given observational partition. proof strategy indicates close connection faithfulness assumption points worth noting here first interesting inasmuch visual causes behavior contain information image predict behavior. information though cause figure general model visual causation. model image caused number hidden nonvisual variables need independent. image observed cause target behavior addition subset hidden variables cause target behavior. confounders create visual spurious correlates behavior image allows predict value however predictive probability assigned image tell causal effect image example barometer widely taken excellent predictor weather. changing barometer needle cause improvement weather. cause weather. contrast seeing particular barometer reading well visual cause whether pack umbrella. notion visual cause depends ability manipulate image. deﬁnition visual manipulation operation changes image image affecting variables manipulated probability distribution generative model given manipulation changes values image pixels change underlying world represented model generated image. formally manipulation similar do-operator standard causal models. however reserve do-operation interventions causal macro-variables visual cause discuss distinction detail below. deﬁne causal partition image space deﬁnition causal partition w.r.t. behavior partition induced equivalence relation deﬁned image space target behavior clear context indicate causal partition cell causal partition figure causal coarsening theorem. observational probabilities given induce observational partition space images causal probabilities induce causal partition indicated left red. allows expect causal partition coarsening observational partition. observational causal probabilities correspond generative model shown fig. behavior informative state non-visual causes target behavior. second allows take classiﬁcation problem data divided observational classes assume causal labels change within observational class. help develop efﬁcient causal inference algorithms section simplify generative model omitting information unrelated behavior assume reﬁnes causal partiobservational partition causal classes c··· delineates tion region image space images belonging region induce regions—say k-th one—can partitioned sub-regions images m-th sub-region k-th causal region induce observational probability assumption observational partition ﬁnite number classes arbitrarily order observational classes within causal class. ordering ﬁxed assign integer image belonging k-th causal class belongs m-th observational class among observational classes contained construction integer explains variation observational class within given causal class. suggests following deﬁnition deﬁnition spurious correlate discrete random variable whose value differentiates observational classes contained causal figure macro-variable model visual causation. using theory visual causation aggregate information present visual micro-variables visual cause spurious correlate according theorem contain information available class. spurious correlate well-deﬁned function whose value ranges maxk like spurious correlate macro-variable constructed pixels make image. together contain visual information relevant contains causal information theorem following statements hold deﬁned above prove theorem appendix guarantees constitute smallest-entropy macro-variables encompass information relationship between fig. shows relationship image space observational causal partitions schematically. cause correlates unobserved common causes information irrelevant pushed independent noise variables macro-variable model lends standard treatment causal graphical models described pearl deﬁne interventions causal variables using standard do-operation. dooperator sets value intervened variable desired value making independent causes affect variables system relationships however unlike standard case causal variables separated location causal variables image involve pixels average brightness image whereas indicate presence absence particular shapes image. intervention causal variable using do-operator thus requires underlying manipulation image respects state causal variables deﬁnition given macro-variables take values image intervention macro-variable given manipulation image man) intervention deﬁned analogously change underlying image keeps value constant. cases impossible manipulate desired value without changing take problem special case. fact standard macrovariable setting causal analysis would expect interventions much restricted physical constraints interventions image space. given theoretical speciﬁcation concepts interest previous section develop algorithms learn visual cause behavior. addition knowledge allow specify manipulator function function that given image return maximally similar image desired causal effect. deﬁnition causal variable metric manipulator function function minˆı∈c− case multiple minima group together equivalence class leave choice representative manipulator function. manipulator searches image closest among images desired causal effect meaning closest depends metric discussed section below. note manipulator function candidates image manipulation underlying desired causal manipulation check whether variables system remain fact unchanged. using closest possible image desired causal effect heuristic approach fulﬁlling requirement. goal perform causal manipulations images manipulator function offers automated solution. manipulator uses given produces images desired causal effect provides strong evidence indeed visual cause behavior. using manipulator function enrich dataset datapoints hope achieving better generalization causal predictive learning tasks. standard machine learning approach learning relation would take observational dataset dobs {)}k=··· learn predictor whose training performance guarantees test error test image i∗). causal feature learning test error observational data insufﬁcient; entirely possible contains spurious information useful predicting test labels nevertheless causal. prediction highly accurate observational data completely inaccurate prediction effect manipulation image however obtain causal dataset observational data train predictor dataset. algorithm uses strategy learn function that presented image returns ﬁxed neural network architecture learn differentiable hypothesis class could susbtituted instead. differentiability necessary section order learn manipulator function. space dcsl {··· causal data {c··· causal classes train neural training algorithm niters number experiment iterations number queries iteration manipulation tuning parameter oracle iteration algorithm performs manipulations number causal queries agent result datapoints )··· natural claim manipulator performs well ˆclk many means target causal labels agree true causal labels. thus deﬁne manipulation error iteration merrl step algorithm picks representative member observational class. tells causal partition coarsens observational one. principle sufﬁcient estimate image observational class order know observational class. choice experimental method estimating causal class step left user depends behaving agent behavior question. example represents whether spiking rate recorded neuron ﬁxed threshold estimating could consist recording neuron’s response laboratory setting multiple times calculating probability spiking ﬁnite sample. causal dataset created step consists observational inputs causal classes. causal dataset acquired experiments number observational classes. ﬁnal step algorithm trains neural network predicts causal labels unseen images. choice method training left user. learned causal neural network create synthetic examples images similar possible originals different causal label. meaning similar possible depends image metric choice taskspeciﬁc crucial quality manipulations. experiments metric induced norm. alternatives include lp-induced metrics distances implicit feature spaces induced image kernels distances learned representation spaces algorithm proposes learn manipulator function using simple manipulation procedure approximates requirements deﬁnition local minima. algorithm inspired active learning techniques uncertainty sampling density weighing starts training causal neural network step observational data available achieved using algorithm next randomly chooses images manipulated target post-manipulation causal labels. loop starts step takes images searches image that among images desired causal class closest original image. note causal class boundaries deﬁned current causal neural since general highly nonlinear function hard inverse sets approximate solution. algorithm thus ﬁnds minimum weighted ˆclk| order illustrate concepts presented article perform causal feature learning experiments. ﬁrst experiment called grating uses observational causal data generated model section grating experiment conﬁrms system learn ground truth cause ignore spurious correlates behavior. second experiment mnist uses images hand-written digits exemplify manipulator function slightly realistic data example transform image maximally similar image another class label. chose problems simple computer vision point view. goal develop theory visual causal feature learning show feasible algorithmic solutions; point engineering advanced computer vision systems. experiment generate data using model fig. minor differences induce v-bar h-bar image restrict observational dataset images pixels ﬁlled random noise restrictions increase clarity presentation. algorithms learn visual cause behavior figure shows progress training process. ﬁrst step uses learn causal labels observational data. train simple neural network data. network used iteration create manipulated exemplars. follow algorithm train manipulator iteratively. fig. illustrates difference manipulator iteration iteration error column shows example manipulations particular kind. columns green labels indicate successful manipulations kinds switching causal variable switching red-labeled columns show cases manipulator failed inﬂuence cause column shows original image manipulated version manipulator believes cause change induce change. red/green horizontal bars show percentage success/error manipulation direction. fig. shows training causally-coarsened observational dataset manipulator fails time. fig. twenty manipulator learning iterations manipulations figure manipulator learning grating. top. plots show progress manipulator function learning algorithm iterations experiments grating problem. manipulation error decreases quickly progressing iterations whereas manipulation distance stays close constant. bottom. original manipulated grating images. text details. hundred unsuccessful. furthermore causally irrelevant image pixels also much better preserved iteration fully-trained manipulator correctly learned manipulate presence h-bar cause changes ignores v-bar strongly correlated behavior cause experiment start mnist dataset handwritten digits. terminology well standard vision dataset already causal data labels assigned experimental setting nature. consider following binary human behavior human observer answers afﬁrmatively question does image contain digit observer judges image contain digit simplicity assume image eipurely basis non-manipulated data). rows perform similar experiments digits. plots show successive manipulators progressively remove original digits’ features target class features image. provide link causal reasoning neural network models recently enjoyed tremendous success ﬁelds machine learning computer vision despite encouraging results image classiﬁcation object detection ﬁne-grained classiﬁcation researchers found visual neural networks easily fooled using adversarial examples learning procedure manipulator function could viewed attempt train classiﬁer robust examples. procedure uses causal reasoning improve boundaries standard correlational classiﬁer however ultimate purpose causal manipulator network extract truly causal features data automatically perform causal manipulations based features. second contribution concerns ﬁeld causal discovery. modern causal discovery algorithms presuppose causal variables well-deﬁned meaningful. exactly presupposition entails unclear clear counter-examples cannot distinct causal variables. also well understood problems causal variables aggregates variables provide account causal macro-variables supervene micro-variables. article attempt clarify construct well-deﬁned causal macro-variables function basic relata causal graphical model. step strikes essential causal methodology successful areas clearly delineated candidate causes causes supervene micro-variables climate science neuroscience economics and—in speciﬁc case—vision. kc’s work funded qualcomm innovation fellowship kc’s pp’s work supported muri grant n---. would like thank cosma shalizi pointers many relevant results paper builds figure manipulator learning mnist mturk. top. contrast grating experiment manipulation distance grows manipulation error decreases. successful manipulator needs change signiﬁcant parts image bottom. visualization manipulator training randomly selected mnist digits. text details. ther task learn manipulator function take image modify minimally become before stop resembling originally. conduct manipulator training separately mnist digits using human annotators amazon mechanical turk. exact training procedure described appendix fig. shows training progress. fig. manipulation error decreases training. fig. visualizes manipulator training progress. ﬁrst randomly chosen mnist manipulated resemble pushed successive -vs-all manipulators trained iterations point rk×n× -dimensional simplex multinomial distributions. isometric ×k-dimensional simplex multinomials. term refer subset lower-dimensional simplex isometric remembering latter comes equipped lebesgue measure r×k. ready show subset satisfy causal coarsening constraint measure zero respect lebesgue measure. this ﬁrst note since ﬁxed αhiγh. causal coarsening constraint says every pair images holds subset distributions satisfy constraint consists holds prove causal coarsening theorem prove less general version order split rather complex proof parts. auxiliary theorem proven using simpler techniques however deliberately techniques transfer directly proof cct. auxiliary theorem among generative models form discussed fig. subset distributions causal partition coarsening observational partition lebesgue measure zero. proof. proof inspired proof used meek prove almost distributions compatible given causal graph faithful. proof strategy thus ﬁrst express proposition given distribution observational partition reﬁne causal partition polynomial equation space distributions compatible model. show polynomial equation trivial i.e. least distribution root. simple algebraic lemma prove theorem. extend meek’s proof technique usage fubini’s theorem lebesgue integral. allows split polynomial constraint multiple different constraints along several distribution parameters. allows additional ﬂexibility creating useful assumptions assume binary discrete variables k×···×km simplicity later on). discreteness assumption crucial simplify reasoning. factorize joint parametrized |hm| parameters parameters another parameters independent. call parameters respectively call subset violates causal coarsening constraint ∪αγz joint distributions violate causal coarsening constraint. want prove lebesgue measure. show this indicator function theorem among generative models form discussed fig. distributions induce given observational partition almost induce causal partition coarsening shown subset consists distributions lebesgue measure zero. since ﬁnitely many pairs images subset distributions violate causal coarsening constraint also fig. provide examples three distributions binary variables three-valued ﬁrst model induces causal partition proper coarsening observational partition thus agrees cct. second model induces observational partition proper coarsening causal partition implies measure-zero case that ﬁxing observational partition carefully tweak parameters align causal partition third model induces causal observational partitions incompatible neither coarsening other. also measure-zero case. provide tetrad contains three models http//vision. caltech.edu/˜kchalupk/code.html. used verify observational causal partition computations. proof. ﬁrst part follows construction second part note bijective correspondence pairs values observational probabilities call correspondence further deﬁne function since f−). value function value thus entropy smaller entropy cases retains predictive information causal. consider following example causal graph consisting three variables causal relations three variables binary positive distribution proof. variables appear proof without definition deﬁned proof auxiliary theorem. take parametrization distributions. fixing observational partition means ﬁxing observational constraints number observational classes. since independent parameter unrestricted reduce number independent parameters want express parameter-space reduction terms parameterization apply proof auxiliary theorem. this observational class choose representative image equations altogether equivalent observational constraints. thus express distribution consistent given observational partition terms full range parameters restricted number independent parameters. rest proof follows similarily proof auxiliary theorem shows within restricted parameter space parameters observational partition reﬁnement causal partition measure zero. view image target behavior hidden confounder analogous set-up main article observational partition classes namely case observational partition causal partition deﬁnition spurious correlate constant since distinctions made within causal classes. would omitted standard causal model. nevertheless model still i.e. causal variable still contains predictive information causal. given construction causal trivial partition example must case retains predictive non-causal information. follows deﬁnitions case predictive non-causal components image always completely separated causal features. experiment started training one-vsneural nets. used cross-validation choose among following architectures hidden units h.u. h.u. used maxout activations training used stochastic gradient descent batches dropout hidden units momentum adjustment iteration learning rate decaying exponential coefﬁcient weight decay enforced maximum norm column hidden units training stopped iterations iteration best validation error chosen. used pylearn package train networks. initial training done training points validation points machine. training points chosen random include images speciﬁc digit class images random digits machine. validation sets composed similarly. machine used algofigure graphical causal model three faithful probability tables. ﬁrst table induces causal partition coarsening observational partition speciﬁcally ﬁgure shows second table induces observational partition corasening causal partition. last table induces causal observational partition neither coarsening other.", "year": 2014}