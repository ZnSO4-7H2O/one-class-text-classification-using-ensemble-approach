{"title": "The Parallel Knowledge Gradient Method for Batch Bayesian Optimization", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "In many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g. when evaluating the performances of several different neural network architectures in a parallel computing environment. In this paper, we develop a novel batch Bayesian optimization algorithm --- the parallel knowledge gradient method. By construction, this method provides the one-step Bayes optimal batch of points to sample. We provide an efficient strategy for computing this Bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy.", "text": "many applications black-box optimization evaluate multiple points simultaneously e.g. evaluating performances several different neural networks parallel computing environment. paper develop novel batch bayesian optimization algorithm parallel knowledge gradient method. construction method provides one-step bayes optimal batch points sample. provide efﬁcient strategy computing bayes-optimal batch points demonstrate parallel knowledge gradient method ﬁnds global optima signiﬁcantly faster previous batch bayesian optimization algorithms synthetic test functions tuning hyperparameters practical machine learning algorithms especially function evaluations noisy. function evaluations possible. paper assume membership domain easy evaluate evaluate points assume evaluations either noise-free additive independent normally distributed noise. consider parallel setting perform simultaneous evaluation typically puts gaussian process prior distribution function updating prior distribution observation choosing next point points evaluate maximizing acquisition function quantiﬁes beneﬁt evaluating objective function evaluated. comparison global optimization algorithms often ﬁnds near optimal function values fewer evaluations consequence useful function evaluation time-consuming training testing complex machine learning algorithms tuning algorithms large-scale dataset recently become popular machine learning highly effective tuning hyperparameters machine learning algorithms previous work assumes evaluate objective function sequentially though recent papers considered parallel evaluations practice often evaluate several different choices parallel multiple machines simultaneously train machine learning algorithm different sets hyperparameters. paper assume access evaluations simultaneously iteration. develop parallel acquisition function guide evaluate next based decision-theoretical analysis. contributions. propose novel batch method measures information gain evaluating points acquisition function parallel knowledge gradient method derived using decision-theoretic analysis chooses points evaluate next optimal average-case respect posterior batch points remaining. naively maximizing q-kg would extremely computationally intensive especially large paper develop method based inﬁnitesimal perturbation analysis evaluate q-kg’s gradient efﬁciently allowing efﬁcient optimization. experiments synthetic functions tuning practical machine learning algorithms q-kg consistently ﬁnds better function values parallel algorithms parallel batch parallel exploration q-kg provides especially large value function evaluations noisy. code paper available https//github.com/wujian/qkg. rest paper organized follows. section reviews related work. section gives background gaussian processes deﬁnes notation used later. section proposes acquisition function q-kg batch section provides computationally efﬁcient approach maximizing q-kg. section presents empirical performance q-kg several benchmarks synthetic functions real problems. finally section concludes paper. related work within past several years machine learning community revisited huge success tuning hyperparameters complex machine learning algorithms. algorithms consist components statistical model describing function acquisition function guiding evaluations. practice gaussian process mostly widely used statistical model ﬂexibility tractability. much literature focuses designing good acquisition functions reach optima evaluations possible. maximizing acquisition function usually provides single point evaluate next common acquisition functions sequential bayesian optimization including probability improvement expected improvement upper conﬁdence bound entropy search knowledge gradient recently papers extended parallel setting aiming choose batch points evaluate next iteration rather single point. suggests parallelizing iteratively constructing batch iteration adding point maximal single-evaluation averaged posterior distribution previously selected points. also proposes algorithm called constant liar\" iteratively constructs batch points sample maximizing singleevaluation pretending points previously added batch already returned values. also work extending parallel setting. proposes gp-bucb policy selects points sequentially criterion ﬁlling batch. time point selected algorithm updates kernel function keeping mean function ﬁxed. proposes algorithm combining pure exploration called gp-ucb-pe. algorithm ﬁrst point selected according criterion; remaining points selected encourage diversity batch. algorithms extending require monte carlo sampling making fast scalable. however criteria usually designed minimize cumulative regret rather immediate regret causing methods underperform wish minimize simple regret. parallel methods construct batch points iterative greedy fashion optimizing single-evaluation acquisition function holding points batch ﬁxed. acquisition function propose considers batch points collectively choose batch jointly optimize acquisition function. recent papers value points collectively include optimizes parallel closed-form formula gradient-based methods proposed jointly optimize parallel criterion proposes parallel version algorithm uses monte carlo sampling optimize parallel acquisition function. compare methods number previous papers numerical experiments demonstrate provide improvement especially problems noisy evaluations. method also closely related knowledge gradient method non-batch setting chooses bayes-optimal point evaluate iteration left ﬁnal solution choose restricted points evaluate. beyond previous work aspects. first generalize parallel setting. second sequential setting allows evaluating acquisition function exactly evaluation requires monte carlo parallel setting develop sophisticated computational techniques optimize acquisition function. recently studies nested batch knowledge gradient policy. however optimize ﬁnite discrete feasible gradient exist. result computation much less efﬁcient ours. moreover focus nesting structure materials science present setting. background gaussian processes section state prior brieﬂy discuss well known results gaussian processes introduce notation used later. gaussian process prior function speciﬁed mean function kernel function assume either exact independent normally distributed measurement errors i.e. evaluation point satisﬁes known function describing variance measurement errors. known also estimate section supposing measured points x··· obtained corresponding measurements combine observed function values prior obtain posterior distribution posterior distribution still gaussian process mean function kernel function follows section propose novel parallel bayesian optimization algorithm generalizing concept knowledge gradient parallel setting. knowledge gradient policy discrete chooses next sampling decision maximizing expected incremental value measurement without assuming point returned optimum must previously sampled point. show compute expected incremental value additional iteration parallel setting. suppose observed function values. stop measuring minx∈a would minimum predictor instead took batch samples minx∈a would minimum predictor difference quantities minx∈a µ−minx∈a increment expected solution quality results additional batch samples. increment solution quality random given posterior samples minx∈a random vector dependence outcome samples. compute probability distribution difference q-kg algorithm values sampling decision z··· according expected value call parallel knowledge gradient factor indicate using notation q-kg. formally deﬁne q-kg factor candidate points sample construction parallel knowledge gradient policy bayes-optimal minimizing minimum predictor decision remaining. q-kg algorithm reduce parallel algorithm function evaluations noise-free ﬁnal recommendation restricted previous sampling decisions. conditions above increment expected solution quality become exactly parallel acquisition function. however computing q-kg gradient expensive. address computational issues section full description q-kg algorithm summarized follows. section provide strategy maximize q-kg gradient-based optimizer. section section describe compute q-kg gradient ﬁnite section describes effective discretize readers note here used compute q-kg factor given sampling decision feasible domain optimize over. discretizing ﬁrst cholesky factor covariance matrix diag{σ)··· σ)}. compute q-kg factor using monte carlo sampling ﬁnite sample compute plug repeat many times take average. estimating gradient q-kg ﬁnite section propose unbiased estimator gradient q-kg using ﬁnite. accessing stochastic gradient makes optimization much easier. express q-kg sample many times take average estimate gradient q-kg technique called inﬁnitesimal perturbation analysis gradient estimation since estimate gradient q-kg efﬁciently ﬁnite apply standard gradient-based optimization algorithms multi-start stochastic gradient ascent maximize q-kg. approximating q-kg inﬁnite discretization speciﬁed maximize q-kg ﬁnite usually inﬁnite. case discretize approximate q-kg maximize approximate q-kg. discretization interesting research topic paper discrete chosen statically evolves time speciﬁcally suggest drawing samples global optima posterior distribution gaussian process description technique). sample denoted extended locations previously sampled points candidate points restated iteration updating posterior gaussian process. numerical experiments conduct experiments different settings noise-free setting noisy setting. settings test algorithms well-known synthetic functions chosen practical problems. following previous literature constant mean prior mat´ern kernel. noisy setting assume constant across domain estimate together hyperparameters using maximum likelihood estimation discretize domain following strategy section general q-kg algorithm performs well better state-of-art benchmark algorithms synthetic real problems. performs especially well noisy setting. describing details empirical results highlight implementation details method open-source implementations benchmark methods. implementation inherits open-source implementation parallel metrics optimization engine fully implemented python interface. reuse regression hyperparameter ﬁtting methods implement q-kg method c++. besides comparing parallel also compare method well-known heuristic parallel implemented spearmint parallel algorithm parallel pure exploration implemented gpoptimization noise-free problems section focus attention noise-free setting evaluate objective exactly. show parallel knowledge gradient outperforms competitive state-of-art benchmarks several well-known test functions tuning practical machine learning algorithms. first test algorithm along benchmarks well-known synthetic test functions branin domain rosenbrock domain ackley domain hartmann domain initiate algorithms randomly sampling points latin hypercube design dimension problem. figure reports mean standard deviation base logarithm immediate regret running random initializations batch size results show q-kg signiﬁcantly better rosenbrock ackley hartmann slightly worse best benchmarks branin. especially rosenbrock ackley q-kg makes dramatic progress early iterations. first tune logistic regression mnist dataset. task classify handwritten digits images -class classiﬁcation problem. train logistic regression training instances given hyperparameters test test instances. tune hyperparameters mini batch size training iterations regularization parameter learning rate report mean standard deviation test error independent runs. results algorithms making progress initial stage q-kg maintain progress longer results better algorithm conﬁguration general. second experiment tune cifar dataset. also -class classiﬁcation problem. train training data certain hyperparameters test test instances. network architecture choose tensorflow tutorial. consists convolutional layers fully connected layers softmax layer ﬁnal classiﬁcation. tune totally hyperparameters mini batch size training epoch regularization parameter learning rate kernel size number channels convolutional layers number hidden units fully connected layers dropout rate report mean standard deviation test error independent runs. example q-kg making better progress parallel even initial stage maintain advantage end. architecture carefully tuned human expert achieve test error around automatic algorithm improves around noisy problems section study problems noisy function evaluations. results show performance gains benchmark algorithms q-kg evident noise-free setting even larger noisy setting. noisy synthetic functions test synthetic functions noise-free setting independent gaussian noise standard deviation function evaluation. algorithms given standard deviation must learn data. results figure show q-kg consistently better least competitive competing methods. also observe performance advantage q-kg larger noise-free problems. noisy logistic regression small test sets testing large test imagenet slow especially must test many times different hyperparameters. speed hyperparameter tuning instead test algorithm subset testing data approximate test error full set. study performance algorithm benchmarks scenario focusing tuning logistic regression mnist. train logistic regression full training test algorithm testing randomly selected samples test provides noisy approximation test error full test set. report mean standard deviation test error full using hyperparameters recommended parallel algorithm independent runs. result shows q-kg better versions parallel ﬁnal test error close noise-free test error synthetic test functions q-kg’s performance advantage noisy setting wider noise-free setting. acknowledgments authors partially supported career cmmi- cmmi- iis- afosr fa--- afosr fa--- afosr fa--. conclusions paper introduce novel batch bayesian optimization method q-kg derived decision-theoretical perspective develop computational method implement efﬁciently. show q-kg outperforms competitive state-of-art benchmark algorithms several synthetic functions tuning practical machine learning algorithms. corresponds synchronous q-kg optimization wait points previous batch ﬁnish searching batch points. however applications wish generate batch points evaluate next points still evaluated values. common training machine learning algorithms different machine learning models necessarily ﬁnish time. generalize asynchronous q-kg optimization. given points still evaluation would like recommend batch points evaluate. synchronous q-kg optimization above estimate q-kg combined points respect points need recommend. proceed gradient-based algorithms. next compare q-kg different levels parallelism fully sequential algorithm. test algorithms different batch sizes noisy synthetic functions branin hartmann whose standard deviation noise results parallel knowledge gradient method provide speed-up goes figure performances q-kg different batch sizes. report mean standard deviation scale immediate regret number iterations. iteration initial designs. iteration later evaluate points recommended q-kg algorithm. unbiasedness stochastic gradient estimator without loss generality assume ﬁxed advance would like prove correct. proceeding deﬁne notation fazq equals component-wise except zij. prove cite theorem requires three conditions make valid exists open neighborhood dimension point fazq continuous ﬁxed fazq differentiable except denumerable given derivative fazq uniformly bounded expectation ﬁnite. condition mean function kernel function continuous differentiable given ˜σn) continuous differentiable result multiplication inverse cholesky operators preserve continuous differentiability. ﬁnite minx∈a nite need show deﬁning continuous differentiable would like show denumerable. prove show contains isolated points. theorem real analysis isolated points denumerable prove contains isolated points deﬁnition isolated point isolated point limit point prove contradiction suppose limit point means exists sequence points y··· belong limn→∞ zij. however deﬁnition derivative limn→∞ contradiction. conclude contains isolated points denumerable. deﬁning also continuous differentiable similarly prove contal buffoni robicquet vayatis parallel gaussian process optimization upper conﬁdence bound pure exploration. machine learning knowledge discovery databases pages springer. hernández-lobato hoffman ghahramani predictive entropy search efﬁcient global optimization black-box functions. advances neural information processing systems pages marmin chevalier ginsbourger differentiating multipoint expected improvement optimal batch design. international workshop machine learning optimization data pages springer. scott frazier powell correlated knowledge gradient simulation optimization continuous parameters using gaussian process regression. siam journal optimization snoek swersky zemel adams input warping bayesian optimization non-stationary functions. proceedings international conference machine learning pages srinivas krause seeger kakade gaussian process optimization bandit setting regret experimental design. proceedings international conference machine learning pages wang reyes brown mirkin powell nested-batch-mode learning stochastic optimization application sequential multistage testing materials science. siam journal scientiﬁc computing b–b.", "year": 2016}