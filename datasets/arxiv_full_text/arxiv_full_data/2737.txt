{"title": "On overfitting and asymptotic bias in batch reinforcement learning with  partial observability", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "This paper stands in the context of reinforcement learning with partial observability and limited data. In this setting, we focus on the tradeoff between asymptotic bias (suboptimality with unlimited data) and overfitting (additional suboptimality due to limited data), and theoretically show that while potentially increasing the asymptotic bias, a smaller state representation decreases the risk of overfitting. Our analysis relies on expressing the quality of a state representation by bounding L1 error terms of the associated belief states. Theoretical results are empirically illustrated when the state representation is a truncated history of observations. Finally, we also discuss and empirically illustrate how using function approximators and adapting the discount factor may enhance the tradeoff between asymptotic bias and overfitting.", "text": "paper stands context reinforcement learning partial observability limited data. setting focus tradeoﬀ asymptotic bias overﬁtting theoretically show potentially increasing asymptotic bias smaller state representation decreases risk overﬁtting. analysis relies expressing quality state representation bounding error terms associated belief states. theoretical results empirically illustrated state representation truncated history observations. finally also discuss empirically illustrate using function approximators adapting discount factor enhance tradeoﬀ asymptotic bias overﬁtting. paper dedicated sequential decision-making problems modeled markov decision processes system dynamics partially observable class problems often called partially observable markov decision processes within setting focus decision-making strategies computed using reinforcement learning approaches rely observations gathered interactions although approaches strong convergence properties classic approaches challenged data scarcity. acquisition observations possible data scarcity gradually phased using strategies balancing exploration exploitation tradeoﬀ. scientiﬁc literature related topic vast; particular bayesian techniques oﬀer elegant formalizing tradeoﬀ. however strategies applicable acquisition observations possible any. within context propose revisit learning paradigm faces similarly supervised learning tradeoﬀ simultaneously minimizing sources error asymptotic bias overﬁtting error. asymptotic bias directly relates choice algorithm algorithm deﬁnes policy class well procedure search within class bias deﬁned performance best candidate optimal policies actual optimal policies. bias depend observations. hand overﬁtting error term induced fact limited amount data available algorithm potentially overﬁt suboptimal policies. overﬁtting error vanishes size quality dataset increase. paper focus studying interactions sources error setting system dynamics partially observable. particular setting needs build state representation history data. increasing cardinality state representation algorithm provided informative representation pomdp price simultaneously increasing size candidate policies thus also increasing risk overﬁtting. analyze tradeoﬀ case algorithm provides optimal solution frequentist-based associated state representation analysis relies expressing quality state representation bounding error terms associated belief states thus deﬁning \u0001-suﬃcient statistics hidden state dynamics. experimental results illustrate theoretical ﬁndings distribution pomdps case state representations truncated histories observations. particular illustrate link also discuss illustrate using function approximators adapting discount factor play role tradeoﬀ bias overﬁtting. provides reader overview elements involved tradeoﬀ. remainder paper organized follows. section formalizes pomdps sets observations state representations. section details main contribution paper analysis bias-overﬁtting tradeoﬀ batch pomdps. section empirically illustrates main theoretical results section concludes. environment starts distribution initial states time step environment state time agent receives observation depends state environment probability agent take action then environment transitions state probability agent receives reward equal mappings taking last observation input. however pomdp setting leads candidate policies likely rich enough capture system dynamics thus suboptimal alternative using history previously observed features better estimate hidden state dynamics. denote histories observed time space possible observable histories. straightforward approach take whole history input candidate policies however taking long history several drawbacks. indeed increasing size candidate optimal policies generally implies computation search within increased risk including candidate policies suﬀering overﬁtting paper speciﬁcally interested minimizing latter overﬁtting drawback keeping informative state representation. deﬁne mapping {φ|h ﬁnite cardinality |φ|. note mapping induces upper bound number candidate policies |πφ| |a||φ|. importance feature space section introduces analyzes bias-overﬁtting decomposition performance policies computed frequentist-based augmented built dataset setting agent behaves optimally respect maximum-likelihood model estimated data chosen abstraction allows removing analysis algorithm converges. ﬁrst deﬁne frequentistbased augmented state space action space estimated transition function number times observe transition divided number times observe never encountered dataset /|σ|∀σ working limited dataset pomdps ﬁxed denote dmπsntrnl random dataset generated according probability distribution dmπsntrnl trajectories length trajectory deﬁned observable history obtained starting following stochastic sampling policy ensures non-zero probability taking action given observable history simplicity denote dmπsntrnl simply purpose analysis also introduce asymptotic dataset dmπsntr→∞nl→∞ would theoretically obtained case could generate inﬁnite number observations paper algorithm cannot generate additional data. challenge determine high-performance policy access ﬁxed dataset paper consider stationary deterministic control policies particular choice induces particular deﬁnition policy space introduce expected return obtained inﬁnite time horizon system controlled using policy pomdp distribution initial observations initial states conditional observation probabilities). note that become clear following deﬁnition usually deﬁned pomdp deﬁnition long asymptotic frequentist-based actually gathers relevant information actual pomdp. indeed model-based context pomdp dynamics known knowledge allows calculating belief state possible deﬁne history action expected immediate reward well transition function next observation proof deferred appendix bound original result based belief states \u0001-suﬃcient statistic note bisimulation metrics also used take account errors belief states less impact condition hidden states aﬀected errors close according bisimulation metrics. theorem pomdp described tuple augmented estimated according deﬁnition dataset assumption transitions possible pair overﬁtting using frequentist-based policy instead πd∞φ podmp bounded follows proof deferred appendix theorem shows using large features allows larger policy class hence potentially leading stronger drop performance available dataset limited theoretical analysis context mdps ﬁnite dataset performed overall theorems help choose good state representation pomdps provide bounds terms appear biasoverﬁtting decomposition equation example additional feature mapping overall positive eﬀect provides signiﬁcant increase information belief state expected return obtained inﬁnite time horizon system controlled using policy s.t. augmented decision process reward s.t. dynamics given deﬁnition frequentist-based policy optimal policy augmented deﬁned term bias actually refers asymptotical bias size dataset tends inﬁnity term overﬁtting refers expected suboptimality ﬁnite size dataset. selecting carefully feature space allows building class policies potential accurately capture information data also generalize well hand using many non-informative features increase overﬁtting stated theorem hand mapping discards useful available information suﬀer asymptotic bias stated theorem given increase information must signiﬁcant enough compensate additional risk overﬁtting choosing large cardinality note could combine bounds theoretically deﬁne optimal choice state representation lower bound guarantees regarding bias-overﬁtting tradeoﬀ. importance function approximators described earlier straightforward mapping obtained discarding features observable history. addition also possible learn processing features selecting suitable function approximator structure constrains policies interesting generalization properties. note theorem similar theorem takes account complexity measures function approximator provide tighter bound. left scope paper bound usually little interest practice speciﬁcally concerning deep learning worth noting case neural networks architectures convolutional layers recurrency particularly well-suited deal large input space oﬀer interesting generalization properties adapted high-dimensional sensory inputs hierarchical patterns found recent successes make convolutional layers and/or recurrent layers solve large scale pomdps. importance discount factor used training phase artiﬁcially lowering discount factor shown improve performance policy solving mdps limited data partially observable setting results transferred frequentist-based selection parameters validation cross-validation balance bias-overﬁtting batch setting case selection tradeoﬀ policy parameters eﬀectively balance biasoverﬁtting tradeoﬀ done similarly supervised learning long performance criterion estimated subset trajectories dataset used training possibility model data frequentist aprandomly sample pomdps distribution refer random pomdp. distribution fully determined specifying distribution possible transition functions distribution reward functions distribution possible conditional observation probabilities random transition functions drawn assigning entry zero value probability probability non-zero entry probability drawn uniformly zeros enforce nonzero value random values normalized. random reward functions generated associating possible reward sampled uniformly independently random conditional observation probabilities generated following probability observe state equal values chosen uniformly randomly normalized pomdps ﬁxed stated otherwise truncate trajectories length time steps. generated pomdp generate datasets probability distribution possible sets trajectories trajectory made history time steps starting initial state taking uniformly random decisions. dataset induces policy want evaluate expected return policy discarding variance related stochasticity transitions observations rewards. policies tested rollouts policy. pomdp able estimate average score deﬁned overﬁtting term appears higher chance picking suboptimal policies illustrated figure note variance term already studied context estimating value functions ﬁnite mdps using frequentist approximations parameters mdps section show experimentally additional feature likely reduce asymptotic bias also increase overﬁtting. dataset deﬁne policy according deﬁnition every history interest current observation used building state frequentist-based augmented mdp. figure illustrations return estimates augmented three diﬀerent policies. policies selected speciﬁcally illustration purpose based criterion best performing worst performing median performing selected policies built trajectories data actual pomdp. actual pomdp expected returns also illustrate eﬀect using function approximators bias-overﬁtting tradeoﬀ. process output state representation deep q-learning scheme figure deep q-learning policies suﬀer less overﬁtting compared frequentist-based approach even though using large features still leads overﬁtting small features. also deep q-learning policies avoid introducing important asymptotic bias neural network architecture rich enough. note variance slightly larger figure vanish additional data. additional stochasticity induced building q-value funcfigure evolution estimated values p∼pσp computed sample pomdps drawn bars used represent variance observed dealing diﬀerent datasets drawn distribution; note usual error bar. values p∼pσp displayed figure observe small features appears better choice dataset poor increasing number trajectories optimal choice terms number features also increases. addition also observe expected variance score decreases number samples increases. variance decreases risk overﬁtting also decreases becomes possible target larger policy class figure evolution estimated values p∼pσp computed sample pomdps drawn neural network function approximator. bars used represent variance observed dealing diﬀerent datasets drawn distribution; note usual error bar. finally empirically illustrate figure eﬀect discount factor. training discount factor lower used actual pomdp additional bias term high discount factor used limited amount data overﬁtting increases. experiments inﬂuence discount factor subtle compared impact state representation function approximator. inﬂuence nonetheless clear better discount factor data available better high discount factor data available. paper discusses bias-overﬁtting tradeoﬀ batch algorithms context pomdps. propose analysis showing that similarly supervised learning techniques face biasoverﬁtting dilemma situations policy class large compared batch data. situations show preferable concede asymptotic bias order reduce overﬁtting. asymptotic bias introduced diﬀerent manners downsizing state representation using speciﬁc types function apfigure evolution frequentist-based case estimated values p∼pµp p∼pσp computed sample pomdps drawn bars used represent variance observed dealing diﬀerent datasets drawn distribution; note usual error bar. proximators lowering discount factor. main theoretical results paper relate state representation originality setting proposed paper compared related work mainly formalize problem batch setting instead online setting. compared originality consider partially observable setting. work proposed paper also interest online settings stage obtaining performant policy given data part solution eﬃcient exploration/exploitation tradeoﬀ. instance sheds lights interest progressively increasing discount factor besides optimizing bias-overﬁtting tradeoﬀ suggests dynamically adapt feature space function approximator", "year": 2017}