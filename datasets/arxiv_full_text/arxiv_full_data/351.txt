{"title": "3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks", "tag": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "abstract": "The success of various applications including robotics, digital content creation, and visualization demand a structured and abstract representation of the 3D world from limited sensor data. Inspired by the nature of human perception of 3D shapes as a collection of simple parts, we explore such an abstract shape representation based on primitives. Given a single depth image of an object, we present 3D-PRNN, a generative recurrent neural network that synthesizes multiple plausible shapes composed of a set of primitives. Our generative model encodes symmetry characteristics of common man-made objects, preserves long-range structural coherence, and describes objects of varying complexity with a compact representation. We also propose a method based on Gaussian Fields to generate a large scale dataset of primitive-based shape representations to train our network. We evaluate our approach on a wide range of examples and show that it outperforms nearest-neighbor based shape retrieval methods and is on-par with voxel-based generative models while using a significantly reduced parameter space.", "text": "figure step-by-step primitive-based shape generation dprnn. illustration given single depth image sequentially predicts sets primitives form shape. time randomly sample primitive generate next primitives conditioning current sample. voxel grid. also primitives holistic representing object parts greatly simpliﬁes reasoning stability connectedness important properties. primitive-based object representations long popular psychology interactive graphics less commonly employed modern computer vision challenges learning predicting models consist arbitrary number parameterized components. goal learn primitive representations objects unannotated meshes. follow encoderdecoder strategy inspired recent work using recursive neural network encode implicit shape representation sequentially generate primitives approximate shape shown fig. challenge training primitive generation network acquiring ground truth data primitive-based shape representations. address challenge propose efﬁcient method based gaussian fields energy minimization iteratively parse shapes primitive components. opsuccess various applications including robotics digital content creation visualization demand structured abstract representation world limited sensor data. inspired nature human perception shapes collection simple parts explore abstract shape representation based primitives. given single depth image object present dprnn generative recurrent neural network synthesizes multiple plausible shapes composed primitives. generative model encodes symmetry characteristics common man-made objects preserves long-range structural coherence describes objects varying complexity compact representation. also propose method based gaussian fields generate large scale dataset primitive-based shape representations train network. evaluate approach wide range examples show outperforms nearest-neighbor based shape retrieval methods on-par voxelbased generative models using signiﬁcantly reduced parameter space. many robotics graphics applications require interpretations sensory data. example picking moving chair predicting whether stack blocks fall looking keys messy desk rely least vague idea object position shape contact connectedness. major challenge represent object geometry predicted noisy partial observations; useful reasoning contact support extent recent efforts often focus voxelized volumetric representations instead propose represent objects primitives compared voxels primitives much compact example primitives parameterized scale-rotation-translation figure d-prnn overview. illustrate method task single depth shape completion. network starts encoding input depth image feature vector sent recurrent generator consisting stacks long short-term memory mixture density network time step network predicts primitives conditioned depth feature previously sampled single primitive. ﬁnal reconstruction result ground truth shown right. timize differentiable loss function using robust techniques optimization process create primitive ground truth solving primitives approximates mesh collection. trained generate primitive-based shapes representative object class’ distribution complete object’s shape given partial observation depth image point cloud. model shapes propose d-prnn rnn-based generative model predicts context-sensitive sequences primitives object-centric coordinates shown figure predict shape depth network trained jointly single depth image sequence primitives conﬁgurations form complete shape. testing network gets input depth sequentially predicts primitives reconstruct shape. generative architecture based long short-term memory mixture density network evaluate proposed generative model comparing baselines state-of-the-art methods. show that even though method less degrees freedom representation achieves comparable accuracy voxel based reconstruction methods. also show encoding symmetry rotation axis constraints network signiﬁcantly boosts performance. primitive-based shape modeling biederman early popularized idea representing shapes collection components primitives called geons early computer vision algorithms attempted recover object-centered volumetric primitives single images computer aided design primitive-based shape representations used scene sketches shape completion point clouds case scans shapes often canonical parts like planes boxes efﬁcient solution large data required primitives used reconstructions urban architectural scenes recently compact parametric representations form template objects primitives introduced. representations however require trivial effort accommodate variable number conﬁgurations within object class trained for. mainly single feed-forward design implicitly forces prediction discrete number variables time. object shape reconstruction attempted given image depth image recently proposed representations prediction approaches data context prediction sensory input mainly either focused partobjectbased retrieval large repositories voxelized volumetric representations better model ﬁtting includes part deformation symmetry present preliminary results automatic shape completion depth classifying hidden voxels deep network. reconstruct challenge training d-prnn primitive generation network lack large scale ground truth primitive based shape reconstruction data. propose efﬁcient method generate data. given point cloud representation shape approach ﬁnds plausible primitives sequential manner e.g. given table algorithm might identify primitive surface ﬁrst legs successively. rectangular cuboids primitives provide plausible abstraction man-made objects. method proposes fast parsing solution decompose shapes varying complexity primitives. formulate successive ﬁtting primitives energy minimization scheme. primitive ﬁtting step resembles method iterative closest point additional challenges. ensures accurate registration provided good initialization case prior knowledge number rough shape primitives. moreover need solve challenging partial matching problem since primitive matches part shape know advance. represent shape primitive scale parameters denotes scale unit cube along three orthogonal axes. position orientation primitive represented translation euler angles respectively. thus primitive parameterized furthermore assume ﬁxed sampling unit cube points {pm}m=...m given point cloud representation shape {qn}n=...n goal primitives {xt}t=... best shape. employ idea gaussian force fields truncated signed distance function formulate following continuously differentiable energy function convex large neighborhood parameters rotation matrix truncation parameter denotes volumetric-wise sampling ratio calculated volume primitive number sampled points helps avoid local minimum results small large primitive. formulation represents error figure sample primitive ﬁtting result. show primitive ﬁtting results chairs tables sofas. overlay ﬁtted primitives sampled point clouds shape. figure failure cases. main causes complex shape details represented primitive blocks smoothing property gaussian force ﬁelds good describing small hollow shape small cluster point clouds easily missed randomized search scheme shapes based predicted skeletons. unlike mesh-based voxel-based shape reconstruction method predicts shapes aggregations primitives beneﬁt lower computational storage cost. graves uses long short-term memory recurrent neural networks generate complex sequences text online handwriting. gregor combine lstm variational auto-encoder called deep recurrent attentive writer architecture image generation. draw architecture pair rnns encoder network compresses real images presented training decoder reconstitutes images receiving codes. rezende extend draw learn generative models structures recover structure images probabilistic inference. d-prnn sequentially generates primitives inspired graves’ work sequentially generate parameterized handwriting strokes pixelrnn approach model natural images sequentially generated pixels. produce parameterized primitives customize encode explicit geometric constraints symmetry rotation. example separately predicting whether primitive rotate along axis much improves results simply predicting rotation values since many objects consist several cuboids. energy function given sensitive parameter larger encourage ﬁtting large primitives allowing larger distances matched point pairs. order prefer tighter ﬁtting primitives introduce concept negative shape represented points sampled non-occupied space inside bounding shape. update energy function given energy formulation described previous section perform primitive ﬁtting sequential manner. iteration randomly initialize primitives optimize primitives best ﬁtting primitive primitive collection. remove points selected primitive iterate. stop points primitive. optimize iterative manner. ﬁrst solve solve experiments optimization converges iterations l-bfgs toolbox optimization step. summarize process pseudo-code given alg. simpliﬁcation symmetry. utilize symmetry characteristics man-made shapes speed primitive parsing procedure. axis-aligned objects symmetric objects common global symmetry plane. compare geometry sides plane decide whether object symmetric not. obtain primitive lies side symmetry plane automatically generate symmetric primitive side plane. reﬁnement. step primitives relatively larger gaussian ﬁeld fast convergence easier optimization. reﬁne ﬁtting ﬁner energy space match primitive detailed shape object. random search scheme enables fast parsing method errors accumulate ﬁnal primitives. avoid problems perform post-reﬁnement step. reﬁne parameters single primitive ﬁxing parameters. parameters obtained initial ﬁtting initialization. deﬁne energy terms respect points points primitive yet. note sequential reﬁnement similar back propagation used train neural networks. experiments perform reﬁnement time primitives. generating primitive-based shapes challenging task complex multi-modal distribution shapes unconstrained number primitives required model complex shapes. propose d-prnn generative recurrent neural network accomplish task. d-prnn trained generate novel shapes randomly conditioning partial shape observations single depth map. overview d-prnn network illustrated fig. network gets input single depth image sequentially predicts primitives form shape. primitive network predicts shape position orientation rotation). additionally step binary passed encoder consists stacks convolutional leakyrelu layers shown fig. time step distribution next primitive perform predicted shown help training mitigate vanishing gradient problem. similar fashion also pass depth feature hidden layers. explain latter primitive conﬁguration sampled distribution predicted predict parameters axis time conditioned previous axis. model joint distribution parameter axis mixture gaussians conditioned previous axis mixture components weight mean standard deviation correlation mixture component respectively predicted fully connected layer note binary stopping sign indicating whether current primitive ﬁnal helps predicting variable-length sequence primitives. experiments randomly sample single drawn stance distribution sequence represents paramez shape ters following order translation conﬁguration axis ﬁrst primitive stopping sign. trained maximizing likelihood ground truth primitive parameters time step calculate gradients explicitly backpropagation shown graves found stepwise supervised training works well avoids sequential sampling used geometric constraints. another challenge predicting primitive-based shape model rotation given rotation axis sensitive slight change rotation values euler angles. found jointly predicting rotation axis rotation value rotation prediction performs better overall primitive distribution modeling alleviated shown fig. quantitative experiments sec. rotation axis figure detailed architectures depth encoder primitive recurrent generator unit d-prnn. architecture descriptions section ﬁrst layer kernels size stride leakyrelu layer slope negative part. second layer consists kernels size followed setting leakyrelu pooling layer. third layer kernels size followed leakyrelu pooling. next fully-connected layers neurons output feature vector sent recurrent generator predict sequence primitives. recurrent generator. apply long short-term memory unit inside recurrent generator shown better alleviating vanishing exploding gradient problems training rnns. architectural design shown fig. prediction unit consists layers recurrently connected hidden layers encode depth feature previously predicted primitive computes output vector used parametrize predictive distribution next possible primitive hidden layer activations computed iterating following equations range show quantitative results automatic shape synthesis. quantitatively evaluate d-prnn tests reconstruction synthetic depth maps using real depth maps input. train d-prnn modelnet categories chairs tables nightstands. employ provided another testing samples class evaluation. train single network shapes classes jointly. experiments avoid overﬁtting hold training samples used choose number training epochs. retrain network using entire training set. since single network trained encode three classes predicting shape depth images example implicit class prediction well. implementation implement d-prnn network using torch. train network primitive-based shape conﬁgurations generated described sec. parameters primitive normalized zero mean standard deviation. observe order primitives generated method described sec. involves much randomness makes training hard. instead pre-sort primitives based height shape center decreasing fashion. simple sorting strategy signiﬁcantly boosts training performance. additionally network trained side symmetric shapes shorten sequence length speed training process. train generative mechanism simple random sampling technique. adam update network parameters learning rate train network batch size synthetic data real data respectively. test time network takes single depth sequentially generates primitives stop sign predicted. initialize ﬁrst feature perform nearest neighbor query based encoded feature depth retrieve similar shape training conﬁguration ﬁrst primitive. shape synthesis d-prnn trained generate primitivebased shapes. fig. shows randomly generated shapes synthesized three shape classes. initialize ﬁrst feature random sampled primitive conﬁguration training set. since ﬁrst feature corresponds width translation x-axis rotation x-axis primitive formally initialization process deﬁned drawing sample discrete uniform distribution parameters discrete samples figure training performance comparison validation synthetic depth modelnet. mixture density loss rotation loss averaged sequence length. rotation values normalized values ranges around compared loss. mixture density estimation rotation value estimation performs better enforcing loss predicting rotation axis. figure shape synthesis result. show various random sampled shapes d-prnn. network trained tested without context input. coloring indicates prediction order. predicted three-layered fully connected network size sigmoid function shown rotation value predicted separate three-layered fully connected network size function. figure sample reconstruction synthetic data shapenet. show input depth probable shape reconstruction d-prnn three successive random sampling results compared ground truth primitive representation. figure sample reconstruction real depth nyudv. show input depth probable shape reconstruction d-prnn successive random sampling results compared ground truth primitive representation. constructed training examples. ﬁgure shows d-prnn learn generate representative samples multiple classes sometimes creates hybrids multiple classes. synthetic data. project synthetic depth maps training meshes. training testing perform rejection-sampling unit sphere views bounded within degrees equator. complete shape predicted using single depth input dprnn. model generate sampling complete shapes match input depth well likely conﬁguration determined mean gaussian probable mixture. report intersection union surface-to-surface distance likely predicted shape ground truth mesh. compute ground truth mesh voxelized resolution calculated based whether voxel centers fall inside predicted primitives not. surfaceto-surface distance computed using points sampled primitive ground truth surfaces distance normalized diameter sphere tightly ground truth mesh table shape evaluation synthetic depth modelnet. explore settings d-prnn without rotation axis constrains compare ground truth primitive nearest neighbor baseline. also compare deep network voxel generation method. table surface-to-surface distance evaluation synthetic depth modelnet. explore settings d-prnn without rotation axis constrains compare ground truth primitive nearest neighbor baseline. upper bound performance method corresponding well primitive model true meshes. baseline nearest neighbor retrieval shape training based embedded depth feature network. enforcing rotation axis constraints d-prnn achieves better performance conforms learning curve shown fig. though nearest neighbor d-prnn based trained encoding d-prnn outperforms baseline table nightstand likely able generate greater diversity shapes limited training data. compare voxel-based reconstruction training testing method data using publicly available code. since generate randomized results measure average result runs. method performs similarly measure. performs better surface distance less sensitive alignment sensitive details structures. performance ground truth primitives conﬁrms much reduced performance surface distance using coarser abstraction real data also test model depth dataset much harder synthetic limited training data fact depth images objects lower resolution noisy often occluded conditions. employ ground truth data labelled hoiem models manually selected represent categories common furniture chair table desk bookshelf sofa. ﬁne-tune network trained synthetic data using training table surface-to-surface distance evaluation real depth nyud explore settings d-prnn without ﬁne-tuning compare ground truth primitive nearest neighbor baseline. depth report results test based evaluation metric synthetic test shown table since nightstand less common training often occluded depth regions similar tables network often predicts primitives shapes tables chairs nightstands resulting worse performance class. sample qualitative results shown fig. shape segmentation. since primitive based reconstructions following meaningful part conﬁgurations naturally another application method apply shape segmentation. please refer supplemental material shape segmentation task details results compare state methods well. conclusions future work. present d-prnn generative recurrent neural network uses recurring primitive based abstractions shape synthesis. d-prnn models complex shapes parametric model advantages capable modeling shapes fewer training examples available large intrainter-class variance. evaluations synthetic real depth reconstruction tasks show results comparable higher degree freedom representations achieved method. future explorations include allowing various primitive conﬁgurations beyond cuboids encoding explicit joints spatial relationship primitives.", "year": 2017}