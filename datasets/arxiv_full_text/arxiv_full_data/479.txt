{"title": "Semi-supervised emotion lexicon expansion with label propagation and  specialized word embeddings", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "There exist two main approaches to automatically extract affective orientation: lexicon-based and corpus-based. In this work, we argue that these two methods are compatible and show that combining them can improve the accuracy of emotion classifiers. In particular, we introduce a novel variant of the Label Propagation algorithm that is tailored to distributed word representations, we apply batch gradient descent to accelerate the optimization of label propagation and to make the optimization feasible for large graphs, and we propose a reproducible method for emotion lexicon expansion. We conclude that label propagation can expand an emotion lexicon in a meaningful way and that the expanded emotion lexicon can be leveraged to improve the accuracy of an emotion classifier.", "text": "amount online data increases methods continuously developed make sense available information. social networks allowed anyone internet connection contribute enormous freely available database. data mostly unorganized text probably abundant unstructured resource. online platforms users express opinions share experiences thus generate complex network mutual inﬂuence. reviews goods services political views commentaries well recommendations applicants exemplify content impact decision-making process consumers voters companies organizations therefore surprising sentiment analysis opinion mining active areas academic industrial research nowadays. term sentiment analysis lacks uniﬁed deﬁnition. generally refers automatic detection user’s evaluative emotional attitude toward topic expressed text. commonly meaning sentiment restricted polarity text i.e. whether text positive negative neutral service product target customer reviews blog posts tweets sufﬁciently informative determine valence text. however many cases expanding binary—or ternary—categories chosen emotions yields higher explanatory power. claim rises ﬁeld emotion classiﬁcation emotion analysis non-binary sentiment analysis. case binary sentiment analysis exist main approaches automatically extract affectual orientation lexicon-based corpus-based. lexiconbased approach considers orientation single words phrases document requires dictionaries words labeled emotion—or emotions— evoke. corpus-based approach seen supervised classiﬁcation task. hence requires emotion-annotated corpora. performance statistical emotion classiﬁer typically good domain classiﬁer trained mediocre applied domains. classiﬁers lack generalizing power source information corpus learn from i.e. context-dependent. consequently lexicon-based approach often preferred provides higher context-independence important limitation lexicon-based method still small size available lexical resources positive effect precision cost recall. propose lexicon expansion routine addresses shortcoming. proposed framework emotion-speciﬁc word embeddings learned corpus texts labeled basic emotions derived vector space model used expand existing emotion lexicon semi-supervised label propagation algorithm. thesis multiple contributions. introduces novel variant label propagation algorithm tailored distributed word representations. applies batch gradient descent accelerate optimization label propagation make optimization feasible large graphs. proposes reproducible method emotion lexicon expansion leveraged improve accuracy emotion classiﬁer. emotion-labeled corpus emotion lexicon necessary resources method. former hashtag emotion corpus collection tweets labeled ekman’s basic emotions recurrent network deep model learns emotion-speciﬁc representations words backpropagation emotion-speciﬁcity word vector refers ability encode affectual orientation strength subset dimensions. next specialized embeddings employed build semantic-similarity graph. emotion lexicon expanded using novel variation label propagation algorithm rest thesis structured follows begin review related work areas emotion classiﬁcation lexicon expansion description statistical approaches used learn task-speciﬁc continuous representations then proposed lexicon expansion method optimization specialized word embeddings presented section presents analysis employed resources. experiments performed learn emotion-speciﬁc embeddings expand lexicon reported section along intrinsic extrinsic evaluation emotion classiﬁcation task section concludes proposes research ideas. emotion classiﬁcation among various areas opinion mining sentiment analysis probably thoroughly explored. sentiment analysis refers automatic detection user’s affectual attitude toward topic commonly goal ﬁeld determine polarity phrases sentences documents hence text typically classiﬁed positive negative neutral. however common extend number valence classes least ﬁner-grained analysis additionally includes positive negative polarity scale. sentiment analysis active useful research area prove limitedly informative essentially maps text one-dimensional space. indeed usual range valence another possible goal sentiment analysis still automatically detect user’s affectual orientation difference text assigned classes emotions. increasing number dimensions used represent emotional orientation opinion mining methods gain interpretative explanatory power. refer extended problem emotion classiﬁcation emotion analysis. possibility classiﬁcation texts emotion categories multinomial classiﬁcation classiﬁer outputs probability distribution emotions multi-label classiﬁcation classiﬁer returns probability emotion. important milestone emotion analysis semeval- affective text task. motivation task seems connection lexical semantics verbally express emotions particular argued emotional orientation strength text determined potentially words compose though disputedly uneven amount. therefore expressions appear neutral also convey affective meaning might semantically related emotional concepts. consider following conveys frustration which terms basic emotions could translated labels anger sadness although constituents appear neutral. similarly clearly expresses affectual orientation emotion lexicon contain sadden saddened terrify terrifying. claim words potentially convey affective meaning inspirational work provides rationale lexicon expansion since possibly terms document contribute affective content employing lexica best types serious limitation. lexicon-based approach relies labeled dictionaries calculate emotional orientation text based words phrases constitute disadvantage dictionaries contain direct affective words i.e. words refer directly affective states. contrast indirect affective words weak connection emotional concepts depends context appear give example american professional baseball player criticized unsatisfactory performance publicly stated contains direct affective words beating poverty labeled expressions anger disgust fear sadness emotion lexicon sentence example lexicon-based methods cannot correctly analyze compositionality negations sarcasm issues solved adding rules chunking parsing. dictionaries approach emotion analysis. happens another relevant method address task. automatic extraction affectual orientation viewed supervised classiﬁcation problem requires emotion-annotated data statistical learning algorithm. often referred corpus-based approach area corpus-based methods researchers proposed different systems that access contextual information potential address issues lexicon-based techniques basic compositionality negation cases obvious sarcasm. corpus-based systems participated semeval- affective text task swat. uses weighted pointwise mutual information compute emotion scores. distribution emotions distribution nouns verbs adjectives adverbs obtained statistical analysis texts crawled search engines. then given words text emotion word deﬁned even traditional supervised system swat. based unigram model swat additionally uses thesaurus obtain synonyms emotion label words finally strapparava mihalcea proposed na¨ıve bayes classiﬁer trained mood-annotated blog posts. swat corpus-based systems participating semeval affective text task upar example lexicon-based counterpart. upar described linguistic approach. parses document uses resulting dependency graph reconstruct said main subject. words document emotion scores based sentiwordnet wordnet affect enriched lexical contrast accentuation. higher weight given score main subject weights computed considering linguistic categories negation modal verbs. exist well combinations main approaches. strapparava mihalcea used hand lexical resources annotate synsets representing emotions moods. emotion list words generated corresponding synset. hand british national corpus variation latent semantic analysis obtain vector space model words documents synsets represented vectors. documents synsets mapped vector space computing normalized vectors words comprised them. emotion represented vector space determining affective orientation essentially matter computing similarity measure input word paragraph text prototypical emotion vectors. best performing system semeval- suggests lexical resources corpora complementary sources information. yang combined main learning methods uniﬁed co-training framework applied valence annotation obtaining state-of-the-art results various english chinese datasets. indeed studies showed performance lexiconcorpus-based approaches complementary terms precision recall. lexicon-based method yields higher recall cost precision likely instance label whenever emotion word related found—i.e. even text neutral rather evocative another emotion. contrast supervised learning systems tend perform poorly terms recall—since vocabulary limited types seen training data—but reach higher precision scores labels assigned ﬁne-grained manner —after training instances considerable neutral content none systems participating semeval- reached accuracy similar sentiment analysis methods. possibly reason hard literature alternative approaches used affective text task similar task proposed successive editions semeval. however interannotator agreement studies conducted semeval headlines corpus remind upper bound accuracy emotion classiﬁcation algorithms namely human accuracy. table shows agreement scores terms pearson correlation found original paper. trained annotators agree simple average plausible untrained respondents would show even lower level agreement. section describe results survey tests ability average untrained person classify short paragraphs emotions. comprehensive list challenges analysis affective text presented mohammad includes e.g. subjective cross-cultural differences. light considerations researchers encouraged consider emotion classiﬁcation ﬁeld potential. accuracy scores explained upper bound. available corpora lexica important attempt organize affective phenomena made ekman introduced concept basic emotions i.e. affective states seem share connection physiological processes universal facial expressions. whether possible identify ﬁxed number categories many moods attitudes traits outside scope thesis. however since deal emotion classiﬁcation relevant meaningful classes disposal. researchers disagree validity theory basic emotions others different opinions affective states constitute justiﬁable elementary categories. ekman’s include anger disgust fear sadness surprise. adding trust anticipation yields plutchik’s eight primary emotions collection seven categories used isear project fear anger sadness disgust shame guilt whereas izard counted nine basic emotions. possible choose variety sets affective states scarce amount annotated datasets constrains choice researchers. semeval- affective text corpus collection news titles extracted newspapers news sites consists headlines labeled ekman’s emotions. attempt obtain ﬁne-grained labels annotators independently assigned every headline–emotion pair emotion score ranging indicates intensively news title conveys corresponding emotion. data made available sets development consisting headlines test headlines. partition reﬂects fact semeval affective text task aimed unsupervised approaches. later work supervised classiﬁcation methods news titles used training data remaining testing moreover many experiments vector emotion scores made coarser-grained order better classiﬁcation algorithms. single-label settings dominant emotion i.e. emotion highest score used headline label multi-label classiﬁcation emotions score higher threshold considered present given headline. task description semeval affective text indicates researchers threshold lower value content made mohammad kiritchenko created corpus tweets leveraging hashtags hashtag emotion corpus consists tweets annotated emotions proposed ekman. corpora compiled sentence-level annotation. sentences extracted fairy tales annotated emotions aman szpakowicz annotated sentences drawn blog posts ekman’s emotions neutral category. neviarouskaya chose nine emotion categories proposed izard label sentences extracted stories variety topics. lexical resources abundant either. wordnet affect word-emotion association lexicon consisting terms occasion semeval affective text task words extracted wordnet affect made available optional use. hashtag emotion lexicon contains lemmas automatically obtained hashtag emotion corpus word-emotion pair comes real-valued association score namely strength association score word emotion three lexica ekman’s emotion lexicon combines eight primary emotions proposed plutchik positive negative polarities total possible labels. emotion lexicon includes english words. manually created crowdsourcing uses binary word-emotion association scores. supervised methods statistical learning require input data represented features. many ways encode words sentences paragraphs. common hand-engineer linguistically motivated features ﬁeld prefers automatic feature generation demand domain expertise extensive manual work allows deﬁne language-independent techniques models. simple common represent words one-hot vectors general used encode categorical features. word represented boolean vector whose length equal size vocabulary word position vocabulary dimension word vector result words represented large sparse vectors various statistical learning algorithms linear classiﬁers. although one-hot representation improved using e.g. counts instead boolean values encode virtually information relation words similarities. deep learning started regain popularity alternative encodings resurged. sparse vectors largely replaced dense continuous representations word embeddings proved effective multiple tasks parsing language modeling named entity recognition machine translation word sense disambiguation. particularly interesting research ﬁeld statistical language modeling successfully uses distributed representations words paradigm unlike one-hot vectors word-embeddings represent words isolation. rather exploit context word learn syntactic semantic properties. predicts word based context whereas skip-gram model predicts context given word. cbow takes input context target word i.e. window consisting history words future words context window size. bag-of-words model cbow ignores order context words. novelty lies continuous vectors represent context. skip-gram model operates opposite direction. receiving word input predicts likely previous next words. similarly cbow skip-gram allows variable-size context windows. skip-gram intuitively formulates probability context word given input word terms softmax function however computation cost derivative probability proportional number words vocabulary. alternative calculating probability context word negative sampling simpliﬁcation noise contrastive estimation task distinguish candidate context word instances drawn noise distribution input word. negative sampling preferred signiﬁcantly reduces computation costs. word representations vary depending model originated interestingly vary according task applied example neural language models specialized learn syntactic properties words order occurrence simple collocations useful part-of-speech tagging parsing. typically small-sized context used goal. another option model speciﬁcally trained learn word semantics using larger contexts perform well analogy questions manking womanx resulting word embeddings show intriguing properties. first words similar meaning mapped similar position vector space. example school university represented similar vectors i.e. close vector space school green distant furthermore possible linear vector calculations answer syntactic semantic analogy questions. analogies introduced answered computing xking xman xwoman syntactic level observed models learn offset corresponding concept plurality e.g. xapple xapples xcar xcars. similar results obtained turney using relational similarity model glove vectors fact glove vectors outperform skip-gram cbow many analogy tasks. finally mikolov showed distributed representations used translate words languages learning linear transformation matrix maps embeddings input language translated embeddings vector space. ﬁeld sentiment analysis possible learn sentiment-speciﬁc word embeddings directly large annotated corpus. tang introduced method extending existing general purpose embedding algorithm collobert weston model resulting representations show better predictive performance supervised sentiment analysis task. ticular given ngram telescope corrupted ngram derived substituting center word random word telescope. training objective original ngram obtains higher language model score corrupted version margin architecture neural model consists lookup layer table linear layer parameters non-linear layer second linear layer parameters language model score ngram deﬁned concurrent objectives modeling syntactic context words ignoring sentiment learning polarity words based affective context rather syntactic one. models made complementary computing linear combination hinge losses equations weighted hyperparameter extension model produces embeddings encode polarity information hence representations constitute useful features polarity annotation tasks. must noted however large annotated corpus used train model. indeed shown quality specialized vectors directly proportional size data set. polarity classiﬁcation task model trained million tweets yields score whereas model trained million tweets obtains score language models described produce good quality word embeddings trained corpora large size range. ﬁeld emotion classiﬁcation suffers lack large annotated corpora. knowledge largest available data hashtag emotion corpus contains tweets. size range likely insufﬁcient learn language model scratch therefore need technique able learn task-speciﬁc distributed representations even smaller data sets. labutov lipson proposed method takes input pretrained word embeddings labeled data rearranges embeddings original vector space without directly learning entire task-speciﬁc language model. alternative approach multiple advantages task-specialization process word embeddings computationally efﬁcient size corpus need overly large pretrained generic word embeddings leveraged established model used learn them. task expanding lexicon solved using variety methods. na¨ıve approach would self-training i.e. ﬁrst labeling portion unlabeled data based labeled instances using newly labeled data points incrementally classify whole unlabeled portion tokens. however since dictionaries typically large initial classiﬁer performs poorly cannot exploit great amount information provided relations unlabeled words constitute vast majority available data. consequence accuracy self-training classiﬁer decrements iteration. different method transductive inference transductive learning learner given hypothesis space training dataset dtrain test dataset dtest distribution. learner tries function minimizes expected number erroneously classiﬁed instances test transductive svms successfully used text classiﬁcation crucial pitfall tsvm optimization problem combinatorial. although joachims proposed algorithm ﬁnds approximative solution using local search order keep optimization problem tractable size test sets cannot exceed instances. threshold probably compared e.g. word types compose hashtag emotion corpus surely intolerable benchmark word types frequency encow corpus additional downside tsvm expects sparse input vectors hence disallows dense word embeddings. linguistically inclined approach compute semantic orientation words based tokens emotion words—or twitter emoticons. method used expand lexicon context-dependent polarity annotation problem lexicon expansion also conceived supervised classiﬁcation task words dictionary used training. bravo-marquez recently deployed corpus million tweets multilabel classiﬁer expand emotion lexicon. proposed classiﬁers three types binary relevance classiﬁer chains bayesian classiﬁer chains. word-level features extracted skip-gram model word-centroid model. although latter draws information multiple features— word unigrams brown clusters ngrams distant polarity— word embeddings learned skip-gram model shown signiﬁcantly outperform word-centroid features boosting classiﬁcation performance. disproportion lexicon words unseen types signals however semi-supervised learning technique appears naturally expansion problem. perspective distributional semanticist words ﬂoat high-dimensional space. conﬁguration suitable building graph words regarded nodes linked weighted edges. representing semantic space graph particularly useful graphs tractable mathematical objects come graph-based semi-supervised label propagation algorithm seems represent strong alternative multi-label classiﬁcation. iterative algorithm propagates labels labeled unlabeled data ﬁnding high density areas words labeled unlabeled deﬁned nodes. edge nodes weighted function proximity i.e. words close semantic space linked strong edges. moreover nodes assigned probability distribution labels. iterate labels propagate graph probability mass redistributed following crucial principle labels propagate faster strongly weighted edges. although label propagation requires hyperparameter optimization efﬁciently solve label propagation problem without iteration shown unique solution. technique appears appropriate lexicon expansion leverages dense word vectors semantic similarities. moreover since word embeddings learned corpora carry context-dependent information purely lexicon-based classiﬁer typically lacks. emotion-speciﬁc embeddings task speciﬁcity neural language models skip-gram cbow based fundamental idea unsupervised problem solved embedding supervised task. particular neural language models predict word given context context given word supervised manner. word represented vector free vary improve performance supervised task. result word embeddings specialized representing relation word habitual context. relation considered stand concept itself thus obtained representations optimal word features variety tasks. similar attempt learn task-speciﬁc word embeddings supervised task. since interested embeddings encode affective content supervised problem emotion classiﬁcation. beforehand cbow model used learn general purpose word representations large unsupervised corpus encow. serve informed initialization model weights. motivation recurrent neural networks traditional neural networks able make full sequential information assumption inputs occur independently other. contrast natural language sequential nature paragraphs sentences successions words randomly drawn vocabulary. speciﬁc word dependent word’s context. recurrent neural networks address sequentiality issue include artiﬁcial memory allows make decisions based past time steps. thus rnns ability represent context. practice information persists structure recurrent networks consist multiple copies network applied different time steps. nonetheless types rnns appropriate analysis natural language. case point simple recurrent network introduced elman cannot properly handle typical natural language phenomenon non-local dependencies. syntactic dependency considered non-local involves sentence linguists identiﬁed many analogous phenomena topicalization whmovement. particularly difﬁcult handle unbounded dependencies phrase moves arbitrarily long distance usual position. modiﬁed exemplify long-distance dependency example deliberately exaggerated attempt demonstrate unboundedness non-local dependencies. nevertheless less extreme long-distance dependencies used many languages deserve appropriate treatment e.g. good model english. noted srns theory able exploit distant information. however fail properties gradient-based learning backpropagation. neural network weights receive updates proportional gradient loss function computed chain rule multi-layer architectures. repeatedly multiplying gradients effect either increasing decreasing error exponentially. behaviors known respectively exploding gradient problem vanishing gradient problem. ﬁrst sight simple recurrent networks seem affected exploding vanishing gradients are—at least basic formulation—one-hidden-layer architectures. careful examination reveals backpropagation time actually unfolds recurrent network series feedforward layers exposing remote time steps exploding vanishing gradients. consequence srns sensitive recent context indifferent remote time steps hidden layer current time step weighting hidden layer previous time step input current time step non-linearity applied. case long-distance dependency gradient backpropagate multiple time steps. however many non-linearities logistic function hyperbolic tangent function small gradients tails small gradients multiplied chain rule differentiation. hence resulting product tends exponentially decrease function number previous time steps. reason long-distance dependencies time steps difﬁcult learn. solution fortunately provided long short-term memories variant recurrent neural networks designed overcome exploding vanishing gradient problem enforcing constant error precisely lstms address unstable gradient problem propagating information time steps using vector—the cell state—that linear combination previous cell state candidate hidden state. allows gradient time making possible capture long-distance dependencies. remains nevertheless challenge lstms consider left context input. since deal natural language sentences non-linear structure access right context word also provides relevant information. example consider tweet wants affective orientation love percolate happy birthday—or perhaps even niece— access backward-ﬂowing information necessary. bidirectional information obtained using recurrent networks presented sequence forwards backwards respectively. connected output layer networks provide complete sequential information every time step property motivates bidirectional lstm. model inputs emotion classiﬁer paragraphs. embedding layer maps words vector representations. embeddings bidirectional lstm followed either softmax layer outputs probability distributions emotion classes sigmoid layer produces probability value class. batch normalization precedes bidirectional lstm layer output layer. forward backward lstms receive input sequence output representation sequence implemented follows. compute state memory cell time need value input gate batch normalization motivated ability improve training speed allowing higher learning rates reduce importance careful initialization possibly regularizer technique introduced reduce covariate shift known neural network problem network’s parameters updated training distribution activation also changes. since networks converge faster inputs zero means unit variances batch normalization attempts distribution inputs network layer producing signiﬁcant speedup training. batch normalization shown networks regularizer well method increase training speed. model explore regularization techniques order avoid overﬁtting regularization dropout. regularization consists adding penalty term ||θ|| objective function trainable model parameters. hand randomly removing units network training referred dropout detail dropout consists temporarily removing learning units connections network. unit probability dropped hyperparameter. since ﬁxed independent probability unit needs learn work randomly chosen subset network. learning unit cannot rely units certain present thinned network. consequence units forced learn relevant features. characteristic motivates dropout. expect dropout guarantee better performance speciﬁcally designed prevent overﬁtting neural networks. lexicon expansion lexicon expansion task deﬁned follows. given emotion classes word types extracted large corpus partitioned lexicon words unlabeled words. ease notation refer cardinalities typically labeling function maps unlabeled word probability distribution classes choose label propagation algorithm propose novel variant thereof order solve lexicon expansion problem. graph-based semi-supervised technique propagates labels labeled unlabeled nodes weighted edges. conceived iterative transductive algorithm shown unique solution. therefore learn directly without iteration. problem setup formally labeled data label distributions thereof. deﬁned subset number dimensions used encode words continuous dense vectors. goal estimate label distribution unlabeled data build fully connected graph using labeled unlabeled words nodes. edges nodes deﬁned closer data points d-dimensional space larger weight wij. logistic function bias motivated need adapt weight computation properties cosine similarity obtain uniform weight formula regardless number parameters rows initialized according lexicon. lexicon provides label word probability value assigned corresponding label. since emotion lexicon maps words multiple labels uniformly distribute probability mass among positive classes. initialization ghahramani consider relevant assign constant probability every label. count properties exponential function large sigmas increase edge weights whereas small sigmas shrink them. precise words label node mostly inﬂuenced nearest labeled node; label probability distribution node reﬂects class frequency data affected virtually labeled nodes graph. ghahramani presented techniques parameter simplest minimum spanning tree nodes. kruskal’s algorithm used build tree whose edges property connecting separate graph components. length ﬁrst edge connecting components characterized different labeled points used approximation minimum distance class regions. finally edge connecting separate graph regions weight approaching technique allows extension parameter parameters control edge weights along dimensions used encode node. formulation label propagation translates vector weight therefore interpreted cosine similarity every elementwise multiplication hadamard product) scaled dimension-speciﬁc formulation gives algorithm ability discerning relevant dimensions power reduce weight irrelevant ones. gradient descent used parameters minimize finally transition probability matrix smoothed interpolation uniform matrix interpolation parameter. smoothed transition matrix deﬁned follows beneﬁt smoothing transition probability matrix best explained example. parameters every cosine similarity negative tail logistic function values approach exponential rate. consider word nearest neighbors cosθ cosθ then .e−. virtually probability mass labels propagate probability matrix needs smoothed avoid problem. label propagation batches label propagation computationally efﬁcient algorithm. since iteration avoided directly computing unique algebraic solution computational resources employed calculation probabilistic transition matrix optimization parameters moreover size represent memory issue. consider hashtag corpus comprised word types. basic version label propagation i.e. weight shared dimensions need store cosine similarity word pair case transition matrix size approximately half-precision ﬂoating point numbers. hand employ hyperparameters elementwise products must stored word pair every product scaled dimension-speciﬁc epoch optimization. resulting matrix requires approximately overcome memory problem introduce label propagation batches. instead keeping entire memory optimization subset vocabulary size randomly selected corresponding submatrix rw×w×d computed. enough random submatrices used optimization obtained parameters approximate resulting optimizing furthermore random submatrices motivated need parameters learn adapt random subset vocabulary. randomly selecting word types produce skewed distribution labeled unlabeled instances possible large amount word types labeled words unlabeled. possibilities contradict assumption label propagation therefore distribution labeled unlabeled instances equal proportion original transition probability matrix. hashtag emotion corpus consists texts annotated ekman’s basic emotions. text assigned single emotion label. corpus includes word types. emotion lexicon contains words. however lexicon words least ekman’s emotion labels—the others either annotated positive negative anticipation trust neutral i.e. label lexicon word tagged average ekman’s emotions. table reports labels-per-lemma statistics. furthermore class distributions uniform lexicon positive emotions under-represented respect negative emotions corpus texts annotated form disproportionately large percentage anger disgust minority classes text hashtag corpus contains average emotion words also occur lexicon approximately third tweets contain any. table presents distribution emotion words among texts. emotion word average frequency lemmas occur least hashtag corpus. finally table illustrates frequent emotion words along emotion labels according emotion lexicon. statistics provided emotion-speciﬁc embeddings ﬁrst step proposed lexicon expansion method consists learning emotionspeciﬁc word embeddings. distributed word representations able encode affectual orientation strength. learn emotion-speciﬁc vector space employ recurrent neural network classiﬁer. classiﬁer labels tweets hashtag corpus ekman’s basic emotions uses word vectors trainable features. model learns classify expect word embeddings encode affectual orientation. proposed deep model based bidirectional lstm followed softmax sigmoid output layer. emotion classiﬁer trained using keras hyperparameters summarized appendix experiment different combinations three regularization techniques regularization batch normalization dropout. choice multinomial multi-label classiﬁcation determines type output layer. softmax output layer used obtain probability distribution emotion classes contrast sigmoid layer used produce probability value emotion class initialization word embeddings rely vector space learned encow corpus using cbow model. encow dataset contains approximately million sentences billion tokens. chosen vector dimensionality suggested mikolov experiment different thresholds word frequency excluding either words whose frequency corpus lower occurring less times. context size training context shown give good performance phrase analogy tasks expect window size desirable trade-off computational complexity ability capture semantic information. emotion lexicon expansion expand emotion lexicon employ novel variant label propagation algorithm. although word vectors disposal label propagation applied approximately vectors correspond word types hashtag corpus. decision motivated need limit execution time propagation algorithm well consideration mentioned subset word embeddings optimized emotion-related tasks. word vectors -dimensional. introduced section label propagation problem solved either multiple trainable hyperparameters. experiment scalar hyperparameters label propagation hyperparameters optimized using tensorﬂow values reported appendix furthermore batch-based variant label propagation introduced overcome issues excessive time memory needs hyperparameter optimization. essentially approximate results standard label propagation optimization—where word similarity graph comprised available word types—with multiple batch optimizations—where subset word types used construct similarity graph. expect parameters learn robustly adapt random subset vocabulary consequence discard irrelevant features. test hypothesis combination corpuslexicon-based approaches improves classiﬁcation expanded emotion lexicon inform classiﬁer augment word embeddings. emotion classiﬁer proposed learning task-speciﬁc word embeddings. embedding layer maps words vector representations. embeddings bidirectional lstm followed softmax sigmoid output layer multinomial multi-label classiﬁcation respectively. experiment regularization batch normalization dropout. classiﬁer trained using keras hyperparameters reported appendix initialization word embeddings leverage vector space previously learned recurrent neural network model -dimensional vector space includes approximately word embeddings. corpus-based portion information provide classiﬁer. feed classiﬁer lexicon-based information append label probability distribution vector word occurring expanded emotion lexicon corresponding pretrained word vector. lexicon expanded word types hashtag corpus embedding receives emotion-speciﬁc initialization. notice label distributions original lexicon words left unvaried properties label propagation. evaluating quality task-speciﬁc embeddings obtained optimization emotion classiﬁer straightforward task. could compute sample similarities words embeddings capture intuition words close another specialized vector space. nevertheless exception indisputable cases unfair expect embeddings adhere judgments affective orientation words judgments inherently subjective. consider example might feel natural postulate similarity happy high similarity scared terriﬁed. however would implicitly reduce dimensionality affective space sadness fear respectively—in terms ekman’s basic emotions. consider informal expression repeatedly occurs hashtag corpus. unclear position occupy multidimensional affective space. embed optimization specialized embeddings classiﬁcation task decide performance classiﬁer extrinsic evaluation metric. second criterion performance lexicon expansion evaluate using -fold crossvalidation. results evaluations presented next subsections. emotion lexicon expansion perform intrinsic evaluation lexicon expansion method choose -fold cross-validation. intersection word types hashtag corpus emotion lexicon partitioned equal sized subsamples. cross-validation repeated times subsamples used validation data. quality expanded lexicon assessed computing average kullbackleibler divergence emotion label probability distributions obtained normalizing emotion lexicon distributions resulting label propagation. three baseline lexicon expansion methods introduced assigning uniform class distribution words assigning words distribution probability mass given majority class based hashtag corpus assigning words prior class distribution hashtag corpus. interestingly uniform class distribution yields even lower divergence prior class distribution hashtag corpus. although divergence uniform distribution clearly cannot used practice deﬁnition uninformative. label propagation scalar parameter method best minimizes average kullback–leibler divergence indicating quality expanded lexicon satisfactory. remarkably batch label propagation performs similarly standard propagation although uses batches size result shows batch approximation work graph propagation least number trainable parameters limited. contrast using batch approximation optimize parameter vector appears require either large batch size large number training batches demand long runtime. optimization performed batches size yields average kullback–leibler divergence increasing batch size number batches average kullback–leibler divergence drops therefore conclude parameter vector ineffective optimized using batch gradient descent. remains unclear generally inadequate task optimization requires entire dataset. therefore report classiﬁcation results based label propagation scalar parameter emotion classiﬁcation evaluate proposed emotion classiﬁer introduce multiple baselines. lower bound represented random classiﬁer. implemented trivial count-based classiﬁer assigns emotion labels based emotion lexicon. particular competitive baseline represented version lstm classiﬁer introduced beginning section exploits emotion-speciﬁc word embeddings. ﬁnal alternative pretrained emotion-speciﬁc embeddings concatenated probability distributions indicated unexpanded emotion lexicon. words occur lexicon vectors concatenated vector probabilities randomly sampled uniform distribution. uniform initialization outperforms overall probability distribution emotions lexicon. precision accuracy score presented classiﬁers reported tables metrics computed using micro-averaging allow comparison previous work reduce effect label imbalance. using macro-averaging would assign weight majority classes classiﬁers tend perform better larger amount training instances. table results classiﬁcation hashtag emotion corpus. although bidirectional lstm classiﬁer represents strong baseline using expanded lexicon boosts classiﬁcation accuracy. hashtag emotion corpus bidirectional lstm classiﬁer introduced baseline outperforms one-vs-all binary features setting relatively high lower bound task. including label distributions emotion lexicon features slightly increases classiﬁer accuracy already indicating corpus-based lexicon-based information complementary. limited increment accuracy explained fact text hashtag corpus includes average emotion words approximately third tweets contain lemmas. lstm classiﬁer shows remarkable increase accuracy expanded lexicon provided. although assume quality expanded lexicon lower quality hand-annotated emotion lexicon wider coverage former seems successfully help classiﬁer. regardless whether lstm classiﬁer uses lexicon lexicon expanded multinomial variant consistently obtains higher accuracy. classiﬁcation report best classiﬁer presented table fact label propagation performed encow embeddings backpropagation supervised learning task possibly suggests expanded lexicon tailored training dataset however performance lstm classiﬁer boosted expanded lexicon even dataset different domain result seems imply hashtag corpus humans classiﬁers section reported inter-annotator agreement studies conducted strapparava mihalcea semeval headlines corpus. shown trained annotators agree pearson correlation using simple average classes using frequency-based average. instead reproducing another inter-annotator agreement study test accuracy untrained person respect emotion-annotated dataset hashtag corpus. survey includes participants. undergraduate graduate students asked read tweets classify ekman’s emotion classes. participant given different tweets total classiﬁed instances. students native english speakers. results survey shown table attempt establish upper bound evidence untrained annotator—in words average person—is considerably less accurate lstm classiﬁers. outcome seems conﬁrm hypothesis accuracy state-of-the-start emotion classiﬁers although appears compared e.g. valence classiﬁers close unknown upper bound. assigning emotion short paragraph hard task human statistical classiﬁer requires contextual information available paragraph itself. possible explanation surprisingly accuracy average person found quality hashtag corpus. mohammad kiritchenko clarify essentially three types tweets affectual orientation straightforward even without ﬁnal hashtag hashtag makes affectual orientation explicit text hashtag seem conﬂict. second third case humans tend answer according uniform prior model’s prior belief corresponds class distribution dataset. accuracy model seems beneﬁt informed prior distribution. ﬁnal consideration that absence clear emotional content humans model expected comply gold standard label selected annotators whose agreement score remarkably low. plausible that complex cases exist simply universally correct emotion label. particular shown label propagation expand emotion lexicon meaningful graph propagation rely task-speciﬁc word embeddings. introduced variations label propagation algorithm novel weight computation allows cosine similarity distance metric batchbased training algorithm reduces time memory needed propagate labels throughout large graphs. moreover presented method learn emotion-speciﬁc word embeddings corpus emotion-annotated short paragraphs specialized word vectors result optimization emotion classiﬁer. proposed bidirectional lstm classiﬁer proved accurate even without additional information lexicon. finally shown feeding class probability distributions learned label propagation classiﬁer improves performance. future want test good optimization multiple label propagation parameters performed using even larger number batches batch size. also plan employ glove embeddings initialization trainable specialized vectors shown capture semantic similarity better wordvec embeddings furthermore want introduce lexical-contrast information task-specialization routine using wordnets to—instead optimizing word embeddings singularly—learn rotation transform entire vector space task-speciﬁc one. emotion-speciﬁc embeddings proven reliable source construction similarity graphs interesting options experiment co-occurrence counts wordnets order discover alternative meaningful word representations. references mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg man´e rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda vi´egas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous systems. software available tensorﬂow.org. http//tensorﬂow.org/. cecilia ovesdotter roth richard sproat. emotions text machine learning text-based emotion prediction. proceedings conference human language technology empirical methods natural language processing. association computational linguistics pages felipe bravo-marquez eibe frank saif mohammad bernhard pfahringer. determining word–emotion associations tweets multi-label classiﬁcation. wi’. ieee computer society pages ronan collobert jason weston l´eon bottou michael karlen koray kavukcuoglu pavel kuksa. natural language processing scratch. journal machine learning research michael gutmann aapo hyv¨arinen. noise-contrastive estimation unnormalized statistical models applications natural image statistics. journal machine learning research sepp hochreiter. untersuchungen dynamischen neuronalen netzen. ph.d. thesis diploma thesis institut f¨ur informatik lehrstuhl prof. brauer technische universit¨at m¨unchen. tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases compositionality. advances neural information processing systems. pages pang lillian shivakumar vaithyanathan. thumbs sentiment classiﬁcation using machine learning techniques. proceedings acl- conference empirical methods natural language processing-volume association computational linguistics pages roland sch¨afer linguistic characterization dfg. processing querying large corpora architecture. proceedings workshop challenges management large corpora. pages klaus scherer harald wallbott. evidence universality cultural variation differential emotion response patterning. journal personality social psychology nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research carlo strapparava rada mihalcea. semeval- task affective text. proceedings international workshop semantic evaluations. association computational linguistics pages carlo strapparava alessandro valitutti oliviero stock. affective weight lexicon. proceedings ﬁfth international conference language resources evaluation. pages peter turney. thumbs thumbs down? semantic orientation applied unsupervised classiﬁcation reviews. proceedings annual meeting association computational linguistics. association computational linguistics pages yang wenting ziyu wenpeng kam-pui chow. lcct semisupervised model sentiment classiﬁcation. human language technologies annual conference north american chapter acl. association computational linguistics solver adagrad learning rate decay learning rate initial learning rate epochs model trained epochs. lstm layer forward backward layers trained output dimensions. increasing number output dimensions provide improvement. solver adagrad learning rate decay learning rate initial learning rate epochs model trained epochs. lstm layer forward backward layers trained output dimensions. increasing number output dimensions provide improvement.", "year": 2017}