{"title": "Machine Learning with World Knowledge: The Position and Survey", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Machine learning has become pervasive in multiple domains, impacting a wide variety of applications, such as knowledge discovery and data mining, natural language processing, information retrieval, computer vision, social and health informatics, ubiquitous computing, etc. Two essential problems of machine learning are how to generate features and how to acquire labels for machines to learn. Particularly, labeling large amount of data for each domain-specific problem can be very time consuming and costly. It has become a key obstacle in making learning protocols realistic in applications. In this paper, we will discuss how to use the existing general-purpose world knowledge to enhance machine learning processes, by enriching the features or reducing the labeling work. We start from the comparison of world knowledge with domain-specific knowledge, and then introduce three key problems in using world knowledge in learning processes, i.e., explicit and implicit feature representation, inference for knowledge linking and disambiguation, and learning with direct or indirect supervision. Finally we discuss the future directions of this research topic.", "text": "abstract—machine learning become pervasive multiple domains impacting wide variety applications knowledge discovery data mining natural language processing information retrieval computer vision social health informatics ubiquitous computing etc. essential problems machine learning generate features acquire labels machines learn. particularly labeling large amount data domain-speciﬁc problem time consuming costly. become obstacle making learning protocols realistic applications. paper discuss existing general-purpose world knowledge enhance machine learning processes enriching features reducing labeling work. start comparison world knowledge domain-speciﬁc knowledge introduce three problems using world knowledge learning processes i.e. explicit implicit feature representation inference knowledge linking disambiguation learning direct indirect supervision. finally discuss future directions research topic. yangqiu song department computer science engineering hong kong university science technology clear water hong kong. e-mail yqsongcse.ust.hk roth department computer science university illinois urbana-champaign urbana usa. e-mail danrillinois.edu introduction machine learning become pervasive multiple domains impacting wide variety applications knowledge discovery data mining natural language processing information retrieval computer vision social health informatics ubiquitous computing etc. major problems machine learning practice generate extract features data acquire labels machines learn. many studies feature engineering labeling work reduction past decades. feature extraction representation. feature engineering handcrafting features domain dependent problems recognized problem applications given features without labels perform feature selection feature extraction better representation handcrafted features learning algorithms. recently deep learning proposed deal data end-to-end learning enables representation learning within deep architecture neural networks however still problems related high-level intelligence current machine learning systems cannot handle. example human knowledge world highly structured. human imagines something higher-order relationships among knowledge dots clue. however current machine learning systems able capture inference process remote relationship among dots. even rapid development deep learning much better representation ability data discovering relationships still problem. example although current neural network language model term memory words capture long-short language process capture global dependencies across documents e.g. cross-document co-reference labeling work reduction. labeling large amount data domain-speciﬁc problem time consuming costly. become obstacle making learning protocols realistic applications. machine learning community also elaborated reduce labeling work done human supervised machine learning algorithms improve unsupervised learning minimum supervision. example semi-supervised learning proposed partially labeled data unlabeled data perform learning hope perform good fully supervised learning. transfer learning uses labeled data relevant domains help learning task target domain learns multiple domains simultaneously. semi-supervised learning transfer learning needs domain knowledge multiple ways achieve learning settings. however general solution principle applying learning settings tasks. words target domain speciﬁc domain knowledge still needed engineered learning process. crowdsourcing considered acquire cheap labels general-level human intelligence. however current crowdsourcing mechanisms still applied relatively simple well-deﬁned tasks still challenge applying machine learning labels diverse speciﬁc data example text semantics topics. text semantic similarity/relatedness fundamental problem natural language processing. regarding different levels text span e.g. word phrase sentence document different ways compute similarity/relatedness example consider short texts following texts. however given text fragments overlapped words few. nonetheless similarity/relatedness fragments high since talking topic. therefore consider context word enrich similarity fragments context could obtained texts world another relate words entities texts external knowledge example know obama related bush since president united states. thus path obama bush external knowledge base directly relate text fragments without seeing words related them. approaches dependent target pieces short texts leverage knowledge general purpose texts knowledge bases. example events language vision. event extraction another component language understanding. given complex deﬁnition event trigger agents instruments targets location time joint inference must applied identify corresponding events. traditional event extraction approaches train machine learning models based annotation speciﬁc domains e.g. event types event types consequently supervised learning systems easily overﬁt domains. however many types events. generalizing trained models domains annotation used. especially relationships among agents instruments targets difﬁcult discover using small number annotated data. thus global approach expected avoid training models overﬁtting small domains. example determination event nugget decided simply computing structured similarity seed examples events similarity coming general purpose knowledge base structure obtained general purpose semantic role labeling furthermore extract knowledge entities relations existing knowledge bases information help perform joint inference entities involved event extraction problem across documents. scene recognition problems also complicated problems image understanding. parsing image pixels features functionality attribute intentionality causality factors considered. constrains make event/scene recognition problem joint inference problem. performing inference commonsense knowledge problem perform joint inference. example likely water. water scene interpreted speciﬁc event e.g. hunting cooking. example co-reference. co-reference resolution problem ﬁnding different entities mentions texts referring person thing. also problems natural language understanding. typical co-reference resolution systems rule-based method learning-based method learning based methods common extract features entities mentions deﬁne deterministic function compare pairs entities mentions identify whether co-referent. even declarative constraints considered paper. note that focus machine learning world knowledge acquisition organization. instead assume world knowledge already existing machines use. mentioned introduction feature engineering needs domain knowledge. example identify disease certain related symptoms observed. moreover supervised learning semi-supervised learning require labels side information e.g. must-link cannot-link constraints domain perform machine learning. domain knowledge reﬂected labels constraints. section focus three aspects machine learning domain knowledge. first introduce intuitive semi-supervised learning based partially labeled data formulated generative process setting. simplest case applying domain knowledge machine learning algorithms strongly related posterior regularization section examples learning settings semi-supervised learning refer corresponding references reading then survey declarative knowledge constrained learning since declarative knowledge related form world knowledge. finally focus domain knowledge transferred domains. hand learning setting good comparison semi-supervised learning insight domain knowledge. hand compare setting domain adaptation world knowledge. generative semi-supervised learning typical generative semi-supervised learning setting learn parameters given unlabeled data small portion labeled data {xlyl}. maximum likelihood estimation cannot solved directly since operation inside logarithm. posterior analytically tractable expectation-maximization algorithm employed. re-write ﬁrst term likelihood based background knowledge co-reference natural language hard problems still difﬁcult systems solve. example examples hard co-reference problem example initially shown illustrates noun phrase refers named entity external knowledge entity celebrity improve determination co-reference. otherwise based lexical syntactical features developed rules perfect generalize cases example shows even harder case. ﬁrst sentence named entities know celebrity perform inference. example shows another example winograd schema challenge indicating commonsense knowledge bird cannot bent whereas branch tree can. shown certain knowledge entities categories attributes help identify co-reference. examples show traditionally perform machine learning mostly focused train model avoids overﬁtting best generalization ability. however even best model parameter tuning skills machine learning algorithms still lack knowledge higher-order relationships entities seen still easily overﬁtting speciﬁc domain trained based therefore general approaches considered. paper present idea machine learning world knowledge. instead considering data speciﬁc domain also consider general purpose knowledge world. general knowledge includes common commonsense knowledge partially domain dependent knowledge. position idea using world knowledge intersection many ﬁelds including machine learning data mining natural language processing knowledge representation etc. start comparing traditionally used domain/background knowledge machine learning algorithms world knowledge. discuss world knowledge useful important problems using world knowledge machine learning algorithms. speciﬁcally need adapt world knowledge domain independent domain problems. introduce important factors machine learning algorithms features labels affected world knowledge. multiple ways represent world knowledge features machine learning algorithms. survey existing approaches summarize three categories homogeneous heterogeneous explicit features implicit features. world knowledge supervision introduce linking inference techniques relate domain problem general-purpose knowledge base. introduce learning paradigms enabled world knowledge. finally discuss future directions ideas machine learning world knowledge conclude introduced. first used decouple learning inference parts algorithm. learning part corresponds parameter estimation learning model inference part corresponds label assignment structured output learning algorithm. introduce detail section decoupling introduce second advantage. focus inference part much simpler representation algorithm handle constraints. example general represent satisﬁability boolean formulas linear algebra form solve problem using well developed optimization tools solve example uses integer linear programming a-star algorithms solve problems shown general form constraints useful represent many constrained learning problems transfer learning transfer learning learning paradigm uses data relevant tasks help target machine learning tasks. formally source domain data {xsys} {xsi ysi}ns feature vector sample source domain label vector data sample) number available data source domain. also target domain data {xti yti}nt feature vector sample target domain label vector number available data source domain. cases sometimes labeled data unlabeled data target domain. goal transfer learning source domain data help improve learning/prediction performance target domain data problem transfer learning domains different terms example train newsgroup classiﬁer based christian hockey transfer knowledge classiﬁcation atheism autos motorbike object detector detect bicycle images also even transfer knowledge text images domain transfer learning corresponds speciﬁc tasks relevant target task. domain knowledge usually means implicit knowledge incorporated learned models distributions source data latent factors related factors target domain thus applying knowledge source domain target domain ﬁrst need know relevant tasks provide knowledge. second need develop speciﬁc algorithm incorporate existing knowledge source domain. regarded major characteristics domain knowledge transfer learning. world knowledge domain adaptation analyze learning paradigms something common domain knowledge help learning algorithms better solution domain speciﬁc problem. example semi-supervised learning needs seed labels must-link cannot-link constraints domain i.i.d. unlabeled data prediction data. declarative constraint driven learning needs background knowledge example semi-supervised learning applies domain knowledge partial labels generative learning process. parameter estimation affected labeled according however strong assumption labeled data {xlyl} i.i.d. unlabeled data declarative constraints declarative constraints introduced context using background knowledge improve machine learning algorithms’ performance. example natural language processing example part-of-speech recognition know verb noun sentence constrain unlabeled sentences satisfy linguistic knowledge another example information extraction applied citation domain constrain word corresponds page label even training data showing extracted page information still obtain constraints. general incorporate constraints declarative formulated certain logic forms machines read use. representative studies include constrained conditional models generalized expectation criteria measurements bayesian framework posterior regularization example posterior regularization constraints used limit solution region parameters fall speciﬁcally originally e-step solve minimize kl-divergence kl||p posterior regularization solve following constrained optimization problem e-step variables make constraints satisfy certain formulations slack variables allow constraints violated weight langrange penalize overall errors constraints made. posterior regularization seen variational approximation generalized expectation criteria measurements bayesian framework must-link cannot-link based clustering algorithms also categorized learning paradigm even though relate general learning framework. inspect algorithm used simply e-step constrained m-step remains traditional algorithms used clustering. compared algorithms semi-supervised learning introduced section follows modify m-steps algorithms keep e-step traditional algorithm used generative models task incorporates knowledge constraints. transfer learning requires source task relevant target task knowledge transferred adapted. means dealing task need human evaluate task incorporate correct knowledge learning process. compared learning paradigms paradigm machine learning world knowledge require human justify domain knowledge. whereas uses general-purpose knowledge obtained large scale general knowledge base general data help learning algorithms improve learning performance. graphs) developed e.g. wordnet wikipedia freebase knowitall wikitaxonomy probase dbpedia yago nell illinois-proﬁler knowledge vault call knowledge bases world knowledge universal knowledge either collaboratively annotated human labelers automatically extracted data. example collaboratively constructed knowledge bases include wordnet wikipedia freebase. knowledge bases extracted based information extraction includes probase dbpedia yago nell knowledge vault. comprehensive comparison scales methodologies knowledge bases found world knowledge annotated extracted collected speciﬁc domain. ﬁrst paper explicitly mentioning machine learning world knowledge general slightly different traditional deﬁnition summarize world knowledge commonsense knowledge common knowledge domain knowledge following since better distinguish different kinds knowledge used. commonsense knowledge. commonsense knowledge important sub-topic artiﬁcial intelligence refer commonsense knowledge knowledge ordinary person expected know normally leave unstated write talk. example cats hunt mice; birds etc. thus commonsense difﬁcult part knowledge collect since resources mentioning knowledge purpose. common knowledge. common knowledge refers knowledge humans generally know world. example united states country; current president united states; etc. different commonsense knowledge resources mentioning knowledge web. note people different educational cultural background different common knowledge. nonetheless collectively speaking common knowledge extracted mostly domain knowledge. domain knowledge knowledge speciﬁc domain. example meaning term molecular biology understood biologist. currently world knowledge bases wikipedia also contain domain knowledge. however categorization mentioned world knowledge bases tried cover common knowledge part partially cover commonsense domain knowledge especially knowledge bases constructed based information extraction. therefore still away solving every problem using current world knowledge bases. however common knowledge already useful enrich data representation introduce weak supervision. general machine learning algorithms learn following data features obtained world extend representation data weak labels automatically obtained world knowledge base. learned model parameter able apply coming data machine learning world knowledge overview collaborative data collection information extraction machine learning widely used. moreover learning algorithms also used inference knowledge bases instead using machine learning construct knowledge bases inference knowledge bases machine learning world knowledge considers existing world knowledge bases help improving existing learning algorithms applications. section consider general machine learning framework incorporate world knowledge machine learning algorithms. multiple ways world knowledge machine learning. mentioned introduction machine learning problems feature extraction/representation label reduction. thus intuitive ways incorporate world knowledge machine learning algorithms classiﬁed categories. however world knowledge designed speciﬁc domain. example world knowledge kinds named entities world want process documents entertainment sports world knowledge names celebrities athletes help terms used science technology useful. thus another issue specify world knowledge domain speciﬁc tasks adapt world knowledge domains. thus summarize issues machine learning world knowledge three categories representation inference learning analogous machine representation. many machine learning algorithms learning world knowledge needs representation data samples. example sequence labeling text named entity recognition predict word’s label features extracted. features one-hot distributional lexical features neural network word embeddings bayesian networks need determine distribution used describe data independency among random variables since world knowledge usually entities world relations representation knowledge categorial structured. moreover knowledge either used features used labels. thus representation world knowledge used machine learning multiple ways. inference. inference means infer knowledge data discover relationships data. example sequence labeling label assignment determined considering possible assignment labels sequence. however efﬁcient ways this e.g. beam search viterbi algorithm possible ways a-star algorithm policy based search also applied. bayesian networks inference involves infer posteriors marginal random variables using variational inference belief propagation random sampling learning world knowledge consider inference problem specifying world knowledge domain problems example given knowledge base document want infer probable categories entities document relationships. process grounding entities document knowledge base usually called entity linking relations also considered usually called semantic parsing general want solve ambiguity problem knowledge speciﬁc problem considering either structural label relationship sequence labeling problem posterior inference bayesian network inference problems. learning. learning refers process estimate parameters models. example sequence labeling parameters weights features. bayesian networks learning refer learning parameters distributions learning structure latent variables. similar machine learning problems learning world knowledge also learning process. depending different representations world knowledge learning processes also different. moreover particular issue world knowledge built speciﬁc domain. thus learning process also problem domain adaption similar introduced transfer learning. however transfer learning domain adaptation usually refers adapt knowledge domain another learning world knowledge domain adaptation refers adapt general knowledge domains. representation world knowledge features section survey existing studies using machine earning world knowledge features. particularly categorize feature representations explicit features implicit features graph based features. explicit homogeneous features traditional distributional representation regarded explicit features representation generated based corpus speciﬁc domain. example co-occurrence syntactic semantic patterns context corpus world knowledge base wikipedia speciﬁc distributed representations developed. here review signiﬁcant development knowledge base based representations textual documents i.e. explicit semantic analysis probabilistic conceptualization extension combinations since models reveal important insight generating features world knowledge. summarize modeling perspective analogous image conceptualization frameworks discussed introduce analyze three ways generate representations descriptive generative discriminative models. world knowledge representation consider generating world knowledge based features original features ei’s features related term document. descriptive generative models consider model probability discriminative model consider directly modeling probability major discussion section follows previous paper explicit semantic analysis generative models ﬁrst paper using term world knowledge extends bag-of-words features categories open directory project shows help improve text classiﬁcation additional knowledge. following this mapping text semantic space provided wikipedia pages ontologies proven useful short text classiﬁcation clustering information retrieval line research approaches generally called explicit semantic analysis simply combines weighted concepts term short text. represent concept vector term example function co-occurrence term entity higher-level concept. original uses tf-idf score shown t-th wikipedia page denoted concept vector denote concept proportion describe whole text containing em}. recalls concepts scores this weight associated e.g. tf-idf score short text. beneﬁt using representation values concept vectors restricted co-occurrence frequencies arbitrarily tuned. regarded generative model since uses concept-term relationship evidence generated features terms estimates latent concept distribution generates features. formulate probability assumed gaussian distribution centered underlying concept distribution maximum likelihood estimate probability ﬂexible example necessarily factorized concept vector co-occurrence frequency concept term sentence document. also deﬁne typicality concept describe term typicality much term instantiate concept probabilistic conceptualization descriptive models probabilistic conceptualization uses different mechanism getting concepts term/entity. applied twitter messages clustering short text categorization bag-of-words labeling search relevance measurement search mining advertising keywords semantic matching semantic frame identiﬁcation given terms short text probabilistic conceptualization tries concepts associated scores best describe terms. suppose general open domain concept ct}. probabilistic conceptualization makes naive bayes assumption conditional probabilities uses score associated here co-occurrence frequency concept term sentences used information extraction normalized number concepts basic assumption behind model given concept observed terms conditionally independent. uses probability rank concepts selects concepts largest probabilities represent text containing terms probabilistic conceptualization regarded imposes partial simple causal markov model since probabilities concept-term relationship. order independency given ﬁrst conditional deﬁne multinomial distribution calculated based evidence co-occurrence knowledge base deﬁne trial selected description short text factorize selecting concepts using among concepts considered maximum posterior estimation posterior illustrates probabilistic conceptualization really optimizes. thus probability equals zero whole probability equals zero. even smoothing technique applied probability mass could small reasonable case. consider relationships em’s. thus generative descriptive model tries jointly model incorporate relationships terms descriptive power also introduced hierarchical classiﬁcation discriminative model another conceptualization classify short text onto predeﬁned taxonomy ontology classiﬁcation regarded discriminative model wants estimate directly modeling probability example learn projection vectors project observed text maximize text clustext classiﬁcation information retering trieval bag-of-words labeling search relevance measurement search mining advertising keywords semantic matching semantic frame identiﬁcation concept vector considered feature vector generate representation short text. typical since hierarchical classiﬁcation extremely large labels costly best choice trying world knowledge base simply features machine learning tasks. instead treating knowledge base source generating features also possible consider structural information provided knowledge base. traditionally graph based algorithms consider knowledge base homogeneous graph homogeneous graph based features e.g. least common ancestor shortest paths etc. disambiguate words reﬁne features text documents even though different kinds relations considered incorporated clear framework formulate explicitly graph based features however working world knowledge graphs sparsity entity relations computational complexity ﬁnding shortest paths possible entities makes shortest path less useful. sense simpler approaches count based features preferred. moreover traditional approaches focus polysemous synonymous properties words means focusing certain types synonym hyponymy-hypernymy relations. however much types relations considered. example freebase thousands entity types relations. effective using types relations considered. section review recent development using heterogeneous information networks represent knowledge graph using meta-path characterize count-based features certain relations entities. thus call approach explicit heterogeneous features. purely graph based feature. however developing features consider structure graph well abstractive level knowledge graph. deﬁnition heterogeneous information network graph entity type mapping relation type mapping denotes entity denotes link denotes entity type denotes relation type number entity types number relation types network schema network denoted graph nodes entity types edges relation types network schema provides high-level description given heterogeneous information network. another important concept meta-path proposed systematically deﬁne relations entities schema level. deﬁnition meta-path path deﬁned graph network schema denoted r−−→ rl−−→ deﬁnes form composite relation types denotes relation composition operator length simplicity type names connected denote meta-path exist multiple relations pair types path network follows meta-path edge belongs relation type call paths path instances denoted concept commuting matrix deﬁned deﬁnition commuting matrix. given network network schema commuting matrix meta-path deﬁned waawaa walal+ waiaj adjacency matrix types represents number path instances objects meta-path text data document semantic parsing semantic ﬁltering ground text world knowledge base. document represented hin. addition named entities provided knowledge base document word also regarded types. following deﬁnition meta-graph sub-graph network schema also denote subgraph original entities subgraph also follow mapping relation type mapping different meta-path chain network schema used meta-graph uses sub-graph deﬁne similarities nodes. however computing similarities based meta-graph difﬁcult meta-path based similarities. implicit features analogous explicit distributional representations many latent/implicit feature representation natural language representation. example latent semantic indexing proposed work explicit features derived context. later probabilistic latent semantic analysis interpret probabilistic latent dirichlet allocation uses bayesian model formulate generative process textual documents bag-of-words. point topic model train representation document-topic distributions topic-word distributions large scale domain independent corpus. many variants topic models distributed topic models etc. topic model used classify domain dependent documents future. plsa regard whole document word’s context global consideration compared distributional representations however natural language processing tasks information extraction tagging local contexts useful. remedy constrain neural network language models called distributed word embedding attracted attention recently given compact representation form generalization property compared traditional lexical representations. language models widely used information retrieval natural language processing many years. however nnlms share advantage continuous representation words showed capability generalization unseen contexts. argue general topic models nnlms incorporate world knowledge models train world’s available resources. however resources unstructured compared highly structured knowledge bases/graphs. general consider knowledge base/graph want entities relations types since semantically useful. example knowledge graph node microsoft look ﬁrst-hot neighbors retrieve properties related entities similar entities thus here focus implicit feature representation related knowledge bases knowledge graphs. compared explicit features easy interpret implicit features encoded read human. however implicit features usually compact good generalization fig. schema document speciﬁed knowledge represented form heterogeneous information network schema contains multiple entity types document word named entities network schema shown fig. represent data. network contains multiple entity types document word named entities relation types connecting entity types. documents linked together many meta-paths. example documents linked meta-path −−−−→politician presidentof document contain politician contain− −−−−−−→document number corresponding meta-path instances used measure similarity documents cannot captured original bag-of-words feature. also represent document features deﬁned meta-paths. simplest meta-path −−−−→word. calculation based meta-paths document contain compute corresponding commuting matrices interests. derive general count-based features entity types deﬁned knowledge graph. existing experiments shown using help text classiﬁcation text clustering based large scale knowledge bases freebase. count based features intuitive. however count normalized different meta-paths difﬁculty compare different meta-path based similarities. thus people thought ways normalization. called pathsimet count based similarity normalized count based similarities entity. given symmetric meta path pathsim entities normalizing commp instead normalize overall commuting matrix normalize individual adjacency matrix. assumes adjacency deﬁnes transition probabilities entities certain relation commuting matrix simulating commuting time entities. recent work shows random walk deﬁned meta-paths could non-stationary expected commuting time deﬁned based random walk discussed performance. authors summarized representation knowledge graph sense statistical relation learning. implicit/latent features entities relations mainly used predict links entities inference knowledge bases knowledge graphs themselves. note consider link prediction problem handled machine learning algorithms approaches surveyed related generalized way. instead reviewing knowledge base embedding methodologies emphasize representation learning algorithms generate features based natural language texts used applications. representation regarded incorporating world knowledge since domain dependent naturally characterize sparsity structural information knowledge based compositional semantics words. moreover generated features general used machine learning algorithms work different tasks. knowledge based topic models. related work train topic model knowledge base. ontology guided hierarchical latent dirichlet allocation uses class labels hierarchy retrieve documents wikipedia trains topic models domain deﬁned class labels. case ontology information used queries submit search wikipedia. hand wikipedia world knowledge base severs additional source provide cleaned relevant documents queries. topic models turn incorporate ontology information guide topical hierarchy construction topics trained wikipedia articles represent general knowledge word distribution queries topics. trained topic models used applications text classiﬁcation traditional topic models recently kb-lda model proposed model hierarchical relations concepts ohlda also model relations like subject-verb-object incorporate linked information. showed kb-lda better capture richer semantic information topics show advantage open tasks knowledge enhanced word embeddings. word embedding also enhanced knowledge graphs. example joint embedding words knowledge graph entities performed ﬁrst aligning text knowledge graph combining objective functions word embedding knowledge graph embedding jointly optimize together. moreover similar approaches used improve knowledge graph embedding approaches interesting since related learning paradigms introduce later section using knowledge graph improve word embedding highly related distant supervision indirect supervision. distant supervision means supervision entity embedding knowledge graph incorporated inexact way. alignment entities unstructured texts perfect. thus different entity senses bring noise word embedding results. indirect supervision means supervision entity embedding directly used supervise words. instead relation embeddings shared word embedding knowledge graph embedding objective functions entities shown text freely optimized based composition word embeddings. combined representations. also combine explicit implicit representations knowledge graph improve results. example simply augmented considering bag-of-concept-embeddings representation approach later reﬁned directly incorporate knowledge graph embedding shown approach robust original especially number concepts used chosen small. inference world knowledge section review inference techniques related world knowledge. incorporate world knowledge base either representation learning important ﬁrst step link free texts knowledge base entities relations call tasks inference assigning labels entities phrases need look global information document even corpus. cannot simply learned inferred based statistics constraints. inference important issues need considered. ambiguity. similar polysemy words entities relation expressions free texts express multiple meanings. example alex smith refer quarterback kansas city chief tight cincinnati bengals. context determine real reference entities mentioned. world knowledge bases need carefully consider problems. section review important problems related inference world knowledge i.e. entity linking semantic parsing. entity linking comprehensive survey different approaches entity linking given focus inference problem entity linking discuss entity linking perspectives local global inference. ﬁrst introduce notations deﬁnition entity linking. ﬁrst deﬁne mention detected needed highlighted free text mentions free text multi-word expression referring named entities objects events philosophy mental states rules etc. determine target encyclopedic resource e.g. wikipedia freebase. task entity linking deﬁne mentions point entities/concepts knowledge base. speciﬁcally deﬁne title entity concept knowledge base ct}. consistent section title refer either concept entity. example example detect martha stewart linked wikipedia categorized person founder winner etc. mentions linked wikipedia task also called wikiﬁcation. also subtle difference entity linking wikiﬁcation. mentions cannot linked wikipidia wikiﬁcation returns null link entity linking task also requires program cluster relevant mentions represent unique concept cluster certain null category. compared introduced section links many related concepts text representation wikiﬁcation links best candidate concept. section discuss difference tasks focus inference problem involved. mention identiﬁcation linking knowledge bases ﬁrst step mention detection identiﬁcation. non-trivial task since natural language arbitrary boundary mentions also arbitrary. thus approaches proposed different ways e.g. using shallow parsing chunks leveraging named taggers developing speciﬁc mention extractors considering n-grams methods existing systems include illinois wikiﬁer uses chunks sub-strings candidates uses prior anchor texts determine potential string; tagme uses prior anchor texts identify mentions; dbpedia spotlight uses dictionary based chunking string matching dbpedia lexicon; aida uses name tagging system mention detection; wikiﬁer uses mention extraction sub-routine detect mentions comparison different approaches presented local inference given mentions detected entity linking task mainly considers link mentions entities concepts knowledge base thus general mention entities concepts local inference uses mention local context features determine entity concept refers example joint probability conditional probabilities rank candidates. probability characterizes commonness mention referring title mention chicago text probability used rank titles chicago city chicago chicago etc. related method mentioned probabilistic conceptualization section uses generate concepts describe entity probability used rank best mention. method usually used initial ranking since robust across different domains. example results shown different topics/genres different domains performance diverse lot. extension initial count based ranking using graph based features also proposed improve local inference results complicated contextual features proposed. general features used compute similarity mention title overall inference solve following maximization problem multiple ways deﬁne similarity function based features including name string matching document surface entity context concept link features proﬁling topic popularity etc. general features constructed traditional similarity metrics cosine jaccard used. also approaches mutual information second order vector composition used similarities evaluation given similarities learning rank weights feature rank candidates titles mentions. different supervised weakly supervised learning approaches machine learning used tool local inference. idea concepts evaluated based features extracted context link mentions knowledge base. again compared probabilistic conceptualization introduced section much features used disambiguate concepts. probabilistic conceptualization simple string matching shallow parsing considered. global inference besides directly comparing entity mentions document candidate concepts knowledge base concepts document also help disambiguate other. example chicago viii used disambiguate entity chicago mentioned together document. thus framework global inference needs developed jointly optimize entity mention concept candidate similarities well concepts relatedness document comparing local inference term added entity mention document. candidate possible related concepts shown document evaluated. concepts documents related likely concepts entity mentioning. ﬁrst relatedness score concepts developed used many systems relatedness scores also developed. comprehensive study different scores given semantic parsing entity linking works linking entity mentions free texts knowledge base. however relations entities considered. also want relations text knowledge base semantic parsing developed. traditionally semantic parsing refers task mapping piece natural language text formal meaning representation different context semantic parsing mean different tasks. example knowledge base grounded semantic parsing form parsing shallow semantic role labeling formally convert given sentence appropriate logic forms. example questions shown below convert different logic forms targets parse simple sentences. multiple ways ﬁnding proper semantic parsing results natural language texts. first local inference involves mapping entity mentions relation expressions natural language texts entities relations knowledge base. entity identiﬁcation similar mention identiﬁcation linking problems entity linking. however knowledge bases like freebase yago less texts describe entities wikipedia has. therefore entity identiﬁcation linking problems difﬁcult. relation expressions paraphrasing usually used identify relations identify correct logic forms text typical determine logic form based cost function possible supervision logic forms. different entity linking naive unsupervised similarity evaluate candidate logic form sentence. therefore proper logic form give text different strategies investigated. intuitive supervised learning learn parameters cost function correct logic forms annotated sentence however supervision heavily requires laboring cost since grounded logic forms knowledge graph exponentially many human judge. thus studies done reduce replace annotation requirement problems. instead direct supervision logic forms indirect supervision answer used train parameters logic forms treated hidden variables cost function. optimize cost function latent variables integrated working parameters. could exponentially increasing logic forms candidates extracted sentence. thus heuristic pruning beam search performed recently staged parsing investigated reports efﬁcient effective results. learning strategies combining imitation learning agenda based parsing also used improve efﬁciency effectiveness semantic parsing also signiﬁcantly reduce search space distant supervision also used supervise semantic parsing. distant supervision refers approach using knowledge base entity relation triples scale documents high frequent triples gold supervise lexical mapping knowledge graph entities predicates texts triples exists example google released data using freebase automatically annotate large document collection clueweb obtain cheap annotation annotation noisy incomplete training model. neural network based learning models proposed replace components ﬁnal learning algorithm grounded semantic parsing example population seattle? example many people live seattle? example λx.population example count live) examples refer meaning. however logic forms could different depending semantic parsing algorithm rely well lexicon build determine paraphrasing similarities predicates semantic parsing provide ﬁne-grained entity types knowledge bases grounding knowledge bases semantic parsing well known question answering. previous semantic parsing algorithms tools developed small scale problems complicated logical forms recently large scale semantic parsing grounding world knowledge bases investigated e.g. using freebase reverb formally entities relations knowledge base. knowledge graph consists triplets form take lambda dependency-based compositional semantics example demonstrate speciﬁc forms grounding natural language texts logic forms types. simpliﬁed lλ-dcs deﬁnes logic language query knowledge base. logical form simple λ-dcs either form unary binary brieﬂy introduce deﬁnition basic λ-dcs logical forms corresponding denotations below unary base entity unary logic form {e}; binary base relation binary logic form join unary logic form denoting join projection binary unary. b.uk intersection denotes intersection education.barackobama). overall text below parse logic forms grounding based semantic parsing approaches applied answer questions world knowledge bases example example president united states. example ype.p eople residentof country.u country.u join together residentof country.u moreover word lexically mapped ype.p eople predicate relationship entity concept. thus relationship entities type information entities naturally incorporated logic form grounding knowledge graph. systems approaches also consider unsupervised train parameters best knowledge ﬁrst grounded unsupervised semantic parsing adopted self-training strategy. uses initial seeds evaluated translation based score bootstraps learning procedure update system parameters. fader paraphrasing evaluate similarities questions possible grounded results grounding restricted simple forms unary binary relations. poon uses dependency tree backbones candidate logic forms tries annotate tree knowledge base entities relations approaches assume given questions logic forms induced maximizing likelihood certain constrained logic forms. thus still limited either small scale knowledge graphs simple logic forms. semantic parsing applied document global inference also performed. similar unsupervised approaches introduced local inference. however local inference leverages question determine parameters uses relationships among entities relations extracted questions disambiguate other. entity linking logic forms ﬁltered know logic forms sentences document corpus. show semantic ﬁltering strategy proposed example motivated approaches generative+discriminative conceptualization represent entity feature vector entity types standard kmeans cluster entities document. suppose cluster entities {e··· ene}. probabilistic conceptualization proposed likely entity types entities cluster. make naive bayes assumpi= score entity type here co-occurrence count entity type entity knowledge base overall number entities type knowledge base. besides probability used rank entity types largest ones selected. case cluster entities common types retained concepts conﬂicts ﬁltered out. also possible apply global inference approaches used entity linking shown section section introduce learning paradigms enabled world knowledge. categorize paradigms ways related world knowledge features ways related world knowledge supervision. paradigms related world knowledge features representations introduced section incorporated learning algorithms. paradigms related world knowledge supervision inferences mainly used categorized entities relations inferred approaches introduced section self-taught learning ﬁrst learning setting enabled universal world knowledge called self-taught learning self-taught learning uses large amount unlabeled data crawled train unsupervised representation learning. supervised classiﬁer applied features trained based unlabeled data. regarded using data better universal data distribution strongly related discriminative classiﬁer ﬁne-tuned essentially semi-supervised learning setting large amount unlabeled data used help supervised learning tasks less labels. particularly self-taught learning decouples representation learning part using unlabeled data classiﬁer training using labeled data require labeled data unlabeled data sampled distribution. applied text classiﬁcation also shares ides features piece short text generated wikipedia classiﬁcation performed knowledge based features. deep learning community also found unsupervised learning using restricted bolzman machines auto-encoders variants comprehensive survey) helpful training deep neural network architectures. unsupervised learning pre-training trained unlabeled data introduce ﬁne-tuning process reﬁne model deep architectures. natural language processing also shown using brown clusters words word embeddings learned large scale unlabeled data training another classiﬁer deep learning speciﬁc tasks help improve task speciﬁc performance. thus representations introduced section regarded pre-training step self-taught learning step many applications. source-free transfer learning originally shown section transfer learning refers setting training source domain containing labeled data. however major challenges. availability source domain data automatically select source domain transfer learning. source-free transfer learning tries solve problem world knowledge instead domain knowledge world knowledge source-free transfer learning wikipedia categories wikipedia categories provide large amount categorized text data. large classiﬁers built based categorized data. coming target domain label similarities evaluated source domains target domain automatically source domain transfer. using world knowledge initial classiﬁer built target domain meta-learning algorithm designed automatically features classiﬁer query unlabeled data cleaned wikipedia corpus traditional semi-supervised learning algorithms graph regularization applied iteratively train classiﬁer based labeled incrementally increasing unlabeled data. ohlda also regarded source-free transfer learning framework. uses topical label keywords search query search wikipedia google uses retrieved documents train hierarchical topic model. topic model used classiﬁer classify documents given ontology labels. dataless classiﬁcation performs nearest neighbor search labels document appropriately selected semantic space representation document semantic space representations labels space. evaluate similarity using appropriate metric select maxi φ)). essentially learning paradigm semantic called supervisionless supervision label names. core problem dataless classiﬁcation semantic space enables good representations documents labels. traditional text classiﬁcation makes bag-of-words representation documents. however comparing labels documents dataless classiﬁcation brevity labels makes simple minded representation resulting similarity measure unreliable. example document talking sports necessarily contain word sports. consequently expressive distributional representations applied e.g. brown cluster neural network embedding topic modeling combinations shown gives best robust results dataless classiﬁcation english documents idea generalized hundreds languages wikipedia language links english english labels documents languages mapped semantic space using cross-lingual extended classify languages world dictionary mapping target language pivot language linked english zero-shot learning also introduced computer vision community recognized natural language processing community zero-shot learning means labeled data coming target domain. however requires source data train model. target classiﬁer bridged based label similarities target labels source labels. sense zero-shot learning similar ﬁrst mechanism source-free transfer learning. another learning mechanism similar name one-shot learning however one-shot learning requires example training zero-shot learning test data different training data paradigms enabled world knowledge supervision world knowledge used features also used supervision. section review learning paradigms enabled world knowledge supervision i.e. distant supervision indirect supervision. distant supervision idea using minimal supervision aligned entity mentions texts explored previously. distant supervision extends idea knowledge entities relationships world knowledge bases e.g. freebase supervision task entity relation extraction since entities relations world knowledge bases aligned mentions texts ﬁrst step distant supervision entities texts. entity linking considered cheaper ways simple string matching named entity tagger employed since training step conﬁdent examples needed. sentences mapped entities relations used labeled examples train relation classiﬁer. since direct annotation entities relations sentence automatically mapped annotation knowledge base used approach called distant supervision. whole process argue human annotation automatically aligned entities accurate enough relation extraction. assume multiple sentences pair entities extracted part support decision relation entities problem formulated multi-instance learning problem proved multi-instance learning signiﬁcantly reduce effect noisy labels. moreover since entities type relations distance supervision also formulated multi-instance multi-label learning problem another view relation extraction knowledge distant supervision formulate problem matrix completion problem also related statistical representation learning knowledge graph studies approach claims unify open information extraction relation classiﬁcation. moreover shown joint representation words knowledge graph also help improve distant supervision relation classiﬁcation recently neural network learning based algorithm tested distant supervision setting extensions different learning settings also proposed. example combination semi-supervised learning background knowledge also help distant supervision’s performance. also uses positive unlabeled learning setting handle distant supervision knowledge base incomplete. indirect supervision besides distant supervision also possible world knowledge indirect supervision. idea indirect supervision general learning setting example uses cheaper annotation indirect supervision complicated learning problems. document-level topic annotation cheaper information extraction annotation. document-level binary multi-class classiﬁcation help reﬁne parameter esitmation problem structural output learning. uses answers supervise semantic parsing problem question answering. target semantic parsing output formal logic forms. however costly label logic forms machines learn. thus using indirect supervision natural reduce labeling work. introduced learning setting privacy preserved machine learning based indirect supervision. context natural language processing example labeling number particular tags instead labeling individual tags indirect supervision. summary different transfer learning indirect supervision different comparing available annotation target machine learning. general learning setting introduced case case. research related world knowledge song considered using fully unsupervised method generate constraints words using external general-purpose knowledge base wordnet document clustering. regarded initial attempt general knowledge indirect supervision help clustering. however knowledge wordnet mostly linguistically related. lacks information named entities types. moreover approach still simple application constrained co-clustering misses rich structural information knowledge base. extend idea indirect supervision using world knowledge bases freebase yago proposed applied document clustering problem uses semantic parsing ground entities relations world knowledge base builds heterogeneous information network formulate structured data documents. information entity categories e.g. celebrities companies politicians propagated group documents topics. conclusion future directions paper formulated problems machine learning world knowledge reviewed related methodologies algorithms involved respective problems. ﬁrst compare learning world knowledge learning domain knowledge categorize problems three folds i.e. representation inference learning. representation introduced different kinds representation world knowledge explicit homogeneous features explicit heterogeneous features implicit features. inference introduced entity linking semantic parsing align free text knowledge bases. learning introduced several learning paradigms enabled world knowledge. still open problems answer current stage think important including commonsense acquisition learning commonsense knowledge. commonsense knowledge problem since artiﬁcial intelligence introduced. world knowledge cover partial commonsense knowledge still knowledge missing. commonsense knowledge important performing inference natural language well many applications thus acquire commonsense knowledge still open problems. representation representation learning. currently comparison traditional distributional representation advanced representation learning. tasks dataless classiﬁcation traditional distributional representation i.e. still performs learning based representations. would important ways improve corresponding representation using advances learning techniques. joint inference learning. problems inference learning introduced paper mostly separated. joint inference learning help boost performance regard natural idea general machine learning world knowledge framework. challenge joint inference learning costly terms computational efﬁciency effectiveness. cross-lingual cross-culture world knowledge. different communities people different background different kind common knowledge information collected world knowledge biased different different languages cultures. interesting compare different languages cultures ways correct bias them. connecting world knowledge cognitive science. evidence human performing transfer learning semi-supervised learning active learning also analysis connecting one-shot learning bayesian learning human learning. would interesting humans leverage world knowledge learning problems tasks. open problems still discovered regard equally important. moreover problem machine learning would interesting important general framework machine learning algorithms world knowledge available. acknowledgments authors wish thank anonymous reviewers previous papers. also thanks collaborators corresponding publications referenced paper discussions suggestions. work supported china fundamental program supported darpa agreement numbers hr---. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright notation thereon. views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied organizations supported work. choi p¨arnamaa bengio neural knowledge language model corr vol. abs/. anast´acio martins calado supervised learning linking named entities knowledge base entries angeli tibshirani manning combining distant partial supervision relation extraction emnlp artzi fitzgerald zettlemoyer semantic parsing combinatory categorial grammars tutorial abstracts auer bizer kobilarov lehmann cyganiak ives dbpedia nucleus open data. springer category detection iccv banko cafarella soderland broadhead etzioni open information extraction ijcai basu bilenko mooney probabilistic framework semi-supervised clustering basu davidson wagstaff constrained clustering advances algorithms theory applications. chapman hall/crc belkin niyogi sindhwani manifold regularization geometric framework learning labeled unlabeled examples journal machine learning research vol. bollacker evans paritosh sturge taylor freebase collaboratively created graph database structuring human knowledge sigmod bragg mausam weld crowdsourcing multi-label classiﬁcation taxonomy creation aaai conference human computation crowdsourcing cassidy deng zheng analysis reﬁnement cross-lingual entity linking information access evaluation. multilinguality multimodality visual analytics third international conference clef initiative chilton little edge weld landay cascade crowdsourcing taxonomy creation sigchi conference human factors computing systems. cimiano staab tane automatic acquisition taxonomies text meets ecml/pkdd workshop adaptive text extraction mining cavtat-dubrovnik croatia collobert weston bottou karlen kavukcuoglu kuksa natural language processing scratch journal machine learning research vol. cunningham ghahramani linear dimensionality reduction survey insights generalizations journal machine learning research vol. faure n´edellec corpus-based conceptual clustering method verb frames ontology acquisition lrec workshop adapting lexical corpus resources sublanguages applications gopal yang recursive regularization large-scale classiﬁcation hierarchical graphical dependencies sigkdd international conference knowledge discovery data mining chicago august hinton salakhutdinov reducing dimensionality data neural networks science vol. hinton osindero fast learning algorithm deep belief nets neural computation vol. jiang song wang zhang semi-supervised learning heterogeneous information networks ensemble meta-graph guided random walks ijcai johnson krishna stark shamma bernstein image retrieval using scene graphs cvpr johnson schuster krikun chen thorat vi´egas wattenberg corrado hughes dean google’s multilingual neural machine translation system enabling zero-shot translation corr vol. abs/. krishna groth johnson hata kravitz chen kalantidis shamma bernstein visual genome connecting language vision using crowdsourced dense image annotations corr vol. abs/. available http//arxiv.org/abs/. chang peirsman chambers surdeanu jurafsky deterministic coreference resolution based entity-centric precision-ranked rules comput. linguist. vol.", "year": 2017}