{"title": "Generative Adversarial Imitation Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.", "text": "consider learning policy example expert behavior without interaction expert access reinforcement signal. approach recover expert’s cost function inverse reinforcement learning extract policy cost function reinforcement learning. approach indirect slow. propose general framework directly extracting policy data obtained reinforcement learning following inverse reinforcement learning. show certain instantiation framework draws analogy imitation learning generative adversarial networks derive model-free imitation learning algorithm obtains significant performance gains existing model-free methods imitating complex behaviors large high-dimensional environments. interested speciﬁc setting imitation learning—the problem learning perform task expert demonstrations—in learner given samples trajectories expert allowed query expert data training provided reinforcement signal kind. main approaches suitable setting behavioral cloning learns policy supervised learning problem state-action pairs expert trajectories; inverse reinforcement learning ﬁnds cost function expert uniquely optimal. behavioral cloning appealingly simple tends succeed large amounts data compounding error caused covariate shift inverse reinforcement learning hand learns cost function prioritizes entire trajectories others compounding error problem methods single-timestep decisions issue. accordingly succeeded wide range problems predicting behaviors taxi drivers planning footsteps quadruped robots unfortunately many algorithms extremely expensive requiring reinforcement learning inner loop. scaling methods large environments thus focus much recent work fundamentally however learns cost function explains expert behavior directly tell learner act. given learner’s true goal often take actions imitating expert—indeed many algorithms evaluated quality optimal actions costs learn—why then must learn cost function possibly incurs signiﬁcant computational expense fails directly yield actions? desire algorithm tells explicitly directly learning policy. develop algorithm begin section characterize policy given running reinforcement learning cost function learned maximum causal entropy characterization introduces framework directly learning policies data bypassing intermediate step. then instantiate framework sections model-free imitation learning algorithm. show resulting algorithm intimately connected generative adversarial networks technique deep learning community recent successes modeling distributions natural images algorithm harnesses generative adversarial training distributions states actions deﬁning expert behavior. test algorithm section outperforms competing methods wide margin training policies complex high-dimensional physics-based control tasks various amounts expert data. background preliminaries denote extended real numbers {∞}. section work ﬁnite state action spaces avoid technical machinery scope paper algorithms experiments later paper high-dimensional continuous environments. stationary stochastic policies take actions given states successor states drawn dynamics model work γ-discounted inﬁnite horizon setting expectation respect policy denote expectation respect trajectory generates γtc] denote empirical expectation respect trajectory samples always refer expert policy inverse reinforcement learning suppose given expert policy wish rationalize irl. remainder paper adopt maximum causal entropy cost function family functions optimization problem γ-discounted causal entropy policy practice provided trajectories sampled executing environment expected cost estimated using samples. maximum causal entropy looks cost function assigns cost expert policy high cost policies thereby allowing expert policy found certain reinforcement learning procedure begin search imitation learning algorithm bypasses intermediate step suitable large environments study policies found reinforcement learning costs learned largest possible cost functions functions rs×a using expressive cost function classes like gaussian processes neural networks crucial properly explain complex expert behavior without meticulously hand-crafted features. here investigate best respect expressiveness examining capabilities rs×a. course large easily overﬁt provided ﬁnite dataset. therefore incorporate convex cost function regularizer rs×a study. note convexity particularly restrictive requirement must convex function deﬁned rs×a function deﬁned small parameter space; indeed cost regularizers finn effective range robotic manipulation tasks satisfy requirement. interestingly fact plays central role discussion nuisance analysis. deﬁne primitive procedure ﬁnds cost function expert performs better policies cost regularized irlψ. interested policy given rl—this policy given running reinforcement learning output irl. characterize useful transform optimization problems policies convex problems. policy deﬁne occupancy measure occupancy measure interpreted distribution state-action pairs agent encounters navigating environment policy cost function basic result valid occupancy measures written feasible afﬁne constraints distribution starting states dynamics model furthermore one-to-one correspondence proposition occupancy measure therefore justiﬁed writing denote unique policy occupancy measure need tool function rs×a convex conjugate rs×a given supy∈rs×a ready characterize policy learned cost recovered proposition irlψ minπ∈π proof proposition appendix proof relies observation optimal cost function policy form saddle point certain function. ﬁnds coordinate saddle point running reinforcement learning output reveals coordinate. proposition tells ψ-regularized inverse reinforcement learning implicitly seeks policy whose occupancy measure close expert’s measured convex function enticingly suggests various settings lead various imitation learning algorithms directly solve optimization problem given proposition explore algorithms sections show certain settings lead existing algorithms novel one. special case constant function particularly illuminating state show directly using concepts convex optimization. corollary constant function irlψ words cost regularization recovered policy exactly match expert’s occupancy measure. show this need lemma lets speak causal entropies occupancy measures proof lemma appendix proposition lemma together allow freely switch policies occupancy measures considering functions involving causal entropy expected costs following lemma lagrangian costs serve dual variables equality constraints. thus dual optimum convex convex strong duality holds; moreover lemma guarantees fact strictly convex primal optimum uniquely recovered dual optimum minρ∈d ﬁrst equality indicates unique minimizer third follows constraints primal problem then lemma occupancy measure satisﬁes argument deduce following dual occupancy measure matching problem recovered cost function dual optimum. classic algorithms solve reinforcement learning repeatedly inner loop algorithm ziebart runs variant value iteration inner loop interpreted form dual ascent repeatedly solves primal problem ﬁxed dual values dual ascent effective solving unconstrained primal efﬁcient case amounts reinforcement learning induced optimal policy primal optimum. induced optimal policy obtained running exactly recovering primal optimum dual optimum; optimizing lagrangian dual variables ﬁxed dual optimum values. strong duality implies induced optimal policy indeed primal optimum therefore matches occupancy measures expert. traditionally deﬁned ﬁnding cost function expert policy uniquely optimal alternatively view procedure tries induce policy matches expert’s occupancy measure. corollary constant resulting primal problem simply matches occupancy measures expert states actions. algorithm however practically useful. reality expert trajectory distribution provided ﬁnite samples large environments expert’s occupancy measure values exactly zero exact occupancy measure matching force learned policy never visit unseen state-action pairs simply lack data. furthermore large environments would like function approximation learn parameterized policy resulting optimization problem ﬁnding appropriate would many constraints points leading intractably large problem defeating purpose function approximation. keeping mind wish eventually develop imitation learning algorithm suitable large environments would like relax following form motivated proposition modifying regularizer smoothly penalizes violations difference occupancy measures. entropy-regularized apprenticeship learning turns certain settings takes form regularized variants existing apprenticeship learning algorithms indeed scale large environments parameterized policies class cost functions rs×a apprenticeship learning algorithm ﬁnds policy performs better expert across optimizing objective classic apprenticeship learning algorithms restrict convex sets given linear combinations basis functions give rise feature vector state-action pair. abbeel syed respectively clinear leads feature expectation matching minimizes distance expected eπ]−eπe eπ]−eπe meanwhile feature vectors maxc∈clinear cconvex leads mwal lpal minimize worst-case excess cost among eπ]− maxi∈{...d} eπ]− individual basis functions maxc∈cconvex show special case certain setting indicator function rs×a deﬁned otherwise write apprenticeship learning objective eπ]−eπe equivalent performing following cost regularizer forces implicit procedure recover cost function lying note scale policy’s entropy regularization strength scaling constant recovering original apprenticeship objective taking cons apprenticeship learning known apprenticeship learning algorithms generally recover expert-like policies restrictive —which often case linear subspaces used feature expectation matching mwal lpal unless basis functions carefully designed. intuitively unless true expert cost function lies guarantee performs better equals aforementioned insight based proposition apprenticeship learning equivalent following understand exactly apprenticeship learning fail imitate forces encoded element include cost function explains expert behavior well attempting recover policy encoding succeed. pros apprenticeship learning restrictive cost classes lead exact imitation apprenticeship learning scale large state action spaces policy function approximation. rely following policy gradient formula apprenticeship objective parameterized policy observing policy gradient reinforcement learning objective cost propose algorithm alternates steps algorithm relies crucially trpo policy step natural gradient step constrained ensure πθi+ stray measured divergence policies averaged states sampled trajectories. carefully constructed step scheme ensures divergence occur high noise estimating gradient refer reader schulman details trpo. trpo step scheme able train large neural network policies apprenticeship learning linear cost function classes environments hundreds observation dimensions. linear cost function classes however limits approach settings expert behavior well-described classes. draw upon algorithm develop imitation learning method scales large environments imitates arbitrarily complex expert behavior. ﬁrst turn proposing regularizer wields expressive power regularizers corresponding clinear cconvex discussed section constant regularizer leads imitation learning algorithm exactly matches occupancy measures intractable large environments. indicator regularizers linear cost function classes hand lead algorithms incapable exactly matching occupancy measures without careful tuning tractable large environments. propose following cost regularizer combines best worlds show coming sections regularizer places penalty cost functions assign amount negative cost expert state-action pairs; however assigns large costs expert heavily penalize interesting property average expert data therefore adjust arbitrary expert datasets. indicator regularizers used linear apprenticeship learning algorithms described section always ﬁxed cannot adapt data can. perhaps important difference however forces costs small subspace spanned ﬁnitely many basis functions whereas allows cost function long negative everywhere. choice motivated following fact shown appendix maximum ranges discriminative classiﬁers equation optimal negative loss binary classiﬁcation problem distinguishing state-action pairs turns optimal loss jensen-shannon divergence squared metric distributions treating causal entropy policy regularizer controlled obtain imitation learning algorithm ﬁnds policy whose occupancy measure minimizes jensen-shannon divergence expert’s. equation minimizes true metric occupancy measures unlike linear apprenticeship learning algorithms imitate expert policies exactly. algorithm equation draws connection imitation learning generative adversarial networks train generative model confuse discriminative classiﬁer distinguish distribution data generated true data distribution. cannot distinguish data generated true data successfully matched true data. setting learner’s occupancy measure analogous data distribution generated expert’s occupancy measure analogous true data distribution. present practical algorithm call generative adversarial imitation learning solving model-free imitation large environments. explicitly wish saddle point expression ﬁrst introduce function approximation parameterized policy weights discriminator network weights then alternate adam gradient step increase respect trpo step decrease respect trpo step serves purpose apprenticeship learning algorithm prevents policy changing much noise policy gradient. discriminator network interpreted local cost function providing learning signal policy—speciﬁcally taking policy step decreases expected cost respect cost function move toward expert-like regions state-action space classiﬁed discriminator. appendix a..) evaluated algorithm baselines physics-based control tasks ranging lowdimensional control tasks classic literature—the cartpole acrobot mountain difﬁcult high-dimensional tasks humanoid locomotion solved recently model-free reinforcement learning environments classic control tasks simulated mujoco appendix complete description tasks. task comes true cost function deﬁned openai ﬁrst generated expert behavior tasks running trpo true cost functions create expert policies. then evaluate imitation performance respect sample complexity expert data sampled datasets varying trajectory counts expert policies. trajectories constituting dataset consisted state-action pairs. tested algorithm three baselines behavioral cloning given dataset state-action pairs split training data validation data. policy trained supervised learning using adam minibatches examples validation error stops decreasing. used algorithms train policies neural network architecture tasks hidden layers units each tanh nonlinearities between. discriminator networks algorithm also used architecture. networks always initialized randomly start trial. task gave gtal algorithm exactly amount environment interaction training. figure depicts results tables appendix provide exact performance numbers. found classic control tasks behavioral cloning suffered expert data efﬁciency compared gtal part able produce policies near-expert performance wide range dataset sizes. tasks generative adversarial algorithm always produced policies performing better behavioral cloning gtal. however behavioral cloning performed excellently reacher task sample efﬁcient algorithm. able slightly improve algorithm’s performance reacher using causal entropy regularization—in -trajectory setting improvement statistically signiﬁcant training reruns according one-sided wilcoxon rank-sum test used causal entropy regularization tasks. mujoco environments large performance boost algorithm baselines. algorithm almost always achieved least expert performance dataset figure performance learned policies. y-axis negative cost scaled expert achieves random policy achieves causal entropy regularization reacher. sizes tested nearly always dominating baselines. gtal performed poorly producing policies consistently worse policy chooses actions uniformly random. behavioral cloning able reach satisfactory performance enough data halfcheetah hopper walker ant; unable achieve humanoid algorithm achieved exact expert performance tested dataset sizes. demonstrated method generally quite sample efﬁcient terms expert data. however particularly sample efﬁcient terms environment interaction training. number samples required estimate imitation objective gradient comparable number needed trpo train expert policies reinforcement signals. believe could signiﬁcantly improve learning speed algorithm initializing policy parameters behavioral cloning requires environment interaction all. fundamentally method model free generally need environment interaction model-based methods. guided cost learning instance builds upon guided policy search inherits sample efﬁciency also inherits requirement model well-approximated iteratively ﬁtted time-varying linear dynamics. interestingly algorithm guided cost learning alternate policy optimization steps cost ﬁtting even though algorithms derived completely differently. approach builds upon vast line work hence like approach interact expert training. method explores randomly determine actions bring policy’s occupancy measure closer expert’s whereas methods interact expert like dagger simply expert actions. ultimately believe method combines well-chosen environment models expert interaction terms sample complexity expert data environment interaction. kingma adam method stochastic optimization. arxiv preprint arxiv. levine abbeel. learning neural network policies guided policy search unknown moore hall. efﬁcient memory-based learning robot control. russell. algorithms inverse reinforcement learning. icml nguyen wainwright jordan. surrogate loss functions f-divergences. annals todorov erez tassa. mujoco physics engine model-based control. intelligent robots systems ieee/rsj international conference pages ieee ziebart maas bagnell dey. maximum entropy inverse reinforcement learning. summing sides shows ¯hρ) equality applying proposition shows equality fact holds strictly concave. turn verifying last statements also follow proposition deﬁnition occupancy measures. first section described cost regularizer leads imitation learning algorithm minimizes jensen-shannon divergence occupancy measures. justify choice show convert certain surrogate loss functions binary classiﬁcation state-action pairs drawn occupancy measures cost function regularizers minimum expected risk speciﬁcally restrict strictly decreasing convex loss functions. nguyen show correspondence minimum expected risks f-divergences jensen-shannon divergence special case. following construction therefore generate imitation learning algorithm minimizes f-divergence occupancy measures long f-divergence induced strictly decreasing convex surrogate proposition suppose strictly decreasing convex function. range deﬁne rs×a proof. verify ﬁrst claim sufﬁces check closed proper convex. convexity follows fact convex concave function followed nonincreasing convex function. furthermore nonempty proper. show closed note strictly decreasing convex range either open interval range ﬁnite everywhere therefore closed. hand range thus implying means closed. showed construct cost function regularizer obtain corollary cost function regularizer logistic loss whose optimal expected risk constant jensen-shannon divergence. corollary a... cost regularizer environments used experiments openai names version numbers environments listed table also lists dimension cardinality observation action spaces amount environment interaction used gtal algorithm shown table reduce gradient variance three algorithms also value functions neural network architecture policies employed generalized advantage estimation exact experimental results listed table means standard deviations computed trajectories. cartpole mountain acrobot reacher statistics computed policies learned random initializations.", "year": 2016}