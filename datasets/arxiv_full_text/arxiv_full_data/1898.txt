{"title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language  Models", "tag": ["cs.LG", "cs.CL", "cs.CV"], "abstract": "Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - \"blue\" + \"red\" is near images of red cars. Sample captions generated for 800 images are made available for comparison.", "text": "inspired recent advances multimodal learning machine translation introduce encoder-decoder pipeline learns multimodal joint embedding space images text novel language model decoding distributed representations space. pipeline effectively uniﬁes joint image-text embedding models multimodal neural language models. introduce structure-content neural language model disentangles structure sentence content conditioned representations produced encoder. encoder allows rank images sentences decoder generate novel descriptions scratch. using lstm encode sentences match state-of-the-art performance flickrk flickrk without using object detections. also best results using -layer oxford convolutional network. furthermore show linear encoders learned embedding space captures multimodal regularities terms vector space arithmetic e.g. *image blue car* \"blue\" \"red\" near images cars. sample captions generated images made available comparison. generating descriptions images long regarded challenging perception task integrating vision learning language understanding. needs correctly recognize appears images also incorporate knowledge spatial relationships interactions objects. even information needs generate description relevant grammatically correct. recent advances made deep neural networks tasks object recognition detection made signiﬁcant breakthroughs short time. task describing images appears tractable ripe advancement. able append large image databases accurate descriptions image would signiﬁcantly improve capabilities content-based image retrieval systems. moreover systems describe images well could principle ﬁne-tuned answer questions images also. paper describes approach problem image caption generation casted framework encoder-decoder models. encoder learn joint image-sentence embedding sentences encoded using long short-term memory recurrent neural networks image features deep convolutional network projected embedding space lstm hidden states. pairwise ranking loss minimized order learn rank images descriptions. decoding introduce neural language model called structure-content neural language model sc-nlm differs existing models disentangles structure sentence content conditioned distributed representations produced encoder. show sampling sc-nlm allows generate realistic image captions signiﬁcantly improving generated captions produced furthermore argue combination approaches naturally experimentation framework good encoder used rank images captions good decoder used generate captions scratch. approach effectively uniﬁes image-text embedding models multimodal neural language models furthermore method builds analogous approaches used machine translation application focus work image description generation ranking also qualitatively analyse properties multimodal vector spaces learned using images sentences. show using linear sentence encoder linguistic regularities also carry multimodal vector spaces. example *image blue car* \"blue\" \"red\" results vector near images cars. qualitatively examine several types analogies structures projections. consequently even global image-sentence training objective encoder still used retrieve locally analogous pairwise ranking methods used machine translation large body work done learning multimodal representations images text. popular approaches include learning joint image-word embeddings well embedding images sentences common space proposed pipeline makes direct ideas. approaches multimodal learning include deep boltzmann machines log-bilinear neural language models autoencoders recurrent neural networks topic-models several bi-directional approaches ranking images captions also proposed based kernel normalized dependency tree recursive networks architectural standpoint encoder-decoder model similar proposed two-step embedding generation procedure semantic parsing. group together approaches generation three types methods described detail template-based methods. template-based methods involve ﬁlling sentence templates triplets based results object detections spatial relationships figure encoder deep convolutional network long short-term memory recurrent network learning joint image-sentence embedding. decoder neural language model combines structure content vectors generating words time sequence. approaches produce accurate descriptions often ‘robotic’ nature generalize ﬂuidity naturalness captions written humans. composition-based methods. approaches harness existing image-caption databases extracting components related captions composing together generate novel descriptions advantage approaches allow much broader expressive class captions ﬂuent human-like template-based approaches. neural network methods. approaches generate descriptions sampling conditional neural language models. initial work area based multimodal neural language models generated captions conditioning feature vectors output deep convolutional network. ideas recently extended multimodal recurrent networks signiﬁcant improvements methods described paper produce descriptions least qualitatively current state-of-the-art composition-based methods description generation systems plagued issues evaluation. bleu rouge used past argued automated evaluation methods unreliable match human judgements. authors instead proposed problem ranking images captions used proxy generation. since generation system requires scoring function access well caption image match optimizing task naturally carry improvement generation. many recent methods since used approach evaluation. none less question transfer improvements ranking generating descriptions remained. argue encoder-decoder methods naturally experimentation framework. encoder gives rank images captions develop good scoring functions decoder representations learned optimize scoring functions generating scoring descriptions. proposed pipeline caption generation already experienced several successes neural machine translation goal develop end-to-end translation system large neural network opposed using neural network additional feature function existing phrase-based system. methods based encoder-decoder principle. encoder used english sentence distributed vector. decoder conditioned vector generate french translation source text. current methods include using convolutional encoder decoder encoder decoder lstm encoder lstm decoder still young research area methods already achieved performance strong phrase-based systems improved start-of-the-art used rescoring. argue natural think image caption generation translation problem. goal translate image description. point view also used allows make existing ideas machine translation literature. furthermore natural correspondence concept scoring functions alignments naturally exploited generating descriptions. section describe image caption generation pipeline. ﬁrst review lstm rnns used encoding sentences followed learn multimodal distributed representations. review log-bilinear neural language models multiplicative neural language models introduce structure-content neural language model. long short-term memory recurrent neural network incorporates built memory cell store information exploit long range context. lstm memory cells surrounded gating units purpose reading writing reseting information. lstms used achieve state-of-the-art performance several tasks handwriting recognition sequence generation speech recognition machine translation among others. dropout strategies also proposed prevent overﬁtting deep lstms. denote matrix training instances time case used denote matrix word representations t-th word sentence training batch. denote input forget cell output hidden states lstm time step lstm architecture work implemented using following equations suppose training given image-description pairs corresponding image description correctly describes image. images represented layer convolutional network trained imagenet classiﬁcation task dimensionality image feature vector dimensionality embedding space number words vocabulary. rk×d rk×v image embedding matrix word embedding matrices respectively. given image description words denote corresponding word representations words representation sentence hidden state lstm time step note approaches computing sentence representations image-text embeddings proposed including dependency tree rnns bags dependency parses denote image feature vector image embedding. deﬁne scoring function ﬁrst scaled unit norm denote parameters learned optimize following pairwise ranking loss contrastive sentence image embedding vice-versa experiments initialize word embeddings pre-computed dimensional vectors learned using continuous bag-of-words model contrastive terms chosen randomly training resampled every epoch. additional details lstm http//people.idsia.ch/~juergen/rnn.html. slight abuse notation refer word index word embedding matrix. keep word embedding matrix ﬁxed. log-bilinear language model deterministic model viewed feedforward neural network single linear hidden layer. word vocabulary represented k-dimensional real-valued vector case encoder. denote matrix word representation vectors vocabulary size. tuple words context size. model makes linear prediction next word representation multiplicative neural language models suppose given vector multimodal vector space association word sequence wn}. example embedded representation image whose description given multiplicative neural language model models distribution word given context previous words vector multiplicative model additional property word embedding matrix instead replaced tensor ×k×g number slices. given i.e. word representations respect computed linear combination slices weighted component here number slices equal dimensionality often unnecessary fully unfactored tensor. e.g. re-represent terms three matrices rf×k rf×g rf×v diag denotes matrix argument diagonal. matrices parametrized pre-chosen number factors conditioning vector referred attribute using third-order model words allows model conditional similarity meanings words change function attributes they’re conditioned denote ‘folded’ matrix word embeddings. given context predicted next word representation given denotes column word representation context matrices. given predicted next word representation factor outputs component-wise product. conditional probability given written denotes column corresponding word contrast log-bilinear model matrix word representations replaced factored tensor derived. compared multiplicative model additive variant found large datasets captioned photo dataset multiplicative variant signiﬁcantly outperforms additive counterpart. thus sc-nlm derived multiplicative variant. describe structure-content neural language model. suppose that along description also given sequence word-speciﬁc structure variables tn}. throughout experiments corresponds part-of-speech word although possibilities used instead. given embedding goal model distribution previous word context forward structure context tnn+k forward context size. figure gives illustration model prediction problem. intuitively structure variables help guide model generation phrase thought soft template help avoid model generating grammatical nonsense. note model shares resemblance nnjm machine translation previous word context predicted words target language forward context words source language. model interpreted multiplicative neural language model attribute vector longer instead additive function structure variables tn+k} embedding vectors structure variables obtained learned lookup table words are. introduce sequence structure context matrices play role word context matrices denote context matrix multimodal vector attribute vector combined structure content information computed max{· relu non-linearity bias vector. vector plays role vector multiplicative model previously described remainder model remains unchanged. experiments factors sc-nlm trained large collection image descriptions several choices available representing conditioning vectors choice would embedding corresponding image. alternative choice approach take condition embedding vector description computed lstm. advantage approach sc-nlm trained purely text alone. allows make large amounts monolingual text improve quality language model. since embedding vectors share joint space image embeddings also condition sc-nlm image embeddings model trained. signiﬁcant advantage conditional language model explicitly requires image-caption pairs training highlights strength multimodal encoding space. space limitations leave full details caption generation procedure supplementary material. table flickrk experiments. recallk median rank best results overall bold best results without oxfordnet features underlined. infront method indicates object detections used along single frame features. main quantitative results establish effectiveness using lstm sentence encoder ranking image descriptions. perform experimental procedure done flickrk flickrk datasets. datasets come images respectively image annotated using sentences independent annotators. explicit text preprocessing. used convolutional network architectures toronto convnet well -layer extracting dimensional image features oxfordnet ﬁnished place ilsvrc classiﬁcation competition. following protocol images used validation testing rest used training. evaluation performed using recallk namely mean number images correct caption ranked within top-k retrieved results also report median rank closest ground truth result ranked list. compare results following methods devise. deep visual semantic embedding model proposed performing zeroshot object recognition used baseline model sentences represented mean word embeddings objective function optimized matches ours. sdt-rnn. semantic dependency tree recursive neural network used learn sentence representations embedding joint image-sentence space. objective used. defrag. deep fragment embeddings proposed alternative embedding full-frame image features take advantage object detections r-cnn detector. descriptions represented dependency parses. objective incorporates global fragment objectives global objective matches ours. m-rnn. multimodal recurrent neural network recently proposed method uses perplexity bridge modalities ﬁrst introduced unlike methods m-rnn ranking loss instead optimizes log-likelihood predicting next word sequence conditioned image. lstms layer units weights initialized uniformly margin found performed well datasets. training done using stochastic gradient descent initial learning rate exponentially decreased. used minibatch sizes flickrk flickrk. momentum used. hyperparameters used oxfordnet experiments. tables illustrate results flickrk flickrk respectively. performance model comparable m-rnn. metrics outperform match existing results others m-rnn outperforms model. m-rnn learn explicit embedding images sentences relies perplexity means retrieval. methods table flickrk experiments. recallk median rank best results overall bold best results without oxfordnet features underlined. infront method indicates object detections used along single frame features. learn explicit embedding spaces signiﬁcant speed advantage perplexity-based retrieval methods since retrieval easily done single matrix multiply stored embedding vectors dataset query vector. thus explicit embedding methods much better suited scaling large datasets. perhaps interestingly fact method m-rnn outperform existing models integrate object detections. contradictory recurrent networks worst performing models. highlights effectiveness lstm cells encoding dependencies across descriptions learning meaningful distributed sentence representations. integrating object detections framework almost surely improve performance well allow interpretable retrievals case defrag. using image features oxfordnet model results signiﬁcant performance boost across metrics giving state-of-the-art numbers evaluation tasks. word embeddings learned skip-gram neural language models shown exhibit linguistic regularities allow models perform analogical reasoning. instance \"man\" \"woman\" \"king\" answered ﬁnding closest vector \"king\" \"man\" \"woman\". natural question whether multimodal vector spaces exhibit phenomenon. would *image blue car* \"blue\" \"red\" near images cars? word vectors sentence vector using example above vblue vred vcar denote word embeddings blue respectively. ibcar ircar denote embeddings images blue cars. training linear encoder model property vblue vcar ibcar vred vcar ircar. follows supplementary material contains qualitative evidence holds several types regularities images. examples consider retrieving top- nearest images. occasionally observed poor result would obtained within top- among good results. found simple strategy removing cases ﬁrst retrieve nearest images re-sort based distance mean images. worth noting kinds regularities well observed lstm encoder since sentences longer words. linear encoder roughly equivalent devise baselines tables perform signiﬁcantly worse retrieval lstm encoder. regularities interesting learned multimodal vector space well ranking sentences images. generated image descriptions roughly images captioned photo dataset images used display results current state-of-the-art composition based approach treetalk lstm encoder sc-nlm decoder trained concatenating flickrk dataset recently released microsoft coco dataset combined give images descriptions training. dataset contains million images single description used training model. dataset larger annotated descriptions noisier personalized. generated results found http//www.cs.toronto.edu/~rkiros/lstm_ scnlm.html image show original caption nearest neighbour sentence training top- generated samples model best generated result treetalk. nearest neighbour sentence displayed demonstrate model simply learned copy training data. generated descriptions arguably nicest ones date. generating description often case small region relevant given time. developing attention-based model jointly learns align parts captions images alignments determine attend next thus dynamically modifying vectors used conditioning decoder. also plan experimenting lstm decoders well deep bidirectional lstm encoders. would like thank nitish srivastava assistance convnet package well preparing oxford convolutional network. also thank anonymous reviewers nips deep learning workshop comments suggestions. kyunghyun bart merrienboer caglar gulcehre fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. emnlp yunchao gong liwei wang micah hodosh julia hockenmaier svetlana lazebnik. improving image-sentence embeddings using large weakly annotated photo collections. eccv. girish kulkarni visruth premraj sagnik dhar siming yejin choi alexander berg tamara berg. baby talk understanding generating simple image descriptions. cvpr farhadi mohsen hejrati mohammad amin sadeghi peter young cyrus rashtchian julia hockenmaier david forsyth. every picture tells story generating sentences images. eccv. margaret mitchell xufeng jesse dodge alyssa mensch amit goyal alex berg kota yamaguchi tamara berg karl stratos daumé iii. midge generating image descriptions computer vision detections. eacl alex graves marcus liwicki santiago fernández roman bertolami horst bunke jürgen schmidhuber. novel connectionist system unconstrained handwriting recognition. tpami jacob devlin rabih zbib zhongqiang huang thomas lamar richard schwartz john makhoul. fast robust neural network joint models statistical machine translation. peter young alice micah hodosh julia hockenmaier. image descriptions visual denotations similarity metrics semantic inference event descriptions. tacl tsung-yi michael maire serge belongie james hays pietro perona deva ramanan piotr dollár lawrence zitnick. microsoft coco common objects context. arxiv preprint arxiv. figure illustrates sample results using model trained dataset. queries downloaded online retrieved images images used training. interest note resulting images depend highly image used query. example searching word ‘night’ retrieves arbitrary images taken night. hand image building predominantly focus return night images ‘day’ subtracted ‘night’ added. similar phenomenon occurs example cats bowls boxes. additional visualizations computed projections cars corresponding colors well images weather occurrences figure results give strong evidence regularities apparent multimodal vector spaces trained linear encoders. course sensible results likely obtained content image correctly recognized subtraction word relevant image image exists sensible corresponding query. sc-nlm trained concatenation training sentences flickrk microsoft coco. given image ﬁrst multimodal space. embedding deﬁne sets candidate conditioning vectors sc-nlm image embedding. embedded image itself. note sc-nlm trained images conditioned images since embedding space multimodal. top-n nearest words sentences. ﬁrst computing image embedding obtain top-n nearest neighbour words training sentences using cosine similarity. retrievals treated ‘bag concepts’ compute embedding vector mean concept. results along candidate conditioning vectors also compute candidate sequences used sc-nlm. this obtain sequences training whose lengths inclusive. captions generated ﬁrst sampling conditioning vector next sampling sequence computing estimate sc-nlm. generate large list candidate descriptions rank candidates using scoring function. scoring function consists feature functions translation model. candidate description embedded multimodal space using lstm. compute translation score cosine similarity image embedding embedding candidate description. scores relevant content candidate image. also augment score multiplicative penalty non-stopwords appear frequently description. language model. trained kneser-ney trigram model large corpus compute logprobability candidate model. scores reasonable english sentence candidate. total score caption weighted translation language models. challenge quantitatively evaluating generated descriptions tuned weights hand qualitative results alone. candidate descriptions ranked scores top- captions returned.", "year": 2014}