{"title": "Latent Tree Models for Hierarchical Topic Detection", "tag": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "abstract": "We present a novel method for hierarchical topic detection where topics are obtained by clustering documents in multiple ways. Specifically, we model document collections using a class of graphical models called hierarchical latent tree models (HLTMs). The variables at the bottom level of an HLTM are observed binary variables that represent the presence/absence of words in a document. The variables at other levels are binary latent variables, with those at the lowest latent level representing word co-occurrence patterns and those at higher levels representing co-occurrence of patterns at the level below. Each latent variable gives a soft partition of the documents, and document clusters in the partitions are interpreted as topics. Latent variables at high levels of the hierarchy capture long-range word co-occurrence patterns and hence give thematically more general topics, while those at low levels of the hierarchy capture short-range word co-occurrence patterns and give thematically more specific topics. Unlike LDA-based topic models, HLTMs do not refer to a document generation process and use word variables instead of token variables. They use a tree structure to model the relationships between topics and words, which is conducive to the discovery of meaningful topics and topic hierarchies.", "text": "allocation assumes documents generated follows first list topics drawn dirichlet distribution. then document topic distribution drawn another dirichlet distribution. topic distribution selecting word according word distribution βzdn topic. given document collection generation process reverted statistical inference determine topics authors themselves. stated review form paper novel results describes previously published authors previously published authors archival journal. maximum likelihood estimate model parameters number free model parameters sample size. maximizing score intuitively means model data well overly model model highest score among ltms latent variables. figure shows models might look like consists four word variables nasa spaceshuttle mission. conclude uni-dimensional following inequality holds strong evidence favor here stands natural logarithm. well known score large sample approximation marginal difference large approximation logarithm bayes factor according cut-off values bayes factor conclude subroutine oneisland maintains working observed variables. initially consists pair variables highest referred seed variables island. variable highest variables added third variable variables added one. step pick variable algorithm usually used practice. starts initial guess parameter values produces sequence estimates θ··· given current estimate next estimate obtained e-step three observed variables. done using next enters loop. beginning variables. parameters estimated earlier lines oneisland ﬁnds variable outside maximum variable inside maximum {nasa space shuttle mission moon}. variable added lunar model adding lunar shown left figure distribution estimated distributions already estimated. pem-lcm estimates distribution running variable lunar added island figure models created. need estimate model left model right. estimation done running parts models variable names rectangles. latent variables must estimated. progressive task also. consider model figure estimate form sub-model picking children instance nasa space children instance orbit earth. estimate distribution running submodel parameters ﬁxed. reduce complexity ncn). large small fraction hence achieve substantial computational savings. meantime still expect obtain good structure small. reason model construction relies salient regularities data regularities preserved subset small. convergence parameter estimation process second phase task improve values parameters {θijk} obtained batch updates model construction phase. standard a.k.a. suppose data randomly divided equal-sized minibatches stepwise updates parameters processing minibatch. maintains collection auxiliary variables nijk initialized experiments. suppose parameters updated times current values {θijk}. next minibatch process. stepwise carries updating follows note equation similar except statistics calculated minibatch rather entire dataset parameter known stepsize given parameter chosen range experiments parameter estimation phase. second mode denoted hlta-step subset randomly sampled data cases used model construction phase stepwise used parameter estimation phase experiments size minibatch parameter stepwise hlta-batch datasets hlta-step", "year": 2016}