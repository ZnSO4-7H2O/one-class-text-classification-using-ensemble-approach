{"title": "Learning to Play Guess Who? and Inventing a Grounded Language as a  Consequence", "tag": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "abstract": "Acquiring your first language is an incredible feat and not easily duplicated. Learning to communicate using nothing but a few pictureless books, a corpus, would likely be impossible even for humans. Nevertheless, this is the dominating approach in most natural language processing today. As an alternative, we propose the use of situated interactions between agents as a driving force for communication, and the framework of Deep Recurrent Q-Networks for evolving a shared language grounded in the provided environment. We task the agents with interactive image search in the form of the game Guess Who?. The images from the game provide a non trivial environment for the agents to discuss and a natural grounding for the concepts they decide to encode in their communication. Our experiments show that the agents learn not only to encode physical concepts in their words, i.e. grounding, but also that the agents learn to hold a multi-step dialogue remembering the state of the dialogue from step to step.", "text": "acquiring ﬁrst language incredible feat easily duplicated. learning communicate using nothing pictureless books corpus would likely impossible even humans. nevertheless dominating approach natural language processing today. alternative propose situated interactions agents driving force communication framework deep recurrent q-networks evolving shared language grounded provided environment. task agents interactive image search form game guess who?. images game provide trivial environment agents discuss natural grounding concepts decide encode communication. experiments show agents learn encode physical concepts words i.e. grounding also agents learn hold multi-step dialogue remembering state dialogue step step. description colour never seen poor substitute seeing language arose transmitting knowledge state world people live evolves world changes time. severing link analysing text static standalone artifact leads problems grounding concepts eﬀectively eliminates exploratory mapping language. contrast humans communicate generally connection environment interactively directions provides necessary grounding concepts also immediate feedback every utterance. importance feedback human language learners shown sachs paper describes case hearing child brought deaf parents learn speak watching television without supervision feedback. circumstances severely delayed acquisition language learn speak properly intervention outside. indicates even humans learn master human language without help guided exploration synthesis feedback i.e. social interactions. paper investigate grounded language emerge letting agents invent language solve shared problem using deep recurrent q-networks precisely agents play game guess who? forces agents come grounded words represent characteristics objects images order game. recent success deep reinforcement learning shown complex tasks huge state spaces learnt using reinforcement learning task multi-agent communication. diﬃcult task states pace includes possible conversation states stationary environment continually change agents learn. paper sukhbaatar shown agents learn continuous communication using later used solve tasks require synchronisation global communication channel. though promising continuous nature communication makes diﬀerent natural language words discrete symbols. deﬁcit rectiﬁed foerster using diﬀerentiable inter-agent learning method developed solve puzzles multi-agent setting agents allowed communicate sending one-bit messages. though communication still continuous training enable gradient propagation regularise model promote discrete solutions discretize testing. model employ paper similar dial diﬀers areas. instead communicating using bits generalise model handle orthogonal messages arbitrary dimension enable vocabularies arbitrary size gradually increasing noise communication channel ensure agents learn symbolic language less negative impact convergence rate training positive impact learning capacity model parameters shared agents since reasonable human perspective. ﬁrst step towards grounding taken letting agents play game involving images. main diﬀerence paper consist learn representations images lazaridou pretrained classiﬁer trained recognise categories objects present images able train agents hold multi-step dialogue contrast step symbol question. end-to-end trainable multiple-agent reinforcement learning model learns near optimal strategy playing guess who? without sharing parameters agents predeﬁned communications protocol. experiments show increasing levels noise communication channel leads improved training speed learning capacity retaining ability learn discrete symbols. finally generalise dial orthogonal messages arbitrary dimension closely resemble human language encompasses hundreds thousands words show improves performance system well makes interpretable. results paper rely mainly theory reinforcement learning deep q-networks concept diﬀerentiable inter-agent learning introduce topics brieﬂy below. traditional single-agent reinforcement learning setting agent observes current state time step takes action according policy receives reward transitions according probability distribution depending current state action state value function approaches seek estimate value diﬀerent policies typically expected return order select good one. return policy γt−τ discount factor trades-oﬀ importance immediate future rewards. speciﬁc policy value state-action pair deﬁned optimal value function maxπ called q-value state-action pair obeys bellman optimality equation agent employs optimal strategy guaranteed achieve highest expected discounted return. reinforcement learning general extended cooperative multi-agent settings agent observes global state selects individual actions receives team reward shared among agents. agents partially observe environment global state hidden agents receive observations correlated state case gradient term error backpropagated message recipient sender. algorithm details found supplementary materials using dial instead traditional independent q-learning techniques shown beneﬁcial terms learning speed terms ability learn. task considered paper version popular guessing game guess who?. guess who? players assigned character characters. character represented image goal game deduce characters player has. players take turns asking questions based visual appearance characters. questions answered truthfully player knows character player assigned. version guess who? consider paper slight variation traditional game. important diﬀerence remove competitive element agent asking-agent answering-agent means agent specialises asking questions answering them collaborate objective solving task. another diﬀerence instead ﬁnding correct character amongst full characters subset sampled given observation input asking agent; experiments used subset four images. answering agent observes correct image taken subset observed asking-agent. figure example version guess who? illustrated. asking-agent four images answering-agent image third image asking-agent. asking-agent sends ﬁrst question receives answer another round question answer instead playing game variable amount time steps ﬁxed space state-action pairs many applications large storing updating values state-action pair computationally intractable. solution dimensionality problem employ concept deep-q-networks idea represent q-function using neural network parameterised i.e. approximates value stateaction pairs. network optimised minimising itloss function eration ydqn parameters target network ﬁxed number iterations. actions chosen training network determined \u0001greedy policy selects action maximises q-value current state probability chooses action randomly probability agents partial observability hausknecht stone propose approach called deep recurrent q-networks where instead approximating q-values feed-forward network approximate q-values recurrent neural network maintain internal state aggregates observations time. modelled adding input network represents hidden state network. foerster introduce idea centralised training decentralised execution i.e. agents trained together evaluated separately. introduce concept diﬀerentiable inter-agent learning messages allowed continuous training need discrete evaluation allows gradients propagate agents messages training. gives agents feedback thus reduces amount learning required trial-and-error. reduce discretisation error could occur discretisation messages evaluation phase messages processed dicretise/regularise unit centralised learning regularises messages according message agent logistic. however since agents recurrent networks possible questions round diﬀerent meanings partitioned further implying maximum average reward higher order evaluate proposed framework larger images realistic images also celeba dataset includes face images unique celebrity identities. images dataset cover large pose variations background clutter. examples images seen table architecture model presented section schematic illustration model shown figure detailed overview agent shown figure agent consists recurrent neural network rolled steps. agents input network -layer gated recurrent unit output network. input network takes incoming information images agent’s previous action embeds separately embeddings using multi-layer perceptrons adding embeddings element-wise create joint embedding passed -layer incoming state generating states state layer output embedding output embedding passed output network used generate action generating passed using \u0001-greedy policy. message variant described section generate one-hot encoding using =dru episode)) training case softmax dimensional normal distribution. argmax evaluation otherwise} used instead. parameters creating embeddings generating output number steps played. case askingagent observes images agent allowed question receive answer answers allowed. answers received asking-agent guess images answering-agent has. guess considered action taken asking-agent represented number images agent holds. equal index character asking-agent reward given agents otherwise receive number question-answer rounds limited always possible game since ﬁxed number words available answers permitted. question selected words possible partition images four parts. four words partitioned ways. fewer partitions size images means several images partition giving answers questions posed therefore making indistinguishable model. implies maximum average reward game asking-agent holds images total classes diﬀerent words allowed equivalent score four image game rounds questions model utilise messages one-hot encoding passed agent next time step. using one-hot encoding disadvantages terms scalability found gave improved results makes easier study underlying eﬀect behind message clouded varying closeness messages occurs binary encoding. batch normalization performed image embedding regularised incoming messages testing non-stochastic versions batch normalization used implies running averages values observed training used instead batch. inspired curriculum learning proposed bengio showed gradually increasing diﬃculty tasks leads improved learning introduce usage increasing noise dial. found using ﬁxed value noise unwelcome trade large risk model never learning anything. small training error excellent testing error model over-encoding information messages leading large discretisation error. solution allow noise linearly increase epochs. enables model learn quickly beginning punish over-encoding information training progresses. compared human speech person beginning learn language requires almost noise free surrounding understand message proﬁcient speaker would problem understanding message noisy environment listening radio conversation busy road. validate ideas series experiments version guess who? well performs also model learns. further evaluate eﬀectiveness increasing noise comparing experiments non-increasing noise. episode randomly select four different images pool guess who? images model trained increasing linearly evaluate model performance four eight diﬀerent words available asking agent onehot embedding answers yes/no available answering agent. game goes three time steps images time steps four images. experiment also replicated celeba dataset creating pool images study case four images eight sixteen diﬀerent words. finally sample messages agents understand properties communication. observe agents adapt messages history communication creating conversation rather individual messages. also want meaning words constant along conversation dependent context. also figure performance model guess who? images asking-agent images vocabulary four eight sixteen diﬀerent words round question-answer performed. results averaged runs. dashed grey lines represents baseline performance asking-agent guesses randomly. performance model ordered highest score largest vocabulary lowest score smallest vocabulary. figure performance model guess who? images asking-agent four images vocabulary four eight diﬀerent words rounds question-answer performed. results averages runs. dashed grey lines represents baseline performance asking-agent guesses randomly. performance model ordered highest score largest vocabulary lowest score smallest vocabulary. compare model model incoming state answering agent gets zeroed removing possibility depending eﬀects depend history conversation aﬀects performance model. experiments \u0001-greedy policy discount factor epoch performs parallel episodes using batch size target network updated every epochs. unless stated otherwise increase linearly keeps average close found work task similar tasks used optimization done rmsprop learning rate experiment multiple times results averaged. code experiments available online github.com/emiliojorge/inventinga-grounded-language. performance four images averaged runs seen figure seen performance obtained four images quite similar independently many questions available. results experiments celeba dataset seen figure shows performance similar even images large. indicates performance model dependent networks memorising images rather generalises combinations images unlikely seen before. idea also supported model trained images tested remaining obtaining similar scores evaluated diﬀerent data. better understand nature questions asking-agent asks sample episodes model images available questions games play i.e. message protocol agents depending images see. message protocol illustrated table table seen asking-agents table learned message protocols askingagent answering-agent depending images agents see. vocabulary size asking agent images allowed question/answer figure performance model images celeba dataset asking-agent four images rounds question-answer performed vocabulary eight sixteen words available. dashed grey lines represents baseline performance asking-agent guesses randomly. questions depend images answers gets answering-agent interpretation answer cases receives answer even answering-agent diﬀerent images. leads answering-agent sometimes making guess wrong image receiving zero reward. answers question diﬀerent images illustrated figure shows images trouble separating answers words impossible asking-agent distinguish two. illustrates problem limited vocabulary leads able partition ways. judging appearances images seems like word could interpreted something along lines his/her head light coloured darkly coloured?. division shown figure reproduced twelve times. often images answers questions used distance measure describe relation images. using distance measure project images two-dimensional space using t-distributed stochastic neighbor embedding seen figure projection also shows similar images appear similar answers questions large degree. comparison seen figure highlights beneﬁts interactive communication. answering agent memory previous communication allows communication create homographs. homographs words spelt diﬀerent meaning depending context. example homograph word rock refer style music also stone. since agents memory conversation grus allows agents adapt meaning allow words diﬀerent signiﬁcance second round question answers depending history communication. since multiple step communication agents using grus possible agents create interactive conversation messages sent depend internal state agent. allows agents adapt messages send depending history communication greater variety communication possible. studying message asking agents sends second question model trained celeba vocabulary words keeping everything else second questions diﬀers situations depending answer ﬁrst question was. shows agents create kind conversation adapting agent instead using sending messages independently. also makes sense instead limiting possible question pairs created ability form homographs instead becomes order evaluate eﬀectiveness increasing noise training compare results guess who? using four images available words increasing constant also figure clearly visible signiﬁcant advantage using variable noise compared constant noise. model learns faster achieves better performance compared models trained constant noise. supports idea communication grounded visual aspects images. paper shown drqn learn play guess who?. further careful analysis results could establish grounding connection between words used agents characteristics figure t-sne embedding images using answers questions distance measure. blue lines indicate correct position images shifted reduce visual cluttering. figure comparison performance model without incoming state answering agent. comparison performed images guess who? dataset vocabulary eight words subset four images question/answers. dashed grey lines represents baseline performance asking-agent guesses randomly. results average performance runs. foerster jakob assael yannis freitas nando whiteson shimon. learning communicate deep multi-agent reinforcement learning. advances neural information processing systems foerster jakob assael yannis freitas nando whiteson shimon. learning communicate solve riddles deep distributed recurrent q-networks. corr abs/. ioﬀe sergey szegedy christian. batch normalization accelerating deep network training reducing internal covariate shift. proceedings international conference machine learning icml mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg petersen stig beattie charles sadik amir antonoglou ioannis king helen kumaran dharshan wierstra daan legg shane hassabis demis. human-level control deep reinforcement learning. nature sachs jacqueline bard barbara johnson marie language learning restricted input case studies hearing children deaf parents. applied psycholinguistics figure performance model diﬀerent noise levels varying linearly dashed grey line represents baseline performance asking-agent guesses randomly. results averaged twelve runs increasing runs constant performance model increasing outperforms models constant objects images game. also show agents create interactive conversation communication adapts previously said communication. finally showed regulating level noise communications channel could large impact training speed well ﬁnal performance system. authors would like acknowledge project towards knowledge-based culturomics supported framework grant swedish research council authors would also like thank jakob foerster yannis assael insightful discussions. references bengio yoshua louradour jérôme collobert ronan weston jason. curriculum learning. proceedings annual international conference machine learning icml kyunghyun merrienboer bart gülçehre çaglar bahdanau dzmitry bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoder-decoder statistical machine translation. proceedings sukhbaatar sainbayar szlam arthur fergus rob. learning multiagent communication backpropagation. sugiyama luxburg guyon garnett advances neural information processing systems trying correctly diﬀerentiate images pool images using available question probability guessing correct image seen following image correct image incorrect image pool. images separable images partition images divided questions. since partition divides equally optimal means images divided images partition inseparable asking agent perform random guess. equation gives case four images slightly diﬀerent. following calculations assume questions asked questions available questions available ﬁrst question phase second phase. means optimal procedure ﬁrst phase second phase. case calculations take consideration multiple images correct partition. gives", "year": 2016}