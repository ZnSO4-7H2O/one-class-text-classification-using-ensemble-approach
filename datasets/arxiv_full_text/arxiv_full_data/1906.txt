{"title": "Sherlock: Scalable Fact Learning in Images", "tag": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "We study scalable and uniform understanding of facts in images. Existing visual recognition systems are typically modeled differently for each fact type such as objects, actions, and interactions. We propose a setting where all these facts can be modeled simultaneously with a capacity to understand unbounded number of facts in a structured way. The training data comes as structured facts in images, including (1) objects (e.g., $<$boy$>$), (2) attributes (e.g., $<$boy, tall$>$), (3) actions (e.g., $<$boy, playing$>$), and (4) interactions (e.g., $<$boy, riding, a horse $>$). Each fact has a semantic language view (e.g., $<$ boy, playing$>$) and a visual view (an image with this fact). We show that learning visual facts in a structured way enables not only a uniform but also generalizable visual understanding. We propose and investigate recent and strong approaches from the multiview learning literature and also introduce two learning representation models as potential baselines. We applied the investigated methods on several datasets that we augmented with structured facts and a large scale dataset of more than 202,000 facts and 814,000 images. Our experiments show the advantage of relating facts by the structure by the proposed models compared to the designed baselines on bidirectional fact retrieval.", "text": "abstract. study scalable uniform understanding facts images. existing visual recognition systems typically modeled differently fact type objects actions interactions. propose setting facts modeled simultaneously capacity understand unbounded number facts structured way. training data comes structured facts images including objects attributes actions interactions fact semantic language view visual view show learning visual facts structured enables uniform also generalizable visual understanding. propose investigate recent strong approaches multiview learning literature also introduce learning representation models potential baselines. applied investigated methods several datasets augmented structured facts large scale dataset facts images. experiments show advantage relating facts structure proposed models compared designed baselines bidirectional fact retrieval. despite recent signiﬁcant advances recognition image captioning visual question answering still large humans machines deep image understanding objects attributes actions interactions another. human visual system able efﬁciently gain visual knowledge learning different types facts never ending many examples aided ability generalize known facts related structure. believe effective fastest close methods possess following characteristics existing visual understanding systems categorized trends factlevel systems high-level systems. fact level systems include object recognition action recognition attribute recognition interaction recognition systems usually evaluated separately fact type therefore uniform. typically systems ﬁxed dictionary facts assuming facts seen training least tens examples treat facts independently. methods cangeneralize learn facts outside dictionary scale unbounded number facts since model size scales number facts. furthermore recognition systems typically uni-directional able return conditional probability fact given image. zero/few-shot learning setting even zero examples fact available typically studied apart traditional recognition setting. aware uniﬁed recognition/few shot learning system learns unbounded facts. image-caption similarity visual question answering promising results. systems typically learning high-level tasks evaluation answer whether systems relate captions questions images fact-level understanding. captioning models output sentences thus mention different types facts principle fact. however devlin reported generated captions lstm-based captioning methods actually exist training data show nearest neighbor methods competitive performance captioning. results call question core understanding generalization capabilities state-of-the-art caption-level systems. limitations prior settings motivated study fact-level understanding setting related ﬁrst trend uniﬁed fact type able learn unbounded number facts. setting allows measuring gained visual knowledge represented facts learnt proposed system solve task. goal method achieves sophisticated understanding objects actions attributes interactions objects possesses desireable properties scalability generalization uniformity bi-directionality structure. approach learn common embedding space language visual views fact mapped location. solution achieving desireable characteristics make basic unit understanding structured fact shown fig. structured embedding space different dimensions record information subject predicate object fact. using embedding space approach allows method scale submit facts train embedding network. test time allows bi-directional retrieval search language facts embed near given image fact vice-versa. retaining structure fact embedding space gives method chance generalize understand s/sp/spo training data components since information kept separate. obtain uniformity introduce wildcards structured fact representation e.g. <mansmiling*> <dog**> wildcard training loss ignores unspeciﬁed components embedded second ﬁrst order visual language facts. carefully designed experiments show uniform method achieves state-of-the-art performance fact-level bidirectional view retrieval existing image-sentence correlation methods view embedding methods version method without structure also scaling generalizing better. contributions propose problem setting study fact-level visual understanding unbounded number facts considering aforementioned characteristics. design investigate several baselines multiview learning literature apply task. propose learning representation models relate different fact types using structure exempliﬁed designed baselines proposed models embed language views visual views facts joint space allows uniform representation different fact types. show value relating facts structure proposed models compared designed baselines several datasets bi-directional fact retrieval. related work order make contrast related work clear start stating scale facts modeling work. let’s assume denotes number unique subjects unique predicates unique objects respectively; scale unique second third order facts bounded |s|×|p| |s|× |p|×|o| possibilities respectively easily reach millions facts. data collected work thus reached unique facts cover lines related research modeling visual facts discrete space recognition objects activities typically modeled mapping function discrete classes. function recently learned using deep learning different systems built recognize fact type images modeling different constrained objects attributes attributed objects scenes human actions interactions several limitations modeling recognition scalability adding fact leads changing architecture meaning adding thousands parameters re-training model example vggnet used scale facts number parameters softmax layer alone close billion. uniformity modeling group facts different requires maintaining different systems retrain several models facts added also doesn’t allow learning correlation among different fact types. however uniformally model visual perception. generalization existing benchmarks setting least tens examples fact realistic assumption might enough examples learn class several works proposed deal problem object recognition settings however suffer aforementioned scalability problems facts increase. bi-directionality modv uni-directional shows representatives settings methods. three axes scalability uniformity generalization. methods typically study seen classes hence generalize unseen classes. fig. setting contrast studied fact recognition settings literature. scalability means number facts studied works. uniformity means setting applied multiple fact types. generalization means performance methods facts zero/few images. modeling zero/few shot fact learning semantic representation classes successful ideas learning examples class using semantic output codes like attributes intermediate layer between features classes. formally composition function main idea collect data sufﬁcient learn intermediate attribute layer classes represented attributes facilitate zero-shot/few-shot learning. however chen realized attribute appearance dependent class opposed earlier models although assumption realistic propose learning different classiﬁers category-attribute pair suffers scalability learning problems pointed restricted certain groups facts recent attribute-based zero-shot learning methods embed images attributes shared space eszsl methods mainly studied case zero-shot learning shown strong performance. contrast studying setting system learn facts many training images facts few/no training images. shows contrast setting setting. although methods mainly studied using attributes semantic representation much smaller scale facts apply state eszsl order study capacity models much larger scale. object recognition continuous space using vision language recent works language vision involve using unannotated text improve object recognition facilitate zero-shot learning. following group approaches model object recognition function maxy similarity function image class represented text. word embedding language models adopted represent class names vectors. setting imagenet dataset object facts thousands examples class. setting orders magnitude facts long-tail distribution. conversely works model mapping unstructured text descriptions classes visual classiﬁer extending visual recognition task unbounded scale facts object recognition also attributes actions interactions model; contrast setting. image-caption similarity methods illustrated earlier goal factlevel understanding. however image-caption similarity methods relevant multi-view learning methods. although different setting found interesting aspects methods study setting. first image-caption similarity system trained image-caption level performs fact-level understanding. second systems could retrained setting providing factlevel annotation every example phrase representing fact image problem deﬁnition representation visual modiﬁers deal three groups facts; fig. first order facts <s**> object scene categories second order facts <sp*> objects performing actions attributed objects third order facts <spo> interactions positional information allowing wild-cards structured representation allow uniform representation different fact types also relate structure. propose model facts embedding structured fact space three continuous hyper-dimensions space object categories scenes space actions interactions attributes positional relations. space interacting objects scenes interact facts. dimenstionalities corresponding respectively. shown fig. ﬁrst order facts like <woman**> <man**> <person**> live hyper-plane space. second order facts live hyper-line parallel axis. finally third order fact like <man walking dog> point visual perception space. inspired concept language modiﬁers could viewed call visual modiﬁers. example second order fact <baby smiling* visual modiﬁer <baby**> third order fact <person playing ﬂute> fact <person visually modiﬁed axes. embedding language images common space algorithm scale efﬁciently. further space used retrieve language view image well visual view language description making model bi-directional. argue modeling visual recognition based notion gives generalization capability. example model learned facts <boy> <girl> <boy petting dog> <girl riding horse> would recognizing unseen fact <boy petting horse>. show capabilities quantitatively later experiments. model setting problem views visual domain language domain structured fact denoting visual view denoting language view instance annotated fact language view =<sgirl priding obike> would corresponding visual view image fact occurs; fig. denote embedding functions visual view structured visual embeddings fact language view language embeddings fact denote concatenation visual view hyper-dimensions’ embedding language view hyper-dimensions’ embedding third-order facts <spo> directly embedded structured fact space ×rdp ×rdo image view language view. based fact modiﬁer observation propose represent second ﬁrst-order facts wild cards illustrated setting ﬁrst-order facts means modiﬁers interest ﬁrst-order facts intuitive. similarly setting secondorder facts indicates modiﬁer interest single-frame actions attributed objects. image contains lower order fact <man> higher order facts <man tall> <man walking dog> also present. hence wild cards ﬁrstsecond-order facts penalized training. propose two-view structured fact embedding model properties mentioned satisfying ﬁrst four properties achieved using generative model connects visual language views importantly inhabit continuous space. model similarity function deﬁned structured fact space. satisfy ﬁfth property building models aforementioned structured wild card representation. objective views fact embedded close other; question model train visual functions model encoder encoder recent success encoders images words respectively. propose models learning facts denoted model model models share structured fact language embedding/encoder differ structured fact image encoder. start deﬁning activation operator input series neural network layers operator applies parameters layer layer compute ﬁnal activation using subnetwork. model model structured fact visually encoded sharing convolutional layer parameters fully connected layer parameters fig. transformation matrices applied produce deﬁne model contrast model different convolutional layers inspired idea modiﬁers starting common convolutional network splits branches producing sets layers denoted followed sets fully connected layers convolutional layers output column loss function model model model assume exp) minimize distance lossw deﬁned distance function. thus minimize distance embeddings visual view language view. solution penalize wild-card facts ignore wild-card modiﬁers loss. <spo> facts facts. hence lossw penalize modiﬁer second-order facts modiﬁers ﬁrst-order facts follows deﬁnition wild-cards. paper used standard euclidean distance. testing training model embed testing learnt models similarly embed test shown language view retrieval compute distance structured embedding image facts structured language embeddings indicates relevance fact given image. visual view retrieval compute distance structured embedding given fact structured visual embedding images test set. ﬁrst second order facts wild-card part ignored computing distance. experiments data collection structured facts order train model connects structured fact language view visual view need collect large scale data form pairs. large scale data collection challenging setting since relies localized association structured language fact image facts occur. began data collection augmenting existing datasets fact language view labels ppmi stanford pascal actions sports visual phrases interact datasets. union datasets resulted facts images broken table also extracted structured facts scene graph dataset manually annotated images graph structure ﬁrst- second- third-order relationships extracted. extracted second-order facts third-order facts. majority positional relationships. also added aforementioned data second third order fact annotation collected mscoco flickrk entities datasets using language approach detailed supplementary. show later section data perform several experiments varying scale validate claims. table shows unique facts large scale dataset. model model constructed vgg- built layer conv__ pool following fully connected layers vgg- similar model initialized randomly rest parameters initialized vgg- trained imagenet match architecture convolutional layers pooling layer vgg- named conv__ pool seven convolution layers. subject layers branches convolution pooling layers architecture vgg- layers named conv__ pool layer makes convolution-pooling layers branch. finally instances layers vgg- network. initialized randomly rest initialized vgg- trained imagenet. multiview ijcv expects features views. visual view features used language view features used glove. since support wild-cards wild-card parts zeros first order second order facts. image-sentence similarity used theano implementations method made publically available authors purpose applying coco pretrained image-caption models show image-caption trained models perform applied fact level recognition setting. order models measure simility image facts setting provide image phrase constructed fact language representation. example <person riding horse converted person riding horse. image-sentence similarity contrast previous setting retrain models providing image-fact training pairs facts converted phrases. results show value learning models fact level instead caption level. evaluation metrics present evaluation metrics language view retrieval visual view retrieval. metrics visual view retrieval retrieve image given language view measure performance image considered positive pair annotations. even retrieved image relevant pair exist considered incorrect. also variants compute based retrieved images useful evaluating large scale experiments. metrics language view retrieval retrieve fact language views given image. accuracy evaluation. also used metric basically rank correct class. important issue setting might multiple facts image. given correct facts given image achieve performance facts must retrieved facts. accordingly means facts retrieved facts. fact language view considered correct pair annotations. hard aforementioned metrics harsh especially large scale setting. instance correct fact image <smanp jumping> answer <sperson pjumping> receives zero credit. also evaluation limited ground truth fact annotations. might several facts image provided annotations miss facts. qualitatively found metrics harsh large scale experiment. deﬁning better metrics future work. small scale experiments performed experiments several datasets ranging scale stanford pascal actions visual phrases union datasets described earlier table sec. used training test splits deﬁned datasets. union datasets unioned training testing annotations ﬁnal split. training/testing splits fact language view corresponding tens visual views split training test sets. test image belongs fact seen images training set. table shows performance model model designed baselines four datasets view retrieval tasks. note model works relatively better model scale size increases shown comparing results pascal dataset larger datasets like stanford visual phrases next section show model clearly better model large scale setting. intuition behind result model learns different convolutional ﬁlters branch understand action/attributes interactions different ﬁlter bank learned discriminate different subjects branch. contrast model trained optimizing bank ﬁlters altogether might conﬂict optimize together; learning image-caption pairs even dataset like mscoco help discriminate tens facts shown experiments. however retraining models providing image-fact pairs makes perform much better shown table compared methods language view retrieval found model perform signiﬁcantly better tacl even retrained setting especially pascal stanford datasets dominated facts; table visual view retrieval performance competitive datasets. think reason structure makes models relate fact types visual modiﬁers notion. although eszsl applicable setting among worst performing methods table could eszsl mainly designed zero-shot learning fact training examples experiments. interestingly chosen visual language features among best methods. next compare methods number facts becomes three orders magnitudes larger tens thousands testing facts unseen training. large scale experiment experiment used union data described sec. augmented data images coco object ﬁrstorder facts. also used object annotations scene graph dataset ﬁrst-order fact annotations maximum images object. finally randomly split annotations split constructing sets training pairs testing pairs total pairs unique table shows coverage different types facts. language view test facts unseen training majority facts example; supplementary material. qualitative results shown fig. fig. model’s ability generalize seen facts. example leftmost image model able correctly identify image <dog riding wave> despite fact never seen training data. left images fig. show variety images retrieve query <airplane ﬂying>. right images fig. note model learns visually distinguish gender group versus single. also correctly retrieve images facts never seen training highlighting harshness metric fig. also shows <airplane ﬂying> zero value giving zero credit since images annotated airplane>. perform retrieval directions used flann library compute nearest neighbors given vice-versa. details nearest-neighbor database creation large scale evaluation could found supplementary. results table indicate model better model retrieval views consistent medium scale results intuition. model also multiple orders magnitude better chance also signiﬁcantly better competing methods. test value structure experiment averaged parts visual language embedding vectors instead keeping structure. removing structure leads noticeable decrease performance metric; table previous smaller scale experiments orders magnitudes smaller also less challenging since facts seen training. figure shows effect scale performance language view retrieval task observable increase improvement model compared baselines large scale setting. additionally performance image-caption similarity methods degrade substantially. think large scale facts majority facts zero training examples. interestingly among best performing methods large scale setting. however model model outperform metrics; table language view retrieval competitive results notices several good visual retrieval results metric gives zero-credit. figure shows large scale knowledge view retrieval results reported table broken fact type number images fact. results show model generally behaves better compared models increase facts. noticed slight increase model model desirable method able generalize understand interaction training examples involving components even zero training examples exact parts table shows performance spos number training examples fig. performance across different datasets. graphs show advantage proposed models scale increases left right. tacl means retrained version means coco pretrained model; fig. performance versus number images fact left objects right attributed objects objects performing actions bottom left interactions bottom right facts. example column means examples least examples part part. example case zero examples <person petting horse> least examples <person petting something=dog/cat/etc least examples something interacting horse horse>. model performs best listed generalization cases table found similar generalization behavior facts examples training. ﬁgures additional results supplementary materials. introduce setting learning unbounded number facts images could referred model gaining visual knowledge. facts could different types like objects attributes actions interactions. studying task consider uniformity generalization scalability bi-directionality structure. investigated several baselines multi-view learning literature adapted proposed setting. proposed learning representation methods outperform designed baseline mainly advantage relating facts structure.", "year": 2015}