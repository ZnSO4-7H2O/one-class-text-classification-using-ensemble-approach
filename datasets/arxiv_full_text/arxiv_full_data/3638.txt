{"title": "Improving Transferability of Adversarial Examples with Input Diversity", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Though convolutional neural networks have achieved state-of-the-art performance on various vision tasks, they are extremely vulnerable to adversarial examples, which are obtained by adding human-imperceptible perturbations to the original images. Adversarial examples can thus be used as an useful tool to evaluate and select the most robust models in safety-critical applications. However, most of the existing adversarial attacks only achieve relatively low success rates under the challenging black-box setting, where the attackers have no knowledge of the model structure and parameters. To this end, we propose to improve the transferability of adversarial examples by creating diverse input patterns. Instead of only using the original images to generate adversarial examples, our method applies random transformations to the input images at each iteration. Extensive experiments on ImageNet show that the proposed attack method can generate adversarial examples that transfer much better to different networks than existing baselines. To further improve the transferability, we (1) integrate the recently proposed momentum method into the attack process; and (2) attack an ensemble of networks simultaneously. By evaluating our method against top defense submissions and official baselines from NIPS 2017 adversarial competition, this enhanced attack reaches an average success rate of 73.0%, which outperforms the top 1 attack submission in the NIPS competition by a large margin of 6.6%. We hope that our proposed attack strategy can serve as a benchmark for evaluating the robustness of networks to adversaries and the effectiveness of different defense methods in future. The code is public available at https://github.com/cihangxie/DI-2-FGSM.", "text": "abstract. though convolutional neural networks achieved stateof-the-art performance various vision tasks extremely vulnerable adversarial examples obtained adding humanimperceptible perturbations original images. adversarial examples thus used useful tool evaluate select robust models safety-critical applications. however existing adversarial attacks achieve relatively success rates challenging black-box setting attackers knowledge model structure parameters. propose improve transferability adversarial examples creating diverse input patterns. instead using original images generate adversarial examples method applies random transformations input images iteration. extensive experiments imagenet show proposed attack method generate adversarial examples transfer much better diﬀerent networks existing baselines. improve transferability integrate recently proposed momentum method attack process; attack ensemble networks simultaneously. evaluating method defense submissions oﬃcial baselines nips adversarial competition enhanced attack reaches average success rate outperforms attack submission nips competition large margin hope proposed attack strategy serve benchmark evaluating robustness networks adversaries eﬀectiveness diﬀerent defense methods future. code public available https//github.com/cihangxie/di--fgsm. fig. success rates comparisons three attacks four networks. ground-truth walking stick marked pink top- conﬁdence distribution plots. adversarial examples crafted inception-v maximum perturbation ﬁrst shows top- conﬁdence distributions clean image indicates networks make right predictions high conﬁdences. second third rows show top- conﬁdence distributions adversarial examples generated fast gradient sign method iterative fast gradient sign method respectively. adversarial examples successfully attack white-box model inception-v cannot transfer black-box models e.g. inception-resnet-v. fourth shows top- conﬁdence distributions adversarial examples generated proposed attack method diverse inputs iterative fast gradient sign method attacks white-box model black-box models successfully. although adversarial examples diﬀerent success rates perceived similar clean image human observer tion object detection semantic segmentation however cnns extremely vulnerable small perturbations input images i.e. human-imperceptible additive perturbations result failure predictions cnns. intentionally crafted images known adversarial examples learning generate adversarial examples help investigate robustness diﬀerent models understand insuﬃciency current training algorithms several methods proposed recently adversarial examples. general attacks categorized types singlestep attacks iterative attacks according number steps gradient computation. white-box setting attackers perfect knowledge network structure weights iterative attacks generate adversarial examples much higher success rates generated single-step attacks. however adversarial examples tested diﬀerent network i.e. black-box setting single-step attacks achieve higher success rates iterative attacks. trade-oﬀ fact iterative attacks tend overﬁt speciﬁc network parameters thus generated adversarial examples rarely transfer networks single-step attacks usually underﬁt network parameters thus producing adversarial examples slightly better transferability. given phenomenon interesting question whether generate adversarial examples high success rates white-box black-box settings. data augmentation shown eﬀective prevent networks overﬁtting training process. speciﬁcally label-preserving transformations e.g. resizing cropping rotating applied images enlarge training set. consequently trained networks stronger ability generalize well unseeing images. meanwhile showed image transformations defend adversarial examples certain situations indicates adversarial examples cannot generalize well diﬀerent transformations. transformed adversarial examples known hard examples attackers served good samples produce transferable adversarial examples. propose diverse input iterative fast gradient sign method improve transferability adversarial examples. iteration unlike traditional methods maximize loss function directly w.r.t. original inputs apply random diﬀerentiable transformations input images probability maximize loss function w.r.t. transformed inputs. particular transformations used random resizing resizes input images random size random padding pads zeros around input images random manner. note that randomized operations previously used defend adversarial examples incorporate attack process create hard diverse input patterns. figure shows adversarial examples generated proposed attack method di-fgsm compares success rates attack methods white-box black-box settings. test proposed attack method several networks whitebox black-box settings. compared traditional iterative attacks results imagenet show di-fgsm gets signiﬁcantly higher success rates black-box models maintains similar success rates white-box models. improve transferability adversarial examples further integrate momentum term attack process; attack multiple networks simultaneously evaluating attack method w.r.t. defense submissions oﬃcial baselines nips adversarial competition enhanced attack reaches average success rate outperforms attack submission nips competition large margin hope proposed attack strategy serve traditional machine learning algorithms known vulnerable adversarial examples recently szegedy pointed cnns also fragile adversarial examples proposed box-constrained l-bfgs method adversarial examples reliably. expensive computation goodfellow proposed fast gradient sign method generate adversarial examples eﬃciently performing single gradient step. method extended iterative version showed generated adversarial examples exist physical world. dong proposed broad class momentum-based iterative algorithms boost transferability adversarial examples. transferability also improved attacking ensemble networks simultaneously besides image classiﬁcation adversarial examples also exist object detection semantic segmentation speech recognition deep reinforcement learning etc.. unlike adversarial examples recognized human nguyen generated fooling images diﬀerent natural images diﬃcult human recognize cnns believe recognizable objects high conﬁdences. conversely many methods proposed recently defend adversarial examples. proposed inject adversarial examples training data increase network robustness. tram`er pointed adversarially trained models still remain vulnerable adversarial examples proposed ensemble adversarial training augments training data perturbations transferred models order improve network robustness further. utilized randomized image transformations inputs inference time mitigate adversarial eﬀects. dhillon pruned random subset activations according magnitude enhance network robustness. prakash proposed framework combines pixel deﬂection soft wavelet denoising defend adversarial examples. leveraged generative models purify adversarial images moving back towards distribution clean images. denote image ytrue denote corresponding ground-truth label. denote network parameters denote loss. adversarial example generation goal maximize loss image constraint generated adversarial example look visually similar original image corresponding predicted label yadv ytrue. paper l∞-norm measure perceptibility adversarial perturbations i.e. ||r||∞ momentum iterative fast gradient sign method mifgsm proposed integrate momentum term attack process stabilize update directions escape poor local maxima. updating procedure similar i-fgsm replacement equation overﬁtting phenomenon denote unknown network parameters. general strong adversarial example high success rates white-box models i.e. black-box models i.e. hand traditional single-step attacks e.g. fgsm tend underﬁt speciﬁc network parameters inaccurate linear appropriation loss thus cannot reach high success rates white-box models. hand traditional iterative attacks e.g. i-fgsm greedily perturb images direction sign local maxima overﬁt speciﬁc network parameters overﬁtted adversarial examples rarely transfer black-box models. order generate adversarial examples strong transferability need better optimize loss alleviate overﬁtting phenomenon. data augmentation shown eﬀective prevent networks overﬁtting training process. meanwhile showed adversarial examples longer malicious simple image transformations applied indicates transformed adversarial images serve good samples better optimization. solution based analysis above propose diverse inputs iterative fast gradient sign method applies image transformations original inputs probability iteration alleviate overﬁtting phenomenon. speciﬁcally image transformations applied random resizing resizes input images random size random padding pads zeros around input images random manner transformation probability controls trade-oﬀ success rates white-box models success rates black-box models observed figure di-fgsm degrades i-fgsm leads overﬁtting. i.e. transformed inputs used attack generated adversarial examples tend much higher success rates black-box models lower success rates white-box models since original inputs seen attackers. intuitively momentum diverse inputs completely diﬀerent ways alleviate overﬁtting phenomenon. combine naturally form much stronger attack i.e. momentum diverse inputs iterative fast gradient sign method overall updating procedure m-di-fgsm similar mi-fgsm replacement equation suggested attacking ensemble multiple networks simultaneously generate much stronger adversarial examples. motivation adversarial image remains adversarial multiple networks likely transfer networks well. therefore strategy improve transferability even further. follow ensemble strategy proposed fuse logit activations together attack multiple networks simultaneously. speciﬁcally attack ensemble models logits fused dataset less meaningful attack images already classiﬁed wrongly. therefore randomly choose images imagenet validation classiﬁed correctly networks test networks consider four normally trained networks i.e. inception-v inception-v resnet-v- inceptionresnet-v three adversarially trained networks i.e. ens-adv-inception-v ens-adv-inception-v ensadv-inception-resnet-v networks publicly available. implementation details parameters diﬀerent attackers follow default settings step size total iteration number min. maximum perturbation still imperceptible human vision momentum term decay factor stochastic transformation function probability i.e. attackers equal attentions original ﬁrst perform adversarial attacks single network using fgsm i-fgsm di-fgsm mi-fgsm m-di-fgsm respectively. craft adversarial examples normally trained networks test seven networks. success rates shown table diagonal blocks indicate white-box attacks oﬀ-diagonal blocks indicate black-box attacks. list networks attack rows networks test columns. table ﬁrst foremost observe m-di-fgsm outperforms baseline attacks large margin black-box models maintains high success rates white-box models. example adversarial examples crafted incres-v m-di-fgsm success rates inc-v inc-vens strong baselines like mi-fgsm obtains corresponding success rates respectively. convincingly demonstrates eﬀectiveness combination input diversity momentum improving transferability adversarial examples. table success rates seven networks attack single network. adversarial examples crafted four normally trained networks. diagonal blocks indicate white-box attacks oﬀ-diagonal blocks indicate black-box attacks much challenging. observe m-di-fgsm always reaches highest success rates black-box models beating methods large margin maintains high success rates white-box models compare success rates i-fgsm di-fgsm eﬀectiveness diverse input patterns solely. generating adversarial examples input diversity di-fgsm signiﬁcantly improves success rates i-fgsm challenging black-box models regardless whether model adversarially trained maintains high success rates white-box models. example adversarial examples crafted res- di-fgsm success rates res- inc-v inc-vens i-fgsm obtains corresponding success rates respectively. compared fgsm di-fgsm also reaches much higher success rates normally trained black-box models comparable performance adversarially trained black-box models. though results table show momentum input diversity signiﬁcantly improve transferability adversarial examples still relatively weak attacking adversarially trained network black-box setting e.g. highest black-box success rate incres-vens therefore follow strategy attack multiple networks simultaneously order improve transferability. consider seven networks here. adversarial examples generated ensemble networks tested ensembled network hold-out network using i-fgsm ditable success rates ensemble attacks. take seven networks consideration. adversarial examples generated ensemble networks tested ensembled network hold-out network sign indicates name hold-out network. observe m-di-fgsm always reaches highest success rates black-box models beating methods large margin maintains high success rates white-box models results summarized table shows success rates ensembled network bottom shows success rates hold-out network challenging black-box setting observe m-di-fgsm always generates adversarial examples better transferability methods networks. example keeping inc-vens hold-out model m-di-fgsm fool inc-vens success rate i-fgsm di-fgsm mi-fgsm success rates respectively. besides compared mi-fgsm observe using diverse input patterns alone i.e. di-fgsm reach much higher success rate hold-out model adversarially trained network comparable success rate hold-out model normally trained network. white-box setting di-fgsm m-di-fgsm reach slightly lower success rates ensemble models compared i-fgsm mi-fgsm white-box setting. fact attacking multiple networks simultaneously much harder attacking single model. however white-box success rates improved assign transformation probability smaller value increase number total iteration smaller step size section conduct series ablation experiments study impact diﬀerent parameters e.g. step sizp di-fgsm m-di-fgsm. consider attacking ensemble networks here since much stronger attacking single network provides accurate evaluation network robustness. perturbation experiments. fig. success rates di-fgsm m-di-fgsm w.r.t. diﬀerent transformation probability generate adversarial examples using ensemble networks attack corresponding ensembled network hold-out network observe attack methods achieve higher black-box success rates lower white-box success rates increase transformation probability ﬁrst study inﬂuence transformation probability success rates white-box blackbox settings. step size total iteration number min. transformation probability varied according relationships showed figure m-di-fgsm degrades mi-fgsm di-fgsm degrades i-fgsm. show success rates various networks figure observe di-fgsm m-di-fgsm achieve higher black-box success rates lower white-box success rates increase. moreover attacks small i.e. small amount transformed inputs utilized black-box success rates increase signiﬁcantly white-box success rates drop little. phenomenon indicates importance adding transformed inputs attack process. trends showed figure also provide useful suggestions constructing strong adversarial attacks practice. example know black-box model network totally diﬀerent existing networks reach maximum transferability. black-box model mixture networks existing networks choose moderate value maximize black-box success rates pre-deﬁned white-box success rates e.g. white-box success rates must greater equal fig. success rates di-fgsm m-di-fgsm w.r.t. diﬀerent total iteration number generate adversarial examples using ensemble networks attack corresponding ensembled network hold-out network observe attack methods beneﬁted iterations performed total iteration number study inﬂuence total iteration number success rates white-box black-box settings. transformation probability step size total iteration number varied results plotted figure di-fgsm black-box success rates white-box success rates always increase total iteration number increase. similar trends also observed m-di-fgsm except black-box success rates adversarially trained models i.e. performing iterations cannot bring extra transferability adversarially trained models. moreover observe success rates m-di-fgsm di-fgsm diminished increase. step size ﬁnally study inﬂuence step size success rates white-box black-box settings. transformation probability order reach maximum perturbation even small step size total iteration number proportional step size i.e. \u0001/α. results plotted figure observe white-box success rates di-fgsm m-di-fgsm boosted smaller step size provided. black-box setting success rates di-fgsm insensitive step size success rates m-di-fgsm still improved smaller step size. fig. success rates di-fgsm m-di-fgsm w.r.t. diﬀerent step size generate adversarial examples using ensemble networks attack corresponding ensembled network hold-out network observe attack methods beneﬁted smaller step provided order examine eﬀectiveness proposed attack methods practice reproduce defense submissions black-box models oﬃcial baselines nips adversarial competition. resource limitation consider defense submissions i.e. tsail iyswim anil thomas oﬃcial baselines i.e. inc-vadv incres-vens incv. test dataset contains images size generating adversarial examples generating adversarial examples follow procedure that ﬁrstly split dataset equally batches; secondly batch maximum perturbation randomly chosen https//github.com/lfz/guided-denoise https//github.com/cihangxie/nips challenge defense https//github.com/anlthms/nips-/tree/master/mmd https//www.kaggle.com/c/nips--non-targeted-adversarial-attack table success rates defense submissions oﬃcial baselines nips adversarial competition. indicates oﬃcial results reported competition. m-di-fgsm obtains highest average success rate beating methods large margin res- inc-vens inc-vens incres-vens inc-vadv ensemble weights equally ﬁrst seven models inc-vadv. total iteration number decay factor conﬁguration mi-fgsm place nips adversarial attack competition. di-fgsm m-di-fgsm choose according trends showed figure results results summarized table also report oﬃcial results mi-fgsm reference validate implementation. performance diﬀerence mi-fgsm mi-fgsm* randomness perturbation magnitude introduced attack process. compared mi-fgsm di-fgsm higher success rates submissions slightly lower success rates baseline models results attack methods similar average success rates. integrating diverse inputs momentum term enhanced attack m-di-fgsm reaches average success rate better methods. example attack submission mi-fgsm nips competition average success rate believe advantage observed even test defense submissions. results also indicate proposed attack method used better tool evaluate robustness various newly developed networks defense methods. provide brief discussion diverse patterns help generate adversarial examples better transferability. hypothesis decision boundaries diﬀerent networks share similar inherent structures training dataset e.g. imagenet. example shown figure diﬀerent networks make similar mistakes presence adversarial examples. incorporating diverse patterns step optimization produces adversarial examples robust small transformations. adversarial examples malicious certain region network decision boundary thus increase chance fool networks i.e. achieve better black-box success rate existing methods. future plan validate hypothesis theoretically empirically. paper propose improve transferability adversarial examples input diversity. speciﬁcally method applies random transformations input images iteration attack process. compared traditional iterative attacks results imagenet show proposed attack method gets signiﬁcantly higher success rates black-box models maintains similar success rates white-box models. improve transferability integrating momentum term attacking multiple networks simultaneously. evaluating enhanced attack defense submissions oﬃcial baselines nips adversarial competition show enhanced attack reaches average success rate outperforms attack submission nips competition large margin hope proposed attack strategy serve benchmark evaluating robustness networks adversaries eﬀectiveness diﬀerent defense methods future. code public available https//github.com/cihangxie/di-fgsm.", "year": 2018}