{"title": "Tuned Models of Peer Assessment in MOOCs", "tag": ["cs.LG", "cs.AI", "cs.HC", "stat.AP", "stat.ML"], "abstract": "In massive open online courses (MOOCs), peer grading serves as a critical tool for scaling the grading of complex, open-ended assignments to courses with tens or hundreds of thousands of students. But despite promising initial trials, it does not always deliver accurate results compared to human experts. In this paper, we develop algorithms for estimating and correcting for grader biases and reliabilities, showing significant improvement in peer grading accuracy on real data with 63,199 peer grades from Coursera's HCI course offerings --- the largest peer grading networks analysed to date. We relate grader biases and reliabilities to other student factors such as student engagement, performance as well as commenting style. We also show that our model can lead to more intelligent assignment of graders to gradees.", "text": "sessment data extend discourse create effective grading system. formulate evaluate intuitive probabilistic peer grading models estimating submission grades well grader biases reliabilities allowing ourselves compensate grader idiosyncrasies. methods improve upon accuracy baseline peer grading systems simply median peer grades root mean squared error finally demonstrate grader related quantities statistical model bias reliability much educationally relevant quantities. speciﬁcally explore summative inﬂuences variables correspond student better grader formative results peer grading aﬀects future course participation. large amount data available able massive open online courses peer grading serves critical tool scaling grading complex openended assignments courses tens hundreds thousands students. despite promising initial trials always deliver accurate results compared human experts. paper develop algorithms estimating correcting grader biases reliabilities showing signiﬁcant improvement peer grading accuracy real data peer grades coursera’s course oﬀerings largest peer grading networks analysed date. relate grader biases reliabilities student factors student engagement performance well commenting style. also show model lead intelligent assignment graders gradees. recent increase popularity massive open-access online courses distributed platforms udacity coursera made possible anyone internet connection enroll free university level courses. however technologies allow scalable ways deliver video lecture content implement social forums track student progress moocs remain limited ability evaluate give feedback complex often open-ended student assignments mathematical proofs design problems essays. peer assessment historically used logistical pedagogical metacognitive aﬀective beneﬁts oﬀers promising solution scale grading complex assignments courses tens even hundreds thousands students. initial mooc-scale peer grading experiments shown promise. recent oﬀering online human computer interaction course demonstrated average student grades mooc exhibit agreement staﬀ-given grades despite initial successes remains much room improvement. estimated student submissions course given grade fell percentage points corresponding staﬀ grade submissions staﬀ given grades. thus critical challenge lies reliably obtain accurate grades peers. peer grading structurally similar moocs traditional brick mortar classrooms results shed light best practices across mediums. time work helps describe unique dynamics peer assessment setting part future cheaper accessible education. work datasets collected consecutive coursera oﬀerings human computer interaction taught stanford professor scott klemmer. courses used calibrated peer grading system order assess weekly student submissions assignments covered number diﬀerent creative design tasks building site. calibration required students correctly assess training submission allowed grade students’ submissions. every assignment student evaluated randomly selected submissions based rubric turn evaluated four classmates. ﬁnal score given submission determined median corresponding peer grades. peer grading anonymized students could evaluating evaluators were. kulkarni details peer grading system. ﬁrst oﬀering peer grading system reﬁned several ways. among things featured modiﬁed rubric addressed shortcomings original peer grading scheme. additionally peer graders divided language groups address concerns graded non-native speaker well observed patriotic grading eﬀect counting submitted least assignment english oﬀerings class students ﬁrst oﬀering students second oﬀering students came diverse backgrounds collectively students around world created submissions receiving peer grades total. table summary dataset. work used data hold set. formulated models based exploratory experiments performed using dataset testing second class ﬁnalized theories models useful. description somewhat simpliﬁcation students also performed self-assessments given higher median self grade provided within percentage points other. consider self assessments work. software peer grading framework used courses designed accommodate experimental validation peer grading. small number submissions assignment marked ground truth graded course staﬀ. since ground truth submissions student graded least week ground truth submissions supergraded average assessments. note students told submissions assigned mark belonged ground truth set. example figure shows network gradee-grader relationships assignment three supergraded ground truth submissions clearly visible. ideal peer grading system mooc satisfy following desiderata provide highly reliable/accurate assessment allocate balanced limited workload across students course staﬀ scalable class sizes tens hundreds thousands students apply broadly diverse collection problem settings. section discuss number ways formulate probabilistic model peer grading address desiderata. models introduce allow algorithmically compensate factors grader biases reliabilities maintaining estimates uncertainty principled way. assume paper student corresponds unique homework submission assignment thus refer students submissions interchangeably. collection graders denoted speciﬁc peer assessments tend land near corresponding submission’s true score corrected bias. models below always refer precision inverse variance normal distribution. present order increasing complexity three statistical models found particularly effective. model ﬁrst model puts prior distributions latent variables assumes example individual grader’s bias nonzero average bias many graders zero. speciﬁcally refers gamma distribution ﬁxed hyperparameters hyperparameters priors biases true scores respectively. experiments also consider simpliﬁed version model reliability every grader ﬁxed value. refer simpler model grader biases allowed vary pg-bias model priors reliability bias play particularly important role model fact typically grades estimate bias reliability grader. simple obtain data grader leverage observations made grader previous assignments. pose model must understand relationship grader’s bias reliability homework homework change time? answer question examine correlation between estimated biases model using dataset consecutive assignments grader’s biases pearson correlation represents utilizable consistency. grader reliability hand correlation. therefore posit model allows grader biases homework depend homework speciﬁcally model assumes model requires normalize grades across different homework assignments consistent scale. experiments example noticed grader biases diﬀerent variances diﬀerent homework assignments. using normalized score however allows propagate student’s underlying bias remaining robust assignment artifacts. model unique aspect peer grading graders students submissions graded. consequently interest understand model relationship between one’s grade one’s grading ability example knowing student scored well assignment cause placing trust student grader vice versa. figure show experiments exploring relationships grader speciﬁc latent variables. particular observe high scoring students tend somewhat reliable graders model formalizes intuition allowing reliability grader depend grade assumes following note model extends introducing dependencies allowing student’s submission score estimate grading ability. time model constrained forcing grader reliability depend single parameter instead allowed vary arbitrarily thus prevents model overﬁtting. ethics incentives. probabilistic inference score students mooc goal could simply optimize accuracy. must also consider fairness comes deciding variables include model. might tempting example include variables race ethnicity gender model better accuracy almost everyone would agree factors could fairly used within scoring mechanism even improved prediction accuracy. another example might model temporal coherence student grades incorporating temporal coherence students scores scoring mechanism would allow students given clean slate homework. model allows inferred true score submission depend graders’ scores seem contentious dependence weak aﬀecting inﬂuence particular grader ﬁnal prediction desirable. interestingly using complex scoring mechanism model fact incentivize good grading. particular student’s grade inﬂuenced closely assessments grader match graders graded assignments. consequently allowing student grades depend performance graders model used scoring mechanism incentive students eﬀort grading. given probabilistic model peer grading discussed above would like infer values unobserved variables true score every submission bias reliability student grader. inference framed problem computing posterior distribution latent variables conditioned obreason circularly knew every submission’s true scores would able easily compute posterior distributions grader biases order estimate biases must know true score submission. address apparent chicken problem turn simple approximate inference methods. experiments reported section gibbs sampling produces collection samples desired posterior distribution. samples used estimate various quantities interest. example given samples posterior distribution true score submission estimate true score also samples quantify uncertainty prediction estimating variance samples posterior section examine peer grading eﬃciency. note ordinary gibbs sampling algorithm performed closed form models pg-bias model requires numerical approximation coupling submission’s true score grader discuss details appendix. visually observe rapid mixing gibbs chains experiments shown section iterations gibbs sampling discarding initial burn-in samples. expectation-maximization alternative approximate inference approach treat true scores grader biases parameters iterative coordinate descent based algorithm obtain point estimates parameters. practice gibbs approaches behave similarly. general advantage signiﬁcantly faster obtaining posterior credible intervals natural using gibbs. peer grading dataset methods produce analogous results. example gibbs rmse scores ﬁrst dataset respectively gibbs running roughly minutes running seconds. refer reader appendix full algorithmic details gibbs well evaluation. measure peer grading accuracy repeatedly simulate score would assigned ground truth submission peer graded. evaluation well would graded single ground truth submission uses step methodology inference using data except peer grades ground truth submission evaluated. gives estimate grader’s biases reliabilities well model priors independent submission evaluated. simulations sampled four student assessments randomly pool peer grades ground truth submission estimate submission’s grade using sample assessments recorde residual estimated grade true grade. ground truth submission simulations report rmse number simulations fell within percentage points true score average standard deviation errors ground truth worst misgrade simulations produced. interesting issue whether consider true grade ground truth submission score given staﬀ consensus hundreds students assessed submission. datasets believe discrepancy staﬀ grade student consensus typically results ambiguities rubric elect mean student consensus ground truth submission true grade. interesting observation came exploration peer graders datasets tendency grade towards mean inﬂating grades low-scoring submissions deﬂating grades high-scoring submissions. remark experiments unsupervised fashion would reasonable staﬀ grades training process order encourage model place trust students consistently grade like instructors. compare probabilistic models grade estimation algorithm used coursera’s platform. baseline model score given students median four peer grades received. speciﬁcally baseline estimation take account individual grader’s biases reliabilities. incorporate prior knowledge distribution true grades. using probabilistic models leads substantially higher grading accuracy. experiments able reduce error prediction ground truth grade similarly second oﬀering course able reduce error second oﬀering means number students received grades within percentage points grade increased figures show eﬀect using model scoring mechanism histogram grading errors table shows complete results model. course improvements observe students signiﬁcantly consistent graders compared students hci. however remark every models outperforms baseline grading system respect every metric indicating best gains peer grading likely come improved class design well statistical modeling. results show models yield best results model outperforming models respect metrics. single change provides signiﬁcant gains accuracy obtained estimating grader’s bias simple model responsible reduction rmse. changes contribute comparatively smaller improvements accurate model. surprisingly modeling grader bias particularly eﬀective modeling grader precision little improve performance. deeper result test model synthetic dataset generated exactly model using synthetic data four grades student diﬃcult model correctly estimate grader reliability. modeling variance grader seems notable impact students grade many assignments experiment also suggests useful though contains expressive power estimating parameters grader reliability statistically tractable four grades student estimating reliability grader. fairness efﬁciency peer grading advantages using probabilistic model peer grading obtain belief distribution grades student. distributions give natural calculating conﬁdent model predicts grade student. fact conﬁdence results trusted open possibility equitable allocation graders. example given point midway peer grading process model highly conﬁdent prediction given student’s score unsure prediction another student. situation ensure student gets fair access quality feedback could reassign graders gradees submissions lowconﬁdence scores given and/or better graders. ﬁrst step towards fair allocation grades ourselves accurate estimates conﬁdence? example would like know interpret means practice bayesian model conﬁdent prediction learner’s true score within actual true score. better understand conﬁdence estimates following experiment ﬁrst performed large number peer grading simulations ground truth. simulation calculate conﬁdent model grade predict ground truth submission within true score respectively. estimated conﬁdences ranges etc. collecting predictions range test pass rate range. example suppose select four assessments ground truth submission figure histogram errors made using baseline scoring mechanism. histogram errors using comparison model conﬁdence actual success rate predictions diagonal simulation. model reports conﬁdence based four assessments predicted grade within true score estimate predictions conﬁdence range. test conﬁdence range example prediction passes estimate fact within ground truth score. worry model might overconﬁdent predictions even wrong. however results shown figure demonstrate conﬁdence estimates conservative side example time model claims conﬁdent prediction model’s estimate correct. since reason believe conﬁdence values accurate employ posterior belief distributions better allocate grades. understand much beneﬁt could improved grade allocation estimate point grading process conﬁdent submission’s score. homework assignment simulate grading taking place rounds. ﬁrst round include ﬁrst grade submitted grader second round included ﬁrst etc. round model using corresponding subset grades count number submissions conﬁdent predicted grades within student’s true grade. rounds grading highly conﬁdent estimated grade submissions figure shows conﬁdent submissions grows grading rounds. experiment demonstrates clear opportunity grades reallocated well pressing need submissions grades. students rounds still unsure submission’s true score. applying probabilistic models peer grading networks allows increase grade accuracy better allocate submissions students grade. another product work assignment belief distribution true score grader bias grader reliability student. large dataset derive understanding peer grading formative summative assessment. focus investigation questions factors inﬂuence well student grades? inﬂuential factors grader ability. explore factors inﬂuence well student grades compare grading residual time spent grading grader grade gradee grade. time spent grading shows particularly interesting trend hypothesized students snap grade peers’ work unreliable tend slightly inﬂate grades. surprising tens thousands grades sweet spot time spent grading. students grade assessments time z-score around signiﬁcantly lower residual standard deviations students take long time grade sweet spot visible look normalized grading times. assignments class sweet spot corresponds around minutes grading. reﬂect less time grader enough chance fully examine gradee’s work long grading session mean grader trouble understanding facet submission. examining relationship grader grade gradee grade aﬀect residual also shows notable trends. graders score higher assignments close monotonically decreasing biases getting better grade homework general makes students reliable graders; notable exception students best grades accurate students well superlative submissions best worst easiest grade submissions standard deviation mean hardest finally results show students least biased grading peers similar score best students signiﬁcantly downgrade worst submissions worst students notably inﬂate best submissions. figure grader consistency function time spent grading. curve comparing performance predicting future class participation given student’s grade bias gradees. order understand relationship between grading performance commenting style compare grading residual comment length well sentiment polarity comment measure polarity comment sentiment analysis word list implement simple sentiment analyzer returns polarity score proportional word valences comment. comment length polarity ﬁlter non-english words. observe comments correspond larger negative residuals typically signiﬁcantly longer suggesting perhaps students write weaknesses submission strong points. said observe overall comments mostly range polarity neutral quite positive suggesting rather highly negative submissions many students make eﬀort balanced comments peers. grader ability future performance. also tested signal grading ability predicting future participation. based theory best graders intrinsically motivated hypothesized reliable grader would diﬀerent dimension information student’s engagement able better predict future engagement. tested hypothesis constructing classiﬁcation task predict whether student would participate next assignment addition student’s grade experimented including grader bias reliability features linear classiﬁer. results show including grader bias reliability improved predictive ability area curve score properties student grades captures dimension engagement missed assignment grade. statistical models present paper seen part long tradition models proposed purposes aggregating information noisy human labelers workers. many works adapt classical item-response theory models problem grading without answer appear literature educational aptitude testing cultural anthropology recently context human computation crowdsourcing educational testing example johnson rogers propose models combining human judgements essays. papers analyze dedicated human graders evaluated hundreds essays allowing rich model ﬁtted per-grader basis. contrast peer grading moocs student assesses handful assignments necessitating constrained models. recent paper setting perhaps similar goldin bayesian models peer grading smaller scale classroom setting. work posits grader bias fact incorporates rubric-speciﬁc biases consider many issues raised grading task reallocation relationship grader bias student engagement example. central themes crowdsourcing literature balancing label accuracy labor cost mooc peer grading systems must contend well. problems typically receives number noisy labels challenge lies resolving correct label deciding whether hire labelers given task. explosion interest recent years widespread applications crowdsourcing example image annotation whitehill present method similar model discrete true image labels well labeler accuracy. work draws crowdsourcing literature problem peer grading unique several ways. example fact graders also gradees peer grading quite diﬀerent typical crowdsourcing settings dichotomy labelers items labeled motivates diﬀerent models crowdsourcing applications goal often lies determining true labels rather understand anything labelers themselves whereas peer grading shown insights glean graders educational value. similar problem peer-grading paper assignment problem peer review process academic conferences. related central challenge problems involves fusing disparate human opinions open-ended creative work many speciﬁc challenges distinct. side information plays much larger role peer review conference chairs typically rely heavily personal elicited knowledge reviewer expertise citation link structure assign reviewer roles peer grading hand seems less sensitive personal preferences single submission equally well graded large fraction students course. paper presents methods making large scale peer grading systems dependable accurate eﬃcient. particular show much gained maintaining estimates grader speciﬁc quantities bias reliability. addition improving peer grading accuracy quantities give unique insight peer grading formative summative assessment. remain number issues addressed future work. considered problem determining submissions need allocated additional graders. however deciding grader best evaluating particular submission open problem whose solution could depend number variables writing styles grader gradee respective cultural linguistic backgrounds particularly important issue global scale course rosters arise moocs. another issue arises study biases graders spend adequate time grading. incentivizing students provide careful high quality feedback peers question paramount importance open-access courses. using model scoring discussed makes student’s score dependent grading performance build justiﬁed incentive directly scoring mechanism. understanding scoring rules game theoretical perspective remains future work. finally clear present scores calculated complicated peer grading model students. communication might easy student’s ﬁnal grade simply mean median peer grades student need know inner workings sophisticated statistical backend? students unhappy lack transparency grading mechanisms hand might feel satisﬁed overall grade. moocs become widespread need reliable grading feedback open ended assignments becomes ever critical. scalable solution shown eﬀective peer grading. addressing shortcomings current peer grading systems hope students everywhere peer grading consequently free online open access educational experience. thank chinmay kulkarni scott klemmer providing assistance datasets leonidas guibas john mitchell discussions support. jonathan huang supported fellowship. grade test without knowing answers bayesian graphical model adaptive crowdsourcing aptitude testing. annual international conference machine learning icml goldin ashley. peering inside peer review bayesian models. proceedings international conference artiﬁcial intelligence education aied’ pages berlin heidelberg springer-verlag. mislevy almond steinberg. bayes nets educational assessment numbers come from. proceedings ﬁfteenth conference uncertainty artiﬁcial intelligence pages morgan kaufmann publishers inc. movellan. whose vote count more optimal integration labels labelers unknown expertise. advances neural information processing systems pages press", "year": 2013}