{"title": "PGMHD: A Scalable Probabilistic Graphical Model for Massive Hierarchical  Data Problems", "tag": ["cs.AI", "cs.LG"], "abstract": "In the big data era, scalability has become a crucial requirement for any useful computational model. Probabilistic graphical models are very useful for mining and discovering data insights, but they are not scalable enough to be suitable for big data problems. Bayesian Networks particularly demonstrate this limitation when their data is represented using few random variables while each random variable has a massive set of values. With hierarchical data - data that is arranged in a treelike structure with several levels - one would expect to see hundreds of thousands or millions of values distributed over even just a small number of levels. When modeling this kind of hierarchical data across large data sets, Bayesian networks become infeasible for representing the probability distributions for the following reasons: i) Each level represents a single random variable with hundreds of thousands of values, ii) The number of levels is usually small, so there are also few random variables, and iii) The structure of the network is predefined since the dependency is modeled top-down from each parent to each of its child nodes, so the network would contain a single linear path for the random variables from each parent to each child node. In this paper we present a scalable probabilistic graphical model to overcome these limitations for massive hierarchical data. We believe the proposed model will lead to an easily-scalable, more readable, and expressive implementation for problems that require probabilistic-based solutions for massive amounts of hierarchical data. We successfully applied this model to solve two different challenging probabilistic-based problems on massive hierarchical data sets for different domains, namely, bioinformatics and latent semantic discovery over search logs.", "text": "data scalability become crucial requirement useful computational model. probabilistic graphical models useful mining discovering data insights scalable enough suitable data problems. bayesian networks particularly demonstrate limitation data represented using random variables random variable massive values. hierarchical data data arranged treelike structure several levels would expect hundreds thousands millions values distributed even small number levels. modeling kind hierarchical data across large data sets bayesian networks become infeasible representing probability distributions following reasons level represents single random variable hundreds thousands values number levels usually small also random variables iii) structure network predeﬁned since dependency modeled top-down parent child nodes network would contain single linear path random variables parent child node. paper present scalable probabilistic graphical model overcome limitations massive hierarchical data. believe proposed model lead easily-scalable readable expressive implementation problems require probabilistic-based solutions massive amounts hierarchical data. successfully applied model solve diﬀerent challenging probabilisticbased problems massive hierarchical data sets diﬀerent domains namely bioinformatics latent semantic discovery search logs. probabilistic graphical models refer family techniques merge concepts graph structures probability models represent conditional dependencies among sets random variables data pgmâăźs useful mining extracting insights large-scale noisy data. major challenges pgms face emerging ﬁeld scalability restriction applied propositional domain extensions already proposed address challenges hierarchical probabilistic graphical models extend work non-propositional domains focus models make bayesian networks applicable non-propositional domains solve scalability issues arise applied massive data sets. massive data sets often exhibit hierarchical properties data divided several levels arranged tree-like structures. data items level depend inﬂuenced data items immediate upper level. kind data appropriate represent probability distribution would bayesian network since dependencies kind data bidirectional. bayesian network considered feasible provide concise representation large ∗department computer science university georgia athens georgia. email aljaddauga.edujamcs.uga.edu †school informatics computing indiana univeristy bloomington email mkorayemcs.indiana.edu ‡careerbuilder norcross email camilo.ortizcareerbuilder.com §careerbuilder norcross email trey.graingercareerbuilder.com ¶department computer science university georgia athens georgia. email jamcs.uga.edu complex carbohydrate research center university georgia athens georgia. email willccrc.uga.edu probability distribution need cannot eﬃciently handled using traditional techniques tables equations scenario case massive hierarchical data however since level represents random variable data items level outcomes random variable. example consider hierarchical data organized follows data items level represent cities data items second level represent diseases city connected diseases appears city. case assume cities diseases. would like represent data consider cities root level outcomes random variable city data items second level outcomes another random variable disease. thus data composed nodes single path city disease conditional probability table disease contain entries. kind data propose simple probabilistic graphical model represent massive hierarchical data eﬃcient way. successfully apply pgmhd diﬀerent domains bioinformatics search analytics main contributions paper follows propose simple eﬃcient scalable probabilistic-based model massive hierarchical data. successfully apply model bioinformatics domain automatically classify graphical models classiﬁed major categories directed graphical models often referred bayesian networks belief networks undirected graphical models often referred markov random fields markov networks boltzmann machines log-linear models probabilistic graphical models consist graph structure parameters. graph structure represents conditionally independent relations probability model parameters consist joint probability distributions probabilistic graphical models often considered convenient numerical representations main reasons pgms used many domains. example hidden markov models considered crucial component speech recognition systems bioinformatics probabilistic graphical models used sequence analysis protein homology detection sequence alignment genome-wide identiﬁcation natural language processing bayesian models used part speech tagging problem pgms general bayesian networks particular suitable representing massive data time complexity learning structure network space complexity storing network thousands random variables. general ﬁnding network maximizes bayesian minimum description length scores np-hard problem bayesian networks bayesian network concise representation large probability distribution handled using traditional techniques tables equations graph bayesian network directed acyclic graph bayesian network consists components representing structure conditional probability tables shown figure node bayesian network must quantiﬁes relationship variable represented node parents network. completeness consistency guaranteed bayesian network since probability distribution satisﬁes bayesian network constraints constraints guarantee unique probability distribution numerical constraints represented independence constraints represented structure itself. independence constraint shown figure variable structure independent variables parents parents known. example information known probability aﬀected information call independent known. independence constraints known markovian assumptions. markov random fields mrfs known also markov networks well-known graphical models graph undirected. graphical model random variables represented vertices edges represent dependency. however clear causal inﬂuence node edges undirected. undirected graph nodes without direct link always conditionally independent variables whereas nodes direct link always dependent mrfs joint probability distribution calculated multiplying normalization factor potential functions assign positive value fully connected nodes called clique. clique fully connected subset nodes associated non-negative potential function potential functions derived notion conditional independence potential function must refer nodes directly connected according cliques potential functions joint probability undirected graph shown figure calculated using following equation mrfs common many ﬁelds like spatial statistics natural language processing communication networks little causal structure guide construction directed graph. hidden markov models hidden markov model statistical time series model used model dynamic systems whose states observable whose outputs are. hmms widely used speech recognition handwriting recognition text-to-speech synthesis hmms rely three main assumptions. first observation time generated process whose state hidden observation. second state hidden process satisﬁes markov assumption state system known states outputs times longer dependent states words state speciﬁc time contains needed information history process predict future process. upon assumptions joint probability distribution sequence states observations factored follows third assumption hidden state variables discrete deﬁne probability distribution observation sequences need specify probability distribution initial state state transition matrix deﬁning output model deﬁning hmms considered subclass bayesian networks known dynamic bayesian networks bayesian networks model systems evolve time section describes related work proposed model diﬀerent perspectives. first describe related hierarchical probabilistic models describe current techniques used automate annotation mass spectrometry data glycomics scenarios test proposed model. close section describing applied proposed model discover latent semantic similarity keywords extracted search logs purposes building semantic search system. probabilistic graphical models hierarchical data probabilistic graphical models require propositional domains overcome limitation extensions proposed extend models non-propositional domains. bayesian hierarchical model used natural scene categorization performs well large sets complex scenes model also applied event recognition human actions interactions another application hierarchical bayesian network identifying changes gene expression microarray experiments authors introduced hierarchical bayesian network extends expressiveness regular bayesian network allowing node represent aggregation simpler types enables modeling complex hierarchical domains. main idea small number hidden variables compressed representation observed variables following restrictions idea mainly compress observed data. although hierarchical bayesian network models extended regular bayesian network represent non-propositional domains able solve issue scalability bayesian networks massive amounts hierarchical data. automated annotation mass spectrometry data glycomics case proposed model automated annotation mass spectrometry data glycomics. glycans third major class biological macro-molecules besides nucleic acids proteins glycomics refers scientiﬁc attempts characterize study glycans deﬁned integrated systems approach study structure-function relationships glycans deﬁned importance emerging ﬁeld study clear accumulated evidence roles glycans cell growth metastasis cell-cell communication microbial pathogenesis. glycans diverse terms chemical structure information density nucleic acids proteins glycan identiﬁcation much diﬃcult protein identiﬁcation proven np-hard problem since unlike protein structures glycan structures trees rather linear sequences. leads large diversity glycan structures which along absence standard representation glycans resulted many incomplete databases stores glycan structures glycan-related data diﬀerent format. example kegg uses format glycosciences.de uses linucs format uses iupac format. although become major analytical technique glycans general method developed automated identiﬁcation glycan structures using tandem data. relative ease peptide identiﬁcation using tandem mainly linear structure peptides availability reliable peptide sequence databases. proteomic analyses mostly complete series fragment ions high abundance often observed. tandem mass spectra mass amino acid sequence corresponds mass diﬀerence high-abundance peaks allowing amino acid sequence deduced. glycomics data series disrupted branched nature molecule signiﬁcantly complicating extraction sequence information. addition groups isomeric monosaccharides commonly share mass making impossible distinguish alone. databases glycans exist limited minimally curated suﬀer badly pollution glycan structures produced nature irrelevant organism study. several algorithms developed attempts semi-automate process glycan identiﬁcation interpreting tandem spectra including cartoonisttwo glych glycopep glycomod glycopeakfinder glycowork-bench simglycan however programs produces incorrect results using polluted databases annotate large datasets containing hundreds thousands spectra. inspection current literature indicates machine learning data mining techniques used resolve issue although great potential successful pgmhd attempts employ machine learning techniques solution automated identiﬁcation glycans using data. semantic similarity semantic similarity metric deﬁned documents terms distance reﬂects likeness meaning well deﬁned natural language processing information retrieval generally major techniques used compute semantic similarity computed using semantic network based computing relatedness terms within large corpus text major techniques classiﬁed corpus-based approach pointwise mutual information latent semantic analysis though outperform mining synonyms applied proposed pgmhd model discover related search terms measuring probabilistic-based semantic similarity search terms. make remarks assumptions. first node ﬁrst level seen root node ones leaves. second observation probabilistic model outcome random variable namely deﬁned clearly latter observations ones used train model. observed proposed model seen special case bayesian network considering network consisting single directed path nodes. however believe leveled directed graph explicitly deﬁnes node outcome random variables leads easily scalable implementation problems consider; improves readability expressiveness implemented network; iii) facilitates training model. probabilistic-based classiﬁcation deﬁned earlier section model predict outcome parent level given observation level classiﬁcation score. given outcome level namely probabilistic-based semantic similarity scoring level identically distributed random variables deﬁned earlier section deﬁne probabilistic-based semantic similarity score outcomes approximating conditional joint probability progressive learning pgmhd designed allow progressive learning. progressive learning learning technique allows model learn gradually time. training data need given time model instead model learn available data integrate knowledge represented one. learning technique attractive data following reasons recursive learning allows results model used training data provided judged accurate user. progressive learning approach pgmhd shown algorithm pgmhd used diﬀerent purposes built trained. pgmhd used predict class level observations random variables level example annotation data pgmhd used predict best glycan level annotate spectrum evaluating annotated peaks level probability scores represent well selected glycan correlates manually curated annotations used train model. figure pgmhd tandem data. root nodes glycans annotate peaks level level nodes glycan fragments annotate peaks level edges represent dependency glycans generates fragments. figure annotation using gelato. scan number scan peak charge charge state peak peak intensity represents abundance peak peak mass charge given peak cartoon annotation peak format feature mass charge glycan glycanid glycan glycan ontology. figure fragments glycan level. observed selected fragmented generate smaller ions used identify glycan structure appropriately annotates ion. theoretical fragments glycan structure used annotate spectrum used annotate corresponding spectrum. pgmhd automate annotation model well suited representing data. recently implemented glycan elucidation annotation tool semi-automated annotation tool glycomics integrated within data processing framework called grits. figures show screen shots gelato annotated spectra. figure shows proﬁle level figures show annotated peaks using fragments glycans chosen candidate annotations proﬁle data represent data shown ﬁgures using proposed model top-layer node assigned proﬁle table corresponds data. then tables unique node created connected parent node using directed edge parent node child node top-layer node stores value representing frequently parent seen training data. however child node layer parent. edge’s weight represents co-occurrence frequency child parent. child node stores total frequency observing child regardless identity parents. combined frequency data makes possible design progressive learning algorithm extract information massive data sets. figure shows pgmhd given data experiments using data collected stem cell samples. size data peaks distributed scans experiments. figure shows learning time using progressive learning technique. test introduced experiment time model training recorded total time required train model. performance results demonstrate eﬃciently progressive learning works pgmhd. test accuracy pgmhd trained model randomly selecting available experiments experiments used test trained model annotating experiments’ peaks using pgmhd. baseline evaluation annotations generated commercial tool simglycan. results accuracy test shown table figure shows average precision recall pgmhd compared average precision recall gelato using dataset peaks distributed experiments. pgmhd latent semantic discovery hadoop also implemented version pgmhd hadoop used latent semantic discovery users’ search terms extracted search logs provided careerbuilder.com. careerbuilder operates largest board u.s. extensive growing global presence millions postings million actively-searchable resumes billion searchable documents million searches hour. search relevancy recommendations team wants discover latent semantic relationships among search terms entered users order build semantic search engine understands user’s query intent order provide relevant results traditional keyword search engine. tackle problem careerbuilder cannot typical synonyms dictionary since keywords used employment search domain represent titles skills companies would found traditional english dictionary. additionally careerbuilder’s search engine supports dozen languages search model language-independent. given search logs users users’ classiﬁcations shown table pgmhd represent kind data placing classes users root nodes placing search terms users second level children nodes. then edge formed linking search term back class user searched frequency search term stored node term frequency speciﬁc search term searched users speciﬁc class stored edge class term. frequency root node summation frequencies edges connect root node children figure shows pgmhd implemented hadoop using map/reduce jobs hive tables. created pgmhd hadoop calculated probabilistic-based semantic similarity score pair terms shared parents. size data analyzed experiment billion search records. decrease noise given data applied pre-ﬁltering technique removing search term used less distinct users. ﬁnal graph representing data contains root nodes child nodes edges. experiment performing latent semantic discovery among search terms using pgmhd hadoop cluster data nodes opteron processor cores ram. table shows sample results terms related terms discovered pgmhd. evaluate model’s accuracy sent results data analysts careerbuilder reviewed random pairs discovered related search terms returned list feedback whether pair discovered related terms related\" unrelated\". calculated accuracy related terms data hadoop developer obiee java python registered nurse registered nurse manager nurse nursing director nursing machine learning data scientist analytics business intellegence statistical analyst lucene hadoop java software developer programmer .net developer developer software nosql data science machine learning hadoop teradata realtor assistant real estate real estate sales sales real estate agent machine learning data analyst data mining analytics data plumber plumbing apprentice plumbing maintenance plumbing sales maintenance scrum project manager agile coach pmiacp scrum master model based upon ratio number related results total number results. results show accuracy discovered semantic relationships among search terms using pgmhd model probabilistic graphical models important many modern applications data mining data analytics. major issue existing probabilistic graphical models scalability handle large data sets making important area research given tremendous modern focus data number data points produced modern computers systems sensors. pgmhd probabilistic graphical model attempts solve scalability problems existing models scenarios massive hierarchical data present. pgmhd designed hierarchical data sets size regardless domain data belongs. paper present experiments diﬀerent domains automated tagging high-throughput mass spectrometry data bioinformatics latent semantic discovery using search logs largest board u.s. cases tested pgmhd show model robust scale thousand entries least billions entries also single computer well parallelized fashion large cluster servers authors would like deeply thank david crandall indiana university providing helpful comments suggestions improve paper. also would like thank kiyoko aoki kinoshita soka university khaled rasheed university georgia valuable discussions suggestions improve model. deep thanks melody porterﬁeld rene ranzinger complex carbohydrate research center university georgia providing data valuable time information shared understand annotation process data.", "year": 2014}