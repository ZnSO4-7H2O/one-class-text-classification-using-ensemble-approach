{"title": "Vector Field Based Neural Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "A novel Neural Network architecture is proposed using the mathematically and physically rich idea of vector fields as hidden layers to perform nonlinear transformations in the data. The data points are interpreted as particles moving along a flow defined by the vector field which intuitively represents the desired movement to enable classification. The architecture moves the data points from their original configuration to anew one following the streamlines of the vector field with the objective of achieving a final configuration where classes are separable. An optimization problem is solved through gradient descent to learn this vector field.", "text": "abstract. novel neural network architecture proposed using mathematically physically rich idea vector ﬁelds hidden layers perform nonlinear transformations data. data points interpreted particles moving along deﬁned vector ﬁeld intuitively represents desired movement enable classiﬁcation. architecture moves data points original conﬁguration following streamlines vector ﬁeld objective achieving ﬁnal conﬁguration classes separable. optimization problem solved gradient descent learn vector ﬁeld. understanding black models work deep neural architectures support vector machines extensively discussed problem literature many works focus visualizing understanding behavior possible address problem comprehending neural networks behavior geometrical sense understanding universal function approximators another possible interpretation considers hidden layers neural networks nonlinear continuous transformations domain space towards work author suggests vector ﬁelds might better handle transformations traditional layers. inspired present work proposes combine neural networks vector ﬁelds order understand data separation data points particles moving along ﬂow. vector ﬁelds also recently used analyze optimization problem generative adversarial networks achieving remarkable results visualization comprehension gans limitations extend them. work introduces novel architecture using vector ﬁelds activation functions. optimize vector ﬁelds parameters stochastic gradient descent using binary cross entropy loss function. applying concept vector ﬁelds neural networks vast amount established mathematical physical concepts abstractions visualizations arises neural networks. instance euler’s method solving ordinary diﬀerential equations used work implement concept data points particles moving ﬂow. simple gaussian kernel function. diﬀerent initialization hyperparameters cost function shows consistent reduction epochs results analyzed. section describes architecture optimization problem. curve solving called streamline vector ﬁeld given particle position time possible physical interpretation vector represents velocity acting particle given point space streamline displacement done particle travels along path time particle position given family vector ﬁelds deﬁned parameters proposed best vector ﬁeld family aiming transform every point input space point transformed space points distinct classes would linearly separable. intuitively vector ﬁeld represents desired movement enable classiﬁcation. figure presents input data transformed vector ﬁeld layer architecture. also presents optimized vector ﬁeld goal linearly separate data. note architecture’s last layer linear separator implemented using logistic function. note variance controlled parameters thought weight vectors provide direction vector ﬁeld ﬁnal layer logistic function binary cross entropy cost acts upon transformed points initial values random vectors distribution corresponding distribution regularization vectors gradient respective j-th components vectors presented equations j-th component wj/wk j-th k-th components vector regularization parameter. ization hyperparameters paper’s experiments. training made entire dataset full-batch fashion hence validation/test set. results presented cost throughout epochs circles dataset. next boundary layer calculated color original data points plotted thus analyze relationship boundary layer original transformed space. finally eﬀects regularization visualized compared color dataset. analyzing cost function along epochs diﬀerent learning rates circles dataset shows reduction cost epochs interesting pattern appears learning rate increases cost function deviation becomes less smooth standard deviation increases well. classiﬁcation bending space extracting center circle outside generates superposition diﬀerent points original space. thus region happens misclassiﬁcation occurs avoided. diminish algorithm’s power create extreme movements regularization. figure acting damper smoothing movements happening original space preventing overlapping diﬀerent points region transformed space. choice facilitate overﬁtting since steps taken data points greater constriction many movements made maximum. it’s possible drawn analogy regularization automatically made choose good values small steps taken streamlines followed. work presents novel neural network based vector ﬁelds. base network move points along space allowing posterior separation points using linear classiﬁer. vector ﬁelds created using kernel functions. approach brings vast amount well established vector ﬁelds’ theory enabling geometrical interpretation neural network works. initialization parameter hyperparameter plays important role cases must taken account. regularization shown damper reducing capability vector ﬁeld move points diminish disruption dataset original space. experiments presented cost function reduction indicates learning capability optimized used move points. work needs done exploring real world datasets evaluating learning performance validation test sets. another topic includes investigation architecture performance hyperparameter small large.", "year": 2018}