{"title": "Unsupervised Learning by Predicting Noise", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Convolutional neural networks provide visual features that perform remarkably well in many computer vision applications. However, training these networks requires significant amounts of supervision. This paper introduces a generic framework to train deep networks, end-to-end, with no supervision. We propose to fix a set of target representations, called Noise As Targets (NAT), and to constrain the deep features to align to them. This domain agnostic approach avoids the standard unsupervised learning issues of trivial solutions and collapsing of features. Thanks to a stochastic batch reassignment strategy and a separable square loss function, it scales to millions of images. The proposed approach produces representations that perform on par with state-of-the-art unsupervised methods on ImageNet and Pascal VOC.", "text": "convolutional neural networks provide visual features perform remarkably well many computer vision applications. however training networks requires signiﬁcant amounts supervision. paper introduces generic framework train deep networks end-to-end supervision. propose target representations called noise targets constrain deep features align them. domain agnostic approach avoids standard unsupervised learning issues trivial solutions collapsing features. thanks stochastic batch reassignment strategy separable square loss function scales millions images. proposed approach produces representations perform state-of-the-art unsupervised methods imagenet pascal voc. recent years convolutional neural networks convnets pushed limits computer vision leading important progress variety tasks like object detection image segmentation success ability produce features easily transfer domains trained massive databases labeled images weaklysupervised data however human annotations introduce unforeseen bias could limit potential learned features capture subtle information hidden vast collection images. several strategies exist learn deep convolutional features annotation either capture signal source form selfsupervision learn underlying distribution images approaches obtain promising performance transfer learning explicitly learn discriminative features. attempts made retrieval based approaches clustering hard scale tested small datasets. unfortunately supervised case data required learn good representations. work propose novel discriminative framework designed learn deep architectures massive amounts data. approach general focus convnets since require millions images produce good features. similar self-organizing maps deep features predeﬁned representations dimensional space. opposed approaches learn features end-to-end fashion traditionally suffers feature collapsing problem. approach deals issue ﬁxing target representations aligning features. representations sampled uninformative distribution noise targets approach also shares similarities standard clustering approches like k-means discriminative clustering addition propose online algorithm able scale massive image databases like imagenet importantly approach barely less efﬁcient train standard supervised approaches reuse optimization procedure designed them. achieved using quadratic loss fast approximation hungarian algorithm. show potential approach training end-to-end imagenet standard architecture namely alexnet supervision. test quality features several image classiﬁcation problems following setting donahue state-of-the-art unsupervised self-supervised learning approaches much simpler train scale. several approaches recently proposed tackle problem deep unsupervised learning based clustering loss tested scale comparable supervised convnet training. coates uses k-means pre-train convnets learning layer sequentially bottom-up fashion. work train convnet end-to-end loss shares similarities k-means. closer work dosovitskiy proposes train convnets solving retrieval problem. assign class image transformation. contrast work approach hardly scale hundred thousands images requires custom-tailored architecture standard alexnet. another traditional approach learning visual representations unsupervised manner deﬁne parametrized mapping predeﬁned random variable images. traditional examples approach variational autoencoders generative adversarial networks lesser extent noisy autoencoders work opposite; images predeﬁned random variable. allows re-use standard convolutional networks greatly simpliﬁes training. generative adversarial networks. among approaches generative adversarial networks share another similarity approach namely explicitly minimizing discriminative loss learn features. models cannot learn inverse mapping donahue recently proposed encoder extract visual features gans. like ours encoder standard convolutional network. however loss aims differentiating real generated images aiming directly differentiating images. makes approach much simpler faster train since need learn generator discriminator. wordvec doersch show spatial context strong signal learn visual features. noroozi favaro extended work. others shown temporal coherence videos also provides signal used learn powerful visual features particular wang gupta show features provide promising performance imagenet. contrast work approaches domain dependent since require explicit derivation weak supervision directly input. autoencoders. many also used autoencoders reconstruction loss idea encode decode image minimizing loss decoded original images. trained encoder produces image features decoder used generate images codes. decoder often fully connected network deconvolutional network sophisticated like pixelcnn network self-organizing map. family unsupervised methods aims learning dimensional representation data preserves certain topological properties particular neural aligns feature vectors input data. input datum assigned vectors winner-takes-all manner. feature vectors spirit similar target representations similar assignment strategy. contrast work target vectors ﬁxed aligned input vectors. since primarly learning input features opposite. discriminative clustering. many methods proposed discriminative losses clustering particular bach harchaoui shows ridge regression loss could learn discriminative clusters. successfully applied several computer vision applications like object discovery video/text alignment work show similar framework designed neural networks. opposed address empty assignment problems restricting possible reassignments permutations rather using global linear constrains assignments. assignments updated online allowing approach scale large datasets. choosing loss function. supervised setting popular choice loss softmax function. however computing loss linear number targets making impractical large output spaces workarounds scale losses large output spaces tygert recently shown using squared distance works well many supervised settings long ﬁnal activations unit normalized. loss requires access single target sample making computation independent number targets. leads following problem using ﬁxed target representations. directly solving problem deﬁned would lead representation collapsing problem images would assigned representation avoid issue ﬁxing predeﬁned target representations matching visual features. precisely matrix deﬁned product matrix containing representations assignment matrix }n×k i.e. formulation forces visual features diversiﬁed avoiding collapsing issue cost ﬁxing target representations. predeﬁning targets issue number small interested case least large number images. choosing target representations. discussed target representations stored simple choice targets would take elements canonical basis larger formulation would similar framework dosovitskiy impractical large hand smaller formulation equivalent discriminative clustering approach bach harchaoui choosing targets makes strong assumptions nature underlying problem. indeed assumes image belongs unique class classes orthogonal. assumption might true classiﬁcation datasets figure approach takes images computes deep features convolutional network matches predeﬁned targets dimensional space. parameters network learned aligning features targets. section present model discuss relations several clustering approaches including kmeans. figure shows overview approach. also show trained massive datasets using online procedure. finally provide implementation details. interested learning visual features supervision. features produced applying parametrized mapping images. presence supervision parameters learned minimizing loss function features produced mapping given targets e.g. labels. absence supervision clear target representations thus need learn well. precisely given images jointly learn parameters mapping target vectors dimension target vectors. rest paper matrix notations i.e. denote matrix whose rows target representations matrix whose rows images slight abuse notation denote matrix features whose rows obtained applying function image independently. since features unit normalized another natural choice uniformly sample target vectors unit sphere. note dimension directly inﬂuence level correlation representations i.e. correlation inversely proportional square root using noise targets equivalent problem interpreted mapping deep features uniform distribution manifold namely ddimension sphere. using predeﬁned representations discrete approximation manifold justiﬁes restriction mapping matrices -to- assignment matrices. sense optimizing crude approximation earth mover’s distance distribution deep features given target distribution relation clustering approaches. using notations several clustering approaches share similarities method. linear case spherical k-means minimizes loss function w.r.t. i.e. guarantees data point assigned single target representation. jointly learn features assignment prevent collapsing data points single target representation. ﬁxed parameter. restricting assignment matrices prevents collapsing issue introduces global constraints suited online optimization. makes approach hard scale large datasets. updating assignment matrix directly solving optimal assignment requires evaluate distances features representations. order efﬁciently solve problem ﬁrst reduce number representations limits permutation matrices i.e. instead perform stochastic updates matrix. given batch samples optimize assignment matrix restriction batch. given subset distinct images update square matrix obtained restricting images corresponding targets. words image re-assigned target previously assigned another image batch. procedure complexity batch leading overall complexity linear number data points. perform update updating parameters features on-line manner. note simple procedure would possible would also consider unassigned representations. stochastic gradient descent. apart update assignment matrix optimization scheme standard supervised approaches i.e. batch normalization noted tygert batch normalization plays crucial role optimizing loss avoids exploding gradients. batch images ﬁrst perform forward pass compute distance images corresponding subset target representations hungarian algorithm used distances obtain optimal reassignments within batch. table comparison softmax square loss supervised object classiﬁcation imagenet. architecture alexnet. features unit normalized square loss report accuracy validation set. experiments solely focus learning visual features convnets. details required train architectures approach described below. standard tricks used usual supervised setting. deep features. ensure fair empirical comparison previous work follow wang gupta alexnet architecture. train using unsupervised loss function. subsequently test quality learned visual feature re-training classiﬁer top. transfer learning consider output last convolutional layer features razavian multi-layer perceptron krizhevsky classiﬁer. pre-processing. observe preprocessing images greatly helps quality learned features. ranzato image gradients instead images avoid trivial solutions like clustering according colors. using preprocessing surprising since hand-made features like sift based image gradients addition preprocessing also perform standard image transformations commonly applied supervised setting random cropping ﬂipping images. optimization details. project output network sphere tygert network trained batch size during ﬁrst batches constant step size. after batches linear decay step size i.e. unless mentioned otherwise permute assignments within batches every epochs. transfer learning experiments follow guideline described donahue perform several experiments validate different design choices nat. evaluate quality features comparing state-of-the-art unsupervised approaches several auxiliary supervised tasks namely object classiﬁcation imagenet object classiﬁcation detection pascal transfering features. order measure quality features measure performance transfer learning. freeze parameters convolutional layers overwrite parameters classiﬁer random gaussian weights. precisely follow training testing procedure speciﬁc datasets following donahue datasets baselines. training imagenet learn convolutional network dataset composed images belong object categories. transfer learning experiments also consider pascal addition fully supervised approaches compare method several unsupervised approaches i.e. autoencoder bigan reported donahue also compare selfsupervised approaches i.e. agrawal doersch pathak wang gupta zhang finally compare state-ofthe-art hand-made features i.e. sift fisher vectors reduce fisher vectors dimensional vector apply unit -layer top. softmax versus square loss. table compares performance alexnet trained softmax square loss. report accuracy validation set. square loss requires features unit normalized avoid exploding gradients. previously observed tygert performances similar hence validating choice loss function. effect image preprocessing. supervised classiimage pre-processing frequently used ﬁcation transformations remove information usually avoided. unsupervised case however observe preferable work simpler inputs avoids learning trivial features. particular observe using grayscale image gradients greatly helps method mentioned sec. order verify preprocessing destroy crucial information propose evaluate effect supervised classiﬁcation. also compare high-pass ﬁltering. table shows impact preprocessing methods accuracy alexnet validation imagenet. none pre-processings degrade perform signiﬁcantly meaning information related gradients sufﬁcient object classiﬁcation. experiment conﬁrms pre-processing lead signiﬁcant drop upper bound performance model. continuous versus discrete representations. compare choice target vectors commonly used clustering i.e. elements canonical basis dimensional space. discrete representation make strong assumption underlying structure problem linearly separated different classes. assumption holds imagenet giving fair advantage discrete representation. test representation range well-suited classes imagenet. matrix contains replications elements canonical basis. assumes clusters balanced veriﬁed imagenet. compare cluster-like representations continuous target vectors transfer task imagenet. using discrete targets achieves accuracy signiﬁcantly worse best performance i.e. possible explanation binary vectors induce sharp discontinuous distances representations. distances hard optimize result early convergence poorer local minima. evolution features. experiment interested understanding quality features evolves optimization cost function. during unsupervised training freeze network every epochs learn classiﬁer top. report accuracy validation imagenet. figure shows evolution performance transfer task optimize unsupervised approach. training performance improves monotonically epochs figure left measure accuracy imagenet training features different permutation rates clear trade-off optimum permutations performed every epochs. right measure accuracy imagenet training features unsupervised approach function number epochs. performance improves longer unsupervised training. unsupervised training. suggests optimizing objective function correlates learning transferable features i.e. features destroy useful class-level information. hand test accuracy seems saturate hundred epochs. suggests overﬁtting rapidly pre-trained features. effect permutations. assigning images target representations crucial feature approach. experiment interested understanding frequently update assignment. indeed updating assignment even partially relatively costly required achieve good performance. figure shows transfer accuracies imagenet function frequency updates. model quite robust choice frequency test accuracy always interestingly accuracy actually degrades slightly high frequency. possible explanation network overﬁts rapidly output leading relatively worse features. practice observe updating assignment matrix every epochs offers good trade-off performance accuracy. visualizing ﬁlters. figure shows comparison between ﬁrst convolutional layer alexnet trained without supervision. take grayscale gradient images input. visualization obtained composing sobel ﬁltering ﬁlters ﬁrst layer alexnet. unsupervised ﬁlters slightly less sharp supervised counterpart still maintain edge orientation information. nearest neighbor queries. loss optimizes distance features ﬁxed vectors. means looking distance features provide information type structure model capfigure images nearest neighbors imagenet according model using distance. query images shown nearest neighbors sorted closer further. features seem capture global distinctive structures. imagenet classiﬁcation. experiment evaluate quality features object classiﬁcation task imagenet. note setup build unsupervised features images correspond predeﬁned image categories. even though access category labels data biased towards classes. order evaluate features freeze layers last convolutional layer train classiﬁer supervision. experimental setting follows noroozi favaro compare model several self-supervised approaches unsupervised approach i.e. donahue note self-supervised approaches losses speciﬁcally designed visual features. like bigans make assumption domain structure features. table compares approaches. figure filters form ﬁrst layer alexnet trained imagenet supervision ﬁlters grayscale since grayscale gradient images input. visualization shows composition gradients ﬁrst layer. features capture relatively complex structures images. objects distinctive structures like trunks fruits well captured approach. however information always related true labels. example image bird matched images capturing information rather table comparison proposed approach state-of-the-art unsupervised feature learning imagenet. full multi-layer perceptron retrained features. compare several self-supervised approaches unsupervised approach i.e. bigan noroozi favaro uses signiﬁcantly larger amount features original alexnet. report classiﬁcation accuracy. methods even though explicitly domainspeciﬁc clues images videos guide learning. models provide performance range clear learn features. finally unsupervised deep features outperformed hand-made features particular fisher vectors sift descriptors. baseline uses slightly bigger classiﬁer performance improved bagging models. difference accuracy shows unsupervised deep features still quite state-of-the-arts among unsupervised features. transferring pascal carry second transfer experiment pascal dataset classiﬁcation detection tasks. model trained imagenet. depending task ﬁnetune layers network solely classiﬁer following donahue experiments parameters convolutional layers initialized ones obtained unsupervised approach. parameters classiﬁcation layers initialized gaussian weights. batch normalization layers data-dependent rescaling parameters table shows comparison model unsupervised approaches. results methods taken donahue except zhang imagenet classiﬁcation task performance self-supervised approaches detection classiﬁcation. among purely unsupervised approaches outperform standard approaches like autoencoders gans large margin. model also table comparison proposed approach state-of-the-art unsupervised feature learning classiﬁcation detection. either features conv ﬁne-tune whole model. compare several self-supervised unsupervised approaches. autoencoder baselines donahue report mean average prevision customary pascal voc. performs slightly better best performing bigan model experiments conﬁrm ﬁndings imagenet experiments. despite simplicity learns feature good obtained sophisticated data-speciﬁc models. paper presents simple unsupervised framework learn discriminative features. aligning output neural network low-dimensional noise obtain features state-of-the-art unsupervised learning approaches. approach explicitly aims learning discriminative features unsupervised approaches target surrogate problems like image denoising image generation. opposed self-supervised approaches make assumptions input space. makes appproach simple fast train. interestingly also shares similarities traditional clustering approaches well retrieval methods. show potential approach visual data interesting domains. finally work considers simple noise distributions alignment methods. possible direction research explore target distributions alignments informative. also would strengthen relation methods based distribution matching like earth mover distance. vincent larochelle lajoie bengio manzagol p.-a. stacked denoising autoencoders learning useful representations deep network local denoising criterion. jmlr", "year": 2017}