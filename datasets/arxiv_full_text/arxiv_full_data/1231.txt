{"title": "Deep Networks with Stochastic Depth", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91% on CIFAR-10).", "text": "abstract. deep convolutional networks hundreds layers signiﬁcant reductions error competitive benchmarks. although unmatched expressiveness many layers highly desirable test time training deep networks comes challenges. gradients vanish forward often diminishes training time painfully slow. address problems propose stochastic depth training procedure enables seemingly contradictory setup train short networks deep networks test time. start deep networks training mini-batch randomly drop subset layers bypass identity function. simple approach complements recent success residual networks. reduces training time substantially improves test error signiﬁcantly almost data sets used evaluation. stochastic depth increase depth residual networks even beyond layers still yield meaningful improvements test error convolutional neural networks arguably popularized within vision community alexnet celebrated victory imagenet competition since notable shift towards cnns many areas computer vision shift unfolds second trend emerges; deeper deeper architectures developed trained. whereas alexnet convolutional layers network googlenet layers respectively recently resnet architecture featured layers network depth major determinant model expressiveness theory practice however deep models also introduce challenges vanishing gradients backward propagation diminishing feature reuse forward propagation long training time. vanishing gradients well known nuisance neural networks many layers gradient information back-propagated repeated multiplication convolution small weights renders gradient information ineffectively small earlier layers. several approaches exist reduce eﬀect practice example careful initialization hidden layer supervision recently batch normalization diminishing feature reuse forward propagation refers analogous problem vanishing gradients forward direction. features input instance computed earlier layers washed repeated multiplication convolution weight matrices making hard later layers identify learn meaningful gradient directions. recently several architectures attempt circumvent problem direct identity mappings layers allow network pass features unimpededly earlier layers later layers long training time serious concern networks become deep. forward backward passes scale linearly depth network. even modern computers multiple state-of-the-art gpus architectures like -layer resnet require several weeks converge imagenet dataset researcher faced inherent dilemma shorter networks advantage information ﬂows eﬃciently forward backward therefore trained eﬀectively within reasonable amount time. however expressive enough represent complex concepts commonplace computer vision applications. deep networks much greather model complexity diﬃcult train practice require time patience. paper propose deep networks stochastic depth novel training algorithm based seemingly contradictory insight ideally would like deep network testing short network training. resolve conﬂict creating deep residual network architectures suﬃcient modeling capacity; however training shorten network signiﬁcantly randomly removing substantial fraction layers independently sample mini-batch. eﬀect network small expected depth training large depth testing. although seemingly simple approach surprisingly eﬀective practice. extensive experiments observe training stochastic depth substantially reduces training time test error reduction training time attributed shorter forward backward propagation training time longer scales full depth shorter expected depth network. attribute reduction test error factors shortening depth training reduces chain forward propagation steps gradient computations strengthens gradients especially earlier layers backward propagation; networks trained stochastic depth interpreted implicit ensemble networks diﬀerent depths mimicking record breaking ensemble depth varying resnets trained many attempts made improve training deep networks. earlier works adopted greedy layer-wise training better initialization schemes alleviate vanishing gradients diminishing feature reuse problems notable recent contribution towards training deep networks batch normalization standardizes mean variance hidden layers respect mini-batch. approach reduces vanishing gradients problem yields strong regularizing eﬀect. recently several authors introduced extra skip connections improve information forward backward propagation. highway networks allow earlier representations unimpededly later layers parameterized skip connections known information highways cross several layers once. skip connection parameters learned training control amount information allowed highways. residual networks simplify highway networks shortcutting identity functions. simpliﬁcation greatly improves training eﬃciency enables direct feature reuse. resnets motivated observation neural networks tend obtain higher training error depth increases large values. counterintuitive network gains parameters therefore better function approximation capabilities. authors conjecture networks become worse function approximation gradients training signals vanish propagated many layers. propose skip connections network. formally denotes output layer represents typical convolutional transformation layer obtain denotes identity transformation assume relu transition function fig. illustrates example function consists multiple convolutional batch normalization layers. output dimensions match authors redeﬁne linear projection reduce dimensions id−) match f−). propagation rule allows network pass gradients features back forth layers identity transformation dropout. stochastically dropping hidden nodes connections popular regularization method neural networks. notable example dropout multiplies hidden activation independent bernoulli random variable. intuitively dropout reduces eﬀect known coadaptation hidden nodes collaborating groups instead independently producing useful features; also makes analogy training ensemble exponentially many small networks. many follow works empirically successful dropconnect maxout dropin similar dropout stochastic depth interpreted training ensemble networks diﬀerent depths possibly achieving higher diversity among ensemble members ensembling depth. diﬀerent dropout make network shorter instead thinner motivated diﬀerent problem. anecdotally dropout loses eﬀectiveness used combination batch normalization experiments various dropout rates show dropout gives practically improvement used -layer resnets batch normalization. view previous approaches extremely valuable consider proposed training stochastic depth complimentary eﬀorts. fact experiments show training stochastic depth indeed eﬀective resnets batch normalization. learning stochastic depth based simple intuition. reduce eﬀective length neural network training randomly skip layers entirely. achieve introducing skip connections fashion resnets however connection pattern randomly altered minibatch. mini-batch randomly select sets layers remove corresponding transformation functions keeping identity skip connection. throughout architecture described architecture already contains skip connections straightforward modify isolates beneﬁts stochastic depth resnet identity connections. next describe network architecture explain stochastic depth training procedure detail. resnet architecture. following construct network functional composition residual blocks encoding update rule fig. shows schematic illustration resblock. example consists sequence layers conv-bn-relu-conv-bn conv stand convolution batch normalization respectively. construction scheme adopted experiments except imagenet bottleneck block detailed typically ﬁlters convolutional layers stochastic depth aims shrink depth network training keeping unchanged testing. achieve goal randomly dropping entire resblocks training bypassing transformations skip connections. denote bernoulli random variable indicates whether resblock active inactive further denote survival probability resblock deﬁnition bypass resblock multiplying funcresblock non-negative ﬁnal relu transition function input output conv-bn-relu sequence begins architecture ﬁrst resblock. non-negative inputs relu transition function acts identity. survival probabilities hyper-parameters training procedure. intuitively take similar values neighboring resblocks. option uniformly obtain single hyper-parameter fig. schematic illustration. linearly decaying survival probability originates intuition earlier layers extract low-level features used later layers therefore reliably present. section perform detailed empirical comparison uniform decaying assignments conclude linear decay rule preferred training stochastic depth surprisingly stable respect throughout expected network depth. forward-backward pass transformation bypassed probability leading network reduced depth. stochastic depth number eﬀective resblocks training denoted becomes random variable. expectation given average number resblocks recover resnet blocks test time. reduction depth signiﬁcantly alleviates vanishing gradients information loss problem deep resnets. note connectivity random updates signiﬁcantly shorter networks direct paths individual layers. provide empirical demonstration eﬀect section training time savings. resblock bypassed speciﬁc iteration need perform forward-backward computation gradient updates. forward-backward computation dominates training time stochastic depth signiﬁcantly speeds training process. following calculations above approximately training time could saved linear decay rule timings practice using implementation consistent analysis computational savings obtained switching uniform probability lowering accordingly. fact fig. shows resnet stochastic depth obtains test error constant depth counterpart cifar- gives speedup. implicit model ensemble. addition predicted speedups also observe signiﬁcantly lower testing errors experiments comparison resnets constant depth. explanation performance improvements training stochastic depth viewed training ensemble resnets implicitly. layers either active inactive resulting table test error resnets trained stochastic depth compared competitive methods previously published name denotes standard data augmentation. resnet constant depth refers reproduction experiments stochastic depth testing requires small modiﬁcations network. keep functions active throughout testing order utilize fulllength network model capacity. however training functions active fraction updates corresponding weights next layer calibrated survival probability. therefore need re-calibrate outputs given function expected number times participates training forward propagation update rule becomes implementation details. data sets compare results resnets proposed stochastic depth original constant depth competitive benchmarks. linear decay rule throughout. experiments report test error epoch lowest validation error. best comparisons construction scheme described case cifar- -layer resnet used cifar- except network -way softmax output. model contains three groups residual blocks diﬀer number ﬁlters feature size group stack residual blocks. numbers ﬁlters three groups respectively. transitional residual blocks i.e. ﬁrst residual block second third group output dimension larger input dimension. following replace identity connections blocks average pooling layer followed zero paddings match dimensions. implementations torch code reproduce results publicly available github https//github.com/yueatsprograms/stochastic_depth. cifar-. cifar- dataset -by- color images representing classes natural scene objects. training test contain images respectively. hold images validation remaining training samples. horizontal ﬂipping translation pixels standard data augmentation techniques adopted experiments following common practice baseline resnet trained epochs mini-batch size initial learning rate divided factor epochs weight decay momentum nesterov momentum dampening suggested stochastic depth network structure optimization settings exactly baseline. settings chosen match setup results shown table resnets constant depth result competitive error test set. resnets trained stochastic depth yield relative improvement result test error. knowledge signiﬁcantly lower best existing single model performance cifar- prior submission without resorting fig. left test error svhn corresponding results column three table right test error cifar- using -layer resnets. points lowest validation errors highlighted case. massive data augmentation fig. shows test error function epochs. point selected lowest validation error circled approaches. observe resnets stochastic depth yield lower test error also slightly higher ﬂuctuations cifar-. similar cifar- cifar- contains -by- color images train-test split classes. baseline method experimental settings exactly cifar-. constant depth resnet yields test error already state-of-the-art cifar- standard data augmentation. adding stochastic depth drastically reduces error best published single model performance knowledge also experiment cifar- cifar- without data augmentation. resnets constant depth obtain cifar- cifar- respectively. adding stochastic depth yields consistent improvements datasets resulting test errors respectively. svhn. format street view house number dataset contains -by- colored images cropped house numbers google street view. task classify digit center. digits training test easier samples additional training. following common practice training samples perform data augmentation. classes randomly select samples training additional forming validation samples total. preprocess baseline network layers. trained epochs beginning learning rate divided epochs depth learning rate schedule selected optimizing validation error baseline many trials. baseline obtains competitive result however seen fig. starts overﬁt beginning second phase learning rate continues overﬁt training. stochastic depth error improves second-best published result svhn knowledge training time comparison. compare training eﬃciency constant depth stochastic depth resnets used produce previous results. table shows training time settings linear decay rule stochastic depth consistently gives speedup conﬁrms analysis section fig. corresponding section hyper-parameter sensitivity empirical analysis. training -layer resnet. tried learn cifar using aggressively deep resnet layers. expected extremely deep network overﬁtted training ended test error worse layer network. repeat experiment -layer network constant stochastic depth. train epochs learning rate ﬁrst epochs warm-up network facilitate initial convergence restore divide epochs results summarized fig. fig. similar resnets constant depth layers yields test error worse -layer constant depth resnet. contrast trained stochastic depth extremely deep resnet performs remarkably well. want highlight trends comparing -layer nets shows training stochastic depth leads relative improvement; comparing networks trained stochastic depth shows increasing architecture layers yields improvement previous record-low test error without sign overﬁtting shown fig. best knowledge lowest known test error cifar- moderate image augmentation ﬁrst time network layers shown reduce test error consider ﬁndings highly encouraging hope training stochastic depth enable researchers leverage extremely deep architectures future. imagenet. ilsvrc classiﬁcation dataset consists classes images total million training validation testing. following common practice report validation errors. follow build -layer resnet bottleneck residual blocks. input output dimensions match skip connection uses learned linear projection mismatching dimensions identity transformation dimensions. implementation based github repository fb.resnet.torch optimization settings theirs except batch size instead spread batch among gpus train constant depth baseline epochs obtain ﬁnal error stochastic depth obtain error epoch slightly higher. observe fig. downward trend validation error stochastic depth still strong previous experience could beneﬁt training. computational saving epochs still ﬁnish almost total time epochs baseline. reaches ﬁnal error also kept baseline running epochs. reaches ﬁnal error fig. ﬁrst convolutional layer’s mean gradient magnitude epoch training. vertical dotted lines indicate scheduled reductions learning rate factor cause gradients shrink. resnet words anonymous reviewer current generation models imagenet still diﬀerent regime cifar. although seems immediate beneﬁt applying stochastic depth particular architecture possible stochastic depth lead improvements imagenet larger models community might soon able train capacities increase. section provide insights stochastic depth presenting series analytical results. perform experiments support hypothesis stochastic depth eﬀectively addresses problem vanishing gradients backward propagation. moreover demonstrate robustness stochastic depth respect hyper-parameter. improved gradient strength. stochastically dropping layers training reduces eﬀective depth gradient back-propagation performed keeping test-time model depth unmodiﬁed. result expect training stochastic depth reduce vanishing gradient problem backward step. empirically support this compare magnitude gradients ﬁrst convolutional layer ﬁrst resblock without stochastic depth cifar- data set. fig. shows mean absolute values gradients. large drops indicated vertical dotted lines scheduled learning rate division. observed magnitude gradients network trained stochastic depth always larger especially learning rate drops. seems support claim stochastic depth indeed signiﬁcantly reduces vanishing gradient problem enables network trained eﬀectively. another indication eﬀect left panel fig. observe test error resnets constant depth approximately plateaus ﬁrst drop learning rate stochastic depth still improves performance even learning rate drops second time. supports stochastic depth combines beneﬁts shortened network training deep models test time. hyper-parameter sensitivity. survival probability hyperparameter method. although used throughout experiments still worth investigating sensitivity stochastic depth respect hyper-parameter. compare test error -layer resnet varying values linear decay uniform assignment rules cifar- data fig. make following observations assignment rules yield better results baseline properly; linear decay rule outperforms uniform rule consistently; linear decay rule relatively robust ﬂuctuations obtains competitive results ranges even rather small survival probability e.g. stochastic depth linear decay still performs well giving reduction training time. shows stochastic depth save training time substantially without compromising accuracy. heatmap right shows test error varied network depth. surprisingly deeper networks better valley heatmap along diagonal. deep enough model necessary stochastic depth signiﬁcantly outperform baseline although shorter networks still beneﬁt less aggressive skipping. reduces network depth training expectation maintaining full depth testing time. training stochastic depth allows increase depth network well beyond layers still obtain reduction test error. simplicity practicality hope training stochastic depth become tool deep learning toolbox help researchers scale models previously unattainable depths capabilities. acknowledgements. thank anonymous reviewers kind suggestions. kilian weinberger supported grants iis- iis- efri-. huang supported international postdoctoral exchange fellowship program china postdoctoral council supported cornell university oﬃce undergraduate research. also thank mates matthew kusner shuang useful interesting discussions. deng dong socher l.j. fei-fei imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference ieee imagenet classiﬁcation advances neural information sermanet eigen zhang mathieu fergus lecun overfeat integrated recognition localization detection using convolutional networks. arxiv preprint arxiv. springenberg j.t. dosovitskiy brox riedmiller striving simplicity convolutional net. arxiv preprint arxiv. szegedy sermanet reed anguelov erhan vanhoucke rabinovich going deeper convolutions. proceedings ieee conference computer vision pattern recognition. h˚astad computational limitations small-depth circuits. bengio simard frasconi learning long-term dependencies gradient descent diﬃcult. neural networks ieee transactions zeiler zhang y.l. fergus regularization neural networks using dropconnect. dasgupta mcallester eds. proceedings international conference machine learning volume jmlr workshop conference proceedings graham fractional max-pooling. arxiv preprint arxiv. agostinelli hoﬀman sadowski baldi learning activation functions improve deep neural networks. arxiv preprint arxiv. snoek rippel swersky kiros satish sundaram patwary adams r.p. scalable bayesian optimization using deep neural networks. arxiv preprint arxiv. srivastava r.k. greﬀ schmidhuber training deep networks. advances neural information processing systems. c.y. gallagher p.w. generalizing pooling functions arxiv preprint netzer wang coates bissacco a.y. reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning. volume granada spain", "year": 2016}