{"title": "Discovering Causal Signals in Images", "tag": ["stat.ML", "cs.CV"], "abstract": "This paper establishes the existence of observable footprints that reveal the \"causal dispositions\" of the object categories appearing in collections of images. We achieve this goal in two steps. First, we take a learning approach to observational causal discovery, and build a classifier that achieves state-of-the-art performance on finding the causal direction between pairs of random variables, given samples from their joint distribution. Second, we use our causal direction classifier to effectively distinguish between features of objects and features of their contexts in collections of static images. Our experiments demonstrate the existence of a relation between the direction of causality and the difference between objects and their contexts, and by the same token, the existence of observable signals that reveal the causal dispositions of objects.", "text": "paper establishes existence observable footprints reveal causal dispositions object categories appearing collections images. achieve goal steps. first take learning approach observational causal discovery build classiﬁer achieves state-of-the-art performance ﬁnding causal direction pairs random variables given samples joint distribution. second causal direction classiﬁer effectively distinguish features objects features contexts collections static images. experiments demonstrate existence relation direction causality difference objects contexts token existence observable signals reveal causal dispositions objects. modern computer vision algorithms excel answering questions observable properties scene image?. achieved leveraging correlations pixels image features across large datasets images. however nuanced understanding images arguably requires ability reason scene depicted image would change response interventions. since list possible interventions long complex ﬁrst step reason intervention removing object. consider counterfactual questions what would scene look like remove car? what would scene look like remove bridge? hand ﬁrst intervention seems rather benign. removing could argue figure goal reveal causal relationships pairs real entities composing scenes world apply novel observational causal discovery technique joint distribution pair related proxy variables computed applying cnns image pixels. since variables expected highly correlated presence corresponding real entities appearance causation between proxy variables suggests causal link real world entities themselves score suggests presence cars causes presence wheels real world.) rest scene depicted image would remain invariant. hand second intervention seems severe. bridge removed scene would general make little sense observe ﬂoating weightlessly river. thus understand presence bridge effect presence car. reasoning similar counterfactuals allows begin asking image? question course poorly deﬁned answer linked causal relationship bridge car. example presence bridge causes presence sense bridge there would either interventional semantics meant causation align current approaches literature causal dispositions discussed causal relations objects present single image representing particular scene. order deploy statistical techniques must work large collection images representing variety scenes. similar objects different causal relationships different scenes. instance image show passing bridge instead bridge. dispositional semantics causation provide address difﬁculty. framework causal relations established objects exercise causal dispositions sometimes informally called powers objects. instance bridge power provide support power cross bridge. although objects present scene necessarily exercise powers foundation dispositional theory causation causal relationships manifestations powers objects. since list potential causal dispositions long complex list possible interventions restrict attention interventions affect presence certain objects scene. particular count number images causal dispositions objects categories exercised manner objects category would disappear remove objects category objects category cause presence objects category greater converse deﬁnition induces network asymmetric causal relationships object categories represents average real-world scenes would modiﬁed make certain objects disappear. fundamental question addressed paper determine whether asymmetric causal relationship inferred statistics observed image datasets. hypothesis image datasets carry observable statistical signal revealing asymmetric relationship object categories results causal dispositions. statistical computer vision algorithms reason causal structure world. small feat given debated statistics whether infer causality purely statistical information without performing interventions. focus contribution establish existence causal signals using newly proposed method. contrast make engineering contribution advancing state-of-the-art standard computer vision tasks using signals beyond scope present paper. object features context features since image datasets provide labels describing causal dispositions objects cannot resort supervised learning techniques causal signal forward hypothesis instead take indirect approach described below. features computed ﬁnal layers convolutional neural network often indicate presence well localized object-like feature scene depicted image study. various techniques developed investigate object-like features appear scene look like image therefore examine large collections images representing different objects interest cats dogs trains buses cars people. locations objects images given form bounding boxes. object interest distinguish between object features context features. deﬁnition object features mostly activated inside bounding object interest context features mostly activated outside bounding object interest. independently parallel also distinguish causal features anticausal features causal features cause presence object scene whereas anticausal features caused presence object scene. made distinction object context features indirect approach relies second hypothesis hypothesis exists observable statistical dependence object features anticausal features. statistical dependence context features causal features nonexistent much weaker. expect hypothesis true many features caused presence object interest fact parts object itself hence likely contained inside bounding box. instance presence often causes presence wheels. contrast context object interest either cause caused presence object. instance asphaltlike features cause presence car’s shadow caused presence car. importantly empirical support favour hypothesis translates support favour hypothesis plan large collection images provide empirical evidence favour hypothesis order must effectively determine object category features causal anti-causal. manner would support hypothesis consequently hypothesis exposition organized follows. discussing related literature section introduces basics causal inference observational data. section proposes algorithm neural causation coefﬁcient able learn causation corpus labeled data. shown outperform previous state-of-the-art causeeffect inference. section makes distinguish causal anticausal features collections images. hypothesized show consistent relationship anticausal features object features. finally section closes exposition offering conclusions directions future research. experiments described paper depend crucially properties features computed convolutional layers zeiler show ﬁnal convolutional layers often interpreted objectlike features. work weak supervision suggests features accurately localized. also build growing literature discussing discovery causal relationships observational data particular neural causation coefﬁcient related offers superior performance learned end-to-end data. notion causal anticausal features inspired believe work ﬁrst observational causal discovery technique targets causal dispositions objects. causation computer vision object least four recent works. pickup observational causal discovery techniques determine direction time video playback. lebeda transfer entropy study causal relationship object camera motions video data. fire video data annotated object status actions infer perceptual causality. work chalupka closer work addresses causation issues images. however work deploys interventional experiments target causal relationships labelling process pixel manipulations result different labels whereas target causal relationships scenes purely observational perspective. critical difference leads different conceptual technological challenges. randomized experiments gold standard causal inference like child drop probe nature gravity experiments rely interacting world reveal causal relations variables interest. experiments expensive unethical impossible conduct must discern cause effect using observational data only without ability intervene domain observational causal discovery. absence assumptions determination causal relations random variables given samples joint distribution fundamentally impossible however still possible determine plausible causal structure practice. joint distributions occur real world different causal interpretations equally likely. causal direction typical variables interest leave detectable signature joint distribution. shall exploit insight build classiﬁer determining cause-effect relation random variables samples joint distribution. aims infer whether particular assumed drawn models causal model anticausal model figure exempliﬁes family models additive noise model effect variable nonlinear function cause variable plus independent random noise second consider observational sample monotone function. causal relationship deterministic noisebased footprints previous paragraphs rendered useless. assume uniform distribution. then probability density function effect increases whenever derivative decreases depicted figure loosely speaking shape effect distribution thus independent mechanism example satisﬁed correct causal direction violated wrong causal direction again asymmetry renders cause distinguishable effect here relevant footprint form independence density possible continue manner considering classes models adding footprints detect causation case. however engineering maintaining catalog causal footprints tedious task catalog likely incomplete. next section thus proposes neural networks learn causal footprints directly data. learn causal footprints data follow pose cause-effect inference binary classiﬁcation task. input patterns effectively scatterplots similar shown figures data point using data form train neural network classify samples probability distributions causal anticausal. since input patterns ﬁxed-dimensional vectors bags points borrow inspiration literature kernel mean embedding classiﬁers construct feedforward neural network form noise mechanism distributions independent. interpreted informal statement includes types independences. independence between cause mechanism formalized independence input variable mechanism independence data source mechanism mapping cause effect. formalized either probabilistically terms algorithmic complexity incarnation uniformitarianism processes nature ﬁxed agnostic distributions causal inputs. second independence cause noise. standard assumption structural equation modeling related causal sufﬁciency. essentially assumption violated causal model small include additional variables terms believing assumptions amounts believing spurious correlations. choices violated anticausal direction violation often leave observable statistical footprint rendering cause effect distinguishable observational data alone exactly causal footprints develop statistical tests them? illustrate types observable causal footprints. first consider linear additive noise model cause noise independent uniform random variables bounded range mechanism linear function crucially impossible construct linear additive noise model cause noise independent random variables illustrated figure variance noise variable varies across different locations cause variable therefore assumption satisﬁed correct causal direction violated wrong causal direction asymmetry makes cause distinguishable artiﬁcially generated data. turns advantageous gives easy access unlimited data. following describe process generate synthetic cause-effect data along training procedure demonstrate performance real-world cause-effect data. synthesis training data causal signals differ signiﬁcantly correlation structures exploited modern computer vision algorithms. particular since ﬁrst second moments always symmetrical causal signals found high-order moments. speciﬁcally construct synthetic observational samples observational sample contains points. points comprising observational sample {}mi drawn heteroscedastic additive noise model vijeij manner generalize homoscedastic noise assumption ubiquitous previous literature cause terms drawn mixture gaussians distributions. construct gaussian sampling mean gaussian standard deviation gaussian followed absolute value unnormalized mixture weight gaussian followed absolute value. sample randominteger uniform. normalize mixture weights one. normalize {xij}mi zero mean unit variance. multilayer perceptrons learned jointly data. figure illustrates proposed architecture term neural causation coefﬁcient short classify sample causal anticausal maps point sample representation computes embedding vector across points classiﬁes embedding vector causal anticausal using neural network classiﬁer importantly proposed neural architecture restricted causeeffect inference used represent learn general distributions. attractive properties. first predicting cause-effect relation samples test time done efﬁciently single forward pass aggregate network. complexity operation linear number samples. contrast computational complexity state-of-the-art cubic number samples. second trained using mixtures different causal anticausal generative models linear non-linear noisy deterministic mechanisms linking causes effects. rich training allows learn diversity causal footprints simultaneously. third differentiable activation functions differentiable function. allows embed larger neural architectures regularization term encourage learning causal anticausal patterns. validation highlights crucial fact even trained abstract data discovers correct causeeffect relationship wide variety real-world datasets. abstract domain-independent causal footprints hide complex image data? disposal necessary tools verify hypotheses. following chose work twenty object categories pascal dataset ﬁrst explain select plausible causal anticausal features object category. show selected anticausal features likely object features located within object bounding selected causal features. establishes hypothesis true consequence also establish hypothesis true. datasets experiments feature extraction network trained imagenet dataset classiﬁer network trained pascal dataset networks identify causal relationships subset mscoco images representing objects belonging twenty pascal categories aeroplane bicycle bird boat bottle chair dining table horse motorbike person potted plant sheep sofa train television. datasets feature heterogeneous images possibly contain multiple objects different categories. objects appear different scales angles partially visible occluded. addition challenges control confounding selection bias effects polluting datasets images. images rescaled ensure shorter side pixels long cropped central square. selecting causal anticausal features ﬁrst task determine features scores computed feature extraction neural network represent real world entities cause presence object interest caused presence object interest effect consider feature scores computed -layer resnet trained imagenet dataset using proven implementation building features pascal dataset train independent network -unit hidden layers recognize pascal categories noise terms sampled gaussian uniform. generalize allow heteroscedastic noise multiply value smoothing spline support deﬁned equation random knots drawn uniform. noisy effect terms {yij}mi normalized zero mean unit variance. train embedding layers classiﬁcation layers followed softmax output layer. hidden layer composition batch normalization hidden neurons rectiﬁed linear unit dropout train iterations using rmsprop default parameters minibatch form given equation size lastly furncc}mi tends zero classiﬁer believes tends classiﬁer believes chose parameters monitoring validation error held-out synthetic observational samples. using held-out cross-validated dropout rate number hidden layers number hidden units layers testing test performance t¨ubingen datasets version collection hundred heterogeneous hand-collected real-world cause-effect observational samples widely used benchmark causal inference literature model highest synthetic held-out validation accuracy correctly classiﬁes cause-effect direction t¨ubingen datasets observational samples. result outperforms previous state-of-the-art observational cause-effect discovery achieves accuracy dataset effect prepare alternate versions mscoco image blacking pixels located outside bounding boxes category objects yielding object-only image blacking pixels located inside bounding boxes category objects yielding context-only image process illustrated figure compute corresponding vectors feature scores note blacking pixels constitute intervention scene represented image. merely procedure impute contribution object bounding boxes feature score. figure shows means standard deviations object-context ratios context-feature ratios estimated anticausal features causal features twenty object categories. predicted hypothesis object features related anticausal features anticausal features exhibit higher object-feature ratio causal features. since effect observed classes interest probability obtaining result chance would result indicates anticausal features useful detecting objects locations robust manner regardless context. stated hypothesis could consistent relationship context features causal features. remarkably remind reader classiﬁer depend object categories trained using synthetic data unrelated images. sanity check obtain results replacing scores correlation coefﬁcient absolute value correlation coefﬁcient. figure blacking image pixels distinguish objectfeatures context-features. show original image corresponding object-only image context-only image category dog. pixels blacked normalizing image order obtain true zero pixels. denote vector log-odds obtained using classiﬁer network. features nonlinearity odds instead class probabilities trained continuous data full support depicted figure category feature apply scatterplot representing joint distribution scores feature score category since scores computed running neural networks image pixels related direct causal relationship. however know scores highly correlated presence objects features real scene. therefore appearance causal relationship scores suggests causal relationship real world entities represent. analyze feature time values taken features appear additional source noise observed statistical dependencies much weaker synthetic training data. avoid detecting causation independent random variables variant trained augmented training addition presenting scatterplot causal directions pick random permutation generate additional uncorrelated example {xiσ yij}mi label best model kind which validation purposes achieves accuracy t¨ubingen pair benchmark. order verify hypothesis sufﬁcient show anticausal features likely object features causal features. category feature must therefore determine whether feature likely object feature context feature. figure average standard deviation object/context feature scores associated causal/anticausal feature scores twenty studied categories. average object feature score associated anticausal feature scores always higher average object feature score associated causal features. separation occur context feature scores. results strong empirical evidence favour hypoteheses probability therefore believe result establishes hypothesis true high certainty. explained section verifying hypothesis manner also implies conﬁrms hypothesis using carefully designed experiment established high order statistical properties image datasets contain information causal dispositions objects generally causal structure real world. experiment relies three main components. first synthetic scatterplots train binary classiﬁer identiﬁes plausible causal anticausal relations. second hypothesise distinction between object features context features natural scenes tween objects interest computing scores odds different objects interest. strongest causal relationships found causes chair causes plant chair causes sofa dining table causes bottle dining table causes chair dining table causes plant television causes chair television causes sofa. related distinction features cause presence object features caused presence object. finally construct experiment leverages static image datasets establish latter hypothesis true. thus conclude must therefore able effectively distinguish features causal anticausal. know signal exist envision reasonable future computer vision algorithms able perceive causal structure real world reason scenes. question signiﬁcant algorithmic advances necessary achieve goal. particular stress importance building large real-world datasets research causal inference extending data-driven techniques like causal inference variables exploring data explicit causal signals arrow time videos", "year": 2016}