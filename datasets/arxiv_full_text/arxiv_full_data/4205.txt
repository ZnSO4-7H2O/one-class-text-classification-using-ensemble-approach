{"title": "Unsupervised Domain Adaptation for Face Recognition in Unlabeled Videos", "tag": ["cs.CV", "cs.AI"], "abstract": "Despite rapid advances in face recognition, there remains a clear gap between the performance of still image-based face recognition and video-based face recognition, due to the vast difference in visual quality between the domains and the difficulty of curating diverse large-scale video datasets. This paper addresses both of those challenges, through an image to video feature-level domain adaptation approach, to learn discriminative video frame representations. The framework utilizes large-scale unlabeled video data to reduce the gap between different domains while transferring discriminative knowledge from large-scale labeled still images. Given a face recognition network that is pretrained in the image domain, the adaptation is achieved by (i) distilling knowledge from the network to a video adaptation network through feature matching, (ii) performing feature restoration through synthetic data augmentation and (iii) learning a domain-invariant feature through a domain adversarial discriminator. We further improve performance through a discriminator-guided feature fusion that boosts high-quality frames while eliminating those degraded by video domain-specific factors. Experiments on the YouTube Faces and IJB-A datasets demonstrate that each module contributes to our feature-level domain adaptation framework and substantially improves video face recognition performance to achieve state-of-the-art accuracy. We demonstrate qualitatively that the network learns to suppress diverse artifacts in videos such as pose, illumination or occlusion without being explicitly trained for them.", "text": "figure propose unsupervised domain adaptation method video face recognition using large-scale unlabeled videos labeled still images. help bridge domains introduce domain synthesized images applying image transformations speciﬁc videos motion blur labeled images simulates video frame still image. utilize images synthesized images unlabeled videos domain adversarial training. finally train video domain-adapted network domain adversarial loss well distilling knowledge pretrained reference network feature matching feature restoration image classiﬁcation losses. besides better understanding training convolutional neural networks ingredient success image-based face recognition availability large-scale datasets labeled face images collected thus source difﬁculty video face recognition attributable lack similar large-scale labeled datasets. number images used train state-of-the-art face recognition engines varies collected least different identities many contrast large-scale labeled video database publicly available date youtube face dataset contains videos total different subjects. although frames labeled difﬁcult collect dataset many variations without surge dataset size labeling effort. despite rapid advances face recognition remains clear performance still image-based face recognition video-based face recognition vast difference visual quality domains difﬁculty curating diverse large-scale video datasets. paper addresses challenges image video feature-level domain adaptation approach learn discriminative video frame representations. framework utilizes large-scale unlabeled video data reduce different domains transferring discriminative knowledge large-scale labeled still images. given face recognition network pretrained image domain adaptation achieved distilling knowledge network video adaptation network feature matching performing feature restoration synthetic data augmentation learning domain-invariant feature domain adversarial discriminator. improve performance discriminator-guided feature fusion boosts high-quality frames eliminating degraded video domainspeciﬁc factors. experiments youtube faces ijb-a datasets demonstrate module contributes feature-level domain adaptation framework substantially improves video face recognition performance achieve state-of-the-art accuracy. demonstrate qualitatively network learns suppress diverse artifacts videos pose illumination occlusion without explicitly trained them. motion objects observer video sequence powerful perceptual tasks shape determination identity recognition face recognition computer vision recent years seen success emerging approaches video face analysis several image-based face recognition engines video face recognition benchmarks arguably true efforts outpaced imagebased face recognition engines perform comparably even better human perception certain settings example veriﬁcation accuracy video frames likely degraded multiple reasons motion out-of-focus blur compression noise scale variations. approaches augment imagebased training data synthetic blur kernels noise demonstrate moderate improvement video face recognition. however attempting bridge domain images videos approach faces fundamental challenges ﬁrst non-trivial sufﬁciently enumerate types blur kernels degrade visual quality videos second possible model transformation images videos sufﬁcient accuracy. work propose data-driven method image video domain adaptation video face recognition. instead collecting labeled video face dataset utilize large-scale unlabeled video data reduce video image domains retaining discriminative power large-scale labeled still images. take advantage labeled image data section proposes transfer discriminative knowledge distilling distance metric feature matching reference network trained web-face dataset video face network avenue leverage image domain labels domain-speciﬁc data augmentation section whereby degrade still images using synthetic motion blur resolution variation video compression noise. then train vdnet able restore original representation image extracted rfnet. augmentation useful effectiveness limited fact types artifacts videos diverse enumerated. section regularize vdnet reduce domain introducing discriminator learns distinguish different domains without supervision identity labels instancelevel correspondence. trained score output discriminator measure conﬁdence similarity feature representation video frame still image. useful ability since poor performance video face recognition often attributed frames sequence substantially poor quality. consequently section proposes discriminator-guided weighted feature fusion aggregate frames video assigning higher weights image-like frames potentially better quality among others. figure illustrates proposed framework. section extensively evaluate proposed framework youtube faces dataset demonstrate performance surpasses prior state-of-the-art. present ablation studies demonstrate importance components. interestingly degradation factors blur illumination occlusions automatically emerge qualitative visualizations frames within sequence ranked domain discriminator scores. demonstrate superiority vdnet existing methods extensive experiments dataset achieving state-of-the-art veriﬁcation accuracy. also demonstrate performance gains baseline methods ijb-a dataset without supervised ﬁne-tuning. work falls class problems unsupervised domain adaptation concerns adapting classiﬁer trained source domain target domain labeled training data target domain ﬁne-tune classiﬁer. among those feature space alignment domain adversarial learning methods closely related approach. basic idea feature space alignment minimize distance domains feature space learning transformation source target features joint adaptation layer embeds features domain-invariant space speciﬁcally tzeng cnns source target domain shared weights network optimized classiﬁcation loss source domain well domain difference measured maximum mean discrepancy metric. gupta consider similar network architecture cross-modality supervision transfer. also exists body work unsupervised domain adaptation transfer adversarial learning domain difference measured discriminator network example consider crossdomain transfer images style another withinstance-level correspondence domains using adversarial loss. coupled constructs individual networks domain partially shared higher-layer parameters generator discriminator generate coherent images domains. unlike works generate images target domain consider featurelevel domain adaptation. figure illustration network architecture rfnet vdnet discriminator gray blocks denote trainable ﬁxed modules respectively. vdnet shares network architecture rfnet also initialized network parameters. trained sort frames video sequence indicating whether frame similar images compatible face recognition engine rejects frames extremely ill-suited face recognition. loss allows vdnet maintain certain degree discriminative information face identity recognition. regards network structure vdnet ﬂexible long matching feature dimensionality rfnet. practice network architecture vdnet rfnet. moreover initialize network parameters vdnet rfnet freeze network parameters higher layers maintain discriminative information learned labeled web-face images illustrated figure note complex distillation methods architectures certainly possible intent simply strong initialization vdnet choices sufﬁce. adaptation synthetic data augmentation data augmentation widely used training deep cnns limited amount training data prevents overﬁtting enhances generalization ability. addition generic data transformations random cropping horizontal ﬂips applying data transformation speciﬁc target domain shown effective generalize video frames consider data augmentation applying transformations linear motion blur image resolution variation video compression noise typical causes quality degradation video. train vdnet restore original rfnet representation image without data augmentation feature restoration loss image transformation kernel expectation distribution work consider three types image transformations following parameters linear motion blur kernel length randomly selected learning domain adversarial neural network appends domain classiﬁer high-level features introduces gradient reversal layer end-to-end learning backpropagation avoiding cumbersome minimax optimization adversarial training. goal dann transfer discriminative classiﬁer source target domain implicitly assumes label spaces domains equivalent work transfer discriminative distance metric hence restriction label space deﬁnition. addition propose domain-speciﬁc synthetic data augmentation enhance performance domain adaptation discriminator outputs feature fusion. previewed section curating large-scale video datasets identity labels onerous task exist datasets still images. makes natural consider image video domain adaptation. however representation challenging bridge blur compression motions artifacts videos. section tackles challenge introducing domain adaptation objectives allow video face recognition network trained large-scale unlabeled videos taking advantage supervised information labeled web-face images distilling knowledge feature matching take advantage labeled web-face images train vdnet distilling discriminative knowledge face recognition engine pretrained labeled web-face dataset call reference network unlike previous works distill knowledge class probabilities matching feature representations networks since access labeled videos. feature generation operator vdnet rfnet. feature matching loss deﬁned image labeling effort. however loss match representations domains global manner effect would marginal contrast domains small discriminator cannot distinguish well. result still take advantage synthetic data augmentation guide discriminator either realize difference domains discriminate additional domain differences known synthetic transformations. naturally leads different discriminator types two-way classiﬁer image synthesized image video three-way classiﬁer among image synthesized image video. two-way two-way softmax classiﬁer discriminate image domain domain synthesized images videos original images image domain synthetically degraded images well random video frames trained belong domain follows since contrast classes becomes apparent including synthetic images second class transformations video domain similar synthetic image transformations easily restored. three-way three-way softmax classiﬁer discriminate images synthesized images video frames three different categories. unlike two-way network three-way network aims distinguish video frames image domain also synthetically degraded images. therefore learn vdnet strong restoration capability synthetic transformations two-way discriminator aims additional factors variation image synthetic image video domains. taking advantage labeled training examples image domain also standard metric learning objectives learn discriminative metric generalizes low-quality images deﬁned aforementioned blur kernels. adopt n-pair loss shown effective learning deep distance metric large number classes. given pairs examples different classes n-pair loss deﬁned follows note n-pair loss could example objective function metric learning synthetic augmentation replaced standard metric learning objectives contrastive loss triplet loss adaptation domain adversarial learning although data augmentation successful many computer vision applications types transformation source target domains always known might many unknown factors variation domains. moreover modeling transformations challenging even known need resort approximation many cases. therefore difﬁcult close domains. rather attempting exhaustively enumerate approximate different types transformations domains learn large-scale unlabeled data facilitate recognition engine robust transformations. adversarial learning provides good framework approach problem whereby generator vdnet regularized close domains domain difference captured discriminator. adversarial loss domains deﬁned expectation training samples discriminator deﬁned vdnet already induces highly abstract features deep cnn. thus architecture simple three fully-connected layer networks. unlike several recent applications adversarial frameworks image translation distinguishing generated real images pixel space rather feature representations. argue desirable relative maturity feature learning face recognition opposed high-quality image generation. discount frames extremely noisy motion blur noise factors favor better recognition. discriminator already trained distinguish still images blurred ones video frames output already used conﬁdence score frame high quality image. training domain contrast image blurred image video discriminator ready provide conﬁdence score test time frame high-quality image speciﬁcally conﬁdence score discriminator aggregated feature vector video frames represented weighted average feature vectors follows note target domain images comes largescaled labeled training examples train discriminative face recognition engine. thus discriminator serves dual role guiding feature-level domain adaptation fusion weighted conﬁdence ﬁtness frame face recognition engine. implementation details face recognition engine also deep trained casia-webface dataset network architecture similar ones used contains layers convolution followed relu nonlinearities pooling layers stride average pooling layer stride except network uses strided convolution replace pooling maxout units every convolution layer instead relu layers. please supplementary material details. model trained deep metric learning objective called n-pair loss described section implementation based torch negative examples time pulling single positive example) used gpus training. faces detected aligned using keypoints gray-scale image patches randomly cropped resized face images network training. model achieves veriﬁcation accuracy labeled faces wild dataset rfnet face recognition engine parameters ﬁxed. vdnet initialized rfnet parameters updated layers except last convolution layers illustrated figure discriminator simple neural network fully-connected layers shown figure twoway networks replace output channel last fullyconnected layer three two. train network using adam optimizer learning rate setting details network architecture hyper parameters found supplementary material. evaluate performance proposed unsupervised domain adaptation framework ﬁrst providing baseline methods section performing ablation study component proposed approach youtube faces dataset section evaluate model trained dataset demonstrates generalization capabilities proposed approach presented section evalutation protocol standard application image-based face recognition engine video face recognition ﬁrst apply face recognition engine frame aggregate extracted features individual frames obtain single representation videos. baselines follow standard protocol extracting normalized features frame horizontally ﬂipped image followed temporal average pooling frames video. discriminatorguided feature fusion follow equation obtain video representation individual normalized features. compute inner product video representations similarity metric. youtube faces dataset dataset contains videos unconstrained face images different people average length frames video. folds video pairs available veriﬁcation experiments fold composed positive negative pairs overlapping identity different folds. videos training folds addition casia-webface dataset train vdnet identity label video used. networks different combinations feature matching feature restoration various data augmentation methods adversarial training discriminator-guided feature fusion presented table feature matching loss. loss enforces vdnet learn similar representations produced rfnet labeled still images contributors good initialization method. combination image classiﬁcation reduces training baseline model. effectiveness loss seen comparing accuracies model table signiﬁcant table video face recognition accuracy standard error dataset. image-classiﬁcation loss feature matching loss feature restoration loss adversarial loss applied training. feature restoration loss consider three types data augmentation namely linear motion blur scale variation jpeg compression noise best performer overlapping standard error boldfaced. performance drop observed model trained withfm loss loss evaluated number frames video. hypothesize feature restoration loss drives vdnet match representation low-quality images high-quality counterpart representation original high-quality images severely damaged causing model lose superior performance high-quality images. requiring network work well high-quality well low-quality images loss observe signiﬁcant improvement evaluated larger number frames video feature restoration loss. consider three types data augmentation described section different combinations presented model table overall loss moderately improves accuracy compared baseline models. speciﬁcally observe compression noise quite effective feature-level restoration used along linear motion blur scale variations. combined adversarial loss observe signiﬁcant improvement feature restoration loss. example model reduce relative error compared model respectively single frame video evaluation regime using randomly selected frames video evaluation. domain adversarial loss. addition feature restoration loss synthetic data augmentation domain adversarial training high-quality images videos contributes reducing domains. demonstrate effectiveness train model loss random video frames additional input source fake labels compared baseline model. shown table model consistently outperforms baseline different number random frames video. note two-way model denotes binary classiﬁcation random face images video frames without artiﬁcially degraded sample. feature restoration loss used training consider types discriminators since introduces additional data domain namely synthetic image domain besides existing domains image video. first merge synthetically degraded images video domain discriminator still two-way classiﬁer next synthetic images considered domain leads three-way discriminator among image synthetic image video comparison model trained without adversarial loss models signiﬁcantly improve recognition performance frame-level features aggregated discriminator-guided fusion three-way model improves performance highly competitive performance previous state-of-the-art face recognition engines facenet centerface different feature aggregation methods shown table note evaluation protocol prior works baseline model networks either much deeper trained signiﬁcantly larger number training images. discriminator-guided feature fusion. proposed fusion strategy selectively adopts high-quality frames discarding poorer ones order improve recognition accuracy. reﬂect quality frame feature fusion module aggregates frames video using weighted average feature vectors based normalized likelihood quantitatively qualitatively evaluate discriminator-guided fusion table figure applying three-way network model baseline model present consistent improvements. contrast fusion strategy two-way network model marginal effect. indicates three-way network learns multi-class discriminator better distinguish among input sources. table veriﬁcation rank-k identiﬁcation accuracy standard error ijb-a dataset. model compared baseline method described section image crops evaluated fused together uniform weights discriminator conﬁdence score. also evaluate removing images signiﬁcant localization errors ijb-a benchmark dataset face recognition wild. contains still images video frames sampled videos different subjects. existence video frames allows set-to-set comparison veriﬁcation opens challenge face recognition problem. challenging many factors variation pose facial expression well various image qualities. splits labeled images each. small-sized training prior works pretrain network using large-scale labeled datasets training split supervised ﬁne-tuning. also pretrained face recognition engine video face recognition network ﬁne-tuned external video data without identity information ijb-a used evaluation only. emphasis method large-scale unlabeled video data improve video face recognition rather ﬁne-tuning manually labeled data. ﬁrst remove videos training identities overlap ijb-a test set. utilize dataset without label information network training unlabeled video ﬁne-tuned network evaluated splits ijb-a test set. perform experiments settings. veriﬁcation task compares genuine impostor samples one-to-one veriﬁcation task search samples enrolled gallery. report true acceptance rate different false acceptance rates veriﬁcation rank- accuracy identiﬁcation table compared baseline model observe slightly worse performance far=. signiﬁcant improvement far=. especially discriminator-guided feature fusion used improvement becomes signiﬁcant. also evaluate following protocol observe non-negligible three-way discriminator scores. shows bottom scored frames. evident high-quality frames score higher low-quality ones. importantly observe notions quality diverse encompass factors variation pose blur lighting occlusions. supports hypothesis several causes domain images videos adversarially trained discriminator better relies enumerating possible factors. analysis reminiscent learn quality video frames unsupervised manner without identity labels whereas utilizes identity labels assign higher score frame contributes classiﬁcation. comparison unsupervised methods. study effectiveness proposed method comparison works unsupervised domain adaptation feature transform correlation alignment methods. extract features still images video frames using rfnet apply feature transform retaining total variation. coral calculate mean covariance features domains training transform features individual frames follows results table show simple feature transform method like work well since distinguish domains computing transformation matrix. hand coral demonstrates moderate improvement upon baseline matching ﬁrst second-order statistics representations domains. method also based feature distribution matching domains discriminator figure sort frames within sequence descending order respect discriminator-guided weights display showing top- bottom- instances respectively. weights shown upper-left corner frame. visualize eight video sequences illustrate following video quality degradation blurring compression noise lighting variation video shot-cut occlusion detection failure pose variation mis-alignment ﬁrst last row. amount localization errors preprocessing. removing poor quality images based localization errors obtain much higher veriﬁcation identiﬁcation results. however proposed model baseline reduced believe low-quality images mostly ﬁltered baseline method still work even better. compared previous works performance proposed method competitive under similar training testing protocols improvement expected training stronger base network larger labeled image datasets types synthetic data augmentations pose variation synthesization face recognition videos presents unique challenges paucity large-scale datasets several factors variation degrade frame quality. work address challenges proposing novel feature-level domain adaptation approach uses large-scale labeled still images unlabeled video data. distilling discriminative knowledge pretrained face recognition engine labeled still images adapting video domain synthetic data augmentation domain adversarial training learn domain-invariant discriminative representations video face recognition. furthermore propose discriminator-guided feature fusion method effectively aggregate features multiple frames effectively rank accordance suitability face recognition. demonstrate effectiveness proposed method video face veriﬁcation ijb-a benchmarks. future work exploit unsupervised domain adaptation achieve continuous improvements growing collection unlabeled videos.", "year": 2017}