{"title": "Bidirectional LSTM-CRF for Clinical Concept Extraction", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "Automated extraction of concepts from patient clinical records is an essential facilitator of clinical research. For this reason, the 2010 i2b2/VA Natural Language Processing Challenges for Clinical Records introduced a concept extraction task aimed at identifying and classifying concepts into predefined categories (i.e., treatments, tests and problems). State-of-the-art concept extraction approaches heavily rely on handcrafted features and domain-specific resources which are hard to collect and define. For this reason, this paper proposes an alternative, streamlined approach: a recurrent neural network (the bidirectional LSTM with CRF decoding) initialized with general-purpose, off-the-shelf word embeddings. The experimental results achieved on the 2010 i2b2/VA reference corpora using the proposed framework outperform all recent methods and ranks closely to the best submission from the original 2010 i2b2/VA challenge.", "text": "automated extraction concepts patient clinical records essential facilitator clinical research. reason ib/va natural language processing challenges clinical records introduced concept extraction task aimed identifying classifying concepts predeﬁned categories state-of-the-art concept extraction approaches heavily rely handcrafted features domain-speciﬁc resources hard collect deﬁne. reason paper proposes alternative streamlined approach recurrent neural network initialized general-purpose off-the-shelf word embeddings. experimental results achieved ib/va reference corpora using proposed framework outperform recent methods ranks closely best submission original ib/va challenge. patient clinical records typically contain longitudinal data patients’ health status diseases conducted tests response treatments. analysing information prove immense value clinical practice also organisation management healthcare services. concept extraction aims identify mentions medical concepts problems test treatments clinical records classify predeﬁned categories. concepts clinical records often expressed unstructured free text making automatic extraction challenging task clinical natural language processing systems. traditional approaches extensively relied rule-based systems lexicons recognise concepts interest. typically concepts represent drug names anatomical nomenclature specialized names phrases part everyday vocabularies. instance resp status interpreted response status. abbreviated phrases acronyms common within medical community many abbreviations speciﬁc meaning differ lexicons. dictionary-based systems perform concept extraction looking terms medical ontologies uniﬁed medical language system intrinsically dictionaryrule-based systems laborious implement inﬂexible cases misspellings although systems achieve high precision tend suffer recall overcome limitations various machine learning approaches proposed maximumentropy classiﬁers support vector machines) simultaneously exploit textual contextual information reducing reliance lexicon lookup state-of-the-art machine learning approaches usually follow two-step process feature engineering classiﬁcation. feature engineering task right laborious demanding expert knowledge become bottleneck overall approach. reason paper proposes highly streamlined alternative employ contemporary neural network bidirectional lstm-crf initialized general-purpose off-the-shelf word embeddings glove wordvec experimental results authoritative ib/va benchmark show proposed approach outperforms recent approaches ranks closely best literature. research date framed specialized case named-entity recognition employed number supervised semi-supervised machine learning algorithms domaindependent attributes text features hybrid models obtained cascading classiﬁers along several pattern-matching rules shown produce effective results moreover given evidence importance including preprocessing steps truecasing annotation combination. system reported highest accuracy ib/va concept extraction benchmark based unsupervised feature representations obtained brown clustering hidden semi-markov model classiﬁer however hard clustering technique brown clustering suitable capturing multiple relations words concepts. reason jonnalagadda demonstrated random indexing model distributed word representations improve clinical concept extraction. moreover jointly used word embeddings derived entire english wikipedia binarized word embeddings derived domain-speciﬁc corpora broader ﬁeld machine learning recent years witnessed proliferation deep neural networks outstanding results tasks diverse visual speech named-entity recognition main advantages neural networks traditional approaches learn feature representations automatically data thus avoiding expensive feature-engineering stage. given promising performance deep neural networks recent success unsupervised word embeddings general tasks paper sets explore state-of-the-art deep sequential model initialized general-purpose word embeddings task clinical concept extraction. formulated joint segmentation classiﬁcation task predeﬁned classes. example consider input sentence provided table notation follows widely adopted in/out/begin entity representation with instance test prbc treatment. paper approach task bidirectional lstm-crf framework word input sentence ﬁrst mapped either random vector vector word embedding. therefore provide brief description word embeddings model hereafter. word embeddings vector representations natural language words preserve semantic syntactic similarities them. vector representations generated either count-based approaches hellinger-pca trained models wordvec glove trained large unsupervised corpora general-nature documents. embedded representation word text represented real-valued vector arbitrary dimensionality recurrent neural networks family neural networks operate sequential data. take input sequence vectors output sequence class posterior probabilities intermediate layer hidden nodes also part model. value hidden node time depends current input previous hidden node ht−. recurrent connection past timeframe enables form shortterm memory makes rnns suitable prediction sequences. formally value hidden node described trained weight matrices input hidden layer past current hidden layers respectively. function sigmoid function adds non-linearity layer. eventually input output layer convolved output weight matrix eventually output normalized multi-class logistic function become proper number probability class set. therefore concept classes. although practice tends biased towards recent inputs. reason long short-term memory network incorporates additional gated memory cell store long-range dependencies bidirectional version lstm computes fort hidden representation timeframe ﬁnal representation ward created concatenating networks hidden layer regarded implicit learned feature enables concept prediction. improvement model provided performing joint decoding entire input sequence viterbi-style manner using ﬁnal output layer. resulting network commonly referred bidirectional lstm-crf ib/va natural language processing challenges clinical records include concept extraction task focused extraction medical concepts patient reports. challenge total concept-annotated reports training testing unannotated reports deidentiﬁed released participants alongside data agreement however part data longer distributed restrictions later introduced institutional review board thus table summarizes basic statistics training test data sets currently publicly available used experiments. models blindly evaluated ib/va test data using strict evaluation criterion requiring predicted concepts exactly match annotated concepts terms boundary class. facilitate replication experimental results used publicly-available library implementation lstm publicly release code. split training parts using ﬁrst training second selection hyperparameters .the hyper-parameters include embedding dimension chosen additional parameters learning drop-out rates sampled uniform distribution range weight matrices randomly initialized uniform distribution within range word embeddings either initialized randomly fetched wordvec glove approximately tokens alphanumeric abbreviated domainspeciﬁc strings available pre-trained embeddings always randomly initialized. early stopping training epochs mollify over-ﬁtting model gave best performance validation retained. accuracy reported terms micro-average score computed using conll score function table shows performance comparison state-of-the-art systems proposed bidirectional lstm-crf different initialization strategies. ﬁrst note bidirectional lstm-crf initialized glove outperforms recent approaches hand best submission ib/va challenge still outperforms approach. however based description provided results directly comparable since experiments used original dataset signiﬁcantly larger number training samples. using general-purpose pre-trained embeddings improves score percentage points random initialization. general results achieved proposed approach close many cases results achieved systems based hand-engineered features. paper explored effectiveness contemporary bidirectional lstm-crf clinical concept extraction. appealing feature approach ability provide end-to-end recognition using general-purpose off-the-shelf word embeddings thus sparing effort time-consuming feature construction. experimental results authoritative ib/va reference corpora look promising bidirectional lstm-crf outperforming recent approaches ranking closely best submission original ib/va challenge. potential improve performance would explore unsupervised word embeddings trained domain-speciﬁc resources mimic-iii corpora", "year": 2016}