{"title": "Strategic Attentive Writer for Learning Macro-Actions", "tag": ["cs.AI", "cs.LG"], "abstract": "We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner by purely interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub- sequences by learning for how long the plan can be committed to - i.e. followed without re-planing. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro- actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.", "text": "present novel deep recurrent neural network architecture learns build implicit plans end-to-end manner purely interacting environment reinforcement learning setting. network builds internal plan continuously updated upon observation next input environment. also partition internal representation contiguous subsequences learning long plan committed i.e. followed without re-planing. combining properties proposed model dubbed strategic attentive writer learn high-level temporally abstracted macroactions varying lengths solely learnt data without prior information. macro-actions enable structured exploration economic computation. experimentally demonstrate straw delivers strong improvements several atari games employing temporally extended planning strategies time general algorithm applied sequence data. also show trained text prediction task straw naturally predicts frequent n-grams demonstrating generality approach. using reinforcement learning train neural network controllers recently rapid progress number challenging control tasks much success methods attributed ability neural networks learn useful abstractions representations stream observations allowing agents generalize similar states. notably agents exploit another type structure present space controls policies. indeed sequences low-level controls lead interesting high-level behaviour agent automatically discover useful macro-actions capable efﬁcient exploration learning. discovery temporal abstractions long-standing problem reinforcement learning sequence prediction general truly scalable successful architectures exist. propose deep recurrent neural network architecture dubbed strategic attentive writer capable learning macro-actions reinforcement learning setting. unlike vast majority reinforcement learning approaches output single action observation straw maintains multi-step action plan. straw periodically updates plan based observations commits plan replanning decision points. replanning decisions well commonly occurring sequences actions i.e. macro-actions learned rewards. encourage exploration macro-actions introduce noisy communication channel feature extractor planning modules taking inspiration recent developments variational auto-encoders injecting noise level network generates randomness plans updates cover multiple time steps thereby creates desired effect. proposed architecture step towards natural decision making wherein observation generate whole sequence outputs informative enough. provides several important beneﬁts. first foremost facilitates structured exploration reinforcement learning network learns meaningful action patterns make longer exploratory steps state space second since model need process observations committed action plan learns allocate computation moments thereby freeing resources plan followed. additionally acquisition macro-actions transfer generalization related problems domain evaluate straw subset atari games require longer term planning show leads substantial improvements scores. also demonstrate generality straw architecture training text prediction task show learns frequent n-grams macro-actions task. following section reviews related work. section deﬁnes straw model formally. section describes training procedure supervised reinforcement learning cases. section presents experimental evaluation straw atari games maze navigation next character prediction tasks. section concludes. learning temporally extended actions temporal abstraction genral long standing problems reinforcement learning options framework provides general formulation. option sub-policy termination condition takes enviroment observations outputs actions termination condition met. agent picks option using policy-over-options subsequently follows termination point policy-over-options queried process continues. notice macro-action particular simpler instance options action sequence decided time macro-action initiated. options typically learned using subgoals ’pseudo-rewards’ provided explicitly given options policy-over-options learned using standard techniques treating options actions. recently demonstrated combining deep learning pre-deﬁned subgoals delivers promising results challenging environments like minecraft atari however subgoal discovery remains unsolved problem. another recent work shows theoretical possibility learning options jointly policy-over-options extending policy gradient theorem options approach tested problem. contrast straw learns macro-actions policy end-to-end fashion environment’s reward signal without resorting explicit pseudo-rewards hand-crafted subgoals. macro-actions represented implicitly inside model arising naturally interplay action commitment plans within network. experiments demonstrate model scales variety tasks next character prediction text atari games. straw deep recurrent neural network modules. ﬁrst module translates environment observations action-plan state variable represents explicit stochastic plan future actions. straw generates macro-actions committing action-plan following without updating number steps. second module maintains commitment-plan state variable determines step network terminates macro-action updates action-plan. action-plan matrix dimension corresponds time possible discrete actions. elements matrix proportional probability taking corresponding action corresponding time step. similarly commitment plan represents probabilities terminating macro-action particular step. updating plans attentive writing technique allows network focus parts plan figure schematic illustration straw playing maze navigation game. input frames indicate maze geometry corresponds position agent green goal tries reach. frame ﬁrst passed convolutional network acting feature extractor straw. rows depict plans given gate straw either updates plans commits them. current observation informative desired outputs. section formally deﬁnes model describe trained later section state network time comprised matrices ra×t matrix action-plan. element proportional probability outputting time total number possible actions maximum time horizon plan. generate action time ﬁrst column transformed distribution possible outputs softmax function. distribution sampled generate action thereby content corresponds plan future actions conceived time single matrix represents commitment-plan network. binary random variable distributed follows step plans updated otherwise means committed macro-actions deﬁned sequence outputs {at}t− produced network steps ‘on’ commitment plans rolled next step using matrix time-shift operator shifts matrix removing ﬁrst column appending column ﬁlled zeros rear. applying reﬂects advancement time. figure illustrates workﬂow. notice commitment network doesn’t compute forward pass thereby saving computation. attentive planning. important assumption underpins usage macro-actions observation reveals enough information generate sequence actions. complexity sequence length vary dramatically even within environment. therefore network focus part plan current observation informative desired actions. achieve this apply differentiable attentive reading writing operations attention deﬁned temporal dimension. technique originally proposed image generation instead used update plans image domain attention operates spatial extent image reading writing pixel values. operates temporal extent plan used read write action probabilities. differentiability attention model makes possible train standard backpropagation. array gaussian ﬁlters applied plan yielding ‘patch’ smoothly varying location zoom. total number possible actions parameter determines temporal resolution patch. grid one-dimensional gaussian ﬁlters positioned plan specifying co-ordinates grid center stride distance adjacent ﬁlters. stride controls ‘zoom’ patch; larger stride larger area original plan visible attention patch lower effective resolution figure schematic illustration action-plan update. given network produces action. write operation creates update action-plan patch attention parameters scaling shifting action-patch according patch ﬁltering performed along temporal dimension only. vector attention parameters i.e. grid position stride standard deviation gaussian ﬁlters. deﬁne attention operations follows write operation takes patch ra×k attention parameters produces matrix size contains patch scaled positioned according analogously read operation takes full plan together attention parameters outputs read patch ra×k extracted according direct readers details. action-plan update. feature representation observation given previous state straw computes update action-plan using algorithm linear functions two-layer perceptron. figure gives illustration update commitment-plan update. introduce module partitions action-plan macroactions deﬁning temporal extent current action-plan followed without re-planning. commitment-plan updated time action-plan i.e. otherwise rolled next time step operator. unlike planning module updated additively overwritten completely using following equations attentive writing operation used gaussian ﬁlter attention patch therefore scalar high value high value chosen attention parameters deﬁne time step re-planning guaranteed happen. vector size ﬁlled shared learnable bias deﬁnes probability re-planning earlier step implied notice used multiplicative gate algorithm allows retroactive credit assignment training gradients write operation time directly architecture deﬁned capable generating macro-actions. section describes macro-actions structured exploration. introduce straw-explorer version straw noisy communication channel feature extractor straw planning modules. activations last layer feature extractor. regress parameters gaussian distribution sampled. vector scalar. injecting noise level network generates randomness level plan updates cover multiple time steps. effect reinforced commitment forces strawe execute plan experience outcome instead rolling update back next step. section demonstrate signiﬁcantly improves score games like frostbite pacman. lout domain speciﬁc differentiable loss function deﬁned network’s output. supervised problems like next character prediction text lout deﬁned negative likelihood correct output. discuss reinforcement learning case later section. extra terms regularisers. ﬁrst cost communication noisy channel deﬁned divergence latent distributions prior since latent distribution gaussian natural choice prior gaussian zero mean standard deviation one. last term penalizes re-planning encourages commitment. reinforcement learning consider standard setting agent interacting environment discrete time. step agent observes state environment selects action ﬁnite possible actions. environment responds state scalar reward process continues terminal state reached αkrt+k+. agent’s behaviour deﬁned policy mapping state space action space. straw produces distribution possible actions passing ﬁrst column action-plan softmax function softmax. action produced sampling output distribution. recently proposed asynchronous advantage actor-critic method directly optimizes policy agent. requires value function estimator variance reduction. estimator produced number ways example separate neural network. natural solution architecture would create value-plan containing estimates. keep architecture simple efﬁcient simply auxiliary action plan corresponds value function estimation. participates attentive reading writing update thereby sharing temporal attention action-plan. plan split action part estimator softmax applied action sampled. policy gradient update lout deﬁned follows entropy policy stimulates exploration primitive actions. network contains random variables pass gradients through. employ re-parametrization trick ∇ct− proposed goal experiments demonstrate straw learns meaningful useful macroactions. three domains increasing complexity supervised next character prediction text maze navigation atari games experimental setup architecture. read write patches dimensional layer perceptron hidden units. time horizon strawe gaussian distribution structured exploration -dimensional. ablative analysis choices provided section feature representation state space particular domain. mazes atari convolutional neural lstm text. provide details corresponding sections. baselines. experiments employ baselines simple feed forward network recurrent lstm network. directly regresses action probabilities value function estimate feature representation. lstm architecture widely used recurrent network demonstrated perform well suite reinforcement learning problems hidden units inputs feature representation observation previous action agent. action probabilities value function estimate regressed hidden state. optimization. method reinforcement learning experiments. shown achieve state-of-the-art results several challenging benchmarks trajectory backpropagation time forward passes network terminal signal received. optimization process runs asynchronous threads using shared rmsprop. hyper-parameters straw lstm baselines. method experiments using randomly sampled hyperparameters. learning rate entropy penalty sampled loguniform interval. learning rate linearly annealed sampled value explore straw behaviour sample coding cost loguniform replanning penalty loguniform. stability clip advantage rt−v methods straw propagate gradients commitment module planning module deﬁne training epoch million observations. straw general sequence prediction architecture. demonstrate capable learning output patterns complex structure present qualitative experiment next character prediction using penn treebank dataset actions case correspond emitting characters macroactions sequences. experiment lstm receives one-hot-encoding characters input. straw module connected top. omit noisy gaussian channel task fully supervised require exploration. actions correspond emitting characters. network trained stochastic gradient descent using supervised negative log-likelihood loss step feed character model updates lstm representation update straw plans according commitment plan trained model record macro-actions sequences characters produced straw committed plan. straw adequately learns structure data macro-actions correspond common n-grams. figure plots frequent macro-action length produced straw. x-axis rank frequency macro-action used straw y-axis it’s actual frequency rank -gram. notice straw correctly learned predict frequent -grams ’the’ ’ing’ ’and’ investigate macro-actions model learns whether useful reinforcement learning conduct experiment random mazes domain. maze grid-world types cells walls corridors. corridor cells marked goal. agent receives small negative reward every step double tries move wall. receives small positive reward steps goal episode terminates. otherwise episode terminates steps. therefore maximize return agent reach goal fast possible. every training episode position walls goal starting position agent randomized. agent fully observes state maze. remainder section presents experiments domain. strawe version structured exploration. feature representation -layer ﬁlters stride followed rectiﬁer nonlinearity. ﬁrst experiment train strawe agent random maze environment. evaluate trained agent novel maze ﬁxed geometry randomly varying start goal locations. visualize positions strawe terminates macro-actions re-plans. figure shows maze intensity corresponds ratio re-planning events cell normalized total amount visits agent. notice corners areas close junctions highlighted. demonstrates straw learns adequate temporal abstractions domain. next experiment test whether temporal abstractions useful. second experiment uses larger random mazes. goal placed arbitrarily agent’s starting position learning becomes extremely hard neither lstm strawe reliably learn good policy. introduce curriculum goal ﬁrst positioned close starting location moved away progress training. precisely position goal using random walk starting point agent. increase random walks length every million training steps starting although task gets progressively harder temporal abstractions remain same. learnt early make adaptation easy. figure plots episode reward training steps straw lstm optimal policy given dijkstra algorithm. notice learn good policy approximately epochs task still simple. goal moves away lstm strong decline reward relative optimal policy. contrast strawe effectively uses macro-actions learned early stays close optimal policy harder stages. demonstrates temporal abstractions learnt straw useful. table comparison straw strawe lstm baselines atari games. score averaged agents architecture epochs training. section presents results subset atari games. compared methods used architecture input preprocessing action repeat feature representation convolutional layer ﬁlters size stride followed convolutional layer ﬁlters size stride followed fully connected layer hidden units. three hidden layers followed rectiﬁer nonlinearity. architecture difference pre-processing stage keep colour channels. chose games require degree planning exploration opposed purely reactive ones pacman frostbite alien amidar hero q-bert crazy climber. also added reactive breakout game sanity check. table. shows average performance agents architecture epochs training. action repeat epoch corresponds four million frames straw strawe reach highest score games. especially strong frostbite pacman amidar. frostbite strawe achieves improvement lstm score. notice structured exploration improves performance games one. straw strawe perform worse lstm breakout although still achieves good score likely breakout requiring fast reaction action precision rather planning exploration. baseline scores worst every game apart hero best. although difference large still surprising result might fewer parameters learn. figure visualization macro-actions frostbite. bottom ﬁgure shows frames straw re-plans; white black zero. notice macro-actions correspond meaningful high-level actions like jumping picking ﬁsh. figure re-planning behaviour amidar. agent controls yellow ﬁgure scores points exploring maze avoiding green enemies. time agent explored area left planning head right score points. enemy blocks straw retreats left drasticly changing plan resotres original plan step path clear heads right. notice enemy near plans smaller macro-actions high value closer origin. examples macro-actions learned strawe frostbite shown ﬁgure notice macro-actions correspond jumping picking ﬁsh. figure demonstrates re-planning behaviour amidar. game agent explores maze yellow avatar. cover maze territory without colliding green ﬁgures notice straw agent changes plan enemy comes near blocks way. backs resumes initial plan enemy takes turn danger avoided. also notice enemy near agent plans shorter macro-actions indicated commitment-plan figure shows percentage time best strawe agent committed plan different games. training progresses strawe learns commit converges stable regime epochs. exception breakout meticulous control required beneﬁcial re-plan often. shows strawe capable adapting environment learns temporal abstractions useful particular case. figure ablative analysis pacman game. figures plot episode reward seen frames different conﬁgurations straw. left right varying action patch size varying code length varying replanning modules. notice models trained epochs unlike models table trained epochs. examine different design choices made investigate impact ﬁnal performance. figure presents performance curves different versions straw pacman game trained epochs. left right ﬁrst plot shows straw performance given different resolution action patch. higher complex update straw generate action plan. second plot shows inﬂuence gaussian channel’s dimensionality used structured exploration results show higher dimensionality generally beneﬁcial beneﬁts rapidly saturate. third plot investigate different possible choices re-planning mechanism compare straw simple modiﬁcations re-plans every step commits plan random amount steps re-planning every step less elegant also much computationally expensive less data efﬁcient. results demonstrate learning commit plan re-plan beneﬁcial. introduced strategic attentive writer architecture demonstrated ability implicitly learn useful temporally abstracted macro-actions end-to-end manner. furthermore straw advances state-of-the-art several challenging atari domains require temporally extended planning exploration strategies also ability learn temporal abstractions general sequence prediction. opens fruitful direction tackling important problem area sequential decision making broadly. references pierre-luc bacon doina precup. option-critic architecture. nips deep workshop marc bellemare yavar naddaf joel veness michael bowling. arcade learning environment alex graves. generating sequences recurrent neural networks. arxiv preprint arxiv. karol gregor danihelka alex graves danilo rezende daan wierstra. draw recurrent neural sepp hochreiter jürgen schmidhuber. long short-term memory. neural computation leslie pack kaelbling. hierarchical learning stochastic domains preliminary results. icml diederik kingma welling. auto-encoding variational bayes. iclr tejas kulkarni karthik narasimhan ardavan saeedi joshua tenenbaum. hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation. arxiv preprint arxiv. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. icml ronald parr stuart russell. reinforcement learning hierarchies machines. nips doina precup. temporal abstraction reinforcement learning. thesis university massachusetts jürgen schmidhuber. neural sequence chunkers. technical report john schulman sergey levine philipp moritz michael jordan pieter abbeel. trust region policy", "year": 2016}