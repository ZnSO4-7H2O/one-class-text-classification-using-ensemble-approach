{"title": "TensorFlow Distributions", "tag": ["cs.LG", "cs.AI", "cs.PL", "stat.ML"], "abstract": "The TensorFlow Distributions library implements a vision of probability theory adapted to the modern deep-learning paradigm of end-to-end differentiable computation. Building on two basic abstractions, it offers flexible building blocks for probabilistic computation. Distributions provide fast, numerically stable methods for generating samples and computing statistics, e.g., log density. Bijectors provide composable volume-tracking transformations with automatic caching. Together these enable modular construction of high dimensional distributions and transformations not possible with previous libraries (e.g., pixelCNNs, autoregressive flows, and reversible residual networks). They are the workhorse behind deep probabilistic programming systems like Edward and empower fast black-box inference in probabilistic models built on deep-network components. TensorFlow Distributions has proven an important part of the TensorFlow toolkit within Google and in the broader deep learning community.", "text": "many developments probabilistic programming languages limited progress backend systems probability distributions. despite fundamental necessity computing log-densities sampling statistics well manipulations composed part probabilistic programs. existing distributions libraries lack modern tools necessary deep probabilistic programming. absent batching automatic diﬀerentiation support compiler optimization composability numerical operations higher-level modules neural network layers eﬃcient memory management. illustration. figure presents template variational autoencoder tensorflow python api; generative model binarized mnist digits trained using amortized variational inference figure implements standard version bernoulli decoder fully factorized gaussian encoder gaussian prior. changing lines figure implements stateof-the-art architecture pixelcnn++ decoder convolutional encoder prior pushed autoregressive ﬂows figures demonstrate power tensorflow distributions fast idiomatic modules composed express rich deep structure. section demonstrates applications kernel density estimators pixelcnn home tensorﬂow.org; source github.com/tensorﬂow/tensorﬂow. namespaces tf=tensorflow; tfd=tf.contrib.distributions; tfb=tf.contrib.distributions.bijectors. tensorflow distributions library implements vision probability theory adapted modern deeplearning paradigm end-to-end diﬀerentiable computation. building basic abstractions oﬀers ﬂexible building blocks probabilistic computation. distributions provide fast numerically stable methods generating samples computing statistics e.g. density. bijectors provide composable volumetracking transformations automatic caching. together enable modular construction high dimensional distributions transformations possible previous libraries workhorse behind deep probabilistic programming systems like edward empower fast black-box inference probabilistic models built deep-network components. tensorflow distributions proven important part tensorflow toolkit within google broader deep learning community. success deep learning—and particular deep generative models—presents exciting opportunities probabilistic programming. innovations deep probabilistic models inference algorithms enabled successes perceptual domains images text audio advanced scientiﬁc applications understanding mouse behavior learning causal factors genomics synthesizing drugs materials reifying applications code falls naturally under scope probabilistic programming systems systems build manipulate computable probability distributions. within past year languages deep probabilistic programming edward expanded deep-learning research enabling forms experimentation faster iteration cycles improved reproducibility. contributions. tensorflow distributions deﬁnes abstractions distributions bijectors. distributions provides collection distributions fast numerically stable methods sampling computing densities many statistics. bijectors provides collection composable transformations eﬃcient volume-tracking caching. tensorflow distributions integrated tensorflow ecosystem example compatible tf.layers neural architectures tf.data data pipelines serving distributed computing tensorboard visualizations. part ecosystem tensorflow distributions inherits maintains integration tensorflow graph operations automatic diﬀerentiation idiomatic batching vectorization device-speciﬁc kernel optimizations accelerator support cpus gpus tensor processing units tensorflow distributions widely used diverse applications. used production systems within google google brain deepmind research prototypes. backend edward xla-compatible whenever possible uses diﬀerentiable random number generators sampling call device-speciﬁc kernels implemented c++. functions tensor inputs also exploit vectorization batches multivariate distributions able exploit additional vectorization structure. numerically stable. operations tensorflow distributions numerically stable across half single double ﬂoating-point precisions tf.float tf.float tf.float). class constructors validate_args numerical asserts. sorflow distributions maintains idioms inputs outputs following tensor-in tensor-out pattern outputs preserving inputs’ dtype preferring statically determined shapes. similarly tensorflow distributions library dependencies besides numpy manages dtypes supports tf-style broadcasting simpliﬁes shape manipulation. universality. order fast distribution abstraction makes explicit restriction class computable distributions. namely distribution oﬀer sample log_prob implementations computable expected polynomial time. example multinomial-logisticnormal distribution fails meet contract. also precludes supporting distributional calculus. example convolution generally analytic distributions support __add__ operator share domain approximate inference. distributionsdo implement approximate inference approximations properties statistics. example monte carlo approximation entropy disallowed function computes analytical deterministic bound entropy allowed. compound distributions conjugate priors dirichlet-multinomial allowed. marginal distribution hidden markov model also allowed since hidden states eﬃciently collapsed forward-backward algorithm statistical computing language provides comprehensive collection probability distributions. inherits classic language accumulated user contributions decades. collection goal comprehensiveness ease user contribution. tensorflow distributions diﬀers object-oriented instead functional enabling manipulation distribution objects; operations also designed fast diﬀerentiable. developers tensorflow ecosystem also google-employed meaning beneﬁt uniﬁcation ecosystem. example popular glmnet packages support speciﬁc distributions model-speciﬁc algorithms; distributions support generic tensorflow optimizers training/testing. scipy stats module python collects probability distributions statistical functions tensorflow’s primary demographic machine learning users researchers; typically python. subsequently modelled scipy; mimicks tensorflow’s modelled numpy. beyond design details implementations drastically diﬀer. example tensorflow distributions enables arbitrary tensordimensional vectorization builds operations tensorflow computational graph supports automatic differentiation accelerators. tensorflow distributions also introduces innovations higher-order distributions distribution functionals bijectors stan math templated library numerical statistical functions automatic differentiation backend stan probabilistic programming language diﬀerent stan focus enabling deep probabilistic programming. lead innovations bijectors shape semantics higher-order distributions distribution functionals. computationally tensorflow distributions also enables variety non-cpu accelerators compiler optimizations static dynamically executed graphs. users subclass distribution relaxing properties. future work support operation cases satisﬁes goals e.g. analytic subset stable distributions normal levy. tensorflow distributions provides collection approximately distributions fast numerically stable methods sampling density many statistics. describe properties innovations below. tensorflow distributions object-oriented. distribution implementation subclasses distribution base class. base class follows public calls private design pattern where e.g. public sample method calls private _sample implemented subclass. handles basic argument validation simpliﬁes sharing function documentation. example normal) eﬀectively equivalent normal scale=). distributions selfdocumenting argument names concise lexicon. never greek prefer example scale rate concentration rather alternative parameterizations sometimes lead argument zoo. migitate this distinguish between cases. numerical stability necessitates them distributions permit mutually exclusive parameters example bernoulli accepts logits probs poisson accepts rate log_rate; neither permits specifying both. alternative parameterizations structural specify diﬀerent classes multivariatenormaltril multivariatenormaldiag multivariatenormaldiagpluslowrank implement multivariate normal distributions diﬀerent covariance structures. dtype defaults ﬂoats ints depending distribution’s support precision given parameters’. distributions integer-valued support tf.int*. distributions real-valued support tf.float*. distinction exists mathematical consistency; practice integer-valued distributions often used indexes tensors. among checks validate_args=true limits integer distributions’ support integers exactly representable same-size ieee ﬂoats i.e. integers canexceed signiﬁcand_bits. allow_nan_stats=trueoperations return nans; otherwise error raised. minimum supported distributions implement following methods sample generate random outcome tensors log_prob compute natural logarithm probability density function random outcome tensors batch_shape_tensor event_shape_tensor describe dimensions random outcome tensors returned tensors. supported distributions implement many additional methods including survival_function quantile mean variance entropy. distribution base class automates implementation related functions prob given log_prob log_survival_fn given log_cdf distribution-speciﬁc statistics permitted; example wishart implements expected determinant matrix variates would meaningful univariate distributions. eﬃciently computable. member functions expected polynomial-time complexity. further vectorized optimized sampling routines tensorflow distributions also favors eﬃcient parameterizations example favor multivariatenormaltrilwhose covariance parameterized outer product lower triangular matrix multivariatenormalfullcovariancewhich requires cholesky factorization. statistically consistent. sample monte carlo approximation statistic converges statistic’s implementation number samples approaches similarly equal derivative respect input; sample equal distribution uniform sampling followed inverse cdf. analytical. member functions analytical excluding sample non-deterministic. example mixture implements analytic expression entropy lower bound method entropy_lower_bound; exact entropy intractable. however method function’s implementation uses monte carlo estimate qualify non-analytical. fixed properties. keeping tensorflow idioms distribution instances ﬁxed shape semantics dtype class methods class properties throughout instance’s lifetime. member functions side eﬀects tensorflow graph. note unlike statefulness exchangeable random primitives sampling memoize calls lazily evaluate inﬁnite-dimensional data structures. handle distributions future work involve sampling method returns another distribution storing samples. preserves immutability enabling marginal representations completely random measures chinese restaurant process compound distribution dirichlet process multinomial distribution namely sample computes pólya urn-like scheme caching number customers table. make distributions fast challenge enable arbitrary tensor-dimensional vectorization. allows users properly utilize multi-threaded computation well array data-paths modern accelerators. however probability distributions involve number notions diﬀerent dimensions; often conﬂated thus diﬃcult vectorize. batch shape describes independent identically distributed draws. namely parameterizations distribution. enables common case machine learning batch examples modelled distribution. figure illustrates partition latent code variational autoencoder combining three shapes single tensor enables eﬃcient idiomatic vectorization broadcasting. member functions comply distribution’s shape semantics dtype. another example initialize batch three multivariate normal distributions batch member diﬀerent mean. partitioning tensor dimensions sample batch event dramatically simpliﬁes user code naturally exploiting vectorization. example describe monte carlo approximation normal-laplace compound distribution procedure automatically vectorized internal calculations tensors represents diﬀerently parameterized normal distributions. sigma automatically broadcast; value applied pointwise thus eliding copies. variational inference algorithms minimize divergence another distribution /h]] using gradient-based optimization. compute monte carlo approximation higher-order distributions distributions functions distributions. deviation tensor-in tensor-out pattern enables modular inherited construction enormous number distributions. outline three examples running illustration composing distributions. transformeddistributionis distribution consisting base distribution invertible diﬀerentiable transform base distribution instance distribution class transform instance bijector class invert transforms exponential distribution natural-log affine negates. general algebraic relationships random variables powerful enabling distributions inherit method implementations parents shape inference parameters construction time. parameter dimensions beyond necessary single distribution instance always determine batch shape. inference event shapes typically required distributions often know priori; example normal univariate. hand multinomial infers event shape rightmost dimension logits argument. dynamic sample batch ranks allowed conﬂate shape semantics dynamic event ranks allowed design choice consistency. note event shape reﬂects numerical shape mathematical deﬁnition dimensionality. example categorical scalar event shape ﬁnite integers; one-to-one mapping exists onehotcategorical vector event shape one-hot vectors. distributions non-scalar event shape include dirichlet wishart sampling common applications distribution. optimize speed implement sampling registering device-speciﬁc kernels tensorflow operations. also well-established algorithms random number generation. example draws normal box-muller transform return independent pair normal samples independent pair uniform samples implementations exist. draws gamma rejection sampling algorithm marsaglia tsang currently implementation exists. reparameterization. distributions employ reparameterization_type property annotates interaction automatic differentiation sampling. currently annotations fully reparameterized reparameterized. illustrate fully reparameterized consider dist normal. sample dist.sample implemented internally tf.random_normal); scale loc. sample reparameterized smooth function parameters scale parameterfree sample contrast common gamma sampler reparameterized uses accept-reject scheme makes samples depend non-smoothly parameters composed tensorflow fully reparameterized distribution enables end-to-end automatic diﬀerentiation functions samples. common case loss depending expectations form function example batches event dimensions. given distribution instance dist batch dimensions independent builds vector valued distribution whose event components default rightmost batch dimension dist. mixture deﬁnes probability mass function mixing weights provided categorical distribution input components arbitrary distributions support. components batches family mixturesamefamilysimpliﬁes construction components rightmost batch dimension. building image_dist mutual information divergence measures kullback-leibler csiszár-morimoto -divergence multi-distribution divergences distance metrics integral probability metrics implement distribution functionals methods distributions. example write functionals normal distributions described distributions sources stochasticity collect properties probability distributions. bijectors deterministic transformations random outcomes equal importance library. consists composable transformations manipulating distributions eﬃcient volume-tracking caching pre-transformed samples. describe properties innovations below. seek minimally invasive interface manipulating distributions. implementing every transformation every distribution results combinatorial blowup realistically maintainable. policy unlikely keep pace pace research. lack encapsulation exacerbates already complex ideas/code discourages community contributions. deep learning rich high-dimensional densities typically invertible volume-preserving mappings mappings fast volume adjustments we’d like eﬃciently idiomatically support them. easy design implement validate. illustrate ﬂexibility transformeddistribution section ability simply swap functions applied distribution surprisingly powerful asset. programmatically bijector distinction enables encapsulation modular distribution constructions inherited fast method implementations. statistically bijectors enable exploiting algebraic relationships among random variables. inverse jacobian bijector subclass corresponds function transformeddistribution uses bijector automate details transformation density allows deﬁne many distributions terms existing ones. bijector implements transform tensor input tensor’s shape changes; tensor presumed random outcome possessing distribution shape semantics. three functions characterize tensor transformed used transformeddistribution.log_prob adjust volume changes transformation. certain settings numerically stable implement forward_log_det_jacobian.because forward reverse jacobians diﬀer sign either implemented. prob. bijectors constant jacobian affine transformeddistribution automatically implements statistics mean variance entropy. following example implements aﬃne-transformed laplace distribution. distribution learnable tf.variables affine parameterized essentially cholesky covariance matrix. makes multivariate construction computationally eﬃcient numerically stable; bijector caching even eliminate back substitution. bijectors compose using higher-order bijectors chain invert. figure illustrates powerful example arflow method composes sequence autoregressive permutation bijectors compactly describe autoregressive shape transformeddistribution implement functions compute event batch shapes. bijectors change tensor’s shape. implement forward_event_shape_tensorand inverse_event_shape_tensor.each takes input shape returns shape representing tensor’s event/batch shape forward inverse transformations. excluding higher-order bijectors currently softmaxcentered changes shape. invert bijector eﬀectively doubles number bijectors swapping forward inverse. useful situations gumbel construction section also useful transforming constrained continuous distributions onto unconstrained real-valued space. example bijector framework extends non-injective transformations i.e. smooth coverings formally smooth covering continuous function entire domain where ignoring sets measure zero domain partitioned ﬁnite union restriction diffeomorphism. examples include absvalue square. implement inverse method return inverse tuple. smooth covering bijectors easily build halfdistributions allocate probability mass positive half-plane original distribution. example build half-cauchy distribution described abstractions distributions bijectors. recall figures showed power combining abstractions changing simple state-of-the-art variational auto-encoders. below show additional applications tensorflow distributions part tensorflow ecosystem. kernel density estimator nonparametric estimator unknown probability distribution kernel density estimation provides fundamental smoothing operation ﬁnite samples useful across many applications. tensorflow distributions kdes ﬂexibly constructed mixturesamefamily. given ﬁnite points write performs softplus-inversion robustly transform gamma unconstrained. enables component automated algorithms automatic diﬀerentiation variational inference u-turn sampler operate real-valued spaces unconstraints expand scope. bijectors automatically cache input/output pairs operations including jacobian. advantageous inverse calculation slow numerically unstable easily implementable. cache occurs computing probability results sample. distribution associated caching lowers cost computing since bijector caching nominal; zero memory computational cost. bijector base class merely replaces graph node another already existing node. since existing node already execution dependency cost caching during graph construction. caching computationally numerically beneﬁcial importance sampling algorithms compute expectations. weight drawn samples’ reciprocal probability. namely caching also nicely complements black-box variational inference algorithms procedures approximate posterior distribution computes log_prob samples. setting sample’s preimage known without computing changing callable extends alternative distribution-based kernels. example lambda multivariatenormaltril multivariate kernel alternative distributions lambda independent. concept also applies bootstrap techniques employ parametric bootstrap stratiﬁed sampling replace equal mixing weights. figure stochastic recurrent neural network state space model nonlinear dynamics. transition mimicks recurrent tanh cell omission multi-layered. describe tensorflow distributions enables edward backend. particular note non-goals tensorflow distributions accomplished higherlevel abstractions. here edward wraps tensorflow distributions random variables associating distribution random outcome tensor tensorflow graph. enables calculus tensorflow applied directly edward random variables; non-goal tensorflow distributions. example figure implements stochastic recurrent neural network whose hidden state random formally time steps model speciﬁes joint distribution time step observed real-valued sequence associated fully-visible autoregressive distribution images batch pixel images small imagenet. variable pixelcnn distribution. makes higher-order autoregressive distribution takes input python callable number autoregressive steps. python callable takes currently observed features returns per-time step distribution. -tf.reduce_sum) loss function maximum likelihood training; x.sample generates images. intend expand distribution interface include supports e.g. real-valued positive unit interval etc. class property. also exploring possibility exposing exponential family structure example providing separate unnormalized_log_prob log_normalizer methods appropriate. part trend towards hardware-accelerated linear algebra working ensure distribution bijector methods compatible tpus including special functions gamma well rejection sampling-based whileloop based sampling mechanisms also natively support distributions sparsetensors. using distributions build estimators natural ergonomic. tpuestimator particular extends estimator conﬁgurations tpus together distributions estimators provide simple scalable platform eﬃciently deploying training evaluation prediction diverse hardware network topologies. tensorflow distributions library implements vision probability theory adapted modern deeplearning paradigm end-to-end diﬀerentiable computation. distributions provides collection distributions fast numerically stable methods sampling computing densities many statistics. bijectors provides collection composable transformations eﬃcient volume-tracking caching. although tensorﬂow distributions relatively already seen widespread adoption inside outside google. external developers built design probabilistic programming statistical systems including edward greta further distribution bijector used design basis similar functionality pytorch computational graph framework well pyro zhusuan probabilistic programming systems thank jasper snoek feedback comments kevin murphy thoughtful discussions since beginning tensorflow distributions. supported google ph.d. fellowship machine learning adobe research fellowship. references martín abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoﬀrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg mané rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda viégas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous systems. hps//www.tensorflow.org/ software available tensorﬂow.org. carpenter andrew gelman matthew hoﬀman daniel goodrich michael betancourt marcus brubaker jiqiang peter allen riddell. stan probabilistic programming language. journal statistical software carpenter matthew hoﬀman marcus brubaker daniel peter michael betancourt. stan math library reverse-mode automatic diﬀerentiation c++. arxiv preprint arxiv. imre csiszár. eine informationstheoretische ungleichung ihre anwendung beweis ergodizitaet markoﬀschen ketten. magyer tud. akad. mat. kutato int. koezl. josé miguel hernández-lobato jorge aguilera-iparraguirre timothy hirzel ryan adams alán aspuru-guzik. automatic chemical design using data-driven continuous representation molecules. arxiv preprint arxiv. goodfellow jean pouget-abadie mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. neural information processing systems. matthew johnson david duvenaud alex wiltschko ryan adams sandeep datta. composing graphical models neural networks structured representations fast inference. neural information processing systems. norman jouppi cliﬀ young nishant patil david patterson gaurav agrawal raminder bajwa sarah bates suresh bhatia boden borchers rick boyle pierre-luc cantin cliﬀord chao chris clark jeremy coriell mike daley matt jeffrey dean gelb tara vazir ghaemmaghami rajendra gottipati william gulland robert hagmann richard doug hogberg john robert hundt hurt julian ibarz aaron jaﬀey alek jaworski alexander kaplan harshit khaitan andy koch naveen kumar steve lacy james laudon james diemthu chris leary zhuyuan kyle lucke alan lundin gordon mackean adriana maggiore maire mahony kieran miller rahul nagarajan ravi narayanaswami kathy thomas norrie mark omernick narayana penukonda andy phelps jonathan ross matt ross amir salek emad samadiani chris severn gregory sizikov matthew snelham souter steinberg andy swing mercedes gregory thorson tian horia toma erick tuttle vijay vasudevan richard walter walter wang eric wilcox hyun yoon. in-datacenter performance analysis tensor processing unit. arxiv preprint arxiv. diederik kingma salimans rafal jozefowicz chen improving variational ilya sutskever welling. inference inverse autoregressive flow. neural information processing systems. kucukelbir dustin tran rajesh ranganath andrew gelman david blei. automatic diﬀerentiation variational inference. journal machine learning research chris maddison andriy mnih whye teh. concrete distribution continuous relaxation discrete random variables. international conference learning representations. john schulman nicolas heess theophane weber pieter abbeel. gradient estimation using stochastic computation graphs. neural information processing systems. jiaxin jianfei chen shengyang yucen yihong yuhao zhou. zhusuan library bayesian deep learning. arxiv preprint arxiv. michael spivak. comprehensive introduction dustin tran matthew hoﬀman saurous eugene brevdo kevin murphy david blei. deep probabilistic programming. international conference learning representations. dustin tran kucukelbir adji dieng maja rudolph dawen liang david blei. edward library probabilistic modeling inference criticism. arxiv preprint arxiv. aäron oord sander dieleman heiga karen simonyan oriol vinyals alex graves kalchbrenner andrew senior koray kavukcuoglu. wavenet generative model audio. arxiv preprint arxiv. christian naesseth francisco ruiz scott linderman david blei. reparameterization gradients acceptance-rejection sampling algorithms. artiﬁcial intelligence statistics. rajesh ranganath sean gerrish david blei. black variational inference. artiﬁcial intelligence statistics. salimans andrej karpathy chen diederik kingma. pixelcnn++ improving pixelcnn discretized logistic mixture likelihood modiﬁcations.", "year": 2017}