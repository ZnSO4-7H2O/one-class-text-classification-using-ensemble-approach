{"title": "Massively Parallel Methods for Deep Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "cs.DC", "cs.NE"], "abstract": "We present the first massively distributed architecture for deep reinforcement learning. This architecture uses four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed store of experience. We used our architecture to implement the Deep Q-Network algorithm (DQN). Our distributed algorithm was applied to 49 games from Atari 2600 games from the Arcade Learning Environment, using identical hyperparameters. Our performance surpassed non-distributed DQN in 41 of the 49 games and also reduced the wall-time required to achieve these results by an order of magnitude on most games.", "text": "arun nair praveen srinivasan blackwell cagdas alcicek rory fearon alessandro maria vedavyas panneershelvam mustafa suleyman charles beattie stig petersen shane legg volodymyr mnih koray kavukcuoglu david silver {arunsnair prav blackwells cagdasalcicek roryf ademaria darthveda mustafasul cbeattie legg vmnih korayk davidsilver google.com google deepmind london present ﬁrst massively distributed architecture deep reinforcement learning. architecture uses four main components parallel actors generate behaviour; parallel learners trained stored experience; distributed neural network represent value function behaviour policy; distributed store experience. used architecture implement deep q-network algorithm distributed algorithm applied games atari games arcade learning environment using identical hyperparameters. performance surpassed non-distributed games also reduced wall-time required achieve results order magnitude games. deep learning methods recently achieved state-ofthe-art results vision speech domains mainly ability automatically learn high-level features supervised signal. recent advances reinforcement learning successfully combined deep learning value function approximation using deep convolutional neural network represent action-value function speciﬁcally method training deep q-networks known enabled learn control policies complex environments high dimensional images inputs method outperformed human professional many games atari platform using network architecture hyper-parameters. however previously applied single-machine architectures practice leading long training times. example took days train algorithm single atari game work goal build distributed architecture enables scale deep reinforcement learning algorithms exploiting massive computational resources. main advantages deep learning computation easily parallelized. order exploit scalability deep learning algorithms made extensive hardware advances gpus. however recent approaches focused massively distributed architectures learn data parallel therefore outperform training single machine example distbelief framework distributes neural network parameters across many machines parallelizes training using asynchronous stochastic gradient descent distbelief used achieve stateof-the-art results several domains shown much faster single training existing work distributed deep learning focused exclusively supervised unsupervised learning. paper develop architecture reinforcement learning paradigm. architecture consists four main components parallel actors generate behaviour; parallel learners trained stored experience; distributed neural network represent value function behaviour policy; distributed experience replay memory. unique property agent inﬂuences training data distribution interacting environment. order generate data deploy multiple agents running parallel interact multiple instances environment. actor store record past experience effectively providing distributed experience replay memory vastly increased capacity compared single machine implementation. alternatively experience explicitly aggregated distributed database. addition generating data distributed actors explore state space effectively actor behaves according slightly different policy. conceptually distinct distributed learners reads samples stored experience experience replay memory updates value function policy according given algorithm. speciﬁcally focus paper variant algorithm applies asgd updates parameters q-network. distbelief parameters q-network also distributed many machines. applied distributed framework known gorila create massively distributed version algorithm. applied gorila games atari platform. outperformed single games outperformed human professional games. gorila also trained much faster nondistributed version terms wall-time reaching performance single roughly times faster games. several previous approaches parallel distributed signiﬁcant part work focused distributed multi-agent systems approach many agents taking actions within single shared environment working cooperatively achieve common objective. computation distributed sense decentralized control algorithms focus effective teamwork emergent group behaviors. another paradigm explored concurrent reinforcement learning agent interact parallel inherently distributed environment e.g. optimize interactions multiple users internet. goal quite different distributed concurrent paradigms simply seek solve single-agent problem efﬁciently exploiting parallel computation. mapreduce framework applied standard solution methods policy evaluation policy iteration value iteration distributing computation involved large matrix multiplications however work narrowly focused batch methods linear function approximation perhaps closest prior work parallelization canonical sarsa algorithm multiple machines. machine instance agent environment running simple reinforcement learning algorithm changes parameters linear function approximator periodically communicated using peerto-peer mechanism focusing especially parameters changed most. contrast architecture allows client-server communication separation acting learning parameter updates; furtherexploit much richer function approximators using distributed framework deep learning. distbelief distributed system training large neural networks massive amounts data efﬁciently using types parallelism. model parallelism different machines responsible storing training different parts model used allow efﬁcient training models much larger feasible single machine gpu. data parallelism multiple copies replicas model trained different parts data parallel allows efﬁcient training massive datasets single process. brieﬂy discuss main components distbelief architecture central parameter server model replicas. central parameter server holds master copy model. parameter server apply incoming gradients replicas model requested send latest copy model replicas. parameter server sharded across many machines different shards apply gradients independently shards. replica maintains copy model trained. copy could sharded across multiple machines example model single machine. replicas calculate gradients given mini-batch send parameter server periodically query parameter server updated version model. replicas send gradients request updated parameters independently hence synced parameters given time. recently algorithm developed practice much stable combined deep qnetworks like q-learning iteratively solves bellman equation adjusting parameters q-network towards bellman target. however shown figure differs q-learning ways. first uses experience replay time-step agent’s interaction environment stores experience tuple replay memory et}. second maintains separate q-networks current parameters parameters respectively. current parameters updated many times time-step copied parameters iterations. every update iteration current parameters updated minimise mean-squared bellman error respect parameters optimizing following loss function update tuple experience sampled uniformly replay memory sample current parameters updated stochastic gradient descent algorithm. speciﬁcally adjusted direction sample gradient loss respect introduce gorila framework massively distributed reinforcement learning. gorila architecture shown figure contains following components actors. reinforcement learning agent must ultimately select actions apply environment. refer process acting. gorila architecture contains nact different actor processes applied nact corresponding instantiations environment. actor generates trajectories experience within environment result actor visit different parts state space. quantity experience generated actors time-steps approximately nact. figure algorithm composed three main components q-network deﬁnes behavior policy target q-network used generate target values loss term replay memory agent uses sample random transitions training q-network. reinforcement learning paradigm agent interacts sequentially environment goal maximising cumulative rewards. step agent observes state selects action receives reward agent’s policy maps states actions deﬁnes behavior. goal agent maximize expected total reward rewards discounted factor return time time-step. speciﬁcally γt−trt step episode termit=t nates. action-value function expected return observing state taking action under policy optimal action-value function maximum possible value achieved policy action-value function obeys argmax fundamental recursion known bellman equation core ideas behind reinforcement learning represent action-value function using function approximator neural network parameters so-called q-network optimized approximately solve bellman equation. example q-learning algorithm iteratively updates action-value function towards sample bellman target well-known q-learning algorithm highly unstable combined non-linear function approximators deep neural networks figure gorila agent parallelises training procedure separating learners actors parameter server. single experiment several learner processes exist continuously send gradients parameter server receive updated parameters. time independent actors also parallel accumulate experience update q-networks parameter server. actor contains replica q-network used determine behavior example using \u0001-greedy policy. parameters q-network synchronized periodically parameter server. experience replay memory. experience tuples generated actors stored sampled either local global experience replay memory learner applies off-policy algorithm minibatch experience order generate gradient vector gradients communicated parameter server; parameters q-network updated periodically parameter server. parameter server. like distbelief gorila architecture uses central parameter server maintain distributed representation q-network parameter vector split disjointly across nparam different machines. machine responsible applying gradient updates subset parameters. parameter server receives gradients learners applies gradients modify parameter vector using asynchronous stochastic gradient descent algorithm. gorila architecture provides considerable ﬂexibility number ways agent parallelized. possible parallel acting generate large quantities data global replay database process data single serial learner. contrast possible single actor generating data local replay memory multiple learners process data parallel learn effectively possible experience. however avoid individual component becoming bottleneck gorila architecture general allows arbitrary numbers actors learners parameter servers generate data learn data update model scalable fully distributed fashion. simplest overall instantiation gorila consider subsequent experiments bundled mode one-to-one correspondence actors replay memory learners bundle actor generating experience local replay probability take random action else argmax execute action environment observe reward next state st+. store update parameters parameter server. sample random mini-batch tuple target terminal calculate loss compute gradients respect network parameters using equation send gradients parameter server. every global steps sync parameters parameter server. memory store experience learner updates parameters based samples experience local replay memory. communication bundles parameters learners communicate gradients parameter server; q-networks actors learners periodically synchronized parameter server. consider speciﬁc instantiation gorila architecture implementing algorithm. described previous section algorithm utilizes copies q-network current q-network parameters target q-network parameters algorithm extended distributed implementation gorila follows. parameter server maintains current parameters actors learners contain replicas current q-network synchronized parameter server every acting step. learner additionally maintains target q-network learner’s target network updated parameter server every gradient updates central parameter server. learners generate gradients using gradient given equation however gradients applied directly instead communicated parameter server. parameter server applies updates accumulated many learners. training algorithm designed ensure stability training neural networks reinforcement learning training using large cluster machines running multiple tasks poses additional challenges. gorila implementation uses additional safeguards ensure stability presence disappearing nodes slowdowns network trafﬁc slowdowns individual machines. safeguard parameter determines maximum time delay local parameters parameters parameter server. gradients older threshold discarded parameter server. additionally actor/learner keeps running average standard deviation absolute loss data sees discards gradients absolute loss higher mean plus several standard deviations. finally used adagrad update rule evaluated gorila conducting experiments atari games using arcade learning environment atari games provide challenging diverse reinforcement learning problems agent must learn play games directly video input changes score provided rewards. closely followed experimental setup using preprocessing network architecture. preprocessed images downsampling extracting luminance channel. q-network convolutional layers followed fully-connected hidden layer. input network obtained concatenating images four previous preprocessed frames. ﬁrst convolutional layer ﬁlters size stride second convolutional layer ﬁlters size stride third ﬁlters size stride next layer fully-connected output units followed linear fully-connected output layer single output unit valid action. hidden layer followed rectiﬁer nonlinearity. experiments gorila used nparam nlearn nact bundled mode. replay memory size million frames used \u0001-greedy behaviour policy annealed ﬁrst million global updates. learner syncs parameters target network every parameter updates performed parameter server. used types evaluations. ﬁrst follows protocol established dqn. trained agent evaluated episodes game trained random number frames skipped repeatedly taking null nothing action giving control agent order ensure variation initial conditions. agents allowed play game frames whichever came ﬁrst scores averaged episodes. refer evaluation procedure null starts. second evaluation method call human starts aims measure well agent generalizes states trained introduced random starting points sampled human professional’s gameplay game. evaluate agent starting points game total frames played counting frames human played reach starting point. total score accumulated agent averaged obtain evaluation score. order make easier compare results games greatly varying range scores present results scale score obtained random agent score obtained professional human game player. random agent selected actions uniformly random evaluated using selected hyperparameter values performing informal search games breakout pong seaquest ﬁxed games. trained gorila times game using ﬁxed hyperparameter settings random network initializations. following periodically evaluated model training kept best performing network parameters ﬁnal evaluation. average ﬁnal evaluations runs compare mean evaluations human expert scores. ﬁrst compared gorila agents trained days single agents trained days. figure shows normalized scores human starts evaluation. using human starts gorila outperformed single games given roughly half training time single dqn. games gorila obtained double score single games gorila dqn’s score times higher. similarly using original null starts evaluation gorila outperformed single games. results show parallel training signiﬁcantly improved performance less training time. also better results human starts compared null starts suggest gorila especially good generalizing potentially unseen states compared single dqn. figure illustrates improvements generalization showing gorila scores human starts normalized respect scores human starts gorila scores null starts normalized figure performance gorila agent atari games human starts evaluation compared performance scores normalized expert human performance. font color indicates method higher score. *not showing scores asterix asteroids double dunk private wizard gravitar human starts scores less random agent baselines. also showing video pinball human expert scores less random agent scores. scores null starts fact gorila performs level similar superior human professional games despite starting states sampled human play. possible reason improved generalization signiﬁcant increase number states gorila sees using parallel actors. next look performance gorila improved training. figure shows quickly gorila reached performance single quickly gorila reached best score under human starts evaluation. gorila surpassed best single scores games hours games hours hours games hours roughly order magnitude reducfigure performance gorila agent atari games human starts null evaluations normalized respect human start null scores respectively. ﬁgure shows generalization improvements gorila compared dqn. *using score human starts random agent score asterix asteroids double dunk private wizard gravitar human starts scores less random agent scores. showing double dunk scores random agent scores negative. **not showing null scores montezuma revenge human start scores random agent scores tion training time required reach single process score. games gorila achieved best score days games performance keeps improving longer training time paper introduced ﬁrst massively distributed architecture deep reinforcement learning. gorila architecture acts learns parallel using distributed replay memory distributed neural network. applied gorila asynchronous variant state-ofthe-art algorithm. single machine previously achieved state-of-the-art results challenging suite atari games previously known whether good performance would continue scale additional computation. leveraging massive parallelism gorila signiﬁcantly outperformed singlegpu games; achieving best results domain date. gorila takes step towards fulﬁlling promise deep learning scalable architecture performs better better increased computation memory. references bellemare marc naddaf yavar veness joel bowling michael. arcade learning environment evaluation platform general agents. arxiv preprint arxiv. dahl george dong deng acero alex. context-dependent pre-trained deep neural networks large-vocabulary speech recognition. audio speech language processing ieee transactions dean jeffrey corrado greg monga rajat chen devin matthieu mark senior andrew tucker paul yang quoc large scale distributed deep networks. advances neural information processing systems graves alex mohamed hinton geoffrey. speech recognition deep recurrent neural networks. acoustics speech signal processing ieee international conference ieee grounds matthew kudenko daniel. parallel reinforcement learning linear function approximation. proceedings european conference adaptive learning agents multi-agent systems adaptation multi-agent learning springer-verlag lauer martin riedmiller martin. algorithm distributed reinforcement learning cooperative multiagent systems. proceedings seventeenth international conference machine learning morgan kaufmann yuxi schuurmans dale. mapreduce parallel reinforcement learning. recent advances reinforcement learning european workshop ewrl athens greece september revised selected papers mnih volodymyr kavukcuoglu koray silver david graves alex antonoglou ioannis wierstra daan riedmiller martin. playing atari deep reinforcement learning. nips deep learning workshop. mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg petersen stig beattie charles sadik amir antonoglou ioannis king helen kumaran dharshan wierstra daan legg shane hassabis demis. human-level control deep reinforcement learning. nature http//dx.doi.org/./nature. silver david newnham leonard barker david weller suzanne mcfall jason. concurrent reinforcement learning customer interactions. proceedings international conference machine learning szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. arxiv preprint going deeper convolutions. arxiv. alien amidar assault asterix asteroids atlantis bank heist battle zone beam rider bowling boxing breakout centipede chopper command crazy climber demon attack double dunk enduro fishing derby freeway frostbite gopher gravitar hero hockey jamesbond kangaroo krull kung master montezuma revenge pacman name game pong private qbert riverraid road runner robotank seaquest space invaders star gunner tennis time pilot tutankham venture video pinball wizard zaxxon alien amidar assault asterix asteroids atlantis bank heist battle zone beam rider bowling boxing breakout centipede chopper command crazy climber demon attack double dunk enduro fishing derby freeway frostbite gopher gravitar hero hockey jamesbond kangaroo krull kung master montezuma revenge pacman name game pong private qbert riverraid road runner robotank seaquest space invaders star gunner tennis time pilot tutankham venture video pinball wizard zaxxon games alien amidar assault asterix asteroids atlantis bank heist battle zone beam rider bowling boxing breakout centipede chopper command crazy climber demon attack double dunk enduro fishing derby freeway frostbite gopher gravitar hero hockey jamesbond kangaroo krull kung master montezuma revenge pacman name game pong private qbert riverraid road runner robotank seaquest space invaders star gunner tennis time pilot tutankham venture video pinball wizard zaxxon games alien amidar assault asterix asteroids atlantis bank heist battle zone beam rider bowling boxing breakout centipede chopper command crazy climber demon attack double dunk enduro fishing derby freeway frostbite gopher gravitar hero hockey jamesbond kangaroo krull kung master montezuma revenge pacman name game pong private qbert riverraid road runner robotank seaquest space invaders star gunner tennis time pilot tutankham venture video pinball wizard zaxxon", "year": 2015}