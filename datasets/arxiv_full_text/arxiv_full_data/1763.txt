{"title": "Integrated Sequence Tagging for Medieval Latin Using Deep Representation  Learning", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "In this paper we consider two sequence tagging tasks for medieval Latin: part-of-speech tagging and lemmatization. These are both basic, yet foundational preprocessing steps in applications such as text re-use detection. Nevertheless, they are generally complicated by the considerable orthographic variation which is typical of medieval Latin. In Digital Classics, these tasks are traditionally solved in a (i) cascaded and (ii) lexicon-dependent fashion. For example, a lexicon is used to generate all the potential lemma-tag pairs for a token, and next, a context-aware PoS-tagger is used to select the most appropriate tag-lemma pair. Apart from the problems with out-of-lexicon items, error percolation is a major downside of such approaches. In this paper we explore the possibility to elegantly solve these tasks using a single, integrated approach. For this, we make use of a layered neural network architecture from the field of deep representation learning.", "text": "paper consider sequence tagging tasks medieval latin part-of-speech tagging lemmatization. basic foundational preprocessing steps applications text re-use detection. nevertheless generally complicated considerable orthographic variation typical medieval latin. digital classics tasks traditionally solved cascaded lexicon-dependent fashion. example lexicon used generate potential lemma-tag pairs token next context-aware pos-tagger used select appropriate tag-lemma pair. apart problems out-of-lexicon items error percolation major downside approaches. paper explore possibility elegantly solve tasks using single integrated approach. this make layered neural network architecture field deep representation learning. ntroduction latin —and historic variants particular— long topic major interest natural language processing especially community digital humanities automated processing latin texts always popular research topic. variety computational applications text re-use detection desirable annotate augment latin texts useful morpho-syntactical lexical information lemmas. paper focus sequence tagging tasks medieval latin part-of-speech tagging lemmatization. given piece latin text task lemmatization involves assigning word single dictionary headword ‘lemma’ baseform label grouping word tokens differ spelling and/or inflection task lemmatization closely related part-of-speech tagging word running text assigned indicating part speech word class difficulty pos-tagging strongly depends course complexity granularity tagset chosen. lemmatization pos-tagging classic forms sequence labeling tags assigned words basis individual appearance well words surround them. challenges lemmatizing tagging medieval latin lemmatization pos-tagging rather basic preprocessing steps generally complicated number interesting challenges latin language poses. first plain stemming might take long many latin suffixes cannot automatically linked unambiguous morphological category. words ending –ter example correspond less different parts speech nouns adjectives pronouns adverbs numeral adverbs prepositions additionally like many languages latin teeming homographs require context disambiguated. token legi lemmatized verb lego noun lex. similarly ambiguous tokens include common forms quae satis venis. lemmatization specifically another problem verb forms show resemblance lemma. fact tuli ‘active person singular perfect’ fero obvious problem applies fero’s perfect participle latus could turn confused homonymous common noun latus tagger learn morphological connection tuli latus fero moving beyond superficial outward appearances properly modelling immediate context surrounding words. atin school-preserved language changed surprisingly little throughout history compared languages. nevertheless witness introduction considerable orthographical variation affecting spelling spacing words especially medieval times. variants latin spelling based convention relate editorial preferences rather linguistic evolutions. well known orthographical alterations distinguish vowels consonants post-medieval distinction occurs th-th century onwards. earlier phonological evolutions within latin caused orthographical peculiarities. important example evolution classical diphtongs <ae> <oe> turn caused occurrence hypercorrected forms aecclesia instead ecclesia. implication preceding example normalizing spelling word simple conversion task goes direction ambiguity <ae> problematic since function case endings consequently carry relevant inflectional information would like tagger detect. examples linguistic deviation found medieval texts alteration <ti> <ci> caused lenition alteration confusion long vowel short loss addition initial strengthened aspiration fortition intrusion etc. aturally orthographical artifacts homography pose challenging problems computational perspective consider surface form poetae which already three different analyses might applicable ‘nominative masculine plural’ ‘genitive masculine singular’ ‘dative masculine singular’. medieval texts form poetae could easily spelled poete spelling turn causes confusion declensions duc-e. thus good model local context ambiguous word forms appear crucial disambiguation. nevertheless latin highly synthetic language generally lacks strict word order therefore trivial extract syntactic patterns latin sentences lack strict word order hand concordance morphological features cause latin display number amphibologies called crash blossoms. sentences allow different syntactic readings survey resources related research basic sequence tagging tasks pos-tagging typically considered ‘solved’ many modern languages english problems remain challenging so-called ‘resourcescarce’ languages latin fewer smaller resources generally available annotated training corpora. section review main corpora currently available including short characterization brief description type annotations include. early onwards latin important research topic emerging community digital humanities specifically undertaking index thomisticus roberto busa s.j. second half corpus contains texts century author thomas aquinas well texts related approximating words searched online. website additionally allows compare sort words phrases quotations similitudes correlations statistical information. index thomisticus team started treebank project close collaboration latin dependency treebank annotation style inspired prague dependency treebank latin grammar pinkster it-tb training sets taken thomas aquinas’ scriptum super sententiis magistri petri lombardi available download conll-format comprise tokens. index thomisticus present treebank venture seminal project proves considerable value progress latin automatic annotation. second project occupied latin treebanking latin dependency treebank developed part perseus project tufts university classical texts caesar cicero jerome vergil ovid petronius phaedrus sallust suetonius manually annotated adopting guidelines syntactic annotation latin treebanks resulting corpus words made available online. treebanking implies full parsing information whereas morphological information included pos-tag relevant. nother noteworthy treebank project proeil goal find information structure systems crosslinguistically different translations bible first phase texts automatically pos-tagged manually corrected. rulebased ‘guesser’ consequently suggested likely dependency relation annotator. annotation scheme syntactic dependencies based finegrained domain verbal arguments adnominal functions training data made available online includes roughly translations considered correct although likely best option either inferred context language’s patterns word order statement latin structuralized sentential order nuanced. fourth project worth mentioning lasla project developed lemmatized corpus comprising classical texts caesar catullus horace ovid virgil searched online registered publicly available download. lemmatization method latin however semi-automatic. firstly word automatically analyzed basis stem case ending results list possible lemmas. stage choice correct lemma correct morphological analysis occurs manually rather time-consuming undertaking reference dictionary producing lemmas lasla used forcellini’s lexicon totius latinitatis reasonable argument least incoherent lemlat neo-latin morphological analyzer first version appeared statistically able lemmatize wordforms origins fifth/sixth century christ chlt lemlat adopts rule-based approach first splits token three parts order perform morphological tagging namely invariable part wordform paradigmatic suffix ending like lasla lemlat unable contextually disambiguate ambiguous forms running text since lexicon-based specifically makes dictionaries georges gradenwitz oxford latin dictionary. latinise corpus comprises total latin words covering time span century b.c. century a.d. annotated combination pre-existing methods. firstly proeil project’s morphological analyser quick latin used lemmatization pos-tagging analyser generated various options disambiguation word. secondly output analyser input treetagger model trained index thomisticus dataset would take context account choose likely lemma pos-tag. latinise accessed online sketch engine freely available. recent promising project comphistsem team applied network theory detect semantic changes diachronic latin corpora. recently released composite lexicon called frankfurt latin lexicon also referred collex.la brings together lemmas various web-based resources additionally ttlab latin tagger released whitaker’s words index thomisticum ramminger’s neulateinische wortliste latin wiktionary latin training data treetagger najock thesaurus several resources. beyond that continuously manually checked corrected updated historians researchers humanities meantime report word forms lemmas objective automatically large corpora patrologia latina. resources collex.la ttlab latin tagger available trial online. ttlab latin tagger hybrid combines linguistic rule-based approach statistical avoiding huge effort rule-based taggers require every target language separately hand avoiding overfitting characteristic statistical taggers trained tested ttlab latin tagger carolingian capitularia —i.e. ordinances latin decreed carolingian rulers split several sections chapters. recent publication team contributed field oversightful survey paper employed capitularia training data produce comparative study taggers lemmatization methods results arguably offer best discussion state present. taggers compared specifically treetagger lapos mate opennlptagger stanford tagger best tagger reported lapos. comes lemmatization team concluded trained lemmatizer provides better results moreover deals better lemmatizing words out-of-vocabulary suffer several variations used lemmagen specific purpose lemmatizer dependent induced rule conditions proves lexicon-based approaches lemmatization always favourable. upcoming article developed idea showing lemmatizer relies statistical inference treats lemmatization sequence labeling problem provides better results lemmagen. lemmatizers based prefix suffix transformations. moreover comphistsem shown ‘joint learning’ lemmatizer tagger also improve overall accuracy lemmatization pos-tagging task especially case marmot tagger —once additional resources word embeddings underlying lexicon collex.la provided— gains highest results. comphistsem-team generous provide annotated capitularia-corpus facilitates comparison results theirs. general trends remaining problems preceding survey demonstrates lemmatization part-of-speech tagging dispose following annotated data index thomisticus treebank latin dependency treebank proiel data capitularia corpus. annotated corpora offer least lemma coarse tags fine-grained morphological analysis. firstly automatic annotation latin texts moving away semi-automated rule-based approaches data-driven machine learning techniques general older approaches strongly dependent static lexica word form would exhaustively list potential morphological analysis e.g. form tag-lemma pairs. case ambiguity statistically trained part-of-speech tagger would used later single best option. first lexicon-based lemmatization approach disadvantage principle unable correctly lemmatize out-of-vocabulary words covered available lexica. case medieval texts orthographic variation renders problem even acute. moreover lexicon-based strategies susceptible problem error percolation trained tagger predicts wrong renders less likely tagger able select correct lemma. comphistsem project leads respect showing statistical lemmatization techniques offer interesting perhaps even robust alternative traditional lexicon-based approaches. secondly comphistsem’s latest results demonstrate taggers include distributed word representations generally superior previous approaches. observation prove relevant next section since architecture makes similar representation strategy. systems attempted learn tasks lemmatization pos-tagging integrated fashion. systems continue learn tasks independently although systems would make cascade taggers output e.g. pos-tagger would subsequently input lemmatizer. nevertheless previous research clearly demonstrated tasks might mutually inform introduction architectural set-up section describe attempt integrated architecture used automated sequence tagging medieval latin several levels e.g. combined lemmatization pos-tagging. architecture comparable nature sequence taggers morfette architecture principle language-independent could easily applied languages corpora. section restricted complete high-level description minor details architecture training procedure consulted code repository associated paper. graphical depiction complete model depicted fig. overall idea behind architecture simple. first create ‘subnets’ encoders subnet used model particular focus token —the token like tag— second subnet serves model lexical context surrounding focus token. result ‘encoding’ subnets joined single representation ‘decoding’ networks generate lemma another predict tag. one-hot word representation latin highly inflected language. order arrive good model individual words vital take account morphemic information subword level. make recent advances field deep representation learning demonstrated recently pieces text efficiently modeled character level upwards therefore present individual words network using simple matrix representation follows represents character column represents respective character positions word number columns length longest word training material longer words test time truncated fixed length shorter words padded all-zero columns. cells populated binary ‘one hot’ values indicating presence absence character specific position word. simplified example representation offered table tokens lowercased conversion order limit size character vocabulary. able example character-level representation individual focus token representation encodes presence characters subsequent positions word shorter words padded all-zero columns. model matrix-representations words using so-called ‘long short-term memory’ layers lstm powerful type sequence modeler currently paid great deal attention field representation learning context natural language processing particular sort ‘recurrent’ modeler iteratively work subsequent positions time series character positions matrix. series lstm able output single dense vector representation entire sequence. lstms interesting sequence modelers arguably capture longer-term dependencies information different positions time series. point view present task could instance expect lstm develop sensitivity presence specific morphemes words word stems inflectional endings. lstm layers stacked other obtain deeper levels abstraction. experiments stacks lstm layers throughout. part character-level representation network architecture separate subnet model lexical neighbourhood surrounding focus token purpose contextual disambiguation. used series tokens starting words focus token token following focus token common contextual parametrization sort sequence tagging part network based concept so-called ‘word embeddings’ traditional machine learning approaches words represented using index vocabulary case vocabulary consisting words token would represented vector binary values would others zero representation disadvantage requires word vectors considerable dimensionality moreover categorical case ‘embeddings’ tokens represented vectors much lower dimensionality offer word representations available information distributed much evenly available units. general idea word embeddings come much lower computational cost also offer smoother representation words able reflect instance closer semantic distance synonyms. computational perspective learning word embeddings typically involves optimizing randomly initialized matrix vocabulary item holds fixed-size vector e.g. dimensions. matrix quickly grow large word embeddings still efficient token single vector matrix updated time leaving rest matrix unaltered. network architecture modelling context surrounding focus token involves select vectors embeddings matrix concatenate single vector. yields model focus token well surrounding context. consequently concatenate ‘encoding’ representations single ‘hidden representation’. decoding parts network produce ultimate output focus token. lemmatization feed ‘hidden representation’ second stack lstms repeating representation times corresponds maximum lemma length encountered training. task ‘decoding’ lstm produce correct lemma generating required lemma character character. extremely challenging approach problem lemmatization. lemmatization previously approached conventional classification setting either lemma considered atomic class label lemmatization solved predicting ‘edit script’ class label could used convert input token lemma. instead lstm-stack output single vector case encoder— output probability distribution characters alphabet character slot lemma. therefore represent lemma character matrix using exact representation method input tokens borrow idea encoder-decoder lstm architecture seminal paper field machine translation showed stacks encoding/decoding lstms used transduce sentences source language target language here however learn series words language series words another language translate series characters token series characters representing corresponding lemma. proposed network architecture ‘multi-headed’ sense single architecture used simultaneously solve multiple tasks integrated fasion. apart ‘lemma-head’ also second ‘head’ architecture aims predict focus token. ‘hidden representation’ obtained encoder feed stack standard dense layers increasingly common representation learning apply dropout layers meaning training time randomly half available values vector zero non-linearity rectified linear units negative values zero. finally produce probability vector label normalized using so-called softmax layer ensuring resulting probabilities one. train network maximum epochs using optimization method called ‘minibatch gradient descent’ initial batch size specifically used rmsprop update mechanism helps networks converge faster keeps track recent gradient history parameter. epochs would decrease current learning rate factor three initial batch size factor fine-updates batch. specific implementation details apply dropout recurrent layer proved detrimental; recurrent layers tanh-nonlinearity nonlinearities tested failed converge. recurrent layers dense layers dimensionality exception final output layer encoding lstm used dimensionality embeddings matrix implemented models using keras sklearn gensim theano libraries trained nvidia titan depending model’s complexity current batch size epoch average would take seconds. igure graphical representation proposed model architecture. model ‘encoding’ subnets model focus token surrounding context result concatened single hidden representation. represent ‘headnets’ aims generate target lemma character-by-character basis; second predicts tag. paper evaluate performance models using traditional accuracy score common linguistic sequence tagging studies make distinction known unknown tokens development test data. unknown tokens refer predictions surface tokens verbatimly encountered training data training neural networks become standard differentiate training development test set. general idea algorithms trained training data number iterations epoch system gain performance evaluated held-out development data. performance system development data longer increasing sign system overfitting training data generalize scale well unseen data. point halt training procedure finally system evaluating actual test data; testing procedure postponed guarantee researchers optimizing system light specific text set. used exact test data eger whose data focusing development data used final instances remaining data; first used training data. importantly objective approach evaluating system division data architecture slight disadvantage comparison previous studies sense system trained available training data. thus models expected slightly worse lexical coverage might result slightly lower scores etc. important aspect pos-tagging complexity granularity tagset used course important impact performance tagger. exploratory paper limit experiments simple tags dataset distinguish basic word classes important issue original annotation standard used capitularia data illustrated using following example consider spelling word oracio shifted classical oratio result lenition medieval times. current lemmatization standard tokens separate lemmas whereas might well mapped lemma. many projects would like lemmatizer collapse spellings superlemma preferably uniform classical spelling therefore produced alternative version capitularia corpus training data’s lemmas normalized towards classicized orthography conventionally found reference dictionary lewis short mongst many important rules retained respective distinctions consonant vowel disappears diphtong <ae> corrected recovered necessary <ti> recovered <ci> inappropriate classical spelling assimilations especially case prepositional prefixes allowed etc. since many tokens capitularia data normalized standard spelling manually correct deviant lemmas lewis short norm thus creating resource train models classical spellings lemmas. regarding lemmatization conventions predominant principle words converted base form nominative singular nouns nominative masculine singular pronouns adjectives ordinal numbers first person singular verbs. choices perhaps worth mentioning. instance comparatives superlatives redressed neutral base forms gerunds participles person singular verb form. adverbs retained original form. below also report results using dataset considered easier sense output lemmas smaller difficult sense character transduction tokens lemmas potentially becomes complicated corrected cases. lemmatization results test scores generally lower successful scores reported eder overall drop around .-.% overall accuracy test set. partially expected given fact training represents theirs thus slightly worse lexical coverage. also formulation lemmatization task character-per-character string generation task complex currently seem outperform conventional approaches particular dedicated tools lemmagen. interestingly however model outperformed results eger reported lexicon-based approaches indicating machine learning approach relaxes overall need large corpus-external lexica. surprisingly accuracy scores tasks remain relatively training data none models reached accuracies particular tagging task indicating relative difficulty modelling tasks scrutiny. results ‘classicized lemmas’ version data generally ballpark non-classicized data. valuable result since string transduction task fact become complex respect worth pointing results pos-tagging task relatively high mostly best corresponding results reported eder somewhat surprising given limited training data used well fact model fairly generic include task-specific bells whistles current postaggers typically include. modern pos-taggers often implement recently predicted tags previous words additional feature help disambiguate current focus token. include features model trivial implement using mini-batch training method. nevertheless results suggest network produces excellent tagging results labels. likelihood inclusion distributed word embeddings advanced state across multiple tasks implementation would first ‘pretrain’ conventional word embeddings model training data using popular implementation wordvec’s skipgram algorithm fact pretraining data much smaller size used eger i.e. whole patrologia latina seem pose serious disadvantage. used resulting embeddings matrix cheap method speed convergence. importantly word embeddings dynamic corresponding weight matrix fact optimized training process optimize even light specific task. arguably word embeddings approach still approach reported eger. word embeddings added static feature although trained much larger dataset. creates interesting perspectives future research. below include visualization word embeddings training using popular t-sne algorithm visibly demonstrated model seems learn useful representations high-level word classes —e.g. preprositions form tight cluster light blue also collocational patterns integrated learning experiment results curiously mixed interestingly respects tasks seem mutually inform themselves. results instance higher case integrated approach suggests pos-tagging helped information backpropagated lemmatization-specific components. surprisingly however case lemmatization scores actually lower integrated experiments. especially true unknown word scores. hypothesize successful lemmatization unknown words makes surplus capacity hidden representation capacity strictly needed predict known word lemmas. integrated architecture pos-tagger require information hidden representation putting pressure surplus capacity. strongly suggests tasks extent competing resources network research matter required. figure typical visualization word embeddings frequent tokens training data epochs optimization. conventional agglomerative cluster analysis data points scatterplot identify word clusters coloured accordingly reading aid. results classicized corpus analysis errors outputted lstm recurrent intrusion unwanted consonants vowels. predictable problem since generated lemmas character-by-character fashion. false lemmatization results could perhaps best described kind computational hypercorrection tagger attempts solve problem —i.e. straight orthographical variation— fact unnecessary. true normalization praesentaliter praesintaliter correction observed would expected token quolebet. sometimes tagger seemed sensitive orthographic problem drew wrong conclusions solving case ymnus normalized omnis instead hymnus. another typical problem proper names recognized such different consequently normalized unrecognisable form. general noticed intrusion consonants vowels sometimes causes fabrication lemma still quite lemma wanted predict lemmatization intromissi intromittu lapidem lapid. another interesting error pluribus rare occasions lemmatized multus indicates word embeddings model struck connection words semantic equivalence. noise provides pointers need solving problem future endeavours. interestingly lemmatization errors eventually made involve small differences character level near lemma ways expected since generated lemma left-to-right. although minor postprocessing might already helpful here lemma table results original non-classicized lemmas capitularia dataset results shown train development test words well known unknown words separately. paper presented attempt jointly learn sequence tagging tasks medieval latin lemmatization pos-tagging. tasks traditionally solved using cascaded approach bypassed integrating tasks single model. model proposed novel machine learning approach based upon recent advances deep representation learning using neural networks. trained tasks separately model yields acceptable scores previously reported studies. interestingly approach lexicon-independent places results line previous studies moved away lexicon-based approaches. learned jointly observed pos-tagging accuracy increased lemmatization accuracy decreased. research required discover competition resources network handled efficient way. important novelty paper produced annotation layer capitularia dataset normalized medieval orthography lemma labels used classicizing them. spite increased difficulty string transduction task model performed reasonably well novel data terms lemmatization. sincerest gratitude goes towards colleagues prof. jeroen deploige prof. verbaal ghent university whose expertise feedback indispensable creation paper preliminary step joint project collaborative authorship twelfth century latin literature stylometric approach gender synergy authority funded research fund ghent. without considerate guidance respectively historical linguistic literary field article would impossible. would moreover like express gratitude comphistsem-team generously provided additional training data anonymous reviewer his/her indispensable feedback process revision. bamman passarotti busa crane annotation guidelines latin dependency treebank index thomisticus treebank. treatment syntactic constructions latin. proceedings sixth international conference language resources evaluation http//nlp.perseus.tufts.edu/syntax/treebank/./docs/guidelines.pdf. bastien lamblin pascanu bergstra goodfellow i.j. bergeron bouchard bengio theano features speed improvements. deep learning unsupervised feature learning. neural information processing systems workshop pattern analysis machine intelligence. analyser. http//www.ilc.cnr.it/lemlat/ approaches. syntax semantics structure statistical translation meeting association computational linguistics dissertation. dublin city university language resources evaluation eger gleim mehler lemmatization morphological tagging german latin comparison survey state-of-the-art. proceedings international conference language resources evaluation eger brück mehler lexicon-assisted tagging lemmatization latin comparison taggers lemmatization methods. proceedings workshop language technology cultural heritage social sciences humanities franzini franzini büchler historical text reuse http//http//etrap.gcdh.de/?page_id=. graves mohamed hinton speech recognition deep recurrent neural networks. acoustics speech haug d.t.t. jøhndal m.l. creating parallel treebank indo-european bible translations. sporleder ribarov proceedings second workshop language technology cultural heritage data hochreiter schmidhuber long short-term memory. neural computation. http http//www.ilc.cnr.it/lemlat/ http http//ilk.uvt.nl/conll/ http http//itreebank.marginalia.it/view/download.php http http//proiel.github.io http http//www.quicklatin.com/ http http//sites.tufts.edu/perseusupdates////querying-the-perseus-ancient-greek-and-latin-treebank-data-inhttp https//perseusdl.github.io/treebank_data/ http http//web.philo.ulg.ac.be/lasla/ http http//www.comphistsem.org/home.html http http//www.corpusthomisticum.org http https//www.sketchengine.co.uk jurafsky d.s. martin j.h. speech language processing. introduction natural language processing knowles mohd notion lemma. headwords roots lexical sets. international journal corpus lecun bengio hinton deep learning. nature. levy goldberg dagan improving distributional similarity lessons learned word embeddings. lewis c.t. short andrews e.a. freund latin dictionary founded andrews' edition freund's latin manning c.d. computational linguistics deep learning. computational linguistics. matjaž mozetič erjavec lavrač lemmagen multilingual lemmatisation induced ripple-down rules. mehler brück gleim geelhaar towards network model coreness texts experiment classifying latin texts using ttlab latin tagger. ontology learning automated text processing applications. text mining. bieman mehler springer international publishing statistique données textuelles compositionality. neural information processing systems workshop language technology cultural heritage social sciences humanities annotation corpora research humanities. mambrini sofia passarotti lemlat. strumento lemmatizzazione morfologica automatica latino. manuscript digital text. problems interpretation markup. proceedings colloquium citti vecchio passarotti dell’orletta improvements parsing index thomisticus treebank. revision combination feature model medieval latin. proceedings international conference language resources evaluation piotrowski natural language processing historical texts. morgan claypool publishers rigg a.g. orthography pronunciation. medieval latin introduction bibliographical guide. mantello f.a.c toutanova cherry global model joint lemmatization part-of-speech prediction. proceedings joint conference annual meeting international joint conference natural language processing afnlp. research. comunicacion social.", "year": 2016}