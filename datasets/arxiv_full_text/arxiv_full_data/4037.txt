{"title": "Transductive-Inductive Cluster Approximation Via Multivariate Chebyshev  Inequality", "tag": ["cs.CV", "cs.AI"], "abstract": "Approximating adequate number of clusters in multidimensional data is an open area of research, given a level of compromise made on the quality of acceptable results. The manuscript addresses the issue by formulating a transductive inductive learning algorithm which uses multivariate Chebyshev inequality. Considering clustering problem in imaging, theoretical proofs for a particular level of compromise are derived to show the convergence of the reconstruction error to a finite value with increasing (a) number of unseen examples and (b) the number of clusters, respectively. Upper bounds for these error rates are also proved. Non-parametric estimates of these error from a random sample of sequences empirically point to a stable number of clusters. Lastly, the generalization of algorithm can be applied to multidimensional data sets from different fields.", "text": "abstract. approximating adequate number clusters multidimensional data open area research given level compromise made quality acceptable results. manuscript addresses issue formulating transductive inductive learning algorithm uses multivariate chebyshev inequality. considering clustering problem imaging theoretical proofs particular level compromise derived show convergence reconstruction error ﬁnite value increasing number unseen examples number clusters respectively. upper bounds error rates also proved. non-parametric estimates error random sample sequences empirically point stable number clusters. lastly generalization algorithm applied multidimensional data sets diﬀerent ﬁelds. estimation clusters approached either batch framework entire data presented diﬀerent initializations seed points prototypes tested model cluster data like k-means fuzzy c-means online strategy clusters approximated examples data presented time using variational dirichlet processes incremental clustering based randomized algorithms widely known approximation adequate number clusters using multidimensional data open problem variety solutions proposed using monte carlo studies bayesian-kullback learning scheme mean squared error setting gaussian mixture model based approaches information theory cite few. work deviates general strategy deﬁning number clusters apriori. deﬁnes level compromise tolerance conﬁdence quality clustering gives upper bound number clusters generated. note similar deﬁning number clusters. indicates level conﬁdence result requirement still estimate adequate number clusters bound. current work focuses dealing issue approximating number clusters online paradigm conﬁdence level speciﬁed. certain aspects ﬁnds similarity recent work conformal learning theory presents novel ﬁnding approximation cluster degree conﬁdence. conformal learning theory foundations employing transductive-inductive paradigm deals idea estimating quality predictions made unlabeled example based already processed data. mathematically given already processed examples conformal predictors give point prediction unseen example conﬁdence level thus estimates conﬁdence quality prediction using original label prediction made moving next unlabeled example. predictions made basis non-conformity measure checks much example diﬀerent already seen examples. considered manuscript ﬁnds motivation foregoing theory online prediction using transductive-inductive paradigm. research work applies concept coupling creation clusters transduction aggregation examples clusters induction. ﬁnds similarity utilizing idea prediction region deﬁned certain level conﬁdence. presents simple algorithm diﬀers signiﬁcantly conformal learning following aspect instead working sequences data contain labels works unlabeled sequences. ﬁrst formulation becomes imperative estimate number clusters known apriori proposed algorithm comes rescue employing chebyshev inequality. inequality helps providing upper bound number clusters could generated random sample sequence. quality prediction conformal learning checked based p-values generated online. current algorithms relaxes restriction checking quality online estimates clusters data presented. foregoing step makes algorithm weak learner sequence dependent. take stock problem global solution adequate number cluster approximated estimating kernel density estimates sample random sequences data. finally level compromise captured parameter inequality gives upper bound number clusters generated. case clustering static images particular parameter value theoretical proofs show reconstruction error converges ﬁnite value increasing number unseen examples number clusters. empirical kernel density estimates reconstruction error random sample sequences examples indicate number clusters high probability reconstruction error. necessary labeled data always present compute reconstruction error. case proposed algorithm stops short density estimation approximated number clusters random sequence examples certain degree conﬁdence. another dimension proposed work generalization multivariate formulation chebyshev inequality known chebyshev inequality helps proving convergence random sequences diﬀerent data. also multivariate formulation chebyshev inequality facilitates providing bounds multidimensional data often aﬄicted curse dimensionality making diﬃcult compute multivariate probabilities. generalizations exist multivariate chebyshev inequality consideration probability content multivariate normal random vector euclidean n-dimensional ball work employs conservative approach employment euclidian n-dimensional ellipsoid restricts spread probability content work provide motivation employment multivariate chebyshev inequality. eﬃcient implementation analysis k-means clustering using multivariate chebyshev inequality shown current work diﬀers k-means providing online setting problem clustering estimating number clusters particular sequence representation data convergence ellipsoidal multivariate chebyshev inequality given level conﬁdence compromise tolerance quality results generating global approximations number clusters non-parametric estimates reconstruction error rates sample random sequences representing data ﬁxing cluster number apriori. must noted k-means solutions diﬀerent diﬀerent initializations particular value value remains ﬁxed. proposed work high probability estimate made regarding minimum number clusters represent data reconstruction error. outlook broadens perspective ﬁnding multiple solutions upper bounded well approximating particular number cluster similar solutions. similarity solutions particular number cluster attributed constraint imposed chebyshev inequality. point noted using increasing levels compromise conﬁdence conformal learners proposed work generates nested solutions. conﬁdence compromise levels generate tight solutions vice versa. thus proposed weak learner provides solutions robust sample. manuscript also extends work employment multivariate chebyshev inequality image representation. work presents hybrid model based hilbert space ﬁlling curve traverse image. since curve preserve local information neighbourhood pixel reduces burden good image representation lower dimensions. side acts constraint processing image particular fashion. current work removes restriction processing images space ﬁlling curves considering pixel sequence represents image consideration. again single sequence adequate enough learner synthesize image recongnizable level. attributed fact unsupervised paradigm number clusters known apriori also learner would sequence dependent. reiterate proposed work addresses issues recognizability deﬁning level compromise user willing make chebyshev parameter sequence speciﬁc solution taking random samples pixel sequences image. latter helps estimating population dependent solution would robust stable synthesis. regularization error approximated number clusters diﬀerent levels compromise leads adequate number clusters synthesize image minimal deviation original image. thus current work provides perspective approximation cluster number particular conﬁdence level. test propositions made problem clustering images taken account. generalizations algorithm made applied diﬀerent ﬁelds involving multidimensional data sets online setting. image. pixel example dimensions assumed examples appear sample space ﬁnite number examples total number unique sequences large currently algorithm works subset unique sequences sampled sequences. probability sequence occur equally likely images berkeley segmentation given examples sequence appear randomly challenge learn association particular example existing clusters create cluster based information provided already processed examples. current algorithm handles issues evaluation nonconformity measure deﬁned multivariate chebyshev’s inequality formulation -nearest neighbour transductive learning respectively. multivariate formulation generalized chebyshev inequality applied example using single chebyshev parameter. inequality tests deviation example mean cluster examples gives lower probabilistic bound whether example belongs cluster investigation. random example passes test associated cluster mean covariance matrix cluster recomputed. case exists cluster qualify association cluster lowest deviation example picked association. also possible assign example random chosen cluster selected clusters induce noise check approximations number cluster. considered current work time being. case failure association algorithm employs transductive algorithm closest neighbour current example processing. neighbour together current example forms cluster. several important implications arise usage probabilistic inequality measure nonconformal measure. elucidated detail later sections. important point consider usage algorithm create cluster. even though known suﬀers problem curse dimensionality problems small dimensions employed transductive learning. proposed work address curse dimensionality issue. also note general supervised conformal learning algorithm prediction made next random example processed. case current unsupervised framework conformal learning algorithm. case current random example fails associate existing clusters under constraint yielded chebyshev parameter helps ﬁnding closest example remaining unprocessed sample data form cluster. thus formation cluster depends conformal learning presented algorithm strictly follow idea ﬁnding conﬁdence prediction labels present tested against. goal reconstruct clusters single pixel sequence represent image. quality reconstruction taken later random sample pixel sequences used estimate probability density reconstruction error rates. note algorithm represents mean examples cluster covariance matrix feature examples cluster. application multivariate chebyshev inequality yields probabilistic bound enforces certain important implications regard clusters generated. purpose elucidation algorithm starﬁsh image taken univariate case stating probability spread value apart minor diﬀerence formulations convey message probabilistic bound imposed random vector number lies outside chebyshev inequality controls degree uniformity feature values examples constitute cluster. association example cluster happens follows random example considered checking association cluster. spread example satisfaction criterion suggests possible cluster could associated. test conducted existing clusters. cluster associated cluster shows minimum deviation random point chosen. cluster chosen size extended example i.e. cluster consideration. thus size clusters grow probabilistic constraint homogeneous manner. highly inhomogeneous image cluster size restricted small deviation pixel intensities cluster tested with. pixels assigned respective decompositions pixels single decomposition assigned average value intensities pixels constitute decomposition. thus done assumption decomposed clusters homogeneous nature degree homogeneity various implications proposed using multivariate chebyshev inequality image representation using space ﬁlling curve. order extend work implications reiterated development. inequality criterion probability associated gives belief based bound satisfaction criterion. order proceed ﬁrst deﬁnition decomposition needed. criterion. point noted that point belongs represented lemma decompositions bounded lower probability bound lemma value reduces size sample upper bound m/cp probabilistically lower bound sample size inducing certain amount error loss information averaging. reduction sample size indicates level compromise image processed. reduction sample size level compromise directly related construction probabilistically bounded decompositions also. since decompositions generated usage bound suggests conﬁdence amount error incurred reconstruction image. particular pixel reconstruction error computed squaring diﬀerence value intensity original image intensity value assigned clustering. since somewhat homogeneous decomposition bounded probabilistically reconstruction error pixels constitute also bounded probabilistically. thus decompositions summation reconstruction errors pixels bounded. bound indicates conﬁdence generated reconstruction error. also lemma since number decompositions clusters upper bounded total reconstruction error also upper bounded. remains proven particular level compromise error rates converge number processed examples number clusters increase. algorithm three error rates computed random sequence examples processed. original pixel image intensity value assigned clustering. reconstruction error pixel norm- ||xi since pixel assigned particular decomposition gets value mean pixels constitute decomposition thus reconstruction error pixel turns ||xi eq||. ||xi eq||. note error also indicates much examples deviate mean respective cluster. examples processed based information present previous examples total error computed processing errdq error rate ptcntr examples errval/ptcntr. finally error rate computed captures deviation examples respective cluster means happen formation cluster. error denoted err. formula minute change conception. errval divided total number point processed formation every cluster. theorem random sequence represents entire image decomposed clusters chebyshev inequality using unsuperprobabilistically lower bound conﬁdence level greater. proof. known total reconstruction error ptcntr examples errdq error rate errval/ptcntr. also known equation example associated particular decomposition satisﬁes constraint since deﬁnes level compromise image lemma decompositions almost homogeneous examples constitute decomposition similar attribute values. similarity attribute values non-diagonal elements covariance matrix inequality approach zero. thus matrix. inequality equates thus error rate errval/ptcntr also upper bounded. diﬀerent decompositions diﬀerent worst case scenario decomposition lowest covariance substituted every decompositions upper bound error important note error rate converges ﬁnite value asymptotically number processed examples increases. initially learner seen enough examples learn solidify knowledge terms stable mean variance decompositions error rate increases examples presented. attributed fact clusters formed often intial stages lack prior knowledge. certain time large number examples encountered help solidify knowledge stabilize decompositions addition examples increment error. stability clusters checked multivariate formulation chebyshev inequality equation stability also casues error rate stabilize thus indicate convergence bounded manner probabilistic conﬁdence level. thus value ptcntr exists upper bound reconstruction error stabilizes ptcntr increases. ated using unsupervised conformal learning algorithm. pixels cluster generated image mean cluster intensity value label. holds clusters generated image. total number clusters generated particular random sequence error rate depicted ﬁgure theorem random sequence represents entire image decomposed clusters chebyshev inequality using unsuperprobabilistically lower bound conﬁdence level greater. proof. error rate computation error cluster formed. upper bound number clusters decompositions increase follows proof similar presented theorem image ﬁgure error rate number clusters. reconstruction error number clusters dependent pixel sequence presented learner. mean particular level compromise values reconstruction error number clusters never converge ﬁnite value random sample pixel sequences represent image processed learner? simpliﬁed possible reconstruction error number clusters particular level compromise best represents image? points problem whether image reconstructed particular level compromise high probability ﬁnding reconstruction error number clusters sample sequences. existence probability value would require knowledge probability distribution reconstruction error increasing number examples number clusters generated. work kernel density estimation used estimate probability distribution reconstruction error err. investigate quality solution obtained error rates generated diﬀerent random sequences evaluated observations. density estimate empirically point least error rates high probability. found error rates number clusters converge particular value given image. sity estimates error rates number clusters obtained random sequences image. found error rates number clusters converge respectively. figure shows graphs same. seen graphs ﬁgure converge nearly similar values. decomposition expands leads generation lower number clusters required reconstruct image. thus expected lower levels compromise reconstruction error number clusters high vice versa. figure shows behaviour reconstruction error number clusters generated level compromise increases. high reconstruction error necessarily mean representation image bad. suggests granularity reconstruction obtained. thus reconstruction image yield ﬁner details level compromise point segmentations high level compromise. regularization level compromise number clusters would lead reconstruction reconstruction error well adequate number decompositions represent image properly. points need remembered applying online learning paradigm. reconstructed results come near original image level imposed compromise. size dataset image increases time consumed number computations involved processing also increases. start with learner would perform well clean images noisy images. adaptations need made processing noisy images pre-processing would necessary step application algorithm. inequalities also taken account multivariate information online. would tough compare algorithm nevertheless current work contributes estimation cluster number unsupervised paradigm using transductive-inductive learning strategy. said ﬁxed chebyshev parameter bootstrapped sequence sampling environment without replacement unsupervised learner converges ﬁnite error rate along ﬁnite number clusters. result terms clustering error rates optimal give aﬃrmative clue image decomposition robust convergent. simple transductive-inductive learning strategy unsupervised learning paradigm presented usage multivariate chebyshev inequality. theoretical proofs convergence number clusters particular level compromise show stability result sequence robustness probabilistically estimated approximation cluster number random sample sequences representing multidimensional data. lastly upper bounds generated number clusters point limited search space.", "year": 2011}