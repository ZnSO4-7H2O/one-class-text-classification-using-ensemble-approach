{"title": "Parseval Networks: Improving Robustness to Adversarial Examples", "tag": ["stat.ML", "cs.AI", "cs.CR", "cs.LG"], "abstract": "We introduce Parseval networks, a form of deep neural networks in which the Lipschitz constant of linear, convolutional and aggregation layers is constrained to be smaller than 1. Parseval networks are empirically and theoretically motivated by an analysis of the robustness of the predictions made by deep neural networks when their input is subject to an adversarial perturbation. The most important feature of Parseval networks is to maintain weight matrices of linear and convolutional layers to be (approximately) Parseval tight frames, which are extensions of orthogonal matrices to non-square matrices. We describe how these constraints can be maintained efficiently during SGD. We show that Parseval networks match the state-of-the-art in terms of accuracy on CIFAR-10/100 and Street View House Numbers (SVHN) while being more robust than their vanilla counterpart against adversarial examples. Incidentally, Parseval networks also tend to train faster and make a better usage of the full capacity of the networks.", "text": "introduce parseval networks form deep neural networks lipschitz constant linear convolutional aggregation layers constrained smaller parseval networks empirically theoretically motivated analysis robustness predictions made deep neural networks input subject adversarial perturbation. important feature parseval networks maintain weight matrices linear convolutional layers parseval tight frames extensions orthogonal matrices non-square matrices. describe constraints maintained efﬁciently sgd. show parseval networks match state-of-the-art terms accuracy cifar-/ street view house numbers robust vanilla counterpart adversarial examples. incidentally parseval networks also tend train faster make better usage full capacity networks. deep neural networks achieve near-human accuracy many perception tasks however lack robustness small alterations inputs test time indeed presented corrupted image barely distinguishable legitimate human predict incorrect labels high-conﬁdence. adversary design so-called adversarial examples adding small perturbation legitimate input maximize likelihood incorrect class constraints magnitude perturbation practice signiﬁcant portion inputs single step direction gradient sign sufﬁcient generate adversarial example even transferable network another trained problem different architecture existence transferable adversarial examples undesirable corollaries. first creates security threat production systems enabling black-box attacks second underlines lack robustness neural networks questions ability generalize settings train test distributions different case distributions legitimate adversarial examples. whereas earliest works adversarial examples already suggested existence related magnitude hidden activations gradient respect inputs also empirically assessed standard regularization schemes weight decay training random noise solve problem current mainstream approach improving robustness deep networks adversarial training. consists generating adversarial examples on-line using current network’s parameters adding training data. data augmentation method interpreted robust optimization procedure paper introduce parseval networks layerwise regularization method reducing network’s sensitivity small perturbations carefully controlling global lipschitz constant. since network composition functions represented layers achieve increased robustness maintaining small lipschitz constant every hidden layer; fully-connected convolutional residual. particular critical quantity governing local lipschitz constant fully connected convolutional layers spectral norm weight matrix. main idea control norm parameterizing network parseval tight frames generalization orthogonal matrices. matrix could help context robustness appeared early experiment algorithm proposed clear conclusion drawn deal convolutional layers. previous work double backpropagation also explored jacobian normalization improve generalization. contribution twofold. first provide deeper analysis applies fully connected networks convolutional networks well residual networks second propose computationally efﬁcient algorithm validate effectiveness standard benchmark datasets. report results mnist cifar- cifar- street view house numbers fully connected wide residual networks trained parseval regularization. accuracy parseval networks legitimate test examples matches state-of-the-art results show notable improvements adversarial examples. besides parseval networks train signiﬁcantly faster vanilla counterpart. remainder paper ﬁrst discuss previous work adversarial examples. next give formal deﬁnitions adversarial examples provide analysis robustness deep neural networks. then introduce parseval networks efﬁcient training algorithm. section presents experimental results validating model providing several insights. early papers adversarial examples attributed vulnerability deep networks high local variations authors argued sensitivity deep networks small changes inputs neural networks learn discriminative information sufﬁcient obtain good accuracy rather capturing true concepts deﬁning classes strategies improve robustness deep networks include defensive distillation well various regularization procedures contractive networks however bulk recent proposals relies data augmentation uses adversarial examples generated online training. shall experimental section regularization complemented data augmentation; particular parseval networks data augmentation appear robust either data augmentation parseval networks considered isolation. consider multiclass prediction setting classes multiclass classiﬁer function argmax¯y∈y parameters learnt score given pair function take neural network represented computation graph directed acyclic graph single root node node takes values function children graph learnable parameters function want learn root i.i.d. samtraining data assume compact. function measures loss example single-label classiﬁcation setting instance common choice log-loss arguments develop depend lipschitz constant loss respect norm interest. formally assume given p-norm interest constant ¯y)| λpz−zp log-loss next subsection deﬁne adversarial examples generalization performance classiﬁer. then make relationship robustness adversarial examples lipschitz constant networks. given input example adversarial example perturbation input pattern small enough nearly undistinguishable network predict incorrect label. given network parameters structure p-norm adversarial example formally deﬁned convolutional layers simplify notation consider convolutions inputs without striding take width convolution write convolutional layers linear layers ﬁrst deﬁne unfolding operator prepares input denoted input length inputs channels unfolding operator maps convolution unfolding considered matrix j-th column \u0001sign y)). fast gradient sign method. case obtain involved method iterative fast gradient sign method several gradient steps performed smaller stepsize obtain local minimum suggests sensitivity adversarial examples controlled lipschitz constant network. robustness framework lipschitz constant also controls difference average loss training generalization performance. precisely denote covering number using γ-balls using supxwy theorem implies every probability i.i.d. sample since covering numbers p-norm ball increases exponentially bound suggests critical control lipschitz constant good generalization robustness adversarial examples. orthonormality constraints ﬁrst signiﬁcant difference parseval networks vanilla counterpart orthogonality constraint weight matrices. requirement calls optimization algorithm manifold orthogonal matrices namely stiefel manifold. optimization matrix manifolds well-studied topic comprehensive survey). simplest ﬁrst-order geometry approaches consist optimizing unconstrained function interest moving direction steepest descent time staying manifold. guarantee remain manifold every parameter update need deﬁne retraction operator. exist several pullback operators embedded submanifolds stiefel manifold based example cayley transforms however learning parameters neural networks methods computationally prohibitive. overcome difﬁculty approximate operator derived following layer-wise regularizer weight matrices ensure parseval tightness optimizing convergence every gradient descent step guarantees stay desired manifold expensive procedure. moreover result parameters ones obtained main gradient update. approximations make algorithm efﬁcient first step descent function gradient regularization term i)wk. consequently every main update perform following secondary update βwkw aggregation layers/transfer functions layers perform inputs residual netowrks fall case values come play. node sums inputs tranfer function layer relu) check input node soon lipschitz constant transfer function parseval regularization introduce section regularization scheme make deep neural networks robust constraining lipschitz constant hidden layer smaller assuming lipschitz constant children nodes smaller one. avoid exponential growth lipschitz constant usual regularization scheme last layer controls overall lipschitz constant network. enforce constraints practice parseval networks ideas maintaining orthonormal rows linear/convolutional layers performing convex combinations aggregation layers. below ﬁrst explain rationale constraints describe approach efﬁciently enforce constraints training. orthonormality weight matrices linear layers need maintain spectral norm weight matrix computing largest singular value weight matrices practical setting unless rows matrix kept orthogonal. weight matrix rdout×din dout parseval regularization maintains idout×dout refers identity matrix. approximately parseval tight frame hence name parseval networks. convolutional layers matrix rdout×din constrained parseval tight frame output rescaled factor maintains singular values input node. generally keeping rows weight matrices orthogonal makes possible control spectral norm weight matrix norm individual rows. robustness achieved rescaling rows -norm smaller experimented constraints -norm rows robustness sense rk|γ well studied problem max) form every therefore solution essentially boils soft thresholding operation. denote sorted coefﬁcients max{k optimal thresholding optionally instead updating whole matrix randomly select subset rows perform update submatrix composed rows indexed sampling based approach reduces overall complexity provided rows carefully sampled procedure accurate monte carlo approximation regularizer loss function optimal sampling probabilities also called statistical leverages approximately equal start orthogonal matrix stay manifold throughout optimization since proportional eigenvalues therefore sample subset columns uniformly random applying projection step. full update result increased overhead convolutional layers picture different large fully connected layers making sampling approach computationally appealing layers. show experiments weight matrices resulting procedure -orthogonal. also note quasi-orthogonalization procedures similar described successfully used previously context learning overcomplete representations independent component analysis convexity constraints aggregation layers parseval networks aggregation layers output convex combination inputs instead e.g. residual networks aggregation node network denote ))n)∈e k-size vector coefﬁcients used convex combination output layer. ensure lipschitz conp constraints stant node call euclidean projection onto positive consequently projection since dominated sorting coefﬁcients typically cheap aggregation nodes children practice number children large exist efﬁcient linear time algorithms ﬁnding optimal thresholding work method detailed perform projection coefﬁcient every gradient update step. evaluate effectiveness parseval networks well-established image classiﬁcation benchmark datasets namely mnist cifar- cifar- street view house numbers train fully connected networks wide residual networks. details datasets models training routines summarized below. cifar. cifar datasets composed natural scene color images size split training images test images. cifar- cifar- respectively classes. datasets adopt following standard preprocessing data augmentation scheme training image ﬁrst zero-padded pixels side. resulting image randomly cropped produce image subsequently horizontally ﬂipped probability also normalize every image mean standard deviation channels. following practice initially images training factor epochs vanilla models applied default weight decay regularization together batch normalization dropout since combination resulted better accuracy increased robustness preliminary experiments. dropout rate cifar svhn. parseval regularized models choose value retraction parameter cifar datasets svhn based performance validation set. cases also adversarially trained models cifar- cifar- following guidelines particular replace examples every minibatch adversarially perturbed version generated using one-step method avoid label leaking mini-batch magnitude adversarial perturbation obtained sampling truncated gaussian centered standard deviation model. also train feedforward networks composed fully connected hidden layers size classiﬁcation layer. input networks images unrolled dimensional vector number channels. used models mnist cifar mainly demonstrate proposed approach also useful non-convolutional networks. compare parseval networks vanilla models without weight decay regularization. adversarially trained models follow guidelines previously described convolutional networks. training. train models divide learning rate every epochs. mini-batches size train model epochs. chose hyperparameters validation re-train model union training validation sets. hyperparameters size subset learning rate decrease rate. using subset rows weight matrix retraction step worked well practice. ﬁrst validate parseval training indeed yields -orthonormal weight matrices. analyze spectrum weight matrices different models plotting histograms singular values compare histograms parseval networks networks trained using standard without weight decay validation set. next train novo best model full images report results test set. svhn street view house number dataset color digit images ofﬁcially split training images test images. following common practice randomly sample images available extra images validation combine rest pictures ofﬁcial training total number training images. divide pixel values preprocessing step report test performance best performing model validation set. models. cifar svhn datasets trained wide residual networks perform standard resnets faster train thanks reduced depth. used wide resnets depth width cifar- cifar-. svhn used wide resnet depth width architecture compare parseval networks vanilla model trained standard regularization adversarial non-adversarial training settings. training. train networks stochastic gradient descent using momentum cifar datasets initial learning rate scaled factor epochs total number epochs. used mini-batches size svhn trained models mini-batches size epochs starting learning rate decreastable classiﬁcation accuracy models cifar- cifar- various regularization scheme. represents value signal noise ratio adversarially perturbed image perceptible human. dataset rows report results non-adversarial training bottom rows report results adversarial training. resnets. table summarizes results experiments wide residual parseval vanilla networks cifar- cifar- svhn. table denote parseval parseval network orthogonality constraint without using convex combination aggregation layers. parseval indicates conﬁguration orthogonality convexity constraints used. ﬁrst observe parseval networks outperform vanilla ones datasets clean examples match state performances cifar- svhn cifar- parseval wide resnet depth instead achieve accuracy comparison best performance achieved vanilla wide resnet pre-activation resnet respectively therefore proposal useful regularizer legitimate examples. also note cases parseval networks combining histograms representing distribution singular values layers fully connected network trained dataset cifar- shown fig. singular values obtained method tightly concentrated around experiment conﬁrms weight matrices produced proposed optimization procedure orthonormal. distribution singular values weight matrices obtained variance nearly many small values large ones. adding weight decay standard leads sparse spectrum weight matrices especially higher layers network suggesting low-rank structure. observation motivated recent work compressing deep neural networks evaluate robustness models adversarial noise generating adversarial examples test various magnitudes noise vector. following common practice fast gradient sign method generate adversarial examples section since adversarial examples transfer network other fast gradient sign method allows benchmark network reasonable settings opponent know network. report accuracy model function magnitude noise. make results easier interpret compute corresponding signal noise ratio input perturbation deﬁned show adversarial examples fig. fully connected nets. figure depicts comparison parseval vanilla networks without adversarial training various noise levels. mnist cifar- parseval networks consistently outperforms weight decay regularization. addition robust adversarial training cifar-. combining parseval networks adversarial training results robust method mnist. results presented table validate important claim parseval networks signiﬁcantly improve robustness vanilla models adversarial examples. adversarial training used accuracy between methods signiﬁcant value best parseval network achieves accuracy best vanilla model models adversarially trained parseval networks remain superior vanilla models cases. interestingly adversarial training slightly improves robustness parseval networks noise setting sometimes even deteriorates contrast combining adversarial training parseval networks effective approach high noise setting. result suggests thanks particular form regularizer parseval networks achieves robustness adversarial examples located immediate vicinity data point. therefore adversarial training helps adversarial examples found away legitimate patterns. observation holds consistently across datasets considered study. given distribution singular values observed figure want analyze intrinsic dimensionality representation learned different networks every layer. local covariance dimension measured covariance matrix data. layer fully connected network compute activation’s φkφk obempirical covariance matrix tain sorted eigenvalues method layer select smallest integer gives number dimensions need explain covariance. also compute quantity examples class considering empirical estimation covariance examples report numbers examples per-class table shows local covariance dimension data consistently higher parseval networks approaches layer network. sgdwd-da contracts data dimensional spaces upper levels network using total dimension parseval networks whole dimension respectively layers. intriguing given sgd-wd-da also increases robustness network apparently parseval networks. average local covariance dimension classes sgd-wd-da contracts class dimensionality contracts data upper layers network. parseval data class contracted overall dimension. results suggest parseval contracts data class lower dimensional manifold hence making classiﬁcation easier. parseval networks converge signiﬁcantly faster vanilla networks trained batch normalization dropout depicted ﬁgure thanks orthogonalization step following gradient update weight matrices well conditioned step optimization. hypothesize main explanation phenomenon. convolutional networks faster convergence obtained expense larger walltime since cost projection step negligible compared total cost forward pass modern architecture thanks small size ﬁlters. introduced parseval networks approach learning neural networks intrinsically robust adversarial noise. proposed algorithm allows optimize model efﬁciently. empirical results three classiﬁcation datasets fully connected wide residual networks illustrate performance approach. amodei dario anubhai rishita battenberg eric case carl casper jared catanzaro bryan chen jingdong chrzanowski mike coates adam diamos greg deep speech end-to-end speech recognition english mandarin. arxiv preprint arxiv. condat laurent. fast projection onto simplex the\\ mathbf ball. mathematical programming denton emily zaremba wojciech bruna joan lecun yann fergus rob. exploiting linear structure within convolutional networks efﬁcient evaluation. adv. nips duchi john shalev-shwartz shai singer yoram chandra tushar. efﬁcient projections onto -ball learning high dimensions. proceedings international conference machine learning fawzi alhussein moosavi-dezfooli seyed-mohsen frossard pascal. robustness classiﬁers adversarial random noise. advances neural information processing systems kaiming zhang xiangyu shaoqing jian. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition moosavi-dezfooli seyed-mohsen fawzi alhussein frossard pascal. deepfool simple accurate method fool deep neural networks. arxiv preprint arxiv. papernot nicolas mcdaniel patrick goodfellow somesh berkay celik swami ananthram. practical black-box attacks deep learning systems using adversarial examples. arxiv preprint arxiv. papernot nicolas mcdaniel patrick somesh swami ananthram. distillation defense adversarial perturbations deep neural networks. security privacy ieee symposium ieee salimans kingma diederik weight normalization simple reparameterization accelerate training deep neural networks. advances neural information processing systems shaham yamada yutaro negahban sahand. understanding adversarial training increasing local stability neural nets robust optimization. arxiv preprint arxiv. szegedy christian zaremba wojciech sutskever ilya bruna joan erhan dumitru goodfellow fergus rob. intriguing properties neural networks. proc. iclr", "year": 2017}