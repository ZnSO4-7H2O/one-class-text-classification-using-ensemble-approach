{"title": "Learning model-based planning from scratch", "tag": ["cs.AI", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Conventional wisdom holds that model-based planning is a powerful approach to sequential decision-making. It is often very challenging in practice, however, because while a model can be used to evaluate a plan, it does not prescribe how to construct a plan. Here we introduce the \"Imagination-based Planner\", the first model-based, sequential decision-making agent that can learn to construct, evaluate, and execute plans. Before any action, it can perform a variable number of imagination steps, which involve proposing an imagined action and evaluating it with its model-based imagination. All imagined actions and outcomes are aggregated, iteratively, into a \"plan context\" which conditions future real and imagined actions. The agent can even decide how to imagine: testing out alternative imagined actions, chaining sequences of actions together, or building a more complex \"imagination tree\" by navigating flexibly among the previously imagined states using a learned policy. And our agent can learn to plan economically, jointly optimizing for external rewards and computational costs associated with using its imagination. We show that our architecture can learn to solve a challenging continuous control problem, and also learn elaborate planning strategies in a discrete maze-solving task. Our work opens a new direction toward learning the components of a model-based planning system and how to use them.", "text": "conventional wisdom holds model-based planning powerful approach sequential decision-making. often challenging practice however model used evaluate plan prescribe construct plan. introduce imagination-based planner ﬁrst model-based sequential decision-making agent learn construct evaluate execute plans. action perform variable number imagination steps involve proposing imagined action evaluating model-based imagination. imagined actions outcomes aggregated iteratively plan context conditions future real imagined actions. agent even decide imagine testing alternative imagined actions chaining sequences actions together building complex imagination tree navigating ﬂexibly among previously imagined states using learned policy. agent learn plan economically jointly optimizing external rewards computational costs associated using imagination. show architecture learn solve challenging continuous control problem also learn elaborate planning strategies discrete maze-solving task. work opens direction toward learning components model-based planning system them. model-based planning involves proposing sequences actions evaluating model world reﬁning proposals optimize expected rewards. several advantages model-based planning model-free methods models support generalization states previously experienced help express relationship present actions future rewards resolve states aliased value-based approximations. advantages especially pronounced problems complex stochastic environmental dynamics sparse rewards restricted trial-and-error experience. even accurate model planning often challenging model used evaluate plan prescribe construct plan. existing techniques model-based planning effective small-scale problems often require background knowledge domain pre-deﬁned solution strategies. planning discrete state action spaces typically involves exploring search tree sequences actions high expected value apply ﬁxed heuristics control complexity problems continuous state action spaces search tree effectively inﬁnite planning usually involves sampling sequences actions evaluate according assumptions smoothness regularities state action spaces figure schematic. manager chooses whether imagine imagining consists controller proposing action imagination evaluating acting consists controller proposing action executed world memory aggregates information iteration updates history. modern methods planning exploit statistics individual episode learn across episodes optimized given task. even fewer attempt learn actual planning strategy itself including transition model policy choosing sample sequences actions procedures aggregating proposed actions evaluations useful plan. introduce imagination-based planner model-based agent learns experience aspects planning process construct evaluate execute plan. learns versus imagine imagining select states actions evaluate help minimize external task loss internal resource costs. training effectively develops planning algorithm tailored target problem. learned algorithm allows ﬂexibly explore exploit regularities state action spaces. framework applied continuous discrete problems. experiments evaluated continuous implementation challenging continuous control task discrete maze-solving problem. novel contributions fully learnable model-based planning agent continuous control. agent learns construct plan model-based imagination. agent uses model environment ways imagination-based novel approach learning build navigate exploit imagination trees. planning ground truth models studied heavily remarkable advances. alphago world champion computer system trains policy decide expand search tree using known transition model. planning continuous domains ﬁxed models must usually exploit background knowledge sample actions efﬁciently several recent efforts addressed planning complex systems however planning uses classical methods e.g. stochastic optimal control trajectory optimization model-predictive control. also various efforts learn plan. classic dyna algorithm learns model used train policy vezhnevets proposed method learns initialize update plan model instead directly maps observations plan updates. value iteration network predictron train deep networks implicitly plan iterative rollouts. however former model latter uses abstract model capture world dynamics applied learning markov reward processes rather solving markov decision processes approach also related classic work meta-reasoning internal schedules computations carry costs order solve task. recently neural networks trained perform conditional adaptive computation results dynamic computational graph. recently trained visual imagination model control simulated billiards systems though system learn plan. inspired hamrick al.’s imagination-based metacontroller learned adaptive optimization policy one-shot decisionmaking contextual bandit problems. however learns adaptive planning policy general challenging class sequential decision-making problems.similar work study looks detail dealing imperfect complex models world working pixels discrete sequential decision making processes. figure imagination strategies. sequences imagined actions states form tree imagination steps. grey nodes indicate states chosen manager imagine dotted edges indicate actions proposed controller black nodes indicate future states predicted imagination three imagination strategies correspond three rows trees constructed deﬁnition planning adopt involves agent proposing sequences actions evaluating model following policy depends proposals predicted results. implements recurrent policy capable planning four components step manager chooses whether imagine act. acting controller produces action executed world. imagining controller produces action evaluated model-based imagination. cases data resulting step aggregated memory used inﬂuence future actions. collective activity ibp’s components supports various strategies constructing evaluating executing plan. iteration either executes action world imagines consequences proposed action. actions executed world indexed sequence imagination steps performs action indexed planning acting processes types data generated external internal. external data includes observable world states executed actions obtained rewards internal data includes imagined states ˆsjk actions ˆajk rewards ˆrjk well manager’s decision whether imagine termed route number actions imaginations performed auxiliary information step. summarize external internal data single iteration history external internal data including present iteration imagined states since previous executed action {ˆsj ˆsjk} initialized current world state manager discrete policy maps history route determines whether agent execute action environment imagine consequences proposed action. imagining route also select previously imagined real state imagine from. deﬁne {act ˆsjk} signal world ˆsjl signals propose evaluate action imagined state ˆsjl. controller contextualized action policy maps state history action state provided input controller determined manager’s choice executing actual state always used. imagining state used mentioned above. different possible imagination strategies detailed below determine state used imagination. imagination model world maps states actions consequent states scalar rewards memory recurrently aggregates external internal data generated iteration update history i.e. constructing plan involves choosing propose imagine actions building record possible sequences actions’ expected quality. sequence actions predicted yield high reward identiﬁed manager choose controller produce appropriate actions. explored three distinct imagination-based planning strategies -step n-step tree differ manager selects actual state imagined states since last action state propose evaluate action. -step imagination must imagine actual state ˆsj. induces depth- tree imagined states actions n-step imagination must imagine recent previously imagined state ˆsjk. induces depth-n chain imagined states actions trees manager chooses whether imagine actual state previously imagined state since last actual action {ˆsj ˆsjk}. induces imagination tree imagined actions proposed previously imagined state evaluated model challenging continuous control task adapted spaceship task call spaceship task agent must pilot initially stationary spaceship space random initial position radius distance units random mass mass units mothership always position agent thrusters force choice accelerates velocity stationary planets scene random positions radius distance units masses vary mass units. planets’ gravitational ﬁelds accelerate spaceship induces complex non-linear dynamics. single action entailed ship ﬁring thrusters ﬁrst time step traveling ballistic motion time steps gravitational dynamics. several factors inﬂuence possible solutions problem. spaceship pilot must linearly increasing price fuel force magnitude greater threshold value distance units i.e. fuel_cost price). incentivizes pilot choose small thruster forces. also included multiplicative noise control incentivizes small controls also bounds resolution future states system accurately predicted. implemented continuous spaceship task using standard deep learning building blocks. memory lstm crucially continuous encodes plan embedding plan context using inputs concatenation subset imagining were acting were manager controller multi-layer perceptrons took input output pjk. took ˆsjpjk input output ˆajk imagining acting respectively. imagination-based model environment interaction network powerful neural architecture model graph-like data. took input returned ˆsjk+ imagining took returned acting. continuous trained jointly optimize loss terms external task internal resource losses. task loss reﬂects cost executing action world including fuel cost ﬁnal distance mothership. resource loss reﬂects cost using imagination particular time step affects manager. could ﬁxed across episode vary number actions taken expressing constraint imagining early less expensive imagining on-the-ﬂy. total loss optimized task resource losses. training consisted optimizing gradient descent parameters agent respect task loss internal resource costs. computational graph architecture divided three distinct subgraphs model manager controller memory. subgraph’s learnable parameters trained simultaneously on-policy independently another. model trained make next-step predictions state supervised fashion error gradients computed backpropagation. data collected observations agent makes acting real world. outputs discrete used reinforce algorithm estimate gradients. policy stochastic also applied entropy regularization training encourage exploration. rewards manager consist negative internal external loss trained jointly. output non-differentiable treated routes chose given episode constants learning. induced computational graph varied function route value route switch determined gradients’ backward ﬂow. order approximate error gradient respect action executed world used stochastic value gradients unrolled full recurrent loop imagined real controls computed error gradients using backpropagation time. trained minimize external loss i.e. fuel cost ﬁnal distance mothership including imagination cost. training regime similar used terms strategies relied beside -step n-step used restricted version imagination-tree strategy manager selected {act ˆsjk} depicted figure reduced range trees model construct presumably easier train ﬁxed number discrete route choices independent future work explore using rnns local navigation within tree handle variable sized sets route choices manager. ﬁrst results show -step lookahead learn model-based imagination improve performance complex continuous sequential control task. figure ﬁrst depicts several example trajectories -step granted three external actions imagined actions external action learned often ﬁrst external actions navigate open regions space presumably avoid complex nonlinear dynamics imposed planets’ gravitational ﬁelds ﬁnal action seek target location. used imagined actions potential actions reﬁned iteratively. figure shows performance different agents granted three external actions different maximum numbers imagined actions. task loss always decreased function maximum number imagined actions. version allowed external action roughly corresponded hamrick al.’s ibmc. agents could imagination represents baselines domain could model compute policy gradients training could model evaluate proposed actions. results provide clear evidence value imagination sequential decision-making even -step lookahead. also examined -step lookahead ibp’s imagination greater resource costs imposed found smoothly decreased total number imagination steps avoid additional penalty eventually using imagination high values reason low-cost agents full imaginations allowed small entropy reward incentivized learn policy increased variance route choices. figure shows result increased imagination cost decreased imagination task loss increased. figure plans constructed agent four different episodes. images left right imagined executed action rollouts time. step covers panels ﬁrst imagination second contains executed action. note real actions depicted imagination steps either blue green. blue actions start current world state. green start previously imagined states right-most column shows tree representing real actions imaginations executed agent. also trained n-step order evaluate whether could construct longer-term plans well restricted imagination-tree determine whether could learn complex plan construction strategies. figure second shows n-step trajectories third fourth rows show imagination-tree trajectories. praticular task execution cost imagination step increases making advantageous plan early. results show n-step performance better -step imagination-tree outperforms both especially maximum imagination steps allowed values fuel cost applied. note imagination-tree becomes useful agent imagination steps use. second preliminary experiment discrete maze-solving task implemented discrete probe different imagination strategies. simplicity imagination used perfect world model. controller given tabular representation history represents incremental changes caused imagination steps. memory’s aggregation functionality implemented accumulating updates induced imagination steps history. controller’s policy determined learned q-value history tensor. manager convolutional neural network took input current table history tensor layout. particular instance explored search strategy. created mazes states would aliased tabular representation. maze could multiple candidate goal locations episode goal location selected random candidates. agent instantiated single q-values maze different goal locations. would also allow model-based planning agent imagined rollouts disambiguate states generalize goal locations unseen training. results maze tasks found supplementary material. figure spaceship task performance task loss function numbers imaginations actions. x-axis represents maximum number imagination steps. y-axis represents average task loss dotted dashed solid lines represent agents allowed actions respectively. zero resource cost applied imagination. imagination function resource cost. agent allowed three actions maximum imaginations action. x-axis represents internal resource cost imagination y-axis represents average number imagination steps agent chose episode task loss function resource cost. x-axis represents internal resource cost imagination y-axis represents average task loss dashed solid lines represent external task loss total loss respectively. d-e. task loss function numbers imaginations imagination strategy. x-axis represents maximum number imagination steps. y-axis represents average task loss dotted dashed solid lines represent agents -step n-step imagination-tree strategies respectively. increasing resource cost applied imagination starting paying nothing increasing execution. differ fuel costs challenging. single maze multiple goals start exploring state aliasing generalization out-ofdistribution states. figure shows prototypical setup consider exploration along imagination rollout examples. training agent ﬁrst three goal positions selected random never fourth. reward every step except agent received reward number steps left total budget steps. amount available actions position maze small limited manager ﬁxed policy always expanded next leaf node search tree highest value. figure also showed effect different number imaginations disambiguate evaluated maze. sufﬁcient imagination steps could agent possible goals importantly could resolve goal positions experienced training. multiple mazes multiple goals illustrate learned manager adapts different environments proposes different imagination strategies. used different mazes possible goal positions others different optimal strategies. introduced figure single maze multiple goals represents position player goal. ﬁrst three conﬁgurations training last tested agent’s ability generalize. imagination steps shaded blue areas different imagination budgets left right. imagination depth corresponds color saturation grid cells. actual action agent took shown solid black arrow. bottom multiple mazes example run. left example mazes candidate goal locations other. middle right four example learned agent maze. agent performed four imagination steps took ﬁrst action performed imagination step. imagination steps rollout updates shown. rollout updates original values rollout colored gray updates blue. regularity manager could exploit made wall layouts correlated number possible goal positions. learned separate tabular control policy maze used shared manager across mazes. figure bottom shows several mazes example learned agent. note maze goals cannot disambiguated using -step imagination–longer imagination sequences needed. n-step trouble resolving maze four goals small number steps cannot jump back check different hypothesized goal position. imagination-tree could adapt search strategy maze thus achieved overall average reward roughly closer optimum either -step n-step ibps. experiment also exposes limitation simple imagination-tree strategy manager choose imagine {ˆsj ˆsjk}. proceed imagining reset back current world state. different path explored part path overlaps previously explored path agent must waste imagination steps reconstructing overlapping segment. preliminary address allow manager work ﬁxed-length macro actions done control policy effectively increases imagination trees’ branching factor making tree shallow. supplementary materials show beneﬁts using macros scaled version tasks considered figure figure shows common imagination trees constructed agent highlighting diversity ibp’s learned planning strategies. introduced approach model-based planning learns construct evaluate execute plans. continuous represents ﬁrst fully learnable method model-based planning continuous control problems using model imagination-based planning gradient-based policy optimization. results spaceship task show ibp’s learned planning strategies strike favorable trade-offs external task loss internal imagination costs sampling alternative imagined actions chaining together sequences imagined actions developing complex imagination-based planning strategies. results maze-solving task illustrate learn build ﬂexible imagination trees inform plan construction process improve overall performance. future applied diverse natural decision-making problems robotic control higher-level problem solving. environment models explored especially operate sensory observations. fact imagination differentiable exploited another form model-based reasoning online gradient-based control optimization. work demonstrates core mechanisms model-based planning learned experience including implicit explicit domain knowledge well ﬂexible plan construction strategies. implementing essential components planning system powerful neural network function approximators helps realize promise model-based planning opens doors learning solution strategies challenging problem domains. references peter battaglia razvan pascanu matthew danilo jimenez rezende interaction networks learning objects relations physics. advances neural information processing systems pages katerina fragkiadaki pulkit agrawal sergey levine jitendra malik. learning visual predictive models physics playing billiards. proceedings international conference learning representations pages nicholas stuart russell david tolpin solomon eyal shimony. selecting computations theory applications. proceedings conference uncertainty artiﬁcial intelligence nicolas heess gregory wayne david silver lillicrap erez yuval tassa. learning continuous control policies stochastic value gradients. advances neural information processing systems pages david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature david silver hado hasselt matteo hessel schaul arthur guez harley gabriel dulacarnold david reichert neil rabinowitz andre barreto predictron end-to-end learning planning. arxiv preprint arxiv. manuel watter jost springenberg joschka boedecker martin riedmiller. embed control locally linear latent dynamics model control images. advances neural information processing systems pages theophane weber sebastien racaniere david reichert lars buesing arthur guez danilo rezende adria puigdomènech badia oriol vinyals nicolas heess yujia razvan pascanu peter battaglia david silver daan wierstra. imagination-augmented agents deep reinforcement learning. arxiv single maze multiple goals mazes used single maze multiple goals study shown figure according reward scheme introduced main paper optimal reward mazes agent imagination optimal strategy would closest goal time. however given enough imagination steps agent would able disambiguate different environments accordingly. figure shows imagination rollouts along corresponding updates planning context clear updates imagination helps adjust agent’s policy adapt better particular episode. figure single maze multiple goals setup. share layout agent forced q-values goal positions. ﬁrst three training goal positions last unseen training used test ability agent generalize. figure imagination rollouts different imagination budget corresponding updates plan context update plots gray arrows represent original values action update blue arrows represent update multiple mazes multiple goals figure shows conﬁguration mazes used exploration. maze goal positions agent q-values mazes. along maze conﬁgurations also showing default policy learned mazes basically points closest possible goal location. figure left shows comparison different imagination strategies compare learned neural imagination strategy ﬁxed strategies including -step n-step imagination. comparison imagination based methods imagination budget step. learned strategy adapt better different scenarios. figure show mazes used scaled version mazes. challenging agent needs explore much bigger space useful information. proposed macroactions using rollouts length basic units. figure right shows comparison different imagination strategies task. using macro-actions consistently improves performance using macro-actions perform well potentially much harder learning problem particular action sequences much longer useful reward information delayed even further. figure multiple mazes setup corresponding learned default policies q-values represented gray arrows q-value action maximum value highlighted black solid arrows. setup used followed closely details identical ones described paper. re-iterate used adam optimizer learning rate decreased percent every time validation error increased. validation error computed every iterations. iteration consists evaluating gradient minibatch episodes. used different initial learning rates different components. training model world used learning rate controller manager rely gradient clipping gradient norm allowed agents trained total iterations. pseudocode complex agent given below figure performance different imagination strategies model gets learned. left maze methods imagination budget steps right maze methods budget imagination steps. make overall trend visible solid lines show exponential moving average shaded regions correspond exponential moving standard deviation. maze beneﬁcial operate -step macro actions instead exploring simple actions only.", "year": 2017}