{"title": "HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale  Visual Recognition", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "In image classification, visual separability between different object categories is highly uneven, and some categories are more difficult to distinguish than others. Such difficult categories demand more dedicated classifiers. However, existing deep convolutional neural networks (CNN) are trained as flat N-way classifiers, and few efforts have been made to leverage the hierarchical structure of categories. In this paper, we introduce hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category hierarchy. An HD-CNN separates easy classes using a coarse category classifier while distinguishing difficult classes using fine category classifiers. During HD-CNN training, component-wise pretraining is followed by global finetuning with a multinomial logistic loss regularized by a coarse category consistency term. In addition, conditional executions of fine category classifiers and layer parameter compression make HD-CNNs scalable for large-scale visual recognition. We achieve state-of-the-art results on both CIFAR100 and large-scale ImageNet 1000-class benchmark datasets. In our experiments, we build up three different HD-CNNs and they lower the top-1 error of the standard CNNs by 2.65%, 3.1% and 1.1%, respectively.", "text": "image classiﬁcation visual separability different object categories highly uneven categories difﬁcult distinguish others. difﬁcult categories demand dedicated classiﬁers. however existing deep convolutional neural networks trained n-way classiﬁers efforts made leverage hierarchical structure categories. paper introduce hierarchical deep cnns embedding deep cnns category hierarchy. hd-cnn separates easy classes using coarse category classiﬁer distinguishing difﬁcult classes using category classiﬁers. hd-cnn training component-wise pretraining followed global ﬁnetuning multinomial logistic loss regularized coarse category consistency term. addition conditional executions category classiﬁers layer parameter compression make hd-cnns scalable large-scale visual recognition. achieve state-of-the-art results cifar large-scale imagenet -class benchmark datasets. experiments build three different hd-cnns lower top- error standard cnns respectively. deep cnns well suited large-scale learning based visual recognition tasks highly scalable training algorithm needs cache small chunk potentially huge volume training data sequential scans achieved increasingly better performance recent years. datasets become bigger number object categories becomes larger complications come along visual separability different object categories highly uneven. categories much harder distinguish others. take categories cifar example. easy tell apple harder tell apple orange. fact apples oranges belong coarse category fruit vegetables buses belong another coarse category vehicles deﬁned within cifar. nonetheless deep models nowadays n-way classiﬁers share fully connected layers. makes wonder whether structure adequate distinguishing difﬁcult categories. natural intuitive alternative organizes classiﬁers hierarchical manner according divide-and-conquer strategy. although hierarchical classiﬁcation proven effective conventional linear classiﬁers attempts made exploit category hierarchies deep models. since deep models large models themselves organizing hierarchically imposes following challenges. first instead handcrafted category hierarchy learn category hierarchy training data cascaded inferences hierarchical classiﬁer degrade overall accuracy dedicated category classiﬁers exist hard-to-distinguish categories? second hierarchical classiﬁer consists multiple models different levels. leverage commonalities among models effectively train all? third would also slower memory-consuming hierarchical classiﬁer novel testing image. alleviate limitations? paper propose generic principled hierarchical architecture hierarchical deep convolutional neural network decomposes image classiﬁcation task steps. hd-cnn ﬁrst uses coarse category classiﬁer separate easy classes another. challenging classes routed downstream category classiﬁers focus confusing classes. adopt module design principle every hd-cnn built upon building block cnn. building block chosen currently ranked single cnns. thus hd-cnns always beneﬁt progress single design. hd-cnn follows coarse-to-ﬁne classiﬁcation paradigm probabilistically integrates predictions category classiﬁers. compared building block corresponding hd-cnn achieve lower error cost manageable increase memory footprint classiﬁcation time. summary paper following contributions. first introduce novel hierarchical architecture called hd-cnn image classiﬁcation. second develop scheme learning two-level organization coarse categories demonstrate various components hd-cnn independently pretrained. complete hd-cnn ﬁne-tuned using multinomial logistic loss regularized coarse category consistency term. third make hd-cnn scalable compressing layer parameters conditionally executing category classiﬁers. performed evaluations medium-scale cifar dataset large-scale imagenet -class dataset method achieved state-of-the-art performance them. work inspired progresses design efforts integrating category hierarchy linear classiﬁers. main novelty method scalable hd-cnn architecture integrates category hierarchy deep cnns. cnn-based models hold state-of-the-art performance various computer vision tasks including image classifcation object detection image parsing recently considerable interest enhancing components including pooling layers activation units nonlinear layers enhancements either improve training expand network learning capacity. work boosts performance orthogonal angle redesign speciﬁc part within existing model. instead design novel generic hierarchical architecture uses existing model building block. embed multiple building blocks larger hierarchical deep cnn. visual recognition vast literature exploiting category hierarchical structures classiﬁcation large number classes using linear classiﬁers common strategy build hierarchy taxonomy classiﬁers number classiﬁers evaluated given testing image scales sub-linearly number classes hierarchy either predeﬁned learnt top-down bottom-up approaches predeﬁned category hierarchy imagenet dataset utilized achieve trade-offs classiﬁcation accuracy speciﬁcity. hierarchical label tree constructed probabilistically combine predictions leaf nodes. hierarchical classiﬁer achieves signiﬁcant speedup cost certain accuracy loss. earliest attempts introduce category hierarchy cnn-based methods reported main goal transferring knowledge classes improve results classes insufﬁcient training examples. various label relations encoded hierarchy. improved accuracy achieved subset training images relabeled internal nodes hierarchical class tree. able improve accuracy original setting training images labeled leaf nodes. hierarchy cnns introduced experimented coarse categories mainly scalability constraints. hdcnn exploits category hierarchy novel embed deep cnns hierarchy scalable manner achieves superior classiﬁcation results standard cnn. following notations used below. dataset consists images yi}i. denote image data label respectively. categories images dataset learn category hierarchy coarse categories hd-cnn designed mimic structure category hierarchy categories divided coarse categories performs end-to-end classiﬁcation illustrated mainly comprises four parts namely shared layers single coarse category component multiple category components single probabilistic averaging layer. left side shared layers. receive image pixel input extract low-level features. conﬁguration shared layers preceding layers building block net. independent layers coarse category component reuses conﬁguration rear layers building block produces prediction image produce prediction {bik}k coarse categories append ﬁne-tocoarse aggregation layer aggregates predictions coarse ones mapping categories coarse ones given. coarse category probabilities serve purposes. first used weights combining predictions made category components. second thresholded enable conditional executions category components whose corresponding coarse probabilities sufﬁciently large. bottom-right independent layers category classiﬁers makes category predictions. component excels classifying small categories produce prediction partial categories. probabilities categories absent partial implicitly zero. layer conﬁgurations mostly copied building block except ﬁnal classiﬁcation layer number ﬁlters size partial instead full categories. coarse category component category components share common layers. reason three-fold. first shown preceding layers deep networks response class-agnostic low-level features corners edges rear layers extract classspeciﬁc features face bird’s legs. since low-level features useful coarse classiﬁcation tasks allow preceding layers shared coarse components. second reduces total ﬂoating point operations memory footprint network execution. practical signiﬁcance deploy hd-cnn real applications. last least decrease number hd-cnn parameters critical success hd-cnn training. stress coarse category components reuse layer conﬁgurations building block cnn. ﬂexible modular design allows choose best module building block depending task hand. goal building category hierarchy grouping confusing categories coarse category dedicated category classiﬁer trained. employ top-down approach learn hierarchy training data. randomly sample held-out images balanced class distribution training set. rest training used train building block net. obtain confusion matrix evaluating held-out set. distance matrix derived diagonal entries zero. transformed symmetric. entry measures easy discriminate categories spectral clustering performed cluster categories coarse categories. result twolevel category hierarchy representing many-to-one mapping coarse categories. here coarse categories disjoint. overlapping coarse categories disjoint coarse categories overall classiﬁcation depends heavily coarse category classiﬁer. image routed incorrect category classiﬁer mistake corrected probability ground truth label implicitly zero there. removing separability constraint coarse categories make hd-cnn less dependent coarse category classiﬁer. therefore categories coarse categories. certain classiﬁer prefer categories likely misclassﬁed coarse category therefore estimate likelihood image category misclassiﬁed coarse category held-out set. coarse category component category components properly pretrained ﬁne-tune complete hd-cnn. category hierarchy well associated mapping learnt category component focuses classifying ﬁxed subset categories. ﬁne-tuning semantics coarse categories predicted coarse category component kept consistent associated category components. thus coarse category consistency term regularize conventional multinomial logistic loss. coarse category consistency learnt ﬁne-to-coarse category mapping provides specify target coarse category distribution {tk}. speciﬁcally fraction training images within coarse category assumption distribution coarse categories across training dataset close within training mini-batch. threshold mapping likelihood using parametric variable categories partial note branching component gives full prediction disjoint prediction overlapping coarse categories category hierarchy mapping extended many-tomany mapping coarse category predictions updated accordingly j|k∈p bij. note exceeds hence perform normalization. overlapping coarse categories also shown useful hierarchical linear classiﬁers embed category components hd-cnn number parameters rear layers grows linearly number coarse categories. given amount training data increases training complexity risk over-ﬁtting. hand training images within stochastic gradient descent mini-batch probabilistically routed different category components. requires larger mini-batch ensure parameter gradients category components estimated sufﬁciently large number training samples. large training mini-batch increases training memory footprint slows training process. therefore decompose hd-cnn training multiple steps instead training complete hd-cnn scratch outlined algorithm ﬁrst pretrain building block using training set. preceding rear layers coarse category component resemble layers building block copy weights coarse category component initialization purpose. fine category components independently pretrained parallel. specialize classifying categories within coarse category therefore pretraining uses images {xi|i coarse category shared preceding laysc already initialized kept ﬁxed stage. category components hd-cnn number parameters memory footprint execution time rear layers scale linearly number coarse categories. ensure hd-cnn scalable large-scale visual recognition develop conditional execution layer parameter compression techniques. conditional execution. test time given image necessary evaluate category classiﬁers insigniﬁcant weights contributions ﬁnal prediction negligible. conditional executions weighted components accelerate hd-cnn classiﬁcation. therefore threshold using parametric variable reset zero category classiﬁers evaluated. parameter compression. hd-cnn number parameters rear layers category classiﬁers grows linearly number coarse categories. thus compress layer parameters test time reduce memory footprint. speciﬁcally choose product quantization approach compress parameter matrix rm×n ﬁrst partitioning horizontally segments width k-means clustering used cluster rows storing nearest cluster indices -bit integer matrix cluster centers single-precision ﬂoating number matrix rk×n achieve compression factor hyperparameters parameter compression evaluate hd-cnn benchmark datasets cifar imagenet hd-cnn implemented widely deployed caffe software. network trained back propagation testing experiments single nvidia tesla card. cifar dataset consists classes natural images. training images testing images. follow preprocess datasets randomly cropped ﬂipped image patches size used training. adopt network three stacked layers denote cifar-nin hd-cnn building block. fine category components share preceding layers conv pool accounts total parameters total ﬂoating point operations. remaining layers used independent layers. building category hierarchy randomly choose images training held-out set. fine categories within coarse categories visually similar. pretrain rear layers category components. initial learning rate decreased factor every iterations. fine-tuning performed iterations large mini-batches size initial learning rate reduced factor iterations. evaluation -view testing extract patches well horizontal reﬂections average predictions. cifar-nin obtains testing error. hd-cnn achieves testing error improves building block category hierarchy. construction category hierarchy number coarse categories adjusted clustering algorithm. also make coarse categories either disjoint overlapping varying hyperparameter thus investigate impacts classiﬁcation error. experiment coarse categories vary value best results obtained overlapping coarse categories shown left. histogram category occurrences overlapping coarse categories shown right. optimal value coarse category number hyperparameter dataset dependent mainly affected inherent hierarchy within categories. figure left hd-cnn -view testing error number coarse categories cifar dataset. right histogram category occurrences overlapping coarse categories. putational complexity memory footprint hd-cnn sublinear number category classiﬁers compared building block net. hd-cnn category classiﬁers based cifar-nin consumes less three times much memory building block without parameter compression. also want investigate impact shared layers classiﬁcation error memory footprint execution time build another hd-cnn coarse category component category components independent preceding layers initialized pretrained building block net. single-view testing central cropping used observe minor error increase using shared layers dramatically reduces memory footprint testing time seconds seconds. conditional executions. varying hyperparameter effectively affect number category components executed. trade-off execution time classiﬁcation error. larger value leads higher accuracy cost executing components categorization. enabling conditional executions hyperparameter obtain substantial speed merely minor increase error testing time hd-cnn times much building block net. parameter compression. category cnns independent layers conv cccp compress reduce memory footprint minor increase error comparison strong baseline. hd-cnn memory footprint times much building block model necessary compare stronger baseline similar complexity hd-cnn. adapt cifar-nin double number ﬁlters convolutional layers accordingly increases memory footprint three times. denote cifar-nindouble obtain error lower building block higher hd-cnn. comparison model averaging. hd-cnn fundamentally different model averaging model averaging models capable classifying full categories trained independently. main sources prediction differences different initializations. hd-cnn category classiﬁer excels classifying partial categories. compare hd-cnn model averaging independently train cifar-nin networks take averaged prediction ﬁnal prediction. obtain error higher hdcnn note hd-cnn orthogonal model averaging ensemble hd-cnn networks improve performance. coarse category consistency. verify effectiveness coarse category consistency term loss function ﬁne-tune hd-cnn using traditional multinomial logistic loss function. testing error higher hd-cnn ﬁne-tuned coarse category consistency comparison state-of-the-art. hd-cnn improves current best methods respectively sets state-of-the-art results cifar ilsvrc- imagenet dataset consists million training images validation images. demonstrate generality hd-cnn experiment different building block nets. cases hdcnns achieve signiﬁcantly lower testing errors building block nets. choose public -layer ﬁrst building block greatly reduced number parameters compared alexnet similar error rates. denoted imagenet-nin. hd-cnn various components share preceding layers conv pool account total parameters total ﬂoating point operations. follow training testing protocols original images resized randomly cropped horizontally reﬂected patches used training. test time makes -view averaged prediction. train imagenet-nin epochs. top- top- errors build category hierarchy take training images held-out overlapping coarse categories. category ﬁne-tuned iterations initial learning rate decreased factor every iterations. fine-tuning complete hd-cnn performed required minibatch size signiﬁcantly higher building block net. nevertheless still achieve top- top- errors improve building block respectively classwise top- error improvement building block shown left. case studies want investigate hd-cnn corrects mistakes made building block net. figure case studies imagenet dataset. represents testing case. column test image ground truth label. column guesses building block imagenet-nin. column coarse category probabilities. column guesses made category components. column ﬁnal guesses made hd-cnn. text details. table comparison testing errors memory footprint testing time building block nets hd-cnns cifar imagenet datasets. statistics collected single-view testing. three building block nets cifar-nin imagenet-nin imagenet-vgg-layer used. testing mini-batch size notations sl=shared layers ce=conditional executions pc=parameter compression. collect four testing cases. ﬁrst case building block fails predict label tiny hermit crab guesses. hd-cnn coarse categories receive coarse probability mass. category component specializes classifying crab breeds strongly suggests ground truth label. combining predictions category classiﬁers hd-cnn predicts hermit crab probable label. second case imagenet-nin confuses ground truth hand blower objects close shapes appearances plunger barbell. hd-cnn coarse category component also conﬁdent coarse category object belongs thus assigns even probability mass coarse categories. category classiﬁers strongly predicts ground truth label rank ground truth label place respectively. overall hd-cnn ranks ground truth label place. demonstrates hd-cnn needs rely multiple category classiﬁers make correct predictions difﬁcult cases. overlapping coarse categories.to investigate impact overlapping coarse categories classiﬁcation train another hd-cnn category classiﬁers using disjoint coarse categories. achieves top- top- errors respectively higher hd-cnn using overlapping coarse category hierarchy conditional executions. varying hyperparameter control number category components executed. trade-off execution time classiﬁcation error shown right. larger value leads lower error cost executed category components. enabling conditional executions hyperparameter obtain substantial speed merely minor increase single-view testing top- error speedup hd-cnn testing time less times much building block net. parameter compression. compress independent layers conv cccp account parameters imagenet-nin. parameter matrices size compression hyperparameters compression factors compression decreases memory footprint merely increases top- error single-view testing figure left class-wise hd-cnn top- error improvement building block net. right mean number executed category classiﬁers top- error hyperparameter imagenet validation dataset. comparison model averaging. hd-cnn memory footprint three times much building block independently train three imagenet-nin nets average predictions. obtain top- error lower building block higher hd-cnn second building block -layer denote imagenet-vgg--layer. layers conv pool shared account total parameters total ﬂoating number operations. remaining layers used independent layers coarse category classiﬁers. follow training testing protocols training ﬁrst sample size range resize image length short edge randomly cropped ﬂipped patch size used training. testing dense evaluation performed three scales averaged prediction used ﬁnal prediction. please refer training testing details. imagenet validation imagenet-vgg--layer achieves top- top- errors respectively. build category hierarchy overlapping coarse categories. implement multi-gpu training caffe exploiting data parallelism train category classiﬁers nvidia tesla cards. initial learning rate decreased factor every iterations. hd-cnn ﬁne-tuning performed. large memory footprint building block hd-cnn category classiﬁers cannot memory directly. therefore compress parameters layers account parameters. parameter matrices size compression hyperparameters compression factors respectively. hd-cnn obtains top- top- errors imagenet validation improves imagenet-vgg--layer respectively. comparison state-of-the-art. currently best nets imagenet dataset googlenet -layer network using multi-scale multicrop testing single googlenet achieves top- error multi-scale dense evaluation single layer obtains top- top- errors improves top- error googlenet hdcnn decreases top- top- errors -layer respectively. furthermore hd-cnn slightly outperforms results averaging predictions vgg--layer vgg--layer nets. demonstrated hd-cnn ﬂexible deep architecture improve existing deep models. showed empirically cifar- imagenet datasets using three different building block nets. part future work plan extend hd-cnn architectures hierarchical levels also verify empirical results theoretical framework.", "year": 2014}