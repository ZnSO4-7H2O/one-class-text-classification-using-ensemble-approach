{"title": "Deep Motif Dashboard: Visualizing and Understanding Genomic Sequences  Using Deep Neural Networks", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Deep neural network (DNN) models have recently obtained state-of-the-art prediction accuracy for the transcription factor binding (TFBS) site classification task. However, it remains unclear how these approaches identify meaningful DNA sequence signals and give insights as to why TFs bind to certain locations. In this paper, we propose a toolkit called the Deep Motif Dashboard (DeMo Dashboard) which provides a suite of visualization strategies to extract motifs, or sequence patterns from deep neural network models for TFBS classification. We demonstrate how to visualize and understand three important DNN models: convolutional, recurrent, and convolutional-recurrent networks. Our first visualization method is finding a test sequence's saliency map which uses first-order derivatives to describe the importance of each nucleotide in making the final prediction. Second, considering recurrent models make predictions in a temporal manner (from one end of a TFBS sequence to the other), we introduce temporal output scores, indicating the prediction score of a model over time for a sequential input. Lastly, a class-specific visualization strategy finds the optimal input sequence for a given TFBS positive class via stochastic gradient optimization. Our experimental results indicate that a convolutional-recurrent architecture performs the best among the three architectures. The visualization techniques indicate that CNN-RNN makes predictions by modeling both motifs as well as dependencies among them.", "text": "deep neural network models recently obtained state-of-the-art prediction accuracy transcription factor binding site classiﬁcation task. however remains unclear approaches identify meaningful sequence signals give insights bind certain locations. paper propose toolkit called deep motif dashboard provides suite visualization strategies extract motifs sequence patterns deep neural network models tfbs classiﬁcation. demonstrate visualize understand three important models convolutional recurrent convolutional-recurrent networks. ﬁrst visualization method ﬁnding test sequence’s saliency uses ﬁrst-order derivatives describe importance nucleotide making ﬁnal prediction. second considering recurrent models make predictions temporal manner introduce temporal output scores indicating prediction score model time sequential input. lastly class-speciﬁc visualization strategy ﬁnds optimal input sequence given tfbs positive class stochastic gradient optimization. experimental results indicate convolutional-recurrent architecture performs best among three architectures. visualization techniques indicate cnn-rnn makes predictions modeling motifs well dependencies among them. recent years explosion deep learning models lead groundbreaking results many ﬁelds computer vision natural language processing computational biology however although models proven accurate widely viewed black boxes complexity making hard understand. particularly unfavorable biomedical domain understanding model’s predictions extremely important doctors researchers trying model. aiming open black present deep motif dashboard understand inner workings deep neural network models genomic sequence classiﬁcation task. introducing suite different neural models visualization strategies ones perform best understand make predictions. understanding genetic sequences fundamental tasks health advancements high correlation genes diseases drugs. important problem within genetic sequence understanding related transcription factors regulatory proteins bind dna. different binds speciﬁc transcription factor binding sites genome regulate cell machinery. given input sequence classifying whether binding site particular core task bioinformatics. task follow step approach. first given particular interest dataset containing samples positive negative tfbs sequences construct three deep learning architectures classify sequences. section introduces three different structures convolutional neural network recurrent neural network convolutional-recurrent neural network trained models predict binding sites second step approach understand models perform explained section introducing three different visualization strategies interpreting models measuring nucleotide importance saliency maps. measuring critical sequence positions classiﬁer using temporal output scores. generating class-speciﬁc motif patterns class optimization. test evaluate models visualization strategies large scale benchmark tfbs dataset. section provides experimental results understanding visualizing three architectures. cnn-rnn outperforms models. visualizations observe cnn-rnn tends focus predictions traditional motifs well modeling long range dependencies among motifs. tfbs classiﬁcation. chromatin immunoprecipitation technologies databases encode made binding site locations available hundreds different tfs. despite advancements major drawbacks chip-seq experiments slow expensive although chip-seq experiments binding site locations cannot patterns common across positive binding sites give insight bind locations. thus need large scale computational methods make accurate binding site classiﬁcations also identify understand patterns inﬂuence binding site locations. order computationally predict tfbss sequence researchers initially used consensus sequences position weight matrices match test sequence simple neural network classiﬁers proposed differentiate positive negative binding sites show signiﬁcant improvements weight matrix matching methods later techniques outperformed generative methods using k-mer features string kernel based systems limited expensive computational cost proportional number training testing sequences. recently convolutional neural network models shown state-of-the-art results tfbs task scalable large number genomic sequences remains unclear neural architectures work best. deep neural networks tfbss. neural models work best tfbs classiﬁcation task examine several different types models. inspired success across different ﬁelds explore variations popular deep learning architectures convolutional neural networks recurrent neural networks cnns dominated ﬁeld computer vision recent years obtaining state-of-the-art results many tasks ability automatically extract translation-invariant features. hand rnns emerged powerful models sequential data tasks natural language processing ability learn long range dependencies. speciﬁcally tfbs prediction task explore three distinct architectures combination cnn-rnn. figure shows overview models. end-to-end deep framework. body three architectures differ implemented model follows similar end-to-end framework easily compare contrast results. nucleotide characters inputs character converted one-hot encoding encoding matrix used input convolutional recurrent convolutional-recurrent module outputs vector ﬁxed dimension. output vector model linearly softmax function last layer learns mapping hidden space output class label space ﬁnal output probability indicating whether input positive negative binding site parameters network trained end-to-end minimizing negative log-likelihood training set. minimization loss function obtained stochastic gradient algorithm adam mini-batch size sequences. dropout regularization method model. genomic sequences believed regulatory mechanisms transcription factor binding inﬂuenced local sequential patterns known motifs. motifs viewed temporal equivalent spatial patterns images eyes face cnns able automatically learn achieve state-of-the results computer vision tasks. result temporal convolutional neural network ﬁtting model automatically extract motifs. temporal convolution ﬁlter size takes input data matrix size length input layer size outputs matrix size nout nout output layer size. speciﬁcally convolution convolution nonlinearity cnns typically maxpooling dimension reduction technique provide translation invariance extract higher level features wider range input sequence. temporal maxpooling matrix pooling size results output matrix formally maxpool implementation involves progression convolution nonlinearity maxpooling. represented convolutional layer network test layer deep cnns. ﬁnal layer involves maxpool across entire temporal domain ﬁxed-size vector softmax classiﬁer. figure shows model convolutional layers. input one-hot encoded matrix convolved several ﬁlters relu nonlinearity produce matrix convolution activations. perform maxpool activation matrix. output ﬁrst maxpool another convolution relu maxpooled across entire length resulting vector. vector transposed linear softmax layer classiﬁcation. designed handle sequential data recurrent neural networks become main neural model tasks natural language understanding. advantage rnns cnns able long range patterns data highly dependent ordering sequence prediction task. given input matrix size produces matrix size embedding size. timestep takes input column vector rnin previous hidden state vector produces next hidden state applying following recursive operation trainable parameters model element-wise nonlinearity. recursive nature rnns model full conditional distribution sequential data dependencies time position sequence timestep imaginary time coordinate running certain direction. handle vanishing gradients problem training basic rnns long sequences hochreiter schmidhuber proposed variant called long short-term memory network handle long term dependencies using gating functions. gates control information written read from forgotten. speciﬁcally lstm cells take inputs produce tanh element-wise sigmoid hyperbolic tangent multiplication functions respectively. input forget output gates respectively. rnns produce output vector timestep input sequence. order classiﬁcation task take mean vectors mean vector hmean input softmax layer. since innate direction genomic sequences bi-directional lstm model. bi-directional lstm input sequence gets lstm networks direction output vectors direction concatenated together temporal direction linear classiﬁer. figure shows model. input one-hot encoded matrix lstm forward backward direction produce matrix column vectors representing lstm output embedding timestep. vectors averaged create vector direction representing lstm output. forward backward output vectors concatenated softmax classiﬁcation. considering convolutional networks designed extract motifs recurrent networks designed extract temporal features implement combination order temporal patterns motifs. given input matrix rt×nin output rt×nout. column vector gets time one-hot encoded vectors input regular model. resulting output rt×d lstm embedding size averaged across temporal domain softmax classiﬁer. figure shows cnn-rnn model. input one-hot encoded matrix layer convolution produce convolution activation matrix. matrix input lstm done regular model original one-hot matrix. output lstm averaged concatenated softmax similar rnn. previous section explained deep models tfbs classiﬁcation task evaluate models perform best. making accurate predictions important biomedical tasks equally important understand models make predictions. accurate uninterpretable models often slow emerge practice inability understand predictions making biomedical domain experts reluctant them. consequently obtain better understanding certain models work better others investigate make predictions introducing several visualization techniques. proposed demo dashboard allows visualize understand dnns three different ways saliency maps temporal output scores class optimizations. certain sequence model’s classiﬁcation logical question which parts sequence inﬂuential classiﬁcation? this seek visualize inﬂuence position prediction. approach similar methods used images simonyan baehrens al.. given sequence length class model provides score function rank nucleotides based inﬂuence score since highly non-linear function deep neural nets hard directly inﬂuence nucleotide mathematically around point approximated linear function computing ﬁrst-order taylor expansion derivative simply step backpropagation model therefore easy compute. pointwise multiplication saliency one-hot encoded sequence derivative values actual nucleotide characters sequence inﬂuence character position output score. finally take element-wise magnitude resulting derivative vector visualize important character regardless derivative direction. call resulting vector saliency tells nucleotides need changed least order affect class score most. equation saliency simply weighted input nucleotides weight indicates inﬂuence nucleotide position output score. since sequential insightful visualize output scores timestep sequence call temporal output scores. assume imaginary time direction running left right given sequence position sequence timestep imagined time coordinate. words check rnn’s prediction scores vary input rnn. input series constructed using subsequences input running along imaginary time coordinate subsequences start ﬁrst nucleotide ends entire sequence exactly sequence recurrent model changes decision negative positive vice versa. since recurrent models bi-directional also technique reverse sequence. cnns process entire sequence once thus can’t view output temporal sequence visualization cnn-rnn. previous visualization methods listed representative speciﬁc testing sample introduce approach extract class-speciﬁc visualization model attempt best sequence maximizes probability positive tfbs call class optimization. formally optimize following equation probability input sequence positive tfbs computed softmax equation trained model speciﬁc regularization parameter. locally optimal stochastic gradient descent optimization respect input sequence. optimization model weights remain unchanged. similar methods used simonyan optimize toward speciﬁc image class. visualization method depicts notion positive tfbs class particular speciﬁc test sequence. three proposed visualization techniques allow manually inspect models make predictions. order automatically patterns techniques also propose methods extract motifs consensus subsequences represent positive binding sites. extract motifs three visualization methods following ways positive test sequence extract motif saliency selecting contiguous length- subsequence achieves highest contiguous length- saliency values. positive test sequence extract motif temporal output scores selecting length- subsequence shows strongest score change negative positive output score. different directly class-optimized sequence motif. neural networks produced state-of-the-art results several important benchmark tasks related genomic sequence classiﬁcation making good candidate use. however models work well poorly understood. recent works attempted uncover properties models work done understanding image classiﬁcations using convolutional neural networks. zeiler fergus used deconvolution approach hidden layer representations back input space speciﬁc example showing features image important classiﬁcation. simonyan explored similar approach using ﬁrst-order taylor expansion linearly approximate network input features relevant also tried optimizing image classes. many similar techniques later followed understand convolutional models importantly researchers found cnns able extract layers translational-invariant feature maps indicate cnns successfully used genomic sequence predictions believed triggered motifs. text-based tasks fewer visualization studies dnns. karpathy explored interpretability rnns language modeling found exist interpretable neurons able focus certain language structure quotes. visualized rnns achieve compositionality natural language sentiment analysis visualizing embedding vectors well measuring inﬂuence input words classiﬁcation. studies show examples validated understanding natural language linguistics. contrarily interested understanding linguistics given dnns al.). main difference work previous works images natural language instead trying understand dnns given human understanding human perception tasks attempt uncover critical signals sequences given understanding dnns. tfbs prediction alipanahi ﬁrst implement visualization method model. visualize model extracting motifs based input subsequence corresponding strongest activation location convolutional ﬁlter since convolutional layer trivial activations back method work well deeper models. attempted technique models found approach using saliency maps outperforms ﬁnding motif patterns quang visualization method convolutional-recurrent model noncoding variant prediction. dataset. order evaluate models visualizations train test cell encode chip-seq datasets used alipanahi al.. dataset average training sequences sequence consists dna-base characters every dataset testing sequences positive sequences extracted genome centered reported chip-seq peak. negative sequences generated dinucleotide-preserving variations models. implement several variations architecture varying hyperparameters. table shows different hyperparameters architecture. trained many different hyperparameters architecture show best performing model type surrounded larger smaller version show isn’t underﬁtting overﬁtting. baselines. meme-chip results alipanahi prediction performance baseline. results applying meme-chip positive training sequences deriving pwms scoring test sequences using scores using pwms. also compare model proposed alipanahi al.. evaluate motif extraction compare convolution activation method used alipanahi quang strongest ﬁrst layer convolution ﬁlter activation back input sequence inﬂuential length- subsequence. table shows mean area curve scores tested models expected models outperform standard models. validates hypothesis positive binding sites mainly triggered local patterns motifs cnns easily ﬁnd. interestingly cnn-rnn achieves best performance among three deep architectures. check statistical signiﬁcance comparisons apply pairwise t-test using scores report tailed p-values table apply t-test best performing models model type. deep models signiﬁcantly better meme baseline. signiﬁcantly better cnn-rnn signiﬁcantly better cnn. order understand cnn-rnn performs best turn dashboard visualizations. figure demo dashboard. dashboard examples gata mafk nfyb positive tfbs sequences. section dashboard contains class optimization middle section contains saliency maps speciﬁc positive test sequence bottom section contains temporal output scores positive test sequence used saliency map. contains known jaspar motifs highlighted pink boxes test sequences contain motifs. evaluate dashboard visualization methods ﬁrst manually inspect dashboard visualizations look interpretable signals. figure shows examples demo dashboard three different positive tfbs sequences. apply visualizations best performing models three architectures. dashboard snapshot speciﬁc contains jaspar motifs gold standard motifs generated biomedical researchers positive tfbs class-optimized sequence architecture positive tfbs test sequence interest jaspar motifs test sequences highlighted using pink saliency model test sequence forward backward temporal output scores recurrent architectures test sequence. saliency maps position inﬂuential prediction. temporal outputs blue indicates negative tfbs prediction indicates positive. saliency temporal output visualizations positive test sequence numbers next model names saliency section indicate score outputs model speciﬁed test sequence. saliency maps visual inspection saliency maps cnns tend focus short contiguous subsequences predicting positive bindings. words cnns clearly model motifs inﬂuential prediction. saliency maps rnns tend spread across entire sequence indicating focus nucleotides together infer relationships among them. cnn-rnns strong saliency values around motifs also nucleotides away motifs inﬂuential model’s prediction. example cnn-rnn model conﬁdent gata tfbs prediction prediction also inﬂuenced nucleotides outside motif. mafk saliency maps cnn-rnn focus wide range nucleotides make predictions doesn’t even focus known jaspar motif make high conﬁdence prediction. temporal output scores sequences tested positions trigger model switch negative tfbs prediction positive near jaspar motifs. observe clear differences forward backward temporal output patterns. certain cases it’s interesting look temporal output scores saliency maps together. important case study examples nfyb example perform poorly cnn-rnn makes correct prediction. observe cnn-rnn able switch classiﬁcation negative positive never does. understand happened saliency maps cnn-rnn focuses distinct regions ﬂips classiﬁcation negative positive. however doesn’t focus either areas reason it’s never able classify positive sequence. fact able classify positive sequence focuses regions cnn-rnn indicate temporal dependencies regions inﬂuence binding. addition fact clear jaspar motif sequence show traditional motif approach always best model tfbss. class optimization class optimization model generates concise representations often resemble known motifs particular recurrent models tfbs positive optimizations less clear though aspects stand notice certain models class optimized sequences optimize reverse complement motif class optimizations useful getting general idea triggers positive tfbs certain automatic motif extraction dashboard. order evaluate dnn’s capability automatically extract motifs compare found motifs method corresponding jaspar motif interest. comparison using tomtom tool searches query motif given motif database returns signiﬁcant matches ranked p-value indicating motif-motif similarity. table summarizes motif matching results comparing visualization-derived motifs known motifs jaspar database. limited comparison datasets jaspar motifs for. compare four visualization approaches saliency convolution activation temporal output scores class optimizations. ﬁrst three techniques sequence speciﬁc therefore report average number motif matches positive sequences last technique particular tfbs positive class. table across multiple visualization techniques ﬁnds motifs best followed cnn-rnn rnn. however since cnns perform worse cnn-rnns scores hypothesize demonstrates also important model sequential interactions among motifs. cnn-rnn combination acts like motif ﬁnder ﬁnds dependencies among motifs. analysis shows visualizing classiﬁcations lead better understanding dnns tfbss. deep neural networks shown accurate models tfbs classiﬁcation. however models hard interpret thus adaptation practice slow. work propose deep motif dashboard explore three different architectures tfbs prediction introduce three visualization methods shed light models work. although visualization methods still require human practitioner examine dashboard start understand models hope work invoke studies visualizing understanding based genomic sequences analysis. furthermore models recently shown provide excellent results epigenomic analysis plan extend demo dashboard related applications.", "year": 2016}