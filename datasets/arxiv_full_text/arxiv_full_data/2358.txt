{"title": "Dynamic Trees: A Structured Variational Method Giving Efficient  Propagation Rules", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Dynamic trees are mixtures of tree structured belief networks. They solve some of the problems of fixed tree networks at the cost of making exact inference intractable. For this reason approximate methods such as sampling or mean field approaches have been used. However, mean field approximations assume a factorized distribution over node states. Such a distribution seems unlickely in the posterior, as nodes are highly correlated in the prior. Here a structured variational approach is used, where the posterior distribution over the non-evidential nodes is itself approximated by a dynamic tree. It turns out that this form can be used tractably and efficiently. The result is a set of update rules which can propagate information through the network to obtain both a full variational approximation, and the relevant marginals. The progagtion rules are more efficient than the mean field approach and give noticeable quantitative and qualitative improvement in the inference. The marginals calculated give better approximations to the posterior than loopy propagation on a small toy problem.", "text": "dynamic trees mixtures tured belief problems fixed cost making exact inference reason approximate methods sampling used. however assume factorised states. posterior prior. structured approach bution non-evidential approximated form used tractably efficiently propagate network proximation propagation mean field approach quantitative inference. better loopy propagation dynamic trees theory components prior distribution conditional ents tree architecture. namic tree model nodes arranged node 'chooses' above. nodes tive parents probabilities denote particular ultimately determine pixel seen leaf nodes network. main subject namic tree model images. hierarchical larly versatile improvements fixed tree belief quadtree networks. useful bilistic inference ever quadtrees quadtrees produce consider possible denote possible nodes indicator. fact node parent node finally represent node state zero otherwise. dynamic tree represented possible work states assume prior factorises purpose asking know image came about. gives prior knowledge might expect image. prior knowledge 'generative image used refine prior perceived knowledge states prior network models) network. whatever ditional value conditional given another conditional moving state state traversing network model directed good define hierarchical model. structure different state given terms possible states edge graph goes parent node child node). words whether affects state probability reasonable dependence occur lationship. probabilistically given data instantiate leaf posterior nodes network. rior probabilities exactly tree mix­ involve ture number trees scales square number nodes. therefore need resort outlined note prior mixture ables integrated work representation child nodes layer above. some­ times useful think terms henceforth parent belief intuition behind dynamic trees inference simulated teriori compared mean field approach approximations cantly unlikely requires factorised tree structure dependencies mean field able capture nodes biasing class cies. loopy propagation inference lief propagation singly actly calculates nected networks. priate pearl noted reasonable that. since various investigated elaborated propagation could well applied tree. dynamic explicit viable. able approximation denote approximate variational approach distribution imize kullback-liebler distribution true posterior. identical tribution becomes divergence zero. otherwise gence greater denotes proposed. constant factorised distribu­ divergence corresponds minimizing first term right hand side called vari­ ational vergence gives upper bound -logp. parameters exponentially ties form distribution general posterior variational approximation calculate marginal requirements bution. determining parameters benefit parameters using form algorithm general cussed ational size largest large. contain nodes layer clique least approach ference keeping cliques. mean field simplest dynamic proximating form takes factorising form shown spontaneous polarises data. occurs posterior resulting effect approximate posterior tree structures. would seem appropriate general approximate ture conditional significant structure culation easily node would involve pairwise pairs improvement ning approach factorises approximating might seem like rather fact tractable. detail denote mean values non-evidential notational stantiated additional tial nodes. probability node node connection take form words ele­ whatever ment matrix simply always generate nodes layer fixed; optimised tional energy contribution. introduced written simply complete process putting procedure proximating evidential value instantiated values propagate calculate conditional propagate optimise already repeat gence. vergence local zero. point convergence point divergence point relevant zero. erally mation picking possibly ignoring calculated interpretation sulting therefore fares simple nontrivial ensure posterior tree structures. four layer network defined node first three layers parents rectly probability diagonal uniform stantiated. accurately iterations recalculation ational approach date procedure. indicative. calculated vergence significantly divergence changed current step). also instructive methods produce. noted mean field result dt-structured distribution mean field distribution. approach resemble highly diagonal conditional nodes. highest comparable mean field method problems neous symmetry nodes polarise appearance lating terior seen figure nodes third layer dominated vents tree structures relating nodes utilising -structured possible reason found", "year": 2013}