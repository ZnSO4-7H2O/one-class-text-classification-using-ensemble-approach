{"title": "Sparse Nested Markov models with Log-linear Parameters", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Hidden variables are ubiquitous in practical data analysis, and therefore modeling marginal densities and doing inference with the resulting models is an important problem in statistics, machine learning, and causal inference. Recently, a new type of graphical model, called the nested Markov model, was developed which captures equality constraints found in marginals of directed acyclic graph (DAG) models. Some of these constraints, such as the so called `Verma constraint', strictly generalize conditional independence. To make modeling and inference with nested Markov models practical, it is necessary to limit the number of parameters in the model, while still correctly capturing the constraints in the marginal of a DAG model. Placing such limits is similar in spirit to sparsity methods for undirected graphical models, and regression models. In this paper, we give a log-linear parameterization which allows sparse modeling with nested Markov models. We illustrate the advantages of this parameterization with a simulation study.", "text": "hidden variables ubiquitous practical data analysis therefore modeling marginal densities inference resulting models important problem statistics machine learning causal inference. recently type graphical model called nested markov model developed captures equality constraints found marginals directed acyclic graph models. constraints called ‘verma constraint’ strictly generalize conditional independence. make modeling inference nested markov models practical necessary limit number parameters model still correctly capturing constraints marginal model. placing limits similar spirit sparsity methods undirected graphical models regression models. paper give log-linear parameterization allows sparse modeling nested markov models. illustrate advantages parameterization simulation study. analysis complex multidimensional data often made diﬃcult twin problems hidden variables dearth data relative dimension model. former problem motivates study marginal and/or latent models latter resulted development sparsity methods. particularly appealing model multidimensional data analysis bayesian network directed acyclic graph model random variables represented vertices graph directed edges them. popularity models stems well understood theory fact elicit intuitive causal interpretation arrow variable variable model interpreted made precise mean ‘direct cause’ models assume variables observed latent variable model based dags simply relaxes assumption. however latent variables introduce number problems diﬃcult correctly model latent state resulting marginal densities quite challenging work with. alternative encode constraints found marginals models directly; recent approach spirit nested markov model advantage nested markov model correctly captures conditional independences equality constraints found marginals models. however discrete parameterization nested markov models disadvantage unable represent constraints various marginals dags concisely non-zero parameters. implies model selection methods based scoring instance) often prefer simpler models fail capture independences correctly contain many fewer parameters generally high dimensional data analyses often shortage samples classical statistical inference techniques work. address issues sparsity methods developed drive many parameters statistical model zero possible still providing reasonable data. sparsity methods developed regression models undirected graphical models even marginal models paper develop log-linear parameterization discrete nested markov models parameters represent odds-ratios within ‘kernels’ viewed interaction parameters kind commonly zero sparsity methods. parameterization allows represent distributions containing ‘verma constraints’ sparse maintaining advantages nested markov models avoiding disadvantages using marginals models directly. drawback standard parameterization nested markov models parameters variation dependent; ﬁxing value parameter constrains ‘legal’ values parameters. direct contrast parameterizations models parameters associated particular markov factor depend parameters associated markov factors. illustrate another diﬃculty example. here subsequent discussions need draw distinctions vertices graphs corresponding random variables distributions ‘kernels.’ following notation denotes vertex corresponding random variable value assignment variable. likewise denotes vertex corresponding random variable assignment set. consider marginal shown fig. wish avoid representing domain directly order commit particular state space unobserved variables because even willing make assumption margin obtained density factorizes according complicated work nested markov models domain ﬁrst construct acyclic directed mixed graph represents marginal using latent projection algorithm graph shown fig. directed arrows resulting admg represent directed paths intermediate nodes unobserved similarly bidirected arrows nested m¨obius parameters described detail subsequent sections parameterize resulting admg quickly discover results model higher dimension relative dimension models share skeleton admg. example binary nested markov model graph fig. parameters binary models corresponding graphs fig. parameters each. leads worry structure learning algorithm tries nested m¨obius parameters recover admg data means score method rewards parameter parsimony prefer sample sizes incorrect independence models given dags preference correct models given admgs simply because models compensate poor data much smaller parameter count. fact precise issue observed simulation studies reported addressing problem m¨obius parameterization easy m¨obius parameters strata context-speciﬁc; words parameterization independent states labeled. instance m¨obius parameters representing confounding values binary model gives parameters. kinds regularities true generative process want exploit create dimension reduction model typically involve lack interactions among variables latent confounder dimensional state space. regularities often translate constraints naturally expressible terms m¨obius parameters. undirected graphical models also known markov random ﬁelds illustrate log-linear models. markov random ﬁeld multivariate binary state space densities represented undirected graph vertices cliques. model represents densities where conditional upon adjacent nodes node independent others. log-linear parameter corresponding subset size viewed representing k-way interactions among appropriate variables model. setting interaction parameters zero consistent results model still asserts conditional independences smaller parameter count strata clique treated symmetrically. instance parameters cliques size zero remained parameters corresponding vertices individual edges would obtain model known boltzmann machine similar idea used give sparse parameterization discrete models remainder paper describe nested markov models give log-linear parameterization models contains similar parameters zero. markov random ﬁeld models parameters associated sets nodes form cliques corresponding undirected graph nested markov models parameters associated special sets nodes corresponding admg called intrinsic sets. further log-linear parameterizations type often incorporate individual-level continuous baseline covariates types interactions among variables directly. fact parameters representing interactions well known log-linear models undirected graphical models certain regression models form special case. cadmg consider collections random variables indexed variables throughout paper random variables take values ﬁnite discrete sets ×u∈a v∈a. always hold vertex district denoted disg maximal bidirected connected containing instance admg shown fig. district node excluded deﬁnition. vertices called ancestral cadmg ancestral subset chg∩ markov blanket deﬁned obtained ﬁxing vertices using valid sequence. denote valid composition ﬁxing operations ﬁxes applied graph applied kernel. slight abuse notation suppress precise ﬁxing sequence chosen. models nested markov models deﬁned several equivalent markov properties. properties nested sense apply recursively either reachable intrinsic sets derived admg. particular nested analogue global markov property dags local markov property dags moralizationbased property dags. deﬁnitions appear proven equivalent possible associate unique admg particular marginal model nested markov model associated cadmgs reach. graph reached sequence distribution obeys nested head factorization property kernel obeys head factorization denote distributions nested markov models deﬁned parameterization maps parameters probabilities cadmg inverse m¨obius transform given generalizes standard markov parameterization dags terms paramparameterization based graphical concepts recursive heads corresponding tails. call parameterization ‘nested ingenuous’ similarity marginal log-linear parameterization called ingenuous contrast loglinear parameterizations exist nested markov models. marginal model parameterizations type ﬁrst introduced deﬁnition extends easily non-binary discrete data case parameters become collections parameters. probability variable assumes values kernel obtained ﬁxing conditioning xtail. shorthand denote parameter deﬁnition assignment values variables indexed dexed subset distribution said parameterized cadmg parameters. missing parameters reﬂect fact depend constraint induced absence edge fig. note constraint conditional independence. fact conditional independences variables corresponding vertices advertised fig. suppose wish kernel non-singleton head tail corresponding intrinsic assume inductive hypothesis already obtained kernels heads claim suﬃcient give -dimensional marginal kernels particular \\{v} reachable since valid ﬁxing sequence children ﬁxable theorem theorem imply illustrate utility setting higher order parameters zero present simulation study based admg fig. graph special case bidirected chains vertices each path directed edges alternating chains number parameters relevant binary nested markov model grows exponentially graphs type. shown m¨obius parameterization graph fig. variation dependent nested ingenuous parameterization graph variation independent. true general. particular parameterizations graph fig. variation dependent. figure histograms showing increase deviance associated setting zero nested log-linear parameters eﬀects higher orders seven respectively. corresponds removing parameters respectively; relevant density plotted case. generated distributions latent variable model associated fig. follows latent variables takes three states equal probability observed variable takes value probability generated independent uniform random variable conditional upon possible value parents. distributions produced independently using method generated dataset size ﬁtted nested model generated graph fig. dataset maximum likelihood estimation using variation algorithm found measured increase deviance associated zeroing nested ingenuous parameters corresponding eﬀects certain order. parameters truly zero would expect increase follow χ-distribution appropriate number degrees freedom; ﬁrst histograms fig. demonstrate distribution actual increases deviance looks much like relevant χ-distribution remove interactions order higher. third histogram shows starts break slightly -way interactions also zeroed. results suggest higher order parameters often useful explaining ﬁnite datasets parsimonious models obtained removing them; similar simulation performed markov case score-based search methods recovering nested markov models investigated found relatively large sample sizes required reliably recover correct graph even examples binary nodes ensuring underlying distributions approximately faithful true graph. phenomenon identiﬁed incorrect parsimonious graphs especially dags tended lower scores correct models include higher order parameters. although guaranteed smaller correct model asymptotically ﬁnite samples applies strong penalties additional parameters little explanatory power. present simulation show parameterization help overcome diﬃculty. using method described previous subsection generated multivariate binary distributions nested markov respect graph fig. distribution generated dataset ﬁtted data correct model parameters well dags given fig. note determining higher order parameters zero given data sample size remains non-trivial. automatic selection might possible l-penalized approach introduced log-linear parameterization nested markov models discrete state spaces. log-linear parameters correspond ‘interactions’ kernels obtained iterative application truncation marginalization steps contrast m¨obius parameters correspond context speciﬁc eﬀects kernels figure experiment section proportion times graph fig. lower dags fig. varying sample sizes; black proportion times restricted version model lower restricted versions either dag. plot fig. shows proportion times score correct model lower dags various sample sizes. correct graph lowest score three graphs less runs sample size increasing around shown means simulation study cases data generated marginal ‘weak confounders’ reduce dimension model ignoring higher order interaction parameters retaining advantages nested markov models compared modeling weak confounding directly dag. addition full models ﬁtted datasets versions models higher order parameters removed; graph fig. restricted zeroing -way parameter -way -way similarly restrict dags -way eﬀects giving model free parameters. fig. shows black proportion times restricted versions true model lower version either model. correct graph lowest score runs rising around note results compared directly since single ground truth used paper generated ensure faithfulness correct graph whereas randomly sampling multiple laws without bothering ensure particular properties laws consistency underlying dag. though eﬃcient closed form mapping ingenuous parameters either m¨obius parameters standard probabilities smaller disadvantage seem. cases ingenuous parameterization used select particular submodel based dataset still reparameterize m¨obius parameters even standard joint probabilities desired. moreover reparameterization step need performed once compared multiple calls ﬁtting procedure identiﬁed particular graph corresponding submodel ﬁrst place. ingenuous m¨obius parameterizations thus complementary. natural application ingenuous parameterization learning graph structure data situations many samples available expect confounding weak. natural application m¨obius parameterization support probabilistic causal inference particular graph cases eﬃcient mapping parameters joint probabilities important.", "year": 2013}