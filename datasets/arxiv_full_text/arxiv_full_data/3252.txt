{"title": "Correlation-based construction of neighborhood and edge features", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Motivated by an abstract notion of low-level edge detector filters, we propose a simple method of unsupervised feature construction based on pairwise statistics of features. In the first step, we construct neighborhoods of features by regrouping features that correlate. Then we use these subsets as filters to produce new neighborhood features. Next, we connect neighborhood features that correlate, and construct edge features by subtracting the correlated neighborhood features of each other. To validate the usefulness of the constructed features, we ran AdaBoost.MH on four multi-class classification problems. Our most significant result is a test error of 0.94% on MNIST with an algorithm which is essentially free of any image-specific priors. On CIFAR-10 our method is suboptimal compared to today's best deep learning techniques, nevertheless, we show that the proposed method outperforms not only boosting on the raw pixels, but also boosting on Haar filters.", "text": "motivated abstract notion lowlevel edge detector ﬁlters propose simple method unsupervised feature construction based pairwise statistics features. ﬁrst step construct neighborhoods features regrouping features correlate. subsets ﬁlters produce neighborhood features. next connect neighborhood features correlate construct edge features subtracting correlated neighborhood features other. validate usefulness constructed features adaboost.mh four multi-class classiﬁcation problems. signiﬁcant result test error mnist algorithm essentially free image-speciﬁc priors. cifar- method suboptimal compared today’s best deep learning techniques nevertheless show proposed method outperforms boosting pixels also boosting haar ﬁlters. paper propose simple method unsupervised feature construction based pairwise statistics features. ﬁrst step construct neighborhoods features regrouping features correlate. subsets features ﬁlters produce neighborhood features. next connect neighborhood features correlate construct edge features subtracting correlated neighborhood features other. method motivated directly notion low-level edge detector ﬁlters. ﬁlters work well practice ubiquitous ﬁrst layer biological artiﬁcial systems learn natural images. indeed four simple feature construction steps directly based abstract notion haar gabor ﬁlters homogeneous locally connected patches contrasting intensities. high-level sense technique also inspired perhaps naive notion natural neural networks work ﬁrst layer pick correlations stimuli settle simple theory world. next pick events correlations broken assign units edge features. another direct motivation comes show pixel order recovered using feature correlations. pixel order recovered immediately apply algorithms explicitly neighborhood-based ﬁlters. point view paper show possible pixel correlations feature construction without going explicit mapping pixel order. validate usefulness constructed features adaboost.mh multi-class classiﬁcation problems. boosting best shallow multi-class classiﬁers especially goal combine simple classiﬁers small subsets large possibly useful features. within multi-class boosting algorithms adaboost.mh state art. statement odds recent results multiclass boosting. data sets paper adaboost.mh hamming trees products clearly outperforms samme abc-boost importantly implementation adaboost.mh suggesting samme compared suboptimal implementation adaboost.mh priors cifar method suboptimal compared today’s best deep learning techniques reproducing basically results earliest attempts data main point experiments proposed method outperforms boosting pixels also boosting haar ﬁlters. also tried technique relatively large data sets. results essentially negative lack correlations between features allow improve significantly shallow adaboost. follows word feature realvalued representation input input feature-building algorithm ouput. intended terminology emphasize procedure applied recursively stacked autoencoders. hyperparameters threshold correlations nevertheless quite diﬀerent roles controls neighborhood size whereas controls distance neighborhoods edge interesting feature. practice found results rather insensitive value parameters interval. manually order control number features. experiments found rarely detrimental increase number features terms asymptotic test error convergence adaboost.mh slowed number features larger thousands images dimensionality input space large subsample pixels j=...d constructing neighborhoods control number neighborhood features again computational rather statistical reasons. simply adaboost.mh decision stumps autoassociative setup. decision stump uses single pixel input outputs prediction pixels. take ﬁrst stumps adaboost picks corresponding pixels construct neighborhoods. small-dimensional sets face opposite problem small number input features limit number neighborhoods. actually highlights limitation algorithm number input features small correlated number generated neighborhood edge features small essentially contain information original features. nevertheless curious whether improvement blowing number features obtain larger number neighborhoods deﬁning thresholds neighborhoods feature data sets heterogeneous feature types also important normalize features usual transformation denote mean standard deviation elements respectively proceeding feature construction optimally course automatic hyperparameter optimization especially since adaboost.mh also three hyperparameters manual grid search fourto-ﬁve dimensional hyperparameter space feasible. aside issue future work. constructed features input shallow classiﬁer. since adaboost.mh hamming trees brieﬂy describe here. full formal description pseudocode documentation multiboost available multiboost.org website along code itself. vector-valued base learners requirement suﬃces base learners weaker wihyi slightly larger zero ]=...k i=...n weight matrix current boosting iteration ±-valued one-hot code label. makes easy turn weak binary classiﬁers multi-class base classiﬁers without requiring multiclass zero-one base error less case decision tree important consequences. first size tree arbitrary tuned freely second deoutput given whereas inner nodes binary function used decide whether instance goes left right. tree constructed top-down node stump optimized greedy manner usual. given unless happens one-hot vector single class output time tree perfectly boostable weighted hamming trees produces real -valued vector length predicted class easily derived using operator. carried experiments four data sets mnist cifar- standard image classiﬁcation data sets pendigit letter sets relatively large benchmarks repository. boosted hamming trees data sets. boosted hamming trees three hyperparameters number boosting iterations number leaves number features considered split settings give ﬂavor random forest ﬁnal classiﬁer). three validated number leaves classical using single validation training set. since adaboost.mh exhibit overﬁtting even after large number iterations large number iterations report average test error last iterations. number features considered split another hyperparameter tuned traditional way. experience larger smaller asymptotic test error hand larger slower algorithm converges error. means controls tradeneighborhood edge features constructed described section estimating correlations done robustly relatively small random samples algorithm ranhand-written digits size experiments mnist ﬁrst baseline adaboost.mh hamming trees leaves pixels achieving test error also adaboost.mh hamming trees leaves roughly -dimensional feature space generated types haar ﬁlters setup produced test error state among boosting algorithms. generating neighborhood edge features ﬁrst autoassociative adaboost.mh decision stumps iterations picked pixels depicted white pixels figure constructed neighborhood features using edge features using important features depicted figure finally adaboost.mh hamming trees leaves constructed features achieving test error best results among methods explicit image priors note adaboost.mh picked slightly neighborhood edge features relatively prior proportions. hand crucial include neighborhood edge features adaboost.mh suboptimal either subset. object categories size giving total features. experiments cifar ﬁrst baseline adaboost.mh hamming trees leaves pixels achieving test error also adaboost.mh hamming trees leaves roughly -dimensional feature space generated types haar ﬁlters setup produced test error generating neighborhood edge features ﬁrst autoassociative adaboost.mh decision stumps iterations picked color channels depicted white colored pixels figure constructed neighborhood features using edge features using finally adaboost.mh hamming trees leaves constructed features achieving test error none results close sate completely either match performance early techniques reported error cifar- main signiﬁcance experiment adaboost.mh neighborhood edge features beat adaboost.mh pixels also adaboost.mh haar features. principle reason neighborhood edge features could work non-image sets. investigate preliminary tests relatively large data sets pendigit letter. contain features several thousand instances. baseline results pendigit using adaboost.mh hamming trees leaves letter using adaboost.mh hamming trees leaves constructed neighborhoods using threshproceeded constructing edge features giving features letter features letter. adaboost.mh hamming trees number leaves baseline experiments using pendigit obtained better baseline letter obtained signiﬁcantly worse baseline reasons larger gain diﬃcult sets. first much correlation features exploit. indeed setting similar values used image sets neighborhoods would small would almost figure initial pixel selection using autoassociative adaboost.mh. white means pixel selected black means not. colors mixtures color channels selected. white means channels selected black means none selected. figure important neighborhood edge features picked adaboost.mh. neighborhood features mark middles pixels neighborhood green dots. edge features mark middles pixels positive neighborhood blue dots pixels negative neighborhood dots. black&white images cases averages mnist test images respond strongest given ﬁlter. question whether stacking neighborhood edge features would work. technical problem re-running feature construction features obtained ﬁrst round clear whether structure simple method exploit. preliminary trials mnist improve results mnist relatively simple complex features rather homogeneous classes. experimenting stacking cifar deﬁnitely next step. another interesting avenue launch large scale exploration non-image benchmark sets whether subclass sets correlation-based feature construction work characterize subclass.", "year": 2013}