{"title": "Review and Evaluation of Feature Selection Algorithms in Synthetic  Problems", "tag": ["cs.AI", "cs.LG"], "abstract": "The main purpose of Feature Subset Selection is to find a reduced subset of attributes from a data set described by a feature set. The task of a feature selection algorithm (FSA) is to provide with a computational solution motivated by a certain definition of relevance or by a reliable evaluation measure. In this paper several fundamental algorithms are studied to assess their performance in a controlled experimental scenario. A measure to evaluate FSAs is devised that computes the degree of matching between the output given by a FSA and the known optimal solutions. An extensive experimental study on synthetic problems is carried out to assess the behaviour of the algorithms in terms of solution accuracy and size as a function of the relevance, irrelevance, redundancy and size of the data samples. The controlled experimental conditions facilitate the derivation of better-supported and meaningful conclusions.", "text": "l.a. belanche* dept. llenguatges sistemes inform`atics universitat polit`ecnica catalunya barcelona spain e-mail belanchelsi.upc.edu *corresponding author abstract main purpose feature subset selection reduced subset attributes data described feature set. task feature selection algorithm provide computational solution motivated certain deﬁnition relevance reliable evaluation measure. paper several fundamental algorithms studied assess performance controlled experimental scenario. measure evaluate fsas devised computes degree matching output given known optimal solutions. extensive experimental study synthetic problems carried assess behaviour algorithms terms solution accuracy size function relevance irrelevance redundancy size data samples. controlled experimental conditions facilitate derivation better-supported meaningful conclusions. feature selection problem ubiquitous inductive machine learning data mining setting importance beyond doubt. main beneﬁt correct selection improvement inductive learner either terms learning speed generalization capacity simplicity induced model. hand scientiﬁc beneﬁts associated smaller number features reduced measurement cost hopefully better understanding domain. feature selection algorithm computational solution guided certain deﬁnition subset relevance although many cases deﬁnition implicit followed loose sense. because inductive learning perspective relevance feature several deﬁnitions depending precise objective looked thus need arises count common sense criteria enables adequately decide algorithm certain situations. ture subset selection algorithms literature assesses performance artiﬁcial controlled experimental scenario. scoring measure computes degree matching output given algorithm known optimal solution. measure ranks algorithms taking account amount relevance irrelevance redundancy synthetic data sets discrete features. sample size eﬀects also studied. results illustrate strong dependence particular conditions algorithm used well amount irrelevance redundancy data description relative total number features. prevent single algorithm specially poor knowledge available structure solution. importantly points direction using principled combinations algorithms reliable assessment feature subset performance. paper organized follows begin section reviewing relevant related work. section precise deﬁnition feature selection problem brieﬂy survey main categorization feature selection algorithms. provide algorithmic description comment several widespread algorithms section methodology tools used empirical evaluation covered section experimental study results general advice data mining practitioner developed section paper ends conclusions prospects future work. previous experimental work feature selection algorithms comparative purposes include bankert kudo sklansky setiono studies artiﬁcially generated data sets like widespread parity monks problems demonstrating improvement synthetic data sets convincing typical scenarios true solution completely unknown. however consistent lack systematical experimental work using common benchmark suite equal experimental conditions. hinders wider exploitation power inherent fully controlled experimental environments knowledge optimal solution possibility injecting desired amount relevance irrelevance redundancy unlimited availability data. another important issue performance assessed. normally done handing solution encountered speciﬁc inducer leaving aside dependence particular inducer chosen much critical aspect namely relation performance reported inducer true merits subset evaluated. sense hypothesis fsas aﬀected ﬁnite sample sizes distort reliable assessments subset relevance even presence sophisticated search algorithm therefore sample size also matter study experimental comparison. problem aggravated using ﬁlter measures since case relation true generalization ability loose problem traditional benchmarking data sets implicit assumption used data sets actually amenable feature selection. meant performance beneﬁts clearly good selection process criterion commonly found similar experimental work. summary rationale using exclusively synthetic data sets twofold irrelevance redundancy well sample size problem diﬃculty. added advantage knowledge optimal solutions case degree closeness solutions thus assessed conﬁdent automated way. procedure followed work consists generating sample data sets synthetic functions number discrete relevant features. sample data sets corrupted irrelevant and/or redundant features handed diﬀerent fsas obtained hypothesis. scoring measure used order compute degree matching hypothesis known optimal solution. score takes account amount relevance irrelevance redundancy suboptimal solution yielded algorithm. main criticism associated artiﬁcial data likelihood problem found realworld scenarios. opinion issue compensated mentioned advantages. able work properly simple experimental conditions strong suspect inadequate general. original features cardinality continuous feature selection problem refers assignment weights feature order corresponding theoretical relevance preserved. binary feature selection problem refers choice subset features jointly maximize certain measure related subset relevance. carried directly many fsas setting cut-point output continuous problem solution. although types seen uniﬁed quite diﬀerent problems reﬂect diﬀerent design objectives. continuous case interested keeping features using diﬀerentially learning process. contrary binary case interested keeping subset features using equally learning process. common instance feature selection problem formally stated follows. performance evaluation measure optimized deﬁned function accounts general evaluation measure inspired precise previous deﬁnition relevance. represent cost variable costs unknown meaningful choice obtained setting interpreted complexity solution. case amounts ﬁnding subset highest among maximum pre-speciﬁed size. scenario amounts ﬁnding smallest subset among minimum pre-speciﬁed performance restrictions optimal subset features need exist; does necessarily unique. scenario solution always exists deﬁning value features. case solution adequate policy progressively lower value solution usually interested all. shall speak type designed solve ﬁrst scenario def. used scenarios shall speak general-type algorithm. addition control whatsoever shall speak free-type algorithm. filter mode feature selection takes place induction step former seen ﬁlter general sense seen particular case embedded mode feature selection used pre-processing. ﬁlter mode independent inducer evaluates model feature selection process. mode equal bias inducer used later assess goodness model. main disadvantage computational burden comes calling inducer evaluate every subset considered features. follows several currently widespread fsas machine learning described brieﬂy commented general-purpose search algorithms genetic algorithms excluded review. none algorithms allow speciﬁcation costs features. work ﬁlter wrapper mode. feature weighing algorithm relief included review experimental comparison complement. also used select subset features although getting subset weights devised. following assume evaluation measure maximized. type algorithm repeatedly generates random subsets computes consistency sample inconsistency deﬁned instances equal considering features belong diﬀerent classes. minimum subset features leading zero inconsistencies. inconsistency count instance deﬁned described algorithm found particularly eﬃcient data sets redundant features arguably main advantage quickly reduces number features initial stages certain conﬁdence however many poor solution subsets analyzed wasting computing resources. evaluate measure algorithm consistency algorithm described algorithm departs portion ﬁnds suﬃciently good solution halts. otherwise instances making inconsistent added portion handed process iterated. intuitively portion cannot small big. small ﬁrst iteration many inconsistencies found added current portion hence similar computational savings modest. authors suggest value proportional number features. motoda reported experimentally adequately chooses relevant features fail noisy data sets case algorithm shown consider irrelevant features. probably sensible noise cases small sample sizes. relief general-type algorithm works exclusively ﬁlter mode. algorithm randomly chooses instance ﬁnds near near miss. former closest instance among instances class latter closest instance among instances diﬀerent class. underlying idea feature relevant separates near miss least separates near hit. result weighed version original feature set. algorithm classes described algorithm quence ﬁrst nested subsets order given decreasing weights calling measure returning subset highest value simulate type scenario sequence checked looking ﬁrst element sequence yields value less chosen important advantage relief rapid assessment irrelevant features principled approach; however make good discrimination among redundant features. algorithm found choose correlated features instead relevant features therefore optimal subset assured variants proposed account several classes similar instances selected averages computed. classical general-type algorithms work ﬁlter wrapper mode. iteratively adds features initial subset trying improve measure always taking account features already selected. consequently ordered list also obtained. backward counterpart. jointly described algorithm number features small doak reported tends show better performance likely evaluates contribution features onset. addition bankert points preferable number relevant features small; otherwise used. interestingly also reported always better performance contrary conclusions doak besides faster practice. algorithms w-sfg w-sbg accuracy inducer evaluation measure. free-type algorithms work ﬁlter wrapper mode. sffg exponential cost algorithm operates sequential fashion performing forward step followed variable number backward ones. essence feature ﬁrst unconditionally added features removed long generated subsets best among respective size. algorithm so-called characteristic ﬂoating around potentially good solution speciﬁed size. backward counterpart sfbg performs backward step followed zero forward steps. algorithms found eﬀective situations among popular nowadays. main drawbacks computational cost unaﬀordable number algorithm type algorithm. actually hybrid composed abb. origin branch bound optimal search algorithm. given threshold search stops node evaluation lower eﬀerent branches pruned. variant bound inconsistency rate data full features used basic idea consists using good starting points abb. expected explore remaining search space eﬃciently. authors reported general eﬃcient terms average cost execution selected relevant features. main question arising feature selection experimental design aspects would like evaluate solution given data set? certainly good algorithm maintains well-balanced tradeoﬀ small-sized competitive solutions. assess issues time diﬃcult undertaking practice given optimal relationship user-dependent. present controlled experimental scenario task greatly eased since size performance optimal solution known advance. experiments precisely contrast ability diﬀerent fsas solution respect relevance irrelevance redundancy sample size. inﬂuence output. values generated random example. problem relevant features diﬀerent numbers irrelevant features added corresponding data sets redundancy work redundancy exists feature take role another. following parsimony principle interested behaviour algorithms front simplest case. algorithm fails identify redundancy situation interesting something aware eﬀect obtained choosing relevant feature randomly replicating data set. problem relevant features diﬀerent numbers redundant features added analogous generation irrelevant features. sample size number instances data sample experiments αknt constant multiplying factor total number features number classes problem. means sample size depend linearly total number features. derive section scoring measure capture degree solution obtained matches correct solution. criterion behaves similarity subsets data analysis sense indicates similar satisfying denote total features partitioned ∪xr′ subsets relevant irrelevant redundant features respectively call correct solutions denote feature subset selected fsa. idea check much common. deﬁne general since necessarily partition score deﬁned terms similarity thus indicates similar idea make ﬂexible measure ponder type divergence correct solution. parameters collected order importance weight assigned situations. precedent point simple model suﬃces check whether solution fsa. relevance redundancy strongly related given feature redundant depending relevant features present notice correct solution unique equally valid. features broken equivalence classes elements class redundant being feature deﬁne binary relation features represent information. clearly equivalence relation. quotient correct solution must size element every subset idea express quotient number redundant features chosen number could chosen given relevant features present solution. precedent notation written establish desired restrictions behavior score. less severe relevant features lacking irrelevant features redundancy solution. reﬂected following conditions also deﬁne observe denominators important expressing fact choosing irrelevant feature three order translate previous inequalities workable conditions parameter introduced express precise relation following equations satisﬁed together suitable chosen values reasonable settings obtained taking though settings possible depending evaluator’s needs. values equal |xr| |xr′ least twice important least half times important speciﬁcally minimum values attained diﬀerences widen proportionally point that practically count overall score. following sections detail experimental methodology quantify various parameters experiments. basic idea consists generating sample data sets using synthetic functions known relevant features. data sets corrupted irrelevant and/or redundant features handed diﬀerent fsas obtained hypothesis divergence deﬁned function obtained hypothesis evaluated score criterion experimental design illustrated fig. fsas used experiments. e-sfg c-sbg relief sfbg sffg wsbg w-sfg. algorithms e-sfg w-sfg versions using entropy accuracy inducer respectively. algorithms c-sbg w-sbg versions using consistency accuracy inducer respectively. since relief e-sfg yield experiments divided three main groups. ﬁrst group explores relationship irrelevance relevance. second explores relationship redundancy relevance. last group study eﬀect diﬀerent sample sizes. group uses three families problems four diﬀerent instances varying number relevant features indicated relevance diﬀerent numbers vary problem follows irrelevance experiments runs zero twice value speciﬁcally value chosen involved quantities integer parity disjunction gmonks. redundancy analogously generation irrelevant features running zero twice value sample size given formula αknt diﬀerent problems generated considering values ﬁxed space reasons representative sample results presented graphical form figs. plots point represents average independent runs diﬀerent random data samples. figs. examples irrelevance relevance four instances problems examples redundancy relevance sample size experiments. cases horizontal axis represents ratios particulars explained above. vertical axis represents average results given score criterion. fig. c-sbg algorithm shows ﬁrst good performance clearly falls dramatically irrelevance ratio increases. note performance perfect contrast fig. relief algorithm presents similar fairly good results four instances problem almost insensitive total number features. fig. algorithm presents good stable performance diﬀerent problem instances parity. contrast tends poor general performance disjunction problem total number features increases. ordered list features according weight automatic ﬁltering criterion necessary transform every solution subset features. procedure used determine suitable point simple ﬁrst weights sorted decreasing order weights variances mean discarded idea look feature wn−wj maximum. intuitively corresponds obtaining maximum weight lowest number features. point xj−. total twelve families data sets generated studying three diﬀerent problems four instances each varying number relevant features relevant features problem gmonks problem generalization classic monks problems original version three independent problems applied sets features take values discrete ﬁnite unordered grouped three problems single computed chunk features. multiple denote ﬁrst value feature second etc. problems following figure selected results experiments irrelevance relevance examples redundancy relevance sample size experiments. horizontal axis ratio quantities. vertical axis average result given score independent runs diﬀerent random data samples. dimensionality ﬁgures w-sfg perform increasingly poorly higher numbers features provided number examples increased linear way. however general long examples added performance better summary complete results displayed fig. algorithms allowing comparison across sample datasets respect studied particular. speciﬁcally figs. show average score algorithm irrelevance redundancy sample size respectively. moreover figs. show average weighed weight assigned diﬃcult problems graphic keys left shows algorithms ordered total average performance bottom. right shows algorithms ordered average performance last abscissa value also bottom. words left list topped algorithm wins average right list topped algorithm ends lead. also useful help reading graphics. fig. shows algorithms together c-sbg overall best. fact bunch algorithms also includes ﬂoating showing close performance. note relief wrappers poor performers. fig. shows wrapper algorithms extract data shortage surprisingly backward wrapper fairly positioned average. sffg algorithm quite good average together c-sbg. however algorithms quite close show kind dependency amount available data. note general poor performance e-sfg likely fact algorithm computes evaluation measure independently feature. weighed versions plots seem alter picture much. closer look reveals diﬀerences algorithms widened. interesting change relief takes lead irrelevance sample size redundancy. whenever willing single algorithm. however view reported results better strategy would various algorithms coupled observe results. speciﬁcally suggest relief interested detecting irrelevance detecting redundancy w-sfg presence small sample size situations. light this conjecture sffg used wrapper fashion could better one-ﬁts-all option small moderate size problems. would like bring attention following points wild diﬀerences performance diﬀerent algorithms data particulars ﬁxing algorithm problem performance dramatically diﬀerent various particulars considered however results coherent scale quite well increasing numbers relevant features. would also like emphasize fact differences outcome yielded algorithms entirely diﬀerent approach problem. rather also attributable lack precise optimization goal example form described deﬁnition another good deal ﬁnite sample size which hand hinders obtention accurate evaluation relevance. other dependence speciﬁc sample reminds every evaluation relevance feature subset regarded outcome random variable diﬀerent samples yielding diﬀerent outcomes. vein resampling techniques like random forests strongly recommended. ﬁnal interesting point relation evaluation given speciﬁc inducer score. interested ascertaining whether higher inducer evaluations imply higher scores. next provide evidence need case means counterexample. conjecture given solution yields data know solution suboptimal sense better solutions exist found. however would expect solution better better performance experiment w-sfg independent runs diﬀerent random data samples size using na¨ıve bayes inducer instance gmonks problem described table shows results ﬁnal inducer performance given well score solutions. runs correspond diﬀerent solutions almost inducer evaluation. also lower evaluation greater score. experiment used show variability results function data sample. seen numbers relevant redundant well irrelevant features depend much sample. look precise features chosen reveals diﬀerent solutions nonetheless give similar evaluation inducer. given incremental nature w-sfg deduced classiﬁer improvements obtained adding completely irrelevant features. ben-bassat ‘use distance measures information measures error bounds fuature evaluation’. krishnaiah kanal eds. handbook statistics vol. pages north holland. task feature selection algorithm provide computational solution feature selection problem motivated certain deﬁnition relevance least performance evaluation measure. algorithm also increasingly reliable sample size pursue solution clearly stated optimization goal. many algorithms proposed literature based quite diﬀerent principles loosely follow recommendations all. research several fundamental algorithms studied assess performance controlled experimental scenario. measure evaluate fsas devised computes degree matching output given known optimal solution. measure takes account particulars relevance irrelevance redundancy size synthetic data sets. results illustrate pitfall relying single algorithm sample data specially poor knowledge available structure solution sample data size limited. results also illustrate strong dependence particular conditions data description namely amount irrelevance redundancy relative total number features. finally shown simple example evaluation feature subset misleading even using reliable inducer. points direction using hybrid algorithms well resampling reliable assessment feature subset performance. doak evaluation feature selection methods application computer security’. technical report cse–– davis university california department computer science. suppose since terms make non-negative necessary zero. implies implies thus hence since must zero therefore suppose checked thus αrra achieve value since situation leads either less john kohavi pﬂeger ‘irrelevant features subset selection problem’. proc. international conference machine learning pages brunswick morgan kaufmann. kudo sklansky comparative evaluation medium large–scale feature selectors pattern classiﬁers’. proc. international workshop statistical techniques pattern recognition pages prague czech republic.", "year": 2011}