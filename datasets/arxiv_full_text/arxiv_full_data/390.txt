{"title": "Active Learning of Inverse Models with Intrinsically Motivated Goal  Exploration in Robots", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "cs.RO"], "abstract": "We introduce the Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC) architecture as an intrinsi- cally motivated goal exploration mechanism which allows active learning of inverse models in high-dimensional redundant robots. This allows a robot to efficiently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterized tasks/goals. The architecture makes the robot sample actively novel parameterized tasks in the task space, based on a measure of competence progress, each of which triggers low-level goal-directed learning of the motor policy pa- rameters that allow to solve it. For both learning and generalization, the system leverages regression techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters. We present experiments with high-dimensional continuous sensorimotor spaces in three different robotic setups: 1) learning the inverse kinematics in a highly-redundant robotic arm, 2) learning omnidirectional locomotion with motor primitives in a quadruped robot, 3) an arm learning to control a fishing rod with a flexible wire. We show that 1) exploration in the task space can be a lot faster than exploration in the actuator space for learning inverse models in redundant robots; 2) selecting goals maximizing competence progress creates developmental trajectories driving the robot to progressively focus on tasks of increasing complexity and is statistically significantly more efficient than selecting tasks randomly, as well as more efficient than different standard active motor babbling methods; 3) this architecture allows the robot to actively discover which parts of its task space it can learn to reach and which part it cannot.", "text": "sagg-riac architecture active learning inverse models highdimensional redundant spaces allows robot learn eﬃciently distributions parameterized motor policies solve corresponding distribution parameterized tasks active sampling parameterized tasks called active goal exploration signiﬁcantly faster direct active sampling parameterized policies active developmental exploration based competence progress autonomously drives system progressively explore tasks increasing learning complexity. introduce self-adaptive goal generation robust intelligent adaptive curiosity architecture intrinsically motivated goal exploration mechanism allows active learning inverse models high-dimensional redundant robots. allows robot eﬃciently actively learn distributions parameterized motor skills/policies solve corresponding distribution parameterized tasks/goals. architecture makes robot sample actively novel parameterized tasks task space based measure competence progress triggers low-level goal-directed learning motor policy parameters allow solve learning generalization system leverages regression techniques allow infer motor policy parameters corresponding given novel parameterized task based previously learnt correspondences policy task parameters. present experiments high-dimensional continuous sensorimotor spaces three diﬀerent robotic setups learning inverse kinematics highly-redundant robotic learning omnidirectional locomotion motor primitives quadruped robot learning control ﬁshing ﬂexible wire. show exploration task space faster exploration actuator space learning inverse models redundant robots; selecting goals maximizing competence progress creates developmental trajectories driving robot progressively focus tasks increasing complexity statistically signiﬁcantly eﬃcient selecting tasks randomly well eﬃcient diﬀerent standard active motor babbling methods; architecture allows robot actively discover parts task space learn reach part cannot. keywords active learning competence based intrinsic motivation curiosity-driven task space exploration inverse models goal babbling autonomous motor learning developmental robotics motor development. quite varied example mapping joint angles hand position visual ﬁeld oscillation legs body translation movement hand visual ﬁeld movement point tool properties hand object sound produces. models analytically elaborated engineer provided robot many cases impossible either physical properties body cannot easily modeled impossible anticipate possible objects robot might interact with thus properties objects. generally impossible model priori possible eﬀects robot produce environment especially robots targeted interact everyday human environments assistive robotics. consequence learning models experience becomes necessary. poses highly diﬃcult technical challenges particular combination following facts models often high-dimensional continuous highly non-stationary spatially sometimes temporally; learning examples collected autonomously incrementally robots; learning detail below happen either self-experimentation observation takes signiﬁcant physical time real world. thus number training examples collected life-time strongly limited regards size complexity spaces. advanced statistical learning techniques dedicated incremental high-dimensional regression elaborated recently regression mechanisms eﬃcient quality quantity data high enough case using unconstrained exploration random exploration. fundamental complementary mechanisms guiding constraining autonomous exploration data collection learning needed. article present particular approach address constrained exploration learning inverse models robots based active learning process inspired mechanisms intrinsically motivated learning exploration humans. explain approach studies combination principles learning eﬃciently inverse models high-dimensional redundant continuous spaces architecture makes robot sample actively novel parameterized tasks task space triggers low-level goal-directed learning motor policy parameters allow solve allows leverage redundancies sensorimotor mapping leading system explore densely subregions space action policies enough achieve possible eﬀects. thus need learn complete forward model contrasts approaches directly sample action policy parameters observe eﬀects task space. system also leverages regression techniques allow infer motor policy parameters corresponding given novel parameterized task based previously learnt correspondences policy task parameters. measure interestingness given goal/task based competence progress empirically evaluated i.e. previous attempts low-level optimization directed similar goals allowed improve capability robot reach goals. common carry exploration constraints guiding mechanisms maximally reduce size and/or dimensionality explored spaces. social guidance important source constraints widely studied robot learning demonstration/imitation external human demonstrator assists robot learning process typically robot teacher manually interacts robot showing behaviors corresponding desired movement goal reproduce. strategy prevents robot performing autonomous exploration space requires attentive demonstrator. techniques allow freedom human teacher robot allowing robot explore. typically happens reinforcement learning framework demonstration originally required goal ﬁxed engineer conceives system nevertheless robot evolves high-dimensional large spaces exploration still constrained. instance studies presented combine framework learning demonstration. experiments engineer ﬁrst deﬁne speciﬁc goal task space handcrafted reward function then human demonstrator provides examples successful motor policies reach goal used initialize optimization procedure. shifting setpoint algorithm introduced schaal atkeson proposes another constrain exploration process. goal ﬁxed handcrafted manner progressive exploration process proposed system explores world gradually start position toward goal creating local model around current position shifting direction goal model reliable enough kinds techniques therefore restrain exploration narrow tubes data targeted learning speciﬁc tasks/goals deﬁned human either programmer non-engineer demonstrator. methods eﬃcient useful many cases. nevertheless framework would like robot learn variety tasks inside unprepared spaces like developmental robotics simply full inverse models conceivable human interacts robot instant engineer designs tunes speciﬁc reward function novel task learned. reason necessary introduce mechanisms driving learning exploration robots autonomous manner. active learning algorithms considered organized constrained selfexploration processes regression setting used learn regression mapping input space output space minimizing sample complexity i.e. minimal number examples necessary reach given performance level. methods typically beginning random sparse exploration build meta-models performances motor learning mechanisms concurrently drive exploration various sub-spaces notion interest deﬁned often consisting variants expected informational gain. large diversity criteria used evaluate utility given sampling candidates maximization prediction errors local density already queried points maximization decrease global model variance expected improvement maximal uncertainty model among others. active-extensions existent learning methods e.g. logistic regression support vector machines gaussian processes recently approaches applied robotic problems even recently consider examples real robots. nevertheless examples consider robotic problems already exist large variety problems building environment maps reinforcement learning body schema learning imitation exploration objects body properties manipulation among many others. another approach exploration came initially diﬀerent problem understanding robots could achieve cumulative open-ended learning autonomously. raised question task-independent mechanisms allow robot interested practicing skills learn tasks speciﬁed design time. communities researchers ﬁrst reinforcement learning second developmental robotics formalized implemented experimented several mechanisms based concept intrinsic motivation grounded theories motivation spontaneous exploration free play development humans well recent ﬁndings neuroscience motivation argumented architectures based intrinsically motivated learning conceptualized active learning mechanisms which addition allowing self-organized formation behavioral developmental complexity also also allow agent eﬃciently learn model world parsimoniously designing experiments/queries. spite similarities work active learning intrinsic motivation strands approaches often diﬀer underlying assumptions constraints leading sometimes diﬀerent active learning algorithms. many active learning models often assumes possible learn model complete world within lifetime and/or world learnable everywhere and/or noise homogeneous everywhere. given assumptions heuristics based exploration parts space learned model maximal uncertainties prediction maximally wrong often eﬃcient. assumptions typically hold real world robots unconstrained environment sensorimotor spaces including body dynamics interactions external world simply much large learned entirely life time; typically subspaces unlearnable inadequate learning biases unobservable variables; noise strongly homogeneous. thus diﬀerent authors claimed typical criteria used traditional active learning approaches search maximal uncertainty prediction errors might trapped become ineﬃcient situations common open-ended robotic environments reason active learning heuristics proposed developmental robotics based psychological concept intrinsic motivations relate mechanisms drive learning agent perform diﬀerent activities sake without requiring external reward diﬀerent criteria elaborated search maximal reduction empirically evaluated prediction error maximal compression progress maximal competence progress instance architecture called robustintelligent adaptive curiosity reﬁnement architecture elaborated open-ended learning aﬀordances skills real robots deﬁnes interestingness sensorimotor subspace velocity decrease errors made robot predicting consequences actions given context within subspace. shown biases system explore subspaces progressively increasing complexity. nevertheless riac similar knowledge based approaches limitations ﬁrst deal spatial temporal non-stationarity model learned face curse-of-dimensionality eﬃcient considering moderate number control dimensions indeed many active learning methods riac needs certain level sampling density order extract compare interest diﬀerent areas space. also performing measure costs time approach becomes ineﬃcient dimensionality control space grows second focus active choice motor commands measures consequences allows learning forward models re-used side eﬀect achieving goals/tasks online inversion approach sub-optimal many cases since explores high-dimensional space motor commands consider achievement tasks indirectly. eﬃcient approach consists directly actively exploring task spaces also often much lower-dimensional actively self-generating goals within task spaces learn associated local coupled forward/inverse models useful achieve goals. process straightforward learning forward model since space redundancy possible learn directly inverse model fact exploring task space used learn subpart forward model enough reaching reachable parts task space local inversion regression leveraging techniques generalizing policy parameters corresponding novel task parameters based previously learnt correspondences framework system able learn perform maximal amount diﬀerent tasks focusing diﬀerent ways perform tasks knowledge-based exploration techniques like riac cannot eﬃcient robots redundant forward models. indeed typically direct robotic system spend copious amounts time exploring variations action policies produce eﬀect disadvantage exploring actions might produce diﬀerent outcomes useful achieve tasks. example learning ways push ball forward instead learning push ball diﬀerent directions. address issue take inspiration infant’s motor exploration/babbling behavior argued teleological introducing goals explicitly inside task space driving exploration level goals goal/task chosen system would reach lower-level goal-reaching architecture typically based coupled inverse forward models might include lower-level goal-directed active exploration mechanism. developmental constraints playing important role infant motor development presented experimentations paper also play important role considering task-level exploration process. first motor synergies shown simplifying motor learning reducing number dimensions control motor synergies often encoded using central pattern generators traditional innate low-level control loops part innate structure allowing robot bootstrap learning skills example combined intrinsically motivated learning. second heuristic inspired observations infants sometimes prepare reaching movements starting rest position resetting robot rest position allows reducing starting states used perform task. paper propose approach allows transpose basic ideas riac architectures combined ideas algorithm multi-level active learning architecture called selfadaptive goal generation riac algorithm unlike riac made active learning forward models mapping action policy parametes eﬀects task space show algorithm allows eﬃcient learning inverse models mapping parameters tasks parameters action policies allow achieve tasks redundant robots. achieved active sampling novel parameterized tasks task space based measure competence progress triggers low-level goal-directed learning motor policy parameters allow solve takes advantage typical redundancy mapping fact often dimensionality task space considered much smaller dimensionality motor primitives/action parameter space. architecture also leverages techniques optimizing action policy parameters single predeﬁned tasks well regression techniques allowing infer motor policy parameters corresponding given novel parameterized task based previously learnt correspondences policy task parameters approaches consider problem autonomous life-long exploration novel parameterized tasks complemetary present work could used low-level techniques low-level learning action parameter policies self-generated tasks sagg-riac architecture. sagg-riac considered active learning algorithm carrying concept competence based intrinsically motivated learning line concepts mastery motivation flow optimal level theories zone proximal development introduced psychology competence based active exploration mechanism according deﬁnition robot pushed perform active exploration goal/operational space opposed motor babbling actuator space. several strands previous research began exploration various aspects family mechanisms. first algorithms achieving competence based exploration allowing general computer programs actively adaptively self-generate abstract computational problems goals increasing complexity studied theoretical computer science perspective high expressivity formalisms allows principle tackle wide diversity problems designed experimented particular family problems learning high-dimensional continuous models robotics. sagg-riac also actively adaptively self-generates goals achieved formalism based applied mathematics dedicated problem learning inverse models continuous redundant spaces. measures interestingness based measure competence perform skill studied well selector chooses perform diﬀerent skills depending temporal diﬀerence error reach skill. study proposed based competence progress select goals pre-speciﬁed skills considered discrete world. show sagg-riac also uses competence progress targets learning high-dimensional continuous robot spaces. mechanism passive exploration task space learning inverse models high-dimensional continuous robotics spaces presented robot learn inverse kinematics trying reach preset order goals pre-speciﬁed grid informing robot limits reachable space. sagg-riac exploration actively driven task space allowing learning process minimize sample complexity show reach high-level performances generalization discover automatically limits reachability. following sections introduce global architecture formalization self-adaptive goal-generation sagg-riac architecture. then study experimentally capabilities allow robot eﬃciently actively learn distributions parameterized motor skills/policies solve corresponding distribution parameterized tasks/goals context three experimental setups learning inverse kinematics highly-redundant robotic learning omnidirectional locomotion motor primitives quadruped robot learning control ﬁshing ﬂexible higher level active learning takes care active self-generation self-selection goals/tasks parameterized task space depending measure interest based level competences reach previously generated goals lower level active learning considers goaldirected active choice active exploration lower-level actions taken reach goals selected higher level depending local measures interest related evolution quality learned inverse and/or forward models; figure global architecture sagg-riac architecture. structure comprised parts deﬁning levels active learning higher level considers active self-generation self-selection goals lower level considers goal-directed active choice active exploration low-level actions order reach goals selected higher level. consider robotic system described state/context space task space ﬁeld parameterized tasks/goals viewed deﬁning ﬁeld parameterized reinforcement learning problems. instance ﬁrst experiment introduced following sections robotic manipulator represents actuator/joint space operational space corresponding cartesian position end-eﬀector relates velocity commands joints. also second experiment involving quadruped motor synergies context always reset state thus inﬂuence learning relates dimensional parameters motor synergy considers frequency amplitude sinusoids controlling position joints time relates position orientation robot execution synergy ﬁxed amount time. states; therefore goal generated low-level goal directed exploration learning mechanism always tries reach starting current state system formalized explained below. spent energy) chosen generates motor policy parameterized sstart well parameters internal forward inverse models already learned previously acquired data data. also important notice computed experiments below regression techniques allowing infer motor policy parameters corresponding given novel parameterized task based previously learnt correspondences policy task parameters true everywhere because presented before every state considered starting state. also goals related terminal condition goal reached policy encodes skill learned process induced lower-level active learning shall indexed goal i.e. formally induced semi-markov options deﬁne policies termination conditions dependent events initiation option current instant. means policy depending history moreover consider cases goals reachable need deﬁne timeout tmax stop goal reaching attempt maximal number actions executed. thus needed stop tmax. eventually using framework options deﬁne process goal self-generation self-generation self-selection parameterized options goal reaching attempt corresponding learning particular option. therefore global sagg-riac process described exploring learning ﬁelds options. sagg-riac goal actively chosen high-level goal directed exploration learning mechanism lower carried numerous ways architecture makes little assumptions them thus compatible many methods described main idea guide system toward goal executing low-level actions allow progressive exploration world toward speciﬁc goal updates time local corresponding forward inverse models leveraging previously learnt correspondences regression. main assumptions methods used lower level incrementally method must able build incrementally local forward inverse models reused later particular considering goals task-space regression techniques presented action policy reaching self-generated goal. learning feedback mechanism added exploration active selection actions depends local measures quality learned model. following experiments introduced diﬀerent methods mechanism optimization inspired algorithm coupled memory-based local forward inverse regression models using local moore-penrose pseudo-inverses generic optimization algorithm mixing stochastic optimization memory-based regression models using pseudo-inverse. kinds techniques could used. optimization part algorithms natural actor-critic architectures model based reinforcement learning algorithms convex optimization algorithms stochastic optimization like path-integral methods regression part using memory-based approach combined eﬃcient data storage access structures scales well computational point view. memory limits would limited resource little assumption low-level regression algorithms made sagg-riac architecture parameterized models allowing control memory requirements neural networks support vector regression gaussian process regression could instead considered improvement given regions task space goals chosen. measure competence computed diﬀerent instants learning process. first estimated reaching attempt direction goal declared terminated. second robotic setups compatible option competence computed low-level reaching attempts. following sections detail diﬀerent cases introduce measure competence given goal reaching attempt dependent metrics similarity point task space attained reaching attempt terminated actual goal respect constraints conditions represented cost system exploits previously learnt models reach goal using computed adequate local regression using low-level goal-directed optimization optimize best current reach self-generated goal collect data allowing measure competence reach since computed might lead diﬀerent active goal self-generation self-selection relies feedback linked notion competence introduced above precisely monitoring progress local competences. ﬁrst need deﬁne notion local competence. consider subspace called region then consider diﬀerent measures competence computed diﬀerent attempted goals time window consisting last estimation interest computed region interest interesti region described absolute value derivative local competences inside hence amplitude local competence progress sliding time window recent goals attempted inside using derivative interest considers variation competences using absolute value considers cases increasing decreasing competences. sagg-riac term competence progress general meaning denote increase decrease competences. increasing competence signiﬁes expected competence gain important. therefore potentially selecting goals regions high competence progress could bring high information gain learned model also drive reaching previously achieved goals. depending starting position potential evolution environment body decrease competences inside already well-reached regions arise. case system able focus regions order least verify possibility figure task space example regions subregions split learning process according competence level. region displays competence level time measure used computation interest according equation re-establish high level competence inside. explains usefulness consider absolute value competence progress shown equation using sliding window order compute value interest prevents system keeping measure competence memory thus limits storage resource needed core sagg-riac architecture. mechanism described riac algorithm introduced previously applied actuator space rather goal/task space done sagg-riac. here kind methods recursive split space split triggered predeﬁned maximum number goals gmax attempted inside. split performed maximizes diﬀerence interest measure described resulting subspaces. allows easy separation areas diﬀering interest therefore diﬀering reaching diﬃculty. precisely split region done selecting among randomly generated splits split dimension position that finally soon least regions exist initial random exploration whole space goals chosen according following heuristics selected according probabilistic distributions selection probability region interesti corresponds current interest region mode inside whole space mode region ﬁrst selected according interest value goal generated close already experimented received lowest competence estimation. order improve quality learned inverse model heuristic inspired observations infant motor exploration learning. ﬁrst proposed berthier infant’s reaching attempts often preceded movements either elevate hand move hand back side. second noticed infants reach objects forever sometimes relax muscles rest. practically characteristics allow reduce number initiation positions reach object simpliﬁes reaching problem letting learn reduced number reaching movements. mechanism transposed robotics motor learning reaching tasks well kind skills locomotion ﬁshing shown experiments below. framework directly allows highlyredundant robotic system reduce space initiation states used learn reach goals also typically prevent experimenting complex actuator conﬁgurations. process sagg-riac specifying rest position reachable without need planning system subsequent reaching attempts traditional active learning methods especially knowledge-based intrinsically motivated exploration system typically designed select actions perform inside values inside already known interval cases challenge select areas would potentially give information system improve knowledge inside ﬁxed range possibilities. argued earlier limit approaches become less less eﬃcient dimensionality control space increases. competence based approaches allow address issue low-dimensional task space identiﬁed. nevertheless case problem arises considering unbounded learning space goals reachable extremely large generally diﬃcult predict limits undesirable engineer identify them. therefore carried large spaces reachable area small part algorithm could necessitate numerous random goal self-generations able estimate interests diﬀerent subregions. order reduce number help system converge easily toward regions competence improved emphasize diﬀerent mechanisms used sagg-riac reaching attempt conservation every point reached inside task space even correspond attempted goal robot performs reaching attempt toward goal instead reaching addition subgoals robotic setups process goal reaching subdivided described using subgoals could ﬁxed pathway toward goal artiﬁcially states reached also respecting constraints estimate competence measure one. consideration heuristics important advantages ﬁrst signiﬁcantly increase number estimations competence thus quantity feedback returned goal self-generation mechanism. reduces number goals self-generated bootstrap system thus number low-level iteration required extract ﬁrst interesting subspaces. also creating areas diﬀerent competence values around already reached states inﬂuence discovery reachable areas. finally result interesting emergent phenomena create growing area increasing competence around ﬁrst discovered reachable areas. indeed obtaining values competences inside reachable areas algorithm able split space ﬁrst regions compute values interest. values interest typically high already reached areas inﬂuence goal self-generation process create goals proximity. level competence becomes important stabilized eﬃciently reached arpseudo-code algorithm present operations saggriac architecture. algorithms simple alternative examples lowlevel goal-directed optimization algorithms used experimental presented function ineﬃcient also built numerous manners described details pseudo-code function judge current model eﬃcient enough reach come closer decided goal model improved order reach following sections present diﬀerent kinds experiments. ﬁrst reaching experiment robotic learn inverse kinematics reach self-generated end-eﬀector positions. uses robot reset action ﬁrst consider quadruped learning omnidirectional locomotion controlling ﬂexible ﬁshing learning ﬂoat precise self-generated positions ﬁxed parameters motor synergy used control robots. thus variation setpoint prevented here variant proposed experiments context evolve always reset like algorithm section propose experiment carried robotic explore learn forward inverse kinematics. also discussing details active exploration approach ﬁrst experimentation case ﬁrstly deﬁne representations models control paradigms involved experiment. here focus robotic systems whose actuators settable positions velocities restrict analysis discrete time models. changes robot geometry directly impact inverse kinematics relating workspace coordinates actuators coordinates learning inverse kinematics useful numerous machine learning cases accurate kinematic model robot available online calibration needed sensor motor imprecision. moreover developmental robotics studies priori knowledge precise model body often avoided implausibility biological point view. following experiment assume inverse kinematics system totally unknown interested studying sagg-riac eﬃciently guide discovery learning inverse kinematics. n-dimensional vector position orientation manipulator’s end-eﬀector m-dimensional vector relative eterized vector controls instantaneous speed redundant manipulator considered solutions inverse relationship generally non-unique problem posed inverse learning algorithms thus determine particular solutions then using jacobian matrix inverting single solution corresponding desired raises problem non-convexity property last equation. solution non-convex problem proposed bullock converted convex problem figure values used compute competence considering manipulator degrees-of-freedom dimensions operational/task space. here position called rest position straight slightly bent. functions datapoints restrained locality proposed useful real time queries incremental learning. learning inverse kinematics typically deals kind constraints local methods thus proposed eﬃcient approach learning following study incremental version approximate nearest neighbors algorithm based tree split using k-means process determine vicinity current also environments introduce contribution need highly robust computationally complex regression methods. using pseudo-inverse moore-penrose compute pseudo-inverse jacobian time values. vector represents joint angles corresponds context/state space vector position manipulator’s end-eﬀector dimensions euclidian space corresponds task space evaluate sagg-riac architecture used robot learn reach reachable points environment arm’s end-eﬀector. learning inverse kinematics online process arises time micro-action robot stores measures memory creates database data contains elements representing discovered change corresponding given conﬁguration measures reused online compute jacobian ∆y/∆α locally move end-eﬀector desired direction ∆ydesired ﬁxed toward selfgenerated goal. therefore consider learning problem dimensions relationship system learn also experiment order clearly illustrate main contribution algorithm consider constraints focus reaching goal positions nevertheless important notice constraint direct inﬂuence low-level active learning sagg-riac thus indirect inﬂuence higher level. using constraint require complex exploration process guided low-level important number iterations level required reach goal could inﬂuence global evolution performances learning process used higher-level sagg-riac. deﬁne competence function euclidian distance goal position ﬁnal reached position normalized starting distance ystart endeﬀector’s starting position. allows instance give competence level considering goal origin position robot approaches goal robot approaches .mm. computing local competence progress subspaces/regions typically requires reaching numerous goals. reaching goal necessitate several micro-actions thus time obtaining competence measures long. also without biasing learning process already explained section improve mechanism taking advantage euclidian nature increase number goals artiﬁcially adding subgoals pathway starting position goal competences computed. therefore considering starting state ystart self-generated goal also consider another increase number competence measures take consideration experimented position end-eﬀector goal reached maximal competence value. typically help system distinguish regions eﬃciently covered discover regions interest. propose method inspired algorithm guide system learn pathway toward selected goal position instantiation sagg-riac architecture uses algorithm considers evolving contexts explained below. reaching phase deals creating pathway current goal position phase consists determining current position optimal micro-action would guide end-eﬀector toward purpose system computes needed end-eﬀector’s displacement ∆ynext yc−yg yc−yg yc−yg normalized vector direction goal) performs action ∆αnext +.∆ynext pseudo-inverse jacobian estimated close vicinity given data collected robot far. action ∆ynext compute high value εmax εmax thus parameter depending range error experienced depending tolerance conceded allow reaching goal positions current learned data. high value εmax prevent exploring learning data values εmax prevent eﬃcient local optimization. incremented micro-action reset goal. timeout used deﬁne goal unreached stop reaching attempt uses counter. maximal quantity micro-actions ﬁxed goal directly proportional number micro-action requires reached. next experiments system allowed perform times distance ystart stopping reaching attempt. means covers less space goals chosen number subgoal goal maximal number elements inside region split gmax also desired velocity units/micro-action number explorative actions moreover reset rest position every reaching attempts. allows reducing initiation prevent system experimenting complex joint positions folded jacobian diﬃcult compute. using value important characteristic beginning learning process. high value prevents learning rapidly achieve maximal amount goal position diﬃculty reuse previously learned data folded unknown positions. bent character rest position also useful avoid begin micro-action close singularity like totally unfolded. also experiment consider experimented position end-eﬀector goal reached maximal competence level fig. represents whole distribution self-generated goals sub-goals selected higher-level active learning module corresponding competences execution micro-actions. global shape distribution points allows observing large values competence levels inside reachable space close vicinity global competence inside remaining space. figure competence values corresponding entire self-generated goals collected experiment micro-actions arm. heterogeneous competence values situated inside reachable space illustrates typical measures competence measured region whole experiment. visualization evolution competence values ﬁgure global competence system reach positions situated grid covers entire task space. estimations competence extract interesting phenomena ﬁrst ﬁrst subﬁgures estimated self-generation goals show system beginning exploration learning process competent attain areas situated close limits reachable space. then subﬁgures show progressive increase competences inside reachable space following increasing radius whose origin situated around end-eﬀector rest position. ﬁrst observation reaching mechanism itself which possessing data acquired allow robot experiment complex joint movements simple ones typically leads limits arm. second phenomenon coupling lower-level active learning inspired heuristic returning yrest every subsequent goals. indeed necessity conﬁdent local model figure evolution competence values corresponding self-generated goals collected experiment micro-actions arm. time indexed number self-generated goals. higher values corresponds position reached using learned data. shift toward positions makes system progressively explore space resetting rest position makes progressively explore space beginning close yrest. finally goal positions physically reachable radius typically present competence reached initially radius spreads enough reach them. fig. shows histograms goal positions self-generated execution micro-actions subﬁgure corresponds speciﬁed time window indexed number generated goals ﬁrst shows that onset learning system already focuses small area around end-eﬀector’s rest position thus discriminates diﬀerences subpart reachable area remaining space second subﬁgure system inversely focusing almost regions space reachable arm. imprecise split space level exploration left small reachable areas edge inside large unreachable regions. typically gives high mean competence region created. then large part unreachable figure evolution distribution self-generated goals displayed time windows indexed number performed goals experiment micro-actions measuring units. black half-circle represents contour area reachable arm. higher values corresponds higher density self-generated goals. areas comparison reachable ones mean competence decreases time. brings interest region thanks mathematical deﬁnition interest level which using absolute value pushes robot toward areas competence decreasing. complex process allows driving exploration kind heterogeneous regions allows dividing eﬃciently task space reachable unreachable regions. then considering global observation subﬁgures conclude system eﬀectively autonomously discovers limits focusing goal self-generation inside reachable areas largest part exploration period. system indeed discovering subpart reachable interest value becoming null totally unreachable areas competence value low. precise observation subﬁgures presented fig. speciﬁcally observe self-generated goals inside reachable area. first perceive system originally focusing area around end-eﬀector’s rest position yrest figure details evolution distribution self-generated goals inside reachable area experiment presented fig. gray points represent end-eﬀector rest position yrest. also comparing ﬁrst subﬁgures last ones observe shift maximum exploration peak toward basis. ﬁrst linked loss interest self-generating goals around end-eﬀector’s rest position. indeed system becomes highly eﬃcient inside region competence level becomes high stationary time leads interest values. time phenomenon also linked increase competences reachable positions end-eﬀector rest position yrest closer basis creates regions interest addition subgoals consideration end-eﬀector’s position goal reached highest competence level important inﬂuences learning process. look traditional active learning algorithms cannot deal open-ended learning well riac-like algorithms diﬀerent sagg-riac notice even techniques deal avoiding excessive exploration unlearnable extremely complex areas learning process still begin period random exploration whole space distinguish extract subparts interesting according used deﬁnition interest. thanks addition sub-goals and/or consideration every endeﬀector’s position sagg-riac addition exploring task space reduce number needed random global exploration improve capability system deal large task spaces. using subgoals indeed creates concentration goals around current end-eﬀector’s position progressively grows according experimented positions. furthermore consideration end-eﬀector’s position estimation competence allows discovering progressively positions reachable high competence level gives fast indication ﬁrst subregions high competences situated. increases number subregions close reachable areas allows computing interest values growing vicinity end-eﬀector’s experimented positions therefore additions competence measures allow system discover focus areas competence high number goal self-generation tackle typical problem fast estimation distinction interesting areas. nevertheless emergent process helps increase number feedbacks required goal self-generation mechanism split space inﬂuence low-level active learning. then timeout deﬁnes goal unreached single reaching attempt becomes crucial considering high-volume task spaces large unreachable parts introduced following section. previous experiment timeout describes goal reached stops reaching attempt deﬁned directly proportional number micro-actions required reach goal. practically introduced section allowed system perform times distance ystart declaring goal reached timeout eﬃcient enough learn eﬃciently discriminating regions diﬀerent complexities middle-size space using timeout based distance goal. also designed stop reaching attempt according following blocking criteria consider self-generated goal low-level exploration reaching mechanisms reach. then system coming closer goal even low-level explorations exploration toward precise goal stops. practical consecutive low-level explorations declare goal unreached compute corresponding competence level. using deﬁnition rapidity discovering blocking situations depend values number explorative actions minimal values parameters allows fastest discoveries decrease quality low-level exploration mechanism exploring reachable spaces considering low-level mechanisms allowing eﬃcient progressive learning sagg-riac algorithm capable discriminate eﬃciently reachable areas high-volume spaces. then also able drive progressive self-generation goals reachable subspaces progressively growing complexities reachability. experiment reachable region task space convex obstacles. ﬁshing experiment below saggriac capable identifying correctly zones reachability given low-level optimization algorithm even holes obstacles goals initially generated unreachable positions positions obstacles prevent reaching provide level competence progress thus system stops trying reach them. also possible imagine given selfgenerated goals might reachable action policy going around obstacle. capability property sagg-riac architecture itself property optimization algorithm action representation used low-level goal-directed mechanism. present experiment low-level optimization simple considering action policies going straight line goal. would used complex optimization leveraging continuous domain planning techniques zones reachability would increased obstacles introduced since low-level system could learn around them. following evaluation consider robotic system previously described design diﬀerent experiments. estimate eﬃciency inverse model learned testing allows average robot reach positions selected inside test database reachable positions also compare sagg-riac three types exploration techniques also comparable sagg-riac actuator technique position reset rest position every micro-actions number micro-actions needed reach distant reachable position. proportional desired velocity units/micro-action well size task space every graph present statistical results obtained launching experiment diﬀerent random seeds times. ﬁrst quantitative experiment designed compare quality inverse models learned using babbling task/operational space instead traditional motor babbling heuristics executed conﬁguration/actuator space. still consider units also suited ﬁrst study dimensions bounded intervals fig. shows evolution capability system reach test goals using inverse model learned technique starting rest position. capability computed using mean euclidian distance goal ﬁnal state reaching attempt. globally results show order learn inverse kinematics highly-redundant exploration goal/operational space significantly eﬃcient exploration actuator space using either random exploration riac-like active learning. moreover better performances actuator-random compared actuator-riac emphasizes original version riac designed eﬃcient learning inverse models highly-redundant systems focusing evaluation mechanisms sagg also make important observation sagg-riac eﬃcient sagg-random considering system already knows limits reachability. precisely observe increase learning speed ﬁnal generalization performances improvement signiﬁes saggriac eﬃciently able progressively discriminate focus areas bring highest informational amount brings learning system useful data create eﬃcient inverse model contrarily sagg-random approach continues select goals already eﬃciently reached areas. figure evolution mean distances goal eﬀector reaching attempts independently randomly generated test goals. sagg-riac sagg-random allowed choose goals sagg-riac using timeout blocking criteria described section allows test quantitative aspect discrimination capability sagg-riac comparison three techniques facing high volume task spaces small subparts reachable. fig. shows sagg-riac method able drive eﬃcient learning space. sagg-random actually spends majority time trying reach unreachable positions. also size task space inﬂuence actuator algorithms compare results figure evolution mean distances goal eﬀector reaching attempts independently randomly generated test goals averaged experiments. sagg-riac sagg-random allowed choose goals within large space corresponding fig. finally test robustness sagg-riac task spaces larger previous section. fig. shows behavior sagg-riac used task spaces diﬀerent sizes times size reachable space compare results random exploration actuator performances technique decrease size considered task space increases. therefore observe sagg-riac obtains better results actuator-random since micro-actions considering results clearly show sagg-riac robust spaces times larger reachable space diﬃculties explore even larger spaces. therefore despite fact sagg-riac eﬃcient large spaces seems challenge autonomous exploration un-prepared spaces totally resolved algorithm human supervisor still necessary deﬁne limits task space. emphasized perspective work complementary techniques used order bring robustness spaces mechanisms inspired notion maturational constraints able limits task space since beginning exploration process. figure evolution mean distances goal eﬀector reaching attempts independently randomly generated test goals averaged experimentations. sagg-riac sagg-random experiment desired velocity units/micro-action number explorative actions moreover reset rest position every reaching attempts increases complexity reaching process. present series experiments aiming test robustness saggriac setups diﬀerent shapes numbers degrees-of-freedom. performed tests used arms whose limb either length decreasing length depending distance arm’s base experiments permit testing eﬃciency algorithm highly redundant systems experiments competence level therefore evaluated goals subgoals. compute tests inverse models micro-actions. fig. illustrates performances learned inverse models used reach goals independent test database evolving along number experimented micro-actions. first globally observe slower decreasing velocity sagg-random sagg-riac compared previous experiment higher value removed consideration every end-eﬀector position. graphs ﬁrst line fig. present reaching errors arms decreasing lengths. ﬁrst subﬁgure shows considering relatively number degrees freedom saggrandom second eﬃcient algorithm. indeed actuatorrandom method eﬃcient sagg-random micro-actions stabilized sagg-random progressively decreasing reaching level actuator-random experiment. high focalization sagg-random outside reachable area leads numerous explorations toward unreachable positions. shown also subﬁgure adding riac active component sagg eﬃciently improves learning capabilities system; sagg-riac reaching errors indeed lowest system. experiments shows sagg methods eﬃcient actuator methods sagg-riac showing signiﬁcant improvement compared every algorithm experiments presented arms limb length show kind results. experiment shows actuator-random eﬃcient sagg-random addition riac allows obtaining signiﬁcant improvement case also considering dof. globally quantitative results presented emphasize high eﬃciency robustness sagg-riac carried highly redundant robotic setups diﬀerent morphologies compared traditional approaches explore actuator space. also showed random exploration goal space eﬃcient used highfigure histograms self-generated goals displayed time windows indexed number performed goals experiment microactions real arm. histogram represents surface covered camera deﬁnes task space. dimensional systems even considering task space times larger reachable subspace. results therefore indicate high potential competence based motor learning learning highlyredundant robots. section test robustness algorithm qualitative point view considering real robotic setup corresponds simulation presented above controlled position. also helping test robustness method quality motors whose averaged noise movement. ﬁxed task space corresponds whole surface observable camera ﬁxed robot three times larger reachable space order allow camera distinguish end-eﬀector create visual referent framework surface used visual tags software artoolkit tracker fig. shows histograms self-generated goals displayed sliding time windows indexed number performed goals experiment micro-actions. observe algorithm manages discover limits reachable area drives exploration inside goal then system continues focus reachable space experimentation alternating diﬀerent areas inside. precisely notice comparing bottom-left subﬁgure positioned second line system seems concentrate time areas situated close basis therefore diﬃcult reach. progressive increase complexity positions explored appeared simulation therefore also happens here. finally last subﬁgure shows system continues exploration toward area central reachable part. high level noise motor control system originally robust part space improvement generalization capacity learning algorithm allows obtaining increase competences already visited areas task space. experiment shows eﬃciency sagg-riac architecture drive learning process real noisy robotic setups iterations well capacity still control complexity exploration considering highly-redundant systems. sometimes stemming pre-wired neuronal structures motor synergies deﬁned coherent activations group muscles. proposed building blocks simplifying scaﬀolding motor behaviors allowing reduction number parameters needed represent complex movements described crucial development motor abilities seen encoding unconscious continuous control muscles simpliﬁes complexity learning process learning complex tasks using parameterized motor synergies indeed corresponds tuning relatively low-dimensional high-level control parameters compared important number degrees freedom controlled following experiments simplify learning process using parameterized motor synergies controlling amplitude phase velocity central pattern generators mathematically using motor synergies simpliﬁes description considered robotic system. framework introduced deﬁned system represented current framework consider sequence actions generated directly parameterized motor synergies means sequence actions directly encoded controlled setting parameters speciﬁed beginning action. instance experiment described section deﬁne synergy figure degrees-of-freedom quadruped controlled using motor synergies parameterized values amplitudes others phases sinusoid tracked motor. experiments consider task space corresponds position orientation quadruped. parameterized sinusoids motor joint track low-level pre-programmed pid-like controller. eventually motor synergies seen encapsulate low-level generation sequences micro-actions allowing system directly focus learning following experiment consider quadruped robot simulated using breve simulator composed joints ﬁrst controlled rotational second rotation therefore consists robot totality directly specify phase amplitude sinusoid controls precise rotational value time. synergies parameterized using continuous values representing phase joint others amplitude {ph..; am..} joint receives command ﬁxed frequency. experimentation consists launching motor synergy ﬁxed amount time starting ﬁxed position. time period resulting position robot extracted dimensions position three dimensions used deﬁne task space robot. also important notice precise areas reachable quadruped using motor synergies cannot estimated beforehand. following implementation algorithm robotic setup aims test sagg-riac driving method allows robot learn eﬃciently accurately attain maximal amount reachable positions avoiding selection many goals inside regions unreachable previously visited. experiment consider constraints focus reaching goal positions every iteration robot reset conﬁguration called origin position deﬁne competence function using euclidian distance goal/robot’s position reaching attempt normalized original distance origin position yorigin goal measure competence compute euclidian distance using dimensions rescaled dimension therefore weight estimation competence using diﬀerent exploration mechanisms. lines represents estimated limits reachability. figure example experimentation quadruped illustration beginning position goal position corresponding reached position whose value used compute measure competence. system continuously estimates distance goal already reached position closest goal. reaching phase manage make system come closer i.e. last eﬀectively reached point attempt toward exploration phase triggered. fig. presents positions explored quadruped inside task space experimentations using exploration mechanisms introduced previously. actuator-random actuator-riac select parameters motor synergies experiment whereas sagg-random sagg-riac selfgenerate goals second sagg-riac contrary sagg-random shows large exploration range surface almost twice much coverage using previous algorithms three times; maximum previous algorithms last results emphasize capability sagg-riac drive learning process inside reachable areas easily accessible section test eﬃciency learned forward/inverse models guide quadruped reach goal positions independently generated test database. consider test database goals generated independantly covering approximately uniformly reachable part task space compute distance goal attempted reached position. fig. shows performances methods introduced previously. first observe higher eﬃciency sagg-riac compared three methods observed iterations. high decreasing velocity reaching error consideration regions limited small number elements allows creating high number regions within small interval time helps system discover focus reachable regions surrounding area. random. also even sagg-random less eﬃcient sagg-riac observe highly decreasing reaching errors compared actuator methods allows signiﬁcantly eﬃcient method considered iterations. again previous experiment also observe sagg-riac allow learn faster master sensorimotor space asymptotic performances also seem better experiments ﬁrst emphasize high eﬃciency methods drives exploration motor synergies terms eﬀects task space. illustrated qualitative results sagg methods especially sagg-riac allows driving exploration order explore large spaces containing areas hardly discovered chance limits reachability diﬃcult predict. then quantitative results showed capability sagg-random sagg-riac methods learn inverse models eﬃciently considering highly-redundant robotic systems controlled motor synergies. experiments consists robot learning control ﬁshing order attain certain positions ﬂoat touches water. setup simulated using breve simulator previous experiment. ﬁxed controlled motor synergies aﬀect velocity joint parameterized tation robot low-level pre-programmed controller tracks desired velocity joint ﬁxed short amount time starting ﬁxed rest position suddenly stopping movement. movement well second after monitor position ﬂoat order detect potential contact water water touched extract figure degrees-of-freedom ﬁxed ﬁshing extremity. controlled using motor synergies aﬀect velocity joint parameterized values. experiments consider twodimensional task space corresponds position ﬂoat touching water performing movement. learning exemplar. sensorimotor space studying behavior sagg-riac relevant according ﬂexible aspect line makes system diﬃcult model analytically highly redundant highly sensitive small variations inputs. following experiment task space consist limited area water surface. consider basis ﬁxed coordinates limits task fig. shows histograms repartition positions reached ﬂoat water surface computed water touched trials running actuator-random saggriac exploration processes. point situated center corresponds base handling ﬁshing situated observing ﬁgures note repartition positions situated inside disk radius delimits position reached line maximally slack. distribution reached positions within disk asymetrical among exploration processes. asymetries ﬁgure fact reﬂecting asymetries robot setup geometry robot symmetric starting/rest conﬁguration also symmetric. coupled structure motor primitives makes structure reachable positions complex asymetric observed especially actuator-random sub-ﬁgure since shows asymetric distribution ﬂoat position reached parameters action primitives sampled uniformly comparing histograms note sagg-riac drives exploration toward positions ﬂoat explored actuator-random large part situated bottom reachable area. thus sagg-riac drives exploration toward diverse regions space. sagg-riac therefore able avoid spending large amounts time exclusively guiding exploration toward areas actuator-random does. extended experimentation setup showed distribution reached points sagg-riac corresponds closely actual whole reachable space. eventually qualitative results emphasize sagg-riac able drive exploration process eﬃciently carried highly redundant complex robots compliant/soft parts. figure histograms positions reached ﬂoat entering contact water ﬁshing experiment contact ﬂoat/water using actuator-random sagg-riac exploration methods. fig. shows mean reaching errors obtained using actuator-random sagg-riac statistically computed experiments diﬀerent random seeds. here comparison methods shows saggriac signiﬁcantly eﬃcient results successful trials. also trials observe small increase reaching errors saggriac. phenomenon discovery motor synergies already mastered goal positions. discovered redundancy reduces generalization capability computing inverse model small amount time parameters motor synergy explored enough disambiguate invert model paper introduced self-adaptive goal generation architecture saggriac active learning inverse models robotics intrinsically motivated goal exploration. first demonstrated high eﬃciency learning inverse models performing exploration driven active self-generation high-level goals parameterized task space instead traditional motor babbling speciﬁed inside low-level control space. active exploration task space leverages redundancy often characterizing sensorimotor robotic spaces strategy drives robots learn maximal amount tasks instead numerous ways perform tasks coupling goal babbling sophisticated intrinsically motivated active learning also allows robot perform eﬃcient autonomous learning limits reachability inverse models unknown high-dimensional body schemas diﬀerent architectures. intrinsically motivated active learning driven active stochastic search areas task space competence progress maximal. also allowed emerging developmental trajectories driving robot progressively focus learn tasks increasing complexities discovering limits reachability avoiding spend much exploration time trying perform impossible tasks. showed approach could allow eﬃcient learning action space continuous high-dimensional experiments performed assuming low-dimensional task space initially provided. frequent low-dimensional task spaces useful engineering problems robotics assume engineer helps robot learner designing hand task space hand would like architecture like sagg-riac developmental framework would assume low-dimensional task spaces pre-speciﬁed robot additional mechanisms added equip robot following related capabilities active selection global measures competence progress forming architecture three levels active learning would natural extension work presented article. allowing non-engineer human drive attention robot toward particular task spaces physical guidance human-robot interfaces allowing robot attracted toward particular dimensions environment introduced. inverse reinforcement learning mechanisms able extract reward functions thanks examples action policies could also seen mean infer interesting task spaces human demonstrations social guidance also used mechanism bootstrap evaluation competence progress identiﬁcation zones reachability large high-dimensional spaces shown presents approach combine intrinsically motivated learning like sagg-riac techniques learning demonstration. learning process using goal babbling drives eﬃciently thanks intrinsic motivations learning still begin period random exploration order discriminate unreachable areas well areas diﬀering interests. becomes problem volume reachable areas task space smaller task space task space becomes high-dimensional. important direction future work take inspiration maturational processes infants constrained learning development numerous physiological cognitive mechanisms limitation sensorimotor apparatus well evolving capabilities brain instance infants reduced visual acuity prevents accessing high visual frequencies well distinguishing distant objects. acuity progressively grows maturation process evolves. using constraints synergy goal babbling intrinsic motivation explored would potentially allow constrain simplify learning since ﬁrst actions robot could crucial considering lifelong learning unbounded task spaces.", "year": 2013}