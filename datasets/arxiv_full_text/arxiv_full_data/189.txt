{"title": "Structured Convolution Matrices for Energy-efficient Deep learning", "tag": ["cs.NE", "cs.AI", "cs.CV", "cs.LG"], "abstract": "We derive a relationship between network representation in energy-efficient neuromorphic architectures and block Toplitz convolutional matrices. Inspired by this connection, we develop deep convolutional networks using a family of structured convolutional matrices and achieve state-of-the-art trade-off between energy efficiency and classification accuracy for well-known image recognition tasks. We also put forward a novel method to train binary convolutional networks by utilising an existing connection between noisy-rectified linear units and binary activations.", "text": "derive relationship network representation energy-efﬁcient neuromorphic architectures block toplitz convolutional matrices. inspired connection develop deep convolutional networks using family structured convolutional matrices achieve state-of-the-art trade-off energy efﬁciency classiﬁcation accuracy well-known image recognition tasks. also forward novel method train binary convolutional networks utilising existing connection noisy-rectiﬁed linear units binary activations. deep convolutional networks have recent times achieved near-human performance array visual auditory cognitive tasks intriguing possibility delivering deep learning applications mobile devices well providing energy-efﬁcient cognitive solutions cloud inspired increasing number researchers search low-precision state-of-the-art convolutional networks deployed extremely energy-efﬁcient platforms binary convolutional networks binary convolutional kernels binary neuron activations ideally suited low-power neuromorphic architectures spike-based communication addition storage computational efﬁciency gained using structured matrices convolutional layers. using structured matrices fully connected layers deep networks studied literature objective reducing number learned parameters. main idea behind approaches restrict connectivity matrix layer known family matrices parametrised variables adapt backpropagation update variables. hand reducing memory computation requirements convolutional layers addressed several approaches model compression mostly training. work propose structured matrices convolutional layers inspired low-power neuromorphic hardware architectures connecting efﬁcient weight representation schema used neuromorphic architectures block toeplitz matrices arise discrete convolution identify family convolution kernels naturally hardware efﬁcient. primary motivation behind investigation tradition discovering algorithms native chosen architecture thereby harvesting best particular architecture offers. incorporate learning structured convolutional matrices traditional stochastic gradient descent trained inference networks hardware-ready. furthermore exploit known equivalence stochastic bounded rectiﬁed linear units deterministic threshold neurons propose novel approach training networks binary neuron activations. procedure allows obtain accurate gradient estimates backward step speed-up convergence training enriches library tools available train low-precision deep neural networks. evaluate system cifar data compare best energy accuracy numbers reported currently available hardware approach reduces number truenorth cores required achieve cifar truenoth cores cores. begin next section discussing binary convolutional networks suitability neuromorphic hardware introduce weight representation mechanism used truenorth architecture section discuss structure discrete convolution matrices present main result connecting structure weight representation truenorth thereby identifying family structured convolution matrices efﬁciently architecture. section outlines methodology training networks structured convolution kernels discusses connection noisy relus binary neurons exploited training. summarize experimental results section followed concluding remarks section binary convolutional networks particularly elegant instance low-precision computing targeted toward object recognition tasks. motivated possibility deploying state-of-theart image recognition convolutional networks low-power mobile devices increasing interest around binary networks networks u-valued neurons throughout network thus best suited deployed neuromorphic hardware using spiking neurons. although restrict focus truenorth architecture study conclusions also applicable platforms offering similar features truenorth chip consists network neurosynaptic cores programmable connectivity synapses neuron parameters multicore array core consists input lines neurons synaptic crossbar array. input line core connect neuron core spike router input line accessible neurons core crossbar thereby resulting block-wise connectivity. communication from- withinchip performed using spikes. truenorth neurons variant integrateand-ﬁre model conﬁgurable parameters neuron’s state variable updates tick synapses individually conﬁgurable on/off states strength assigned look-up table. speciﬁcally neuron -entry table -bit signed-integers input line core assigned input-type synapse determines strength using input-type source side index table neuron target side. summary non-negative integers crossbar weight matrix factored three components binary connectivity matrix input-type vector length integers strength-value functions neurons core. extend functions operate vectors element-wise weight matrix core written spgq denotes hadamard product matrices. account fact truenorth neuron connect input features network structure paper constrained partitioning features layer equally sized groups. neurons group connect input features group output features different groups generated disjoint input features. number groups layer chosen total ﬁlter size well number output features group less equal promote sparsity connections allow convolutional ﬁlters u-valued. number neurons spike response input image inﬂuence energy utilization well number classiﬁcation unit time underlying hardware support. mind strive train networks sparse binary activations. networks studied work u-valued neurons average fraction less considerations result networks sparse connectivity weights activations. discrete convolution written matrix multiplication resulting ‘convolution matrix’ block toeplitz structure establishing connection weight representation toeplitz matrices identify convolution kernels efﬁciently truenorth architecture. need notations giving formal statement connection. arbitrary matrix denote i-th column jq-th element hij. arbitrary scalar-valued function real numbers extend operate matrices applying element-wise. functions denotes function composition. denote convolution data matrix convolution kernel matrix matrix vecpy denote vectorization fact denotes convolution matrix denotes zero matrices appropriate dimension nˆpn´ toeplitz matrix constructed i-th column convolution kernel npn´ matrix referred block toeplitz convolution matrix write wpkq make dependence explicit. sparsity circulant nature convolution matrices fully exploited approaches accelerate training inference suppose dimension data matrix ideally every choice kernel corresponding convolution matrix wpkq represented form choice type vector strength functions binary connectivity matrix however representation requires input-type vector common columns weight allow toeplitz matrices non-square. name symmetric kernels motivated fact kernels generated using commutative notational simplicity ignore boundary effects assume convolution stride one-to-one relationship convolution kernels block toeplitz matrices form theorem asserts block toeplitz matrices associated symmetric kernels characterized theorem represented characterization also suggests constructive procedure generate symmetric kernels. proof theorem outlined appendix. example also included appendix explains problem relating weight representation convolution matrices detail. theorem non-negative integers kernel matrix. exist ctsiun refer convolution kernels identiﬁed theorem symmetric kernels notation sympf refer particular instance. following examples ordered-pair notation deﬁne functions show well-known convolutional operators family symmetric kernels identiﬁed theorem example notice symmetric kernels include instances separable well nonseparable matrices. simple search reveals unique pairs commutative permutations four elements used equation moreover since consider u-valued convolution kernels work function restricted u-valued thus possible choices since possible choices components independent total possible symmetric kernels dimension limited discussion thus section convolutional matrices ease presentation. however recall deep convolutional networks matrices convolutional kernels extend deﬁnition symmetric kernels cover important case. symmetric kernel deﬁned typical deep learning setting goal learn kernels every convolutional layer order minimize loss function using gradient-based optimization method. case want kernels class symmetric kernels efﬁciently truenorth architecture. strategy follows ﬁrst allow every kernel network learned without constraints. next goal generate trained network maps truenorth performance validation data saturates unconstrained real-valued kernel replaced suitable member symmetric kernels described below. rlˆlˆm denotes unconstrained convolution kernel fρσσbprslˆlˆm subsequently kernel replaced sympf˚ network. done kernels layer simultaneously parameters frozen remainder training. notice however elements real stage training. backpropagation used train binarize ensuing period. used expectation maximization algorithm speed-up search weights layer converted symmetric kernels network trained least epoch next layer weights replaced. rectiﬁed linear unites typical form non-linearity used deep networks. approach seeks binary activations obtained threshold functions neuromorphic harware spike-based communication infrastruture efﬁciently. strategy begin training relu activations make increasingly noisy replaced binary activations end. using stochastic neuron activation functions explored literature various purposes early attempt using noisy relu objective obtaining binary activations reported single-layer networks. well-known information theory best communicate using amplitudebounded channel presence additive noise form shown figure discrete input values appropriate choice noise variance binary signaling optimal. moreover unlike information theory noise distribution variance presupposed freedom choose vary throughout training period. seek utilize connection train binary convolutional network using noisy relus. training bounded noisy relu shown figure zero mean uniform distributed random variable distributed range noise source. range random variable slowly increased towards training period noisy relus replaced threshold neuron shown figure layer time tuning network. gradient relu saturating backward step throughout training. networks trained gpus using matconvnet deep learning libraries stochastic gradient descent used dropout layers momentum weight decay batch normalization parameters learned training mapped hardware using reusable composable network description functions called corelets corelets created work automatically compile learned network parameters independent neuromorphic platform platform-speciﬁc hardware conﬁguration directly program truenorth chips. proposed method evaluated using image recognition benchmark dataset cifar- cifar- dataset consists color natural images pixels size classes training images test images. evaluated representation power symmetric kernels designing convolution networks using four truenorth chips trained cifar dataset. recent results well experiments seem suggest pooling layers unsuitable networks binary neuron activations. networks built stacking four sets convolution layers contains four convolution layers. smaller network described table large network used obtained simply increasing number features layer. ﬁnal output features divided uniformly among classes evaluation performed classiﬁcation hardware tick. layer patch size/ output size/ patch size/ output size/ patch size/ output size/ patch size/ output size/ groups sets table structure -chip symmetric kernel convolution network. network consists four sets characterize performance trained -chip convolution networks deployed truenorth board packed neuromorphic cores measure classiﬁcation accuracy throughput. -chip networks simulation classiﬁcation accuracies measured dataset. results shown table comparison energy-efﬁcient deep networks truenorth using symmetric kernels able deploy network twice many features number cores possible using approach described thus able obtain better accuracy amount hardware even though allowed convolutional kernels strict subset u-valued kernels allowed multi-chip networks achieved near state-of-the-art accuracy signiﬁcantly less number truenorth cores improving energy efﬁciency two-fold. study shows convolutional networks built using structured kernels surprisingly rich maintain representational power backpropagation adapted learn presence constraints. furthermore investigation suggests co-designing algorithms architecture offer successful strategy toward even energy efﬁcient deployment platform deep learning. appendix remark suppose truenorth core receives vectorized matrix along input-lines computes convolution laplacian operator example hence easily veriﬁed wpkq matrix. since satisﬁes conditions theorem example construct ctsiu wpkq. deﬁne matrix vecpgq type-vector consequently input element assigned type gij. next construct binary matrix column time deﬁne strength functions. deﬁne intermediate matrix follows since ctsiu r-th column simply element-wise product srpgq follows fact masks components contribute lq-th convolution result all-one column vectors simply elements matrix them. follows deﬁnition convolution follows since r-th element row-vector vecpxqt wpkq corresponds lq-th convolution result follows since holds follows since arbitrary last equality completes proof ﬁrst part theorem applied example following example shows construction input-type vector binary connectivity matrix strength functions much detail concreteness. example consider convolving matrix kernels denote resulting matrix kernel placed corresponding elements matrix needed compute below consider performing matrix multiplication using truenorth core. naturally input vector vecpxq routed input lines truenorth core order shown matrices either side hadamard product implemented using crossbar selecting appropriate binary connectivity matrix four strength functions matrix deﬁnes input-type sufﬁcient able compute aforementioned convolution. obvious choice binary matrix remainder discussion focuses choice type vector strength functions. since strength function common rows particular column rows distinct non-zero entries column must assigned distinct types. illustrate further consider following kernel rows ﬁrst column free choose color based subsequent columns remain uncolored now. however even though colors chosen considering ﬁrst column important rows subsequent column color contain distinct non-zero entries. case coloring scheme said ‘conﬂict’ conﬂictfree coloring scheme exist. notice current coloring contain conﬂicts proceed remaining columns time type-vector simply indicates color index corresponding coloring scheme represents valid coloring solution conﬂict-free. strength function deﬁned letting sipcq unique non-zero value associated color index column solution satisﬁes general suppose kernel satisﬁes fpσi´ pρqqq function commuting permutations illustrate idea behind coloring procedure never conﬂicts simplicity assume function invertible. since exactly four distinct entries rows matrix whose ﬁrst column contains non-zero entry colored four distinct colors without conﬂict column. assume coloring based non-zero entries ﬁrst column rows already assigned color shown below precisely needed. similar analysis used show rows fact deﬁning matrix colored without conﬂicts matrix pρqq letting vecpgq obtain conﬂict-free coloring solution outline proof theorem extending arguments remark straightforward manner proves ﬁrst part theorem. outline proof second half theorem assuming proof general case straightforward extension arguments here. assume contains least four distinct non-zero entries statement theorem. suppose exist tsiun wpkq construct recall range cannot contain zero consequently srpgq cannot contain zero entries. lq-th convolution depends txij furthermore since theorem statement also requires contain zeros lq-th convolution depends txij non-trivially. hence denotes matrix constructed vecphq) must satisfy follows fact valid equating right-hand-side follows observing contains four distinct non-zero elements domain range strength functions contain exactly four elements thus become bijections hence inverses exist similar using deﬁning follows follows using follows implies σpgq implies σpgq σpgq σpσpgqq) follows applying order reversed before–for example implies σpgq implies σpgq hence σpgq σpσpgqq. last equalities show commute elements since contains four distinct elements must commute sake uniqueness silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot mastering game deep neural networks tree search nature vol. esser merolla arthur cassidy appuswamy andreopoulos berg mckinstry melano barch nolfo datta amir taba flickner modha convolutional networks fast energy-efﬁcient neuromorphic computing corr vol. abs/. available http//arxiv.org/abs/. courbariaux bengio j.-p. david binaryconnect training deep neural networks binary weights propagations advances neural information processing systems park choi yang shin compression deep convolutional neural networks fast power mobile applications corr vol. abs/. available http//arxiv.org/abs/. merolla arthur alvarez-icaza cassidy sawada akopyan jackson imam nakamura brezzo esser appuswamy taba amir flickner risk manohar modha million spiking-neuron integrated circuit scalable communication network interface science vol. ionescu vantzos sminchisescu matrix backpropagation deep networks structured layers proceedings ieee international conference computer vision nakkiran alvarez prabhavalkar parada compressing deep neural networks using rankconstrained topology proceedings annual conference international speech communication association dally deep compression compressing deep neural network pruning trained quantization huffman coding corr vol. abs/. available http//arxiv.org/abs/. sainath kingsbury ramabhadran auto-encoder bottleneck features using deep belief networks ieee international conference acoustics speech signal processing icassp kyoto japan march available http//dx.doi.org/./icassp.. sainath kingsbury sindhwani arisoy ramabhadran low-rank matrix factorization deep neural network training high-dimensional output targets acoustics speech signal processing ieee international conference painkras plana garside temple galluppi patterson lester brown furber spinnaker -core system-on-chip massively-parallel neural network simulation solid-state circuits ieee journal vol. pfeil grübl jeltsch müller müller petrovici schmuker brüderle schemmel meier networks universal neuromorphic computing substrate arxiv preprint arxiv. moradi indiveri event-based neural network architecture asynchronous programmable synaptic memory biomedical circuits systems ieee transactions vol. park neftci cauwenberghs k-neuron -mevents/s -pj/event asynchronous micro-pipelined integrate-and-ﬁre array transceiver biomedical circuits systems conference ieee. cassidy merolla arthur esser jackson alvarez-icaza datta sawada wong feldman cognitive computing building block versatile efﬁcient digital neuron model neurosynaptic cores neural networks international joint conference ieee chetlur woolley vandermersch cohen tran catanzaro shelhamer cudnn efﬁcient primitives deep learning corr vol. abs/. available http//arxiv.org/abs/. qadeer hameed shacham venkatesan kozyrakis horowitz convolution engine balancing efﬁciency ﬂexibility specialized computing sigarch computer architecture news vol. ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift proceedings international conference machine learning amir datta risk cassidy kusnitz esser andreopoulos wong flickner alvarez-icaza cognitive computing programming paradigm corelet language composing networks neurosynaptic cores neural networks international joint conference preissl wong datta flickner singh esser risk simon modha compass scalable simulator architecture cognitive computing conference high performance computing networking storage analysis", "year": 2016}