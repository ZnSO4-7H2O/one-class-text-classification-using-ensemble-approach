{"title": "An Equivalence of Fully Connected Layer and Convolutional Layer", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "This article demonstrates that convolutional operation can be converted to matrix multiplication, which has the same calculation way with fully connected layer. The article is helpful for the beginners of the neural network to understand how fully connected layer and the convolutional layer work in the backend. To be concise and to make the article more readable, we only consider the linear case. It can be extended to the non-linear case easily through plugging in a non-linear encapsulation to the values like this $\\sigma(x)$ denoted as $x^{\\prime}$.", "text": "article demonstrates convolutional operation converted matrix multiplication calculation fully connected layer. article helpful beginners neural network understand fully connected layer convolutional layer work backend. concise make article readable consider linear case. extended non-linear case easily plugging non-linear encapsulation values like denoted many tutorials explain fully connected layer convolutional layer separately mention fully connected layer special case convolutional layer naghizadeh sacchi comes method convert multidimensional convolution operations convolution operations still convolutional level. illustrate conv operations computed matrix multiplication convert conv layers layers analyze properties conv layers equivalent layers e.g. uncertainty conv layers apply methods layers conv layers e.g. network morphism computation conv operations matrix multiplication manner efﬁcient needs much memory storage. convolutional neural network consists conv layers. fashionable various types networks derive residual network inception network work non-trivial understand convolutional operation well. formally convolutional operation deﬁned continuous dimension. denote convolutional operation. following sections organized follows. section shows details matrix multiplication fully connected layer. then section introduces common explanations convolutional operations. section demonstrates convert convolutional operation matrix multiplication. section shows result simple experiment training equivalent networks fully connected network convolutional neural network. notation rest note scalar variables denoted non-bold font lowercases e.g. scalar values. matrix vectors denoted bold font capitals lowercases respectively. example ra×b means matrix shape means column vector dimensions. figure network fully connected layers neurons layer respectively. layers denoted output vector layer rn×. represent weight matrix rn×n column vector column weight vector corresponding neuron layer thus output given many tutorials convolutional operation deep learning unintelligible beginners deep learning. section illustrate understand compute convolutional operation matrix multiplication manner. section states common explanations convolutional operation. convolutional operation point-wise multiplication often used simplicity instead convolutional operations shown point-wise matrix multiplication variables shown following equation index ﬁlter input patch example compute convolution patch ilter point-wise multiplication. difference convolution point-wise multiplication convolutional operation needs reverse ﬁlter along every dimension. channels respectively. take ﬁlters example shown figure every kernel size rkh×kw×cin height width number channels respectively. three different colors differentiate three ﬁlters respectively. dashed lines figure depict convolution operation yellow ﬁlter green patch result corresponding position bout. every ﬁlter moves across left right step size different color positions bout output kernel color. process deﬁned convolutional operation denoted represent kernels rf×kh×kw×cin number kernels denote bout bin. exists relationship input shape output shape convolutional operation. stride denoted width direction denoted height direction respectively. usually value represent practice desired output shape often need zeros around borders input. denote number rows columns want side three main padding ways non-zeo padding half-padding full-padding show relationships input shape output shape convolutional operation extend analysis give details convert conv layer layer. adopt convolutional view point shown figure assume contained padding part batch size kernel moves across spatial space stride step equal extracting patches size according movement kernel input kernel convolved point-wise multiplied patches. patch ﬂattened vector dimension r×khkwcin. patches constitute matrix whose dimension shown part figure hout wout means input conv layer seen hout wout inputs layer. whole matrix figure denoted dimension accordingly ﬁlter also ﬂattened column vector shape khkwcin ﬂattened ﬁlters make ﬁlter matrix shown figure denoted whose dimension number ﬁlters. output given whose shape want convert output matrix multiplication back output conv layer reshape result shape example image shape mnist. ﬁlter shape stride ease explanation. figure demonstrates stretching process result. patches overlap width height ﬁlter equal stride. extract patches patch shape ﬂatten patch vector figure stack vertically together shown figure ﬁlter also ﬂattened column vector. ﬁlter ﬁlters stacked horizontally. experiment shown figure instead limitation apis keras. separated matrices. matrix matrix shape like part figure shape operation divided multiplications matrices deep learning framework implementation converting convolutional operation matrix multiplication efﬁcient mapping function index method saves memory. mapping function describes relationship elements matrix stretching patches input matrix. don’t give detailed example scope article. simplify statement assume batch size equal i.e. know indexes algorithm converting convolutional operation matrix multiplication input feature shape padding; filters shape stride output feature bout shape experiment keras construct equivalent formulation layer number parameters shown figure ignore ﬂatten activation layers figure networks going learn identity function conv layer kernel size stride difference networks ﬁrst layer conv layer whose ﬁlter shape ﬁrst layer network dense layer whose weight shape batch size mean square error loss function. optimization method learning rate. training images validation images randomly sampled mnist training data validation data. weight initialization method acording train network original input data shape converted data shape based algorithm random seed networks similar initialization. simplify training process bias. networks trained epochs. training loss curve validation loss curve showed figure training validation loss curves network almost optimization. also train networks adam optimization compare results thing note mentioned section input data images network actually reshaped makes difference doesn’t affect weights ﬁrst dense layer network. separates matrix matrices matrix multiplies weights shape reshape data also extract outputs ﬁrst layer network denoted network respectively. images u||f result finally used compute compute plot histograms weights conv layers shown figure histograms almost network. histograms adam almost overlap. ﬂatten wcnn frobenius norm also tried adam method optimize networks training validation loss curves networks overlapped u||f f-norm perfectly like figure shown figure adam gets ﬂattened wcnn caused adaptive learning rates parameter larger update infrequent smaller update frequent parameters. note illustrate equivalence layer conv layer speciﬁc condition. convolutional operation safely converted matrix multiplication gives novel perspective understand convolutional neural network also case analysis difﬁcult convert conv layer layer analyze behavior layer manner analyze uncertainty layer manner. figure networks used experiments. ﬁrst value tuple batch size. position channels channel last. details input output shape please refer keras documents.", "year": 2017}