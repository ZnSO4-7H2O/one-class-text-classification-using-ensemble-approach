{"title": "Scalable Sparse Subspace Clustering by Orthogonal Matching Pursuit", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Subspace clustering methods based on $\\ell_1$, $\\ell_2$ or nuclear norm regularization have become very popular due to their simplicity, theoretical guarantees and empirical success. However, the choice of the regularizer can greatly impact both theory and practice. For instance, $\\ell_1$ regularization is guaranteed to give a subspace-preserving affinity (i.e., there are no connections between points from different subspaces) under broad conditions (e.g., arbitrary subspaces and corrupted data). However, it requires solving a large scale convex optimization problem. On the other hand, $\\ell_2$ and nuclear norm regularization provide efficient closed form solutions, but require very strong assumptions to guarantee a subspace-preserving affinity, e.g., independent subspaces and uncorrupted data. In this paper we study a subspace clustering method based on orthogonal matching pursuit. We show that the method is both computationally efficient and guaranteed to give a subspace-preserving affinity under broad conditions. Experiments on synthetic data verify our theoretical analysis, and applications in handwritten digit and face clustering show that our approach achieves the best trade off between accuracy and efficiency.", "text": "niques methods based applying spectral clustering afﬁnity matrix obtained solving optimization problem incorporates nuclear norm regularization become extremely popular simplicity theoretical correctness empirical success. methods based so-called self-expressiveness property data lying union subspaces originally proposed property states point union subspaces written linear combination data points subspaces. unique solution exist solutions whose entries subspace example point always written linear combination points solutions called subspace preserving since preserve clustering subspaces. given subspace preserving build afﬁnity matrix every pair points |cij|+|cji| apply spectral clustering cluster data. subspace preserving existing methods regularize norm solve problem form instance sparse subspace clustering algorithm uses norm encourage sparsity prior work shown gives subspace-preserving solution subspaces independent data different subspaces satisfy certain separation conditions data subspace well spread similar results exist cases data corrupted noise outliers selfexpressiveness based methods different regularizations coefﬁcient matrix least squares regression uses regularization rank representation rank subspace clustering nuclear norm minimization encourage subspace clustering methods based nuclear norm regularization become popular simplicity theoretical guarantees empirical success. however choice regularizer greatly impact theory practice. instance regularization guaranteed give subspace-preserving afﬁnity broad conditions however requires solving large scale convex optimization problem. hand nuclear norm regularization provide efﬁcient closed form solutions require strong assumptions guarantee subspace-preserving afﬁnity e.g. independent subspaces uncorrupted data. paper study subspace clustering method based orthogonal matching pursuit. show method computationally efﬁcient guaranteed give subspace-preserving afﬁnity broad conditions. experiments synthetic data verify theoretical analysis applications handwritten digit face clustering show approach achieves best trade accuracy efﬁciency. many computer vision applications motion segmentation hand written digit clustering face clustering data different classes well approximated union dimensional subspaces. scenarios task partition data according membership data points subspaces. formally given points ∈rd}n lying unknown number subspaces {si}n unknown dimensions {di}n subspace clustering problem clustering data groups group contains data points subspace. problem received great attention past decade many subspace clustering algorithms developed including iterative algebraic statistical spectral clustering based methods review). sparse rank methods. among existing techlow-rank. based these study regularizations mixture propose regularizations blend nuclear norm. advantage regularized nuclear norm regularized lrsc sparsity regularized solution computed closed form data matrix thus computationally attractive. however resulting subspace preserving subspaces independent data uncorrupted. thus need methods guarantee subspace-preserving afﬁnity broad conditions computationally efﬁcient. paper contributions. work study selfexpressiveness based subspace clustering method uses orthogonal matching pursuit sparse representation lieu -based basis pursuit method. method termed ssc-omp kinship original referred ssc-bp paper. main contributions paper theoretical conditions afﬁnity produced sscomp subspace preserving demonstrate efﬁciency large scale problems. speciﬁcally show that subspaces data deterministic ssc-omp gives subspace-preserving subspaces independent else subspaces sufﬁciently separated data well distributed. subspaces data drawn uniformly random ssc-omp gives subspace-preserving dimensions subspaces sufﬁciently small relative ambient dimension factor controlled sample density number subspaces. related work. worth noting idea using already considered core contribution work provide much weaker succinct interpretable conditions afﬁnity subspace preserving case arbitrary subspaces. particular conditions naturally related ssc-bp reveal insights relationship between sparsity-based subspace clustering methods. moreover experimental results provide much detailed evaluation behavior ssc-omp large-scale problems. also worth noting conditions gives subspace-preserving representation also studied paper presents much comprehensive study subspace clustering problem providing results deterministic independent deterministic arbitrary random subspace models. particular result deterministic arbitrary models much stronger main result algorithm approaches subspace clustering problem ﬁnding sparse representation point terms data points. since point expressed terms points sparse representation always exists. principle solving following optimization problem optimization problems studied extensively compressed sensing community e.g. tutorials well known that certain conditions dictionary solutions same. however results compressed sensing apply subspace clustering problem columns union subspaces solution need unique motivated extensive research conditions solutions useful subspace clustering. shown subspaces either independent disjoint data noise free well distributed provide sparse representation subspace preserving deﬁned next. deﬁnition representation point terms dicincluding independent subspaces well arbitrary subspaces. study case subspaces data points drawn random. independent deterministic subspace model algorithm sparse subspace clustering orthogonal matching pursuit input data parameters kmax compute |c∗| |c∗|. done ssc-bp. procedure summarized algorithm shown effective sparse recovery advantage admits simple fast implementations. however note existing conditions correctness sparse recovery strong subspace clustering problem. particular note matrix need satisfy mutual incoherence restricted isometry properties points subspace could arbitrarily close other. importantly conditions applicable goal recover unique sparse solution. fact sparse solution unique since linearly independent points represent point therefore need conditions output guaranteed subspace preserving. section devoted studying sufﬁcient conditions ssc-omp gives subspace-preserving representation. analysis assumes data noiseless. termination parameters algorithm kmax large enough also assume columns normalized unit norm. make results consistent state-of-the-art results ﬁrst study case subspaces deterministic notice subspaces independent disjoint i.e. intersect origin. however pairwise disjoint subspaces need independent e.g. three lines disjoint independent. notice also subset independent subspaces also independent. therefore subspaces independent subspaces independent hence disjoint. particular implies {si}n indem=i independent. establish conditions ssc-omp gives subspace-preserving afﬁnity independent subspaces important note computing goal select points subspace process selecting points occurs step algorithm products points current residual computed point highest product chosen. since ﬁrst iteration residual could immediately choose point another subspace whenever product point another subspace larger product points subspace. following theorem shows that even though select points wrong subspaces iterations proceed coefﬁcients associated points subspaces zero end. therefore guaranteed subspace-preserving representation. theorem subspaces independent gives subspace-preserving representation data point. proof. assume since kmax large gives exact representation i.e. thus since independent coefﬁcients data points must zero. arbitrary deterministic subspace model consider general class subspaces need independent disjoint investigate conditions gives subspace-preserving representation. following rd×ni denotes submatrix containing points subspace; denotes matrix point removed; denote respectively vectors contained columns i−j. easy sufﬁcient condition subspace preserving step algorithm point maximizes product lies subspace since equal minus projection onto subspace spanned selected point follows simple induction argument follows selected points residuals {qk}. suggests condition subspace preserving must depend products data points subset residuals motivates following deﬁnition lemma. deﬁnition residual vectors computed step omp. residual directions associated matrix point deﬁned proof. using induction argument easy condition implies sequence residuals ﬁctitious problem omp. hence output construction subspace-preserving. intuitively lemma tells product between residual directions subspace data points subspaces smaller product residual directions subspace points subspace gives subspace-preserving representation. condition intuitive perspective intuitive perspective subspace clustering rely geometry problem. speciﬁcally directly depend relative conﬁguration subspaces distribution data subspaces. follows derive conditions subspaces data guarantee condition holds. need additional deﬁnitions. deﬁnition coherence sets points unit norm deﬁned maxx∈x coherence measures degree similarity between sets points. case left hand side bounded coherence bek=i coherence small implies data points different subspaces sufﬁciently separated deﬁnition inradius convex body radius largest euclidean ball inscribed note points subspace step ptk+b conditions thus show subspace points coherence points subspaces points uniformly located large inradius. agreement intuition points different subspaces well separated points within subspace well distributed. comparison corollary theorem note lemma condition tighter condition making theorem preferable. corollary advantage sides condition depend directly data points condition depends residual points making algorithm speciﬁc. still small long points near intersection actually even strong assumption since intersection subspace. thus could close intersection coherence. argument also works condition admittedly speciﬁc distributions points possible exists arbitrarily close near intersection. however worst case scenario unlikely happen consider random model discussed next. arbitrary random subspace model section considers fully random union subspaces model basis elements subspace chosen uniformly random unit sphere ambient space data points subspace uniformly distributed unit sphere subspace. theorem shows sufﬁcient condition holds true high probability given conditions subspace dimension ambient space dimension number subspaces number data points subspace. theorem assume random union subspaces model subspaces equal dimension number data points subspace density total number data points subspaces output subspace preserving probability interpretation condition dimension subspaces small relative ambient dimension also shows number subspaces increases factor also increases making condition difﬁcult satisﬁed. terms density shown exists then easy term depends log) monotonρ ically increasing function makes condition easier satisﬁed density points subspaces increases. moreover probability success also increasing function greater threshold value. consequence density points increases condition theorem becomes easier satisfy probability success also increases. section compare results ssc-omp methods general form methods include ssc-bp uses norm regularizer lrsc nuclear norm uses norm. also compare results ssc-omp. comparison terms whether solutions given alternative algorithms subspace-preserving. independent subspaces. independence strong assumption union subspaces. assumption subspace trivial intersection every subspace also union subspaces. case turns especially easy large category self-expressive subspace clustering methods ssc-bp lrsc able give subspacepreserving representations. thus easy case proposed method good state-of-the-art methods. arbitrary subspaces. best knowledge subspaces independent guarantee correctness lrsc lsr. ssc-bp shown representation subspace-preserving dual directions associated comparing result condition right hand sides same. however left hand sides directly comparable general relationship known sets nonetheless notice number points sets since card card nidi. therefore assume points distributed uniformly random unit sphere expected larger making condition ssc-omp less likely satisﬁed ssc-bp. comparing condition left hand sides comparable random model contain points. however right hand side less equal since data normalized makes condition ssc-omp difﬁcult hold ssc-bp. however difference expected vanish large scale problems ssc-omp computationally efﬁcient section random subspaces. random model shows ssc-bp gives subspace-preserving representation probability methods whose output generally subspace preserving second measures close coefﬁcients subspace preserving. percentage subspace-preserving representations percentage points whose representations subspace-preserving. inexactness solvers coefﬁcients absolute value less considered zero. subspace-preserving solution gives subspace-preserving representation error compute fraction norm comes subspaces average i.e. i/cj) true afﬁnity. subspace-preserving gives performance subspace clustering depends subspace-preserving property also connectivity similarity graph i.e. whether data points cluster form connected component graph. connectivity undirected graph weights rn×n degree matrix diag vector ones second smallest eigenvalue normalized laplacian d−/w measure connectivity graph; range zero case compute graph connected algebraic connectivity cluster take quantity mini performance subspace clustering methods. clustering accuracy percentage correctly labeled data points. computed matching estimated true labels πjqtrue permutation groups qest qtrue estimated ground-truth labeling data respectively entry equal point belongs cluster zero otherwise. running time clustering task using rmatlab. reported numbers experiments secrandomly generate subspaces dimension ambient space dimension subspace contains sample points randomly generated unit sphere varied number points varies ssc-omp algorithm kmax ssc-bp -magic solver. computational complexity ssc-bp subspace-preserving representation percentage error plotted figure observe probability ssc-omp gives subspace-preserving solution grows density data point increases. high probability exactly same. difference ssc-bp higher probability success ssc-omp however easy difference probability goes zero density goes inﬁnity. means performance difference vanishes scale problem increases. results ssc-omp. finally compare results ssc-omp. deﬁne principal angle subspaces merit result introduces subspace angles condition satisﬁes intuition algorithm likely work subspaces apart other. however condition shows intricate relationship intra-class property inter-class property greatly complicates interpretation condition. importantly shown appendix condition restrictive makes theorem stronger result. section ﬁrst verify theoretical results ssc-omp compare ssc-bp doing experiments synthetic data using random model. speciﬁcally show even subspaces independent solution subspace-preserving probability grows density data points. second test performance proposed method clustering images handwritten digits human faces conclude ssc-omp achieves best trade between accuracy efﬁciency. methods. compare performance state-of-the-art spectral subspace clustering methods including lrsc ssc-bp spectral curvature clustering real experiments code provided respective authors computing representation matrix parameters tuned give best clustering accuracy. apply normalized spectral clustering afﬁnity |c∗| |c∗| except spectral clustering step. metrics. metrics evaluate degree subspace-preserving property satisﬁed. ﬁrst direct measure whether solution subspace preserving not. however comparing state figure performance ssc-omp ssc-bp synthetic data. data drawn subspaces dimension ambient dimension subspace contains number points overall number points varied shown scale. ssc-bp however maximum number points tested time limit. notice bottom right ﬁgure also uses scale y-axis. subspace clustering perspective interested well method performs terms clustering accuracy well efﬁcient method terms running time. results plotted figure together connectivity ﬁrst observe ssc-omp good connectivity ssc-bp. could partly fact fewer correct connections ﬁrst place shown subspace-preserving percentage. clustering accuracy ssc-omp also outperformed ssc-bp. comes surprise sparse representations produced ssc-omp subspace-preserving well connected ssc-bp. however observe density data points increases difference clustering accuracy also decreases ssc-omp seems achieve arbitrarily good clustering accuracy large also evident figure ssc-omp signiﬁcantly faster orders magnitude faster ssc-bp clustering points. conclude increases difference clustering accuracy sscomp ssc-bp reduces ssc-omp signiﬁcantly faster makes preferable large-scale problems. experiment evaluate performance different subspace clustering methods clustering images handwritten digits. mnist dataset contains grey scale images handwritten digits experiment randomly chosen images digits chosen. image compute feature vectors using scattering convolution network feature vector concatenation coefﬁcients layer network translation invariant deformation stable. feature vector size feature vectors images projected dimension using pca. subspace clustering techniques applied projected features. results reported table numbers show ssc-omp ssc-bp give much smaller subspace-preserving representation error methods ssc-bp better ssc-omp. consistent theoretical analysis guarantee lrsc give subspacepreserving representation non-independent subspaces ssc-bp higher probability giving subspacepreserving representation ssc-omp. table performance subspace clustering methods mnist dataset. data consists randomly chosen number images digits features extracted scattering network projected dimension using pca. considering running time methods ssc-bp requires much computation especially number points large. though ssc-omp iterative method computation time twice lrsc closed form solutions. qualiﬁes proposed method large scale problems. experiment evaluate performance different subspace clustering methods extended yale dataset contains frontal face images individuals different illumination conditions size case data points original face images downsampled pixels. experiment randomly pick individuals take images data clustered. clustering performance different methods reported table terms subspace-preserving recovery observe slightly better performance sscbp ssc-omp cases. three methods large subspace-preserving representation errors especially number subjects terms clustering accuracy methods fairly well number clusters except worse others. number subjects increases lrsc maintain accuracy even worse ssc-omp ssctable performance subspace clustering methods eyaleb dataset. ’na’ denotes running error returned solver. data consists face images different illumination conditions randomly picked individuals. images downsampled size size used feature vectors subjects subspace-preserving representation error ssc-omp ssc-bp maintain reasonably good performance although accuracy also degrades gradually. ssc-bp performs slightly better number subjects ssc-omp performs better studied sparse subspace clustering algorithm based omp. derived theoretical conditions under ssc-omp guaranteed give subspacepreserving representation. conditions broader state-of-the-art methods based nuclear norm regularization slightly weaker ssc-bp. experiments synthetic real world datasets showed ssc-omp much accurate state-ofthe-art methods based nuclear norm regularization twice slow. hand ssc-omp slightly less accurate ssc-bp orders magnitude faster. moreover demonstrated subspace clustering experiments points. overall ssc-omp provided best accuracy versus computation trade-off large scale subspace clustering problems. note optimization algorithm ssc-bp inefﬁcient large scale problems recent work presents scalable algorithm elastic based subspace clustering. comparison work left future research. appendices provide proofs theoretical results paper. also provide parameters clustering methods studied handwritten digits face image clustering experiments. theorem. subspaces independent gives subspace-preserving representation every data point. proof. consider data point need show output subspace-preserving. assumption termination parameters kmax means particular always terminates iteration seen hold follows. algorithm computes nothing prove. thus complete proof suppose proceed prove algorithm columns indexed always linearly independent. evident step algorithm residual vector orthogonal every column indexed thus choosing entry added step algorithm points linearly dependent points indexed would zero inner product would picked. since columns added iteration know columns linearly independent must contain least linearly independent vectors conclude claimed. light result denoting follows line algorithm range matrix denotes columns indexed section provide detailed proof lemma proof follows straight forwardly comparing inductively steps procedure procedure ﬁctitious problem omp. idea procedures follow path condition lemma satisﬁed. proof. number iterations computed procedure prove solution subspace preserving showing contains indexes points i-th subspace. shown induction contains points i-th subspace every introduced deﬁnition plays essential role proof. notational clarity denote residual vector generated iteration algorithm residual vectors denoted induction also show terminate whenever first case argument contains indexes points subspace trivially satisﬁed since empty. also satisﬁed line algorithm given contains points subspace show ˆqk+ contains indexes points subspace could shown noticing added mqk|. entry step algorithm given m≤nm=j here since qk/qk then using condition know give index corresponds point guarantees contains points subspace moreover picked point evidently point picked iteration omp. follows step algorithm resultant residuals ˆqk+ also equal. case means ˆqk+ ﬁctitious problem terminate step. ﬁnishes mathematical induction. fact terminates iterations follows following facts established produces computations omp; collection vectors selected linearly independent contained subspace dimension equal ﬁrst prove maxkk=i maxkk=i µ/ri. this sufﬁces show µ/ri notice point subspace could written linear combination points i.e. speciﬁcally pick given following optimization program proof theorem theorem. assume random model subspaces equal dimension number data points subspace density total number data points subspaces output subspace preserving probability assumption. residual point also uniform distribution unit sphere depends points independent uniformly distributed. furthermore pair points distributed independently points independent. thus result equation applicable here. since pairs inner product using union bound purpose reproducible results report parameters used methods real data experiments. algorithm kmax true subspace dimension synthetic experiments digit clustering face clustering. regularization digit clustering face clustering. lrsc model parameters digit clustering face clustering. dimension digit clustering face clustering. -magic ssc-bp synthetic experiments. digit face clustering noisy variation ssc-bp digit clustering sparse outlying entries variation ssc-bp face clustering /µe. algorithms constants chosen optimize performance fair comparison allow standard pre/postprocessing used whenever improve clustering accuracy. preprocessing allow normalization original data points using norm postprocessing allow normalization coefﬁcient vectors using norm. experiments synthetic data pre/post-processing. digit clustering preprocessing applied ssc-bp post-processing used ssc-omp ssc-bp. face clustering preprocessing applied ssc-omp lrsc post-processing used ssc-bp lrsc.", "year": 2015}