{"title": "Quantization based Fast Inner Product Search", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We propose a quantization based approach for fast approximate Maximum Inner Product Search (MIPS). Each database vector is quantized in multiple subspaces via a set of codebooks, learned directly by minimizing the inner product quantization error. Then, the inner product of a query to a database vector is approximated as the sum of inner products with the subspace quantizers. Different from recently proposed LSH approaches to MIPS, the database vectors and queries do not need to be augmented in a higher dimensional feature space. We also provide a theoretical analysis of the proposed approach, consisting of the concentration results under mild assumptions. Furthermore, if a small sample of example queries is given at the training time, we propose a modified codebook learning procedure which further improves the accuracy. Experimental results on a variety of datasets including those arising from deep neural networks show that the proposed approach significantly outperforms the existing state-of-the-art.", "text": "propose quantization based approach fast approximate maximum inner product search database vector quantized multiple subspaces codebooks learned directly minimizing inner product quantization error. then inner product query database vector approximated inner products subspace quantizers. different recently proposed approaches mips database vectors queries need augmented higher dimensional feature space. also provide theoretical analysis proposed approach consisting concentration results mild assumptions. furthermore small sample example queries given training time propose modiﬁed codebook learning procedure improves accuracy. experimental results variety datasets including arising deep neural networks show proposed approach signiﬁcantly outperforms existing state-of-the-art. many information processing tasks retrieval classiﬁcation involve computing inner product query vector database vectors goal returning database instances largest inner products. often called maximum inner product search problem. formally given database {xi}i=···n query vector drawn query distribution want argmaxx∈x deﬁnition trivially extended return top-n largest inner products. mips problem particularly appealing large scale applications. example recommendation system needs retrieve relevant items user inventory millions items whose relevance commonly represented inner products similarly large scale classiﬁcation system needs classify item categories number categories large brute-force computation inner products linear scan requires time space becomes computationally prohibitive number database vectors data dimensionality large. therefore valuable consider algorithms compress database compute approximate ||.|| norm. indeed database vectors scaled ||x|| constant mips problem becomes equivalent lnns θnns problems studied extensively literature. however norms database vectors vary often true practice mips problem becomes quite challenging. inner product satisfy basic axioms metric triangle inequality co-incidence. instance possible paper focus mips problem database query vectors arbitrary norms. main contribution paper develop quantization-based inner product search method address mips problem. formulate problem quantization codebook learning directly minimizes quantization error inner products furthermore small sample example queries provided training time propose constrained optimization framework improves accuracy also provide concentration-based theoretical analysis proposed method extensive experiments four real-world datasets involving recommendation deep-learning based classiﬁcation tasks show proposed approach consistently outperforms state-of-the-art techniques ﬁxed space ﬁxed time scenarios mips problem studied decade. instance cohen studied context document clustering presented method based randomized sampling without computing full matrix-vector multiplication. authors described procedure modify tree-based search adapt mips criterion. recently bachrach proposed approach transforms input vectors mips problem becomes equivalent lnns problem transformed space solved using pca-tree. mips problem received renewed attention recent seminal work shrivastava introduced asymmetric locality sensitive hashing technique provable search guarantees. also transform mips lnns popular technique speciﬁcally alsh applies different vector transformations database vector query respectively maxx∈x ||x|| constant satisﬁes nonnegative integer. hence mapped dimensional space asymmetrically. shrivastava showed mips original space equivalent lnns space. proposed hash function followed -dimensional vector whose entries sampled i.i.d llsh form standard gaussian sampled uniformly authors later proposed improved version alsh based signed random projection transforms vector using slightly different procedure represents binary code. then hamming distance used mips. recently neyshabur srebro argued symmetric transformation sufﬁcient develop provable approach mips problem query restricted unit norm. used transformation similar used bachrach augment original vectors maxx∈x||x|| q||q||. showed transformation signiﬁcantly improved results based paper take quantization based view mips problem show leads even better accuracy ﬁxed space ﬁxed time budget variety real world tasks. instead augmenting input vectors higher dimensional space approximate inner products mapping vector subspaces followed independent quantization database vectors subspace. work simple procedure generating subspaces. vector’s elements ﬁrst permuted using random permutation. permuted vector mapped subspaces using simple chunking done product codes ease notation rest paper assume d/k. subspace containing blocks database vectors {x}i=...n quantized codebook rl×ck number quantizers subspace without loss generality assume then database vector quantized subspace c-dimensional one-hot assignment vector exactly rest thus database vector quantized single dictionary element subspace. given quantized database vectors exact inner product approximated note approximation ’asymmetric’ sense database vectors quantized query vector quantize well lead increased approximation error. fact asymmetric computation database vectors still carried efﬁciently look tables similar except entry table product columns provide concentration inequalities estimator sec. next describe learning quantization codebooks different subspaces. focus different training scenarios database vectors given sample example queries also provided latter result signiﬁcant performance gain queries follow distribution database vectors. note actual queries used test time different example queries hence unknown training time. goal learn data quantizers minimize quantization error inner product approximation given assuming subspace independent expected squared error expressed eq∼q non-centered query covariance matrix subspace minimizing error equivalent solving modiﬁed k-means problem subspace independently. instead using euclidean distance mahalanobis distance speciﬁed used assignment. standard lloyd’s algorithm solution subspace iteratively alternating steps lloyd’s algorithm known converge local minimum also note resulting quantizers always euclidean means corresponding partitions hence lemma applicable well leading unbiased estimator. procedure requires non-centered query covariance matrix known query samples available training time. case possibility assume queries come distribution database vectors i.e. experiments show version performs reasonably well. however small example queries available training time besides estimating query covariance matrix propose impose novel constraints lead improved quantization described next. applications possible access small example queries course actual queries used test-time different set. given exemplar queries propose modify learning criterion imposing additional constraints minimizing expected quantization error. given query since interested ﬁnding database vector highest dot-product ideally want product query quantizer larger product quantizer. denote matrix containing subspace assignment vectors database vectors thus modiﬁed optimization given standard hinge loss nonnegative coefﬁcient. iterative procedure solve optimization alternates solving beginning codebook initialized random database vectors mapped subspace. then iterate following three steps find violated constraints element triplet i.e. note violated constraint found step equivalent ﬁnding nearest neighbor mahalanobis space speciﬁed update rule step becomes stationary point ﬁrst term. thus constraints violated procedure becomes identical k-means-like procedure described sec. steps guaranteed increase value objective practice found iterative procedure signiﬁcantly sped modifying step perturbation stationary point ﬁrst term single gradient step second term. time complexity step practice much cheaper limit number constraints iteration step takes step okc) time. experiments constraints iteration also step size iteration maximum number iterations section present concentration results quality quantization-based inner product search method. space constraints proofs theorems provided appendix. start deﬁning quantities. deﬁnition given ﬁxed event exact product least quantized version either smaller larger intuitively probability event measures chance difference exact quantized product large exact product large. would like probability small. next introduce concept balancedness subspaces. deﬁnition vector chunked subspaces chunking ηbalanced following holds every since input data satisfy balancedness condition next show random permutation tends create balanced subspaces. obviously random permutation applied vector entries change product. theorem vector dimensionality perm version applying random permutation dimensions. expected perm -balanced. another choice creating balancedness random rotation also change dotproduct. leads even better balancedness property discussed appendix next show probability upper bounded exponentially small quantity indicating quantized products accurately approximate large exact products quantizers means obtained mahalanobis k-means described sec. note case quantized dot-product unbiased estimator exact dot-product shown lemma theorem assume dataset dimensionality resides entirely ball radius centered further η-balanced applied pointwise )]k=···k martingale. denote qmax maxk=...k maxq∈q then exist sets theorem shows probability decreases exponentially number subspaces increases. consistent experimental observation increasing leads accurate retrieval. furthermore assume subspace independent slightly restrictive assumption martingale assumption made theorem berry-esseen inequality obtain even stronger upper bound given below. theorem suppose maxk=...k maxx datapoint quantizer subspace assume a\u0001|x| conducted experiments datasets summarized below movielens dataset consists user ratings collected movielens site users. setup described alsh paper extract latent dimensions results. dataset contains database vectors query vectors. netﬂix netﬂix dataset comes netﬂix prize challenge contains ratings users gave netﬂix movies. process suggested leads dimensional data. database vectors query vectors. imagenet dataset comes state-of-the-art googlenet image classiﬁer trained imagenet. goal speed maximum dot-product search last i.e. classiﬁcation layer. thus weight vectors different categories form database query vectors last hidden layer embeddings imagenet validation set. data dimensions database query vectors. videorec dataset consists embeddings user interests trained deep neural network predict relevant videos user. number videos repository network trained multi-label logistic loss. imagenet dataset last hidden layer embedding network used query vector classiﬁcation layer weights used database vectors. goal speed maximum product search query database vectors. database vector dimensions query contains vectors. following focus retrieving top- highest inner product neighbors movielens netﬂix experiments. imagenet dataset retrieve top- categories common literature. videorec dataset retrieve top- videos recommendation user. experiment three variants technique quip-cov uses database vectors training replaces k-means like codebook learning sec. quip-cov uses estimated held-out exemplar query k-means like codebook learning quip-opt uses full optimization based quantization compare performance state-of-the-art methods signed alsh alsh simple also compare pca-tree version adapted inner product search proposed shown better results ip-tree proposed quantization based methods perform much better pca-tree shown appendix. conduct sets experiments ﬁxed number bits used techniques kept same ﬁxed time time taken techniques ﬁxed same. ﬁxed experiments number bits quip variants codebook size subspace ﬁxed leading -bit representation database vector subspace. number subspaces varied leading representation respectively. ﬁxed time experiments ﬁrst note proposed quip variants table lookup based distance computation figure precision recall curves different methods movielens netﬂix datasets retrieving top- items. baselines signed alsh alsh simple proposed methods quip-cov quip-cov quip-opt. curves ﬁxed experiments plotted solid line baselines proposed methods number bits used respectively left right. curves ﬁxed time experiment plotted dashed lines. ﬁxed time plots ﬁxed plots proposed methods. baseline methods number bits used ﬁxed time experiments respectively running time comparable proposed methods. based techniques popcnt-based hamming distance computation. depending number bits used found popcnt times faster table lookup. thus ﬁxed-time experiments increase number bits lsh-based techniques times ensure time taken methods same. figure shows precision recall curves movielens netﬂix figure shows imagenet videorec datasets. quantization based approaches outperform based methods signiﬁcantly techniques number bits. even ﬁxed time experiments quantization based approaches remain superior lsh-based approaches even though former uses times less bits latter leading signiﬁcant reduction memory footprint. among quantization methods quip-cov typically performs better quip-cov performance large. theory non-centered covariance matrix queries quite different database leading drastically different results. however comparable performance implies often safe learning codebook. hand small example queries available quip-opt outperforms quipcov quip-cov four datasets. learns codebook constraints steer learning towards retrieving maximum product neighbors addition minimizing quantization error. overall training quip-opt quite fast requiring minutes using single-thread implementation depending dataset size. quantization based inner product search techniques described provide signiﬁcant speedup brute force search retaining high accuracy. however search complexity still linear number database points similar binary embedding methods exhaustive scan using hamming distance database size large linear scan even fast computation able provide required search efﬁciency. section describe simple procedure enhance speed quips based data partitioning. basic idea tree-quantization hybrids combine tree-based recursive data partitioning quips applied partition. training time ﬁrst learns locality-preserving tree hierarchical k-means tree followed applying quips partition. practice shallow tree learned leaf contains thousand points. course special case tree-based partitioners partitioner k-means. query time query assigned partition deal errors caused hard partitioning data. soft assignment query multiple partitions crucial achieving good accuracy high-dimensional data. figure precision recall curves videorec dataset retrieving top- items comparing quantization based methods tree-quantization hybrid methods. conduct ﬁxed comparison non-hybrid methods hybrid methods bits. non-hybrid methods considerable slower case conduct ﬁxed time experiment time retrieval ﬁxed taken hybrid methods non-hybrid approaches give much lower accuracy case. hybrid approaches tree-quip-cov tree-quip-opt) partitions query assigned nearest partitions based dot-product partition centers. tree-quip hybrids lead speed quips leading overall end-to-end speed brute force search. illustrate effectiveness hybrid approach plot precision recall curve fixed-bit fixedtime experiment videorec figure fixed-bit experiments tree-quantization methods almost accuracy non-hybrid counterparts versions) resulting speed ﬁxed-time experiments clear time budget hybrid approaches return much better results scan datapoints searching. conclusion described quantization based approach fast approximate inner product search relies robust learning codebooks multiple subspaces. proposed variants leads simple kmeans-like learning procedure outperforms existing state-of-the-art signiﬁcant margin. also introduced novel constraints quantization error minimization framework lead even better codebooks tuned problem highest dot-product search. extensive experiments retrieval classiﬁcation tasks show advantage proposed method existing techniques. future would like analyze theoretical guarantees associated constrained optimization procedure. addition tree-quantization hybrid approach tree partitioning quantization codebooks trained separately. future work consider training jointly. additional experimental results results imagenet videorec datasets different number neighbors different number bits shown figure addition compare performance approach pca-tree. recall curves respect different number returned neighbors shown figure figure recall curves different techniques different numbers returned neighbors plot recall curve instead precision recall curve pca-tree uses original vectors compute distances therefore precision recall top-k search. number bits used plots except signed alsh-fixedtime alsh-fixedtime simple lsh-fixedtime bits. pca-tree perform well datasets mostly fact dimensionality datasets relatively high trees known susceptible dimensionality. note original paper bachrach used datasets dimensionality figure upper bound probability event vector obtained random rotation η-balanced function number subspaces left ﬁgure corresponds right different curves correspond different data dimensionality another possibility random rotation performed instance applying random normalized hadamard matrix hadamard matrix matrix entries taken rows form orthogonal system. random normalized hadamard matrix obtained ﬁrst multiplying random diagonal matrix rescaling factor dimensionality data. since product invariant regards permutations rotations equivalent problem. theorem vector dimensionality applying linear transformation transformed vector η-balanced probability least −de− number blocks. proof. start following azuma’s concentration inequality also later lemma random variables ...} martingale following holds last inequality comes fact course argument valid blocks thus conclude expectation transformed vector -balanced. prove concentration inequalities regarding result. denote vivihjihji. ≤i<i≤d ξii| ﬁxed already boundedness balancedness conditions regarding datapoints assumed obtain exponentiallystrong concentration results regarding unbiased estimator considered paper. next show results obtained even boundedness balancedness conditions hold. present proof theorem figure upper bound probability event function number subspaces left ﬁgure corresponds right different curves correspond different data dimensionality assume entire data unit-ball norm uniformly split across chunks. therefore minimizing minimize variance random variable measures discrepancy exact answer quantized answer product query space truncated ﬁxed block. denote assume different blocks correspond independent sets dimensions. assumption often reasonable practice. case strengthen methods obtaining tight concentration inequalities. proof theorem covers scenario given below. proof. assume ﬁrst general case balancedness assumed. begin proof previous section i.e. consider random variable goal ﬁrst upper bound ar). proof theorem", "year": 2015}