{"title": "Barnes-Hut-SNE", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "The paper presents an O(N log N)-implementation of t-SNE -- an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots and that normally runs in O(N^2). The new implementation uses vantage-point trees to compute sparse pairwise similarities between the input data objects, and it uses a variant of the Barnes-Hut algorithm - an algorithm used by astronomers to perform N-body simulations - to approximate the forces between the corresponding points in the embedding. Our experiments show that the new algorithm, called Barnes-Hut-SNE, leads to substantial computational advantages over standard t-SNE, and that it makes it possible to learn embeddings of data sets with millions of objects.", "text": "paper presents o-implementation t-sne embedding technique commonly used visualization high-dimensional data scatter plots normally runs implementation uses vantage-point trees compute sparse pairwise similarities input data objects uses variant barnes-hut algorithm approximate forces corresponding points embedding. experiments show algorithm called barnes-hut-sne leads substantial computational advantages standard t-sne makes possible learn embeddings data sets millions objects. data-visualization techniques essential tool data analyst allow analyst visually explore data generate hypotheses. limitations traditional visualization techniques histograms scatter plots parallel coordinate plots overview) facilitate visualization data variables time. order idea structure variables data therefore necessary perform automatic analysis data making visualization instance learning low-dimensional embedding data. embedding data object represented low-dimensional point nearby points correspond similar data objects distant points correspond dissimilar data objects. low-dimensional embedding readily visualized e.g. scatter plot parallel coordinate plot. plethora embedding techniques proposed last decade e.g. creating twothree-dimensional embeddings readily visualized scatter plot family techniques based stochastic neighbor embedding recently become popular. techniques compute similarity matrix original data space low-dimensional embedding space; similarities take form probability distribution pairs points high probabilities correspond similar objects points. probabilities generally deﬁned normalized gaussian student-t kernels makes focuses preserving local data structure. embedding learned minimizing kullback-leibler divergence probability distributions original data space embedding space respect locations points embedding. resulting cost function non-convex minimization typically performed using ﬁrstsecond-order gradient-descent techniques gradient kullback-leibler divergence interpreted n-body system points exert forces other. limitations computational memory complexity scales quadratically number data objects practice limits applicability data sets thousand points. visualize larger data sets landmark implementations used hardly satisfactory solution. paper develop algorithm requires computation memory. algorithm computes sparse approximation similarities original data objects using vantage-point trees subsequently approximates forces points embedding using barnes-hut algorithm algorithm commonly used astronomers perform n-body simulations. barnes-hut algorithm reduces number pairwise forces needs computed exploiting fact forces exerted group points point relatively away similar. large body previous work focused decreasing computational complexity algorithms scale quadratically amount data implemented naively. studies focus speeding nearest-neighbor searches using space-partitioning trees cover trees vantage-point trees using locality sensitive hashing approaches motivated strong performance reported earlier work metric trees approximate similarities input objects algorithm. several prior studies also developed algorithms speed n-body computations. prominently developed dual-tree algorithm similar spirit barnes-hut algorithm work. dual-tree algorithm consider interactions single points groups points like barnes-hut algorithm considers interactions groups points. preliminary experiments found dual-tree barneshut algorithms perform used context t-sne barnes-hut algorithm conceptually simpler. prior work also used fast gaussian transform speed computation gaussian n-body interactions. since t-sne forces exerted bodies non-gaussian approach cannot readily applied here. t-distributed stochastic neighbor embedding minimizes divergence distributions distribution measures pairwise similarities original data objects distribution measures pairwise similarities corresponding points embedding. suppose given data objects function computes distance pair objects e.g. euclidean distance. learn s-dimensional embedding object represented point t-sne deﬁnes joint probabilities measure pairwise similarity objects symmetrizing conditional probabilities follows herein bandwidth gaussian kernels perplexity conditional distribution equals predeﬁned perplexity optimal value varies object found using simple binary search; details. heavy-tailed distribution used measure similarity corresponding points embedding embedding normalized student-t kernel used measure similarities rather normalized gaussian kernel account difference volume highlow-dimensional spaces locations embedding points learned minimizing kullbackleibler divergence joint distributions k=lyk yl))−. evaluation joint distributions respective normalization terms pairs points. since t-sne scales quadratically number objects applicability limited data sets thousand data objects; beyond that learning becomes slow. input similarities computed using gaussian kernel probabilities corresponding dissimilar input objects inﬁnitesimal. therefore sparse approximation probabilities without substantial negative effect quality ﬁnal embeddings. particular compute sparse approximation ﬁnding nearest neighbors data objects redeﬁning pairwise similarities herein represents nearest neighbors perplexity conditional distribution equals nearest neighbor sets found time building vantage-point tree data set. vantage-point tree. vantage-point tree node stores data object radius ball centered object non-leaf nodes children data objects located inside ball stored left child node whereas data objects located outside ball stored right child. tree constructed presenting data objects one-by-one traversing tree based whether current data object lies inside outside ball creating leaf node object stored. radius leaf node median distance object objects inside ball represented parent node. construct vantage-point tree objects need necessarily points high-dimensional feature space; availability metric sufﬁces. xj.) nearest-neighbor search performed using depth-ﬁrst search tree computes distance objects stored nodes target object whilst maintaining list current nearest neighbors distance furthest nearest neighbor current neighbor list. value determines whether node explored still objects inside ball whose distance target object smaller left node searched still objects outside ball whose distance target object smaller right node searched. order children searched depends whether target object lies inside outside current node ball left child examined ﬁrst object lies inside ball odds nearest neighbors target object also located inside ball. right child searched ﬁrst whenever target object lies outside ball. fattr denotes attractive forces whereas frep denotes repulsive forces computing attractive forces fattr computationally efﬁcient; done summing non-zero elements sparse distribution yj)− computed however naive computation repulsive forces frep develop barnes-hut algorithm approximate frep efﬁciently consider three points yj≈yi ykyj situation contributions frep roughly equal. barnes-hut algorithm exploits constructing quadtree current embedding traversing quadtree using depth-ﬁrst search iii) every node quadtree deciding whether corresponding cell used summary gradient contributions points cell. quadtree. quadtree tree node represents rectangular cell particular center width height. non-leaf nodes four children split cell four smaller cells northwest northeast southwest southeast center parent node leaf nodes represent cells contain point embedding; root node represents cell contains complete embedding. node store center-of-mass embedding points located inside corresponding cell ycell total number points inside cell ncell. quadtree nodes constructed time inserting points one-by-one splitting leaf node whenever second point inserted cell updating ycell ncell visited nodes. approximating gradient. approximate repulsive part gradient frep note cell sufﬁciently small sufﬁciently away point contriijz frep roughly similar points inside cell. butions icellz deﬁne therefore approximate contributions ncellq performqicellz ycell)−. ﬁrst approximate frepz depth-ﬁrst search quadtree assessing node whether node used summary embedding points located corresponding cell. i=jyi yj)− way. approximations thus obtained used compute frep frep frepz condition proposed decide whether cell used summary points cell. condition compares distance cell target point size rcell represents length diagonal cell consideration threshold trades speed accuracy preliminary experiments also explored various conditions take account rapid decay student-t tail lead alternative conditions lead better accuracy-speed trade-off. dual-tree algorithms. whilst barnes-hut algorithm considers point-cell interactions speed-ups obtained computing cell-cell interactions. done using dual-tree algorithm simultaneously traverses quadtree twice every pair nodes decides whether interaction corresponding cells used summary interactions points inside cells. perhaps surprisingly figure computation time required embed mnist digits using barnes-hutsne -nearest neighbor errors corresponding embeddings function trade-off parameter approach perform barnes-hut algorithm preliminary experiments. computational advantages dual-tree algorithm evaporate computing interaction cells still needs determine points interaction applies. done searching cell storing list children node tree construction. approaches computationally costly. results experiments dual-tree algorithms presented appendix. performed experiments four large data sets evaluate performance barnes-hut-sne. code algorithm available http//homepage.tudelft.nl/j/tsne. data sets. performed experiments four data sets mnist data cifar- data iii) norb data timit data set. mnist data contains grayscale handwritten digit images size pixels corresponds classes. cifar- data annotated subset million tiny images data contains images size pixels leading -dimensional input objects; image corresponds classes. norb data contains grayscale images toys different classes rendered uniform background lighting conditions elevations azimuths images contain pixels. timit data contains speech data mfcc delta delta-delta features extracted leading -dimensional features frame data phone labels. used timit training frames experiments. experimental setup. experiments follow experimental setup closely possible. particular initialize embedding points sampling gaussian variance gradient-descent optimizer iterations setting initial step size step size updated optimization scheme additional momentum term weight ﬁrst iterations afterwards. perplexity ﬁxed following data sets dimensionality larger preprocessed using reduce dimensionality ﬁrst learning iterations multiplied pij-values user-deﬁned constant explained trick enables t-sne better global structure early stages optimization. preliminary experiments found trick becomes increasingly important obtain good embeddings data size increases becomes harder optimization figure compution time required embed mnist digits -nearest neighbor errors corresponding embeddings function data size standard t-sne barnes-hut-sne. note required computation time shown y-axis left ﬁgure plotted logarithmic scale. present results three sets experiments. ﬁrst experiment investigate effect trade-off parameter speed quality embeddings produced barnes-hutsne mnist data set. second experiment investigate computation time required barnes-hut-sne function number data objects third experiment construct visualize embeddings four data sets. results. figure presents results experiment varied speed-accuracy trade-off parameter used learn embedding. ﬁgure shows computation time required construct embeddings mnist digit images well -nearest neighbor error corresponding embeddings. results presented ﬁgure show trade-off parameter increased value approximately without negatively affecting quality embedding. time increasing value leads substantial improvements terms amount computation required time required embed mnist digits reduced seconds experiment standard t-sne would take days complete full mnist data set.) figure compare standard t-sne barnes-hut-sne terms computation time required embedding mnist digit images function data size -nearest neighbor errors corresponding embeddings. experiments ﬁxed parameter trades speed accuracy results presented ﬁgure show barnes-hut-sne orders magnitude faster standard t-sne whilst difference quality constructed embeddings negligible. prominently computational advantages barnes-hut-sne rapidly increase number objects data increases. figure presents embeddings four data sets constructed using barnes-hut-sne. colors points indicate classes corresponding objects; titles plots indicate computation time used construct corresponding embeddings. before ﬁxed four experiments. results ﬁgure shows barnes-hut-sne construct highquality embeddings e.g. mnist handwritten digit images minutes. visually structure embeddings similar.) results also show barnesfigure barnes-hut-sne visualizations four data sets mnist handwritten digits cifar- tiny images norb object images timit speech frames colors point indicate classes corresponding objects. titles ﬁgures indicate computation time used construct corresponding embeddings. figure best viewed color. version mnist embedding original digit images shown presented figure results show that like standard t-sne barnes-hut-sne good preserving local structure data embedding instance visualization clearly shows orientation main sources variation within cluster ones. presented t-sne algorithm called barnes-hut-sne constructs sparse approximation similarities input objects using vantage-point trees approximates t-sne gradient using variant barnes-hut algorithm. algorithm runs rather requires memory. experimental evaluation barnes-hut-sne shows substantially faster standard t-sne facilitates visualization data sets millions data objects scatter plots. drawback barnes-hut-sne provide error bounds indeed exist alternative algorithms provide error bounds explore alternatives future work whether used bound error made t-sne gradient computations bound error ﬁnal embedding. another limitation barneshut-sne used embed data three dimensions. generalizations higher dimensions infeasible size tree grows exponentially dimensionality embedding space. said that limitation severe since t-sne mainly used visualization moreover relatively straightforward replace quadtree metric trees scale better high-dimensional spaces. future work plan scale algorithm developing parallelized implementations data sets large fully stored memory. also investigate effect varying value optimization. addition plan explore extent adapted versions algorithm used speed techniques relational embedding author supported eu-fp social signal processing netherlands institue advanced study author thanks geoffrey hinton many helpful discussions anonymous reviewers helpful comments.", "year": 2013}