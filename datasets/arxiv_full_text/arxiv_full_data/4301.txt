{"title": "Hybrid Linear Modeling via Local Best-fit Flats", "tag": ["cs.CV", "stat.ML"], "abstract": "We present a simple and fast geometric method for modeling data by a union of affine subspaces. The method begins by forming a collection of local best-fit affine subspaces, i.e., subspaces approximating the data in local neighborhoods. The correct sizes of the local neighborhoods are determined automatically by the Jones' $\\beta_2$ numbers (we prove under certain geometric conditions that our method finds the optimal local neighborhoods). The collection of subspaces is further processed by a greedy selection procedure or a spectral method to generate the final model. We discuss applications to tracking-based motion segmentation and clustering of faces under different illuminating conditions. We give extensive experimental evidence demonstrating the state of the art accuracy and speed of the suggested algorithms on these problems and also on synthetic hybrid linear data as well as the MNIST handwritten digits data; and we demonstrate how to use our algorithms for fast determination of the number of affine subspaces.", "text": "abstract present simple fast geometric method modeling data union afﬁne subspaces. method begins forming collection local best-ﬁt afﬁne subspaces i.e. subspaces approximating data local neighborhoods. correct sizes local neighborhoods determined automatically jones’ numbers collection subspaces processed greedy selection procedure spectral method generate ﬁnal model. discuss applications tracking-based motion segmentation clustering faces different illuminating conditions. give extensive experimental evidence demonstrating state accuracy speed suggested algorithms problems also synthetic hybrid linear data well mnist handwritten digits data; demonstrate work supported grants dms-- dms- dms-- dms--. thanks action editor reviewers careful reading comments; peter jones mauro maggioni amit singer discussions motivated exploration multiscale svd-based algorithm; ehsan elhamifar ren´e vidal answering various questions regarding code providing initial version code available public; allen yang clarifying estimation number clusters gpca; stimulating multi-manifold modeling workshop. teng zhang wang gilad lerman school mathematics university minnsota e-mail {zhang wangx lerman}umn.edu arthur szlam courant institute mathematical sciences york university e-mail aszlamcourant.nyu.edu several problems computer vision motion segmentation face clustering give rise modeling data multiple subspaces. referred hybrid linear modeling alternatively subspace clustering. tracking-based motion segmentation extracted feature points clustered according different moving objects. afﬁne camera model vectors coordinates feature points corresponding moving rigid object afﬁne subspace dimension thus clustering different moving objects equivalent clustering different afﬁne subspaces. similarly face clustering proved images lambertian object variety lighting conditions form convex polyhedral cone image space cone accurately approximated low-dimensional linear subspace thus cluster certain images faces algorithms. mathematical formulation assumes data {xi}n lies ﬂats requires partition corresponding ﬂats. would like able data corrupted additive noise outliers; case also want throughout paper outliers corrupted data points i.e. points generated distribution assigns sufﬁciently small probability small neighborhoods around underlying subspaces. different corrupting selected entries data points. several algorithms suggested solving problem example k-ﬂats algorithm variants methods based direct matrix factorization generalized principal component analysis local subspace afﬁnity ransac locally linear manifold clustering agglomerative lossy compression spectral curvature clustering sparse subspace clustering theoretical guarantees particular algorithms appear recommend recent review vidal many algorithms described require initial guess subspaces. example k-ﬂats algorithm iterative method requires initialization needs carefully choose collections data points close underlying dﬂats. algorithms require information suspected deviations hybrid linear model; example ransac model parameter corresponding level noise. propose straightforward geometric method estimation local subspaces inspired local subspace estimates used model parameters initialize algorithm. basic idea data sampled hybrid linear model many points principal components appropriately sized neighborhood give good approximation subspace belongs using local subspaces infer global hybrid linear model suggested linear subspaces; however small neighborhoods adaptive structure data appropriately sized neighborhood needs larger noise subspace recognized. however neighborhood cannot large contains points multiple subspaces. correct choice size carefully quantiﬁed section addition studying estimate local subspaces describe complete algorithms natural extensions local estimation slbf many data sets ﬁrst obtains state speed nearly state accuracy second obtains state accuracy reasonable times remark test accuracy various scenarios particular intersecting subspaces outliers. work theoretically justify choice initializer hopeful developing complete theory justifying algorithms. particular believe theory valid setting suggested soltanolkotabi cand`es analyzing algorithm additional noise restricting fraction outliers also interested rigorously quantifying limitations algorithms make precise local best-ﬁt heuristic using numbers give algorithm approximately optimal neighborhoods sense fact prove certain geometric conditions. using local best-ﬁt heuristic introduce slbf algorithms hlm. point randomly chosen subset data best-ﬁt ﬂats optimal neighborhoods build global model different methods perform extensive experiments motion segmentation data face clustering handwritten digits artiﬁcial data showing algorithms particular slbf accurate real synthetic problems runs extremely fast cropped face data actually indicate fundamental problem local methods like slbf though suggest workaround works particular data. demonstrate local best-ﬁt heuristic used algorithms. particular give experimental evidence show k-ﬂats algorithm improved initialization based local best-ﬁt heuristic. also heuristic estimate main parameters ransac show combination elbow method quickly determine number subspaces. rest paper organized follows. section describe slbf algorithms state theorem giving conditions guarantee good neighborhoods found. section carefully tests slbf algorithms artiﬁcial data synthetic hybrid linear models real data motion segmentation video sequences face clustering handwritten digits recognition. also demonstrates determine number clusters applying fast algorithm paper together straightforward elbow method. section concludes brief discussion mentions possibilities future work. describe methods slbf heart estimation local ﬂats capturing global structures data methods ﬁrst candidate ﬂats best-ﬁt ﬂats local optimal neighborhoods algorithms process candidates different ways uses energy minimization slbf uses spectral approach. choose candidate ﬂats capture global structure data ﬁtting ‘optimal’ local neighborhoods data points. point deﬁne optimal neighborhood largest ball contains points sampled cluster indeed neighborhoods smaller optimal mainly contain noise around underlying subspace consequently local best-ﬁt ﬂats match underlying ﬂat. hand larger neighborhood optimal contain points underlying resulting best-ﬁt match underlying ﬂats. note choice neighborhood equivalent choice radius refer scale -log scale). possible take guess optimal scale parameter found possible choose optimal scale reasonably well automatically adapting given point start smallest scale look larger larger neighborhoods given point smallest scale noise cause local neighborhood higher dimension points neighborhood becomes better better approximated scale-invariant sense points belonging ﬂats enter neighborhood. denotes number points denotes projection onto minimization d-ﬂats note numerator approximation error best-ﬁt scale denominator scale notion scale-invariant error introduced utilized using scale-invariant error reformulate criterion choosing optimal neighborhood precisely. start smallest neighborhood containing nearest neighbors increase number nearest neighbors iteration check number neighborhood using estimate optimal neighborhood last smaller previous neighborhood procedure summarized algorithm experimentally robust outliers inlier since nearest neighbors inliers also tend inliers. following theorem tries justify strategy estimating optimal scale around point showing continuous setting ﬁrst local minimizer approximately distance nearest cluster contain therefore choose size neighborhood following algorithm simplest choice squared distances point nearest i.e. power however scenarios energy robust outliers mean squared error theoretical support experimental support). method also allows using energy functions hard minimize indeed requires evaluating energy candidate conﬁgurations. example data requires stronger robustness outliers following energy algorithm closely related ransac since candidate subspaces data set. however algorithm gives advantage choosing good candidates ransac d-ﬂat arbitrarily chosen points. setting) approximately obtain optimal neighborhood. rather standard extend estimates measures probabilistic setting i.i.d. data sampled continuous distribution. theorem hold high probability sufﬁciently large sample size proof theorem appendix. algorithm energy minimization randomized local best-ﬁt ﬂats input data dimension subspaces number candidate planes number output ﬂats/clusters number passes parameters local scale calculation. slbf algorithm processes candidate subspaces spectral clustering method. ﬁrst ﬁnds neighborhoods {ni}n points algorithm d-ﬂats {li}n {xi}n neighborhoods. forms matrices follows step algorithm need algorithm times thus complexity order note comes full sort distances restrict ﬁxed number scales replaced constant. step algorithm requires decompositions matrices size order obtain ﬁrst vectors thus also complexity step algorithm requires evaluation matrix representing distances ||xi xi|| costs operations since distance point subspace costs moreover passes complexity order therefore step algorithm complexity order last step algorithm complexity order comes construction matrix distances points subspaces. combining complexities together overall complexity algorithm; before number scales independently terms replaced constant. finally applies spectral clustering matrix precisely slbf follows main algorithm replacing matrix multiplying unit eigenvectors step corresponding square roots eigenvalues skipping step remark last changes commonly used similarity matrix considered gram matrix e.g. euclidean isomap discussed slbf spectral clusteringbased method similar ssc. algorithms construct afﬁnity matrix whose ij-th entry represents similarity points apply spectral clustering using afﬁnity matrix. ideally afﬁnities points cluster order afﬁnities points different clusters order indeed afﬁnity slbf cluster expect close close means close thus close otherwise cluster expect sufﬁciently sufﬁciently implies ˆsij close choice clearly affects heuristic argument size ˆsij theoretically larger noise close cluster tween subspaces) restricts applicability lsa. also similarity idea slbf indeed view ﬁtting candidate subspaces based data points however practice operate differently particular based local information algorithm also spectral method similar afﬁnities global step algorithm complexity order since applies algorithm every point expensive calculation steps algorithm construction requires complexity order eigenvalue decomposition step complexity order k-means algorithm step complexity order iterations k-means. note ﬁrst minimum theorem excludes left endpoint thus excluded algorithm experiments noticed data without much noise useful allow ﬁrst scale count local minimum allow algorithm refer implementation slbf techniques tailored motion segmentation data lbfms slbf-ms. section conduct experiments artiﬁcial real data sets verify effectiveness proposed algorithm comparison algorithms. many situations methods propose fast accurate; however section show failure mode method discuss corrected. cannot large ˆsij close cluster. therefore plxk/|nj| estimated noise data around point parameter. following strategy choose different values consequently obtain several segmentation results choose segmentation smallest error remark slbf robust outliers. indeed magnitude smaller therefore subsequent application k-means suffer points arbitrarily large values. verify magnitude embedded points smaller note diagonal elements smaller since uσut diagonal elements uσut also smaller therefore norm rows also smaller similar slbf also based ﬁtting local subspaces. however subspace local neighborhoods ﬁxed number points adaptive. moreover local subspaces forced linear time chosen parameters balance time accuracy. nevertheless insisted parameters data sets experiments even though particular parameters could obtain even better near perfect results data sets. experiments sections computer intel core .ghz memory experiments sections machine intel core quad .ghz memory. algorithm initialized randomly picking tuples initialized random guess. since algorithms like tend converge local minimum restarts moppca restarts recorded misclassiﬁcation rate smallest error algorithms. number restarts restricted running time accuracy. algorithm value suggested code. ransac algorithms depend user supplied inlier threshold. ransac oracle inlier bound given true noise variance model thus clearly advantage algorithms listed. ransac estimate inlier threshold local best-ﬁt ﬂats heuristic paper. best-ﬁt neighborhoods points using latter heuristic estimate least error approximation d-ﬂats neighborhoods. inlier bound average errors. number clusters resulting larger choose largest clusters identify points rest clusters outliers. cases ransac algorithm breaks report n/a. reason ransac sensitive estimate overestimate result removal points belonging subspace algorithm exhaust points detecting subspaces. remark gpca cannot naturally deal outliers therefore robust gpca multivariate trimming parameters ‘angletolerance’ ‘boundarythreshold’ respectively. compare algorithms following algorithms mixtures ppca k-ﬂats local subspace analysis spectral curvature clustering random sample consensus agglomerative lossy compression gpca voting/robust gpca throughrest paper matlab codes gpca moppca algorithms http//perception.csl.uiuc.edu/gpca algorithm http//www. vision.jhu.edu/db algorithm http//www.math.umn.edu/∼lerman/scc algorithm http// perception.csl.uiuc.edu/coding/motion/ ransac algorithm http//www.vision.jhu.edu/code/ algorithm http//www.cis.jhu.edu/∼ehsan/ssc.htm. algorithm also slightly modiﬁed version tailored motion segmentation step algorithm refer scc-ms following notation matrix matrix whose columns left singular vectors also denote diagonal matrix whose elements left singular values a∗c. k-means step scc-ms applied directly rows matrix scc). artiﬁcial data represents various instances linear subspaces dimensions ﬁxed equal follow refer setting mixed follow refer setting fixing randomly generate different instances corresponding hybrid linear models according code http//perception.csl.uiuc.edu/gpca. precisely experiments linear subspaces corresponding dimensions randomly generated. random variables sampled within subspace sums variables. sampled uniform distribution d-dimensional ball radius subspace sampled d-dimensional multivariate normal distribution mean covariance matrix id×d. then subspace samples generated according distribution described. next data corrupted uniformly distributed outliers cube sideransac ssc) support mixed dimensions natively assume subspace maximum dimension experiment. gpca support mixed dimensions natively gpca algorithm specify dimensions subspace mixed-dimension case various artiﬁcial instances hybrid linear modeling advantage especially obvious many outliers afﬁne subspaces. robustness outliers result loss function random sampling. slbf slbf-ms better afﬁne cases spectral clustering. also unlike many methods proposed methods natively support afﬁne subspace models results ransac show local best-ﬁt heuristic effectively used estimate main parameter ransac i.e. estimate local noise. table formally given video sequence denote number frames sequence either independently moving objects background also move motion camera. number moving objects plus background sequence also feature points detected objects background. coordinates feature point image frame every trajectory feature point across frames. actual task motion segmentation separate trajectory vectors clusters representing underlying motions. shown afﬁne camera model trajectory vectors corresponding different moving objects background across image frames live distinct afﬁne subspaces dimension three following theory implement algorithm compare algorithm following ones improved gpca motion segmentation kﬂats local linear manifold clustering local subspace analysis multi stage learning spectral curvature clustering scc-ms sparse subspace clustering ransac also shows running time lbf/lbf-ms less running time algorithms especially gpca ransac ssc. difference large enough also proposed algorithm initialization others. lbf-ms algorithms slower single k-ﬂats usually takes many restarts k-ﬂats decent result. notice choice algorithm function similar manner number restarts slbf slbf-ms cost time large construction matrix spectral clustering still comparable speed faster spectral-clustering based algorithms. test proposed algorithms hopkins database motion segmentation available http//www.vision.jhu.edu/data/hopkins. data contains video sequences along coordinates certain features extracted tracked sequence frames. main task cluster feature vectors according different moving objects background video. consists gpca llmc ransac copy results http//www.vision.jhu.edu/data/hopkins perform experiments scc-ms ssc-n lbf-ms slbf slbf-ms perform experiments record mean misclassiﬁcation rate median misclassiﬁcation rate algorithm ﬁxed different type motions experiment repeated times. average misclassiﬁcation rates standard deviation running time recorded tables demonstrated figure misclassiﬁcation rates different misclassiﬁcation rates different explained possible evolutions codes since remark though misclassiﬁcation rates scc-ms even slightly better misclassiﬁcation rates table figure algorithms work well hopkins database. methods tested slbf-ms ssc-n accurate algorithms. besides slbf/slbf-ms ssc-n sccms better lbf-ms. however table lbfms times faster ssc-n slbfms also times faster ssc. many cases energy lower labels obtained true labels. thus suspect reason slbf/slbf-ms works better lbf/lbf-ms good clustering hopkins data requires additional type information combined subspace clustering table slbf-ms slbf ssc-n negligible randomness. indeed randomness come k-means step randomness effectively reduced restarting strategy. lbf-ms random still comparable standard deviations good algorithms hopkins database scc/scc-ms. test lbf-ms slbf slbf-ms compare k-ﬂats extended yale face database available http//vision.ucsd.edu/ leekc/extyaledatabase/extyaleb.html. data shows failure mode algorithms; show engineer workaround. subsets extended yale face database consisting face images persons under varying lighting conditions. objective cluster images according persons. implementation ﬁxed repeat algorithm randomly chosen subsets persons. model applicable database images face variable lighting lies three-dimensional linear subspace shadow considered nine-dimensional subspace shadow considered experiments found images person database roughly -dimensional subspace therefore ﬁrst reduce dimension data include gpca algorithm since slow work well database. also include ransac since code provided returns errors examples. setting follows exactly chooses values range ﬁrst table poor discriminating linear clusters data set. failure occurs combination factors ﬁrst relatively sparse sampling data points -dimensional cluster second relative nearness underlying subspaces other. particular almost neighborhood given point points afﬁne clusters consequently optimal scale. example face images persons ﬁfth points closer subspace spanned ﬁrst principal components points cluster second nearest neighbors thirds points closer subspace nearest neighbors. sense single -dimensional rather -dimensional sets. example average distance point -dimensional best subspace points cluster average distance -dimensional best subspace whole data classical method determining number clusters elbow past adding clusters signiﬁcantly decrease error. search elbow ﬁnding maximum second order difference logarithm following sections compare i.e. applying part gpca number artiﬁcial data sets real data sets. experiments machine intel core quad .ghz memory. test slbf artiﬁcial data compare methods artiﬁcial data sets generated matlab code borrowed gpca package http//perception.csl. uiuc.edu/gpca. subspace initial data points uniformly sampled unit cube subspace centered around origin corrupted gaussian noise standard deviation last four experiments restrict angle subspaces least separation. dimension given kmax sod. different values choose estimated majority. choose average noise neighborhood using local best-ﬁt heuristic distortion rate input true noise level distortion rate. gpca original idea number clusters project data onto -dimensional subspace tolerance rank detection whereas average norm point data thus loses little terms relative ﬁtting error considering spanned single subspace. however points actually closer subspace spanned face subspace spanned face little global method still able discriminate afﬁne clusters. problem data large variance directions irrelevant classiﬁcation task unusual. standard method dealing situation whiten data; i.e. reduce value large singular values. crude whitening obtained simply removing ﬁrst principal components. exclude ﬁrst principal components reducing dimension lbf/slbf algorithms table results greatly improved become competitve. sophisticated whitening results improved. finally work mnist data data consists several thousand images digits work subsets data contain digits choose images digit random. apply reduce dimension gpca rest algorithms. choice provide richer testing opportunities however unavailable gpca cannot handle often stuck process data section experiment times using correct number clusters record misclassiﬁcation rates standard deviation running time tables table slbf slbf-ms best algorithms among methods terms misclassiﬁcation rates although misclassiﬁcation rates larger scc-ms lbf-ms also good algorithms data set. almost good slbf slbf-ms fails lbf-ms k-ﬂats fastest algorithms mnist data set. algorithm independent parts gpca algorithm thus extremely fast perform high ambient dimensions. even tried ideas applying several algorithms nevertheless work well thus report them. experiment repeated times repeated times table work best dimensions real problems choice usually unknown. local best-ﬁt heuristic provides good estimation distortion rate helps reduce running time. good artiﬁcial data. options suffer computause extended yale face database section testing algorithms detecting number clusters. ambient dimension reduced methods intrinsic dimension subspaces different clustering algorithms kmax respectively clusters. gpca tolerance affect performance experiment. experiment repeated times repeated times speed). following section apply lbfms slbf slbf-ms whitening. error rates ﬁnding correct number clusters computation time recorded table methods including gpca given intrinsic subspace dimension. different clustering algorithms kmax respectively clusters. gpca tolerance affect performance experiment. experiment repeated times repeated times speed). error rates ﬁnding correct number clusters computation time recorded tables methods determining number clusters becomes difﬁcult real larger real table project data -dimensional space gpca fail cases except digits outperform others although efﬁcient. demonstrate choice neighborhoods algorithm used robust initialization k-ﬂats. work geometric farthest insertion. ﬁxed neighborhood sizes neighbors goes follows pick random point best-ﬁt point neighborhood point data farthest best-ﬁt neighborhood choose point farthest continue. stop ﬂats; initialization k-ﬂats. also believe possible provide theoretical framework performance guarantees noise slbf. speciﬁcally hope prove quantitative form following alternative suppose data lies union d-dimensional afﬁne sets perhaps additive noise outliers. either large fraction points optimal neighborhoods contained afﬁne clusters principal components neighborhoods good approximations clusters; slbf recover good approximations afﬁne clusters data looks locally lower d-dimensional even though cluster globally d-dimensional high curvature; case pure optimal neighborhoods local estimation accurately represent afﬁne clusters. fig. color neighborhood size obtained local bestﬁt heuristic. color value represents number neighbors chosen point. note algorithm chooses smaller neighborhoods points closer intersection planes. plane plane. data designed favor small neighborhoods. next data three random ﬂats gaussian noise outliers generated using matlab code gpca section data designed favor large neighborhood choices. finally work data points sampled planes figure error rates k-ﬂats farthest insertion initialization ﬁxed neighborhoods size plotted error rates farthest insertion adapted neighborhoods averaged runs figure although method always beat best ﬁxed neighborhood quite close; always signiﬁcantly better wrong ﬁxed neighborhood size. methods signiﬁcantly better random initialization. presented simple geometric method based selecting local best-ﬁt ﬂats. size local neighborhoods determined automatically using numbers; proven certain geometric conditions method approximately ﬁnds optimal local neighborhoods. give extensive experimental evidence demonstrating state accuracy speed algorithm synthetic real hybrid linear data. believe promising next step adapt method multi-manifold clustering. method quite good unions ﬂats cannot successfully handle unions curved manifolds. expect gluing together groups local best-ﬁt ﬂats related smoothfig. using neighborhood choice improve initialization k-ﬂats ﬁrst visualization three data sets seconds shows corresponding ﬁgures vertical axis accuracy horizontal axis ﬁxed neighborhood size geometric farthest insertion initialization ﬂats. line result using adapted neighborhoods. data sets described section random initialization leads misclassiﬁcation rates greater three data sets. claim minimizer second expression satisﬁes denote orthonormal vector passes orthonormal vectors span orthonormal vectors span prove eigenvector second eigenremark function often local minimum exactly demonstrate particular case evident rather typical. assume sufﬁciently small r)). following argument decreasing interval", "year": 2010}