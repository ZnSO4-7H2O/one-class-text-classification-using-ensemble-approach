{"title": "Learning to Win by Reading Manuals in a Monte-Carlo Framework", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Domain knowledge is crucial for effective performance in autonomous control systems. Typically, human effort is required to encode this knowledge into a control algorithm. In this paper, we present an approach to language grounding which automatically interprets text in the context of a complex control application, such as a game, and uses domain knowledge extracted from the text to improve control performance. Both text analysis and control strategies are learned jointly using only a feedback signal inherent to the application. To effectively leverage textual information, our method automatically extracts the text segment most relevant to the current game state, and labels it with a task-centric predicate structure. This labeled text is then used to bias an action selection policy for the game, guiding it towards promising regions of the action space. We encode our model for text analysis and game playing in a multi-layer neural network, representing linguistic decisions via latent variables in the hidden layers, and game action quality via the output layer. Operating within the Monte-Carlo Search framework, we estimate model parameters using feedback from simulated games. We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide. Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 34% absolute improvement and winning over 65% of games when playing against the built-in AI of Civilization.", "text": "domain knowledge crucial eﬀective performance autonomous control systems. typically human eﬀort required encode knowledge control algorithm. paper present approach language grounding automatically interprets text context complex control application game uses domain knowledge extracted text improve control performance. text analysis control strategies learned jointly using feedback signal inherent application. eﬀectively leverage textual information method automatically extracts text segment relevant current game state labels task-centric predicate structure. labeled text used bias action selection policy game guiding towards promising regions action space. encode model text analysis game playing multi-layer neural network representing linguistic decisions latent variables hidden layers game action quality output layer. operating within monte-carlo search framework estimate model parameters using feedback simulated games. apply approach complex strategy game civilization using oﬃcial game manual text guide. results show linguistically-informed game-playing agent signiﬁcantly outperforms language-unaware counterpart yielding absolute improvement winning games playing built-in civilization. paper study task grounding document content control applications computer games. applications agent attempts optimize utility function learning select situation-appropriate actions. complex domains ﬁnding winning strategy challenging even humans. therefore human players typically rely manuals guides describe promising tactics provide general advice underlying task. surprisingly textual information never utilized control algorithms despite potential greatly improve performance. goal therefore develop methods achieve automatic fashion. consider instance text shown figure excerpt user manual game civilization text describes game locations action build-city eﬀectively applied. stochastic player access text would gain knowledge hard would repeatedly attempt action myriad states thereby learning characterization promising state-action pairs based observed game outcomes. games large state spaces long planning horizons high-branching factors approach prohibitively slow ineﬀective. algorithm access text however could learn correlations words text game attributes e.g. word river places rivers game thus leveraging strategies described text select better actions. guides provide wealth information eﬀective control strategies including situation-speciﬁc advice well general background knowledge. beneﬁt information algorithm learn mapping text guide states actions control application. mapping allows algorithm state-speciﬁc advice matching state attributes verbal descriptions. furthermore relevant sentence found mapping biases algorithm select action proposed guide document. mapping modeled word-level ideally would also information encoded structure sentence predicate argument structure. instance algorithm explicitly identify predicates state attribute descriptions directly structures inherent control application. well-known methods information extraction prior work primarily focused supervised methods. setup text analysis state dependent therefore annotations need representative entire state space. given enormous state space continually changes game progresses collecting annotations impractical. instead propose learn text analysis based feedback signal inherent control application e.g. game score. feedback computed automatically step game thereby allowing algorithm continuously adapt local observed game context. plication text guides provide complete step-by-step advice situations player encounter. even advice available learned mapping noisy resulting suboptimal choices. therefore need design method achieve eﬀective control absence textual advice robustly integrating automatically extracted information available. address challenge incorporating language analysis monte-carlo search state-of-the-art framework playing complex games. traditionally framework operates state action features. extending monte-carlo search include textual features integrate sources information principled fashion. address challenges uniﬁed framework based markov decision processes formulation commonly used game playing algorithms. setup consists game stochastic environment goal player maximize given utility function state player’s behavior determined action-value function assesses goodness action state based attributes incorporate linguistic information formulation expand action value function include linguistic features. state action features known point computation relevant words semantic roles observed. therefore model text relevance hidden variable. similarly hidden variables discriminate words describe actions describe state attributes rest sentence. incorporate hidden variables action-value function model non-linear function approximation using multi-layer neural network. despite added complexity parameters non-linear model eﬀectively learned monte-carlo search framework. monte-carlo search actionvalue function estimated playing multiple simulated games starting current game state. observed reward simulations update parameters neural network backpropagation. focuses learning current game state allowing method learn language analysis game-play appropriate observed game context. test method strategy game civilization notoriously challenging game immense action space. source knowledge guiding model oﬃcial game manual. baseline employ similar monte-carlo search based player access textual information. demonstrate linguisticallyinformed player signiﬁcantly outperforms baseline terms number games won. moreover show modeling deeper linguistic structure sentences further improves performance. full-length games algorithm yields improvement language unaware baseline wins games built-in hand-crafted civilization video method playing game available http//groups.csail.mit.edu/rbg/code/civ/video. code data work along complete experimental setup preconﬁgured environment virtual machine available http//groups.csail.mit.edu/rbg/code/civ. section provide intuition beneﬁts integrating textual information learning algorithms control. section describes prior work language grounding emphasizing unique challenges opportunities setup. section also positions work large body research monte-carlo based players. section presents background monte-carlo search applied game playing. section present multi-layer neural network formulation action-value function combines information text control application. next present monte-carlo method estimating parameters non-linear function. sections focus application algorithm game civilization section compare method range competitive game-playing baselines empirically analyze properties algorithm. finally section discuss implications research conclude. section provide intuitive explanation textual information help improve action selection complex game. clarity ﬁrst discuss beneﬁts textual information supervised scenario thereby decoupling questions concerning modeling representation related parameter estimation. assume every state represented features given state goal select best possible action ﬁxed model task multiclass classiﬁcation choice represented feature vector tween learn classiﬁer eﬀectively need training suﬃciently covers possible combinations state features actions. however domains complex state spaces large number possible actions many instances state-action feature values unobserved training. show generalization power classiﬁer improved using textual information. assume training example addition state-action pair contains sentence describe action taken given state attributes. intuitively want enrich basic classiﬁer features capture correspondence states actions words describe them. given sentence composed word types features form every assuming action described using similar words throughout guide expect text-enriched classiﬁer would able learn correspondence features similar intuition holds learning correspondence state-attributes descriptions represented features features classiﬁer connect state action based evidence provided guiding sentence occurrences contexts throughout training data. text-free classiﬁer support association action appear similar state context training set. beneﬁts textual information extend models trained using control feedback rather supervised data. training scenario algorithm assesses goodness given state-action combination simulating limited number game turns action taken observing control feedback provided underlying application. algorithm built-in mechanism employs observed feedback learn feature weights intelligently samples space search promising state-action pairs. algorithm access collection sentences similar feedback-based mechanism used sentences match given stateaction pair stateaction-description features algorithm jointly learns identify relevant sentences actions states descriptions. note used classiﬁcation basis discussion section reality methods learn regression function. section ﬁrst discuss prior work ﬁeld grounded language acquisition. subsequently look areas speciﬁc application domain i.e. natural language analysis context games monte-carlo search applied game playing. work broad area research grounded language acquisition goal learn linguistic analysis non-linguistic situated context appeal formulation lies reducing need manual annotations non-linguistic signals provide powerful albeit noisy source supervision learning. traditional grounding setup assumed non-linguistic signals parallel content input text motivating machine translation view grounding task. alternative approach models grounding control framework learner actively acquires feedback non-linguistic environment uses drive language interpretation. summarize approaches emphasizing similarity diﬀerences work. many applications linguistic content tightly linked perceptual observations providing rich source information learning language grounding. examples parallel data include images captions robocup game events paired text commentary sequences robot motor actions described natural language large diversity properties parallel data resulted development algorithms tailored speciﬁc grounding contexts instead application-independent grounding approach. nevertheless existing grounding approaches characterized along several dimensions illuminate connection algorithms perceptual data discretize non-linguistic signal representation facilitates alignment. instance barnard forsyth segment images regions subsequently mapped words. approaches intertwine alignment segmentation single step tasks clearly interrelated. application segmentation required state-action representation nature discrete. many approaches move beyond discretization aiming induce rich hierarchical structures non-linguistic input instance fleischman parse action sequences using context-free grammar subsequently mapped semantic frames. chen mooney represent action sequences using ﬁrst order logic. contrast algorithm capitalizes structure readily available data state-action transitions. inducing richer structure state-action space beneﬁt mapping diﬃcult problem right ﬁeld hierarchical planning of-words approach represent input documents recent methods relied richer representation linguistic data syntactic trees semantic templates method incorporates linguistic information multiple levels using feature-based representation encodes words well syntactic information extracted dependency trees. shown results richer linguistic representations signiﬁcantly improve model performance. procedure crucially depends well words aligned non-linguistic structures. reason models assume alignment provided part training data grounding algorithms alignment induced part training procedure. examples approaches methods barnard forsyth liang models jointly generate text attributes grounding context treating alignment unobserved variable. recent work moved away reliance parallel corpora using control feedback primary source supervision. assumption behind setup textual information used drive control application application’s performance correlate quality language analysis. also assumed performance measurement obtained automatically. setup conducive reinforcement learning approaches estimate model parameters feedback signal even noisy delayed. line prior work focused task mapping textual instructions policy control application assuming text fully speciﬁes actions executed environment. example previous work approach applied task translating instructions computer manual executable actions. vogel jurafsky demonstrate grounding framework eﬀectively navigational directions corresponding path map. second line prior work focused full semantic parsing converting given text formal meaning representation ﬁrst order logic methods applied domains correctness output accurately evaluated based control feedback example output database query executed provides clean oracle feedback signal learning. line work also assumes text fully speciﬁes required output. method also driven control feedback language interpretation task fundamentally diﬀerent. assume given text document provides highlevel advice without directly describing correct actions every potential game state. furthermore textual advice necessarily translate single strategy fact text describe several strategies contingent speciﬁc game states. reason strategy text cannot simply interpreted directly policy. therefore goal bias learned policy using information extracted text. achieve complete semantic interpretation rather partial text analysis compute features relevant control application. eisenstein automatically extract information collection documents help identify rules game. information represented predicate logic formulae estimated unsupervised fashion generative model. extracted formulae along observed traces game play subsequently inductive logic program attempts reconstruct rules game. high-level goal similar i.e. extract information text useful external task several diﬀerences. firstly eisenstein analyze text game disjoint steps model tasks integrated fashion. allows model learn text analysis pertinent game play time using text guide game play. secondly method learns text analysis game play feedback signal inherent game avoiding need pre-compiled game traces. enables method operate eﬀectively complex games collecting suﬃciently representative game traces impractical. gorniak develop machine controlled game character responds spoken natural language commands. given traces game actions manually annotated transcribed speech method learns structured representation text aligned action sequences. learned model used interpret spoken instructions grounding actions human player current game state. method learn play game enables human control additional game character speech. contrast gorniak develop algorithms fully autonomously control actions player game. furthermore method operates game’s user manual rather human provided contextually relevant instructions. requires model identify text contains information useful current game state addition mapping text productive actions. finally method learns game feedback collected active interaction without relying manual annotations. allows eﬀectively operate complex games collecting traditional labeled traces would prohibitively expensive. monte-carlo search state-of-the-art framework successfully applied prior work playing complex games poker scrabble real-time strategy games framework operates playing simulated games estimate goodness value diﬀerent candidate actions. game’s state action spaces complex number simulations needed eﬀective play become prohibitively large. previous application addressed issue using orthogonal techniques leverage domain knowledge either guide prune action selection estimate value untried actions based observed outcomes simulated games. estimate used bias action selection. based algorithm games relies techniques. describe diﬀerences application techniques prior work. domain knowledge shown critically important achieving good performance complex games. prior work achieved manually encoding relevant domain knowledge game playing algorithm example manually speciﬁed heuristics action selection hand crafted features value functions encoding expert knowledge contrast approaches goal automatically extract domain knowledge relevant natural language documents thus bypassing need manual speciﬁcation. method learns text interpretation game action selection based outcomes simulated games mcs. allows identify leverage textual domain knowledge relevant observed game context. figure markov decision process. actions selected according policy function given current state execution selected action causes transition state according stochastic state transition distribution previous approaches estimating value untried actions relied techniques. ﬁrst upper conﬁdence bounds tree heuristic used concert monte-carlo tree search variant mcs. augments action’s value exploration bonus rarely visited state-action pairs resulting better action selection better overall game performance second technique learn linear function approximation action values current state based game feedback even though method follows latter approach model action-value non-linear function approximation. given complexity application domain non-linear approximation generalizes better linear shown results signiﬁcantly improves performance. importantly non-linear model enables method represent text analysis latent variables allowing textual information estimate value untried actions. task leverage textual information help turn-based strategy game given opponent. section ﬁrst describe monte-carlo search framework within method operates. details linguistically informed monte-carlo search algorithm given section aspects representation game i.e. deﬁned implicitly game rules. step game game-playing agent observe current game state select best possible action agent executes action game state changes according state transition distribution invoking game code black-box simulator i.e. playing game. action agent receives reward according reward function game playing setup value reward indication chances winning game state crucially reward signal delayed i.e. non-zero value game ending states loss tie. game playing agent selects actions according stochastic policy speciﬁes probability selecting action state expected total reward executing action state following policy termed action-value function total reward i.e. maximizes chances winning game. optimal action-value function known optimal game-playing behavior would select action highest computationally hard optimal policy many well studied algorithms available estimating eﬀective monte-carlo search algorithm shown figure simulation-based search paradigm dynamically estimating action-values given state estimate based rewards observed multiple roll-outs simulated game starting state speciﬁcally roll-out algorithm starts state repeatedly selects executes actions according simulation policy sampling state transitions game completion time ﬁnal reward measured action-value function updated accordingly. monte-carlo control updated action-value general roll-outs game completion. simulations expensive case domain roll-outs truncated ﬁxed number steps. however depends availability approximate reward signal truncation point. experiments built-in score game reward. reward noisy available every stage game. figure overview monte-carlo search algorithm. game state independent simulated games roll-outs done best possible game action roll-out starts state actions selected according simulation policy policy learned roll-outs roll-outs improving policy turn improves roll-out action selection. process repeated every actual game state simulation policy relearned scratch time. function used deﬁne improved simulation policy thereby directing subsequent roll-outs towards higher scoring regions game state space. ﬁxed number roll-outs performed action highest average ﬁnal reward simulations selected played actual game state process repeated state encountered actual game action-value function relearned scratch game state. simulation policy usually selects actions maximize action-value function. however sometimes valid actions also randomly explored case valuable predicted current conceivable sharing action-value function across roll-outs diﬀerent game states would beneﬁcial empirically case experiments. possible reason domain game dynamics change radically many points game e.g. technology becomes available. change occurs actually detrimental play according action-value function previous game step. note however action-value function indeed shared across roll-outs single game state parameters updated successive roll-outs. learned model helps improve roll-out action selection thereby improves game play. setup relearning scratch game state shown beneﬁcial even stationary environments success monte-carlo search depends ability make fast local estimate action-value function roll-outs collected simulated play. however games large branching factors feasible collect suﬃcient roll-outs especially game simulation computationally expensive. thus crucial learned action-value function generalizes well small number roll-outs i.e. observed states actions rewards. achieve model action-value function linear combination state action attributes note learning action-value function monte-carlo search related reinforcement learning fact approach standard gradient descent updates estimate parameters however crucial diﬀerence techniques general goal applicable state agent observe existence. monte-carlo search framework learn specialized current state essence relearned every observed state actual game using states actions feedback simulations. relearning seem suboptimal distinct advantages ﬁrst since needs model current state representationally much simpler global action-value function. second simpler representation learned fewer observations global actionvalue function properties important state space extremely large case domain. goal work improve performance monte-carlo search framework described above using information automatically extracted text. section describe achieve terms model structure parameter estimation. achieve leveraging textual information improve game-play method needs perform three tasks identify sentences relevant current game state label sentences predicate structure predict good game actions combining game features text features extracted language analysis steps. ﬁrst describe tasks modeled separately showing integrate single coherent model. discussed section small fraction strategy document likely provide guidance relevant current game context. therefore eﬀectively information given document ﬁrst need identify sentence relevant current game state action model decision log-linear distribution deﬁning probability relevant sentence using text guide action selection addition using word-level correspondences would also like leverage information encoded structure sentence. example verbs sentence might likely describe suggested game actions. access information inducing task-centric predicate structure sentences. label words sentence either action-description statedescription background. given sentence precomputed dependency parse model word-by-word labeling decision log-linear fashion i.e. distribution predicate labeling sentence given addition encoding word type part-of-speech also includes dependency parse information word. features allow predicate labeling decision condition syntactic structure sentence. relevant sentence identiﬁed labeled predicate structure algorithm needs information along attributes current game state select best possible game action redeﬁne action-value function weighted linear combination features game text information figure structure neural network model. rectangle represents collection units layer shaded trapezoids show connections layers. ﬁxed real-valued feature function transforms game state action strategy document input vector second layer contains disjoint sets hidden units encodes sentence relevance decisions predicate labeling. softmax layers unit active time. units third layer ﬁxed real valued feature functions active units respectively. text analysis models action-value function described form three primary components text-aware game playing algorithm. construct single principled model components representing diﬀerent layers multi-layer neural network shown figure essentially text analysis decisions modeled latent variables second hidden layer network ﬁnal output layer models action-value function. input layer neural network encodes inputs model i.e. current state candidate action document second layer consists disjoint sets hidden units operates stochastic -of-n softmax selection layer activation function units layer standard softmax function hidden unit weight vector corresponding number units layer. given activation function mathematically equivalent log-linear distribution layers operate like log-linear models. node activation softmax layer simulates sampling log-linear distribution. layer replicate log-linear model sentence relevance equation node representing single sentence. similarly unit layer represents complete predicate labeling sentence equation example kind correlations learned model consider figure here relevant sentence already selected given game state. predicate labeling sentence identiﬁed words irrigate settler describing action take. game roll-outs return higher rewards irrigate action settler unit model learn association action words describe similarly learn association state description words feature values current game state e.g. word city binary feature nearcity. allows method leverage automatically extracted textual information improve game play. learning method performed online fashion game state algorithm performs simulated game roll-out observes outcome simulation updates parameters action-value function shown figure three steps repeated ﬁxed number times actual game state. information roll-outs used select actual game action. algorithm relearns parameters action-value function every game state specializes action-value function subgame starting learning specialized game state common useful games complex state spaces dynamics learning single global function approximation particularly diﬃcult consequence function specialization need online learning since cannot predict games states seen intention incorporate action-value function information relevant sentence. therefore practice perform predicate labeling sentence selected relevance component model. figure example text game attributes resulting candidate action features. left portion game state arrows indicating game attributes. also left sentence relevant game state along action state words identiﬁed predicate labeling. right candidate actions settler unit along corresponding features. mentioned relevant sentence irrigate better actions executing lead future higher game scores. feedback features shown allow model learn eﬀective mappings actionword irrigate action irrigate state-word city game attribute near-city. since model non-linear approximation underlying action-value function game learn model parameters applying non-linear regression observed ﬁnal utilities simulated roll-outs. speciﬁcally adjust parameters stochastic gradient descent minimize mean-squared error action-value ﬁnal utility observed game state action resulting update model parameters form learning rate parameters ﬁnal layer sentence relevance layer predicate labeling layer respectively. derivations update equations given appendix game test model civilization multi-player strategy game either earth randomly generated world. player acts ruler civilization starts game units i.e. settlers workers explorer. goal expand civilization developing technologies building cities units game either controlling entire world successfully sending spaceship another world. game world divided grid typically squares grid location represents tile either land sea. figure shows portion world particular instance game along game units player. experiments consider two-player game civilization squares smallest allowed freeciv. size used novice human players looking easier game well advanced players wanting game shorter duration. test algorithms built-in player game diﬃculty level default normal setting. deﬁne game state monte-carlo search game world along attributes tile location attributes player’s cities units. examples attributes shown figure space possible actions city unit deﬁned game rules given current game state. example cities construct buildings harbors banks create units various types; individual units move around grid perform unit speciﬁc actions irrigation settlers military defense archers. since player controls multiple cities units player’s action space turn deﬁned combination possible actions cities units. experiments average player controls approximately units unit possible actions. resulting action space player large i.e. eﬀectively deal large action space assume given state actions individual city unit independent actions cities units player. time maximize parameter sharing using single action-value function cities units player. freeciv diﬃculty settings novice easy normal hard cheating. evidenced discussions game’s online forum human players game even novice setting hard. figure portion game instance civilization game. three cities several units single player visible map. also visible diﬀerent terrain attributes tiles grassland hills mountains deserts. critically important monte-carlo search algorithm availability utility function evaluate outcomes simulated game roll-outs. typical application algorithm ﬁnal game outcome terms victory loss used utility function unfortunately complexity civilization length typical game precludes possibility running simulation roll-outs game completion. game however provides player real valued game score noisy indicator strength civilization. since playing two-player game player’s score relative opponent’s used utility function. speciﬁcally ratio game score players. components method operate features computed basic text game attributes. text attributes include words sentence along parts-of-speech dependency parse information dependency types parent words. basic game attributes encode game information available human players game’s graphical user interface. examples attributes shown figure identify sentence relevant current game state candidate action sentence relevance component computes features combined basic attributes game sentence text. features types ﬁrst computes cartesian product attributes game attributes candidate sentence. second type consists binary features test overlap words candidate sentence text labels current game state candidate action. given word tokens manual overlap labels game similarity features highly sparse. however serve signposts guide learner shown results method able operate eﬀectively even absence features performs better present. predicate labeling unlike sentence relevance purely language task operates basic text attributes. features component compute cartesian product candidate predicate label word’s type part-of-speech dependency parse information. ﬁnal component model action-value approximation operates attributes game state candidate action sentence selected relevant predicate labeling sentence. features layer compute three cartesian product attributes candidate action attributes game state predicate labeled words relevant sentence. overall compute approximately features respectively resulting total features full model. figure shows examples features. oﬃcial game manual civilization strategy guide document. text manual uses vocabulary word types composed sentences average words long. manual contains information rules game game user interface basic strategy advice diﬀerent aspects game. stanford parser default settings generate dependency parse information sentences game manual. figure diagram experimental framework showing monte-carlo player server primary game playing aims multiple game servers simulated play. communications multiple processes comprising framework unix sockets in-memory system. across experiments start game initial state steps. step perform monte-carlo roll-outs. roll-out simulated game steps halting simulation evaluating outcome. note simulated game step algorithm needs select action game unit. given average number units player results decisions roll-outs. pairing decisions corresponding roll-out outcome used datapoint update model parameters. ﬁxed learning rate experiments. method baselines independent games manner evaluations averaged across runs. experimental settings across methods model parameters initialized zero. addition instrumentation code freeciv changed increase simulation speed several orders magnitude remove bugs caused game crash. best knowledge game rules functionality identical unmodiﬁed freeciv version tions simply separate instances freeciv game. instance freeciv game made server process runs actual game client process controlled monte-carlo player. start roll-out simulations initialized current state primary game game save/reload functionality freeciv. figure shows diagram experimental framework. experiments typical desktop single intel core cpus algorithms implemented execute simulation roll-outs parallel connecting independent simulation games. computational setup approximately simulation roll-outs executed second full model single game steps runs hours. since treat freeciv game code black special care taken ensure consistency across experiments code compiled speciﬁc machine single ﬁxed build environment experiments identical settings ﬁxed machines running ﬁxed conﬁguration wish evaluate aspects method well improves game play leveraging textual information accurately analyzes text learning game feedback. evaluate ﬁrst aspect comparing method various baselines terms percentage games built-in freeciv. ﬁxed heuristic algorithm designed using extensive knowledge game intention challenging human players. such provides good open-reference baseline. evaluate method measuring percentage games averaged independent runs. however full games sometimes last multiple days making diﬃcult extensive analysis model performance contributing factors. reason primary evaluation measures percentage games within ﬁrst game steps averaged independent runs. evaluation underestimate model performance game player gaining control entire game within steps considered loss. since games remain tied steps equally matched average players playing other likely rate close zero evaluation. adequately characterize performance method evaluate respect several diﬀerent aspects. section ﬁrst describe game playing performance analyze impact textual information. then investigate quality text analysis produced model terms sentence relevance predicate labeling. table rate method several baselines within ﬁrst game steps playing built-in game games neither lost still ongoing. model’s rate statistically signiﬁcant baselines. results averaged across independent game runs. standard errors shown percentage wins. table shows performance method several baselines primary -step evaluation. scenario language-aware monte-carlo algorithm wins average games substantially outperforming baselines best non-languageaware method rate dismal performance random baseline game’s built-in playing itself indications diﬃculty winning games within ﬁrst steps. shown table evaluated full length games method rate compared best text-unaware baseline. constrained follow rules game access information typically available human players information technology cities units it’s opponents. methods hand restricted actions information available human players. note performance methods full games diﬀerent listed previous publications previous numbers biased code freeciv caused game sporadically crash middle game play. originally believed crash random subsequently discovered happen often losing games thereby biasing rates methods upwards. numbers presented game ﬁxed crashes observed experiments. figure observed game score function monte-carlo roll-outs text-aware full model text-unaware latent-variable model. model parameters updated roll-out thus performance improves roll-outs. seen full model’s performance improves dramatically small number roll-outs demonstrating beneﬁt derives textual information. verify characterize impact textual advice model’s performance compare several baselines access textual information. simplest methods game only models action-value function linear approximation game’s state action attributes. non-text-aware method wins games conﬁrm method’s improved performance simply inherently richer non-linear approximation also evaluate ablative non-linear baselines. ﬁrst these latent variable extends linear actionvalue function game latent variables. essence four layer neural network similar full model second layer’s units activated based game information. baseline wins games signiﬁcantly improving linear game baseline still trailing text-aware method second ablative baseline randomized text identical model except given randomly generated document input. generate document randomly permuting locations words game manual thereby maintaining document’s statistical properties terms type frequencies. ensures number latent variables baseline equal full model. thus baseline model capacity equal text-aware method access textual information. performance baseline wins games conﬁrms information extracted text indeed instrumental performance method. figure provides insight textual information helps improve game performance shows observed game score monte-carlo roll-outs full model latent-variable baseline. seen ﬁgure textual information guides model high-score region search space quicker non-text aware method thus resulting better overall performance. evaluate performance method varies amount available textual-information conduct experiment random portions text given algorithm. shown figure method’s performance varies linearly function amount text randomized text experiment corresponding point information available text. sentence relevance component model uses features compute similarity words sentence text labels game state action. assumes availability seed vocabulary names game attributes. domain unique text labels present game occur vocabulary game manual. results sparse seed vocabulary words covering word types word tokens manual. despite sparsity seed vocabulary potentially large impact model performance since provides initial word groundings. evaluate importance initial grounding test method empty seed vocabulary. setup full model wins games showing seed words important method also operate eﬀectively absence. characterize contribution language game performance conduct series evaluations vary type complexity linguistic analysis performed method. results evaluation shown table ﬁrst these sentence relevance highlights contributions language components model. algorithm identical full model lacks predicate labeling component wins games showing essential identify textual advice relevant current game state deeper syntactic analysis extracted text substantially improves performance. evaluate importance dependency parse information language analysis vary type features available predicate labeling component model. ﬁrst ablative experiments dependency information removes dependency features leaving predicate labeling operate word type features. performance baseline rate clearly shows dependency features crucial model performance. remaining three methods dependency label dependency parent dependency parent word drop dependency feature named after. contribution features model performance seen table figure performance text-aware model function amount text available construct partial documents randomly sub-sampling sentences full game manual. x-axis shows amount sentences given method ratio full text. leftmost extreme performance randomized text baseline showing performance trend point useful textual information. table rates several ablated versions model showing contribution diﬀerent aspects textual information game performance. sentence relevance identical full model except lacks predicate labeling component. four methods bottom table ablate speciﬁc dependency features predicate labeling component full model. inherent disadvantage non-linear models compared simpler linear models increase computation time required parameter estimation. monte-carlo search setup model parameters re-estimated simulated roll-out. therefore given ﬁxed amount time roll-outs done simpler faster model. nature performance monte-carlo search improves number rollouts. trade-oﬀ model complexity roll-outs important since simpler model could compensate using roll-outs thereby outperform complex ones. scenario particularly relevant games players limited amount time turn. explore trade-oﬀ vary number simulation roll-outs allowed method game step recording win-rate average computation time game. figure shows results evaluation roll-outs. complex methods higher computational demands results clearly show even given ﬁxed amount computation time game step text-aware model still produces best performance wide margin. qualitatively methods described learn basic rush strategy. essentially attempt develop basic technologies build army take opposing cities quickly possible. performance diﬀerence diﬀerent models essentially well learn strategy. basic reasons algorithms learn rush strategy. first since attempting maximize game score methods implicitly biased towards ﬁnding fastest happens rush strategy playing built-in civilization second complex strategies typically require coordination multiple game units. since models assume game units independent figure examples method’s sentence relevance predicate labeling decisions. shows sentences predicted relevant not. shows predicted predicate structure three sentences indicating state descriptiona action description background words unmarked. mistakes identiﬁed crosses. described section text analysis method tightly coupled game playing terms modeling terms learning game feedback. seen results thus text analysis indeed help game play. section focus game-driven text analysis itself investigate well conforms common notions linguistic correctness. comparing model predictions sentence relevance predicate labeling manual annotations. figure shows examples sentence relevance decisions produced method. evaluate accuracy decisions would ideally like ground-truth relevance annotation game’s user manual. however impractical since relevance decision dependent game context hence speciﬁc time step game instance. therefore evaluate sentence relevance accuracy using synthetic document. create document combining original game manual equal number sentences known irrelevant game. sentences collected randomly sampling wall street journal corpus evaluate sentence relevance synthetic document measuring accuracy game manual sentences picked relevant. evaluation method achieves average accuracy given model diﬀerentiate game manual text wall street journal number seem disappointing. furthermore seen figure sentence relevance accuracy varies widely game progresses high average initial game steps. reality pattern high initial accuracy followed lower average entirely surprising oﬃcial game manual civilization written ﬁrst time players. such focuses initial portion game providing little strategy advice relevant subsequent game play. reason observed sentence relevance trend would also expect ﬁnal layer neural network emphasize game features text features ﬁrst steps game. indeed case seen figure using textual information. results evaluation several values given figure showing initial phase game indeed information game manual useful. fact hybrid method performs well full model achieving rate. shows method figure diﬀerence norms text features game features output layer neural network. beyond initial steps game method relies increasingly game features. figure graph showing availability textual information initial steps game aﬀects performance full model. textual information given model ﬁrst steps beyond point algorithm access text becomes equivalent latent variable model i.e. best non-text model. table predicate labeling accuracy method random baseline. column s/a/b shows performance three-way labeling words state action background column shows accuracy task diﬀerentiating state action words. figure shows examples predicate structure output model. evaluate accuracy labeling comparing gold-standard annotation game manual. table shows performance method terms accurately labels words state action background also accurately diﬀerentiates state action words. addition showing performance improvement random baseline results display clear trend evaluations labeling accuracy higher initial stages game. expected since model relies heavily textual features beginning game verify usefulness method’s predicate labeling perform ﬁnal experiments predicate labels selected uniformly random within full model. random labeling results rate performance similar sentence relevance model uses predicate information. conﬁrms method able identify predicate structure which noisy provides information relevant game play. figure shows examples textual information grounded game associations learned words game attributes ﬁnal layer full model. example model learns strong association note ground truth labeling words either action-description state-description background based purely semantics sentence independent game state. reason manual annotation feasible unlike case sentence relevance. paper presented novel approach improving performance control applications leveraging information automatically extracted text documents time learning language analysis based control feedback. model biases learned strategy enriching policy function text features thereby modeling mapping words manual state-speciﬁc action selection. eﬀectively learn grounding model identiﬁes text relevant current game state induces predicate structure text. linguistic decisions modeled jointly using non-linear policy function trained monte-carlo search framework. empirical results show model able signiﬁcantly improve game rate leveraging textual information compared strong language-agnostic baselines. also demonstrate despite increased complexity model knowledge acquires enables sustain good performance even number simulations reduced. moreover deeper linguistic analysis form predicate labeling text improves game play. show information syntactic structure text crucial analysis ignoring information large impact model performance. finally experiments demonstrate tightly coupling control linguistic features model able deliver robust performance presence noise inherent automatic language analysis. portions work previously presented conference publications article signiﬁcantly extends previous work notably providing analysis model properties impact linguistic representation model performance dependence model bootstrapping conditions tradeoﬀ model’s representational power empirical complexity paper also signiﬁcantly increases volume experiments base conclusions. addition provide comprehensive description model providing full mathematical derivations supporting algorithm authors acknowledge support darpa bolt program darpa machine reading program batelle microsoft research faculty fellowship. thanks anonymous reviewers michael collins tommi jaakkola leslie kaelbling nate kushman sasha rush luke zettlemoyer group suggestions comments. opinions ﬁndings conclusions recommendations expressed paper authors necessarily reﬂect views funding organizations. parameter model estimated standard error backpropagation derive parameter updates consider slightly simpliﬁed neural network shown below. network identical model sake clarity single second layer instead parallel second layers parameter updates parallel layers similar therefore show derivation addition updates ﬁnal layer. model nodes network activated softmax function; third layer computed deterministically active nodes second layer function gx); output linear combination weighted goal minimize mean-squared error gradient descent. achieve updating model parameters along gradient respect parameter. using general term indicate model’s parameters update takes form listed predicate labellings computed text-aware method example sentences game manual. predicted labels indicated words letters action-description state-description background respectively. incorrect labels indicated check mark along correct label brackets. shown examples word game-attribute associations learned model. game attributes strongest association feature weight listed three example words attack build grassland. fourth word settler seven attributes non-zero weights experiments used collect statistics. label word type label part-of-speech word label parent word dependency tree label dependency type dependency parent word label part-of-speech dependency parent word label word leaf node dependency tree. label word leaf node dependency tree. label word matches state attribute name. label word matches unit type name. label word matches action name. following templates used compute features action-value approximation. unless otherwise mentioned features look attributes player controlled model. percentage world controlled. percentage world explored. player’s game score. opponent’s game score. number cities. average size cities. total size cities. number units. number veteran units. wealth gold. excess food produced. excess shield produced. excess trade produced. excess science produced. excess gold produced. excess luxury produced. name technology currently researched. percentage completion current research. percentage remaining current research. number game turns current research completed. turns remaining current construction completed. surplus food production city. surplus shield production city. surplus trade production city. surplus science production city. surplus gold production city. surplus luxury production city. distance closest friendly city. average distance friendly cities. city governance type. type building unit currently construction. types buildings already constructed city. type terrain surrounding city. type resources available city’s neighborhood. type unit. moves left unit current game turn. current health unit. hit-points unit. unit veteran. distance closest friendly city. average distance friendly cities. type terrain surrounding unit. type resources available unit’s neighborhood. enemy unit neighborhood. enemy city neighborhood.", "year": 2014}