{"title": "Stochastic Conjugate Gradient Algorithm with Variance Reduction", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Conjugate gradient methods are a class of important methods for solving linear equations and nonlinear optimization. In our work, we propose a new stochastic conjugate gradient algorithm with variance reduction (CGVR) and prove its linear convergence with the Fletcher and Revves method for strongly convex and smooth functions. We experimentally demonstrate that the CGVR algorithm converges faster than its counterparts for six large-scale optimization problems that may be convex, non-convex or non-smooth, and its AUC (Area Under Curve) performance with $L2$-regularized $L2$-loss is comparable to that of LIBLINEAR but with significant improvement in computational efficiency.", "text": "variance reduction effect sgd. shalev-shwartz zhang introduced sdca train convex linear prediction problems linear convergence rate. however methods require storage gradients makes unsuitable complex application storing gradients impractical. stochastic variance reduced gradient method proposed johnson zhang speeds convergence stochastic ﬁrst-order methods reducing variance gradient estimates. another promising line work devoted stochasticization second-order quasi-newton method particularly l-bfgs algorithm. wang studied stochastic quasi-newton methods nonconvex stochastic optimization. mokhtari ribeiro used stochastic gradients lieu deterministic gradients determine descent directions approximate objective function’s curvature. moritz introduced stochastic variant l-bfgs incorporated idea variance reduction. gower proposed limited-memory stochastic block bfgs update variance reduction approach svrg. however limited-memory stochastic quasi-newton methods often require vector pairs efﬁciently compute product prohibitive case limited memory large-scale machine learning problems. fletcher revves ﬁrst showed extend linear conjugate gradient method non-linear functions called method. polak ribiere proposed another conjugate gradient method method. gilbert nocedal proved modiﬁed method wolfe-powell linear search globally convergent sufﬁcient descent condition. practical computation method method method generally believed efﬁcient conjugate gradient methods essentially restart direction occurs. although convergence method method method established numerical results good. work propose stochastic variant conjugate gradient method integrates variance reduction method called cgvr. following advantages requires iterations quickly converge idea svrg. parameters cgvr sensitive datasets empirical settings often work well; particular unlike step size determined wolfe line search. stores last gradient vector like contrast quasi-newton variants often store vector pairs. cgvr l-regularized l-loss abstract—conjugate gradient methods class important methods solving linear equations nonlinear optimization. work propose stochastic conjugate gradient algorithm variance reduction prove linear convergence fletcher revves method strongly convex smooth functions. experimentally demonstrate cgvr algorithm converges faster counterparts large-scale optimization problems convex non-convex non-smooth performance l-regularized l-loss comparable liblinear signiﬁcant improvement computational efﬁciency. parameters machine learning model estimates well parameter i-th point. widely used solve classiﬁcation regression clustering ranking matrix completion problems among others. stochastic gradient descent variants widely used algorithms minimize empirical risk many large-scale machine learning problems. kingma diederik introduced adam method computes adaptive learning rates parameter. sutskever showed momentum used well-designed random initialization slowly increasing schedule momentum parameter could train dnns rnns. however success variants methods heavily relies setting initial learning rate decay strategy learning rate. xiao-bo department information science engineering henan university technology zhengzhou henan china email xbjingmail.com; xu-yao zhang associate professor national laboratory pattern recognition institute automation chinese academy sciences beijing china; kaizhu huang works head department electrical electronic engineering xian jiaotongliverpool university; computer network information center chinese academy sciences beijing. denotes number elements correspondingly algorithm uses stochastic estimates gradient ∇fs. addition stochastic approximations inverse hessian different decouple estimation gradient estimation hessian. algorithm performs full gradient computation every iterations updates inverse hessian approximation every iterations. vector computed product stochastic approximate hessian vector difference consecutive sequences length product hrgt directly obtained twoloop recursion whose inputs recent vector pairs contributions follows propose stochastic variant conjugate gradient variance reduction wolfe line search gradient computation built sub-samples. conduct series experiments large-scale datasets four state-of-the-art learning problems convex non-convex non-smooth. experimental results show cgvr converges faster large-scale datasets several algorithms performance l-regularized l-loss comparable liblinear signiﬁcant improvement computational efﬁciency. remainder paper organized follows. section brieﬂy introduce svrg slbfgs algorithms. section propose cgvr algorithms prove linear convergence strongly convex smooth functions. section conduct experiments convergence generalization compare cgvr counterparts. finally draw conclusions section loops alg. outer loop full gradient computed. retain snapshot every iterations. inner loop randomly select example dataset produce gradient lower variance. options select next although option better choice option takes iterations obtain next convergence analysis available option although svrg speeds convergence reducing variance gradient estimates sensitive learning rate. slbfgs requires vector pairs compute product need calculate hessian matrix following propose algorithm called stochastic conjugate gradient variance reduction overcome disadvantages. adapt algorithm svrg obtain stochastic conjugate gradient variance reduction algorithm alg. compute variance reduced gradient randomly produced t-th loop k-th iteration search direction fail descent direction unless satisﬁes certain conditions. avoid situation requiring step length satisfy strong wolfe conditions sides taken expectations random variables although also possible using fskt linear search clearly faster. values commonly used algorithm. initial search step since cgvr algorithm replaces role classical algorithm borrow conclusions algorithm also following condition convergence analysis perform inexact line search identify step length. line search algorithm divided steps ﬁrst step begins initial estimate constantly increases estimate ﬁnds suitable step length range includes desired step length listed alg. second step invoked calling function called zoom successively decreases size interval acceptable step length identiﬁed. stop line search zoom procedure cannot attain lower function value iterations. zoom procedure middle point interval candidate instead complex quadratic cubic bisection interpolation. tricks work well practice. dataset contains attributes census form household asked used predict whether household income greater among attributes continuous attributes excepting eight categorical ones discretized quintiles obtain binary attributes dataset covtype derived forest dataset classes. modiﬁed binary classiﬁcation separate class ijcnn used ijcnn classes. dataset neural network competition time series data produced physical system. dataset artiﬁcial perfectly linearly separable dataset. input data consist random binary -dimensional vector fraction inputs. whether label depends whether product stored vector input vector greater less susy dataset used distinguish signal process produces supersymmetric particles background process not. ﬁrst features kinematic properties measured particle detectors accelerator. last features functions ﬁrst features. similar susy dataset higgs used distinguish signal process produces higgs bosons background process not. ﬁrst features kinematic properties last seven features functions ﬁrst features. finally datasets used binary classiﬁcation problems. preprocessing stage feature value dimensions scaled range max-min scaler. algorithms implemented using armadillo linear algebra library intel explore convergence algorithm used entire data minimize function values four learning problems. compare generalization algorithms classiﬁcation randomly divided entire data three parts testing validation remainder training. used divisions algorithms. searching optimal parameters candidate maximize score validation svrg cgvr must calculate gradient s-lbfgs must also calculate hessian matrix. implementations used numerical methods estimate gradient hessian matrix small const fair comparison used original version liblinear software discussion generalization effective general libsvm training models large-scale problems. know measurer represents area curve graphical plot demonstrates discriminant ability binary classiﬁcation model discrimination threshold varied. discriminant threshold real value output liblinear discrete class label given threshold many identical output values result notably large uncertainty sorting used calculate value. thus modiﬁed predict function liblinear directly output discriminant value default liblinear optimizes dual form lregularized l-loss problem selected dataset research object reported measures outer loops used identical random seed initialize vector uniformly distributed interval cgvr slbgfs sampling size |skt| used calculate gradient vector hessian matrix function. cgvr slbfgs memory size hessian update interval accelerated relevant direction dampens oscillations using momentum method momentum coefﬁcient commonly svrg slbfgs attempted three different constant stepsizes figs. show function values running time cgvr algorithms different learning rates increasing parameter learning rate obtained upper lower parts ﬁgures separately correspond parameters respectively. four columns subﬁgures demonstrate different observe svrg slbfgs reduce loss extent increasing setting appropriate learning rate. cgvr algorithm quickly approach minimum value function inner loops slowly decreases value increases. thus cgvr notably insensitive parameter requires inner loops quickly converge. fig. shows running time algorithms increases increase running time slbfgs svrg algorithm varies learning rate. although cgvr takes comparable time algorithms parameter identical still great advantage terms time efﬁciency cgvr requires iterationsof inner loops quickly converge. number inner loop iterations compare convergence several algorithms largescale datasets. best model corresponds optimal leaning rate chosen svrg slbfgs optimal learning rate taken minimizes value ﬁnal iteration validation set. figs. show convergence algorithms repectively. although parameter varies algorithm speciﬁed problem shows similar convergence identical dataset. unstably convergences ridge problem inappropriate learning rate cause large ﬂuctuations loss value. slbfgs show better convergence svrg because slbfgs svrg sensitive learning rates. general converges faster slbfgs svrg. cgvr fastest convergence almost four problems even loss value reaches notably small value. also observe algorithms converge faster sqhinge problem problems. analyze generalization cgvr algorithm show average scores svrg slbfgs cgvr random splits datasets model fig. furthermore compare average performance execution time cgvr liblinear large-scale datasets learning rate selected regularization coefﬁcient drawn parameter liblinear equal training parameters model parameters optimized space grid holdvalidation training validation datasets. fig. shows cgvr outperforms counterparts signiﬁcantly datasets. svrg slbfgs show notably close generalization performance. classical algorithm shows better generalization performance svrg slbfgs. combining discussion convergence algorithms conclude fig. convergence four loss functions three variance reduction algorithms different learning rates datasets. x-axis represents parameter y-axis logarithm loss value number legends learning rate; upper lower rows correspond regularization parameter respectively. fig. running time four loss functions three variance reduction algorithms different learning parameters datasets. x-axis represents parameter y-axis logarithm running time seconds logarithm base number legends learning rate; upper lower rows correspond regularization parameters respectively. fig. convergence four loss functions algorithms datasets. y-axis represents logarithm loss value; x-axis represents number iterations outer loop. fig. convergence four loss functions algorithms datasets. y-axis represents logarithm loss value; x-axis represents number iterations outer loop. fig. shows algorithm cgvr achieves scores comparable liblinear datasets. four discussed problems cgvr performs best datasets solving sqhinge problem exactly loss function optimized liblinear default settings. furthermore liblinear runs faster small-scale datasets cgvr datasets ijcnn whose sample sizes less however datasets millions tens millions data points susy higgs algorithm cgvr runs faster liblinear must iterate times. paper proposed conjugate gradient algorithm based variance reduction prove linear convergence cgvr fletcher-reeves update. large-scale datasets empirical results show power algorithms large-scale classiﬁcation four classic learning problems loss function related models convex function nonconvex function non-smooth function. advantages algorithm cgvr follows requires iterations quickly converge. parameters cgvr insensitive datasets empirical settings often work well. requires less storage space running like algorithms; must store last gradient vector whereas slbfgs must store vector pairs. cgvr l-regularized l-loss achieves generalization performance comparable liblinear providing great improvements computational efﬁciency large-scale machine learning problems. future work implementation algorithm using numerical gradient easily apply problems sparse dictionary learning low-rank matrix approximation problems. work partially supported fundamental research funds henan provincial colleges universities henan university technology national natural science foundation china national research development program national basic research program china chen shang low-rank matrix approximation stability proceedings international conference international conference machine learning volume icml’ jmlr.org york sutskever martens dahl hinton importance initialization momentum deep learning proceedings international conference international conference machine learning volume icml’ jmlr.org atlanta roux schmidt bach stochastic gradient method exponential convergence rate finite training sets proceedings international conference neural information processing systems nips’ curran associates inc. shalev-shwartz zhang stochastic dual coordinate ascent methods regularized loss mach. learn. res. johnson zhang accelerating stochastic gradient descent using predictive variance reduction proceedings international conference neural information processing systems nips’ curran associates inc. moritz nishihara jordan linearly-convergent stochastic l-bfgs algorithm gretton robert proceedings international conference artiﬁcial intelligence statistics vol. proceedings machine learning research pmlr cadiz spain prokhorov ijcnn neural network competition baldi sadowski whiteson searching exotic particles high-energy physics deep learning nature communications sanderson curtin armadillo template-based library xiao-bo associate professor school information science engineering henan university technology. received ph.d. degree pattern recognition intelligent systems national laboratory pattern recognition institute automation chinese academy sciences beijing china research interests include mining machine learning work appeared pattern recognition neurocomputing. xu-yao zhang associate professor national laboratory pattern recognition institute automation chinese academy sciences beijing china. received degree computational mathematics wuhan university wuhan china degree pattern recognition intelligent systems institute automation chinese academy sciences beijing china visiting researcher cenparmi concordia university march march visiting scholar montreal institute learning algorithms university montreal. research interests include machine learning pattern recognition handwriting recognition deep learning. kaizhu huang works head department electrical electronic engineering xian jiaotong-liverpool university. that associate professor national laboratory pattern recognition institute automation chinese academy sciences huang obtained ph.d. degree chinese univ. hong kong worked researcher fujitsu centre cuhk university bristol huang working machine learning pattern recognition neural information processing. recipient asian paciﬁc neural network assembly distinguished younger researcher award. published books springer international research papers e.g. journals conferences serves associate editors neurocomputing cognitive computation springer nature data analytics. served programme committees many international conferences iconip ijcnn iwaci eann kdir. especially serves chairs several major conferences workshops e.g. aaai senior acml publication chair iconip icdar acpr iconip guang-gang geng received ph.d. degree state laboratory management control complex systems institute automation chinese academy sciences beijing china. computer network information center chinese academy sciences beijing currently professor china internet network information center. current research interests include machine learning adversarial information retrieval search.", "year": 2017}