{"title": "Structure Learning in Bayesian Networks of Moderate Size by Efficient  Sampling", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We study the Bayesian model averaging approach to learning Bayesian network structures (DAGs) from data. We develop new algorithms including the first algorithm that is able to efficiently sample DAGs according to the exact structure posterior. The DAG samples can then be used to construct estimators for the posterior of any feature. We theoretically prove good properties of our estimators and empirically show that our estimators considerably outperform the estimators from the previous state-of-the-art methods.", "text": "department computer science department statistics iowa state university ames tian department computer science iowa state university ames huaiqing department statistics iowa state university ames study bayesian model averaging approach learning bayesian network structures data. develop algorithms including ﬁrst algorithm able efﬁciently sample dags according exact structure posterior. samples used construct estimators posterior feature. theoretically prove good properties estimators empirically show estimators considerably outperform estimators previous state-of-the-art methods. keywords bayesian networks structure learning bayesian model averaging sampling bayesian networks graphical representations multivariate joint probability distributions widely used various data mining tasks probabilistic inference causal modeling core bayesian network representation bayesian network structure. bayesian network structure whose nodes represent random variables x··· problem domain whose edges correspond direct probabilistic dependencies. semantically bayesian network structure encodes conditional independence assumptions variable conditionally independent non-descendants given parents. semantics bayesian network structure provides compact representation joint distributions supports efﬁcient algorithms answering probabilistic queries. furthermore semantics bayesian network structure often provide deep insight problem domain open door cause-and-effect analysis. real situation underlying bayesian network typically unknown learned observed data. motivation structure learning learned structure inference decision making. example learned model predict classify instance data. another structure-learning motivation closely related semantics bayesian network structures discovering structure problem domain. example context biological expression data discovery causal dependence relation among different genes often primary interests. semantics bayesian network structure existence edge node node interpreted fact variable directly inﬂuences variable existence directed path node node interpreted fact eventually inﬂuences furthermore certain assumptions existence directed path node node indicates causes thus learned bayesian network structure answer interesting questions whether gene controls gene turn controls gene examining whether directed path node node node learned structure. mentioned friedman koller extraction kinds interesting structural features often primary goal discovery task. several general approaches learning structures. approach treat model selection problem. approach deﬁnes scoring criterion measures well structure data ﬁnds optimal score given data however data size small relative number variables posterior often gives signiﬁcant support number dags using single maximuma-posteriori model could lead unwarranted conclusions therefore desirable bayesian model averaging approach posterior probability feature interest computed averaging possible dags bayesian model averaging however computationally challenging number possible network structures super-exponential number variables tractable algorithms developed special cases averaging trees averaging dags given node ordering since dynamic programming algorithms developed computing exact posterior probabilities structural features edges subnetworks algorithms exponential time space complexity capable handling bayesian networks moderate size around variables limitation algorithms compute posteriors modular features edge compute non-modular features path combined path limited-length path recently parviainen koivisto developed algorithm compute exact posterior probability path feature certain assumption. algorithm exponential time space complexity handle bayesian network fewer variables since algorithm deal path feature non-modular features various users would interested corresponding problems still computed algorithm proposed far. note generally posterior combined feature obtained posterior individual feature independence among features hold generally. actually comparing user know effect feature upon feature obtain user typically needs obtain ﬁrst. another limitation algorithms expensive perform data prediction tasks. compute exact posterior observational data case algorithms re-run data case solution computing posterior arbitrary non-modular feature drawing samples posterior used approximate full bayesian model averaging estimating posterior arbitrary feature number algorithms developed drawing sample dags using bootstrap technique markov chain monte carlo techniques madigan york developed structure mcmc algorithm uses metropolis-hastings algorithm space dags. friedman koller developed order mcmc procedure operates space orders. order mcmc shown able considerably improve structure mcmc mixing convergence markov chain outperform bootstrap approach friedman well. eaton murphy developed hybrid mcmc method ﬁrst runs algorithm koivisto develop global proposal distribution runs mcmc phase space. experiments showed hybrid mcmc converged faster structure mcmc order mcmc hybrid mcmc resulted accurate structure learning performance. improved mcmc algorithm traversing space addition edge reversal move developed grzegorczyk husmeier shown superior structure mcmc nearly efﬁcient order mcmc mixing convergence. recently niinimaki proposed partial order mcmc method operates space partial orders. partial order mcmc includes order mcmc special case shown superior order mcmc terms mixing structural learning performance appropriate bucket size set. common drawback mcmc algorithms guarantee quality approximation ﬁnite runs. approach approximating full bayesian model averaging using k-best bayesian network structures studied tian shown competitive hybrid mcmc. several state-of-the-art algorithms work order space including exact algorithms approximate algorithms order mcmc partial order mcmc assume special form structure prior termed order-modular prior computational convenience. however assumption order-modular prior consequence corresponding prior cannot represent desirable priors uniform prior space; computed posterior probabilities biased since larger number topological orders assigned larger prior probability. whether computed posterior bias ordermodular prior inferior counterpart without bias depends application scenario beyond scope paper. detailed discussion issue please related papers method helps order mcmc correct bias proposed ellis wong paper ﬁrst develop algorithm uses results algorithm koivisto sood efﬁciently sample orders according exact order posterior assumption order-modular prior. next develop time-saving strategy process sampling dags consistent given orders. assuming bounded node in-degree.) resulting algorithm ﬁrst algorithm able sample dags according exact posterior order-modular prior assumption. empirically show algorithm considerably accurate considerably efﬁcient order mcmc partial order mcmc moderate algorithm applicable. moreover estimator based algorithm several desirable properties; example unlike existing mcmc algorithms quality estimator guaranteed controlling number dags sampled algorithm. main application algorithm address limitation exact algorithms order estimate posteriors various non-modular features arbitrarily speciﬁed users. additionally algorithm also used efﬁciently perform data prediction tasks estimating large number data cases finally develop algorithm correct bias algorithm extending idea ellis wong theoretically prove estimator based iw-dds several desirable properties; empirically show estimator superior estimators based hybrid mcmc method k-best algorithm state-of-the-art algorithms estimate posterior feature without order-modular prior assumption. analogously iw-dds algorithm mainly addresses limitation exact algorithm tian order estimate posteriors arbitrary non-modular features additionally used efﬁciently perform data prediction tasks application situation prefers avoid bias order-modular prior. section brieﬂy review bayesian approach learning bayesian networks data related algorithms order mcmc algorithm section present order sampling algorithm algorithm iw-dds algorithm; prove good properties estimators based algorithms. empirically demonstrate advantages algorithms section section concludes paper. finally appendix provides proofs conclusions including propositions theorems corollary referenced paper. bayesian network encodes joint probability distribution random variables node representing variable convenience typically work index represent variable index represent parent represent corresponding index set. often called family.) thus represented vector assume given training data particular instantiation variables consider situations data complete every variable assigned value. bayesian approach learning bayesian networks training data compute posterior probability where appropriate parameter priors scorei closed form solution. paper assume local scores computed efﬁciently data. standard assumption structure prior structure-modular prior note subscript intentionally added mean corresponding probability obtained without order-modular prior assumption. different probability computed order-modular prior assumption marked subscript distinction. compute posterior probability hypothesis interest averaging possible dags. example often interested computing posteriors structural features. structural feature represented indicator function feature present otherwise. full bayesian model averaging posterior since summing possible dags generally infeasible problem using contemporary computer approach computing posterior draw samples posterior used estimate posterior algorithms work order space rather space. deﬁne order variables total order represented vector predecessors order clear consistent order denoted subset denote linear orders following largely follow notation koivisto indicator function returning value. example edge feature represented setting setting order-modular prior interested posterior p≺/p≺. obtained joint probability computed meaning always equals easily achieved setting constant koivisto sood show accordingly algorithm koivisto sood consists following three steps. ﬁrst step computes {i}. time complexity step assumption maximum in-degree number variables cost computing single local marginal likelihood scorei data instances. second step computes −{i}. assumed maximum in-degree step takes time using truncated m¨obius transform technique extended standard fast m¨obius transform algorithm third step computes deﬁning following function extended work koivisto sood koivisto includes algorithm koivisto sood ﬁrst three steps appends additional steps edges computed knn) time space. foundation additional steps introduction following function introduced algorithms make signiﬁcant contributions structure learning bayesian networks fundamental limitation compute posteriors modular features. next section show results algorithm koivisto sood efﬁciently draw samples used compute posteriors arbitrary features. idea order mcmc metropolis-hastings algorithm draw order samples ≺no} invariant distribution number sampled orders. purpose need able compute obtained setting denote resulted setting constant similarly deﬁne special cases respectively setting constant order compute arbitrary non-modular features draw samples drawing order samples. given order sampled drawing parents node according given samples estimate feature posterior using shown order sampling algorithm {i}) computed algorithm koivisto sood draw order sample efﬁciently drawing element order one. order represented element order. computed algorithm koivisto sood already sufﬁcient guide order sampling process. abstract point view results computed algorithm koivisto sood analogous answers provided p-oracle stated theorem jerrum theorem jerrum states p-oracle always able provide exact counting information accepting conﬁgurations currently given conﬁguration probabilistic turing machine serve uniform generator every accepting conﬁguration reached equal positive probability. situation instead providing exact counting information results computed algorithm koivisto sood able provide exact joint probability subsequence order shown proof proposition appendix result order sampling efﬁciently performed based deﬁnition conditional probability distribution. drawing order sample easily sample drawing parents node according described friedman koller naturally leads algorithm termed direct sampling follows time complexity algorithm follows. step takes knn) time discussed section step sampling order takes time. step sampling takes time. thus overall time complexity algorithm nk+no). since typically assume order sampling process affect overall time complexity algorithm efﬁciency. time complexity algorithm depends assumption maximum indegree assumption fairly innocuous discussed page article friedman koller dags large families tend scores. accordingly assumption widely used literature maximum in-degree greater experiments. note sampling step algorithm takes time. actually dominate overall running time algorithm moderate sample size reaches several thousands. therefore efﬁciency algorithm developed time-saving strategy sampling step described details remark given samples estimator exact posterior arbitrary feature constructed letting denote time cost determining structural feature nodes constructing takes time. edge feature cnfp path feature fp.) need order samples algorithm consisting steps called direct order sampling given order samples modular feature parent-set feature edge feature computed estimated space represented vector therefore overall memory requirement algorithm step takes memory space; steps take memory space. theorem estimator based algorithm following desirparticular corollary essentially hoeffding bound e−no\u0001 states order ensure probability error estimator algorithm bounded least need require sample size property existing mcmc algorithms have used obtain quality guarantee estimator algorithm. running time sampling step algorithm actually dominate overall running time algorithm moderate sample size reaches several thousands. thus following introduce strategy effectively reducing running time sampling step efﬁciency overall algorithm achieved. created time sampling list z}σj achieved using binary search following observation reason reducing time based running time sampling step. sampled orders even possible essentially multinomial distribution trials smaller every created stored. running time sampling least created possible example number possible values achieves maximum among sampling takes time least samples. accordingly worst-case running time sampling families logzn) hand strategy usually also save running time even number possible values larger probability mass usually uniformly distributed among possible values majority probability mass concentrated values number smaller probability values appear order samples accordingly probability running time sampling least samples. result expected running time sampling families nono)); expected running time sampling j={no nono)}). typically small local score scorei uniform all. correspondingly likely multinomial probability mass function concentrate dominant method follows. upper limit total number probability intervals pre-speciﬁed based memory used computer. time upper limit reached sampling step indi recycle currently stored cates large amount memory used store according usage frequencies serve estimates |d)s. reclaimed ensure least prememory infrequently used speciﬁed number probability intervals recycled memory. addition order better created possibly gets reclaimed sort sampled orders according posterior executing sampling step dds. underlying rationale relatively close indicates relatively close constant) likely share component. equals likely equals share every thus similar posteriors tend close sorting likely used reclamation. furthermore increases common probability orders share component reclamation creases. accordingly sorting probability reusing also increase. result beneﬁt time-saving strategy typically increase increases. experimental results show time-saving strategy sampling step effective. please discussion section sample mean sample standard deviation running time sampling step reported table table subsection present sampling algorithm general structure-modular prior effectively correcting bias order-modular prior. mentioned section bias assumption order-modular prior. essentially based order-modular prior different based structure-modular prior fact common setting always equals always equal following relation holds sampled algorithm. unfortunately hard compute state-of-the-art algorithm proposed niinimaki koivisto computing takes time. therefore following propose estimator much efﬁciently computed estimator shown bias respect good estimator typically appropriate directly used estimate noticing problem ellis wong propose correct bias order mcmc method follows ﬁrst order mcmc draw order samples; unique order sampled orders keep drawing dags consistent joint probabilities less pre-speciﬁed large proportion ﬁnally resulting union samples treated inspired idea ellis wong develop bias-correction strategy computationally efﬁcient theoretically ensure resulting estimator desirable properties. bias-corrected algorithm termed iw-dds follows since checking equality dags takes time using vector representations usage hash table expected time cost space cost bias correction step therefore expected time cost iw-dds algorithm nk+no) required memory space iw-dds algorithm note gets sampled corresponding joint probability easily computed stored therefore constructing estimator constructing estimator iw-dds also takes time denotes time cost determining structural feature nodes. ellis wong show effectiveness method correcting bias merely experiments ﬁrst theoretically prove estimator desirable properties follows. theorem structural feature respect exact posterior estimator based samples iw-dds algorithm using following properties p⊀/p⊀ essentially represents cumulative posterior probability mass dags provides sound interval must reside. outside sound interval.) width sound interval nondecreasing o−no function dags step resulting always superset original thus situations small possible approach tractable number samples desired small-width interval obtained. also note expressed following equivalent form bias-correction strategy used iw-dds solves computation problem existing idea ellis wong ensures desirable properties estimator stated theorem full sampled order keeping drawing dags consistent sampled less large proportion unfortunately strategy computational problem number variables small number data instances small. super-exponential number dags consistent order possible non-negligible portion probability mass distributed almost uniformly majority consistent dags small. consequently required number dags sampled sampled order extremely large leading large computation costs. sampling dags consistent sampled order expected time cost memory requirement memory requirement exceeds memory running computer hard disk used temporarily store sampled dags way. limit experiments data sets variables.) take data child example order randomly sampled order sampling algorithm experiment shows reach dags need reach address problem based efﬁciency order sampling algorithm strategy samples sampled order step large computation costs sampled order avoided data set. meanwhile unlike strategy ellis wong strategy delete duplicate order samples. therefore order gets sampled times order sampling step essentially dags sampled unique order sampling step. thus number occurrences implicitly serves importance indicator among orders. furthermore strategy ellis wong guarantee sampled dags independent even large computation costs spent sampling huge number dags sampled order. essentially multiple dags sampled ﬁxed order according strategy ellis wong independent. example given edge gets sampled order implies node precedes node given order conditional probability reverse edge gets sampled ﬁxed order becomes zero independent. general number sampled orders ﬁxed even number sampled dags sampled order keeps increasing every consistent none sampled orders still chance getting sampled. contrast sampling strategy iw-dds able guarantee property dags sampled step independent stated theorem property actually ensuring good properties estimator stated theorem competing state-of-the-art algorithms also applicable moderate size hybrid mcmc method k-best algorithm ﬁrst competing method hybrid mcmc includes algorithm koivisto knn) space complexity ﬁrst phase uses computed posteriors edges make global proposal second phase mcmc phase eventually converges hybrid mcmc correct bias coming order-prior assumption provide samples according posterior estimator constituted using feature hybrid mcmc empirically shown converge faster structure mcmc order mcmc accurate structure learning performance obtained note since rev-mcmc method shown nearly efﬁcient order mcmc mixing convergence hybrid mcmc expected converge faster rev-mcmc method long moderate hybrid mcmc applicable. infeasible large space cost.) limitation hybrid mcmc obtain interval speciﬁed theorem additionally convergence rate estimator hybrid mcmc theoretically provided authors. second competing method k-best algorithm applies technique obtain collection dags best scores uses dags constitute estimator advantage k-best algorithm estimator also property speciﬁed theorem provide sound interval iw-dds. however k-best algorithm time complexity nn−) space complexity time spent best-ﬁrst search solutions shown flerova thus increase dramatically increase computation costs k-best algorithm small. result obtain interval width similar iw-dds much time space costs required k-best. experiments using ordinary desktop computation problem becomes severe since take small values k-best algorithm exhausts memory. accordingly obtained k-best usually smaller iw-dds even reach memory limit computer. implemented algorithms language tool called bnlearner several experiments demonstrate capabilities. tested data sets include real data sets machine learning repository tic-tac-toe glass wine housing credit letter tumor vehicle german. tested data sets also include three synthetic data sets ﬁrst synthetic data generated gold-standard -node bayesian network built second synthetic data insur generated -node subnetwork insurance bayesian network third synthetic data child -node child bayesian network used tsamardinos data sets contain discrete variables missing values four data sets since large number data instances available also vary corresponding learning performance. experiments section linux ordinary desktop .ghz intel pentium processor memory extra speciﬁcation provided. addition maximum in-degree assumed experiments. partial order mcmc method implemented beandisco language tool provided niinimaki current version beandisco estimate posterior edge feature niinimaki stated po-mcmc readily enables estimating posterior structural feature sampling dags consistent order. since investigated data case moderate able rebel language implementation algorithm koivisto exact posterior every edge assumption order-modular prior. therefore criterion absolute differences measure feature learning performance data case exact posterior investigated feature corresponding ˆp≺| since investigated feature edge feature order-modular prior. smaller indicate better performance structure discovery. note criterion closely related another criterion since sad/). thus data case conclusion based comparison values based comparison values since constant data case. setting po-mcmc according suggestion optimal setting niinimaki bucket size data cases except tic-tac-toe. bucket size data case tic-tac-toe tic-tac-toe variables setting cause tool beandisco throw run-time error. ﬁrst iterations burn-in took partial order samples intervals iterations. thus iterations total. po-mcmc sampled partial order obtained means total order linear extensions sampled partial order included obtain example data since bucket size total orders included sampled partial order inclusion information large number total orders consistent sampled partial order gives great learning power po-mcmc method; inclusion efﬁciently computed algorithm parviainen koivisto assumptions order-modular prior maximum in-degree finally po-mcmc estimated posterior edge to-be-learned feature edge feature also algorithm comparison. algorithm algorithm orders sampled. theoretically expect learning performance better performance additional approximation coming sampling step avoided dos. listing performance mainly intend examine much performance decreases additional approximation sampling order. however since capable learning non-modular features comparison po-mcmc method method main task. table shows experimental results terms data case variables instances table lists running time costs corresponding table three methods performed independent runs data case. sample mean sample standard deviation values method denoted respectively listed along method table correspondingly sample mean total running time method denoted shown table time cost computing edge using end. similarly reported total running time method also includes relatively tiny time cost computing edge using end.) addition sample mean sample standard deviation running time three steps denoted respectively listed last columns table note still show mean running time step independent runs though step random algorithm all. running time step exactly randomness uncontrolled factors internal status computer. showing clearly percentage total running time step typically takes comparing tables clearly illustrate performance advantage method pomcmc method. overall time costs based samples much smaller corresponding costs po-mcmc method based mcmc iterations partial order space. using much shorter time method much smaller po-mcmc method data cases. exceptional cases glass insur insur using method still smaller using po-mcmc method difference large relatively po-mcmc method.) furthermore since given two-sample test unequal variances conclude strong evidence real mean using method smaller real mean using po-mcmc method data cases. exceptional data case glass p-value test conclude signiﬁcance level real mean using method smaller real mean using po-mcmc method. four exceptions test accept null hypothesis signiﬁcant difference real means methods. thus advantage algorithm po-mcmc method learning bayesian networks moderate clearly seen though value po-mcmc method still remains larger algorithm infeasible. terms total running time algorithm table shows running time step always accounts largest portion. running time sampling step less seconds samples cases. though order sampling step sampling step involve randomness variability running time actually small. seen ratio ratio ratio ranges across cases much smaller upper bound ratio indicates time-saving strategy introduced remark effectively reduce running time sampling step. addition running time sampling step often decreases increases clearly seen four data sets different values take data letter example increases corresponding decreases second decrease. summary effectiveness time-saving strategy introduced remark clearly shown table finally present experimental results varying sample size. ﬁrst choose data case letter example. tried sample size independently times sample mean sample standard deviation edge features. po-mcmc bucket size totally mcmc iterations partial order space discarded ﬁrst mcmc iterations burn-in thinning parameter partial orders ﬁnally sampled. again independently po-mcmc times sample mean sample standard deviation edge features. figure shows performance methods terms edge features error represents sample standard deviation across runs method example figure shows algorithm; po-mcmc method. exactly matches results previously shown table correspondingly figure shows po-mcmc running time seconds. advantage clearly seen combining figures real mean signiﬁcantly smaller po-mcmc p-value returned two-sample test terms running time total running time short relative po-mcmc. example increases respect reaches seconds shorter po-mcmc seconds. therefore learning performance sample size signiﬁcantly better po-mcmc data case letter also performed experiment experimental settings data cases tictac-toe wine child german. please refer supplementary material experimental results. conclusion learning performance clearly drawn examining ﬁgures shown supplementary material. methods estimate posteriors features without order-modular prior assumption. implementation hybrid mcmc implementation k-best made available online corresponding authors. since investigated data case moderate able poster language implementation algorithm tian exact posterior edge structure-modular prior. therefore −ˆp⊀|) measure performance three relatively fair comparison terms running time used rebel tool implementation algorithm koivisto perform computation phase dp+mcmc; fair comparison changed scoring criterion bdeu score equivalent sample size perform computation mcmc phase used matlab implementation bdagl windows ordinary laptop intel core memory. mcmc used pure global proposal since setting reported eaton murphy best performance edge discovery mcmc original bdagl found out-of-memory error data case variables experiments. original bdagl intends pre-compute local scores possible families store array later usage phase mcmc phase. solve out-ofmemory issue updated original matlab code bdagl provided bdagl-new package also available http//www.cs.iastate.edu/˜jtian/software/bnlearner/bnlearner.htm. main update that assumed maximum in-degree local scores families whose sizes assumed maximum in-degree pre-computed stored hash table. bdagl-new experiments data cases paper performed without error. iterations performed experimental results. totally mcmc iterations time discarded ﬁrst iterations burn-in period. thinning parameter ﬁnal samples. result time statistics phase mcmc phase directly compared ones methods. data case performed independent mcmc runs based outcome rebel results. iw-dds performed independent runs data case results. k-best note ﬁxed since randomness computed results. result. tic-tac-toe glass wine housing credit letter best dags tic-tac-toe glass wine housing credit cases cases letter. tumor experiments showed tumor k-best program memory out-of-memory issue vehicle insur; child german. fact take value greater experiments conﬁrms claim computation problem k-best algorithm terms space cost. table shows experimental results terms data case table shows running time costs corresponding table −p≺|) edge posterior computed exact method tian edge posterior computed exact method koivisto values reported column indicate bias assumption order-modular prior. next column values three methods listed table dp+mcmc method iw-dds method random shown methods. outcome k-best algorithm random shown. finally table also shows cumulative posterior probability mass k-best algorithm iw-dds method. tables clearly demonstrate advantage method methods. using much shorter computation time method less corresponding dp+mcmc data cases. exceptional case furthermore based two-sample test unequal variances conclude signiﬁcance level real mean using method less corresponding using dp+mcmc cases; exceptional cases meanwhile using method always much smaller using dp+mcmc cases indicates higher stability performance method. similarly using much shorter computation time method less k-best cases. exception furthermore based one-sample test conclude signiﬁcance level real mean using method less using k-best cases. obtained mcmc phase dp+mcmc method similar situations also occur letter insur child indicates small mcmc phase dp+mcmc method unable reduce bias method koivisto cases based mcmc iterations. running time please note iw-dds always less running time phase dp+mcmc method. step method uses algorithm koivisto sood ﬁrst three steps algorithm koivisto phase dp+mcmc method uses steps algorithm koivisto words compared algorithm koivisto sood algorithm koivisto includes larger constant factor hidden knn) notation though algorithms time complexity. difference make total running time iw-dds even less running time phase dp+mcmc method remaining steps iw-dds faster last steps algorithm koivisto example data case child iw-dds seconds corresponding running time phase dp+mcmc method seconds. actually table shows cases iw-dds less running time phase dp+mcmc method. addition shown section effectiveness time-saving strategy also clearly seen table example ratio ranges across cases much smaller upper bound ratio table also shows resulting cumulative probability mass k-best iwg∈g p⊀/p⊀ computed using poster tool.) table iw-dds greater k-best data cases. exceptional cases tumor letter interestingly four exceptional cases iw-dds signiﬁcantly smaller k-best. possible reason best dags tend similar local structures relatively large local scores large number dags sampled iw-dds include various local structures node inclusion various local structures seems effective improving structural learning performance. demonstrate iw-dds obtain large efﬁciently small increased increment performance data cases letter child german. again performed independent runs data case results. figures show increase respect increase three data cases. correspondingly figures indicate increase respect increase three data cases running time seconds. combining ﬁgures clear iw-dds efﬁciently achieve large take data case german example time cost seconds iw-dds collect samples corresponding mean reach therefore feature data case german iw-dds provide sound interval width note k-best provide meaningless sound interval huge width reach data case german running memory. also note ratio decreases increases constant respect increase rate actually decreases increases.) witnesses statement remark beneﬁt time-saving strategy typically increase increases. finally present experimental results iw-dds varying sample size. section experiments performed data cases tic-tac-toe wine letter child german. iw-dds tried sample size independently iw-dds times sample mean sample standard deviation edge features. dp+mcmc totally mcmc iterations discarded ﬁrst mcmc iterations burn-in thinning parameter dags sampled. again independently mcmc times sample mean sample standard deviation edge features. k-best different experimental settings used different data cases out-of-memory issue. data cases tic-tac-toe wine k-best program data case letter k-best k-best program memory expensive space cost. corresponding result k-best would compared result iw-dds out-of-memory issue child german running k-best program. note since randomness outcome k-best algorithm always k-best program ﬁxed edge features. experimental results comparing three methods based data case tic-tac-toe shown figures figure shows performance three methods terms edge features error represents sample standard deviation across runs dp+mcmc iw-dds figure shows dp+mcmc iw-dds well k-best running time seconds. advantage iw-dds clearly seen combining figures comparing dp+mcmc iw-dds uses shorter running time real mean signiﬁcantly smaller corresponding real mean dp+mcmc p-value two-sample test unequal variances. comparing k-best iw-dds uses shorter running time k-best iw-dds real mean signiﬁcantly smaller k-best p-value one-sample test. therefore learning performance iw-dds signiﬁcantly better performance methods data case tic-tac-toe. three methods terms edge features figure shows corresponding time costs three methods. difference figure figure corresponding result k-best marked star compared iw-dds advantage iw-dds clearly seen combining figures comparing dp+mcmc iw-dds uses much shorter running time real mean signiﬁcantly smaller corresponding real mean dp+mcmc p-value two-sample test unequal variances. iw-dds seconds even less running time phase dp+mcmc method note dp+mcmc shown large. dp+mcmc even larger indicates performance dp+mcmc stable based mcmc iterations. comparing k-best iw-dds uses much shorter running time real mean signiﬁcantly smaller k-best since p-value corresponding one-sample test less therefore learning performance iw-dds also signiﬁcantly better performance methods data case letter experimental results three data cases wine child german represented similarly supplementary material. conclusion learning performance clearly drawn examining ﬁgures shown supplementary material. sections provide experimental results learning performance nonmodular features. section known method compute true/exact posterior probability non-modular feature except brute force enumeration dags quality corresponding learned approximate method cannot precisely measured. section current po-mcmc tool supports estimation posterior edge feature comparison method po-mcmc made edge feature. though algorithm parviainen koivisto compute exact posterior path feature p≺.) idea showing algorithms signiﬁcantly better performance computing fundamental structural features better quality samples respect corresponding expect also superior computing complicated structural features using samples. verify expectation performed experiments real data iris machine learning repository well-studied data coronary since small enumerating dags able compute true posterior probability interesting non-modular feature demonstration purpose investigated following interesting non-modular features. directed path feature node node denoted represents situation variable eventually inﬂuences variable limited-length directed path feature path length represents variable inﬂuence variable intermediate variable. combined path feature interpreted situation variable eventually inﬂuences variable turn eventually inﬂuences variable combined path feature means variable eventually inﬂuences variable variable combined path feature represents variable eventually inﬂuences variable variable compared performance edge feature corresponding performance feature dp+mcmc k-best iw-dds. experimental results data sets show iw-dds signiﬁcantly smaller competing method edge feature iw-dds also signiﬁcantly smaller competing method investigated non-modular feature using samples. thus expectation supported experiments. detailed experimental results follows. following experimental design data coronary iw-dds tried sample size independently iw-dds times sample mean sample standard deviation edge feature non-modular features dp+mcmc mcmc iterations discarded ﬁrst mcmc iterations burn-in thinning parameter dags sampled. independently mcmc times sample mean sample standard deviation edge feature k-best k-best k-best edge feature since randomness outcome k-best algorithm. experimental results data coronary demonstrated figure figure figure shows performance three methods edge feature error represents sample standard deviation across runs dp+mcmc iw-dds correspondingly figure figure show performance three methods investigated non-modular features respectively. combining figure figures clearly iw-dds signiﬁcantly smaller competing method edge feature iw-dds also signiﬁcantly smaller competing method investigated non-modular features. speciﬁcally comparing dp+mcmc edge feature real mean iw-dds signiﬁcantly smaller corresponding dp+mcmc p-value two-sample test unequal variances. consistently investigated non-modular feature real mean iw-dds also signiﬁcantly smaller corresponding dp+mcmc p-value test. comparing k-best edge feature real mean iw-dds signiﬁcantly ˆp⊀|) path feature ˆp⊀|) path feature whose length ˆp⊀|) combined feature xyzy=z ˆp⊀|) combined feature xyzx=z |p⊀> z|d) ˆp⊀> z|d)|) combined feature purpose experimental setting dp+mcmc k-best merely testify claimed if-then conditional statement performance iw-dds signiﬁcantly better performance competing method edge feature performance iw-dds also signiﬁcantly better competing method investigated non-modular feature using samples. smaller k-best p-value one-sample test. consistently investigated non-modular feature real mean iw-dds also signiﬁcantly smaller k-best p-value test. also performed kind experiments data iris results demonstrated figure figure comparing dp+mcmc comparison result edge feature investigated real mean iw-dds also signiﬁcantly smaller corresponding dp+mcmc p-value two-sample test unequal variances. comparing k-best comparison result edge feature investigated real mean iw-dds also signiﬁcantly smaller k-best p-value one-sample test. thus conclusion similar coronary drawn iw-dds signiﬁcantly smaller competing method edge feature iw-dds also signiﬁcantly smaller competing method investigated non-modular features. testify quality guarantee estimator based algorithm performed experiments based data cases relatively large algorithm shown table based hypothesis testing approach conclude strong evidence performance guarantee estimator holds data cases. details experiments follows. ﬁrst experiments choose data case letter largest among data cases shown table ﬁrst consider setting parameters speciﬁed serves performance requirement. setting sample size intend show estimator coming performance guarantee hoeffding inequality holds. directed edge feature investigated since posterior edge easily obtained using algorithm koivisto edge call event |ˆp≺ event violation learning deﬁne indication variable event violation learning thus bernoulli random variable success probability pvio independently repeat algorithm times average estimator ˆpvio edge. note mean ˆpvio pvio variance ˆpvio pvio/r rˆpvio binomial distribution trial number success probability pvio. since expect pvio small pvio large relative pvio choose large make variance ˆpvio relatively small respect mean ˆpvio. figure shows histogram ˆpvio directed edges. edges clearly seen corresponding ˆpvio much smaller marked vertical bar. edges corresponding ˆpvio’s exactly equal even largest ˆpvio corresponding successes among trials one-sided hypothesis testing reject null hypothesis pvio conclude pvio p-value less therefore hoeffding inequality holds learning edge parameter setting. next consider another setting parameters demanding performance requirement. setting sample size want show estimator coming performance guarantee satisfying hoeffding inequality. logic independently repeat algorithm times average estimator pvio edge. figure shows histogram ˆpvio directed edges. edge clearly seen corresponding ˆpvio much smaller even largest ˆpvio corresponding successes among trials one-sided hypothesis testing reject null hypothesis pvio conclude pvio p-value less therefore hoeffding inequality also holds parameter setting. second experiments choose data case tic-tac-toe largest among data cases shown table kind experiments performed data case. parameter setting corresponding result shown figure edges corresponding ˆpvio clearly much smaller even largest pvio corresponding successes among trials one-sided hypothesis testing conclude pvio p-value less parameter setting corresponding result shown figure edge corresponding ˆpvio clearly seen much smaller even largest ˆpvio corresponding successes among trials one-sided hypothesis testing conclude pvio p-value less thus hoeffding inequality also holds experiments data case. finally data case tic-tac-toe increase increment time. plot hoeffding bound e−no\u0001 probability violation pvio figure also plot maximum mean ˆpvio’s figure ˆpvio edge average independently running algorithm times. max{/} larger larger since expect ˆpvio smaller larger figure clearly maximum ˆpvio’s always hoeffding bound furthermore one-sided hypothesis testing reject null hypothesis pvio e−no\u0001 conclude pvio e−no\u0001 p-value less therefore hoeffding inequality holds develop algorithms efﬁciently sampling bayesian network structures sampled dags used build estimators posteriors features interests. theoretically show estimators several desirable properties. example unlike existing mcmc algorithms estimators based algorithm satisfy hoeffding bound therefore enjoy quality guarantee estimation given number samples. empirically show estimators considerably outperform previous state-of-the-art without assuming order-modular prior. algorithms capable estimating posteriors arbitrary features exact algorithms available computing modular features order-modular prior time knn) space computing path features order-modular prior time space computing modular features structure-modular prior time space bottleneck algorithms ﬁrst computation step algorithm koivisto sood whose space cost therefore application algorithms limited data sets algorithm koivisto sood able around variables current desktops parallel implementation algorithm demonstrated data variables using cluster including totally processors memory hand event gets sampled according algorithm occurs orders consistent gets sampled step algorithm gets sampled sampled order step algorithm. therefore based based strong large numbers converges almost surely note that theorem property converges almost surely implies converges probability consistent estimator denote dags. deﬁne equal user chooses prior every however user additional domain knowledge he/she sets prior exclude dags priori proper subset note denote indicator function. rewrite ˆp⊀/ˆp⊀ exact posterior order-modular prior assumption. also note", "year": 2015}