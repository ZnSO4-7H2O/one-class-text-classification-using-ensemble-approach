{"title": "Filtering Variational Objectives", "tag": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "abstract": "When used as a surrogate objective for maximum likelihood estimation in latent variable models, the evidence lower bound (ELBO) produces state-of-the-art results. Inspired by this, we consider the extension of the ELBO to a family of lower bounds defined by a particle filter's estimator of the marginal likelihood, the filtering variational objectives (FIVOs). FIVOs take the same arguments as the ELBO, but can exploit a model's sequential structure to form tighter bounds. We present results that relate the tightness of FIVO's bound to the variance of the particle filter's estimator by considering the generic case of bounds defined as log-transformed likelihood estimators. Experimentally, we show that training with FIVO results in substantial improvements over training the same model architecture with the ELBO on sequential data.", "text": "used surrogate objective maximum likelihood estimation latent variable models evidence lower bound produces state-of-the-art results. inspired this consider extension elbo family lower bounds deﬁned particle ﬁlter’s estimator marginal likelihood ﬁltering variational objectives fivos take arguments elbo exploit model’s sequential structure form tighter bounds. present results relate tightness fivo’s bound variance particle ﬁlter’s estimator considering generic case bounds deﬁned log-transformed likelihood estimators. experimentally show training fivo results substantial improvements training model architecture elbo sequential data. learning statistical models gradient descent straightforward objective function gradients tractable. presence latent variables however many objectives become intractable. neural generative models latent variables currently dominant approaches optimizing lower bounds marginal log-likelihood restricting class invertible models using likelihood-free methods work focus ﬁrst approach introduce ﬁltering variational objectives tractable family objectives maximum likelihood estimation latent variable models sequential structure. speciﬁcally denote observation -valued random variable. assume process generating involves unobserved z-valued random variable joint density family goal recover maximizes marginal log-likelihood likelihood function deﬁned generally intractable integral. circumvent marginalization common approach optimize variational lower bound marginal log-likelihood evidence lower bound common bound deﬁned variational posterior distribution whose support includes lower-bounds marginal log-likelihood choice bound tight true posterior thus joint optimum mle. practice common restrict tractable family distributions jointly optimize elbo stochastic gradient ascent penalty optimizing assumptions tends force posterior satisfy factorizing assumptions variational family reduces capacity model strategy addressing decouple tightness bound quality example observed interpreted unnormalized importance weight proposal given using samples proposal produces tighter bound known importance weighted auto-encoder bound iwae. indeed follows jensen’s inequality unbiased positive monte carlo estimator marginal likelihood results lower bound optimized mle. ﬁltering variational objectives build idea treating particle ﬁlter’s likelihood estimator objective function. following call objectives deﬁned log-transformed likelihood estimators monte carlo objectives work show tightness scales like relative variance estimator constructed. well-known variance particle ﬁlter’s likelihood estimator scales favourably simple importance sampling models sequential structure thus fivo potentially form much tighter bound marginal log-likelihood iwae. main contributions work introducing ﬁltering variational objectives careful study monte carlo objectives. section review maximum likelihood estimation maximizing elbo. section study monte carlo objectives provide basic properties. deﬁne ﬁltering variational objectives section discuss details optimization present sharpness result. finally cover related work present experiments showing sequential models trained fivo outperform models trained elbo iwae practice. brieﬂy review techniques optimizing elbo surrogate objective. restrict focus latent variable models model factors tractable conditionals parameterized differentiably parameters models problem optimizing expectation-maximization algorithm approach problem seen coordinate ascent fully maximizing alternately iteration rarely applies general maximizing ﬁxed corresponds generally intractable inference problem. instead approach mild assumptions model perform gradient ascent following monte carlo estimator elbo’s gradient assume taken family distributions parameterized differentiably parameters follow unbiased estimator elbo’s gradient sampling updating parameters qφ)∇φ gradients computed conditional sample learning rate. estimators follow elbo’s gradient expectation variance reduction techniques usually necessary lower variance gradient estimator derived reparameterizable distribution reparameterizable distributions simulated sampling distribution depend applying deterministic transformation differentiable unbiased estimator elbo gradient consists sampling updating parameter qφ|x)). given gradients sampling process unfortunately variational following gradients tends reduce capacity model match assumptions variational family. penalty removed considering generalizations elbo whose tightness controlled means closenesss e.g. consider next section. lower bounds marginal log-likelihood thus used mle. motivated previous section present results convergence generic mcos marginal log-likelihood show tightness closely related variance estimator deﬁnes verify elbo lower bound using concavity jensen’s inequality argument relies unbiasedness thus generalize considering unbiased marginal likelihood estimator treating objective function models indexes amount computation needed simulate e.g. number samples particles. deﬁnition monte carlo objectives. unbiased positive estimator monte carlo objective deﬁned consider additional examples appendix. avoid notational clutter omit arguments e.g. observations model default arguments clear context. whether compute stochastic gradients efﬁciently depends speciﬁc form estimator underlying random variables deﬁne many likelihood estimators converge almost surely advantage consistent estimator driven towards increasing present sufﬁcient conditions convergence description rate proposition properties monte carlo objectives. monte carlo objective deﬁned unbiased positive estimator then cases convergence bound monotonic e.g. iwae true general. relative variance estimators var/p) tends well studied property gives tool comparing convergence rate distinct mcos. example study marginal likelihood estimators deﬁned particle ﬁlters relative variance estimators scales favorably comparison naive importance sampling. suggests particle ﬁlter’s introduced next section generally tighter bound iwae. ﬁltering variational objectives family mcos deﬁned marginal likelihood estimator particle ﬁlter. models sequential structure e.g. latent variable models audio text relative variance naive importance sampling estimator tends scale exponentially number steps. contrast relative variance particle ﬁlter estimators scale favorably number steps—linearly cases thus results section suggest fivos serve tighter objectives iwae sequential models. observations sequences -valued random variables denoted also assume data generation process relies sequence unobserved z-valued latent variables denoted focus sequential latent variable models factor particle ﬁlter sequential monte carlo algorithm propagates population weighted particles steps using combination importance sampling resampling steps alg. detail particle ﬁlter takes arguments observation number particles model distribution variational posterior factored satisfy resampling criteria resampling step renormalized. current weights sampled proportion weights current population performed particles replacement. common resampling schemes include resampling every step resampling t))− drops copied next step along strongly consistent estimator uniformly integrable lfivo resampling distinguishing feature lfivo resampling removed fivo reduces iwae. resampling amount immediate variance allows ﬁlter discard weight particles high probability. figure visualizing fivo; resample particle trajectories determine inheritance next step propose accumulate loss gradients lattice objective gradients solid resampling gradients dotted blue. effect refocusing distribution particles regions higher mass posterior sequential models reduce variance exponential linear number time steps resampling greedy process possible particle discarded step could attained high mass step practice best trade-off adaptive resampling schemes given particle ﬁlter’s likelihood estimator improves simple importance sampling terms variance expect lfivo fivo bound optimized stochastic gradient ascent framework used elbo. found practice effective simply follow monte carlo estimator biased gradient reparameterized gradient estimator biased full fivo gradient three kinds terms term deﬁned conditional random variables alg. gradient terms every distribution alg. depends parameters; adaptive resampling used additional terms account change fivo respect decision resample. section derive fivo gradient reparameterized ﬁxed resampling schedule followed. derive full gradient appendix. detail assume parameterized differentiable assume reparameterizable family alg. reparameterized. assume ﬁxed resampling schedule indicator function indicating whether resampling occured step lfivo depends parameters resampling probabilities terms inside expectation form given single forward pass alg. reparameterized monte carlo estimator however terms resampling events contribute majority variance estimator. thus gradient estimator found effective practice consists gradient solid arrows figure explore experimentally section elbo fivo variational objective taking variational posterior argument. important question whether fivo achieves marginal log-likelihood optimal guarantee models independent given xt−. proposition sharpness filtering variational objectives. lfivo argmaxq lfivo models satisfy assumption deriving optimal general complicated resampling dynamics. restricted model class proposition optimal condition future observations xt+t explored experimentally richer models section found allowing condition xt+t reliably improve fivo. consistent view resampling greedy process responds intermediate distribution ﬁnal. still found impact effect outweighed advantage optimizing tighter bound. marginal log-likelihood central quantity statistics probability long interest bounding literature relating bounds call monte carlo objectives typically focused problem estimating marginal likelihood itself. jensen’s inequality forward reverse estimator detect failure inference methods. iwae clear inﬂuence work fivo seen extension bound. elbo enjoys long history efforts improve elbo itself. generalize elbo considering arbitrary operators model variational posterior. closely related work body work improving elbo increasing expressiveness variational posterior. example augment variational posterior deterministic transformations ﬁxed jacobians extend variational posterior admit markov chain. approaches learning neural latent variable models include importance sampling approximate gradients posterior sequential monte carlo approximate gradients posterior. distinct contribution sense inference sake estimation ultimate goal. knowledge idea treating output inference objective itself completely novel fully appreciated literature. although idea shares inspiration methods optimize convergence markov chains note idea optimize estimator particle ﬁlter independently concurrently considered bound call fivo cast tractable lower bound elbo deﬁned particle ﬁlter’s non-parameteric approximation posterior. additionally derive expression fivo’s bias ﬁlter’s distribution certain target process. work distinguished study convergence mcos includes fivo investigation fivo sharpness experimental results stochastic rnns. experiments sought compare models trained elbo iwae fivo bounds terms ﬁnal test log-likelihoods explore effect resampling gradient terms fivo investigate lack sharpness affects fivo consider models trained fivo stochastic state. explore questions trained variational recurrent neural networks elbo iwae fivo bounds using tensorflow benchmark sequential modeling tasks natural speech waveforms polyphonic music. datasets known difﬁcult model without stochastic latent states vrnn sequential latent variable model combines deterministic recurrent neural network stochastic latent states step. observation distribution conditioned directly indirectly rnn’s state length sequence model’s posterior factors conditiont= pt)gt) variational posterior xt). distributions latent variables factorized gaussians output distributions depend dataset. single-layer lstm conditionals parameterized fully connected neural networks hidden layer size lstm hidden layer. used residual parameterization variational posterior. table test marginal log-likelihood bounds models trained elbo iwae fivo. elbo iwae models report max{lliwae fivo models report lfivo pianoroll results nats timestep timit results nats sequence relative elbo details evaluation methodology absolute numbers appendix. fivo resampled particles dropped fivo iwae used batch size elbo used batch sizes match computational budgets alias method). models report bounds using variational posterior trained jointly model. models trained fivo report lfivo provide strong baselines report maximum across bounds max{lliwae models trained elbo iwae. additional details appendix. evaluated vrnns trained elbo iwae fivo bounds polyphonic music datasets nottingham folk tunes chorales musedata library classical piano orchestral music piano-midi.de midi archive dataset split standard train valid test sets represented sequence -dimensional binary vectors denoting notes active current timestep. mean-centered input data modeled output factorized bernoulli variables. used units hidden state latent state size polyphonic music models except chorales models used units. report bounds average log-likelihood timestep table models trained fivo bound signiﬁcantly outperformed models trained either elbo iwae bounds four datasets. cases improvements exceeded timestep cases optimizing fivo outperformed optimizing iwae elbo timit dataset standard benchmark sequential models contains utterances average duration seconds spoken different speakers. utterances divided training size test size divided training validation size training size splits exactly timit utterance represented sequence real-valued amplitudes split sequence -dimensional frames data preprocessing limited mean centering variance normalization timit output distribution factorized gaussian report average log-likelihood bound sequence relative models trained elbo. again models trained fivo signiﬁcantly outperformed models trained iwae elbo table models work trained gradients include term comes resampling steps. omitted term outsized effect gradient variance often increasing orders magnitude. explore effects term experimentally trained vrnns without resampling gradient term timit polyphonic music datasets. using resampling term attempted control variance table train marginal log-likelihood bounds models comparing smoothing nonsmoothing variational posteriors. report max{lliwae elbo iwae models lfivo fivo models. models trained pianoroll results nats timestep timit results nats sequence relative non-smoothing elbo. details evaluation methodology absolute numbers appendix. using moving-average baseline linear number timesteps. datasets models trained without resampling gradient term outperformed models trained term large margin training held-out data. many runs resampling gradients failed improve beyond random initialization. representative pair train log-likelihood curves shown figure gradients without resampling term earlier convergence better solution. stress empirical result principle biased gradients lead divergent behaviour. leave exploring strategies reduce variance unbiased estimator future work. sharpness fivo achieve marginal log-likelihood optimal variational posterior optimal condition future observations contrast elbo iwae sharp depend future observations. investigate effects this deﬁned smoothing variant vrnn takes additional input hidden state deterministic backwards observations allowing condition future observations. trained smoothing vrnns using elbo iwae fivo report evaluation training table smoothing helped models trained iwae enough outperform models trained fivo. expected smoothing reliably improve models trained fivo. test performance similar appendix details. known pathology training stochastic latent variable models elbo stochastic states unused. empirically associated collapse variational posterior network model prior investigate this plot divergence averaged dataset indeed models trained introduced family ﬁltering variational objectives class lower bounds marginal likelihood extend evidence lower bound. fivos suited neural latent variable models. trained models elbo iwae fivo bounds found models trained fivo signiﬁcantly outperformed models across four polyphonic music modeling tasks speech waveform modeling task. future work include exploring control variates resampling gradients fivos deﬁned sophisticated ﬁltering algorithms mcos based differentiable operators like leapfrog operators deterministically annealed temperatures. general hope paper inspires machine learning community take fresh look literature marginal likelihood estimators—seeing objectives instead algorithms inference. thank matt hoffman matt johnson danilo rezende jascha sohl-dickstein theophane weber helpful discussions support project. doucet partially supported epsrc grant ep/k/. teh’s research leading results received funding european research council european union’s seventh framework programme grant agreement", "year": 2017}