{"title": "Sequence-Level Knowledge Distillation", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight pruning on top of knowledge distillation results in a student model that has 13 times fewer parameters than the original teacher model, with a decrease of 0.4 BLEU.", "text": "neural machine translation offers novel alternative formulation translation potentially simpler statistical approaches. however reach competitive performance models need exceedingly large. paper consider applying knowledge distillation approaches proven successful reducing size neural models domains problem nmt. demonstrate standard knowledge distillation applied word-level prediction effective also introduce novel sequence-level versions knowledge distillation improve performance somewhat surprisingly seem eliminate need beam search best student model runs times faster state-of-the-art teacher little loss performance. also signiﬁcantly better baseline model trained without knowledge distillation bleu greedy decoding/beam search. applying weight pruning knowledge distillation results student model fewer parameters original teacher model decrease bleu. proaches. systems directly model probability next word target sentence simply conditioning recurrent neural network source sentence previously generated target words. simple surprisingly accurate systems typically need high capacity order perform well sutskever used -layer lstm hidden units layer zhou obtained state-of-the-art results english french -layer lstm units layer. sheer size models requires cutting-edge hardware training makes using models standard setups challenging. issue excessively large networks observed several domains much focus fully-connected convolutional networks multi-class classiﬁcation. researchers particularly noted large networks seem necessary training learn redundant representations process therefore compressing deep models smaller networks active area research. deep learning systems obtain better results tasks compression also becomes important practical issue applications running deep learning models speech translation locally cell phones. existing compression methods generally fall categories pruning knowledge distillation. pruning methods zero-out weights entire neurons based importance criterion lecun hessian identify weights whose removal minimally impacts objective function remove weights based thresholding absolute values. knowledge distillation approaches learn smaller student network mimic original teacher network minimizing loss student teacher output. work investigate knowledge distillation context neural machine translation. note differs previous work mainly explored non-recurrent models multiclass prediction setting. model trained multi-class prediction word-level tasked predicting complete sequence outputs conditioned previous decisions. difference mind experiment standard knowledge distillation also propose versions approach attempt approximately match sequence-level distribution teacher network. sequence-level approximation leads simple training procedure wherein student network trained newly generated dataset result running beam search teacher network. experiments compress large state-ofthe-art lstm model sequence-level knowledge distillation able learn lstm roughly matches performance full system. similar results compressing model smaller data set. furthermore observe proposed approach beneﬁts requiring beam search test-time. result able perform greedy decoding model times faster beam search model comparable performance. student models even efﬁciently standard smartphone. finally apply weight pruning student network obtain model fewer parameters original teacher model. released code models described paper. sequence-to-sequence attention source/target sentence respectively source/target lengths. machine translation involves ﬁnding probable target sentence given source possible sequences. models parameterize encoder neural network reads source sentence decoder neural network produces distribution target sentence given source. employ attentional architecture luong achieved state-ofthe-art results english german translation. knowledge distillation knowledge distillation describes class methods training smaller student network perform better learning larger teacher network generally assume teacher previously trained estimating parameters student. knowledge distillation suggests training matching student’s predictions teacher’s predictions. classiﬁcation usually means matching probabilities either scale crossentropy figure overview different knowledge distillation approaches. word-level knowledge distillation cross-entropy minimized student/teacher distributions word actual target sequence well student distribution degenerate data distribution probabilitiy mass word sequence-level knowledge distillation student network trained output beam search teacher network highest score sequence-level interpolation student trained output beam search teacher network highest target sequence large sizes neural machine translation systems make ideal candidate knowledge distillation approaches. section explore three different ways technique applied nmt. word-level knowledge distillation systems trained directly minimize word lword-nll position. therefore teacher model standard knowledge distillation multi-class cross-entropy applied. deﬁne distillation sentence objective seen minimizing crossentropy degenerate data distribution model distribution knowledge distillation assume access learned teacher distribution possibly trained data set. instead minimizing cross-entropy observed data instead minimize cross-entropy teacher’s probability distribution parameterizes teacher distribution remains ﬁxed. note cross-entropy setup identical target distribution longer sparse distribution. training attractive since gives information classes given data point less variance gradients sequence-level objective worthwhile. gives teacher chance assign probabilities complete sequences therefore transfer broader range knowledge. thus consider approximation objective. using mode seems like poor approximation teacher distribution approximating exponentially-sized distribution single sample. however previous results showing effectiveness beam search decoding lead belief large portion mass lies single output sequence. fact experiments beam size accounts distribution german english thai english summarize sequence-level knowledge distillation suggests train teacher model beam search training model train student network cross-entropy dataset. step identical word-level process except newly-generated data set. shown figure k-best list beam search. would increase training factor beam size captures distribution german english thai english. another alternative monte carlo estimate sample teacher model however practice found mode work well. sequence-level knowledge distillation word-level knowledge distillation allows transfer local word distributions. ideally however would like student model mimic teacher’s actions sequence-level. sequence distribution particularly important wrong predictions propagate forward testtime. consider case sequence-level knowledge distillation. before simply replace distribution data probability distribution derived teacher model. however instead using single word prediction represent teacher’s sequence distribution sample space possible sequences justify training knowledge distillation perspective following generative process suppose true target sequence ﬁrst generated underlying data distribution suppose target sequence observe noisy version unobserved true sequence i.e. example noise function independently replaces element random element small probability. case ideally student’s distribution match mixture distribution setting noise assumption signiﬁcant probability mass around neighborhood therefore argmax mixture distribution likely something natural approximation argmax mixture distribution illustrate framework figure visualize distribution real example figure english-german data comes training sentences take newstest/newstest newstest test set. keep frequent words replace rest unk. teacher model lstm train student models thai-english data comes iwslt sentences while employ simple noise function illustrative purposes generative story quite plausible consider elaborate noise function includes additional sources noise phrase reordering replacement words synonyms etc. could view translation having sources variance modeled separately variance source sentence variance individual translator sequence-level interpolation next consider integrating training data back process train student model mixture sequence-level teachergenerated data original training data lseq-nll αlseq-kd train observed teachergenerated data. however process nonideal reasons unlike standard knowledge distribution doubles size training data requires training teachergenerated sequence true sequence conditioned source input. latter concern particularly problematic since observe often quite different. alternative propose single-sequence approximation attractive setting. approach inspired local updating method discriminative training statistical machine translation local updating suggests selecting training sequence close high probability teacher model adopt ﬁne-tuning approach begin training pretrained model train smaller learning rate english-german generate seqinter data smaller portion training efﬁciency. methods complementary combined other. example train teacher-generated data still include word-level cross-entropy term teacher/student ﬁne-tune towards seq-inter data starting baseline model trained original data results experiments shown table word-level knowledge distillation improve upon baseline sequence-level knowledge distillation better english german performs similarly thai english. combining results gains models indicating methods provide orthogonal means transferring knowledge teacher student word-kd transferring knowledge local level seq-kd transferring knowledge global level. sequence-level interpolation addition improving models trained word-kd seq-kd also improves upon original teacher model trained actual data ﬁnetuned towards seq-inter data fact greedy decoding ﬁne-tuned model similar performance beam search original model allowing faster decoding even identically-sized model. hypothesize sequence-level knowledge distillation effective allows student network model relevant parts teacher distribution instead ‘wasting’ parameters trying model entire instance ‘seq-kd seq-inter word-kd’ table means model trained seq-kd data ﬁnetuned towards seq-inter data mixture cross-entropy loss word-level. figure visualization sequence-level interpolation example german english sentence tage anreise sind zimmer-annullationen kostenlos. beam search plot ﬁnal hidden state hypotheses using t-sne show corresponding probabilities contours. example sentence beam beam search quite away gold train model sentence beam highest gold word-level knowledge distillation student trained original data additionally trained minimize cross-entropy teacher distribution word-level. tested found work better. sequence-level knowledge distillation student trained teacher-generated data result running beam search taking highest-scoring sequence teacher model. beam size table results english-german thai-english test sets. bleuk= bleu score beam size bleu gain baseline model without knowledge distillation greedy decoding; bleuk= bleu score beam size bleu gain baseline model without knowledge distillation beam size perplexity test set; probability output sequence greedy decoding params number parameters model. best results within category highlighted bold. space translations. results suggest indeed case probability mass seqkd models assign approximate mode much higher case baseline models trained original data example english german argmax seq-kd model accounts total probability mass corresponding number baseline. also explains success greedy decoding seq-kd models—since modeling around teacher’s mode student’s distribution peaked therefore argmax much easier ﬁnd. seq-inter offers compromise between greedily-decoded sequence accounting distribution. higher bleu results indicate necessarily case. perplexity baseline english german model perplexity corresponding seq-kd model despite fact seq-kd model signiﬁcantly better greedy beam search decoding. decoding speed run-time complexity beam search grows linearly beam size. therefore fact sequencelevel knowledge distillation allows greedy decoding signiﬁcant practical implications running systems across various devices. test speed gains teacher/student models smartphone check average number source words translated second geforce titan samsung galaxy smartphone. student model times faster greedy decoding teacher model beam search similar performance. weight pruning although knowledge distillation enables training faster models number parameters student models still somewhat large word embeddings dominate parameters. example table performance student models varying weights pruned. rows models without pruning. params number parameters model; prune percentage weights pruned based absolute values; bleu bleu score beam search decoding retraining pruned model; ratio ratio number parameters versus original teacher model english german model word embeddings account approximately parameters. size word embeddings little impact run-time word embedding layer simple lookup table affects ﬁrst layer model. therefore focus next reducing memory footprint student models weight pruning. weight pruning recently investigated found parameters large model pruned little loss performance. take best english german student model prune parameters removing weights lowest absolute values. retrain pruned model seq-kd data learning rate ﬁne-tune towards seq-inter data learning rate observed retraining proved crucial. results shown table ﬁndings suggest compression beneﬁts achieved weight pruning knowledge distillation orthogonal. pruning weight student model results model fewer parameters original teacher model decrease bleu. pruning weights results appreciable decrease bleu model models trained word-level knowledge distillation also tried regressing student network’s top-most hidden layer time step teacher network’s top-most hidden layer pretraining step noting romero obtained improvements similar technique feed-forward models. found give comparable results standard knowledge distillation hence pursue further. promising recent results eliminating word embeddings completely obtaining word representations directly characters character composition models many fewer parameters word embedding lookup tables combining methods knowledge distillation/pruning reduce memory footprint systems remains avenue future work. student/teacher network pretraining step obtain smaller word embeddings teacher model regression. also work transferring knowledge across different network architectures chan show deep non-recurrent neural network learn rnn; geras train mimic lstm speech recognition. kuncoro recently investigated knowledge distillation structured prediction single parser learn ensemble parsers. approaches compression involve rank factorizations weight matrices sparsity-inducing regularizers binarization weights weight sharing finally although motivated sequence-level knowledge distillation context training smaller model techniques train mixture model’s predictions data local updating hope/fear training searn dagger minimum risk training compressing deep learning models active area current research. pruning methods involve pruning weights entire neurons/nodes based criterion. lecun prune weights based approximation hessian show simple magnitude-based pruning works well. prior work removing neurons/nodes include srinivas babu mariet ﬁrst apply pruning neural machine translation observing different parts architecture admit different levels pruning. knowledge distillation approaches train smaller student model mimic larger teacher model minimizing loss between teacher/student predictions romero additionally regress intermediate hidden layers work investigated existing knowledge distillation methods introduced sequence-level variants knowledge distillation provide improvements standard word-level knowledge distillation. chosen focus translation domain generally required largest capacity deep learning models sequence-to-sequence framework successfully applied wide range tasks including parsing summarization dialogue ner/pos-tagging image captioning video generation speech recognition anticipate methods described paper used similarly train smaller models domains.", "year": 2016}