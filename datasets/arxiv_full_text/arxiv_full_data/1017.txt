{"title": "On Optimality Conditions for Auto-Encoder Signal Recovery", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Auto-Encoders are unsupervised models that aim to learn patterns from observed data by minimizing a reconstruction cost. The useful representations learned are often found to be sparse and distributed. On the other hand, compressed sensing and sparse coding assume a data generating process, where the observed data is generated from some true latent signal source, and try to recover the corresponding signal from measurements. Looking at auto-encoders from this \\textit{signal recovery perspective} enables us to have a more coherent view of these techniques. In this paper, in particular, we show that the \\textit{true} hidden representation can be approximately recovered if the weight matrices are highly incoherent with unit $ \\ell^{2} $ row length and the bias vectors takes the value (approximately) equal to the negative of the data mean. The recovery also becomes more and more accurate as the sparsity in hidden signals increases. Additionally, we empirically demonstrate that auto-encoders are capable of recovering the data generating dictionary when only data samples are given.", "text": "auto-encoders unsupervised models learn patterns observed data minimizing reconstruction cost. useful representations learned often found sparse distributed. hand compressed sensing sparse coding assume data generating process observed data generated true latent signal source recover corresponding signal measurements. looking auto-encoders signal recovery perspective enables coherent view techniques. paper particular show true hidden representation approximately recovered weight matrices highly incoherent unit length bias vectors takes value equal negative data mean. recovery also becomes accurate sparsity hidden signals increases. additionally empirically also demonstrate auto-encoders capable recovering data generating dictionary data samples given. recovering hidden signal measurement vectors long studied problem compressed sensing sparse coding successful applications. hand autoencoders useful unsupervised representation learning uncovering patterns data. focus learning mapping reconstructed vector desired close possible entire data distribution. show paper consider actually generated true sparse signal process switching perspective analyze shows capable recovering true signal generated data yields useful insights optimality model parameters auto-encoders terms signal recovery. words perspective lets look signal recovery point view forward propagating recovers true signal analyze conditions encoder part recovers true decoder part acts data generating process. main result shows true sparse signal approximately recovered encoder high probability certain conditions weight matrix bias vectors. additionally empirically show practical setting data observed optimizing objective leads recovery data generating dictionary true sparse signal together well studied auto-encoder framework best knowledge. known empirically theoretically useful features learned usually sparse important question hasn’t answered whether capable recovering sparse signals general. important question sparse coding entails recovering sparsest approximately satisﬁes given data vector overcomplete weight matrix however since problem complete usually relaxed solving expensive optimization problem rm×n ﬁxed overcomplete dictionary regularization coefﬁcient data signal recover. special case makhzani frey analyzed condition linear recover support hidden signal. general objective hand minimizes expected reconstruction cost reconstruction cost encoding decoding activation function bias vectors paper consider linear activation general case. notice however case auto-encoders activation functions non-linear general contrast sparse coding objective. addition case separate parameter hidden representation corresponding every data sample individually. instead hidden representation every sample parametric function sample itself. important distinction optimization problem identity well deﬁned presence regularization overcompleteness dictionary. however problem assume true signal generates observed data dictionary bias vector ﬁxed. hence mean recovery sparse signals framework generate data using generation process estimate indeed recover true activation functions bias vector properties lead good recovery? however given true overcomplete solution unique. question arises possibility recovering however show recovery using mechanism strongest signal sparsest possible compressed sensing theory guarantees uniqueness sufﬁciently incoherent observed data bias vector noise vector rm×n weight matrix true hidden representation want recover. throughout analysis assume signal belongs following class distribution assumption bounded independent non-negative sparse every hidden unit independent random variable following density function arbitrary normalized distribution bounded interval mean dirac delta function zero. short hand follows distribution bins. notice pjµhj distribution assumption naturally sparse coding intended signal non-negative sparse. perspective also justiﬁed based following observation. neural networks relu activations hidden unit pre-activations gaussian like symmetric distribution assume distributions mean centered hidden units’ distribution relu large mass rest mass concentrates ﬁnite positive lmax pre-activations concentrate symmetrically around zero. show next section relu indeed capable recovering signals. side note distribution assumption take shapes similar exponential rectiﬁed gaussian distribution simpler analyze. allow arbitrary normalized distribution. restriction assumption bounded. however change representative power distribution signiﬁcantly because distributions used modeling neurons small tail mass; practice generally interested signals upper bounded values. generation process considered section justiﬁed because data generation model ﬁnds applications number areas notice measurement vector general noisy denotes actual signal reﬂects combination dictionary atoms involved generating observed samples hence serves true identity data. sparse distributed representation observed desired hidden representations. empirically shown representations truly sparse distributed usually yield better linear separability performance decoding bias consider data generation process bias vector take arbitrary value similar ﬁxed particular data generation process. however following remark shows recover sparse code data sample generated also capable recovering sparse code data generated vice versa. remark rm×n ﬁxed vector. thus without loss generality assume data generated analyse separate class signals category– continuous sparse binary sparse signals follow bins. notational convenience drop subscript simply refer parameter since bias vector auto-encoder signal recovery mechanism analyze throughout paper deﬁned deﬁnition data sample generated process rm×n ﬁxed matrix noise deﬁne auto-encoder signal recovery mechanism ˆhse recovers estimate activation function. binary sparse signal analysis first consider noiseless case data generation theorem element follow bins auto-encoder signal recovery mechanism sigmoid activation function matrix cast column vector. analysis ﬁrst analyse properties weight matrix results strong recovery bound. notice terms piaii) aii) need large possible needs close zero possible. sake analysis lets problem gets reduced maximizing ratio angle property coherence rows weight matrix highly incoherent close again ease analysis lets replace small positive number j=j=iwj/wi finally since would want term maximized hidden unit equally obvious choice weight length finally lets analyse bias vector. notice instantiated element encoding bias aijpj. since essentially mean binary hidden unit state recovery bound noisy data generation scenario. proposition element follow bins auto-encoder signal recovery mechanism sigmoid activation function bias measurement vector noise vector independent matrix cast column vector. assumed distribution noise random variable term effect recovery noise distribution orthogonal hidden weight vectors. again properties lead better recovery noiseless case. notice thus expression expanding bias unaffected error statistics long compute data mean. section ﬁrst consider case data generated linear process encoding bias certain properties signal recovery bound strong. consider case data generated non-linear process recovered well mechanism. deep non-linear networks means forward propagating data hidden layers network parameters satisfy required conditions implies hidden layer recovers true signal generated corresponding data. moved proofs appendix better readability. continuous sparse signal recovery theorem element follow bins distribution ˆhrelu auto-encoder signal recovery mechanism rectiﬁed linear activation function bias measurement vector matrix cast column vector. analysis ﬁrst analyze properties weight matrix results strong recovery bound. max) max) large possible simultaneously maxj needs close zero possible. first notice term since lmaxj deﬁnition terms containing always positive contributes towards stronger recovery less becomes stronger signal becomes sparser assume rows weight matrix highly incoherent unit length safe assume close deﬁnition properties assumed. small positive value approximately close zero. argument holds similarly term. thus strong signal recovery bound would obtained weight matrix highly incoherent hidden vectors unit length. aijpjµhj notice aijehj expanding state recovery bound noisy data generation scenario. proposition element follow bins distribution ˆhrelu auto-encoder signal recovery mechanism rectiﬁed linear activation function bias measurement vector noise random vector independent matrix cast column vector. notice assumed distribution variable denotes noise. also term effect recovery noise distribution orthogonal hidden weight vectors. hand properties lead better recovery noiseless case. however case bias element bias deﬁnition bins pjµhj thus ehi. thus expression bias unaffected error statistics since data observe results hidden signal given would interesting analyze distribution generated data. would provide insight kind pre-processing would ensure stronger signal recovery. theorem data generated covariance matrix diag rm×n unit length rows maximally incoherent covariance matrix generated data approximately spherical satisfying data generated using maximally incoherent dictionary length) guarantees highly uncorrelated uncorrelated near identity covariance. would ensure hidden units following layer also uncorrelated training. covariance matrix identity hidden units equal variance. analysis acts justiﬁcation data whitening data processed zero mean identity covariance matrix. notice although generated data zero mean recovery process subtracts data mean hence affect recovery. connections existing work auto-encoders analysis reveals conditions parameters lead strong recovery ultimately implies data reconstruction error. however arguments hold recovery point view. training data lead learning identity function. thus usually trained along bottle-neck make learned representation useful. bottle-neck de-noising criteria given constrain lengths weight vectors ﬁxed length regularization term minimizes weighted cosine angle every pair weight vectors. result weight vectors become increasingly incoherent. hence achieve goals adding additional constraint dae– constraining weight vectors unit length. even apply explicit constraint expect weight lengths upper bounded basic objective itself would explain learning incoherent weights regularization.on side note analysis also justiﬁes tied weights auto-encoders. sparse coding involves minimizing using sparsest possible analysis theorem shows signal recovery using mechanism becomes stronger sparser signals words given data sample weight matrix long conditions weight matrix bias recovery mechanism recovers sparsest possible signal; justiﬁes using auto-encoders recovering sparse codes independent component analysis assumes observe data generated process elements independent mixing matrix. task recover given data. data generating process precisely assumed section based assumption results show properties recover independent signals auto-encoders used recovering signals weight matrix k-sparse makhzani frey propose zero values hidden units smaller top-k values sample training. done achieve sparsity learned hidden representation. strategy justiﬁed perspective analysis well. bound derived signal recovery using signal recovery mechanism shows recover noisy version true sparse signal. since noise recovered signal unit roughly proportional original value de-noising recovered signals achieved thresholding hidden unit values done either using ﬁxed threshold picking values. data whitening theorem shows data generated bins incoherent weight matrices roughly uncorrelated. thus recovering back signals using auto-encoders would easier pre-process data uncorrelated dimensions. empirically verify fundamental predictions made section serve justify assumptions made well conﬁrm results. verify following optimality rows weight matrix unit length highly incoherent signal recovery; effect sparsity signal recovery; practice recover true sparse signal also dictionary used generate data. optimal properties weights bias analysis signal recovery section shows signal recovery bound strong data generating weight matrix rows unit length; rows highly incoherent; bias vector element negative expectation pre-activation; signal dimension independent. order verify this generate signals bins uniform distribution simplicity. generate corresponding data sample using incoherent weight matrix length rescaled notice rows cannot orthogonal). recover signal using scalars vary respectively. also generate signals bins dirac delta function generate corresponding data sample following procedure continuous signal case. signal recovered using penalized error weighted sparse error also achieved equally. specially needed case trivially setting recovered zero. along incoherent weight matrix also generate data separately using highly coherent weight matrix sampling element randomly uniform distribution scaling unit length. according analysis least error incoherent matrix coherent matrix yield higher recovery error different choice error heat maps continuous binary recovery shown incoherent weight matrix empirical optimal precisely apre continuous binary recovery respectively. interesting note binary recovery quite robust choice recovery denoised thresholding binary signal inherently contains less information thus easier recover. coherent weight matrix apre instead also experiment noisy recovery case generate data using incoherent weight matrix data dimension independent gaussian noise mean standard deviation varying signal recovery schemes quite robust noise particular binary signal recovery robust conforms previous observation. effect sparsity signal recovery analyze effect sparsity signals recovery using mechanism shown section order generate incoherent matrices using different methods– gaussian orthogonal addition generated weight matrices normalized unit length. sample signals generate data using conﬁgurations mentioned section time vary hidden unit activation probability duplicate generated data adding noise copy sample gaussian distribution mean standard deviation according analysis noise mean effect recovery mean value shouldn’t effect; standard deviation affects recovery. weight matrices recovery error reduces increasing sparsity additionally recovery robust noise. also recovery error trend almost always lower orthogonal weight matrices especially signal sparse. recall theorem suggests stronger recovery incoherent matrices. look coherence rm×n sampled gaussian orthogonal methods varying found orthogonal initialized matrices signiﬁcantly lower coherence even though orthogonalization done column-wise explains signiﬁcantly lower recovery error orthogonal matrices ﬁgure figure effect signal sparseness continuous binary signal recovery. noise parenthesis indicate generated data corrupted gaussian noise. sparser signals recovered better. recovery data dictionary showed conditions good recovery sparse signal practice however access general. therefore section empirically demonstrate indeed recover optimizing objective. generate signals bins distribution section data generate using incoherent weight matrix recover data dictionary notice although given sparse signal data dictionary unique number equivalent solutions since permute dimension check original data dictionary recovered therefore pair rows greedily select pairs result highest product value. measure goodness recovery looking values paired products. addition since know pairing calculate apre evaluate quality recovered hidden signal. observed optimizing objective recover original data dictionary ﬁnal achieved apre continuous binary signal recovery less achieved section however note experiments observed data information regarding exposed. surprisingly observed binary signal recovery robust compared continuous counterpart attribute lower information content. also experiments noisy data achieved similar performance section noise less signiﬁcant results strongly suggests capable recovering true hidden signal practice. paper looked sparse signal recovery problem auto-encoder perspective provide novel insights conditions recover signals. particular signal recovery stand point assume observed data generated sparse hidden signals according assumed data generating process then true hidden representation approximately recovered weight matrices highly incoherent unit length bias vectors described equation recovery also becomes accurate increasing sparsity hidden signals. data generation perspective found data generated signals property roughly uncorrelated thus pre-process data uncorrelated dimensions encourage stronger signal recovery. given measurement data empirically show reconstruction objective recovers data generating dictionary hence true signal conditions observations allow view various existing techniques data whitening independent component analysis etc. coherent picture considering signal recovery. emmanuel candes terence tao. near-optimal signal recovery random projections universal encoding strategies? information theory ieee transactions emmanuel candes justin romberg terence tao. stable signal recovery incomplete inaccurate measurements. communications pure applied mathematics xavier glorot yoshua bengio. understanding difﬁculty training deep feedforward neural networks. international conference artiﬁcial intelligence statistics pages sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. francis bach david blei editors icml volume jmlr proceedings pages jmlr.org vinod nair geoffrey hinton. rectiﬁed linear units improve restricted boltzmann machines. proceedings international conference machine learning pages proofs remark rm×n ﬁxed vector. proof thus hand direction proved similarly. theorem element follow bins auto-encoder signal recovery mechanism sigmoid activation function bias measurement vector since bound holds applying union bound units yields desired result. proposition element follow bins autoencoder signal recovery mechanism sigmoid activation function bias measurement since inequality becomes identical equation rest proof similar theorem theorem element follow bins distribution hrelu auto-encoder signal recovery mechanism rectiﬁed linear activation function bias measurement vector proposition element follow bins distribution auto-encoder signal recovery mechanism rectiﬁed linear activation function bias measurement vector noise random vector theorem data generated covariance matrix diag rm×n unit length rows maximally incoherent covariance matrix generated data approximately spherical satisfying show recovery error signals generated coherent weight matrix expected recovery result poor values unpredictable. minimum average percentage recovery error continous signal binary signal noisy signal recovery independent gaussian noise data mean standard deviation ranging note data normally within range noise quite signiﬁcant standard deviation clear even noisy case recover dictionary however recovery strong noise large continous signals precise value case continous thus inﬂuenced noise dictionary recovery poor result poor signal recovery. hand recovery robust case recovering binary signals. similar results found apre recovered hidden signals. reason robust recovery binary signal information content lower binarize recovered hidden signal thresholding denoised recovery. optimizing objective binary singal recovery case small trick simulate binarization signal. analysis recovery error reasonable binarize recovery using threshold. however optimizing using gradient based method unable this. simulate effect offset pre-activation constant multiply pre-activation constant signiﬁes input push post-activation values towards words optimize following objective binary signal recovery figure cosine similarity greedy paired rows noisy binary continous recovery. left right noise stand deviations respectively. upper lower represent percentile.", "year": 2016}