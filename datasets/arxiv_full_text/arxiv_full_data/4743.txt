{"title": "Distributed Non-Stochastic Experts", "tag": ["cs.LG", "cs.AI"], "abstract": "We consider the online distributed non-stochastic experts problem, where the distributed system consists of one coordinator node that is connected to $k$ sites, and the sites are required to communicate with each other via the coordinator. At each time-step $t$, one of the $k$ site nodes has to pick an expert from the set ${1, ..., n}$, and the same site receives information about payoffs of all experts for that round. The goal of the distributed system is to minimize regret at time horizon $T$, while simultaneously keeping communication to a minimum.  The two extreme solutions to this problem are: (i) Full communication: This essentially simulates the non-distributed setting to obtain the optimal $O(\\sqrt{\\log(n)T})$ regret bound at the cost of $T$ communication. (ii) No communication: Each site runs an independent copy : the regret is $O(\\sqrt{log(n)kT})$ and the communication is 0. This paper shows the difficulty of simultaneously achieving regret asymptotically better than $\\sqrt{kT}$ and communication better than $T$. We give a novel algorithm that for an oblivious adversary achieves a non-trivial trade-off: regret $O(\\sqrt{k^{5(1+\\epsilon)/6} T})$ and communication $O(T/k^{\\epsilon})$, for any value of $\\epsilon \\in (0, 1/5)$. We also consider a variant of the model, where the coordinator picks the expert. In this model, we show that the label-efficient forecaster of Cesa-Bianchi et al. (2005) already gives us strategy that is near optimal in regret vs communication trade-off.", "text": "consider online distributed non-stochastic experts problem distributed system consists coordinator node connected sites sites required communicate coordinator. time-step site payoﬀs experts round. goal distributed system minimize regret time horizon simultaneously keeping communication minimum. extreme solutions problem full communication essentially simulates ously achieving regret asymptotically better communication better give novel algorithm oblivious adversary achieves non-trivial trade-oﬀ regret already gives strategy near optimal regret communication trade-oﬀ. paper consider well-studied non-stochastic expert problem distributed setting. standard setting total experts available decisionmaker consult round must choose follow advice experts round observes payoﬀ vector denotes payoﬀ would received following advice expert payoﬀ received decision-maker non-stochastic setting adversary decides payoﬀ vectors time step. rounds regret decision maker diﬀerence payoﬀ would received using ∗this research carried author harvard university supported part grant nsf-ccfsingle best expert times hindsight payoﬀ actually received i.e. goal minimize regret; general maxa∈ problem non-stochastic setting captures several applications interest experiment design online ad-selection portfolio optimization etc. references therein.) tight bounds regret non-stochastic expert problem obtained so-called follow regularized leader approaches; time decision-maker chooses distribution consider setting decision maker distributed system several diﬀerent nodes select experts and/or observe payoﬀs diﬀerent time-steps. settings common e.g. internet search companies google bing several nodes answer search queries performance revealed user clicks. point view making better predictions useful pool available data. however involve signiﬁcant communication quite costly. thus obvious trade-oﬀ cost communication cost inaccuracy leads question consider distributed computation model consisting central coordinator node connected site nodes. site nodes must communicate using coordinator node. time step distributed system receives query indicates must choose expert follow. round distributed system observes payoﬀ vector. consider diﬀerent models described detail below site prediction model sites receives query given time-step coordinator prediction model query always received coordinator node. models payoﬀ vector always observed site nodes. thus communication required share information payoﬀ vectors among nodes. shall models yield diﬀerent algorithms performance bounds. goal algorithm implemented distributed system randomness decide expert pick decide communicate nodes. focus simultaneously minimizing expected regret expected communication used algorithm. recall expected regret word query sense explicitly giving information context merely indication occurrence event forces site coordinator choose expert. particular context provided query algorithms considered paper ignore context thus non-contextual expert setting. show paper challenging problem keep analysis simple focus bounds terms number sites time horizon often important scaling parameters. particular algorithms variants follow perturbed leader hence bounds optimal terms number experts believe dependence number experts algorithms strengthened using diﬀerent regularizer. also lower bounds shown terms larger using techniques similar theorem give appropriate dependence adversaries non-stochastic setting assume adversary decide payoﬀ vectors time-step also site receives payoﬀ vector oblivious adversary cannot actions distributed system i.e. selection expert communication patterns random bits used. however oblivious adversary know description algorithm. addition knowing description algorithm adaptive adversary stronger record past actions algorithm arbitrarily decide future payoﬀ vectors site allocations. communication explicitly account message sizes. however since interested scaling require message size depend number sites number time-steps number experts words assume substantially smaller messages used algorithms contain real numbers. standard distributed systems literature assume communication delay i.e. updates sent node received recipients future query arrives. results still hold weaker assumption number queries received distributed system duration required complete broadcast negligible compared site prediction model time step sites receives query pick expert payoﬀ vector payoﬀ expert revealed site decision-maker receives payoﬀ corresponding expert actually chosen. site prediction model commonly studied distributed machine learning settings payoﬀ vectors also choice sites receive query decided adversary. simple algorithms model communication site maintains cumulative payoﬀ vectors corresponding queries received them thus implementing independent versions fpl. suppose site using communication asymptotically lower turns signiﬁcantly challenging question. main positive result ﬁrst distributed expert algorithm oblivious adversarial setting using sub-linear communication. finding algorithm case adaptive adversary interesting open problem. coordinator prediction model every time step query received coordinator node chooses expert however round site nodes observes payoﬀ vector payoﬀ vectors choice sites decided adversary. model also natural explored distributed systems streaming literature references therein). full communication protocol equally applicable getting optimal regret bound cost substantial communication. here decision-maker limited budget spend part budget observe payoﬀ information. optimal strategy request payoﬀ information randomly probability time-step communication budget. refer algorithm crucial diﬀerences model label-eﬃcient setting communication occur site send cumulative payoﬀ vectors comprising previous updates coordinator rather latest one. diﬀerence that unlike label-eﬃcient case sites knowledge local regrets decide communicate. however lower bounds natural types algorithms show advantages probably help better guarantees. case oblivious adversary results weaker show certain natural types algorithms applicable directly setting. called regularized leader algorithms maintain cumulative payoﬀ vector regularizer select expert time consider variants distributed setting distributed counter algorithms forecaster uses version cumulative payoﬀ vector make assumptions forecaster ˜pt. maintained using sub-linear communication applying techniques distributed systems literature delayed regularized leader regularized leaders don’t explicitly maintain approximate version cumulative payoﬀ vector. instead arbitrary communication protocol make prediction using cumulative payoﬀ vector regularizer. show section distributed counter approach yield non-trivial guarantee site-prediction model even oblivious adversary. possible show similar lower bound coordinator prediction model omitted since follows easily idea site-prediction model combined explicit communication lower bound given section shows delayed regularized leader approach yield non-trivial guarantees even oblivious adversary coordinator prediction model suggesting algorithm near optimal. related work recently signiﬁcant interest distributed online learning questions however works focused mainly stochastic optimization problems. thus techniques used reducing variance mini-batching applicable setting. questions network structure network delays interesting setting well however present work focuses establishing non-trivial regret guarantees distributed online non-stochastic experts setting. study communication resource distributed learning also considered however body work seems applicable oﬄine learning. related work distributed functional monitoring particular distributed counting sketching techniques successfully applied oﬄine machine learning problems however ﬁrst analyze performance-communication trade-oﬀ online learning algorithm standard distributed functional monitoring framework application distributed counter online bayesian regression proposed lower bounds discussed below show approximate distributed counter techniques directly yield non-trivial algorithms. describe algorithm simultaneously achieves non-trivial bounds expected regret expected communication. begin making assumptions simplify exposition. first assume experts. generalization experts easy discussed remark section. second assume exists global query counter available sites co-ordinator keeps track total number queries received across sites. discuss assumption remark section. often case online algorithms assume time horizon known. otherwise standard doubling trick employed. notation used section deﬁned table algorithm description algorithm dfpl described figure make algorithm described figure takes parameter amount added noise block phase running copy across blocks expert followed entire block synchronizing block. eﬀectively makes cumulative payoﬀ block payoﬀ vector block fpl. block average total time steps. begin stating guarantee fpl. lemma consider case sequence payoﬀ vectors maxt |pt|∞ number experts following guarantee expected regret ﬁxed ahead time. without loss generality expert denotes random variable greater payoﬀ hindsight. recall regret playing fpl) step phase block respect ﬁrst expert. particular negative expert best expert block even though globally expert better. fact exactly algorithm exploits gains regret communication-expensive step phase saving communication block phase. analysis guarantees fri) denotes random variable actual regret fpl) regret respect expert case otherwise fri) note expression negative. case pi)+ putting everything together write setting parameters lemma strategy show term becomes large term also large magnitude negative compensating eﬀect term note asymptotics terms parameters getting communication form case best expert identiﬁed quickly furthermore large often. case although term large large) compensated negative regret term expression large often best expert identiﬁed quickly must enough blocks three cases exhaust possibilities hence matter nature payoﬀ sequence expected regret dfpl bounded required. expected total communication easily seen blocks step used contribute remark algorithm generalized experts recursively dividing experts applying algorithm meta-experts shown section appendix. however bound obtained section optimal terms number experts observation lemma imply theorem remark assumption global counter necessary algorithm divides input blocks size however impediment suﬃcient block sizes range assuming coordinator always signals beginning block distributed counter guarantees tight approximation number queries received block messages communicated section give lower bound distributed counter algorithms site prediction model. distributed counters allow tight approximation guarantees i.e. factor additive approximation k/β) observe noise used communication required tempting suitable using approximate quite large whenever site receives query cumulative payoﬀ expert additive accuracy furthermore communication used maintain counter. site uses cumulative payoﬀs local information choose expert queried. however negative result shows even highly accurate counter nonstochasticity payoﬀ sequence cause algorithm regret. furthermore show distributed algorithm implements counters additive error sites least co-ordinator prediction model mentioned earlier possible label-eﬃcient forecaster upper bound total amount communication allowed use. label-eﬃcient predictor translates following simple protocol whenever site receives payoﬀ vector forward particular payoﬀ coordinaobserve principle possibility better algorithms setting mainly reasons sites send payoﬀ vectors co-ordinator send cumulative payoﬀs rather latest ones thus giving information sites decided communicate function payoﬀ vectors instead randomly. however present lower-bound shows natural family algorithms achieving regret arbitrary communication protocol satisﬁes following whenever site communicates coordinator site report local cumulative payoﬀ vector. coordinator makes decision execute using latest cumulative payoﬀ vector. proof theorem appears appendix theorem consider distributed non-stochastic expert problem coordinator prediction model. algorithm kind described achieves regret back sites sites latest cumulative payoﬀ vector. whenever communication occur cost refer mini-batch similar spirit minibatch algorithms used stochastic optimization problems. algorithm distributed counter technique huang maintain cumulative payoﬀ expert. whenever counter update occurs coordinator must broadcast nodes make sure current update. consider types synthetic sequences. ﬁrst zig-zag sequence length increase/decrease. ﬁrst time steps payoﬀ vector always next time steps payoﬀ vector next time-steps payoﬀ vector zig-zag sequence also sequence used proof lower bound theorem second two-state state payoﬀ simulations predictions sites. fig. shows performance algorithms sequences results averaged across runs randomness algorithms. fig. shows worstcase cumulative communication worst-case cumulative regret trade-oﬀ three algorithms dfpl mini-batch described sequences. general hard compare algorithms non-stochastic inputs results conﬁrm non-stochastic sequences inspired lower-bounds paper algorithm dfpl outperforms related techniques.", "year": 2012}