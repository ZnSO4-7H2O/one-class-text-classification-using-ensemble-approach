{"title": "Learning to Invert: Signal Recovery via Deep Convolutional Networks", "tag": ["stat.ML", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "abstract": "The promise of compressive sensing (CS) has been offset by two significant challenges. First, real-world data is not exactly sparse in a fixed basis. Second, current high-performance recovery algorithms are slow to converge, which limits CS to either non-real-time applications or scenarios where massive back-end computing is available. In this paper, we attack both of these challenges head-on by developing a new signal recovery framework we call {\\em DeepInverse} that learns the inverse transformation from measurement vectors to signals using a {\\em deep convolutional network}. When trained on a set of representative images, the network learns both a representation for the signals (addressing challenge one) and an inverse map approximating a greedy or convex recovery algorithm (addressing challenge two). Our experiments indicate that the DeepInverse network closely approximates the solution produced by state-of-the-art CS recovery algorithms yet is hundreds of times faster in run time. The tradeoff for the ultrafast run time is a computationally intensive, off-line training procedure typical to deep networks. However, the training needs to be completed only once, which makes the approach attractive for a host of sparse recovery problems.", "text": "promise compressive sensing offset signiﬁcant challenges. first real-world data exactly sparse ﬁxed basis. second current highperformance recovery algorithms slow converge limits either non-real-time applications scenarios massive back-end computing available. paper attack challenges head-on developing signal recovery framework call deepinverse learns inverse transformation measurement vectors signals using deep convolutional network. trained representative images network learns representation signals inverse approximating greedy convex recovery algorithm experiments indicate deepinverse network closely approximates solution produced state-of-the-art recovery algorithms hundreds times faster time. tradeoff ultrafast time computationally intensive off-line training procedure typical deep networks. however training needs completed once makes approach attractive host sparse recovery problems. inverse problem many important applications recovering undersampled linear measurements measurement matrix problem ill-posed general hence order successfully recover original signal type structure dimensionality reduced without losing information. compressive sensing special case problem signal sparse representation i.e. exists basis matrix coefﬁcients promise offset signiﬁcant challenges. ﬁrst challenge real-world data exactly sparse ﬁxed basis. work pursued learning data-dependent dictionaries sparsify signals redundancy resulting approaches degrades recovery performance. second challenge current high-performance recovery algorithms slow converge limits either non-real-time applications scenarios massive back-end computing available. paper attack challenges headdeveloping signal recovery framework call deepinverse learns inverse transformation measurement vectors signals using deep convolutional network. trained representative images network learns representation signals inverse approximating greedy convex recovery algorithm experiments indicate deepinverse network closely approximates solution produced state-ofthe-art recovery algorithms hundreds times faster time. tradeoff ultrafast time computationally intensive off-line training procedure typical deep networks. however training needs completed once makes approach attractive host sparse recovery problems. ﬁrst paper study problem structured signal recovery undersampled measurements using deep learning approach work employed stacked denoising autoencoder unsupervised feature learner. main drawback approach network consists fully-connected layers meaning units consecutive layers connected other. thus signal size grows network. imposes large computational complexity weights layer paper weights implement adjoint operator second preserve dimensionality processing dispense downsampling max-pooling operations. usual paradigm assume measurement matrix ﬁxed. therefore linear combination training dtrain consists pairs signals corresponding measurements. similarly test dtest consists pairs including original signals corresponding measurements. training learn nonlinear mapping signal proxy original signal rn+k−×n+k− denote ﬁlter bias values corresponding k-th feature ﬁrst layer relu max. finally subsampling operator takes output relu original signal size ignoring borders created zero-padding input. feature maps second third convolutional layers developed similar manner. ﬁlter shapes biases could different second third layers network principles layers ﬁrst layer. denote number ﬁlters ﬁrst second third layers respectively. denote output convolutional network parameters training algorithm also lead overﬁtting. solution issue implemented divide signal smaller non-overlapping overlapping blocks sense/reconstruct block separately. approach deals curse dimensionality blocky measurement matrix unrealistic many applications. work followed used fullyconnected layer along convolutional neural networks recover signals compressive measurements. approach also used blocky measurement matrix deep convolutional networks consist three major layers ﬁrst convolutional layer core networks. layer consists learnable ﬁlters limited receptive ﬁeld replicated across entire visual ﬁeld form feature maps. second relu nonlinearity layer causes nonlinearity decision function overall network. third pooling layer form downsampling provides translation invariance. backpropagation algorithm used train whole network tune ﬁlters convolutional layers. three layers play important roles dcns’ primary application image classiﬁcation. words dcns reduces dimensionality given image series convolutional pooling layers order extract label image. authors introduced probabilistic framework provides insights success dcns image classiﬁcation task. dcns distinctive features make uniquely applicable sparse recovery problems. first sparse connectivity neurons. second shared weights across entire receptive ﬁelds layer increases learning speed comparing fully-connected networks. develop deepinverse signal recovery framework learns inverse transformation measurement vectors signals using special dcn. trained representative images network learns representation signals inverse approximating greedy convex recovery algorithm. typically accomplishing dimensionality increase requires several modiﬁcations conventional architecture. first boost dimensionality input employ fully connected linear layer. while general could learn figs. indicate deepinverse offers recovery probability psnr performance comparable state-of-the-art recovery algorithms table shows deepinverse time tiny fraction current algorithms. fact makes deepinverse especially suitable applications need low-latency recovery. table shows effect adding input noise recovery performance d-amp deepinverse. undersampling ratio input noise deepinverse robust noise comparing d-amp. finally figure shows convergence backpropagation training algorithm different iterations deepinverse. also shows average psnr images test dataset different methods several iterations deepinverse starts outperform minimization p-amp. although d-amp better performance -layer deepinverse general consider points. first training deepinverse layers network larger capacity hence expect offer better recovery performance. leave studying deepinverses larger capacities topic future work. second deepinverse specially useful applications need lowlatency recovery time able divide images smaller blocks i.e. need apply sensing matrix entire signal rather smaller blocks mentioned earlier main goals paper show deep learning framework recover images undersampled measurements without need divide images small blocks recover block separately. purpose deepinverse receives signal proxy i.e. input. addition layers following speciﬁcations. ﬁrst layer ﬁlters channel size second layer ﬁlters channels size third layer ﬁlter channels size trained deepinverse using cropped subimages natural images imagenet dataset test images drawn imagenet images used training purposes. figure shows plot average probability successful recovery different undersampling ratios three different recovery algorithms d-amp total variation minimization p-amp note include results simulation results since approaches speciﬁcally designed block-based recovery whereas paper focus recovering signals without subdivision. undersampling ratio deepinverse better performance state-of-the-art recovery methods. however undersampling ratio increases d-amp outperforms deepinverse. although figure shows every undersampling ratio method works better others clear winner terms reconstruction quality. figure compares average psnr monte carlo test samples different undersampling ratios algorithms. figure shows histograms psnrs recovered test images indicating deepinverse outperforms d-amp images test set. table effect noise average psnr reconstructed test images d-amp deepinverse added noise images test set. noise-folding variance noise observe reconstruction larger input noise. paper developed deepinverse framework sensing recovering signals. shown framework learn structured representation training data efﬁciently approximate signal recovery small fraction cost state-of-the-art recovery algorithms. duarte-carvajalino sapiro learning sense sparse signals simultaneous sensing matrix sparsifying dictionary optimization tech. rep. dtic document kulkarni lohit turaga kerviche ashok reconnet non-iterative reconstruction images compressively sensed random measurements proc. ieee int. conf. comp. vision pattern recognition davenport laska treichler baraniuk pros cons compressive sensing wideband signal acquisition noise folding versus dynamic range ieee trans. signal processing vol.", "year": 2017}