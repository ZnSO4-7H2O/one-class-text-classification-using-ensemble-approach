{"title": "TensorQuant - A Simulation Toolbox for Deep Neural Network Quantization", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Recent research implies that training and inference of deep neural networks (DNN) can be computed with low precision numerical representations of the training/test data, weights and gradients without a general loss in accuracy. The benefit of such compact representations is twofold: they allow a significant reduction of the communication bottleneck in distributed DNN training and faster neural network implementations on hardware accelerators like FPGAs. Several quantization methods have been proposed to map the original 32-bit floating point problem to low-bit representations. While most related publications validate the proposed approach on a single DNN topology, it appears to be evident, that the optimal choice of the quantization method and number of coding bits is topology dependent. To this end, there is no general theory available, which would allow users to derive the optimal quantization during the design of a DNN topology. In this paper, we present a quantization tool box for the TensorFlow framework. TensorQuant allows a transparent quantization simulation of existing DNN topologies during training and inference. TensorQuant supports generic quantization methods and allows experimental evaluation of the impact of the quantization on single layers as well as on the full topology. In a first series of experiments with TensorQuant, we show an analysis of fix-point quantizations of popular CNN topologies.", "text": "sentations particularly interesting ﬁrst custom hardware fpgas asics data storage transfer carried customized format also computation. second case distributed systems communication diﬀerent nodes becoming main bottleneck. custom data representation largely reduce amount data needs communicated thus reduce energy consumption increase throughput. common approach reduce amount data quantization mapping large continuous discrete values discrete smaller set. several quantization methods reduce amount stored transferred data neural networks. common approach quantize data clustering means conﬁning data representation discrete values special case quantization ﬁxed point representation numbers recent research implies training inference deep neural networks computed precision numerical representations training/test data weights gradients without general loss accuracy. beneﬁt compact representations twofold allow signiﬁcant reduction communication bottleneck distributed training faster neural network implementations hardware accelerators like fpgas. several quantization methods proposed original -bit ﬂoating point problem low-bit representations. related publications validate proposed approach single topology appears evident optimal choice quantization method number coding bits topology dependent. general theory available would allow users derive optimal quantization design topology. paper present quantization tool tensorflow framework. tensorquant allows transparent quantization simulation existing topologies training inference. tensorquant supports generic quantization methods allows experimental evaluation impact quantization single layers well full topology. ﬁrst series experiments tensorquant show analysis ﬁx-point quantizations popular topologies. deep neural networks suﬀer amount data needs stored transferred training inference. data usually represented ﬂoating point numbers convenient handle standard hardware. however required memory energy achieved throughput hardware depend approximately linearly number bits necessary represent data. several publications suggest ﬂoating point representation taking resources would necessary successfully train networks perform inference important cases smaller data reprediscrete distance nearest neighbours fixed point numbers popular representation custom hardware like fpgas asics. extreme case quantization binary ternary quantization bits respectively used represent value. although compression parameters high methods need gradients represented ﬂoating point training. various methods like using hashing trick logarithmic quantization etc. among methods ﬁxed point representation gained attention emergence powerful custom hardware datacenters like google’s tensor processing unit amazon services’ instances microsoft azure’s fpga-based cloud services. many publications claim rounding inevitably happens every operation custom hardware using ﬁxed point format modelled single rounding step applied layer computed ﬂoating point precision argument quantization noise introduced every rounding step transformed single source noise layer since operations linear activation function rectifying linear unit words matter point noise level increased thus rounding suﬃcient approximation. paper question whether quantization noise introduced operation close single quantization step layer answered directly simulating cases comparing other. addition location rounding rounding method also investigated. even simple rounding methods nearest rounding rounding impact network’s accuracy. quantization methods often tested simple datasets small topologies quickly trained lenet alexnet mnist cifar. results experiments generalized bigger networks datasets. here results directly obtained simulating state-of-the-art topologies inception resnet order investigate quantization dnns toolbox tensorflow called tensorquant introduced. full spectrum functions oﬀered tensorflow utilized augmented ability quantize layer fully emulate ﬁxed point format data processing. implementation could emulate custom size ﬁxed point computation common neural network simulation framework. short main contributions paper structured follows chapter introduces used methods terms. extrinsic intrinsic rounding introduced explained central role. chapter presents tensorquant toolbox explains features detail. gives overview much eﬀort user needs applying toolbox projects. toolbox used investigate ﬁxed point quantization chapter experiments carried large topologies inception resnet ﬁxed point number integer word size least signiﬁcant bits interpreted fractional portion number. word size width deﬁned number bits used store single numerical value. negative numbers saved two’s complement thus range ﬁxed point number resolution ﬁxed point number converting number ﬂoating point representation ﬁxed point causes loss accuracy. original number ﬁxed point range determined equation usual approach saturate number positive negative marginal value respectively. fractional part chosen using rounding method. commonly known nearest rounding quantization applied layer called extrinsic quantization. case layer computed ﬂoating point precision output sent next layer reduced data format. extrinsic quantization scenario encountered distributed system many computation nodes data processed ﬂoating-point precision within nodes quantized sent another node order reduce required bandwidth. example convolution layer quantization function number features coordinates within ﬁlter coordinates within feature. hand intermediate results every arithmetic operation within layer quantized. scenario found custom hardware data stored processed ﬁxed point format thus data restricted certain precision point. since type quantization applied deeper level compared extrinsic called intrinsic quantization. convolution layer example formally computed simulating intrinsic quantization needs memory time extrinsic case. reasonable intrinsic quantization trying emulate behaviour hardware. fixed point quantization reasonable method apply intrinsically simply referred rounding paper. training gradient quantized update weights applied formally described number rounded closest element ﬁnite ﬁxed point numbers resolution another round towards zero throw away fractional part number given resolution cuts binary representation number many bits corresponds regardless sign. example binary number second fractional bit. rounding method investigated often literature stochastic rounding problem apply rounding functional subunits topology. good example subunits inception modules occurring inception type topologies. instead rounding layers once rounded time. good reasons justifying approach. first accuracy loss happens quantized layer cannot regained subsequent layers. second likely various layers require diﬀerent word sizes fractional bits order keep accuracy baseline value. therefore exists several bottlenecks determine word size entire topology. subunits topology rounded another diﬀerent word sizes fractional bits. accuracy inference recorded run. subunit best combination lowest word size least fractional bits determined accuracy stays compared unquantized topology. amongst best combinations subunit highest word size identiﬁed bottleneck. several bottleneck subunits word size unit highest fractional bits chosen bottleneck. tensorquant toolbox able quantize neural network designed tensorflow intrinsically extrinsically. changes user’s original tensorflow topology description need made. also user provide speciﬁcation layers shall quantized. tensorquant manages quantization layers building running process. overview diﬀerent components tensorquant given ﬁgure quantizers core toolbox quantizer objects carry quantization tensors. simple interface takes tensor outputs quantized version. example given listing quantizer interface forces quantize method invokes quantization kernel. quantization process carried kernel written c++. possible write quantizers entirely python although case intrinsic quantization utilizes prohibitively many resources. changes user’s topology file quantizers applied node topology would laborious assign hand. tensorflow slim framework series convenience functions implemented tensorquant automate application quantizers topology. example describing topology given listing changes topology prepared quantization shown listing assigning layers quantization locations quantization applied controlled tensorflow variable namespaces. user specify entire variable name single matching substrings layers quantization applied example listing variable scope lenet contains layers conv pool ﬁrst convolution layer accessed identiﬁer lenet/conv. layers lenet scope accessed identiﬁer lenet. user speciﬁes dictionaries intrinsic extrinsic rounding. keys identiﬁers values quantizer objects. dictionaries passed layer factories. layer factories layers built factory functions. layer type factory returns function interface standard tensorflow layers. factory takes dictionaries input arguments intrinsic extrinsic quantization. pseudocode factory function given listing implementation extrinsic intrinsic quantization implementing extrinsic quantization straight forward quantization applied directly layer output. quantization independent computed layer thus applied directly layer types. intrinsic case however straight forward approach since speciﬁc operation layer needs considered. unfortunately re-implement layer type quantization applied desired calculation steps. quantizer objects previously shown listing thus additional kernels need written. re-design standard tensorflow framework used much possible. downside intermediate results need stored tensors means increase required memory. mitigate problem batch size reduced. increases adding extrinsic quantization model time build model less runtime layers contain trained parameters e.g. ﬁlter weights. proper representation ﬁxed point operations values automatically quantized intrinsic quantizer passed calculation. adding quantization interfere tensorflow namespaces. therefore model parameters loaded save ﬁles model trained without quantization. extrinsic gradient quantization independent tensorflow version. intrinsic quantization needs reimplement layers therefore incompatible versions gradient quantization gradient quantization described equation implemented easily. controlling training gradients intercepted quantizer applied passed optimizer function. tensorquant toolbox used apply ﬁxed point quantization dnns. simulations focused popular topologies inception inception resnet resnet networks trained imagenet reference also provide results lenet mnist dataset. learning experiments alexnet trained imagenet. networks trained parameters taken tensorflow model library speciﬁcally slim github page main metric test accuracy. unquantized version topology serves baseline accuracies given relative values. validation used inference simply perform design space exploration reasonable time especially case intrinsic quantization. using smaller validation valid interesting whether baseline accuracy reached plus quantization without form retraining cannot improve accuracy. figure zero down nearest stochastic rounding methods applied inception topology. dots mark relative accuracy rounding applied intrinsically triangles mark extrinsic rounding. method. deviation baseline accuracy coming word size. rounding methods lead equally good accuracies. intrinsic case choice rounding method impact accuracy. using stochastic rounding inception topology doubles required word size compared nearest rounding whereas zero rounding those. resnet topology nearest zero rounding lead similar accuracies stochastic rounding still worse methods. nearest rounding method requires least amount bits therefore best method investigated ones. used following experiments. stochastic rounding hand demanding methods. inﬂuence diﬀerent rounding methods introduced section investigated inception resnet topologies. figures show relative accuracy word width used rounding. fractional bits half word size. intrinsic extrinsic rounding plotted ﬁgure diﬀerent markers. extrinsic rounding almost aﬀected rounding intrinsic rounding subunit approach explained section used. table compares accuracies rounding layers subunit approach inception resnet relative accuracy close subunit approach method equivalent rounding entire network once results same. intrinsic rounding applied topologies table word size ﬁxed column. entries show maximum achievable relative accuracy used fractional bits. inception type topologies require less word size resnet type ones achieve full benchmark accuracy. bits enough inception whereas resnets need bits. hint diﬀerent topology types diﬀerent bottlenecks even though layer types. bottlenecks located later section conclusion comparing inception resnet type topologies amongst word size depend depth topology. attributed batch normalization layers normalize activations leave layer. range activation values kept same therefore ﬁxed point quantization working well. results rounding topologies extrinsically shown table structured intrinsic case. subunit method needed here extrinsic rounding require much memory therefore layers quantized without problems. notice word sizes columns diﬀerent. extrinsic rounding achieves baseline accuracy less word size intrinsic case. bits already enough stay close full accuracy regardless topology type. also portion fractional bits lower intrinsic case meaning output values layers transferred precision. unit method used. table results presented table before. however lenet trained mnist topologies imagenet. bits word width lenet achieves relative accuracy topologies close zero. previous results resnet needs word width inception illustrates even though three topologies cnns utilize layer types results cannot generalized topology another intrinsic case. seeing results subunit approach table actual subunit requirements regarding word size fractional bits shown tables inception resnet topology respectively. column represents threshold relative accuracy. entries state minimum required word size fractional bits threshold. units demanding are. units inception resnet topologies requirement bits word width bits fractional part layers within subunit convolution layers. however bottleneck layers network determine required word width entire hardware architecture. ﬁrst layer needs high word size topologies. inception bottlenecks appearing middle table general rule bottlenecks appear beginning topology. case distributed systems interesting well topology trained gradients communicated computation nodes quantized. before quantization method ﬁxed point nearest rounding. accuracies relative network trained exactly without gradient quantization. table shows relative test accuracies diﬀerent word size fractional bits combinations lenet table alexnet. gradient quantized quite rigorously lenet. bits suﬃcient train lenet relative accuracy mnist dataset. alexnet hand needs least fractional bits trained comparison topologies shows point generalizing results simple topoligies like lenet larger deeper networks. interesting observation made looking training loss alexnet. figure shows training loss number steps alexnet topology trained word sizes table training loss going initial phase descend fractional bits. despite training loss going test accuracy high. reason used function training loss comprises cross entropy l-regularizer. quantization renders l-regularizer ineﬀective during computation gradient case fractional bits cross entropy minimized. since weights bound regularizer magnitudes grow freely. however regularizer contributes high value overall training loss. paper investigated ﬁxed point data format similar thoroughness complex network investigated inception suggest using bits word length bits fractional part. result somewhat coincides ﬁndings extrinsic rounding bits word size fractional bits suﬃcient topology. diﬀerence results could come used framework model parameters. designers custom hardware using word size implementations topologies results suggest careful general word size guarantees suﬃcient topologies. investigated cases large enough layers. results training coincide although used extrinsic rounding instead. fractional part needs relatively high whereas integer part needed. increase training loss using nearest rounding training also observed despite using extrinsic quantization. paper stochastic rounding makes training loss converge normally. tensorquant toolbox allows explore diﬀerent quantization methods tensorflow framework. unique feature ﬁxed point data format emulated arithmetic level thus allowing closest similarity custom hardware presented framework. important result experiments case intrinsic rounding word size demanding previously thought. intrinsic rounding sensitive rounding method. possible generalize required word width topology another diﬀerent bottleneck layers. results training section show gradient quantization allows even less generalize topologies. planned extend tensorquant toolbox much further especially functionality related training. goal fully emulate learning distributed system comprising custom hardware thus using ﬁxed point data format. quantization methods implemented study possible strategies reduce bandwidth distributed systems. also tensorquant applied topologies like recurrent neural networks. expected requirements regarding data representation diﬀer cnns.", "year": 2017}