{"title": "Learning to Communicate to Solve Riddles with Deep Distributed Recurrent  Q-Networks", "tag": ["cs.AI", "cs.LG"], "abstract": "We propose deep distributed recurrent Q-networks (DDRQN), which enable teams of agents to learn to solve communication-based coordination tasks. In these tasks, the agents are not given any pre-designed communication protocol. Therefore, in order to successfully communicate, they must first automatically develop and agree upon their own communication protocol. We present empirical results on two multi-agent learning problems based on well-known riddles, demonstrating that DDRQN can successfully solve such tasks and discover elegant communication protocols to do so. To our knowledge, this is the first time deep reinforcement learning has succeeded in learning communication protocols. In addition, we present ablation experiments that confirm that each of the main components of the DDRQN architecture are critical to its success.", "text": "jakob foerster† yannis assael† nando freitas shimon whiteson university oxford united kingdom canadian institute advanced research cifar ncap program google deepmind propose deep distributed recurrent qnetworks enable teams agents learn solve communication-based coordination tasks. tasks agents given pre-designed communication protocol. therefore order successfully communicate must ﬁrst automatically develop agree upon communication protocol. present empirical results multiagent learning problems based well-known riddles demonstrating ddrqn successfully solve tasks discover elegant communication protocols knowledge ﬁrst time deep reinforcement learning succeeded learning communication protocols. addition present ablation experiments conﬁrm main components ddrqn architecture critical success. recent years advances deep learning instrumental solving number challenging reinforcement learning problems including high-dimensional robot control visual attention atari learning environment above-mentioned problems involve single learning agent. however recent work begun address multi-agent deep competitive settings deep learn†these authors contributed equally work. recently shown success. cooperative settings tampuu adapted deep q-networks allow agents tackle multi-agent extension ale. approach based independent qlearning agents learn q-functions independently parallel. however approaches assume agent fully observe state environment. also extended address partial observability single-agent settings considered. knowledge work deep reinforcement learning considered settings partially observable multi-agent. problems challenging important. cooperative case multiple agents must coordinate behaviour maximise common payoff facing uncertainty hidden state environment teammates observed thus act. problems arise naturally variety settings multi-robot systems sensor networks paper propose deep distributed recurrent qnetworks enable teams agents learn effectively coordinated policies challenging problems. show naive approach simply training independent agents long short-term memory networks inadequate multi-agent partially observable problems. therefore introduce three modiﬁcations ddrqn’s success last-action inputs supplying agent previous action input next time step agents approximate action-observation histories; inter-agent weight sharing single network’s weights used agents network conditions agent’s unique enable fast learning also allowing diverse behaviour; disabling experience replay poorly suited non-stationarity arising multiple agents learning simultaneously. evaluate ddrqn propose multi-agent reinforcement learning problems based well-known riddles hats riddle prisoners line must determine colours; switch riddle prisoners must determine visited room containing single switch. riddles used interview questions companies like google goldman sachs. environments require convolutional networks perception presence partial observability means require recurrent networks deal complex sequences single-agent works languagebased tasks. addition partial observability coupled multiple agents optimal policies critically rely communication agents. since communication protocol given priori reinforcement learning must automatically develop coordinated communication protocol. results demonstrate ddrqn successfully solve tasks outperforming baseline methods discovering elegant communication protocols along way. knowledge ﬁrst time deep reinforcement learning succeeded learning communication protocols. addition present ablation experiments conﬁrm main components ddrqn architecture critical success. single-agent fully-observable reinforcement learning setting agent observes current state discrete time step chooses action according potentially stochastic policy observes reward signal transitions state st+. objective maximize expectation discounted return here weights target network frozen number iterations updating online network gradient descent. uses experience replay learning agent builds dataset experiences across episodes. q-network trained sampling mini-batches experiences uniformly random. experience replay helps prevent divergence breaking correlations among samples. also enables reuse past experiences learning thereby reducing sample costs. extended cooperative multi-agent settings agent observes global receives team reward lects individual action shared among agents. tampuu address setting framework combines independent q-learning applied two-player pong agents independently simultaneously learn q-functions independent q-learning principle lead convergence problems strong empirical track record independent assume full observability i.e. agent receives input. contrast partially observable environments hidden instead agent receives observation correlated general disambiguate feed-forward network approximate recurrent neural network maintain internal state aggregate observations time. modelled adding extra input represents hidden state network yielding thus drqn outputs time step. drqn tested partially observable version portion input screens blanked out. work consider settings multiple agents partial observability agent receives private time step maintains internal state however assume learning occur centralised fashion i.e. agents share parameters etc. learning long policies learn condition private histories. words consider centralised learning decentralised policies. interested settings multiple agents partial observability coexist agents incentive communicate. communication protocol given priori agents must ﬁrst automatically develop agree upon protocol. knowledge work deep considered settings work demonstrated deep successfully learn communication protocols. straightforward approach deep partially observable multi-agent settings simply combine drqn independent q-learning case agent’s q-network represents conditions agent’s individual hidden state well observation. approach call naive method performs poorly show section instead propose deep distributed recurrent q-networks makes three modiﬁcations naive method. ﬁrst last-action input involves providing agent previous action input next time step. since agents employ stochastic policies sake exploration general condition actions action-observation histories observation histories. feeding last action input allows approximate action-observation histories. second inter-agent weight sharing involves tying weights agents networks. effect network learned used agents. however agents still behave differently receive different observations thus evolve different hidden states. addition agent receives index input makthird disabling experience replay simply involves turning feature dqn. although experience replay helpful single-agent settings multiple agents learn independently environment appears nonstationary agent rendering experience obsolete possibly misleading. given modiﬁcations ddrqn learns q-function θi). note form target q-value hats riddle described follows executioner lines prisoners single puts blue prisoner’s head. every prisoner hats people front line anyone behind him. executioner starts asks last prisoner colour hat. must answer blue. answers correctly allowed live. gives wrong answer killed instantly silently. night line-up prisoners confer strategy help them. figure illustrates setup. switch riddle described follows hundred prisoners newly ushered prison. warden tells starting tomorrow placed isolated cell unable communicate amongst other. warden choose prisoners uniformly random replacement place central interrogation room containing light bulb toggle switch. prisoner able observe current state light bulb. wishes toggle light bulb. also option announcing believes prisoners visited interrogation room point time. announcement true prisoners free false prisoners executed. warden leaves prisoners huddle together discuss fate. agree protocol guarantee freedom? optimal strategy prisoners agree communication protocol ﬁrst prisoner says blue number blue hats even otherwise remaining prisoners deduce colour given hats front responses heard behind them. thus everyone except ﬁrst prisoner deﬁnitely answer correctly. formalise hats riddle multi-agent task deﬁne state space total number agents {blue red} agent’s colour {blue red} action took m-th step. time steps agent take null action. m-th time step agent observation reward zero except episode total number agents correct action label relevant observation number strategies analysed inﬁnite time-horizon version problem goal guarantee survival. wellknown strategy prisoner designated counter. allowed turn switch prisoner turns once. thus counter turned switch times tell. formalise switch riddle deﬁne state space off} position switch current visitor interrogation room tracks agents already interrogation room. time step agent observes agent interrogation room null otherwise. agent interrogation room actions tell none}; otherwise action none. episode ends agent chooses tell maximum time step reached. reward outputs added element-wise. passed lstm network subsequently follow similar proyk cedure hats observed deﬁning lstms. finally last values lstm networks used approximate q-values action mlp. network trained minibatches episodes. furthermore adaptive variant curriculum learning pave scalable strategies better training performance. sample examples multinomial distribution curricula corresponding different current bound raised every time performance becomes near optimal. probability sampling given inversely proportional performance compared normalised maximum reward. performance depicted figure ﬁrst evaluate ddrqn compare tabular q-learning. tabular q-learning feasible agents since state space grows exponentially addition separate tables agent precludes generalising across agents. figure shows results ddrqn substantially outperforms tabular q-learning. addition ddrqn also comes near performance optimal strategy described section ﬁgure also shows results ablation experiment inter-agent weight sharing removed ddrqn. results conﬁrm inter-agent weight sharing performance. figure results hats riddle agents comparing ddrqn without inter-agent weight sharing tabular q-table hand-coded optimal strategy. lines depict average runs conﬁdence intervals. figure hats agent observes answers preceding agents colour front variable length sequences processed rnns. first answers heard passed single= outputs layer mlps passed lstm netadded element-wise. similarly observed lstma. last values lstms used approximate learning rate proposed architectures make rectiﬁed linear units lstm cells. details network implementations described supplementary material source code published online. compare strategies ddrqn learns optimal strategy computing percentage trials ﬁrst agent correctly encodes parity observed hats answer. table shows encoding almost perfect agents encode parity learn different distributed solution nonetheless close optimal. believe qualitatively solution corresponds agents communicating information hats answers instead ﬁrst agent. figure illustrates model architecture used switch riddle. agent modelled recurrent neural network lstm cells unrolled dmax time-steps denotes number days episode. experiments limit dmax order keep experiments computationally tractable. figure shows results shows ddrqn learns optimal policy beating naive method hand coded strategy tell last day. veriﬁes three modiﬁcations ddrqn substantially improve performance task. following paragraphs analyse importance individual modiﬁcations. analysed strategy ddrqn discovered looking sampled episodes. figure shows decision tree constructed samples corresponds optimal strategy allowing agents collectively track number visitors interrogation room. prisoner visits interrogation room options either prisoners visited room before. three prisoners been third prisoner would already ﬁnished game. remaining options encoded position respectively. order carry strategy prisoner learn keep track whether visited cell currently figure compares performance ddrqn variant switch disabled. around episodes diverge performance. hence clearly identiﬁable point learning prisonfigure switch ddrqn outperforms naive method simple hand coded strategy tell last achieving oracle level performance. lines depict average runs conﬁdence interval. figure switch episodes ddrqn line clearly separates performance line switch test start exceeding tell last day. point agents start discover strategies evolve communication using switch. lines depict mean runs conﬁdence interval. start learning communicate switch. note switch enabled ddrqn outperform hand-coded tell last strategy. thus communication switch required good performance. figure shows performance runs ddrqn clearly beats hand-coded tell last strategy ﬁnal performance approaches oracle. however remaining runs ddrqn fails signiﬁcantly outperform hand-coded strategy. analysing learned strategies suggests prisoners typically encode whether prisoners room positions switch respectively. strategy generates false negatives i.e. prisoner enters room always tells generates false positives around time. example strategies included supplementary material. modiﬁcations contribute substantially ddrqn’s performance. inter-agent weight sharing important without agents essentially unable learn task even last-action inputs also play signiﬁcant role without performance substantially exceed tell last strategy. disabling experience replay also makes difference performance replay never reaches optimal even after episodes. result surprising given non-stationarity induced multiple agents learning parallel. non-stationarity arises even though agents track action-observation histories rnns within given episode. since memories reset episodes learning performed agents appears non-stationary perspective. figure switch ddrqn manages discover perfect strategy visualise decision tree figure. position switch encodes prisoners visited interrogation room encodes prisoner has. partial observability artiﬁcially introduced blanking fraction input screen deep recurrent reinforcement learning also applied textbased games naturally partially observable recurrent also successful email campaign challenge examples apply recurrent however single-agent domains. without combination multiple agents partial observability need learn communication protocol essential feature work. paper proposed deep distributed recurrent q-networks enable teams agents learn solve communication-based coordination tasks. order successfully communicate agents tasks must ﬁrst automatically develop agree upon communication protocol. presented empirical results multi-agent learning problems based well-known riddles demonstrating ddrqn successfully solve tasks discover elegant communication protocols addition presented ablation experiments conﬁrm main components ddrqn architecture critical success. future work needed fully understand improve scalability ddrqn architecture large numbers agents e.g. switch riddle. also hope explore local minima structure coordination strategy space underlies riddles. another avenue improvement extend ddrqn make various multi-agent adaptations q-learning beneﬁt using deep models efﬁciently cope high dimensional perceptual signals inputs. future tested replacing binary representation colour real images hats applying ddrqn scenarios involve real world data input. advanced proposal using riddles test ﬁeld multi-agent partially observable reinforcement learning communication also hope research spur development interesting challenging domains area. figure switch tied weights last action input performance ddrqn. experience replay prevents agents reaching oracle level. experiment executed lines depict average runs conﬁdence interval. however particularly important communicationbased tasks like riddles since value function communication actions depends heavily interpretation messages agents turn q-functions. plethora work multi-agent reinforcement learning communication e.g. however work assumes pre-deﬁned communication protocol. exception work kasai tabular q-learning agents learn content message solve predator-prey task. approach similar q-table benchmark used section contrast ddrqn uses recurrent neural networks allow memory-based communication generalisation across agents. another example open-ended communication learning multi-agent task given however evolutionary methods used learning communication protocols rather using deep shared weights enable agents develop distributed communication strategies allow faster learning gradient based optimisation. furthermore planning-based methods employed include messages integral part multi-agent reinforcement learning challenge however work extended references assael j.-a. wahlstr¨om sch¨on deisenroth data-efﬁcient learning feedback policies image pixels using deep dynamical models. arxiv preprint arxiv. kasai tenmoto kamiya learning communication codes multi-agent reinforcement learning problem. ieee conference soft computing industrial applications mnih kavukcuoglu silver rusu veness bellemare graves riedmiller fidjeland ostrovski petersen beattie sadik antonoglou king kumaran wierstra legg hassabis human-level control deep reinforcement learning. nature nair srinivasan blackwell alcicek fearon maria panneershelvam suleyman beattie petersen legg mnih kavukcuoglu silver massively paralv deep methods deep reinforcement learning. learning workshop icml silver huang maddison c.j. guez sifre driessche schrittwieser antonoglou panneershelvam lanctot dieleman grewe nham kalchbrenner sutskever lillicrap leach kavukcuoglu graepel hassabis mastering game deep neural networks tree search. nature spaan gordon vlassis decentralized planning uncertainty teams communicating agents. international joint conference autonomous agents multiagent systems", "year": 2016}