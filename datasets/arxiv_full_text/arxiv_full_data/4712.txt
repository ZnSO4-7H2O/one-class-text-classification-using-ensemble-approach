{"title": "Off-Policy Shaping Ensembles in Reinforcement Learning", "tag": ["cs.AI", "cs.LG"], "abstract": "Recent advances of gradient temporal-difference methods allow to learn off-policy multiple value functions in parallel with- out sacrificing convergence guarantees or computational efficiency. This opens up new possibilities for sound ensemble techniques in reinforcement learning. In this work we propose learning an ensemble of policies related through potential-based shaping rewards. The ensemble induces a combination policy by using a voting mechanism on its components. Learning happens in real time, and we empirically show the combination policy to outperform the individual policies of the ensemble.", "text": "note even though effects reward shaping latent learning context bound limited since large part beneﬁts guiding exploration learning witness signiﬁcant rise performance making validation effectiveness reward shaping purely means faster knowledge propagation. section discussion. unlike existing ensembles ﬁrst policy ensemble architecture capable learning online real-time sound w.r.t. convergence realistic setups guarantees provided horde limitation applied latent learning setup ensure convergence. outline following section give brief overview definitions notation. section motivates horde multiple shaping signals form ensemble. section summarizes architecture describes rank voting mechanism used combining policies. section gives experimental results mountain domain section concludes discusses future work directions. background environment agent usually modeled markov decision process given -tuple states actions available agent transition function denoting probability ending state upon taking action state reward function denoting expected reward transition upon taking action markovian assumption reward depend denotes discrete time step. stochastic policy deﬁnes probability distribution actions state abstract. recent advances gradient temporal-difference methods allow learn off-policy multiple value functions parallel withsacriﬁcing convergence guarantees computational efﬁciency. opens possibilities sound ensemble techniques reinforcement learning. work propose learning ensemble policies related potential-based shaping rewards. ensemble induces combination policy using voting mechanism components. learning happens real time empirically show combination policy outperform individual policies ensemble. reinforcement learning framework agent learns interacting environment. bulk algorithms focus on-policy setup agent learns policy executing. offpolicy setup agent’s behavior target policies allowed differ arguably versatile practice hindered convergence issues arising combined function approximation e.g. popular q-learning potentially diverges issue recently resolved advancement family gradient temporal-difference methods greedy-gq interesting implication possibility learn multiple tasks parallel shared experience stream sound framework architecture dubbed horde sutton spirit ensemble methods idea context learning single task faster. larger devise ensembles policies improve learning speed task online real time without incurring extra sample computational costs. choose policies ensemble related potential-based reward shaping. reward shaping well-known technique speed learning process injecting domain knowledge reward function. idea considering multiple shaping signals instead single relatively recent devlin observe improves performance multi-agent context brys using multi-objectivization formalism demonstrate usefullness treating different shapings correlated objectives scenario consider paper off-policy learning ﬁxed behavior scenario maei refer latent learning. often setup applications environment samples costly failure highly penalized making usual trial error tactic implausible e.g. robotic applications. vrije universiteit brussel belgium email {anna.harutyunyan hence scalable. sutton formalize framework parallel real-time off-policy learning naming horde. demonstrate horde able learn predictive goal-oriented value functions real-time single unsupervised stream sensorimotor experience. successful applications horde realistic robotic setups take different angle existing literature attempt power horde learning single task multiple viewpoints. reward shaping reward shaping augments true reward signal additional heuristic reward provided designer. originally thought scaling methods handle difﬁcult problems generally suffers infeasibly long learning times. applied carelessly however shaping slow even prevent ﬁnding optimal policy show grounding shaping rewards state potentials necessary sufﬁcient ensuring preservation policies original mdp. potential-based reward shaping maintains potential function deﬁnes auxiliary reward function main discounting factor. intuitively potentials encode desirability state shaping reward transition signals positive negative progress towards desirable states. potential-based shaping repeatedly validated speed learning problems uninformative rewards refer rewards augmented shaping signals shaped rewards value functions w.r.t. shaped value functions greedy policies induced shaped value functions shaped policies. shaped policies converge policy base policy differ learning process. ensembles shapings section motivate horde well-suited framework ensemble learning surveying ensemble methods reinforcement learning argue policies obtained potential-based reward shaping good candidates ensemble. ensemble techniques boosting bagging widely used supervised learning effective methods reduce bias variance solutions. ensembles extremely sparse thus far. previous uses ensembles policies involved independent runs policy combination happening post-factum limited practical usage since requires large computational sample overhead assumes repeatable setup improve learning speed. others general lack convergence guarantees either using mixed onoffpolicy learners q-learners function approximation general off-policy setup seems inevitable considering ensembles policies; surely interesting policies reﬂect information different behavior since strength sutton give horde terms general value functions auxilary inputs paper always assume greedy policy w.r.t. shared demons related base reward shaping reward. environment dynamics unknown solve applying family temporal difference algorithms iteratively estimate value functions. following update rule popular q-learning method simplest form reward received transition learning rate step size error drawn according given eligibility traces controlled trace decay parameter used speed knowledge propagation state action spaces large continuous tabular representations sufﬁce needs function approximation state space represented features algorithms learn value parameter vector linear case horde unfortunately cause off-policy bootstrapping methods q-learning diverge even simple problems family gradient temporal-difference algorithms resolve issue ﬁrst time keeping constant per-step complexity provided ﬁxed behavior accomplish this performing gradient descent reformulated objective function ensures convergence ﬁxpoint introducing gradient bias update. mechanistically requires maintaining learning second weights along following update rules off-policy learning allows learn policy regardless behavior policy followed. need limit single policy learn arbitrary number policies single stream environment interactions computational considerations bottleneck. methods reliably converge realistic setups unlike second order algorithms similar guarantees constant time memory per-step please refer maei’s dissertation full details simplest form update rules gradient temporaldifference algorithms namely augments update eligibility traces. ensemble learning lies diversity information components contribute q-learning setup reliable presence unofﬁcial mantra practice sufﬁciently similar policy q-learning used diverge even despite famous counterexamples ensembles diverse q-learners bound larger disagreement amongst behavior policy much larger potential becoming unstable. ability learn multiple policies reliably parallel realistic setup provided horde architecture. reason believe horde ideally suited framework ensemble learning turn question choice components ensemble. recall larger ensembles speed learning single task real time. krogh vedelsby show context neural networks effective ensembles accurate diverse components namely make errors different parts space. context diversity expressed several aspects related dimensions learning process diversity experience diversity algorithms diversity reward signals. diversity experience naturally implies high sample complexity assumes either multi-agent setup learning stages. diversity algorithms convergence issues unless algorithms sound off-policy argument above. marivate littman consider diversity mdps improving performance generalized ensemble trained sample mdps also requires two-stage learning process. context improving learning speed focus latter aspect diversity diversity reward signals. discussed section potential-based reward shaping provides framework enriching base reward incorporating heuristics express desirability states. usually think multiple heuristics single problem effective different situations. combining na¨ıvely e.g. linear scalarization potentials uninformative since heuristics counterweigh parts space cancel out. hand typically infeasible designer handcode tradeoffs without executing shaping separately. horde provides sound framework learn maintain shapings parallel enabling possibility using ensemble methods combination. shaping off-policy note straying convention using reward shaping off-policy latent learning setup. effects reward shaping learning process usually considered guidance exploration learning laud dejong formalize showing difﬁculty learning dependent reward horizon measure number decisions learning agent must make experiencing accurate feedback reward shaping artiﬁcially reduces horizon. setting assume control agent’s behavior performance beneﬁts section must explained different effect. namely shaping rewards updates faster knowledge propagation observe decoupled guidance exploration off-policy latent learning setup. maintain horde shapings greedy-gqlearners. reward function vector f|d|− |d|− potential-based rewards given potentials provided designer. adopt terminology sutton refer individual agents within horde demons. demon learns greedy policy w.r.t. reward refer demons learning shaped rewards shaped demons. point learning devise combination policy collecting votes action preferences shaped demons wiering discuss several intuitive ways e.g. majority voting rank voting boltzman multiplication etc. describe rank voting used paper general choice ensemble combination designer depend speciﬁcs problem architecture. even though base demon contribute vote maintain part ensemble. rank voting demon ranks actions according greedy policy casting vote most vote least preferred actions. voting schema deﬁned policies rather value functions mitigates magnitude bias. slightly modify formulation ranking q-values instead policy probabilities i.e. ranking demon. qd). combination ensemble policy acts greedily w.r.t. cumulative preference values section give comparison results individuals ensemble combination policy. remind reader policies eventually arrive solution focus time takes there. focus attention classical benchmark domain mountain task drive underpowered hill state system composed current position current velocity car. actions discrete throttle agent starts position velocity goal position rewards every time step. episode ends goal reached steps elapsed. state space approximated standard tile-coding technique using tilings parameter vector learnt action. behavior policy uniform distribution actions time step. thus architecture demons learns base reward others respective shaping rewards. combination policy formed rank voting found outperform majority voting variant q-value voting problem. third shaping turns helpful universally. case would likely prefer single shaping assume information available priori realistic situation. make experiment interesting consider scenarios without best shaping. ideally would like combination method able outperform comparable shapings ﬁrst scenario pick best shaping second scenario. used learning parameters tuned selected trace decay parameter step size second weights greedy-gq vector step sizes value functions demons. independent runs episodes each. evaluation done interrupting off-policy learner every episodes executing demon’s greedy policy once. learning allowed evaluation. graphs reﬂect average base reward. initial ﬁnal performance refer ﬁrst last run. results fig. tables show individual shapings alone learning speed signiﬁcantly. combination method meets desiderata either statistically matches better best shaping stage overall outperforming single shapings. exception ﬁnal performance scenario performance best shaping signiﬁcantly different scaling potentials general challenging problem reward shaping research. finding right scaling factor requires priori tuning factor generally assumed constant state space. scalable nature horde could used lift problem learning multiple preset scales potential combining either voting method like described here meta-learner. section tuned individually value functions differ magnitude. learned shaped rewards used voting method combine them. validated approach classical mountain domain considering scenarios without clearly best shaping signal. former scenario combination outperformed single shapings latter able match performance best shaping. general expect larger beneﬁts larger problems; extensive suite experiments subject future work. primary limitation horde requirement keep behavior policy ﬁxed important case relaxing constraint would expand effectiveness architecture. topic ongoing research community. future work work considered ad-hoc voting approach combining shapings. possible future directions would learn optimal combination ways predicting shared ﬁtness value w.r.t. policies induced learnt value functions. challenge meta-learning happen much faster pace useful speeding main learning process. case shapings doubly case since eventually converge policy. size window opportunity related size problem. scalability horde allows learning potentially thousands value functions efﬁciently parallel. context shaping rarely sensible actually deﬁne thousands distinct shapings could imagine deﬁning shaping potentials many different scaling factors each demon combining shapings group. would mitigate scaling problem potentially make representation ﬂexible non-static scaling factors throughout state space. roughly similar ﬂavor approach marivate littman learn solve many variants problem best parameter settings generalized mdp. could attempt learn best potential functions before needs realistic attainability learning time since argued best potential function correlates optimal value function learning would solve base problem render potentials pointless. references baird ‘residual algorithms reinforcement learning function approximation’ proceedings twelfth international conference machine learning morgan kaufmann brys harutyunyan vrancx m.e. taylor kudenko now´e ‘multi-objectivization reinforcement learning’ technical report ai-tr-- vrije universiteit brussel gave ﬁrst policy ensemble sound capable learning real time exploiting power horde architecture learn single policy well. value functions ensemble artefact value-function methods learnt off-policy behavior policy rarely reaching goal. given longer learning periods closer closer attainable optimum choose concern context paper main focus lies improving learning time within off-policy framework. fauer schwenker ‘ensemble methods reinforcement learning function approximation.’ eds. carlo sansone josef kittler fabio roli volume lecture notes computer science springer grzes kudenko ‘theoretical empirical analysis reward shaping reinforcement learning’ machine learning applications fourth international conference grzes kudenko ‘online learning shaping rewards reinforcement learning’ neural networks international conference artiﬁcial neural networks {icann} h.r. maei r.s. sutton general gradient algorithm temporal-difference prediction learning eligibility traces’ proceedings third conf. artiﬁcial general intelligence. h.r. maei szepesv´ari bhatnagar r.s. sutton ‘toward offpolicy learning control function approximation’ proceedings twenty-seventh international conference machine learning eds. johannes f¨urnkranz thorsten joachims omnipress harada russell ‘policy invariance reward transformations theory application reward shaping’ proceedings sixteenth international conference machine learning morgan kaufmann p.m. pilarski m.r. dawson degris j.p. carey k.m. chan j.s. hebert r.s. sutton ‘adaptive artiﬁcial limbs real-time approach prediction anticipation’ robotics automation magazine ieee r.s. sutton h.r. maei precup bhatnagar silver szepesvri wiewiora ‘fast gradient-descent methods temporal-difference learning linear function approximation’ proceedings international conference machine learning r.s. sutton modayil delp degris p.m. pilarski white precup ‘horde scalable real-time architecture learning knowledge unsupervised sensorimotor interaction’ international conference autonomous agents multiagent systems volume aamas richland international foundation autonomous agents multiagent systems. tsitsiklis analysis temporal-difference learning function approximation’ technical report ieee transactions automatic control hasselt insights reinforcement learning formal analysis empirical evaluation temporal-difference learning algorithms ph.d. dissertation utrecht university", "year": 2014}