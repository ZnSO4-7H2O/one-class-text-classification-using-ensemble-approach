{"title": "DelugeNets: Deep Networks with Efficient and Flexible Cross-layer  Information Inflows", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Deluge Networks (DelugeNets) are deep neural networks which efficiently facilitate massive cross-layer information inflows from preceding layers to succeeding layers. The connections between layers in DelugeNets are established through cross-layer depthwise convolutional layers with learnable filters, acting as a flexible yet efficient selection mechanism. DelugeNets can propagate information across many layers with greater flexibility and utilize network parameters more effectively compared to ResNets, whilst being more efficient than DenseNets. Remarkably, a DelugeNet model with just model complexity of 4.31 GigaFLOPs and 20.2M network parameters, achieve classification errors of 3.76% and 19.02% on CIFAR-10 and CIFAR-100 dataset respectively. Moreover, DelugeNet-122 performs competitively to ResNet-200 on ImageNet dataset, despite costing merely half of the computations needed by the latter.", "text": "deluge networks deep neural networks efﬁciently facilitate massive cross-layer information inﬂows preceding layers succeeding layers. connections layers delugenets established cross-layer depthwise convolutional layers learnable ﬁlters acting ﬂexible efﬁcient selection mechanism. delugenets propagate information across many layers greater ﬂexibility utilize network parameters effectively compared resnets whilst being efﬁcient densenets. remarkably delugenet model model complexity gigaflops network parameters achieve classiﬁcation errors cifar- cifar- dataset respectively. moreover delugenet- performs competitively resnet- imagenet dataset despite costing merely half computations needed latter. deep learning methods particularly convolutional neural networks revolutionized ﬁeld computer vision. cnns integral components many recent computer vision techniques spread across diverse range vision application areas hence developing sophisticated cnns prime research focus. years many variants architectures proposed. works focus improving activation functions focus increasing heterogeneity convolutional ﬁlters within layers lately idea improving cnns greatly deepening gained much traction following immense successes residual networks image classiﬁcation. preceding layers outputs input. compared traditional non-residual deep networks outputs preceding layers resnets reach succeeding layers minimal obstructions even preceding layer succeeding layer separated long layer-distance. however crosslayer connections preceding succeeding layers resnet ﬁxed selective therefore succeeding layers able prioritize deprioritize output channels certain preceding layers. instead outputs preceding layers lumped together simplistic additive operation making tough succeeding layers perform layer-wise information selection. inﬂexibility residual connections also hinders ability resnets learn cross-layer interactions correlations. densely connected networks overcome drawback resnets convolutional layers consider extra dimension depth/cross-layer dimension addition spatial feature channel dimensions used regular convolutions. densenets input feature maps succeeding layers concatenations preceding layers outputs rather simple summations. hence applying convolution operations concatenated feature maps convolutional ﬁlters learn spatial cross-channel cross-layer correlations altogether entailing heavy amounts parameters computations. densenet-bc recently introduced efﬁcient variant densenet ﬁlters consider cross-channel cross-layer correlations. despite that considering densenets’ composite layers receive inputs several dozens preceding layers computation parameter requirements still rather high. figure deluge network components composite layer block transition component block. red-colored arrows indicate cross-layer depthwise convolutions. output channels) layer compared typical image classiﬁcation cnns. however crucial considerable network width contended decreasing output width much harmful networks representational power. furthermore visualizing densenet’s weight norms huang showed features preceding composite layers reused directly succeeding composite layers rather infrequent manner. diminished features processed relatively expensive convolution operations densenets. thus paper propose class cnns called deluge networks enable ﬂexible cross-layer connections regular output width composite layer. result using regular output width information inﬂows preceding layers succeeding layers delugenets massive contrast lesser information inﬂows densenets. delugenets inspired separable convolutions efﬁciency convolutions improved separating combined dimensions involved resulting separable convolutions. delugenets designed depth/cross-layer dimension processed independently rest using novel variant convolutional layer called cross-layer depthwise convolutional layer described section cross-layer depthwise convolutional layers handle cross-layer interactions correlations without getting burdened dimensions. facilitate cross-layer connections delugenets efﬁcient effective manner. experiments show superior performances delugenets terms classiﬁcation accuracy parameter efﬁciency remarkably computational complexity. much attention years. incorporate classiﬁcation losses intermediate hidden layers allowing unimpeded supervised signals reach layers. similar spirit googlenets inception models attach auxiliary classiﬁers intermediate layers encourage feature discriminativeness lower network layers. delugenets contrast readily backpropagate supervised signals earlier layers without relying additional losses connections supporting ﬂexible information inﬂows preceding succeeding layers. another stream works focusing improving information ﬂows layers deep networks also focus work. highway networks make long-short-termmemory )-inspired gating mechanism control information linear nonlinear pathways. appropriately learned gating functions information unimpededly across many layers thought kind ﬂexible mechanism combine cross-layer information inﬂows. propose residual networks compute residual functions outputs linear nonlinear pathways without complex gating mechanisms. resnets shown tackle well vanishing gradient network degradation problems occur deep networks. pre-activation variants resnet normalize incoming activations beginnings residual blocks improve information regularization. instead going deeper wide-resnets improve upon originally proposed resnets convolutional ﬁlters/channels less numbers layers motivated high model complexity resnets terms depths parameter numbers several dropping-based regularization methods developed regularizing large resnet models. resnets seen less ﬂexible special case delugenets cross-layer connection weights separable convolutions adopted construct efﬁcient convolutional networks. earlier works compress convolutional networks ﬁnding low-rank approximation convolutional layers pre-trained networks. network-in-network employs pointwise convolutional layers enrich representation learning efﬁcient manner. pointwise convolutions generally coupled convolution variants achieve separable convolutions. flattened convolutional networks equipped onedimensional convolutional ﬁlters dimensions processed sequentially trained scratch. maximal channel-spatial separability conventional convolutional layer replaced depthwise separable convolution demonstrated xception contrast existing works mainly deal channel-spatial separability work paper deals cross-layer-channel separability. also best knowledge paper ﬁrst work cross-layer depth/channelwise convolutions. similar existing cnns vggnet alexnet delugenets gradually decrease spatial sizes increase feature channels feature maps bottom layers linear classiﬁcation layer attached end. layers operating feature dimensions grouped form block. delugenets input particular layer comes preceding layers block. information directly ﬂowing blocks. within block cross-layer information ﬂows connections established cross-layer depthwise convolutions transition next block described section perform cross-layer depthwise convolution followed strided spatial convolution obtain feature matching dimensions. structure block delugenets illustrated figure individual layers separated vertical dashed lines. composite layer cnns layer often refers composite layer several basic layers rectiﬁed linear unit convolutional batch normalization layers. inspired bottleneck-kind composite layer bn-relu-conv-bn-relu-conv-bn-relu-conv delugenets illustrated figure kind composite layer designed improve parameter efﬁciency deep networks employing spatial convolutional layers beginning reduce channel dimension increase channel dimension. resnet models proposed base channel dimensions increased times. however increase times paper reason allocate computational parameter budgets train deeper delugenets. composite layer also shown work well deep neural networks combine information multiple sources resnets proposed delugenets. primary reason works well combined multi-source information normalized layers passed upcoming weight layers. reduces internal covariate shift regularizes model effectively passing unnormalized multi-source information weight layers. facilitate efﬁcient ﬂexible cross-layer information inﬂows paper develop cross-layer depthwise convolution method. cross-layer depthwise convolutional layer concatenates channels feature outputs many layers applies -independent ﬁlters concatenated channels. equipped ﬁlters delugenets able process depth/layer dimension independently rest mentioned section figure gives graphical illustration cross-layer depthwise convolution operation. cross-layer depthwise convolutional layers facilitate inﬂows information preceding composite layers succeeding composite layers. suppose denotes denotes layer arbitrary composite layer c-th channel preceding −i)-th composite layer’s output. number preceding composite layers well preceding block transition output c-th channel input composite layer ﬁlter weights bias respectively channel. streamline equations spatial location-related notations weights biases assumed shared across spatial locations input feature maps mentioned earlier. parameter cost adding cross-layer depthwise convolutional layer existing network architecture relatively compared network parameters. arbitrary composite layer network number additional parameters merely number feature channels. experimentally extra parameters average make entire model parameters. terms computational complexity cross-layer depthwise convolutions average cost more compared baseline models without convolutions. densenets hand require heavy amounts computations parameters connect preceding layers cross-layer output concatenations followed spatial convolutions. advantages cross-layer depthwise convolutional layers beneﬁcial encourage features generated preceding composite layers taken inputs many times succeeding layers naturally leads parameter efﬁciency need redundantly learn ﬁlters generate features succeeding layers case features needed later. furthermore conventional relubased convolutional networks features turned relu activation functions cannot recovered network parts layers. delugenets cross-layer depthwise convolutional layers output preceding composite layer transformed differently succeeding composite layer serve input. consequently input features turned beginning certain succeeding composite layers active others. cnns ﬁlter weights shared across many spatial locations feature maps. weight sharing mechanism acts effective regularizer. similar cnn’s weight sharing mechanism features delugenets’ preceding composite layers shared succeeding composite layers. result weights composite layers delugenets become regularized. based consideration allocate model parameters spatially smaller network blocks setting number composite layer succeeding block larger preceding block. motivation behind achieve lower computational complexity relying cross-layer feature reuse less parametersharing across spatial locations regularization. allocation scheme differs resnets many layers parameters allocated blocks feature maps regularize ﬁlters better less blocks spatially small feature maps reduce overﬁtting besides encouraging feature reuse cross-layer depthwise convolutional layers advantageous perspective gradient ﬂow. gradient ﬂows delugenets regulated multiplicative interactions ﬁlter weights cross-layer depthwise convolutional layers composite layers receive unique backpropagated gradient signals even come block. true resnet models composite layers within block receive identical backpropagated gradient signals simple addition operation. different network blocks operate feature maps different spatial channel dimensions. block transition need transform feature match spatial channel dimensions next block. resnetlike models block transition done either strided convolution strided average pooling channel padding. block transition designs preserve information previous block minimal transformation well dismissing non-linear activation function. block transition designs suboptimal delugenets allow direct information last composite layer previous block conceivably hinder information ﬂows composite layers. propose block transition component cross-layer depthwise convolutional layer followed spatial convolutional layer. crosslayer depthwise convolutional layer allows direct information inﬂow composite layers previous block therefore summarizing outputs composite layers previous block. then strided spatial convolutional layer transforms summarized feature matching spatial channel dimensions. strided convolutional layer chosen strided convolutional layer latter wastes features receives many feature map’s spatial model highway network fractalnet resnet resnet resnet resnet identity mappings resnet identity mappings resnet swapout resnet stochastic depth resnet stochastic depth wide-resnet wide-resnet wide-resnet densenet densenet densenet-bc densenet-bc delugenet- delugenet- wide-delugenet- rigorously validate effectiveness delugenets evaluate delugenets image classiﬁcation datasets varied degrees challengingness cifar- cifar- imagenet experimental code written torch available https// github.com/xternalz/delugenets. cifar- cifar- datasets cifar- cifar- subsets tiny images dataset annotated serve image classiﬁcation datasets. training images testing images cifar datasets. pre-processing subtract channel-wise means images divide channel-wise standard deviations. training data augmentation carried moderately horizontal ﬂipping random crops taken images padded pixels side. cifar-based models training carried using single gpu. implementation total different delugenet models implemented evaluated cifar datasets. similar delugenet models blocks ﬁrst block works spatially feature maps followed feature maps second third blocks respectively. vary terms numbers composite layers feature channel dimensions blocks. minimize manual tuning architectural hyperparameters design different delugenet models based simple principle follows parameter allocation scheme mentioned section second block times numbers composite layers feature channel dimension ﬁrst block third block times second’s base widths delugenet- comes larger composite layer counts make much deeper model. wide-delugenet- wider variant delugenet- base widths composite layer counts remain same. proposed models cifar datasets differ numbers output labels train models stochastic gradient descent total training epochs nestorov momentum weight decay rate existing models compare paper dropout-like regularization either fairer comparison. starting learning rate decayed factor epoch mini-batch size delugenet model parameters initialized using he’s initialization method results top- classiﬁcation errors achieved delugenets existing models cifar datasets presented table results existing models obtained directly respective papers. shown table delugenets beneﬁt deepening widening parameter efﬁciency delugenets able perform well despite requiring much lower numbers learnable parameters compared existing models. parameter efﬁciencies delugenets second densenetbcs aggressively compress reduce feature channels save parameters. notably delugenet- performs competitively wide-resnet cifar- cifar- datasets merely parameters compared parameters wide-resnet. besides wide-delugenet- achieves cifar classiﬁcation errors comparable densenet fewer parameters. computational complexity addition parameter numbers report model complexities delugenets several comparable models terms ﬂoating-point operation numbers. overall delugenets signiﬁcantly fewer model complexities models. surprisingly delugenet- requires flops required wide-resnet achieve similar classiﬁcation errors. although delugenets cannot exactly match outperform densenet-bc achieve appreciable classiﬁcation errors rather close densenet-bcs fractions densenet-bcs’ complexity costs. lower model complexities delugenets attributed parameter allocation scheme well capability cross-layer depthwise convolutions alleviating overﬁtting even spatially smaller network blocks parameters/layers spatially larger counterparts. ablation study work propose cross-layer depthwise convolutional layer kind block transition design spatial convolution differentiate delugenets existing networks. better understand contributions components construct resnet-like baselines proposed delugenet models. types baselines delugenet models ﬁrst baseline cross-layer depthwise convolutions replaced residual connections. alternatively residual connections seen cross-layer depthwise convolutional layers whose weights ﬁxed ones pointed section second baseline similar ﬁrst except equipped convomodel resnet-like baseline conv shortcut conv shortcut delugenet- resnet-like baseline conv shortcut conv shortcut delugenet- resnet-like baseline conv shortcut conv shortcut wide-delugenet- lutional shortcuts block transitions similar proposed block transition design. mentioned aspects baselines corresponding delugenets same including training settings. evaluate baselines cifar-. results shown table block transitions convolutional shortcuts mildly improve performances delugenet-’s delugenet-’s baselines. however slight overﬁtting adding convolutional shortcuts wide-delugenet-’s baseline. overﬁtting issue greatly eased cross-layer depthwise convolutions wide-delugenet-. evidenced signiﬁcant performance improvements delugenets baselines biggest contributor crosslayer depthwise convolutional layer. parameter costs incurred adding layers marginal. smallest delugenet model delugenet- parameters complexity gflops suprisingly outperforms biggest resnet-like baseline parameters complexity gflops. furthermore tiny increases complexity needed cross-layer depthwise convolutions achieve considerable performance gains. ﬁndings reafﬁrm advantages proposed cross-layer depthwise convolutional layer deep networks. cross-layer connectivity better understanding cross-layer depthwise convolutional layers compute layer-wise l-norms cross-layer depthwise convolutional ﬁlter weights delugenet- cifar- cifar-. provide visualizations figure weight’s l-norms normalized dividing maximum layer-wise l-norms every block. consider cross-layer depthwise convolutional layers block transition block tranfigure layer-wise l-norms cross-layer depthwise convolution weights. columns corresponds different block transition stage networks. vertical axes indicate indices preceding composite layers horizontal axes indicate normalized l-norm values. longer horizontal composite layer larger contribution. sition cross-layer depthwise convolutional layer classiﬁcation layer. cross-layer depthwise convolutional layers highest numbers cross-layer connections networks. generally preceding composite layers contribute reasonably dominating. weights longer uniform layers trained models different connection rigidity exhibited resnets. ﬁrst second block transitions last composite layers always contribute most somehow approximating behaviors conventional neural networks incoming information comes solely layer current layer. hand cross-layer depthwise convolutional layer connected third network block early composite layers generally contribute most ﬁnal composite layer contributes moderately. reckon features computed earlier composite layers fairly ready classiﬁcation subsequent composite layers reﬁne further. phenomenon also observed resnets upper layers could deleted withhurting performance much. addition notice composite layers ﬁrst block delugenet- hardly contributions block transition observation suggest layer sparsity imagenet dataset widely used large-scale image classiﬁcation dataset recent years. report results validation images. follow data augmentation scheme adopted googlenet/inception resnet-v following augmentation techniques scale aspect ratio augmentation pca-based lighting augmentation photometric distortions horizontal ﬂipping. images normalized subtracting channel-wise means dividing channel-wise standard deviations. implementation implement evaluate different delugenet models imagenet dataset. similar resnet models passed ﬁrst block feature downsampled spatial dimensions max-pooling. base feature channel dimensions imagenet-based delugenet models identical resnets network architectural details follow resnets’ closely necessarily optimal delugenets. moreover emphasize great simplicity choosing composite layer counts delugenets setting number composite layers block larger equal preceeding block. contrast carefully tuned composite layer counts resnets. speciﬁcations delugenet models follows delugenet- composite layer counts blocks respectively. delugenet- delugenet- deeper delugenet models composite layer counts respectively. imagenet-based models initialized similarly cifar models. training carried total training epochs nestorov momentum weight decay rate start learning rate decay factor every thirty epoch. training mini-batch size view large model image sizes train models multigpu mode gpus splitting mini-batch portions. standard training settings similar used train imagenet-based resnets. results top- top- classiﬁcation errors achieved delugenets imagenet validation dataset presented table along numbers ﬂoatingpoint operations required models process image. comparison include results resnet-v wide-resnet densenet delugenet- merely parameters outperform resnet- even resnet- besides less parameters half wide-resnet-’s flops delugenet performs comparably wide-resnet-. deeper models delugenet- delugenet- push classiﬁcation errors substantially. remarkably delugenet- attains classiﬁcation errors comparable resnet-’s despite needing half computations required resnet ﬂexible cross-layer connections established cross-layer depthwise convolutions delugenet- robust overﬁtting issue caused allocating parameters spatially smaller blocks. moreover delugenet- outperforms densenet given similar model complexities. given similar considerably lower model budgets delugenets able surpass resnets although delugenets’ composite layer counts conﬁgured rather simple manner. residual operation allows memory buffers shared reused across consecutive composite layers. however delugenets densenets output activations gradients last convolutional layer every composite layer retained persistently training. instance doing training inference wide-delugenet cifar- occupied memory roughly resnetbaseline counterpart requires .g}. smaller inference training hand densenet densenetbc densenet-bc require ˆi.g} spectively. wide-delugenet- memory-efﬁcient densenet densenet-bcs memory-costly. extend depthwise convolutional layers cross-layer depthwise convolutional layers facilitate cross-layer connections proposed delugenets. cross-layer information inﬂows delugenets ﬂexible massive experiments indicate delugenets quite comparable state-of-the-art models terms accuracies delugenets lower model complexities. suggests delugenets potentials energy-efﬁcient deep learning. future would like investigate regularization techniques context cross-layer connectivity well applying delugenets vision applications. research carried rapid-rich object search nanyang technological university singapore. rose supported national research foundation singapore interactive digital media strategic research programme. gratefully acknowledge resources support provided nvaitc singapore.", "year": 2016}