{"title": "Efficient Markov Network Structure Discovery Using Independence Tests", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We present two algorithms for learning the structure of a Markov network from data: GSMN* and GSIMN. Both algorithms use statistical independence tests to infer the structure by successively constraining the set of structures consistent with the results of these tests. Until very recently, algorithms for structure learning were based on maximum likelihood estimation, which has been proved to be NP-hard for Markov networks due to the difficulty of estimating the parameters of the network, needed for the computation of the data likelihood. The independence-based approach does not require the computation of the likelihood, and thus both GSMN* and GSIMN can compute the structure efficiently (as shown in our experiments). GSMN* is an adaptation of the Grow-Shrink algorithm of Margaritis and Thrun for learning the structure of Bayesian networks. GSIMN extends GSMN* by additionally exploiting Pearls well-known properties of the conditional independence relation to infer novel independences from known ones, thus avoiding the performance of statistical tests to estimate them. To accomplish this efficiently GSIMN uses the Triangle theorem, also introduced in this work, which is a simplified version of the set of Markov axioms. Experimental comparisons on artificial and real-world data sets show GSIMN can yield significant savings with respect to GSMN*, while generating a Markov network with comparable or in some cases improved quality. We also compare GSIMN to a forward-chaining implementation, called GSIMN-FCH, that produces all possible conditional independences resulting from repeatedly applying Pearls theorems on the known conditional independence tests. The results of this comparison show that GSIMN, by the sole use of the Triangle theorem, is nearly optimal in terms of the set of independences tests that it infers.", "text": "present algorithms learning structure markov network data gsmn∗ gsimn. algorithms statistical independence tests infer structure successively constraining structures consistent results tests. recently algorithms structure learning based maximum likelihood estimation proved np-hard markov networks diﬃculty estimating parameters network needed computation data likelihood. independence-based approach require computation likelihood thus gsmn∗ gsimn compute structure eﬃciently gsmn∗ adaptation grow-shrink algorithm margaritis thrun learning structure bayesian networks. gsimn extends gsmn∗ additionally exploiting pearl’s well-known properties conditional independence relation infer novel independences known ones thus avoiding performance statistical tests estimate them. accomplish eﬃciently gsimn uses triangle theorem also introduced work simpliﬁed version markov axioms. experimental comparisons artiﬁcial real-world data sets show gsimn yield signiﬁcant savings respect gsmn∗ generating markov network comparable cases improved quality. also compare gsimn forward-chaining implementation called gsimn-fch produces possible conditional independences resulting repeatedly applying pearl’s theorems known conditional independence tests. results comparison show gsimn sole triangle theorem nearly optimal terms independences tests infers. graphical models important subclass statistical models possess advantages include clear semantics sound widely accepted theoretical foundation graphical models used represent eﬃciently joint probability distribution domain. used numerous application domains ranging discovering gene expression pathways bioinformatics computer vision problem naturally arises construction models data solution problem besides theoretically interesting itself also holds potential advancing state-of-the-art application domains models used. paper focus task learning markov networks data domains variables either discrete continuous distributed according multidimensional gaussian distribution. graphical models consist parts undirected graph parameters. example markov network shown figure learning models data consists interdependent tasks learning structure network given learned structure learning parameters. work focus problem learning structure domain data. present algorithms structure learning data gsmn∗ gsimn gsmn∗ algorithm adaptation markov networks algorithm margaritis thrun originally developed learning structure bayesian networks. gsmn∗ works ﬁrst learning local neighborhood variable domain using information subsequent steps improve eﬃciency. although interesting useful itself gsmn∗ point reference performance regard time complexity accuracy achieved gsimn main result work. gsimn algorithm extends gsmn∗ using pearl’s theorems properties conditional independence relation infer additional independences independences resulting statistical tests previous inferences thus avoiding execution tests data. allows savings execution time data distributed communication bandwidth. rest paper organized follows next section present previous research related problem. section introduces notation deﬁnitions presents intuition behind algorithms. section contains main algorithms gsmn∗ gsimn well concepts practical details related operation. evaluate gsmn∗ gsimn present results section followed summary markov networks used physics computer vision communities historically called markov random ﬁelds. recently interest spatial data mining applications geography transportation agriculture climatology ecology others broad popular class algorithms learning structure graphical models score-based approach exempliﬁed markov networks della pietra della pietra laﬀerty mccallum score-based approaches conduct search space legal structures attempt discover model structure maximum score. intractable size search space i.e. space legal graphs super-exponential size score-based algorithms must usually resort heuristic search. step structure search probabilistic inference step necessary evaluate score bayesian networks inference step tractable therefore several practical score-based algorithms structure learning developed markov networks however probabilistic inference requires calculation normalizing constant problem known np-hard number approaches considered restricted class graphical models however srebro karger prove ﬁnding maximum likelihood network np-hard markov networks tree-width greater work area structure learning undirected graphical models concentrated learning decomposable example learning non-decomposable presented work hofmann tresp approach learning structure continuous domains non-linear relationships among domain attributes. algorithm removes edges greedily based leave-one-out cross validation log-likelihood score. non-score based approach work abbeel koller introduces class efﬁcient algorithms structure parameter learning factor graphs class graphical models subsumes markov bayesian networks. approach based parameterization gibbs distribution potential functions forced probability distributions supported generalization hammersley-cliﬀord theorem factor graphs. promising theoretically sound approach lead future practical eﬃcient algorithms undirected structure learning. work present algorithms belong independence-based constraintbased approach independence-based algorithms exploit fact graphical model implies independences exist distribution domain therefore data provided input algorithm work conducting conditional independence tests data successively restricting number possible structures consistent results tests singleton inferring structure possible one. desirable characteristic independence-based approaches fact require probabilistic inference discovery structure. also algorithms amenable proofs correctness bayesian networks independence-based approach mainly exempliﬁed algorithms learn markov blanket step learning bayesian network structure grow-shrink algorithm iamb variants hiton-pc hiton-mb mmpc mmmb max-min hill climbing widely used ﬁeld. algorithms restricted classes trees polytrees also exist. learning markov networks previous work mainly focused learning gaussian graphical models assumption continuous multivariate gaussian distribution made; results linear dependences among variables gaussian noise recent approaches included works dobra hans jones nevins west pe˜na sch¨afer strimmer focus applications gaussian graphical models bioinformatics. make assumption continuous gaussian variables paper algorithms present applicable domains appropriate conditional independence test gsmn∗ gsimn algorithms presented apply case arbitrary faithful distribution assumed probabilistic conditional independence test distribution available. algorithms ﬁrst introduced bromberg margaritis honavar contributions present paper include extending results conducting extensive evaluation experimental theoretical properties. speciﬁcally contributions include extensive systematic experimental evaluation proposed algorithms data sets sampled artiﬁcially generated networks varying complexity strength dependences well data sets sampled networks representing real-world domains formal proofs correctness guarantee proposed algorithms compute correct markov network structure domain stated assumptions. denote random variables capitals sets variables bold capitals particular denote variables domain. name variables indices instance refer third variable simply denote data size notation denote proposition independent conditioned disjoint sets variables denotes conditional dependence. shorthand improve readability. markov network undirected graphical model represents joint probability distribution node graph represents random variables domain absences edges encode conditional independences among them. assume underlying probability distribution graph-isomorph faithful means faithful undirected graph. graph said faithful distribution graph connectivity represents exactly dependencies independences existent distribution. detail means disjoint sets independent given vertices separates vertices vertices graph words means that removing vertices exists path remaining graph variable variable example figure variables separates generally shown necessary suﬃcient condition distribution graph-isomorph independence relations satisfy following axioms disjoint sets variables individual variable operation algorithms also assume existence oracle answer statistical independence queries. standard assumptions needed formally proving correctness independence-based structure learning algorithms gsmn∗ gsimn independence-based algorithms learning structure markov network domain. approach works evaluating number statistical independence statements reducing structures consistent results tests singleton inferring structure possible one. mentioned above theory assume existence independence-query oracle provide information conditional independences among domain variables. viewed instance statistical query oracle practice oracle exist; however implemented approximately statistical test evaluated data example discrete data pearson’s conditional independence chi-square test mutual information test etc. continuous gaussian data statistical test used measure conditional independence partial correlation determine conditional independence variables given data statistical test returns p-value. p-value test equals probability obtaining value test statistic least extreme actually observed given null hypothesis true corresponds conditional independence case. assuming p-value test statistical test concludes dependence less equal threshold i.e. thus learn structure theoretically suﬃces perform tests i.e. test pair variables unfortunately non-trivial domains usually involves test conditions large number variables. large conditioning sets produce sparse contingency tables result unreliable tests. number possible conﬁgurations variables grows exponentially size conditioning set—for example cells test involving binary variables table data point cell would need data least exponential size i.e. exacerbating problem data point cell typically necessary reliable test recommended cochran cells contingency table less data points test deemed unreliable. therefore gsmn∗ gsimn algorithms attempt minimize conditioning size; choosing order examining variables irrelevant variables examined last. section present main algorithms gsmn∗ gsimn supporting concepts required description. purpose aiding understanding reader discussing ﬁrst describe abstract gsmn algorithm next section. helps showing intuition behind algorithms laying foundation them. sake clarity exposition discussing ﬁrst algorithm gsmn∗ describe intuition behind describing general structure using abstract gsmn algorithm deliberately leaves number details unspeciﬁed; ﬁlled-in concrete gsmn∗ algorithm presented next section. note choices abstract gsmn algorithm shown algorithm given input data variables gsmn computes nodes adjacent variable completely determine structure domain algorithm consists main loop learns markov blanket node domain using algorithm. constructs markov network structure connecting variable algorithm ﬁrst proposed margaritis thrun shown algorithm consists phases grow phase shrink phase. grow phase proceeds attempting variable current hypothesized neighbors contained initially empty. grows variable iteration grow loop found dependent given current hypothesized neighbors ordering variables examined grow phase variables might true neighbors underlying mn—these called false positives. justiﬁes shrink phase algorithm removes false positive testing independence conditioned found independent shrink phase cannot true neighbor gsmn removes assuming faithfulness correctness independence query results shrink phase contains exactly neighbors underlying markov network. next section present concrete implementation gsmn called gsmn∗. augments gsmn specifying concrete ordering variables examined main loop gsmn well concrete order variables examined grow shrink phases algorithm section discuss ﬁrst algorithm gsmn∗ learning structure markov network domain. note reason introducing gsmn∗ addition main contribution gsimn algorithm comparison reasons. particular gsimn gsmn∗ identical structure following order examination variables diﬀerence inference gsimn introducing gsmn∗ therefore makes possible measure precisely beneﬁts inference performance. gsmn∗ algorithm shown algorithm structure similar abstract gsmn algorithm. notable diﬀerence order variables examined speciﬁed; done initialization phase so-called examination order grow order variable determined. priority queues initially permutation position variable queue denotes priority e.g. means variable highest priority followed ﬁnally similarly position variable determines order examined grow phase initialization phase algorithm computes strength unconditional dependence pair variable given unconditional p-value independence test pair variables denoted algorithm. particular algorithm gives higher priority variables lower average p-value indicating stronger dependence. average deﬁned grow order variable algorithm gives higher priority variables whose p-value variable small ordering intuition behind folk-theorem states probabilistic inﬂuence association attributes tends attenuate distance graphical model. suggests pair variables high unconditional p-value less likely directly linked. note ordering heuristic guaranteed hold general. example hold underlying domain bayesian network e.g. spouses independent unconditionally dependent conditional common child. note however example apply faithful domains i.e. graph-isomorph markov network. also note correctness algorithms present depend holding i.e. prove appendices gsmn∗ gsimn guaranteed return correct structure assumptions stated section above. also note computational cost calculation empty conditioning set. attempt infer dependence propagation. attempt infer independence propagation. else statistical test data. true p-value statistical test return initialization phase. main loop includes three phases propagation phase grow phase shrink phase propagation phase optimization variables already computed collected sets contains variables sets passed independence procedure igsmn∗ shown algorithm purpose avoiding execution tests algorithm. justiﬁed fact that undirected graphs markov blanket markov blanket variables already found contain blanket cannot members exists variables rendered conditionally independent previous step independence therefore inferred easily. note experiments section paper evaluate gsmn∗ without propagation phase order measure eﬀect propagation optimization performance. turning propagation accomplished simply setting sets empty set. another diﬀerence gsmn∗ abstract gsmn algorithm condition additional optimization avoids independence test case found independent initialization phase since case would imply independent given conditioning axiom strong union. crucial diﬀerence gsmn∗ abstract gsmn algorithm gsmn∗ changes examination order grow order every variable changes ordering proceed follows grow phase variable examination order dictates next variable examined last added growing phase examined grow order variables found dependent also changed; done maximize number optimizations gsimn algorithm shares algorithm structure gsmn∗. changes grow order therefore explained detail section gsimn presented. ﬁnal diﬀerence gsmn∗ abstract gsmn algorithm restart actions grow shrink phases gsmn whenever current markov blanket modiﬁed present gsmn∗. restarting loops necessary algorithm original usage learning structure bayesian networks. task possible true member blanket found initially independent grow loop conditioning found dependent later conditioned superset could happen unshielded spouse i.e. common children existed direct link underlying bayesian network. however behavior impossible domain distribution faithful markov network independence given must hold superset axiom strong union restart grow shrink loops therefore omitted gsmn∗ order save unnecessary tests. note that even though possible behavior impossible faithful domains possible unfaithful ones also experimentally evaluated algorithms real-world domains assumption markov faithfulness necessarily hold demonstrate operation gsmn∗ graphically concept independence graph introduce. deﬁne independence graph undirected graph conditional independences dependencies single variables represented annotated edges them. solid edge variables annotated represents fact found dependent given conditioning enclosed parentheses edge represents independence dependence inferred eqs. shown graphically instance figure dotted edge annotated represents fact absence edge variables indicates absence information independence dependence variables conditioning set. example figure illustrates operation gsmn∗ using independence graph domain whose underlying markov network shown figure ﬁgure shows independence graph grow phase variable ﬁrst examination order grow orders shown ﬁgure.) according vertex separation underlying network variables found dependent growing phase i.e. section present prove theorem used subsequent gsimn algorithm. seen main idea behind gsimn algorithm attempt decrease number tests done exploiting properties conditional independence relation faithful domains i.e. eqs. properties seen inference rules used derive independences ones know true. careful study axioms suggests simple inference rules stated triangle theorem below suﬃcient inferring useful independence information inferred systematic application inference rules. conﬁrmed experiments section figure independence graph depicting triangle theorem. edges graph labeled sets represent conditional independences dependencies. solid edge labeled means dependent given label enclosed parentheses means edge inferred theorem. represent triangle theorem graphically using independence graph construct section figure depicts rules triangle theorem using independence graphs. triangle theorem used infer additional conditional independences tests conducted operation gsmn∗. example shown figure illustrates application triangle theorem example presented figure independence information inferred triangle theorem shown curved edges figure illustration triangle theorem example figure variables enclosed parentheses correspond tests inferred triangle theorem using adjacent edges antecedents. example result inferred i-triangle rule independence dependence example independence edge inferred d-triangle rule adjacent edges annotated respectively. annotation inferred edge intersection annotations example application i-triangle rule edge inferred edges annotations respectively. annotation inferred edge intersection annotations previous section possibility using rules triangle theorem infer result novel tests grow phase. gsimn algorithm introduced section uses triangle theorem similar fashion extend gsmn∗ inferring value number tests gsmn∗ executes making evaluation unnecessary. gsimn gsmn∗ work exactly diﬀerences concentrated independence procedure instead using independence procedure igsmn∗ gsmn∗ gsimn uses procedure igsimn shown algorithm procedure igsimn addition attempting propagate blanket information obtained examination previous variables also attempts infer value independence test provided input either strong union axiom triangle theorem. attempt successful igsimn returns value inferred otherwise defaults statistical test data purpose assisting inference process gsimn igsimn maintain knowledge base pair variables containing outcomes tests evaluated knowledge bases empty beginning gsimn algorithm maintained within test procedure igsimn. igsimn attempts infer independence value input triplet applying single step backward chaining using strong union triangle rules i.e. searches knowledge base {kxy antecedents instances rules input triplet consequent. strong union rule used direct shown eqs. also contrapositive form. direct form used infer independences therefore refer i-su rule contrapositive form i-su rule becomes referred d-su rule since used infer dependencies. according d-triangle d-su rules dependence inferred knowledge base contains figure illustration operation gsimn. ﬁgure shows grow phase consecutively examined variables ﬁgure shows variable examined second according change examination order lines algorithm variables enclosed parentheses correspond tests inferred triangle theorem using adjacent edges antecedents. results shown highlighted executed inferred tests done changes grow orders variables occur inside grow phase currently examined variable particular variable algorithm reaches line i.e. igsimn false variables found dependent promoted beginning grow order illustrated figure variable depicts grow phase consecutively examined variables ﬁgure curved edges show tests inferred igsimn grow phase variable grow order changes grow phase variable complete variables promoted beginning queue. rationale observation increases number tests inferred gsimn next step change examination grow orders described chosen inferred tests learning blanket variable match exactly required algorithm future step. particular note example inferred dependencies variable found dependent exactly required initial part grow phase variable shown highlighted figure independence tests inferred resulting computational savings. general last dependent variable grow phase maximum number dependences independences inferred provides rationale change grow order selection algorithm examined next. shown assumptions gsmn∗ structure returned gsimn correct i.e. computed gsimn algorithm equals exactly neighbors proof correctness gsimn based correctness gsmn∗ presented appendix section discuss number practical issues subtly inﬂuence accuracy eﬃciency implementation gsimn. order application i-su d-su i-triangle d-triangle rules within function igsimn. given independence-query oracle order application matter—assuming rules inferring value independence guaranteed produce value soundness axioms eqs. practice however oracle implemented statistical tests conducted data incorrect previously mentioned. particular importance observation false independences likely occur false dependencies. example case domain dependencies weak—in case pair variables connected underlying true network structure incorrectly deemed independent paths long enough. hand false dependencies much rare— conﬁdence threshold statistical test tells probability false dependence chance alone assuming i.i.d. data test chance multiple false dependencies even lower decreasing exponentially fast. practical observation i.e. dependencies typically reliable independences provide rationale igsimn algorithm works. particular igsimn prioritizes application rules whose antecedents contain dependencies ﬁrst i.e. d-triangle d-su rules followed i-triangle i-su rules. eﬀect uses statistical results typically known greater conﬁdence ones usually less reliable. second practical issue concerns eﬃcient inference. gsimn algorithm uses onestep inference procedure utilizes knowledge base {kxy containing known independences dependences pair variables implement inference eﬃciently utilize data structure purpose storing retrieving independence facts constant time. consists arrays dependencies another independencies. array size number variables domain. cell array corresponds pair variables stores known independences form list conditioning sets. conditioning list knowledge base represents known independence important note length list tests done variable execution gsimn thus always takes constant time retrieve/store independence therefore inferences using knowledge base constant time well. also note uses strong union axion igsimn algorithm constant time well accomplished testing sets stored subset superset inclusion. evaluated gsmn∗ gsimn algorithms artiﬁcial real-world data sets. experimental results presented show simple application pearl’s inference rules gsimn algorithm results signiﬁcant reduction number tests performed compared gsmn∗ without adversely aﬀecting quality output network. particular report following quantities weighted number tests. weighted number tests computed summation weight test executed weight test deﬁned +|z|. quantity reﬂects time complexity algorithm used assess beneﬁt gsimn using inference instead executing statistical tests data. standard method comparison independence-based algorithms justiﬁed observation running time statistical test triplet proportional size data number variables involved i.e. construct non-zero entries contingency table used test examining data point data exactly once time proportional number variables involved test i.e. proportional }∪z| +|z|. order assess impact inference running time report execution time algorithm. normalized hamming distance. hamming distance output network structure underlying model another measure quality output network actual network used generate data known. hamming distance deﬁned number reversed edges network structures i.e. number times actual edge true network missing returned network edge absent true network exists algorithm’s output network. value zero means output network correct structure. able compare domains diﬀerent dimensionalities normalize accuracy. real-world data sets underlying network unknown hamming distance calculation possible. case impossible know true value independence. therefore approximate statistical test entire data limited randomly chosen subset learn network. measure accuracy compare result experiments involving data sets used statistical test estimation conditional independences. mentioned above rules thumb exist deem certain tests potentially unreliable depending counts contingency table involved; example rule cochran deems test unreliable cells contingency table less data points test. requirement answer must obtained independence algorithm conducting test used outcomes tests well experiments. eﬀect possibly unreliable tests quality resulting network measured accuracy measures listed above. ﬁrst experiments underlying model called true model true network known markov network. purpose experiments conduct controlled evaluation quality output network systematic study algorithms’ behavior varying conditions domain size amount dependencies true network contains variables generated randomly follows network initialized nodes edges. user-speciﬁed parameter network structure average node degree equals average number neighbors node. given every node neighbors determined randomly uniformly selecting ﬁrst pairs random permutation possible pairs. factor necessary edge contributes degree nodes. known-model experiments assume result statistical queries asked gsmn∗ gsimn algorithms available assumes existence oracle answer independence queries. underlying model known oracle implemented vertex separation. beneﬁts querying true network independence first ensures faithfulness correctness independence query results allows evaluation algorithms assumptions correctness. second tests performed much faster actual statistical tests data. allowed evaluate algorithms large networks—we able conduct experiments domains containing variables. ﬁrst report weighted number tests executed gsmn∗ without propagation gsimn. results summarized figure shows ratio weighted number tests gsimn versions gsmn∗. hundred true networks generated randomly pair ﬁgure shows mean value. limiting reduction weighted number tests depends primarily average degree parameter reduction gsimn large dense networks approximately compared gsmn∗ propagation compared gsmn∗ without propagation optimization demonstrating beneﬁt gsimn gsmn∗ terms number tests executed. reasonable question performance gsimn extent inference procedure complete i.e. tests gsimn needs operation number tests infers compare number tests inferred measure this compared number tests done gsimn number done alternative algorithm call gsimnfch gsimn-fch diﬀers gsimn function ifch shown algorithm replaces function igsimn gsimn. ifch exhaustively produces independence statements inferred properties eqs. using forward-chaining procedure. process iteratively builds knowledge base containing truth value conditional independence predicates. whenever outcome test required queried value test found returned gsimn-fch performs test uses result standard forward-chaining automatic theorem prover subroutine produce independence statements inferred test result adding facts comparison number tests executed gsimn gsimn-fch presented figure shows ratio number tests gsimn gsimn-fch. ﬁgure shows mean value four runs corresponding network generated randomly pair unfortunately days execution gsimn-fch unable complete execution domains containing variables more. therefore present results domain sizes only. ﬁgure shows every ratio exactly i.e. tests inferable produced triangle theorem gsimn. smaller domains ratio exception single case experiments evaluate gsmn∗ gsimn data sampled true model. allows realistic assessment performance algorithms. data sampled true markov network using gibbs sampling. exact learning experiments previous section structure true network required generated randomly fashion described above. sample data known structure however also needs specify network parameters. random network parameters determine strength dependencies among connected variables graph. following agresti used log-odds ratio measure strength probabilistic inﬂuence binary variables deﬁned network parameters generated randomly log-odds ratio every pair variables connected edge graph speciﬁed value. experiments used values every pair variables network. figures show plots normalized hamming distance true network output gsmn∗ gsimn domain sizes variables respectively. plots show hamming distance gsimn comparable ones gsmn∗ algorithms figure shows weighted number tests gsimn gsmn∗ sampled data points domains average degree parameters log-odds ratios gsimn shows reduced weighted number tests respect gsmn∗ without propagation cases compared gsmn∗ propagation cases sparse networks weak dependences i.e. reduction larger domain sizes reduction much larger observed exact learning experiments. actual execution times various data sizes network densities shown figure largest domain verifying reduction cost gsimn various data sizes. note reduction proportional number data points; reasonable test executed must entire data construct contingency table. conﬁrms claim cost inference gsimn small compared execution time tests themselves indicates increasing cost beneﬁts gsimn even large data sets. also conducted sampled data experiments well-known real-world networks. known repository markov networks drawn real-world domains instead utilized well-known bayesian networks widely used bayesian network research available number repositories. generate markov networks bayesian network structures used process moralization consists steps connect pair nodes bayesian network common child undirected edge remove directions edges. results markov network local markov property valid i.e. node conditionally independent nodes domain given direct neighbors. procedure conditional independences lost. this however aﬀect accuracy results compare independencies output network moralized markov network conducted experiments using real-world domains hailﬁnder insurance alarm mildew water. domain sampled varying number data points corresponding bayesian network using logic sampling used input gsmn∗ gsimn algorithms. compared network output algorithms original moralized network using normalized hamming distance metric previously described. results shown figure normalized hamming distance network output gsmn∗ gsimn true markov networks network using varying data sizes sampled markov networks various real-world domains modeled bayesian networks. also measured weighted cost three algorithms domains shown fig. plots show signiﬁcant decrease weighted number tests gsimn respect gsmn∗ algorithms cost gsimn cost gsmn∗ without propagation average savings cost gsimn cost gsmn∗ without propagation average savings artiﬁcial data studies previous section advantage allowing controlled systematic study performance algorithms experiments real-world data necessary realistic assessment performance. real data challenging come non-random topologies underlying probability distribution faithful. conducted experiments number data sets obtained machine learning data repository continuous variables data sets discretized using method widely recommended introductory statistics texts dictates optimal number equally-spaced discretization bins continuous variable number points figure ratio weighted number tests gsimn versus gsmn∗ diﬀerence accuracy gsimn gsmn∗ real data sets. ratios smaller positive bars indicate advantage gsimn gsmn∗. numbers x-axis indices data sets shown table table weighted number tests accuracy several real-world data sets. evaluation measure best performance gsmn∗ gsimn indicated bold. number variables domain denoted number data points data real-world data structure underlying bayesian network unknown impossible measure hamming distance resulting network structure. instead measured estimated accuracy network produced gsmn∗ gsimn comparing result number conditional independence tests network learned result tests performed data approach similar estimating accuracy classiﬁcation task unseen instances inputs triplets class attribute value corresponding conditional independence test. used real-world data input gsmn∗ gsimn entire data test. corresponds hypothetical scenario much smaller data available researcher approximates true value test outcome entire data set. since number possible tests exponential estimated independence accuracy sampling triplets randomly evenly distributed among possible conditioning sizes tests triplets constructed follows first variables drawn randomly second conditioning determined picking ﬁrst variables random permutation denoting triplets triplet idata result test performed entire data inetwork result test performed data sets table shows detailed results accuracy weighted number tests gsmn∗ gsimn algorithms. results also plotted figure horizontal axis indicating data index appearing ﬁrst column table figure plots quantities graph real-world data sets ratio weighted number tests gsimn versus gsmn∗ algorithms diﬀerence accuracies. data improvement gsimn gsmn∗ corresponds number smaller ratios positive histogram accuracy diﬀerences. observe gsimn reduced weighted number tests every data maximum savings gsmn∗ without propagation gsmn∗ propagation moreover data sets gsimn resulted improved accuracy somewhat reduced accuracy compared gsmn∗ propagation paper presented algorithms gsmn∗ gsimn learning eﬃciently structure markov network domain data using independence-based approach evaluated performance measurement weighted number tests require learn structure network quality networks learned artiﬁcial real-world data sets. gsimn showed decrease vast majority artiﬁcial real-world domains output network quality comparable gsmn∗ cases showing improvement. addition gsimn shown nearly optimal number tests executed compared gsimn-fch uses exhaustive search produce independence information inferred pearl’s axioms. directions future research include investigation topology underlying markov network aﬀects number tests required quality resulting network especially commonly occurring topologies grids. another research topic impact number tests examination grow orderings variables. algorithm examines every variable inclusion grow phase added grow phase considers removal shrinking phase note test executed growing phase call grow test similarly tests executed shrinking phase; test called shrink test proof. assume shrink phase. then either added grow phase removed shrink phase former true line found independent line latter true found independent line cases strong union opposite direction proved lemma below. however proof involved requiring auxiliary lemmas observations deﬁnitions. main auxiliary lemmas lemma presented next inductively extend conditioning dependencies found grow shrink tests remaining variables v−{x lemma shows that certain independence holds conditioning dependence increased variable. introduce notation deﬁnitions prove auxiliary lemmas. denote value grow phase i.e. variables found dependent grow phase value shrink phase also denote variables found independent grow phase sequence variables shrunk i.e. found independent shrink phase. sequence assumed ordered follows variable found independent shrinking phase. preﬁx ﬁrst variables denoted test performed algorithm deﬁne integer preﬁx containing variables found independent loop furthermore abbreviate proof. according line algorithm beginning shrink phase variables found independent afterward conducted removed line thus time performed conditioning becomes theorem every dependence model satisfying symmetry decomposition intersection unique markov network produced deleting complete graph every edge holds i.e. gsimn algorithm diﬀers gsmn∗ test subroutine igsimn instead igsmn∗ turn diﬀers number additional inferences conducted obtain independencies inferences direct applications strong union axiom triangle theorem using correctness gsmn∗ therefore conclude gsimn algorithm correct.", "year": 2014}