{"title": "Training of Deep Neural Networks based on Distance Measures using  RMSProp", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "The vanishing gradient problem was a major obstacle for the success of deep learning. In recent years it was gradually alleviated through multiple different techniques. However the problem was not really overcome in a fundamental way, since it is inherent to neural networks with activation functions based on dot products. In a series of papers, we are going to analyze alternative neural network structures which are not based on dot products. In this first paper, we revisit neural networks built up of layers based on distance measures and Gaussian activation functions. These kinds of networks were only sparsely used in the past since they are hard to train when using plain stochastic gradient descent methods. We show that by using Root Mean Square Propagation (RMSProp) it is possible to efficiently learn multi-layer neural networks. Furthermore we show that when appropriately initialized these kinds of neural networks suffer much less from the vanishing and exploding gradient problem than traditional neural networks even for deep networks.", "text": "abstract—the vanishing gradient problem major obstacle success deep learning. recent years gradually alleviated multiple different techniques. however problem really overcome fundamental since inherent neural networks activation functions based products. series papers going analyze alternative neural network structures based products. ﬁrst paper revisit neural networks built layers based distance measures gaussian activation functions. kinds networks sparsely used past since hard train using plain stochastic gradient descent methods. show using root mean square propagation possible efﬁciently learn multi-layer neural networks. furthermore show appropriately initialized kinds neural networks suffer much less vanishing exploding gradient problem traditional neural networks even deep networks. introduction types neural networks nowadays trained using stochastic gradient descent however gradient-based approaches suffer several drawbacks. critical drawback vanishing gradient problem makes hard learn parameters \"front\" layers n-layer network. problem becomes worse number layers architecture increases. especially critical recurrent neural networks trained unfolding deep feedforward networks. time step input sequence processed network layer created. second drawback gradient-based approaches potential stuck local minima saddle points. overcome vanishing gradient problem several methods proposed past early methods consisted pre-training weights using unsupervised learning techniques order establish better initial conﬁguration followed supervised ﬁne-tuning backpropagation. unsupervised pre-training used learn generally useful feature detectors. ﬁrst hinton used stacked restricted boltzmann machines greedy layer-wise unsupervised training deep neural networks xavier initialization glorot step overcome vanishing gradient problem. xavier initialization initial weights neural networks sampled gaussian distribution variance function number neurons given layer. time rectiﬁed linear units introduced non-linearity e-mail thomas.kurbielqiagen.com. shahrzad khaleghian department marketing mercator school management university duisburg-essen germany e-mail shahrzad.kurbieluni-due.de. scale-invariant around saturate large input values xavier initialization relus alleviated problems sigmoid activation functions vanishing gradient. another method particularly used cope vanishing gradient problem recurrent neural networks long short-term memory network introducing memory cells gradient propagated back much earlier time without vanishing. newest effective ways resolve vanishing gradient problem residual neural networks resnets yield lower training error shallower counterparts simply reintroducing outputs shallower layers network compensate vanishing data. main reason behind vanishing gradient problem fundamental difference forward backward pass neural networks activation functions based products forward pass highly non-linear nature backward pass completely linear ﬁxed values activations obtained forward pass. backpropagation network behaves linear system suffers problem linear systems tendency either explode iterated. paper address problem vanishing gradient exploring properties neural networks based distance measures alternative neural networks based products neural networks based distance measures weight vectors neurons represent landmarks input space. activation neuron computed weighted distance input vector corresponding landmark. determining distance possible applying different measures. work apply commonly used gaussian kernel parameters network learnt consist landmarks weights distance measure. please note used d-gaussian function aligned axes. particular rotation matrix used. next traverse d-gaussian function along trajectory deﬁned resulting trajectory space depicted fig. track point traversing trajectory ﬁrst climbs gaussian function descends climbs again. hence resulting curve function exhibits higher complexity constituent parts fig. curve generated described manner used deﬁne e.g. x-coordinate much complex path fig. repeating approach recursive manner allows approximate continuous functions demonstrated below. section show network gaussian layers interconnected feed-forward used approximate arbitrary bounded continuous functions. demonstration purposes simple example fig. shape together approximation depicted fig. parameters network optimized using backpropagation algorithm introduced next section. approximation practice neural networks activation functions based products applied well. sl+} denotes number layers network fig. output activations depends input parameters network implicitly assumed following formulas. forward pass layer inputs denoted outputs denoted depicted fig. value j-th output depends inputs layer calculated using following gaussian function cost function cost function measures well neural network performs training samples desired output denote cost function respect single training sample represent parameters network based distance measures. given training samples overall cost function average cost functions individual training examples plus regularization term cost functions typically used multiplying square term splitting terms possible organize parameters matrices matrixvector operations. thus take advantage fast linear algebra routines quickly perform calculations network. rmsprop possible train neural networks based distance measures gaussian activations functions using plain mini-batch gradient descent momentum. extremely slow convergence achieved way. case even shallow networks. solution apply rmsprop renders training possible ﬁrst place although variance activations slightly decreasing every layer vanishing late layers furthermore dying saturated neurons observed case relus behaviour observed backpropagted gradients alternating optimization method discovered technique improves convergence training deep neural networks drastically. trick optimize sets weights nodes layers alternating way. certain number iterations epoch weights optimized treated constants. certain number iterations opposite done. subsequently ﬁrst step repeated ﬁnalization step late epochs optionally combined optimization performed. positive side effect applying alternating optimization method lies reduced computational load. regularization help prevent overﬁtting make regularization since sets parameters optimized neural networks based distance measures overall cost function form denotes number samples training regularization strengths. general selected independently getting optimal results. second third term tend decrease magnitude weights thus helps prevent overﬁtting. another preventing overﬁtting using neural networks based distance measures apply alternating optimization method introduced previous section. optimizing half coefﬁcients moment reduces actual number degrees freedom though acting form regularization. initialization neural networks based products sensible initialization weights crucial convergence especially training deep neural networks. initialization weights controlling center gaussian functions straightforward denotes normal distribution. experiments show values work well network architectures. initialization depend neither number inputs number outputs layer. experiments mnist subsection demonstrate classiﬁcation performance presented neural networks based distance measures gaussian activation functions comparable classiﬁcation performance traditional neural networks. well-known mnist benchmark handwritten digit images. mnist consists datasets handwritten digits size pixels training testing goal experiment primarily achieve state-of-the-art results would require convolutional layers novel elastic training image deformations. small fully-connected network hidden layers nodes layer. input network consists pixel coordinates desired output merely grayvalue corresponding pixel coordinate. apply full batch gradient descent. quadratic cost regularization utilize degrees freedom. result approximation depicted fig. input signal consists pixel intensities original gray scale images mapped real values pixel intensity range dimension input signal network architecture built fully-connected hidden layers units layer. crossentropy cost function apply regularization radii terms i.e. variable learning rate shrinks multiplicative constant epoch. since neural networks general capable approximating continuous functions approximated image exhibits blurry appearance. slightly increased number layers hidden units approximation made indistinguishable original. approximating probability density function last example approximate simple probability density function consisting bivariate gaussian distribution covariance matrix identity matrix i.e. rotated gaussian function see. fig. tς−t since layer neural networks based distance measures gaussian activation functions consists gaussian functions aligned main axes want well minimal network consisting fullyconnected hidden layer units able approximate function. result depicted right hand side fig. approximation vicinity peak excellent. surrounding area slight distortion visible. overall approximation images section demonstrate ability presented neural networks based distance measures gaussian activation functions approximate arbitrary functions. small grayscale natural image. images general form interpreted functions conclusions future work paper revisited neural network structures based distance measures gaussian activation functions. showed training type neural network feasible using stochastic gradient descent combination rmsprop. showed also proper initialization networks vanishing gradient problem much less traditional neural networks. future work examine neural network architectures built combinations traditional layers layers based distance measures gaussian activation functions introduced paper. particular layers based distance measures output layer showing promising results. currently analyzing performance neural networks based distance measures used deep recurrent networks. klaus debes alexander koenig horst-michael gross. transfer functions artiﬁcial neural networks. journal media neural cognitive science education deep feedforward neural networks. artiﬁcial intelligence statistics pages goodfellow yoshua bengio aaron courville. deep learning. press http//www.deeplearningbook.org.", "year": 2017}