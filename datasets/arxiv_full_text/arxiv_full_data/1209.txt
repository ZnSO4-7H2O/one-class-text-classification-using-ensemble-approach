{"title": "ZNN - A Fast and Scalable Algorithm for Training 3D Convolutional  Networks on Multi-Core and Many-Core Shared Memory Machines", "tag": ["cs.NE", "cs.CV", "cs.DC", "cs.LG"], "abstract": "Convolutional networks (ConvNets) have become a popular approach to computer vision. It is important to accelerate ConvNet training, which is computationally costly. We propose a novel parallel algorithm based on decomposition into a set of tasks, most of which are convolutions or FFTs. Applying Brent's theorem to the task dependency graph implies that linear speedup with the number of processors is attainable within the PRAM model of parallel computation, for wide network architectures. To attain such performance on real shared-memory machines, our algorithm computes convolutions converging on the same node of the network with temporal locality to reduce cache misses, and sums the convergent convolution outputs via an almost wait-free concurrent method to reduce time spent in critical sections. We implement the algorithm with a publicly available software package called ZNN. Benchmarking with multi-core CPUs shows that ZNN can attain speedup roughly equal to the number of physical cores. We also show that ZNN can attain over 90x speedup on a many-core CPU (Xeon Phi Knights Corner). These speedups are achieved for network architectures with widths that are in common use. The task parallelism of the ZNN algorithm is suited to CPUs, while the SIMD parallelism of previous algorithms is compatible with GPUs. Through examples, we show that ZNN can be either faster or slower than certain GPU implementations depending on specifics of the network architecture, kernel sizes, and density and size of the output patch. ZNN may be less costly to develop and maintain, due to the relative ease of general-purpose CPU programming.", "text": "abstract—convolutional networks become popular approach computer vision. important accelerate convnet training computationally costly. propose novel parallel algorithm based decomposition tasks convolutions ffts. applying brent’s theorem task dependency graph implies linear speedup number processors attainable within pram model parallel computation wide network architectures. attain performance real shared-memory machines algorithm computes convolutions converging node network temporal locality reduce cache misses sums convergent convolution outputs almost wait-free concurrent method reduce time spent critical sections. implement algorithm publicly available software package called znn. benchmarking multi-core cpus shows attain speedup roughly equal number physical cores. also show attain speedup many-core speedups achieved network architectures widths common use. task parallelism algorithm suited cpus simd parallelism previous algorithms compatible gpus. examples show either faster slower certain implementations depending speciﬁcs network architecture kernel sizes density size output patch. less costly develop maintain relative ease general-purpose programming. standard formulation supervised learning starts parametrized class mappings training desired input-output pairs loss function measuring deviation actual output desired output. goal learning minimize average loss training set. popular minimization method stochastic gradient descent. input sequence parameters mapping updated minus direction gradient loss respect parameters. concerned class mappings known convolutional networks fig. convnet computation graph. leftmost node input image rightmost nodes output images. edges represent convolution transfer function pooling/ﬁltering operations. caffe torch theano. convnet learning also distributed multiple machines however relatively little work parallelizing convnet learning single shared memory machines. introduce software package called implements novel parallel algorithm convnet learning multi-core many-core machines. implements convnets special case. employ either direct convolution chooses methods autotuning layer network. convolution previously applied convnets running gpus even advantageous convnets cpus. deﬁne convnet using directed acyclic graph called computation graph node represents image edge image ﬁltering operation. multiple edges converge node node sums outputs ﬁltering operations represented edges. convenience discussion assume images kernels isotropic dimensions though restriction necessary znn. image ﬁltering operations four following types. convolution weighted linear combination voxels within sliding window computed location window image. weights linear combination called kernel. input image size kernel size output image size image size decreases output voxel exists sliding window fully contained input image. convolution allowed sparse meaning every image voxel within sliding window enters linear combination. max-pooling divides image size blocks size divisible maximum value computed block yielding image size max-ﬁltering maximum within sliding window computed location window image. window size input image size output image size max-ﬁltering performed sequential max-ﬁltering arrays three directions. array keep heap size containing values inside sliding window. element array inserted removed once operation taking position sliding window heap contain maximum value. transfer function adds number called bias voxel image applies nonlinear function result. nonlinear function typically nondecreasing. common choices logistic function hyperbolic tangent half-wave rectiﬁcation. nodes convergent edges adjacent graph separated nonlinear ﬁltering edges. reasonable constraint composition convolutions collapsed single convolution thereby simplifying graph. max-pooling convnet context visual object recognition special case deﬁnition given above. max-ﬁlterings used. size input image convolutions max-poolings reduce output image exactly pixel/voxel. single output representing whether input image belongs given object class outputs representing membership object classes. localization detection desired well recognition slide window large image apply max-pooling convnet location window input image size convnet ﬁeld view size output image size sliding window max-pooling convnet also useful context boundary detection image segmentation however computationally wasteful literally implement computation way. efﬁcient maxﬁltering convnet max-ﬁltering layer increases sparsity subsequent convolutions factor equal size max-ﬁltering window approach called skip-kernels ﬁlter rarefaction equivalent results max-fragmentation-pooling implement above general sparsity convolution need increase lock step max-ﬁltering controlled independently. sparsity control capability confer great deal ﬂexibility convnets. could useful implementing scale-invariant convnet convolutions shared kernel weights performed multiple scales capture scale-invariant features. scale-invariant convolution easily achieved controlling sparsity convolutions. unlike max-pooling max-ﬁltering decrease resolution ﬁltered images. thus every image max-ﬁltering convnets keeps original resolution. particularly beneﬁcial multi-scale approach images multiple resolutions combined together construct representation. max-pooling convnets upsampling commonly used adjust different resolutions images multiple levels. max-ﬁltering convnet contrast removes need upsampling elegant much efﬁcient way. backpropagation algorithm calculating gradient loss function respect trainable parameters convnet kernels biases. input calculation proceeds several phases turns backward pass represented another graph looks forward computation graph except direction every edge reversed. output nodes forward graph become input nodes backward graph initialized gradient loss function respect voxels output nodes forward graph. nodes backward graph associated images distinct ones associated nodes forward graph. every edge backward graph multiplication transpose jacobian matrix operation represented corresponding edge forward computation graph. four edge operations forward graph become following four edge operations backward graph. convolution jacobian convolution forward pass becomes convolution backward pass. kernel same except reﬂected along three dimensions. input image size kernel size output image size image size increases output voxel exists whenever sliding window overlap input image. max-pooling jacobian within block voxels zeroed except identiﬁed maximum within block forward pass. image size expanded image size max-ﬁltering jacobian every element image size initialized zero. position sliding window appropriate value input accumulated position maximum element selected window forward pass. forward backward passes complete forward images nodes forward computation graph backward images nodes backward computation graph used update kernels biases follows. kernel update convolution going node node forward graph gradient loss respect kernel computed convolving reﬂected forward image node backward image node valid convolution performed yielding image size kernel. multiplied small learning rate parameter subtracted kernel. bias update bias node gradient loss calculated voxels backward image node scalar result multiplied small learning rate parameter subtracted bias. single convolution image size kernel size well-known method becomes efﬁcient direct method sufﬁciently large kernel sizes. crossover point equal complexity satisﬁes less well-known fft-direct crossover occurs smaller kernel sizes convnet single convolution image node shared edges node performs layerwise auto-tuning choose fft-based direct convolution laeyer. complexity reduced memoizing ffts images kernels obtained forward pass reuse backward pass weight update. possibility previously noted passing implemented limited onboard memory reduction complexity approximately third entire gradient learning calculation represented task dependency graph node represents four forward operations four backward operations update operations described above. additional tasks interface training set. data provider obtains training sample used single round training loss gradient calculates gradient loss respect network output. table computational complexity fully connected convolutional layer maps input images output images using kernels. complexity n×n×n image assumed complexity measured number ﬂoating point operations. additionally backward pass executed current forward pass forward task also depends previous update task relevant gradient learning iterative gradient calculated repeatedly cycling forward backward update. fig. shows convnet learning graph corresponding convnet computation graph fig. convenience analysis steps iteration gradient learning followed steps next iteration. therefore forward tasks bottom graph backward tasks top. topmost dark circle nodes represent tasks calculate gradient loss respect output network obtained previous forward pass. yellow circle middle represents task providing input image forward tasks bottom. note update tasks pooling/ﬁltering. deﬁne time required processors perform learning iteration. would like parallel algorithm achieves large speedup t/tp ideally approaches linear speedup possible wide convnet architectures contain many convolutions done parallel. formalize intuition following. estimate theoretically achievable speedup layered architectures every convolutional layer fully connected. before time complexity measured number ﬂoating point instructions. already estimate summing times tables layer network. estimate analyze following algorithm employing inﬁnite number processors. move sequentially layers perform forward tasks layer parallel. compute loss gradient output nodes parallel. move sequentially backward layers perform backward tasks layer parallel. perform weight updates kernels biases parallel. since layers done sequentially total time forward pass contributions layer speciﬁed tables time backward pass calculated similarly. since kernel bias updates done parallel total update time maximum individual update times speciﬁed tables forward backward update times yields time complexity gradient learning iteration. formulas tables depend widths layers tasks layer done parallel. exception complexity convolutional layer depends logarithmically width summing results convergent convolutions requires amount time using binary collapse algorithm described plots theoretically achievable speedup networks different width depth shown fig. cases limit large network width scales like large scales like follows diverges bound equal limit large according fig. network width reaches ﬁxed fraction maximal value increases behavior consistent expect approach ﬁxed fraction since scales like happen power means theoretically achievable speedup approaches maximum value even networks rather modest widths. fig. task dependency graph. edge computation graph generates multiple nodes task dependency graph corresponding forward backward update tasks. node colors indicate transfer function convolution pooling/ﬁltering input provider loss gradient tasks placed global queue non-update dependencies satisﬁed rationale behind design choice forward task scheduled execution without required update task done force execution update task followed forward task requires result update hence increase memory locality. tasks queue sorted priority. priorities chosen increase temporal locality computation minimize latency computation. introduce unique strict orderings nodes convnet’s computation graph based longest distance decreasing order output input node respectively. nodes distance ordered unique way. priority forward task edge equal position output node ordering based distance output nodes similarly priority backward task equal ordering based distance input nodes. ensures prioritize tasks longest path sink node task dependency intuitively favor lower latency schedules. strict ordering tasks distance increases temporal locality assuring multiple tasks distance scheduled prefer execute ones computing images accumulated thus increasing probability fig. theoretically achievable speedup using direct convolution fft-based convolution memoizing enabled. multiple lines color represent networks different depths ranging kernels networks size brent’s theorem guarantees existence parallel algorithm achieves large speedup training wide convnets. turn problem designing parallel algorithm actually achieves large speedup practice. since brent’s theorem assumes synchronization communication overhead design algorithm minimize synchronization overhead increase temporal locality computation reduce cache misses. central quantity algorithm global priority queue contains tasks ready executed together priority. predetermined number workers execute tasks global queue. update tasks lowest priority tasks. execution forced result required forward pass increases cache locality result used immediately. time update tasks executed there’s forward backward tasks ready executed. design ensures thread ever waiting completion update task rather executes required update task itself delegates forward subtask thread currently executing update task. tasks executed workers. worker picks executes task highest priority queue. forward task algorithm shown algorithm main functionality forward task apply appropriate forward-transform given input image accumulate result stored output node. task adds last image queues dependent forward tasks execution. main functionality forward task shown procedure do-forward. however procedure executed update task previous round completed. ensured creating subtask containing main functionality calling force method. force function receives update task forward subtask parameters. goal function execute forward subtask also make sure update task completed. order method ﬁrst examines state update task following executing update task currently executed thread; case forward subtask gets attached update task. ﬂags thread executing update task execute forward subtask backward task algorithm shown algorithm scheduled execution dependencies backward task satisﬁed. backward task applies appropriate backward-transform given image queues appropriate update task execution lowest common value priority. similarly forward task transformed image added stored input node thread last image queues dependent tasks execution. update tasks algorithm shown algorithm first gradient loss calculated multiplied small learning rate subtracted training parameters finally forward subtask attached detached executed. important minimize amount time spent critical sections parts code executed single thread time. main three points algorithm require synchronization memory management operations global task queue concurrent summations. operations global task priority queue synchronized. queue implemented heap lists lowering complexity insertion deletion total number tasks queue number distinct values priority tasks inside queue. depending network structure number much smaller total number tasks queue especially true wide networks. multiple edges converge node computation graph means multiple convolutions executed parallel need results accumulated sum. additions synchronized; thread allowed change sum. naive strategy waiting threads ﬁnished adding images would lead critical section time scales linearly image size propose novel method eliminates dependence image size performing pointer operations inside critical section works follows. suppose multiple threads executing add-to-sum algorithm thread points different image. would like pointer images stored object computation terminates. accomplished thread repeatedly reset pointer stored point instead. thread succeeds stops working. thread fails adds value pointed location referenced sets pointer nil. every thread continues work succeeds. last thread succeeds contain correct answer. note algorithm time-consuming additions outside critical section routines cost using memory. custom allocator dedicated images usually large dedicated small objects used auxiliary data structures. allocators maintain global pools memory chunks. pool contains chunks sizes lock-free queues described implemented part boost library used implement pool operations. difference allocators memory alignment—the image memory allocator ensures proper memory alignment utilizing simd instructions. memory shared allocators. chunk memory size requested ﬁrst rounded nearest power appropriate pool examined available memory chunks. there’s available chunk return remove pool. chunks available allocate system return de-allocating chunk memory simply added appropriate pool memory ever returned system. means memory usage program never decrease. practice convnet training consist single loop performing work memory usage peaks rounds. worst case strategy lead near memory usage overhead; however available memory rarely limiting factor training network. future might consider implementing advanced memory allocators ones thread-local pools addition global pool ones higher granularity available chunk sizes reduce size overhead. performed measurements speedup achieved proposed parallel algorithm relative serial algorithm using systems listed table amazon instances cores chosen benchmarking readily available anyone. -way system included cores though relatively specialized piece hardware. even larger number cores also benchmarked xeon phitmknights corner. measurements used intel compiler intel libraries ffts direct convolution. convnets contained four fully-connected convolutional layers kernels followed transfer function layer rectiﬁed linear function max-ﬁltering layers. convolutional layer sequence layer types ctmctmctct. output patch size convnets contained fully-connected convolutional layers kernels followed rectiﬁed fig. achieved speedups networks compared serial algorithm. linear transfer function layer max-ﬁltering layers sequence layer types ctmctmctctctct. output patch size measurements performed ﬁrst running gradient learning algorithm warm-up rounds averaging time required next rounds. measurements averaged rounds. convnets implemented special case convnets setting dimensions size one. width convnets varied described below. convolution employed direct convolution illustrate methods; reversing yields similar results. network architectures kernel sizes also yield similar results. fig. shows speedup attained various cpus function parameters number worker threads network width. graph shows result varying number workers network width held ﬁxed. achieve near maximal possible speedup requires sufﬁciently wide networks sufﬁciently many worker threads value maximal speedup equal number cores larger wide network multicore cpus speedup increases linearly number worker threads equals number cores. increase continues slower rate. wide networks xeon phitm speedup increases linearly number worker threads equals number cores slowly double number even slower number hardware threads. maximal achieved speedups networks different widths shown figs. preceding results show efﬁciently utilize cpus also important know resulting performance compares implementations convnet learning. therefore benchmarked caffe chose caffe theano titan core amazon instance chose particular comparison alternatives seemed unfair. example could specialized hardware cores instance. comparison seemed unfair specialized hardware would much costly titan less accessible amazon also could used instances amazon currently much slower titan half onboard ram. caffe default cudnn implementations used. convnets used theano ofﬁcial release caffe still support convnets. caffe theano code publicly available repository. benchmark times sparse training meaning convnet used produce predictions pixels output patch form lattice period every dimension. loss predicted output pixels layers max-pooling. noted before also perform dense training meaning convnet used produce predictions every pixel output patch applying convnet window slides across every valid location input patch. requiring caffe theano perform dense training could accomplished computing sparse outputs assemble dense output. method inefﬁcient would contest znn. comparison convnets shown fig. faster caffe theano sufﬁciently large kernels makes sense convolution efﬁcient direct convolution sufﬁciently large kernels. large kernels generally used practice competitive implementations networks. hand opens possibility efﬁciently training networks large kernels might practical application future. comparison convnets shown fig. comparable theano even modest kernel sizes outperforms theano kernel sizes greater. kernel sizes currently relevant practical applications benchmark makes sense expect crossover point complexity direct convolution occur smaller kernel sizes working memory another computational resource important training convnets. given limited amount onboard memory unable theano train networks kernel sizes larger also unable caffe train many networks enables training larger networks mostly typical system much even gpu. titan example onboard ram. additionally achieve even higher speed using extra space case memoization. using fft-based convolutions memoization disabled efﬁcient usage proposed methods. memory overhead methods proposed implemented publicly available license either fftw intel ffts either provided code intel libraries direct convolution. using fftw instead yields scalability lower absolute performances differences single thread performances libraries. repository also provides alternative scheduling strategies simple fifo lifo well complex ones based work stealing alternative scheduling strategies achieve noticeably lower scalability proposed paper networks. however speciﬁc networks might beneﬁt alternative scheduling algorithms. future work include automatic detection best scheduling strategy. achieves high performances efﬁciently utilizing available cpus. expect increase number cores chip future making even practical. fact already used achieve state results boundary detection computation dendritic arbor densities large amount available efﬁciently train large convnets large kernels. allows easy extensions efﬁciently train convnet arbitrary topology allowing research. unlike znn’s task parallelization model current implementations employ simd parallelism perform computation whole layer time thus limiting network structure. mainly computation parallelized single thread computes value single voxel output image. libraries like cudnn provide optimized primitives fully connected convolutional layers reducing required convolutions layer matrix multiplication parallelized gpu. extending functionality requires user provide parallelized implementation layer type typically requires great knowledge programming might take long time. contrary that znn’s task parallelism allows easy extensions simply providing serial functions forward backward pass well gradient computation required. znn’s repository contains sample extensions providing functionality dropout multi-scale networks. shelhamer donahue karayev long girshick guadarrama darrell caffe convolutional architecture fast feature embedding proceedings international conference multimedia bergstra breuleux bastien lamblin pascanu desjardins turian warde-farley bengio theano math expression compiler proceedings python scientiﬁc computing conference vol. austin dean corrado monga chen devin senior tucker yang large scale distributed deep networks advances neural information processing systems masci giusti ciresan fricout schmidhuber fast learning algorithm image segmentation max-pooling convolutional networks image processing ieee international conference ieee sermanet eigen zhang mathieu fergus lecun overfeat integrated recognition localization detection using convolutional networks arxiv preprint arxiv. wang yuan huang training large scale deep neural networks intel xeon many-core coprocessor proceedings ieee international parallel distributed processing symposium workshops ipdpsw ieee computer society ciresan giusti gambardella schmidhuber deep neural networks segment neuronal membranes electron microscopy images advances neural information processing systems kanazawa sharma jacobs locally scale-invariant convolutional neural networks arxiv preprint arxiv. sermanet lecun trafﬁc sign recognition multi-scale convolutional networks neural networks international joint conference ieee michael scott simple fast practical nonblocking blocking concurrent queue algorithms proceedings ﬁfteenth annual symposium principles distributed computing boost s¨umb¨ul zlateski vishwanathan masland seung automated computation arbor densities step toward identifying neuronal cell types frontiers neuroanatomy vol. srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overﬁtting journal machine learning research vol.", "year": 2015}