{"title": "Advances in exact Bayesian structure discovery in Bayesian networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We consider a Bayesian method for learning the Bayesian network structure from complete data. Recently, Koivisto and Sood (2004) presented an algorithm that for any single edge computes its marginal posterior probability in O(n 2^n) time, where n is the number of attributes; the number of parents per attribute is bounded by a constant. In this paper we show that the posterior probabilities for all the n (n - 1) potential edges can be computed in O(n 2^n) total time. This result is achieved by a forward-backward technique and fast Moebius transform algorithms, which are of independent interest. The resulting speedup by a factor of about n^2 allows us to experimentally study the statistical power of learning moderate-size networks. We report results from a simulation study that covers data sets with 20 to 10,000 records over 5 to 25 discrete attributes", "text": "consider bayesian method learning bayesian network structure complete data. recently koivisto sood presented algorithm single edge computes marginal posterior probability time number attributes; number parents attribute bounded constant. paper show posterior probabilities potential edges computed total time. result achieved forward{backward technique fast mobius transform algorithms independent interest. resulting speedup factor allows experimentally study statistical power learning moderate-size networks. report results simulation study covers data sets records discrete attributes. learning structure bayesian network model central goal many applications bayesian networks. causal discovery example aims identication causal relations among attributes qualitatively represented edges bayesian network unfortunately structure learning generally hard nite number observations reliably identify true structure evaluating possible network structures|whose number generally grows superexponentially number attributes|is computationally intractable. practice often passes relatively small data heuristic evaluation algorithm. dicult quantify uncertainty learning results although uncertainty limited data vague background knowledge rigorously expressed using standard statistical concepts extra uncertainty induced algorithm performance guarantees complicated gauge control. thus desirable extent scope exact algorithms completely avoid extra layer uncertainty. paper puts forward bayesian approach structure learning improve recently found exact algorithm koivisto sood computes marginal posterior probability given subnetwork e.g. single edge time; assume indegree i.e. number parents attribute bounded constant. sections show probabilities potential edges simultaneously computed total time. resulting computational saving factor signicant practice note algorithms practical networks attributes. achieve improvement using technique analogous forward{backward method hidden markov models another ingredient fast mobius transform algorithm analyze carefully section elaborating previous results improved exact algorithm allows study statistical power structure discovery moderatesize networks. given model specication number attributes maximum number parents attribute many observations collect reliably discover network structure? given data size expect large proportion structure learned data? obvious number obcomplete bayesian learning framework introduce prior network structure. follow friedman koller augment model random variable species linear order attributes. formally linear order index represented vector denes predecessors order i.e. also write completely subset denote linear orders structure consistent order denote function subsets nonnegative reals. noted prior respect markov equivalence; discussion friedman koller koivisto sood second part standard assumption given structure parameters decompose independent local components depending local structure furthermore pi). koivisto sood show ordermodular model complete data possible compute joint posterior probability subnetwork time allowing exact bayesian learning networks attributes. ciently evaluate local marginal likelihood; closedform expressions exist e.g. multinomial linear gaussian model conjugate priors.) next review ingredients algorithm koivisto sood notation. normalization constant needed factor absorbed functions however makes sense carry computations normalization constant constant irrelevant posterior summaries servations plays crucial role here less clear statistical power behaves function maximum indegree number states attribute. section study statistical power edge discovery varying settings study complementary previous work husmeier uses markov chain monte carlo method focuses small data sets large sparse dynamic bayesian networks. bayesian network vector variables species probability distribution network structure encodes conditional independence assertions among variables directed acyclic graph. represent graph vector subset index species parents graph. indexing subsets example denotes vector along structure factorizes probability distribution product local conditional distributions. usually conditional distributions belong parametric family convenient write probability distribution contains parameters local conditional distributions. notation used henceforth supports bayesian approach treats network structure parameters random variables used model multiple vectors called data denoted bayesian learning framework vectors judged exchangeable probability distribution data given structure expressed thanks improved algorithm able analyze several hundred random data sets attributes. computational resources would restrict experiments networks attributes used original algorithm koivisto sood however computations dierent edges involve large proportion overlapping elements. example switching edge another edge corresponds changing function hence aecting function thereby function only. intuitively possible exploit overlap reduce total time requirement. next describe calculations arranged total running time reduces compute orders combine left right contributions. node break orders nested sums outer inner conditionally remaining sets short). observe inner factorizes product left right term uv). point enlighting note forward{backward expression already oers n-fold speedup naive algorithm. this suppose precomputed left right contributions respect trivial feature then evaluate posterior probability edge need recompute function subsets fvg. step takes time; koivisto sood next section. step takes time well. thus total running time edges finally note posterior probability feature obtained denominator computed like trivial feature consequence \\priors\" need specied constant factors. absorb constants.) focus following problem given complete data order-modular model compute every edge posterior probability network structure contains note problem solved total time running algorithm koivisto sood separately edge. arrived following algorithm computing marginal posterior probabilities possible edges. functions dened respect trivial feature denote maximum indegree characterize running time algorithm under assumption maximum indegree constant. step takes time. step takes time already mentioned; next section. steps computed time using given recursions. node step computed time show next section. step takes time. thus total time requirement postponed section discussion certain summation formulas namely dene functions summations viewed mobius transformations subset lattice. elaborate recent results called truncated mobius transforms clarity presentation introduce generic notation|the connections notation terms used previous section implicit. variant obius transform) dened replacing summation conditions straightforward compute mobius transform takes time fast mobius transform algorithm takes time; e.g. kennes smets consider scenario need evaluate function sets contain elements given function number call corresponding transform k-truncated downward mobius transform. remainder section show time suces evaluating transformation. result dual similar results koivisto sood upward mobius transform computed time vanishes sets contain constant number elements. fast truncated downward mobius transform algorithm works follows. encode every subset bijectively vector otherwise; denote consider following algorithm. first then iteratively transform function another function follows si+; satisfying compute section summarizes experiment concerning statistical power discovering edges bayesian networks. briey various random simulated data sets dierent sizes computed posterior probability possible edge nally summarized discrepancy true network structure inferred edges. consider scenario tries learn existence nonexistence many edges possible. simplicity restrict consideration undirected edges care orientation incorrect. decision made possible edge outcome summarized number true positive false positive true negative false negative edges. loss function numbers could derive optimal bayesian decision. however rather xing function follow husmeier curve summarize learning performance dierent possible loss functions. threshold parameter taking values undirected edge claimed present network posterior probability pjx) pjx) exceeds value plot sensitivity complementary specicity note constructed edge course intended represent valid structure. data-generating model particularly informative direction edges. entire network estimates could checked markov equivalence true network checks individual directed edges somewhat problematic. note inferred undirected edges compatible true network expect correct markov equivalence class identied data. observe analysis also applies fast truncated mobius transform algorithm koivisto sood which|using terminology introduced section|computes k-truncated upward mobius transform. thus following result also proves conjecture posed koivisto sood regarding precise role time complexity. data drawn model pn;k;r also analyzed pn;k;r. choice motivated simplicity also bayesian interpretation statistical power expectation prior. example bayesian power learning edge probability posterior guess presence edge correct. probabilities w.r.t. model pn;k;r estimated monte carlo averages sample realizations network structures data sets drawn model pn;k;r. however since curves summarize learning ability explicitly estimate power learning individual edge. said examine distribution curves pn;k;r rather computing averages. figure shows networks attributes maximum indegree number states power edge discovery grows relatively smoothly number data points expected. interestingly also notice increasing maximum indegree mild eect whereas increasing number states seems somesurprising eect figure power edge discovery networks attributes. dierent maximum indegree number states sensitivity plotted complementary specicity ranges axes plot shows curve nested subsets single random data set. straight diagonal line expected curve random predictor. data points edges reliably discovered -state attributes binary attributes. explained smaller information content binary attributes. results number attributes data samples qualitatively similar. suggested results shown figure seems clear increase decrease power number attributes grows. noted however figure shows results data samples conguration. inspecting results samples change view. figure power edge discovery networks maximum indegree dierent number nodes number states sensitivity plotted complementary specicity ranges axes plot shows curve nested subsets single random data set. straight diagonal line expected curve random predictor. power edge discovery complete picture also explore variation within curves conguration number data points observe variance decreases number data points increases. also notice that already suggested increasing number states decreases average power fewer data figure variance power edge discovery networks attributes maximum indegree dierent number data points number states sensitivity plotted complementary specicity ranges axes plot shows curve random data set. straight diagonal line expected curve random predictor. presented exact algorithm computing marginal posterior probability every edge bayesian network. algorithm runs total time number attributes oering -fold speedup recent algorithm koivisto sood figure variance power edge discovery networks attributes maximum indegree dierent number data points number states sensitivity plotted complementary specicity ranges axes plot shows curve random data set. straight diagonal line expected curve random predictor. also potential applications. prior sensitivity analysis concerns robustness learning results perturbations model inexact methods problematic usually cannot tell apart variances model algorithm. another application validation heuristic methods comparing results produced heuristic algorithm exact results. tests attributes reveal shortcomings heuristic vice versa observing heuristic algorithm performs well networks size suggests algorithmic point view important open problem perhaps issue space complexity. possible reduce space requirement presented algorithm without sacricing much running time?", "year": 2012}