{"title": "Advantages and Limitations of using Successor Features for Transfer in  Reinforcement Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "One question central to Reinforcement Learning is how to learn a feature representation that supports algorithm scaling and re-use of learned information from different tasks. Successor Features approach this problem by learning a feature representation that satisfies a temporal constraint. We present an implementation of an approach that decouples the feature representation from the reward function, making it suitable for transferring knowledge between domains. We then assess the advantages and limitations of using Successor Features for transfer.", "text": "consider markov decision process ﬁnite state space ﬁnite action space transition function speciﬁes probability transitioning state state selecting action every transition reward speciﬁed reward function further assume discount factor weights tradeoffs immediate long term rewards. several algorithms developed estimate qfunction however important question represent current q-function estimate. example suppose state space consists states actions estimate q-function stored vector dimension dayan presented successor features particular type basis function represents state feature vector ψψψπ given policy feature representation ψψψπ similar feature representation successor states. idea originates bellman question central reinforcement learning learn feature representation supports algorithm scaling re-use learned information different tasks. successor features approach problem learning feature representation satisﬁes temporal constraint. present implementation approach decouples feature representation reward function making suitable transferring knowledge domains. assess advantages limitations using successor features transfer. reinforcement learning studies problem computing optimal control strategy using one-step interactions sampled environment. selected action environment also provides reward single scalar number. goal compute control strategy also called policy maximizes cumulative reward received interacting environment. challenge setting transferring knowledge environment another reward speciﬁcation changes remaining speciﬁcation environment stays ﬁxed. paper consider approach presented barreto uses successor features compute representation environment transferred across different reward functions. present implementation method show learning representation signiﬁcant beneﬁts transfer also fundamental limitations. note that depending choice basis function hold exactly estimate linear approximation true q-function. objective ﬁnding good representation basis function holds exactly possible. barreto re-visited approach context transferring feature representation within mdps reward function varies. various different approaches presented problem survey) barreto approach transfer problem learning feature representation descriptive entire mdps used transfer across different reward functions. intuitively q-function combines information reward function itself well temporal ordering received rewards. temporal ordering induced current policy transition dynamics determine trajectories generated. φφφt reward feature time step trajectory started suppose basis function tabulates state-action space i.e. φφφsa one-hot bitvector dimension ×a|. case weight vector thought full reward model written vector. means interpreted separation q-function factor describing rewards factor describing ordering rewards observed. hence barreto propose learn successor feature ψψψsa satisfying fitted q-iteration similar method outlined zhang derive learning algorithm reward model model simultaneously minimizing loss functions. reward model ﬁtted minimizing reward loss every collected transition computing target estimate previous update iteration used. unlike mnih al.’s deep q-learning target ysas vector single scalar variable. learning representation loss objective algorithm outlines implemented learning method. learning stabilized sampling batch transitions using entire batch make gradient descent update. experiments grid world algorithm ﬁrst evaluated grid world navigation task four actions down left right. transitions stochastic probability agent moves sideways. rewards entering goal cell right corner otherwise zero reward given. every episode started bottom right corner discount factor actions selected using ε-greedy policy respect current q-value estimates probability actions selected uniformly random probability action highest q-value estimate used. compare fitted implementation fitted q-iteration implementation. ensure fair comparison fitted q-iteration identical fitted except fitted q-iteration minimizes loss objective experiments q-function fitted q-iteration uses basis function tabulating state action space weight vector learned described further basis function used estimating reward model also tabulates state-action space; reward model always exactly represent true reward function. representation learned linear transform tabular basis function basis functions chosen tabular linear tabular one-hot basis function algorithms constrained representation always capture true value function reward model successor features. figure episode length best fitted q-iteration fitted run. experiments repeated times average episode length plus standard deviation plotted. shorter episode sooner agent reach reward state—a shorter episode better. figure compares performance fitted algorithm fitted q-iteration. algorithms converge good solution perform navigation task steps training. fitted algorithm converges slower explained fact learn full reward model form good q-value estimates. figure shows fitted algorithm robustly minimizes loss objectives. figure evolution loss objectives. fitted minimizes using adagrad gradient descent optimizer implemented tensorﬂow learning rate performed best loss objective learning rate performed best loss objective ﬁtted q-iteration implementation performed best learning rate otherwise tensorﬂow’s default parameters used. fitted algorithm also tested transfer settings start goal locations changed periodically ﬁxed different locations. changing goal location equivalent changing reward function holding transition dynamics ﬁxed. comparison fitted learning fitted q-iteration. fitted q-iteration used learning rate fitted learning used learning rate learning rate reward model. comparison different weight resetting strategies algorithm. green curve figure blue curve shows episode length weights reinitialized training rounds green curve keeps matrix between reward function changes. blue curve used learning rate reward model. figure performance results repeatedly moving start goal position cell every episodes. total three different start goal positions used repeated. episode length capped steps. mentation fitted implementation start goal locations moved grid cell. reward function changed weight parameter fitted algorithm re-initialized zero. fitted qiteration trained weights kept every reward function change. initial training slower fitted algorithm change reward function degrades performance signiﬁcantly less comparison fitted qiteration demonstrating robustness fitted algorithm. figure compares different resetting strategies fitted learning algorithm weights re-initialized reward function change learned kept training rounds. keeping weight matrix boosts performance signiﬁcantly. veriﬁes assumption presented barreto al.. transfer signiﬁcant reward changes test used transfer different domains algorithms evaluated grid world goal location rotated four corners grid. start location always corner diagonally across grid goal. changing start goal locations causes reward function optimal policy change signiﬁcantly. stabilize learning ensure sufﬁcient exploration algorithms select actions using ε-greedy policy. probability decayed according rule episode index. episode index reset zero every reward function change. ensuring sufﬁcient exploration allows fitted algorithm efﬁciently re-estimate reward model. several repeats four goal locations. ordering different goal locations changed experiment. change reward function impact algorithms fitted algorithm outperforms fitted q-iteration signiﬁcantly. table compares average episode length across episodes shows fitted algorithm outperforms fitted q-iteration signiﬁcantly. figure shows loss functions fitted algorithm evolves experiment. updates done every steps expected reward loss seem decrease signiﬁcantly steady oscillates instead. however estimates seem good enough achieve signiﬁcant performance difference fitted q-iteration. interestingly loss oscillates training high values. figure shows failure setting fitted algorithm annealed ﬁrst optimal policy ﬁrst reward function learned preserved across subsequent changes. result learning curve ﬁrst episodes fitted hits episode time-out steps next reward conﬁguration. reward function similar ﬁrst presented agent again fitted solves problem easily reuses weights learned beginning experiment. words fitted able transfer solution learned ﬁrst episodes tested reward functions. figure comparison fitted q-iteration fitted algorithm rotating every episodes goal location four corners grid. fitted qiteration uses learning rate fitted learning uses learning rate learning rate reward model. episodes capped steps. better understanding loss objective oscillates consider transfer example shown figure example mdps actions deterministic transitions indicated arrows. rewards indicated arrow labels mdps differ reward speciﬁc transitions. difference reward causes optimal policy different policy selects action optimal ﬁrst mdp; policy selects action state action elsewhere optimal second mdp. left side figure shows successor feature optimal policies different mdps. difference caused constrained similar features agent sees future. however features seen governed policy. highlights limitation using successor features transfer learned representation transferrable optimal policies. solving previously unseen learned representation fact representation re-learned individual seen experiments. figure contribute oscillations loss objective. failure case shown figure representation transfer instead represents initialization gradient optimizer cannot adjust reward function. behaviour surprising experiment goal location changed different corner grid causing optimal policy change signiﬁcantly. positive test case shown figure mitigated resetting policy ﬁrst uniformly random exploration thought smoothing transitions different reward functions. result also agrees ﬁrst transfer experiment shown figure reward function optimal policy changed slightly representations corresponding optimal policy reward function likely similar. result algorithm adjust reward function quickly. barreto also presented empirical results using variation generalized value iteration version puddle world location puddle changed slightly. experiment shows signiﬁcant performance boost transferring representation similar slight reward change test case changes reward function cause drastic change optimal policy. presented empirical results demonstrate interesting advantage dis-advantage transferring mdps differ reward function. able show signiﬁcant performance boost using approach also highlighted learned feature representation dependent policy learned for. hence representations unsuitable choice context typically interested transferring knowledge tasks different optimal policies. fact transferring representation tasks gives signiﬁcant boost learning speed also suggests learning transferrable feature representation might interesting direction pursue. however feature representation needs independent task’s optimal policy. references abadi mart´ın agarwal ashish barham paul brevdo eugene chen zhifeng citro craig corrado greg davis andy dean jeffrey devin matthieu ghemawat sanjay goodfellow harp andrew irving geoffrey isard michael yangqing jozefowicz rafal kaiser lukasz kudlur manjunath levenberg josh man´e monga rajat moore sherry murray derek olah chris schuster mike shlens jonathon steiner benoit sutskever ilya talwar kunal tucker paul vanhoucke vincent vasudevan vijay vi´egas fernanda vinyals oriol warden pete wattenberg martin wicke martin yuan zheng xiaoqiang. tensorflow large-scale machine learning heterogeneous systems http//tensorflow. org/. software available tensorﬂow.org. antos andr´as szepesv´ari csaba munos r´emi. learning near-optimal policies bellman-residual minimization based ﬁtted policy iteration single sample path. international conference computational learning theory springer konidaris george osentoski sarah thomas philip. value function approximation reinforcement learning using fourier basis. proceedings twenty-fifth aaai conference artiﬁcial intelligence pages august mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg human-level control deep reinforcement learning. nature sutton richard generalization reinforcement learning successful examples using sparse coarse coding. advances neural information processing systems zhang jingwei springenberg jost tobias boedecker joschka burgard wolfram. deep reinforcement learning successor features navigation across similar environments. arxiv preprint arxiv.", "year": 2017}