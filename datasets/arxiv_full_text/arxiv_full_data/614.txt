{"title": "Deriving Neural Architectures from Sequence and Graph Kernels", "tag": ["cs.NE", "cs.CL", "cs.LG"], "abstract": "The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications.", "text": "graph identiﬁes unit/mapping applied arcs specify relative arrangement/order operations. process designing computational graphs associated operations classes objects often guided insights expertise rather formal process. recent work substantially narrowed desirable computational operations associated objects representations acquired. example value iteration calculations folded convolutional architectures optimize representations facilitate planning similarly inference calculations graphical models latent states variables atom characteristics directly associated embedding operations appeal kernels combinatorial structures deﬁne appropriate computational operations. kernels give rise well-deﬁned function spaces possess rules composition guide built simpler ones. comparison objects inherent kernels often broken elementary relations counting common sub-structures possible substructures. example string kernel refer possible subsequences graph kernel would deal possible paths graph. several studies highlighted relation feed-forward neural architectures kernels unaware prior work pertaining kernels associated neural architectures structured objects. paper introduce class deep recurrent neural embedding operations formally characterize associated kernel spaces. resulting kernels parameterized sense neural operations relate objects interest virtual reference objects kernels. reference objects parameterized readily optimized end-to-end performance. design neural architectures structured objects typically guided experimental insights rather formal process. work appeal kernels combinatorial structures sequences graphs derive appropriate neural operations. introduce class deep recurrent neural operations formally characterize associated kernel spaces. recurrent modules compare input virtual reference objects kernels. similar traditional neural operations reference objects parameterized directly optimized end-to-end training. empirically evaluate proposed class neural architectures standard applications language modeling molecular graph regression achieving state-of-the-art results across applications. many recent studies focus designing novel neural architectures structured data sequences annotated graphs. instance lstm complex recurrent units easily adapted embed structured objects sentences molecules vector spaces suitable later processing standard predictive methods. embedding algorithms typically integrated end-to-end trainable architecture tailor learnable embeddings directly task hand. figure unrolled view derived recurrent module horizontal lines denote decayed propagation vertical lines represent linear mapping propagated internal states rd×d outerproduct. words underlying mapping kernel ≤i<j≤|x| λ|x|−i−xi note could alternatively partial additive scoring kernel function generalized n-grams again commit realization section. string kernel introduce class recurrent modules whose internal feature states embed computation string kernels. modules project kernel mapping multi-dimensional vector space owing combinatorial structure projection realized factorized efﬁcient computation. example kernel discussed above corresponding neural component realized network operates like rnns processing input token updating internal states. elementwise multiplication replaced addition special case additive variant becomes word-level convolutional neural following sections introduce neural components derived string graph kernels well deep versions. space limitations defer proofs supplementary material. string kernels sequence notations deﬁne sequence tokens repre {xi}l sents element denotes length. whenever clear context omit subscript directly denote sequence. pair ukvk inner product. kernel function subscript denote underlying mapping i.e. φiφi. string kernel string kernel measures similarity between sequences counting shared subsequences example strings bi-gram string kernel counts number bi-grams λxij λykl context-dependent weights indicator returns weight factors realized various ways. instance temporal predictions language modeling substrings appear later higher impact prediction. thus realization λxij λ|x|−i− λykl λ|y|−k− used determine weights given constant decay factor case token sequence vector shall replace exact match inner product kernel function rewritten proposition corollary lemma theorem since ˜wij mapping composed kernel. proof applies linear combination since kernel functions closed addition. address case multiple layers module stacked construct deeper networks. output states l-th layer layer input sequence. show layer stacking corresponds recursive kernel construction kernel deﬁned l-th kernel) proven feed-forward networks ﬁrst generalize sequence kernel deﬁnition enable recursive construction. notice deﬁnition uses linear kernel subroutine measure similarity substructures within sequences. therefore replace similarity measures introduced base particular string kernel kernels. generalized sequence kernel recursively deﬁned denotes pre-activation mapping denotes underlying kernel mapping non-linear activation l-th post-activation kernel. based deﬁnition deeper model also interpreted kernel computation. theorem consider deep string kernel layers activation function ﬁnal output state state proposed class embeds string kernel computation. i-th entry state vector represents i-th matrix deﬁne reference sequence constructed taking i-th matrix theorem preﬁx consisting ﬁrst tokens string kernel j-gram shown eq.. evaluates kernel function words network embeds sequence similarity computation assessing similarity input sequence reference sequence wij. interpretation similar cnns ﬁlter reference pattern search input. string kernel takes non-consecutive n-gram patterns consideration applying non-linear activation practice non-linear activation function polynomial sigmoid-like activation added internal states produce ﬁnal output state turns many activations also functions reproducing kernel hilbert space certain kernel functions zhang true underlying kernel composition string kernel kernel containing activation. give formal statements below. lemma multi-dimensional vectors ﬁnite norm. consider function non-linear activation functions polynomials sigmoid function exists kernel functions underlying mapping reproducing kernel hilbert space i.e. mapping constructed particular inverse-polynomial kernel activations. proposition layer string kernel nonlinear activation discussed lemma function input belongs rkhs introduced composition string kernel kernel composition deﬁned underlying mapping hence φσ)φσ). derivation neural module could extended classes graph kernels subtree kernels generally speaking kernel functions factorize graphs local sub-structures i.e. kloc) could either multiplication addition. locσ denotes denotes non-linear activation post-activation kernel involved. generalized kernel could realized modifying into function recursively deﬁned local kernel dimensions depth width pre-activation kernel l-th layer loc) post-activation kernel locσ). recursively deﬁne types kernels data. speciﬁcally derive neural components graphs section. notations graph deﬁned vertex associated feature vector neighbor node denoted following previous notations kernel function underlying mapping denote postactivation kernel induced composed underlying mapping φσ). show proposed model embeds random walk kernel. show this construct reference walk consisting vectors parameter matrices. v··· vi’s feature vector theorem state value satisﬁes note deﬁnition exactly equation additive composition. weisfeiler-lehman kernel graph kernel given relabeling function weisfeiler-lehman kernel base kernel depth deﬁned i-th relabeled graph respectively. weisfeiler-lehman kernel kernel deﬁnition random walk kernel base kernel propose following recurrent module recurrent module still instance deep kernel even though parameters shared. minor difference additional random walk kernel connects i-th layer output layer. linear combination deep random walk kernels therefore corollary theorem have proposition weisfeiler-lehman kernel iterations random walk kernel base kernel belongs rkhs derived deep kernel purpose generality. model could simpliﬁed setting without losing representational power case rewrite network reparametrization section show model could enhanced sharing weight matrices across layers. parameter tying mechanism allows model embed weisfeiler-lehman kernel clarity brieﬂy review basic concepts weisfeilerlehman kernel below. weisfeiler-lehman graph relabeling weisfeilerlehman kernel borrows concepts weisfeilerlehman isomorphism test labeled graphs. idea algorithm augment node labels sorted node labels neighbor nodes compress augmented labels short labels relabeling process repeated times. i-th iteration generates labeling nodes graph initial labeling generalized graph relabeling observation graph relabeling operation could viewed neighbor feature aggregation. result relabeling process naturally generalizes case nodes associated continuous feature vectors. particular relabeling function. node recursive neural networks alternative architectures model hierarchical structures syntax trees logic forms. instance socher employs recursive networks sentence classiﬁcation node dependency tree sentence transformed vector representation. proposed tree-lstm incorporates lstm-style architectures transformation unit. dyer recently introduced recursive neural model transitionbased language modeling parsing. speciﬁcally discussed paper ideas extend similar neural components hierarchical objects graph networks current graph neural architectures perform either convolutional recurrent operations graphs. duvenaud developed neural fingerprint chemical compounds convolution operation neighbor node features followed linear transformation. model differs generalized kernels networks aggregate neighboring features non-linear way. approaches e.g. bruna henaff rely graph laplacian fourier transform. recurrent architectures proposed gated graph neural networks neighbor features aggregated function. considers different architecture graph viewed latent variable graphical model. recurrent model derived belief propagation-like algorithms. approach closely related terms neighbor feature aggregation resulting recurrent architecture. nonetheless focus paper providing framework recurrent networks could derived deep graph kernels. kernels neural nets work follows recent work demonstrating connection neural networks kernels example zhang showed standard feedforward neural nets belong larger space recursively constructed kernels similar results made convolutional neural nets general computational graphs extend prior work kernels neural architectures structured inputs particular sequences graphs. another difference train model. prior work appeals convex optimization improper learning proposed networks building blocks typical non-convex ﬂexible neural network training. sequence graph kernel discussed constant decay value regardless current input. however often case since importance input vary across context applications. extension make neural gates adaptively control decay factor. give illustrative examples gated string kernel replacing constant decay sigmoid gate modify single-layer sequence module compared original string kernel decay factor λxij longer λ|x|−i− rather adaptive value based current context. gated random walk kernel similarly could introduce gates different walks different weights related work sequence networks considerable effort gone designing effective networks sequence processing. includes recurrent modules ability carry persistent memories lstm well non-consecutive convolutional modules others. recently zoph exempliﬁed reinforcement learning-based search algorithm optimize design recurrent architectures. proposed neural networks offer similar state left-over question whether proposed class operations despite formal characteristics leads effective architecture exploration hence improved performance. section apply proposed sequence graph modules various tasks empirically evaluate performance neural network models. tasks include language modeling sentiment classiﬁcation molecule regression. dataset setup penn tree bank corpus benchmark. dataset contains million tokens total. standard train/development/test split dataset vocabulary size model conﬁguration following standard practice initial learning rate decrease learning rate constant factor certain epoch. back-propagate gradient unroll size dropout regularization. unless otherwise speciﬁed train -layer networks normalized adaptive decay. following highway connections within layer gated decay factor transformation gate highway connections. results table compares model various state-ofthe-art models. small model million parameters achieves test perplexity already outperforming many results achieved using much larger network. increasing network size million obtain test perplexity standard dropout. adding variational dropout within recurrent cells improves perplexity finally model achieves perplexity recurrence depth increased state-of-the-art results reported note zilly uses neural layers zoph adopts complex recurrent cell found reinforcement learning based search. network architecturally much simpler. table comparison state-of-the-art results ptb. denotes number parameters. following recent work share input output word embedding matrix. report test perplexity model. lower number better. contiguous n-gram patterns. clearly variant performs worse compared recurrent variants moreover test perplexity improves train constant decay vector part model parameters. finally last variants utilize neural gates improving performance. dataset setup evaluate model sentence classiﬁcation task. stanford sentiment treebank benchmark dataset consists parsed english sentences annotated root level phrase level using class ﬁne-grained labels. standard split training development testing. following previous work also evaluate model binary classiﬁcation variant benchmark ignoring neutral sentences. table classiﬁcation accuracy stanford sentiment treebank. block recursive networks; block convolutional recurrent networks; block baseline methods. higher number better. following recent work rlstm publicly available -dimensional glove word vectors unlike prior work tunes word vectors normalize vectors ﬁxed simplicity. model conﬁguration best model -layer network hidden dimension average hidden states across concatenate averaged vectors layers input ﬁnal softmax layer. model optimized adam dropout probability results table presents performance model networks. report best results achieved across independent runs. best model obtains test accuracies ﬁne-grained binary tasks respectively. model constant decay factor also obtains quite high accuracy outperforming baseline methods shown table. dataset setup evaluate graph models harvard clean energy project benchmark used duvenaud evaluation dataset. dataset contains million candidate molecules molecule labeled power conversion efﬁciency value. table experiments harvard clean energy project. report root mean square error test set. ﬁrst block lists results reported reference. fair comparison reimplemented best model models trained setup. results setup reported second block. model mean predicator weisfeiler-lehman kernel degree= weisfeiler-lehman kernel degree= embedded mean field embedded loopy setup neural fingerprint embedded loopy weisfeiler kernel weisfeiler kernel gated feature duvenaud atoms bonds. initial atom features include atoms element degree number attached hydrogens implicit valence aromaticity indicator. bond feature concatenation bond type indicator whether bond conjugated whether bond ring. model conﬁguration model weisfeiler-lehman recurrent iterations models optimized adam learning rate decay factor results table report performance model baseline methods. neural fingerprint -layer convolutional neural network. convolution applied atom sums neighbors’ hidden state followed linear transformation non-linear activation. embedded loopy recurrent architecture recurrent iterations. maintains message vectors atom bond propagates vectors message passing fashion. table shows model achieves stateof-the-art various baselines. proposed class deep recurrent neural architectures formally characterized underlying computation using kernels. linking kernel neural operations template deriving families neural architectures sequences graphs. hope theoretical view kernel neural networks helpful future model exploration. thank prof. song sharing harvard clean energy project dataset. also thank zhang vikas garg david alvarez tianxiao shen karthik narasimhan reviewers helpful comments. work supported darpa make-it program contract wnf---. cheng jianpeng dong lapata mirella. long short-term memory networks machine reading. proceedings conference empirical methods natural language processing youngmin saul lawrence kernel methods deep learning. bengio schuurmans lafferty williams culotta advances neural information processing systems chung junyoung gulcehre caglar kyunghyun bengio yoshua. empirical evaluation gated recurrent neural networks sequence modeling. arxiv preprint arxiv. iparraguirre jorge bombarell rafael hirzel timothy aspuruguzik al´an adams ryan convolutional networks graphs learning molecular ﬁngerprints. advances neural information processing systems dyer chris ballesteros miguel ling wang matthews austin smith noah transition-based dependency parsing stack long short-term memory. proceedings annual meeting association computational linguistics beijing china july dyer chris kuncoro adhiguna ballesteros miguel smith noah recurrent neural network grammars. proceedings conference north american chapter association computational linguistics diego california june yarin ghahramani zoubin. theoretically grounded application dropout recurrent neural netadvances neural information processing works. systems heinemann livni eban elad elidan globerson amir. improper deep kernels. proceedings international conference artiﬁcial intelligence statistics hinton geoffrey srivastava nitish krizhevsky alex sutskever ilya salakhutdinov ruslan improving neural networks preventing co-adaptation feature detectors. arxiv preprint arxiv. iyyer mohit manjunatha varun boyd-graber jordan daum´e hal. deep unordered composition rivals syntactic methods text classiﬁcation. proceedings annual meeting association computational linguistics kalchbrenner grefenstette edward blunsom phil. convolutional neural network modelling sentences. proceedings annual meeting association computational linguistics kumar ankit irsoy ozan ondruska peter iyyer mohit james bradbury ishaan gulrajani zhong victor paulus romain socher richard. anything dynamic memory networks natural language processing. joshi hrishikesh barzilay regina jaakkola tommi tymoshenko katerina moschitti alessandro marquez lluis. semi-supervised question retrieval gated convolutions. arxiv preprint arxiv. shervashidze nino schweitzer pascal leeuwen erik mehlhorn kurt borgwardt karsten weisfeiler-lehman graph kernels. journal machine learning research socher richard pennington jeffrey huang eric andrew manning christopher semisupervised recursive autoencoders predicting sentiment distributions. proceedings conference empirical methods natural language processing socher richard perelygin alex jean chuang jason manning christopher andrew potts christopher. recursive deep models semantic compositionality sentiment treebank. proceedings conference empirical methods natural language processing october sheng socher richard manning christoimproved semantic representations treepher prostructured long short-term memory networks. ceedings annual meeting association computational linguistics lodhi huma saunders craig shawe-taylor john cristianini nello watkins chris. text classiﬁcation using string kernels. journal machine learning research zhang yuchen jason jordan michael regularized neural networks improperly learnable proceedings internapolynomial time. tional conference machine learning theoretical results apply variants sequence kernels associated neural components. give examples section. table shows three network variants corresponding three realizations string kernels provided table connection lstms interestingly many recent work reached similar architectures empirical exploration. example greff found simplifying lstms removing input gate coupling forget gate signiﬁcantly change performance. however forget gate crucial performance. consistent theoretical analysis empirical results figure moreover balduzzi ghifary suggest linear additive state computation sufﬁces provide competitive performance compared lstms remarks theorem demonstrates state vectors seen projecting kernel mapping low-dimensional space using parameterized matrices {w}i similar word embeddings cnns input projected obtain low-dimensional representation. underlying mapping kernel function whose reproducing kernel hilbert space contains non-linear activation used string kernel layer. pre-activation kernel post-activation kernel. show values string kernel states contained rkhs contained rkhs theorem given deep n-gram string kernel non-linear activation assuming lies rkhs kernel function underlying mapping inﬁnite dimension underlying mapping non-linear activation inﬁnite dimension. proof still apply since dimensions still countable vectors ﬁnite norm walks length starting note force valid walk starting considers walks length additive version illustration theorem deep random walk kernel layers activation function assuming", "year": 2017}