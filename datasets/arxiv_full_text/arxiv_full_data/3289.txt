{"title": "Deeply Learning the Messages in Message Passing Inference", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Deep structured output learning shows great promise in tasks like semantic image segmentation. We proffer a new, efficient deep structured model learning scheme, in which we show how deep Convolutional Neural Networks (CNNs) can be used to estimate the messages in message passing inference for structured prediction with Conditional Random Fields (CRFs). With such CNN message estimators, we obviate the need to learn or evaluate potential functions for message calculation. This confers significant efficiency for learning, since otherwise when performing structured learning for a CRF with CNN potentials it is necessary to undertake expensive inference for every stochastic gradient iteration. The network output dimension for message estimation is the same as the number of classes, in contrast to the network output for general CNN potential functions in CRFs, which is exponential in the order of the potentials. Hence CNN message learning has fewer network parameters and is more scalable for cases that a large number of classes are involved. We apply our method to semantic image segmentation on the PASCAL VOC 2012 dataset. We achieve an intersection-over-union score of 73.4 on its test set, which is the best reported result for methods using the VOC training images alone. This impressive performance demonstrates the effectiveness and usefulness of our CNN message learning method.", "text": "deep structured output learning shows great promise tasks like semantic image segmentation. proﬀer eﬃcient deep structured model learning scheme show deep convolutional neural networks used directly estimate messages message passing inference structured prediction conditional random fields message estimators obviate need learn evaluate potential functions message calculation. confers signiﬁcant eﬃciency learning since otherwise performing structured learning potentials necessary undertake expensive inference every stochastic gradient iteration. network output dimension message estimators number classes rather exponentially growing order potentials. hence scalable cases large number classes involved. apply method semantic image segmentation achieve impressive performance demonstrates eﬀectiveness usefulness message learning method. learning deep structured models attracted considerable research attention recently. popular approach deep structured model formulating conditional random ﬁelds using deep convolutional neural networks potential functions. combines power cnns feature representation learning ability crfs model complex relations. typical approach joint learning crfs cnns learn potential functions optimizing objective e.g. maximizing log-likelihood. joint learning shown impressive performance semantic image segmentation. joint learning cnns crfs stochastic gradient descent typically applied optimizing conditional likelihood. approach requires marginal inference calculating gradient. loopy graphs marginal inference generally expensive even using approximate solutions. given learning potential functions typically requires large number gradient iterations repeated marginal inference would make training intractably slow. applying approximate training objective solution avoid repeat inference; pseudo-likelihood learning piecewise learning examples kind approach. work advocate direction eﬃcient deep structured model learning. conventional approaches ﬁnal prediction result inference based learned potentials. however ultimate goal ﬁnal prediction propose directly optimize inference procedure ﬁnal prediction. focus extensively studied message passing based inference algorithms. discussed directly learn message estimators output required messages inference procedure rather learning potential functions conventional learning approaches. learned message estimators obtain ﬁnal prediction performing message passing inference. explore direction eﬃcient deep structured learning. propose directly learn messages message passing inference training deep cnns end-to-end learning fashion. message learning require inference step gradient calculation allows eﬃcient training. cast traditional classiﬁcation problems. network output dimension message estimation number classes network output general potential functions crfs exponential order potentials hence based message learning signiﬁcantly fewer network parameters thus scalable especially cases large number classes involved. number iterations message passing inference explicitly taken consideration message learning procedure. paper particularly interested learning messages able oﬀer high-quality prediction results message passing iteration making message passing inference fast. combining strengths cnns crfs segmentation explored several recent methods. methods resort simple combination classiﬁers crfs without joint learning. deeplab-crf ﬁrst train fully pixel classiﬁcation applies dense method post-processing step. later method extends deeplab jointly learning dense crfs cnns. rnn-crf also performs joint learning cnns dense crfs. implement mean-ﬁeld inference recurrent neural networks facilitates end-to-end learning. methods usually cnns modelling unary potentials only. work trains cnns model unary pairwise potentials order capture contextual information. jointly learning cnns crfs also explored applications like depth estimation work explores joint training markov random ﬁelds deep networks predicting words noisy images image classiﬁcation. above-mentioned methods combine cnns crfs based upon conventional approaches. jointly learn incorporate pre-trained potential functions perform inference/prediction using potentials. contrast method directly learns message estimators message passing inference rather learning potentials. inference machine proposed relevant work discussed idea directly learning message estimators instead learning potential functions structured prediction. train traditional logistic regressors hand-crafted features message estimators. motivated tremendous success cnns propose train deep cnns based message estimators end-to-end learning style without using handcrafted features. unlike approach aims learn variable-to-factor message estimators proposed method aims learn factor-to-variable message estimators. thus able naturally formulate variable marginals ultimate goal inference training objective approach jointly learns cnns crfs pose estimation learn marginal likelihood body parts ignore partition function likelihood. message learning discussed work exact relation pose estimation approach message learning remains unclear. describing message learning method review crf-cnn joint learning approach discuss limitations. input image denoted corresponding labeling mask denoted energy function denoted measures score prediction given input image consider following form conditional likelihood indexes factor factor graph; denotes variable nodes connected factor potential function potential function unary pairwise high-order potential function. recent method describes examples constructing general based unary pairwise potentials. take semantic image segmentation example. predict pixel labels test image mode joint label distribution solving maximum posteriori inference problem argmax also obtain ﬁnal prediction calculating label marginal distribution variable requires solve marginal inference problem y\\yp indicates output variables excluding general graph cycles inference problems known np-hard thus approximate inference algorithms applied. message passing type widely applied algorithms approximate inference loopy belief propagation tree-reweighted message passing mean-ﬁeld approximation examples message passing methods. denote i-th training image segmentation mask; number training images; weight decay parameter. apply stochastic gradient descent optimize problem learning energy function constructed cnns gradient easily computed applying chain rule conventional cnns. however partition function brings diﬃculties optimization. gradient direct calculation gradients computationally infeasible general graphs. usually necessary perform approximate marginal inference calculate gradients iteration however repeated marginal inference extremely expensive discussed training usually requires huge number iterations hence inference based learning approach general scalable even infeasible. conventional approaches potential functions ﬁrst learned inference performed based learned potential functions order generate ﬁnal prediction. contrast approach directly optimizes inference procedure ﬁnal prediction. propose learn estimators directly output required intermediate values inference algorithm. focus message passing based inference algorithm extensively studied widely applied. prediction procedure message vectors recursively calculated based learned potentials. propose construct learn cnns directly estimate messages message passing procedure rather learning potential functions. particular directly learn factor-to-variable message estimators. message learning framework general accommodate message passing based algorithms loopy belief propagation mean-ﬁeld approximation variants. discuss using loopy calculating variable marginals. shown yedidia loopy close relation bethe free energy approximation. typically message k-dimensional vector encodes information label distribution. variable-factor connection need recursively compute variable-to-factor message βp→f factor-to-variable message numerical reasons operation applied marginals deriving message passing algorithms. unnormalized variable-to-factor message computed variables connected factor variables excluding variable factor-to-variable messages variable node able calculate marginal distribution variable denotes variable node connected node factor factor graph. refer variable node neighboring node variables connected factor excluding node clearly pairwise factor connects variables contains variable node. equations show factor-to-variable message depends potential ′→q. factor-to-variable message calculated neighboring node factor conventional learning approaches learn potential function follow equations compute messages calculating marginals. discussed given goal estimate marginals necessary exactly follow equations involve learning potential functions calculate messages. directly learn message estimators rather indirectly learning potential functions conventional methods. consider calculation message depends observation messages ′→q. denotes observations correspond node factor able formulate factor-to-variable message estimator takes inputs outputs message vector directly learn estimators. since message depends number previous messages formulate sequence message estimators model dependence. thus output previous message estimator input following message estimator. message passing strategies loopy synchronous asynchronous passing. focus synchronous message passing messages computed passing neighbors. synchronous passing strategy results much simpler message dependences asynchronous strategy simpliﬁes training procedure. deﬁne inference iteration pass graph synchronous passing strategy. propose learn based factor-to-variable message estimator. message estimator models interaction neighboring variable nodes. denote message estimator. factor-to-variable message calculated refer dependent message feature vector encodes dependent messages neighboring nodes connected node note dependent messages output message estimators previous inference iteration. case running message passing iteration dependent messages thus need incorporate general exposition describe case running arbitrarily many inference iterations. choose eﬀective strategy generate feature vector dependent messages. discuss simple example. according deﬁne feature vector k-dimensional vector aggregates dependent messages. case computed deﬁnition clearly shows message estimation requires evaluating sequence message estimators. another example concatenate dependent messages construct feature vector diﬀerent strategies formulate message estimators diﬀerent iterations. simple strategy using message estimator across inference iteration. case message estimator becomes recursive function thus based estimator becomes recurrent neural network another strategy formulate diﬀerent estimator inference iteration. denotes network parameter need learn. indicator function equals input true otherwise. denote k-dimensional output vector message estimator network node factor zpfk k-th value network output corresponding k-th class. consider possible strategies implementing cnns. example describe strategy analogous network design denote fully convolutional network convolutional feature generation traditional fully connected network message estimation. given input image network output rn×n×r convolutional feature feature size dimension feature vector. spatial position feature corresponds variable node graph. denote feature vector corresponding variable node likewise averaged vector feature vectors correspond nodes recall nodes connected factor excluding node pairwise factors contains node. construct feature vector node-factor pair concatenating finally concatenate node-factor feature vector dependent message feature vector input second network thus input dimension running inference iteration input alone. ﬁnal output second network k-dimensional message vector generate ﬁnal message vector general based potential function conventional crfs potential network usually required large number output units example requires outputs pairwise potentials large number output units would signiﬁcantly increase number network parameters. leads expensive computations tend over-ﬁt training data. contrast learning message estimator need formulate output units network. clearly scalable cases large number classes. normalizer. ideal variable marginal probability ground truth class remaining classes. consider cross entropy loss ideal marginal estimated marginal. work propose learn variable-to-factor message unlike approach learn factor-to-variable message able naturally formulate variable marginals ultimate goal prediction training objective. moreover learning βp→f approach message estimator depend neighboring nodes given variable nodes diﬀerent number neighboring nodes consider ﬁxed number neighboring nodes concatenate features generate ﬁxed-length feature vector classiﬁcation. case learning message estimator depends ﬁxed number neighboring nodes thus problem. importantly learn message estimators training traditional probabilistic classiﬁers hand-craft features contrast train deep cnns end-to-end learning style without using hand-craft features. advantage message learning able explicitly incorporate expected number inference iteration learning procedure. number inference iteration deﬁnes learning sequence message estimators. particular useful learn estimators able high-quality prediction running number inference iterations. contrast conventional potential function learning crfs able directly incorporate expected number inference iterations. particularly interested learning message estimators using message passing iteration inference fast. case might preferable large-range neighborhood connections large range interaction captured running inference pass. evaluate proposed message learning method semantic image segmentation. publicly available pascal dataset object categories background category dataset. contains images training images images test set. following common setting training augmented images including extra annotations provided images. intersection-over-union score evaluate segmentation performance. learning prediction method message passing iteration. recent work learns multi-scale fully convolutional cnns unary pairwise potential functions capture contextual information. follow learning method replace potential functions proposed message estimators. consider types spatial relations constructing pairwise connections variable nodes. surrounding spatial relation node connected surround nodes. above/below spatial relation node connected nodes above. pairwise connections neighborhood size deﬁned range box. learn type unary message estimator types pairwise message estimators total. type pairwise message estimator surrounding spatial relations above/below spatial relations. formulate network type message estimator. formulate message estimators multi-scale fcnns apply similar network conﬁguration network convolution blocks fully connected layers networks initialized using vgg- model train layers using back-propagation. ﬁrst evaluate method set. compare several recent based methods available results set. results shown table method achieves best performance. mentioned contextdcrf learns based potential functions crfs capture contextual information. contextdcrf follows conventional learning prediction scheme ﬁrst learn potentials perform inference based learned potentials output ﬁnal predictions. result shows learning message estimators able achieve similar performance compared learning potential functions crfs. note message passing iteration training prediction inference time cost almost negligible. hence method enables much eﬃcient inference. improve performance perform simple data augmentation training. generate extra scales training images ﬂipped images training. result denoted ours+ result table. evaluate method test set. compare recent stateof-the-art methods competitive performance. results described table since ground truth labels available test evaluate method evaluation server. achieve impressive performance test also include comparison methods trained much larger coco dataset performance comparable methods method uses much less number training images. proposed deep message learning framework structured prediction. learning deep message estimators message passing inference reveals direction learning deep structured model. learning message estimators eﬃcient involve expensive inference steps gradient calculation. network output dimension message estimation number classes increase order potentials thus message learning less network parameters scalable number classes compared conventional potential function learning. impressive performance semantic segmentation demonstrates eﬀectiveness usefulness proposed deep message learning. framework general readily applied structured prediction applications.", "year": 2015}