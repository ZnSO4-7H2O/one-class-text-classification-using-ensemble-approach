{"title": "Understanding Adversarial Training: Increasing Local Stability of Neural  Nets through Robust Optimization", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We propose a general framework for increasing local stability of Artificial Neural Nets (ANNs) using Robust Optimization (RO). We achieve this through an alternating minimization-maximization procedure, in which the loss of the network is minimized over perturbed examples that are generated at each parameter update. We show that adversarial training of ANNs is in fact robustification of the network optimization, and that our proposed framework generalizes previous approaches for increasing local stability of ANNs. Experimental results reveal that our approach increases the robustness of the network to existing adversarial examples, while making it harder to generate new ones. Furthermore, our algorithm improves the accuracy of the network also on the original test data.", "text": "propose general framework increasing local stability artiﬁcial neural nets using robust optimization achieve alternating minimization-maximization procedure loss network minimized perturbed examples generated parameter update. show adversarial training anns fact robustiﬁcation network optimization proposed framework generalizes previous approaches increasing local stability anns. experimental results reveal approach increases robustness network existing adversarial examples making harder generate ones. furthermore algorithm improves accuracy network also original test data. fact anns might unstable locally demonstrated shown highly performing vision anns mis-classify examples barely perceivable differences correctly classiﬁed examples. examples called adversarial examples originally found solving optimization problem respect trained net. adversarial examples tend exist naturally training test data. local instability manifested existence somewhat disturbing. case visual data example would expect images close natural human metric mapped nearby points hidden representation spaces consequently predicted class. moreover shown different models different architectures trained different training sets tend mis-classify adversarial examples similar fashion. addition disturbing existence adversarial examples model stability perspective fact generated simple structured procedures common different models used perform attacks models making fail easily consistently claimed adversarial examples exist ’blind spots’ data domain training testing data occur naturally; however blind spots might close sense naturally occurring data. several works proposed adversarial examples training anns reported increase classiﬁcation accuracy test data goal manuscript provide framework yields full theoretical understanding adversarial training well optimization schemes based robust optimization. speciﬁcally show generating using adversarial examples training anns derived powerful notion robust optimization many applications machine learning closely related regularization. propose general algorithm robustiﬁcation training show generalizes previously proposed approaches. essentially algorithm increases stability anns respect perturbations input data iterative minimization-maximization procedure network parameters updated respect worst-case data rather original training data. furthermore show connections method existing methods generating adversarial examples adversarial training demonstrating methods special instances robust optimization framework. point yields principled connection highlighting fact existing adversarial training methods robustify parameter optimization process. structure paper follows section mention recent works analyze adversarial examples attempt improve local stability. section present basic ideas behind robust optimization connections regularization machine learning models. section present training framework possible variants practical version. experimental results given section section brieﬂy concludes manuscript. notation denote labeled training features label. loss network parameters denoted function quantiﬁes goodness-of-ﬁt parameters observations holding ﬁxed viewing function occasionally write jθy. corresponds small additive adversarial perturbation added adversarial example refer perturbed example i.e. along original label denote norm |x|p denote norm vector maxi{|x|}. given vectors euclidean xiyi. given function denote similar approach generating adversarial examples used also fundamental idea construct small perturbation data point order force method mis-classify training example incorrect label authors point dimension large changing entry small value yields perturbation signiﬁcantly change innerproduct product weight vector result chooses adversarial perturbation alternative formulation problem naturally shows adversarial perturbation obtained. take ﬁrst-order approximation loss function around true training example small perturbation gradient computed efﬁciently using backpropagation approach generating adversarial examples rather fast. sequel show computation example framework present here. reported adversarial examples generated speciﬁc network mis-classiﬁed similar fashion networks possibly different architectures using different subsets data training. authors claim phenomenon related strong linear nature neural networks have. speciﬁcally claim different models learned neural nets essentially close linear model hence giving similar predictions adversarial examples. works nicely demonstrate classiﬁers achieve high test accuracy without actually learning true concepts classes predict. rather base predictions discriminative information sufﬁces obtain accurate predictions test data however reﬂect learning true concept deﬁnes speciﬁc classes. result consistently fail recognizing true class concept examples conﬁdently give wrong predictions speciﬁcally designed examples several papers propose training procedures objective functions designed make function computed change slowly near training test points. adversarial examples generated back training set. procedure reported increase classiﬁcation accuracy test data. following loss function proposed equation authors report resulting improved test accuracy also better performance adversarial examples. give intuitive explanations training procedure adversary game min-max optimization balls. manuscript attempt make second interpretation rigor deriving similar training procedure framework. adversarial training performed without requiring knowledge true label. rather loss function contains term computing kullback-leibler divergence predicted distributions label i.e. kl||p). adversarial examples generated music data. authors report back-feeding adversarial examples training result improved resistance adversarial examples. authors ﬁrst pre-train layer network contractive autoencoder which assuming data concentrates near lower dimensional manifold penalizes jacobian encoder every training point encoding changes directions tangent manifold authors assume points belonging different classes tend concentrate near different sub-manifolds separated density areas. consequently encourage output classiﬁcation network constant near every training point penalizing product network’s gradient basis vectors plain tangent data manifold every training point. done using loss function contractive autoencoder loss also used authors propose increase robustness minimization loss function contains term penalizes jacobians function computed layer respect previous layer. authors propose regularize anns penalizing operator norm weight matrix every layer. thing lead pushing lipschitz constant function computed layer down small perturbations input result large perturbations output. aware empirical result using approach. stability rotations translations. learned representation claimed stable also small deformations created additive noise. however performance network often inferior standard convnets trained supervised fashion. interesting theoretical arguments presented shown robustness classiﬁer adversarial examples depends distinguishability classes; show sufﬁciently large distinguishability necessary condition classiﬁer robust adversarial perturbations. distinguishability expressed example distance means case linear classiﬁers covariance matrices case quadratic classiﬁers. solutions optimization problems sensitive small perturbations input data optimization problem sense optimal solution given current data turn highly sub-optimal even infeasible solution given slight change data. desirable property optimal solution remain nearly optimal small perturbations data. since measurement data typically precision-limited might contain errors requirement solution stable input perturbations becomes essential. robust optimization area optimization theory aims obtain solutions stable level uncertainty data. uncertainty deterministic worst-case nature. assumption perturbations data drawn speciﬁc sets called uncertainty sets. uncertainty sets often deﬁned terms type uncertainty parameter controlling size uncertainty set. cartesian product sets usually denoted goal robust optimization obtain solutions feasible well-behaved realization uncertainty among feasible solutions optimal would minimal cost given worst-case realization robust optimization problems thus usually min-max formulation objective function minimized respect worst-case realization perturbation. example consider standard linear programming problem given data case goal obtain solution robust perturbations data. clearly solution well-behaved perturbations data arbitrary. hence restrict allowing perturbations exist uncertainty corresponding robust optimization formulation robust counterpart optimization problem sometimes complicated solve original problem. propose algorithms approximately solving robust problem based algorithm original problem. approach closely related algorithm propose manuscript. next section discuss connection robust optimization regularization. regularization serves important role deep learning architectures methods dropout sparsiﬁcation serving examples. robust optimization applied various settings statistics machine learning including example several parameter estimation applications. particular strong connection robust optimization regularization; several cases shown solving regularized problem equivalent obtaining robust optimization solution non-regularized problem. example shown solution regularized least squares problem norm norms columns result shown sparsity solution xopt consequence robustness. regularized support vector machines also shown robustness properties shown solutions norm regularization obtained non-regularized robust optimization problems finally ridge regression also viewed variant robust optimization problem. namely shown inspired robust optimization paradigm propose loss function training anns. approach designed make network’s output stable small neighborhood around every training point neighborhood corresponds uncertainty example ball radius around respect norm select neighborhood representative point network’s output induce greatest loss; require network’s output target output assuming many test points indeed close training points class expect training algorithm regularization effect consequently improve network’s performance test data. furthermore since adversarial examples typically generated proximity training test points expect approach increase robustness network’s output adversarial examples. uncertainty corresponding example viewed optimizing network parameters respect worst-case data rather original training data; i’th worst-case data point chosen uncertainty uncertainty sets determined type uncertainty selected based problem hand. optimization done standard iterative fashion iteration algorithm optimization sub-procedures performed. first network parameters held ﬁxed every training example additive adversarial perturbation selected then network parameters updated respect perturbed data ∆xi. maximization related adversarial example generation process previously proposed szegedy shown equation clearly ﬁnding exact equation intractable general. furthermore performing full optimization process sub-procedures iteration practical. hence propose minimize surrogate sub-procedure reduced single ascent descent step; iteration perform single ascent step approximation ˆ∆xi followed single descent step update surrogate consider ﬁrst-order taylor expansion loss around example yields proposed training procedure formalized algorithm words algorithm performs alternating ascent descent steps ﬁrst ascend respect training example descend respect network parameters note procedure never updated respect original training data; rather always updated respect worst-case examples close original training points respect uncertainty sets sequel remark solve equation special cases general could algorithm like l-bfgs projected gradient descent finally note iteration algorithm forward backward passes network performed using original training data compute adversarial perturbations using perturbed data compute update hence expect training time twice long comparing standard training. examples uncertainty sets number cases consider uncertainty sets example norm ball centered radius respect norm interesting choices norms. thus approximated using normalized steepest ascent step respect norm steepest ascent step respect ball obtained sign gradient sign∇jθyi choosing ball therefore yield perturbation every entry changed amount steepest ascent respect ball coincides direction gradient ∇jθyi. choosing ball yield sparse perturbation small number entries changed observe three cases steepest ascent direction derived gradient ∇jθyi computed efﬁciently using backpropagation. section norms generate adversarial examples equation compare performance algorithm using types uncertainty sets. loss function equation proposed viewed variant approach chosen ball around since updated respect adversarial examples generated equation steepest ascent step respect norm. namely simply solution equation case update presented equation also relate proposed methodology manifold tangent classiﬁer loss function following assumption suppose data exists low-dimension smooth manifold solution equation πbx∇xjθy orthogonal projection matrix onto subspace norm equal thus acts regularization gradient loss respect training sample projected along tangent space analogous regularization presented equation another small perturbations tangent manifold cause small changes loss turn result small perturbations output network input section experiment proposed training algorithm popular benchmark datasets mnist cifar- case compare robustness network trained using algorithm network trained standard fashion. baseline trained convnet relu units convolutional layers pooling every convolutional layer fully connected layers top. convnet accuracy mnist test set. refer network baseline net. used baseline generate collection adversarial examples using equation norm balls. speciﬁcally adversarial perturbation computed step steepest ascent direction w.r.t corresponding norm. step w.r.t uncertainty fast method step w.r.t uncertainty direction gradient; steepest ascent direction w.r.t uncertainty sets comes changing pixel corresponding entry largest magnitude gradient vector. interesting note using equation uncertainty possible make network mis-classify image changing single pixel. several examples presented figure figure adversarial examples generated mnist dataset w.r.t baseline equation uncertainty. original test examples. bottom adversarial examples single pixel changed. original examples presented correctly classiﬁed baseline adversarial examples mis-classiﬁed. altogether generated collection adversarial examples baseline network zero accuracy generated correctly classiﬁed test points. sample adversarial examples presented figure refer collection amnist. used algorithm re-train norm refer resulting nets robustiﬁed nets. table summarizes accuracy robustiﬁed mnist test data collection amnist adversarial examples. seen three robustiﬁed nets classify correctly many adversarial examples amnist uncertainty giving best performance. addition three robustiﬁed nets improve accuracy also original test data i.e. adversarial training acts regularizer improves network’s generalization ability. observation consistent ones next checked whether harder generate adversarial examples robustiﬁed nets baseline net. that used fast method various values generate adversarial examples baseline robustiﬁed nets. measured classiﬁcation accuracy respect adversarial examples generated parameters. results shown figure clearly three robustiﬁed nets signiﬁcantly robust generation adversarial examples. figure mnist dataset experiment comparison baseline robustiﬁed nets uncertainty sets). adversarial examples generated equation respect various values classiﬁcation accuracy plotted. seen nets trained using algorithm signiﬁcantly robust adversarial examples. summarize mnist experiment observed networks trained algorithm improved performance original test data improved performance original adversarial examples generated w.r.t baseline robust generation adversarial examples. section constructed adversarial examples baseline using equation uncertainty sets. altogether constructed adversarial examples mis-classiﬁed baseline constructed correctly classiﬁed test images. denote acifar. sample acifar presented figure figure sample acifar adversarial examples. original cifar- test examples corresponding adversarial examples baseline net. original test examples classiﬁed correctly baseline adversarial examples mis-classiﬁed. adversarial examples shown generated baseline using equation uncertainties. table compares performance baseline robustiﬁed nets cifar- test data collection acifar adversarial examples. consistently results mnist experiment well robustiﬁed nets classify correctly many adversarial examples amnist also outperform baseline original test data. uncertainty) various values generate adversarial examples baseline robustiﬁed nets. results shown figure adversarial examples consistently harder generate robustiﬁed consistent observation mnist experiment. figure cifar- dataset experiment comparison baseline robustiﬁed nets uncertainty). adversarial examples generated equation respect various values classiﬁcation accuracy plotted. robustiﬁed nets robust generation adversarial examples. summarize cifar- experiment observed well robustiﬁed nets improve performance original test data making nets robust generation adversarial examples. mnist experiment uncertainty yields best improvement test accuracy. addition robustiﬁed nets require number parameter updates converge baseline net. attempt theoretically understand successful empirical results adversarial training proposed framework robust optimization neural nets network’s prediction encouraged consistent small ball respect norm. implementation done using minimization-maximization approach loss minimized worst-case examples rather original data. framework explains previously reported empirical results showing incorporating adversarial examples training improves accuracy test data. addition showed loss function published fact special case algorithm certain type uncertainty thus explaining intuitive interpretations given paper. also showed connection algorithm manifold tangent classiﬁer showing corresponds robustiﬁcation training. experimental results mnist cifar- datasets show algorithm indeed acts regularizer improves prediction accuracy also original test examples consistent previous results furthermore showed adversarial examples harder generate network trained using proposed approach comparing network trained standard fashion. by-product also showed able make neural mis-classify correctly-classiﬁed image changing single pixel. explaining regularization effect adversarial training vein practical experience authors knew drop-out acts regularization without formal rigor justiﬁcation. later work wager wang liang created rigorous connection dropout weighted ridge regression.", "year": 2015}