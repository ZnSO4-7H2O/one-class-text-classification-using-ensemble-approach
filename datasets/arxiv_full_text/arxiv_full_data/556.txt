{"title": "Convolutional Neural Network Architectures for Matching Natural Language  Sentences", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Semantic matching is of central importance to many natural language tasks \\cite{bordes2014semantic,RetrievalQA}. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layer-by-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models.", "text": "semantic matching central importance many natural language tasks successful matching algorithm needs adequately model internal structures language objects interaction them. step toward goal propose convolutional neural network models matching sentences adapting convolutional strategy vision speech. proposed models nicely represent hierarchical structures sentences layerby-layer composition pooling also capture rich matching patterns different levels. models rather generic requiring prior knowledge language hence applied matching tasks different nature different languages. empirical study variety matching tasks demonstrates efﬁcacy proposed model variety matching tasks superiority competitor models. matching potentially heterogenous language objects central many natural language applications generalizes conventional notion similarity relevance since aims model correspondence between linguistic objects different nature different levels abstractions. examples include top-k re-ranking machine translation dialogue natural language sentences complicated structures sequential hierarchical essential understanding them. successful sentence-matching algorithm therefore needs capture internal structures sentences also rich patterns interactions. towards propose deep neural network models adapt convolutional strategy speech natural language. explore relation representing sentences matching them devise novel model naturally host hierarchical composition sentences simple-to-comprehensive fusion matching patterns convolutional architecture. model generic requiring prior knowledge natural language putting essentially constraints matching tasks. part continuing effort understanding natural language objects matching main contributions summarized follows. first devise novel deep convolutional network architectures naturally combine hierarchical sentence modeling layer-by-layer composition pooling capturing rich matching patterns different levels abstraction; second perform extensive empirical study tasks different scales characteristics demonstrate superior power proposed architectures competitor methods. roadmap start introducing convolution network section basic architecture sentence modeling related existing sentence models. based that section propose architectures sentence matching detailed discussion relation. section brieﬂy discuss learning proposed architectures. section report empirical study followed brief discussion related work section convolutional sentence model start proposing convolutional architecture modeling sentences. illustrated figure takes input embedding words sentence aligned sequentially summarize meaning sentence layers convolution pooling reaching ﬁxed length vectorial representation ﬁnal layer. convolutional models convolution units local receptive ﬁeld shared weights design large feature adequately model rich structures composition words. convolution shown figure convolution layer- operates sliding windows words convolutions deeper layers deﬁned similar way. generallywith sentence input convolution unit feature type-f them) layer- effects pooling two-fold shrinks size representation half thus quickly absorbs differences length sentence representation ﬁlters undesirable composition words length variability variable length sentences fairly broad range readily handled convolution pooling strategy. speciﬁcally all-zero padding vectors last word sentence maximum length. eliminate boundary effect caused great variability sentence lengths convolutional unit gate sets output vectors all-zeros input zeros. given sentence input output type-f ﬁlter location layer given elements vector equals otherwise gate working max-pooling positive activation function keeps away artifacts padding layers. actually creates natural hierarchy all-zero padding consisting nodes neural would contribute forward process backward propagation analysis convolutional architecture convolutional unit combined max-pooling compositional operator local selection mechanism recursive autoencoder figure gives example could happen ﬁrst layers input sentence mat. illustration purpose present dramatic choice parameters make convolution units focus different segments within -word window. example feature maps give compositions vector. different feature maps offer variety compositions conﬁdence encoded values pooling chooses composition type adjacent sliding windows e.g. feature maps group rightmost sliding windows. relation recursive models convolutional model differs recurrent neural network recursive auto-encoder several important ways. first unlike take single path word/phrase composition determined either separate gating function external parser natural sequential order instead takes multiple choices composition large feature different leaves choices pooling afterwards pick appropriate segments composition. window width type composition would much richer rae. second convolutional model take supervised training tune parameters speciﬁc task property vital supervised learning-to-match framework. however unlike recursive models convolutional architecture ﬁxed depth bounds level composition could tasks like matching limitation largely compensated network afterwards take global synthesis learned sentence representation. relation shallow convolutional models proposed convolutional sentence model takes simple architectures consists convolution layer max-pooling entire sentence feature map. type models local convolutions global pooling essentially soft local template matching able detect local features useful certain task. since sentencelevel sequential order inevitably lost global pooling model incapable modeling complicated structures. hard convolutional model degenerates senna-type architecture limit number layers pooling window inﬁnitely large. architecture-i architecture-i illustrated figure takes conventional approach ﬁrst ﬁnds representation sentence compares representation sentences multi-layer perceptron essentially siamese architecture introduced applied different tasks nonlinear similarity function although arc-i enjoys ﬂexibility brought convolutional sentence model suffers drawback inherited siamese architecture defers interaction sentences individual representation matures therefore runs risk losing details important matching task representing sentences. words forward phase representation sentence formed without knowledge other. canadequately circumvented backward phase convolutional model learns extract structures informative matching population level. architecture-ii view drawback architecture-i propose architecture-ii built directly interaction space sentences. desirable property letting sentences meet high-level representations mature still retaining space individual development abstraction sentence. basically layer- take sliding windows sentences model possible combinations one-dimensional convolutions. segment segment feature d-convolution ﬁrst convolution obtain level representation interaction sentences obtain high level representation encodes information sentences. general two-dimensional convolution formulated ˆz−) concatenates corresponding vectors receptive ﬁeld layer-−. pooling different mechanism case selects among compositions different segments also among different local matchings. pooling strategy resembles dynamic pooling similarity learning context distinctions happens ﬁxed architecture much richer structure similarity. analysis arc-ii order preservation convolution pooling operation architecture-ii order preserving property. generally contains information words although generated slightly different segments pooling orders however retained conditional sense. experiments show arc-ii trained triples randomly shufﬂes words consistently gains ability ﬁnding correct usual contrastive negative sampling setting however happen arc-i. model generality hard show arc-ii actually subsumes arc-i special case. indeed arc-ii choose keep representations sentences separated ﬁnal arc-ii actually fully like arc-i illustrated figure speciﬁcally feature maps ﬁrst convolution layer either devoted devoted output segment-pair naturally divided corresponding groups. result output ﬁlter denoted rank-one possessing essentially information result ﬁrst convolution layer arc-i. clearly pooling follows reduce pooling separateness preserved. limit parameters second convolution units ensure individual development different levels abstraction side fully recover functionality arc-i. suggested order-preserving property generality arc-ii architecture offers capability also inductive bias individual development internal abstraction sentence despite fact built interaction sentences. result arc-ii naturally blend seemingly diverging processes successive composition within sentence extraction fusion matching patterns them hence powerful matching linguistic objects rich structures. intuition veriﬁed superior performance arc-ii experiments different matching tasks. training employ discriminative training strategy large margin objective. suppose given following triples oracle matched better following ranking-based loss objective predicted matching score includes parameters convolution layers mlp. optimization relatively straightforward architectures standard back-propagation. gating function easily adopted gradient discounting contribution convolution units turned gating function. words stochastic gradient descent optimization models. proposed models perform better mini-batch easily parallelized single machine multi-cores. regularization architectures early stopping enough models medium size large training sets small datasets however combine early stopping dropout deal serious overﬁtting problem. -dimensional word embedding trained wordvec embedding english words learnt wikipedia chinese words learnt weibo data experiments suggest ﬁne-tuning word embedding improve performances models cost longer training. vary maximum length words different tasks cope longest sentence. -word window throughout experiments test various numbers feature maps optimal performance. arc-ii models tasks eight layers arc-i performs better less layers hidden nodes. relu activation function models yields comparable better results sigmoid-like functions converges faster. experiments report performance proposed models three matching tasks different nature compare competitor models. among them ﬁrst tasks matching language objects heterogenous natures third natural example matching homogeneous objects. moreover three tasks involve languages different types matching distinctive writing styles proving broad applicability proposed models. competitor methods wordembed ﬁrst represent short-text embedding words contains. matching score short-texts calculated embedding documents input; deepmatch take matching model train datasets urae+mlp unfolding recursive autoencoder dimensional vector representation sentence wordembed; senna+mlp/sim senna-type sentence model sentence representation; experiment sentence completion artiﬁcial task designed elucidate different matching models capture correspondence clauses within sentence. basically take sentence reuters with balanced clauses divided comma ﬁrst clause second task recover original second clause given ﬁrst clause. matching considered heterogeneous since relation nonsymmetrical lexical semantic levels. deliberately make task harder using negative second clauses similar original ones training testing. representative example given follows although state four votes electoral college models trained million triples tested positive pairs accompanied four negatives results shown table proposed models nearly half cases right large margin table sentence completion. sentence models models without explicit sequence modeling. arc-ii outperforms arc-i signiﬁcantly showing power joint modeling matching sentence meaning. another convolutional model senna+mlp performs fairly well task although still running behind proposed convolutional architectures since shallow adequately model sentence. surprising urae comes last task might caused facts representation model trained reuters split-sentence setting hurts parsing vital quality learned sentence representation. experiment matching response tweet trained model million original pairs collected weibo major chinese microblog service compared experiment writing style obviously free informal. positive pair random responses negative examples rendering million triples training. example given below original response standing tweet randomly selected response damn work overtime weekend rest buddy. hard better start polishing resume. hold original pairs test matching model ability pick original response four random negatives results reported table task slightly easier experiment training instances purely random negatives. requires less grammatical rigor detailed modeling loose local matching patterns arc-ii beats models large margins convolutional sentence models arc-i senna+mlp come next. experiment paraphrase identiﬁcation paraphrase identiﬁcation aims determine whether sentences meaning problem considered touchstone natural language understanding. experiment included test methods matching homogenous objects. benchmark msrp dataset contains instances training test. training instances report test performance early stopping. stated earlier model specially tailored modeling synonymy generally requires instances work favorably. nevertheless generic matching models still manage perform reasonably well achieving accuracy score close best performer based hand-crafted features still signiﬁcantly lower state-of-the-art achieved unfolding-rae features designed task discussions arc-ii outperforms others signiﬁcantly training instances relatively abundant superiority arc-i however less salient sentences deep grammatical structures matching relies less local matching patterns experimenti. therefore raises interesting question balance representation matching representations objects whether guide learning process something like curriculum learning another important observation convolutional models perform favorably bag-of-words models indicating importance utilizing sequential structures understanding matching sentences. quite interestingly shown experiments arc-i arc-ii trained purely random negatives automatically gain ability telling whether words given sentence right sequential order therefore surprising auxiliary task identifying correctness word order response enhance ability model original matching tasks. noticed simple embedding learned wordvec yields reasonably good results three tasks. hypothesize wordvec embedding trained vector summation simple composition hence retains fair amount meaning short text segment. contrast bag-of-words models like deepmatch related work matching structured objects rarely goes beyond estimating similarity objects domain exceptions like dealing language objects methods still focus seeking vectorial representations common latent space calculating matching score inner product. work done building deep architecture interaction space texts-pairs largely based bag-of-words representation text models related long thread work sentence representation. aside models recursive nature fairly common practice word-embedding represent short-text mostly classiﬁcation little work convolutional modeling language. addition recent model sentence representation dynamic convolutional neural network work relies heavily carefully designed pooling strategy handle variable length sentence relatively small feature tailored classiﬁcation problems modest sizes. conclusion propose deep convolutional architectures matching natural language sentences nicely combine hierarchical modeling individual sentences patterns matching. empirical study shows models outperform competitors variety matching tasks.", "year": 2015}