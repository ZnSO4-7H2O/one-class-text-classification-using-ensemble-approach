{"title": "Consensus Attention-based Neural Networks for Chinese Reading  Comprehension", "tag": ["cs.CL", "cs.NE"], "abstract": "Reading comprehension has embraced a booming in recent NLP research. Several institutes have released the Cloze-style reading comprehension data, and these have greatly accelerated the research of machine comprehension. In this work, we firstly present Chinese reading comprehension datasets, which consist of People Daily news dataset and Children's Fairy Tale (CFT) dataset. Also, we propose a consensus attention-based neural network architecture to tackle the Cloze-style reading comprehension problem, which aims to induce a consensus attention over every words in the query. Experimental results show that the proposed neural network significantly outperforms the state-of-the-art baselines in several public datasets. Furthermore, we setup a baseline for Chinese reading comprehension task, and hopefully this would speed up the process for future research.", "text": "reading comprehension embraced booming recent research. several institutes released cloze-style reading comprehension data greatly accelerated research machine comprehension. work ﬁrstly present chinese reading comprehension datasets consist people daily news dataset children’s fairy tale dataset. also propose consensus attention-based neural network architecture tackle cloze-style reading comprehension problem aims induce consensus attention every words query. experimental results show proposed neural network significantly outperforms state-of-the-art baselines several public datasets. furthermore setup baseline chinese reading comprehension task hopefully would speed process future research. ultimate goal machine intelligence read comprehend human languages. among various machine comprehension tasks recent research cloze-style reading comprehension task attracted lots researchers. cloze-style reading comprehension problem aims comprehend given context document answer questions based nature document answer single word document. thus cloze-style reading comprehension described triple adopting attention-based neural network approaches machine able learn relationships document query answer. known neural network based approaches need large-scale training data train reliable model predictions. hermann published cnn/daily mail news corpus cloze-style reading comprehensions content formed news articles summarization. also hill released children’s book test corpus research training samples generated automatic approaches. that automatically generating large-scale training data neural network training essential reading comprehension. furthermore difﬁcult problems reasoning summarization context need much data learn higher-level interactions. though seen many improvements public datasets researchers suggested dataset requires less high-level inference expected furthermore public datasets automatically generated indicate pattern training testing phase nearly same easier machine learn patterns. paper release chinese reading comprehension datasets including people daily news datasets children’s fairy tale datasets. highlight datasets human evaluated dataset testing purpose. harder machine answer questions automatically generated questions human evaluated dataset processed accordance pattern automatic questions. detailed analysis given following sections. main contributions paper follows also propose reﬁned neural network aims utilize full representations query deal cloze-style reading comprehension task model outperform various state-of-the-art baseline systems public datasets. rest paper organized follows. section brieﬂy introduce existing cloze-style datasets describe chinese reading comprehension datasets detail. section show reﬁned neural network architecture cloze-style reading comprehension. experimental results public datasets well chinese reading comprehension datasets given section related work described section make brief conclusion work paper. ﬁrst begin brief introduction existing cloze-style reading comprehension datasets introduce chinese reading comprehension datasets people daily children’s fairy tale. existing cloze-style datasets typically main genres cloze-style datasets publicly available stem english reading materials. cnn/daily mail. news articles often come short summary whole report. spirit this hermann constructed large dataset web-crawled daily mail news data. firstly regard main body news article document query formed summary article entity word replaced placeholder indicate missing word. ﬁnally replaced entity word answer query. also proposed anonymize named entity tokens data re-shufﬂe entity tokens every sample order exploit general relationships anonymized named entities rather common knowledge. chen studies datasets showed anonymization less useful expected. children’s book test. also dataset called children’s book test released hill built children’s book story. different previously published cnn/daily mail datasets formed document consecutive sentences book regard sentence query word blanked placeholder. missing word chosen named entities common nouns verbs prepositions. verbs prepositions less dependent document studies focusing datasets. people daily children’s fairy tale datasets part introduce chinese reading comprehension datasets detail. though many solid works previously described public datasets studies chinese reading comprehension datasets. makes datasets different previous works listed below. pre-processed daily mail datasets available http//cs.nyu.edu/˜kcho/dmqa/ datasets available http//www.thespermwhale.com/jaseweston/babi/cbtest.tgz datasets available http//hfl.iflytek.com/chinese-rc/. figure example training sample people daily datasets xxxxx represents missing word. example document consists sentences sentence chosen query. people daily. roughly collected news articles people daily website. following process news articles triple form dqa. detailed procedures follows. given certain document composed sentences randomly choose answer word document. note that restrict answer word noun well answer word appear least twice document. partof-speech sentence segmentation identiﬁed using toolkit distinguish named entities common nouns hill did. third given query document target prediction recover answer generate tremendous triples training proposed neural network without assumptions nature original corpus. note that unlike previous work using method mentioned above document re-used different queries makes general generate large-scale training data neural network training. figure shows example people daily datasets. children’s fairy tale. except validation test people daily news data also present out-of-domain test sets well. out-of-domain test sets made children’s fairy tale fairly different news genre. reason out-of-domain test sets that children’s fairy tale mainly consists stories animals virtualized characters dataset test automatically generated using algorithms described above made human suggest latter harder former one. automatically generated test sets aware co-occurence ﬁxed collocation words thus pattern around query blank exactly appeared document much easier machine identify correct answer. building human evaluation test eliminated types samples makes harder machine comprehend. intuitively human evaluation test harder previously published cloze-style test sets. section introduce attention-based neural network model cloze-style reading comprehension task namely consensus attention reader model primarily motivated kadlec aims directly estimate answer document instead making prediction full vocabularies. noticed concatenating ﬁnal representations query states enough representing whole information query. propose utilize every time slices query make consensus attention among different steps. formally given training triple construct network following way. ﬁrst convert one-hot representation document query continuous representations shared embedding matrix query typically shorter document sharing embedding weights query representation beneﬁted embedding learning document side better separating embedding matrices individually. different bi-directional rnns contextual representations document query capture contextual information history future. implementation bi-directional gated recurrent unit modeling. take hdoc hquery represent contextual representations document query -dimension tensor shape. that directly make product hdoc hquery importance document word respect query word time then every time step query probability distribution document denoted means attention value word document time length document. consensus attention individual attentions explicitly deﬁne merging function α...α. denote vocabulary training efﬁciency generalization people daily datasets truncate full vocabulary shortlist unknown words mapped different speciﬁc symbols using method proposed vocabulary truncation cbtest dataset. neural network setups dimensions embedding layer hidden layer dropout task listed table trained model several epochs choose best model according performance validation set. models trained tesla gpu. model implemented theano keras results public datasets verify effectiveness proposed model ﬁrst tested model public datasets. evaluation carried news datasets cbtest ne/cn datasets statistics datasets listed table pre-processing done datasets. experimental results given table evaluate model terms accuracy. time limitations evaluate model ensemble. news. performance news datasets shows model attention reader decrease validation improvements test set. failed outperform stanford model. stanford utilized glove embeddings normalized probabilities named entities document rather words could make difference results. model optimize certain type dataset make general. cbtest ne/cn. cbtest dataset model gives slight improvements reader improvements validation improvements test set. cbtest though slight drop validation declines boost test absolute improvements suggest model effective beneﬁcial consider every slices query answering. results chinese reading comprehension datasets results chinese reading comprehension datasets listed table that proposed reader signiﬁcantly outperform reader types test maximum improvements test-auto dataset. results indicate making consensus attention multiple time steps better relying single attention similar model ensemble also consensus voting result different models. also evaluated different merging functions. results methods signiﬁcantly outperform heuristics heuristics failed outperform reader. possible reason explained operation sensitive noise. non-answer word given high probability time step query could easily diminish noise averaging/summing time steps. higher value given non-answer word situation noise removed preserve till ﬁnal attentions inﬂuence predictions lot. also noticed that though achieved accuracy among people daily datasets signiﬁcant drop test sets. furthermore human evaluated test meets sharp decline accuracy automatically generated test set. analyses regard datasets out-of-domain tests training data test data poses declines test sets. problems remedied introducing similar genre training data. regardless absolute accuracies datasets human test much harder machine read comprehend discussed before. results automatically generated queries human-selected questions. note that human-evaluated test query also formulated original sentence document suggest general form queries another rise comprehension difﬁculties. example instead asking went xxxxx morning change general question form where morning makes harder machine comprehend general question form training data. hermann proposed methodology obtaining large quantities triples. using method large number training data obtained without much human intervention make possible train reliable neural network study inner relationships inside triples. used attention-based neural networks task. evaluation cnn/dailymail datasets showed approach effective traditional baselines. hill also proposed similar approach large scale training data collections children’s book reading comprehension task. using window-based memory network self-supervision heuristics surpass methods predicting named entities common nouns benchmark. reader closely related work kadlec proposed simple model using attention result directly pick answer document rather computing weighted representation document using attention weights like previous works. proposed model typically motivated pointer network model aims solve particular task answer single word appear document least once. experimental results show model outperforms previously proposed models large margin public datasets proposed effective generate exploit large-scale pseudo training data zero pronoun resolution task. main idea behind approach automatically generate largescale pseudo training data using neural network model resolve zero pronouns. also propose two-step training pre-training phase adaptation phase also applied tasks well. experimental results ontonotes corpus encouraging proposed approach signiﬁcantly outperforms state-of-the-art methods. work proposed entirely chinese reading comprehension dataset diversity existing cloze-style reading comprehension datasets. moreover propose reﬁned neural network model called consensus attention-based reader. though many impressive progress made public datasets believe current machine comprehensions still pre-mature stage. discussed previous section answer pseudo query document enough machine comprehension. general question form seen comprehensive processing human brains. though human-evaluated test still somewhat easy machine comprehend releasing dataset move step forward real-world questions becomes good bridge automatic questions real-world questions. paper introduce ﬁrst chinese reading comprehension datasets people daily children’s fairy tale. furthermore also propose neural network model handle cloze-style reading comprehension problems. model able take question words accounts computing attentions document. among many public datasets model could give signiﬁcant improvements various state-of-the-art baselines. also baseline chinese reading comprehension datasets hopefully make starter future studies. future work carried following aspects. first would like work another human-evaluated dataset contain real-world questions difﬁcult existing datasets publicly available. second going investigate hybrid reading comprehension models tackle problems rely comprehensive induction several sentences. would like thank anonymous reviewers thorough reviewing proposing thoughtful comments improve paper. work supported national leading technology research project grant projects national natural science foundation china grant national natural science youth foundation china grant wanxiang zhenghua ting liu. chinese language technology platform. proceedings international conference computational linguistics demonstrations pages association computational linguistics. kyunghyun bart merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder–decoder statistical machine proceedings conference empirical methods natural language processing translation. pages association computational linguistics. karl moritz hermann tomas kocisky edward grefenstette lasse espeholt mustafa suleyman phil blunsom. teaching machines read comprehend. advances neural information processing systems pages jeffrey pennington richard socher christopher manning. glove global vectors word representation. proceedings conference empirical methods natural language processing pages association computational linguistics. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research", "year": 2016}