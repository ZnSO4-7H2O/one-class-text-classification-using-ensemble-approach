{"title": "A Data and Model-Parallel, Distributed and Scalable Framework for  Training of Deep Networks in Apache Spark", "tag": ["stat.ML", "cs.AI", "cs.CV", "cs.DC", "cs.LG"], "abstract": "Training deep networks is expensive and time-consuming with the training period increasing with data size and growth in model parameters. In this paper, we provide a framework for distributed training of deep networks over a cluster of CPUs in Apache Spark. The framework implements both Data Parallelism and Model Parallelism making it suitable to use for deep networks which require huge training data and model parameters which are too big to fit into the memory of a single machine. It can be scaled easily over a cluster of cheap commodity hardware to attain significant speedup and obtain better results making it quite economical as compared to farm of GPUs and supercomputers. We have proposed a new algorithm for training of deep networks for the case when the network is partitioned across the machines (Model Parallelism) along with detailed cost analysis and proof of convergence of the same. We have developed implementations for Fully-Connected Feedforward Networks, Convolutional Neural Networks, Recurrent Neural Networks and Long Short-Term Memory architectures. We present the results of extensive simulations demonstrating the speedup and accuracy obtained by our framework for different sizes of the data and model parameters with variation in the number of worker cores/partitions; thereby showing that our proposed framework can achieve significant speedup (upto 11X for CNN) and is also quite scalable.", "text": "abstract. training deep networks expensive time-consuming training period increasing data size growth model parameters. paper provide framework distributed training deep networks cluster cpus apache spark. framework implements data parallelism model parallelism making suitable deep networks require huge training data model parameters memory single machine. scaled easily cluster cheap commodity hardware attain signiﬁcant speedup obtain better results making quite economical compared farm gpus supercomputers. proposed algorithm training deep networks case network partitioned across machines along detailed cost analysis proof convergence same. developed implementations fully-connected feedforward network lstm architectures. present results extensive simulations demonstrating speedup accuracy obtained framework diﬀerent sizes data model parameters variation number worker cores/partitions; thereby showing proposed framework achieve signiﬁcant speedup also quite scalable. many recent advances deep networks involve ﬁtting large models massive datasets obtain good results. training networks time-consuming training period ranging days even weeks. thus distributed implementation deep networks data partitioned stored multiple machines becomes important. additionally model parameters need distributed across multiple machines calls eﬃcient mechanism update model parameters distributed setting reducing communication overhead maintaining accuracies typical non-distributed environments. much work done build distributed frameworks training deep networks. distbelief google project adams microsoft distributed frameworks meant training large scale models deep networks thousands machines utilizing data model parallelism. training deep networks model-parallel system cluster using inﬁniband proposed central parameter server model used worker nodes asynchronously fetch update gradients master node holds updated model parameters; though systems better suited sparse gradient updates. google’s recent tensorflow framework improved version distbelief system used distributed training deep networks specifying computational graphs. firecaﬀe uses cluster gpus scale deep networks inﬁniband/cray interconnects. systems described show high performance terms speedup accuracy grained control scheduling. however demanding communication requirements unlikely exhibit scaling cluster cpus. moreover systems highly customized terms either requirement particular hardware software tools hence diﬃcult integrate generalpurpose batch computational frameworks spark mapreduce. gpus restricted expensive large model parameters training speedup small model memory cpu-gpu transfers prove signiﬁcant bottleneck. recently integrated spark caﬀe. model however implemented data parallelism without model parallelism. work produce distributed scalable framework training deep networks implementing data model parallelism using apache spark reason choosing spark provides open-source generic batch computational framework makes easy implement distributed optimization algorithms cluster commodity hardware. spark’s mllib/ml libraries oﬃcially released implementations multilayer perceptron classiﬁers implementing data parallelism. implementations lstm; makes implementation unique important. knowledge work implements data model parallelism apache spark network architectures. moreover obtained signiﬁcant speedup using scalable distributed framework spark. main contributions work summarized below downpour proposed provides asynchronous distributed implementation stochastic gradient descent. training instances divided across diﬀerent machines machine copy whole model/neural network called model replica. network enough divided across diﬀerent machines machine operates data model replica compute changes parameters values periodically sent central parameter server parameter update done according learning rule. updated parameters fetched parameter server asynchronously delay individual machines. gradient updates out-of-order weights date. however shown delay harmful initially harmless number iterations size training data increases case data parallelism massive amount training examples case model parallelism neural network memory single machine. hence training model allows work deep networks large data model sizes eﬃciently achieve signiﬁcant speedups. though forth idea model parallelism never discussed exact algorithm used training network. work propose implement simple algorithm distributed backpropagation case deep network memory single node hence network divided across diﬀerent nodes. term processes used represent computational units distributed across nodes i.e. single node contain process. processes designated master rest slaves. phases distributed backpropagation described below theoretical cost analysis model make method given reproduced convenience understanding. factors inﬂuence execution time modelthe time taken computation time taken communication ignoring computation time general equation cost analysis tlat latency associated sending message tdata time required transmit unit data inversely proportional network speed total number messages sent epoch number units data sent. assume number processes number training examples training examples initially master process sends entire input data processes’ portion output labels rest slave processes generating messages equations number neurons layer error vector layer total number layers network size data structure forward pass hidden layers process sends output neurons processes. assuming communication all-to-all broadcast hypercube structure single process requires messages sent generating total messages backward pass results generation types messages hidden layer. master process sends error vector previous layer processes followed process sending portion error vector current layer master resulting messages therefore value calculated derive upper bound expected regret hence prove model delayed stochastic gradient descent case data network distributed across diﬀerent machines converges. proof based along lines discussed details found appendix. cluster setup order carry experiments setup cluster nodes lab. machines four machines intel xeon processor/gb ram/ cores remaining processor ram/ cores; thereby giving total cores. designated master node rest slaves/workers. formed multi-node hadoop cluster distributed file system storage part apache spark distributed mode setup that. implementation details ﬁrst step training test data stored form distributed training data rdds diﬀerent machines gradients calculated independently individual machines. model enough distributed across diﬀerent machines partitioning weight matrix column-wise training proposed algorithm distributed backpropagation. gradients collected diﬀerent machines master node update parameters done according learning rule. process carried repeated number iterations till model converges. prediction made separate test set. diﬀerent types neural networks changes made forward backward passes performed individual machines hyperparameters changed accordingly. threads individual machines attain speedup. made jblas mallet libraries linear algebra optimization respectively. case network partitioning communication processes machine threads processes diﬀerent machines akka framework called within spark jobs. results obtained averaged multiple runs variations loads network latencies. results obtained using cpus extended version mnist dataset used benchmark dataset carrying experiments fully connected nets cnn. contains million samples binary images handwritten digits task character level prediction using lstm used project gutenberg dataset contains complete works shakespeare. network-speciﬁc training architecture details experimented diﬀerent number hidden layers number neurons layer kernel sizes networks. state speciﬁc details used obtain results mentioned next section. used three layer fully connected neurons input layer neurons ﬁrst second hidden layers respectively followed neurons output layer corresponding output classes. model parallelism part varied number neurons number layers generate suitable number model parameters. among gradient descent techniques conjugate gradient descent found fastest showed best results. three convolution layers contained kernel size feature maps kernel size feature maps kernel size feature maps respectively; ﬁrst second convolution layers followed mean pooling layer size last layer layer softmax. mini-batch gradient descent batch size used. lstm used randomly sampled sequence characters input network. number neurons input output layer network equals size vocabulary number lstm/rnn units ﬁrst second hidden layers kept respectively. output obtained sampling characters network epoch. lstm block implemented discussed consisting four gates peephole connections. mini-batch gradient descent batch size along rmsprop used optimization. developed implementation multilayer rnn/lstm capable handing variable length input output sequences padding masking. started obtaining plots experimentally prove convergence proposed algorithm. fig. error converges number epochs fig. shows mean gradient values converge zero time expected. results cluster performance using nets plotted fig. mins) fig. using plotted fig. hours) fig. diﬀerent sizes data samples. plots number model parameters kept ﬁxed number data samples varied. second phase experiments kept number data samples ﬁxed varied number model parameters diﬀerent network architectures. plotted results variation number model parameters number partitions fig. able accommodate million parameters memory single machines. partitions correspond term processes used explain algorithm model parallelism above. plot size model increases speedup. expected algorithm performs well large sizes model parameters. also seen given model size much improvement performance certain number partitions.this can’t obtain speedup beyond certain point distributed systems. however saturation point increases increase model sizes. nets rnn/lstm also similar pattern results obtained though speedup obtained varied across networks. speedup compared fully connected nets reduced communication overhead machines local connectivity structure cnn.we also implemented character-level predictor using lstm. plotted speedup obtained increase number worker machines fig. ﬁgure seen speedup increases uniformly increase number worker nodes thereby showing implementation distributed lstm also scalable. accuracy results implementations couldn’t included length constraints. work developed generic distributed scalable framework training deep networks apache spark implementing data parallelism model parallelism. proposed algorithm distributed backpropagation model parallelism along detailed cost analysis mathematical proof convergence. successfully applied framework diﬀerent neural network architectures like fully connected nets lstm. extensive simulations concluded framework pretty scalable performance improves nodes cluster. achieved speedup samples speedup samples fully connected nets speedup billion model parameters using cluster cpus.we conclude framework performs best increase size data samples large number model parameters used conveniently deep learning applications without requirement expensive hardware. hence framework distributed scalable economic oﬀers huge ﬂexibility terms usage integration", "year": 2017}