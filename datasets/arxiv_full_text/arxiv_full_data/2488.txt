{"title": "Sparse Linear Dynamical System with Its Application in Multivariate  Clinical Time Series", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Linear Dynamical System (LDS) is an elegant mathematical framework for modeling and learning multivariate time series. However, in general, it is difficult to set the dimension of its hidden state space. A small number of hidden states may not be able to model the complexities of a time series, while a large number of hidden states can lead to overfitting. In this paper, we study methods that impose an $\\ell_1$ regularization on the transition matrix of an LDS model to alleviate the problem of choosing the optimal number of hidden states. We incorporate a generalized gradient descent method into the Maximum a Posteriori (MAP) framework and use Expectation Maximization (EM) to iteratively achieve sparsity on the transition matrix of an LDS model. We show that our Sparse Linear Dynamical System (SLDS) improves the predictive performance when compared to ordinary LDS on a multivariate clinical time series dataset.", "text": "linear dynamical system elegant mathematical framework modeling learning multivariate time series. however general difﬁcult dimension hidden state space. small number hidden states able model complexities time series large number hidden states lead overﬁtting. paper study methods impose regularization transition matrix model alleviate problem choosing optimal number hidden states. incorporate generalized gradient descent method maximum posteriori framework expectation maximization iteratively achieve sparsity transition matrix model. show sparse linear dynamical system improves predictive performance compared ordinary multivariate clinical time series dataset. developing accurate models dynamical systems critical successful applications outcome prediction decision support optimal control. large spectrum models developed successfully applied purposes literature paper focus popular model time series analysis linear dynamical system application clinical time series develop method learn performs better future value predictions learned small amount complex multivariate time series dataset. widely used model time series analysis real-valued sequences. model markovian assumes dynamic behaviour system captured well using small realvalued hidden-state variables linear-state transitions corrupted gaussian noise. observations similarly hidden states real-valued. brieﬂy observations time linear combinations hidden state values time. applications model parameters known priori majority real-world applications model parameters unknown need learn data consists observation sequences assume generated model. done using standard learning approaches problem learning model gives rise numerous important questions given multivariate observation sequences many hidden states needed represent system dynamics well? moreover since transition observation matrices depend number hidden states prevent overﬁt model parameters number examples small? work address issues presenting method based sparse representation able adjust number hidden states time prevent overﬁt model. approach builds upon probabilistic formulation model casts optimization parameters slds approach distinctly different previous work formulates traditional kalman ﬁlter one-step update optimization procedure incorporates sparsity constraints achieve sparsity hidden states. trains training example tries sparse linear combination coefﬁcients order combine ensemble models. neither directly achieve sparsity parameters furthermore performance resulting models still depends optimal number hidden states. introduces bayesian nonparametric approach identiﬁcation observation-only linear systems hidden states involved. underlining assumption observations obtained linear combinations previous observations system inputs restrictive model complex multivariate time series makes model sensitive noisy observations outliers. test sparse solution problem modeling dynamics sequences laboratory test results. show improves learning model leads better accuracy predicting future time-series values. paper organized follows. section review basics linear dynamical system. section describe slds method sparsifying parameters. inference learning details slds explained section experimental results compare slds method ordinary presented section section summarize work outline possible future extensions. linear dynamical system real-valued time series model represents observation sequences indirectly help hidden states. {zt} {yt} deﬁne sequences hidden states observations respectively. models dynamics sequences terms state transition probability state-observation probability probabilities modeled using following equations observation vector made hidden states vector. transitions among current previous hidden states linear captured terms transition matrix stochastic component transition modeled zero-mean gaussian noise zero mean covariance matrix observations sequence derived hidden states sequence. dependencies linear modeled using emission matrix zero mean gaussian noise models stochastic relation states observation. addition deﬁned initial state distribution mean covariance matrix complete parameters parameters model learned using either expectation-maximization algorithm spectral learning algorithms section propose sparse representation able adjust number hidden states time prevents overﬁt model. speciﬁcally impose regularizers every element transition matrix leads zero entries transition matrix zero entries transition matrix indeed reduce actual number parameters sparsify hidden states avoid overﬁtting problem real data even number hidden states originally picked large. achieve sparsity transition matrix introduce laplacian prior element since laplacian priors equivalent regularizations general laplacian distribution following form location parameter scale parameter. here assume every element independent following laplacian density exp. e-step follow backward algorithm compute sufﬁcient statistics expected likelihood. m-step separate optimization parts. optimization iteration m-step need maximize respect equivalent minimizing function convex non-differentiable easily decompose parts shown eq.. since differentiable adopt generalized gradient descent algorithm minimize update rule step size iteration proximal function proxαk proxαk deﬁned soft-thresholding function sβαk mt−|t mtt−|t using simple algebraic manipulation arrive g||f mt−|t||f frobenius norm inequality holds sub-multiplicative property frobenius norm. since know β||a|| lipschitz continuous mt−|t||f according initial value optimal value theorem gives simple step size generalized gradient updates also guarantees fast convergence rate. optimization parameters estimated similarly approach taking corresponding derivative setting zero solving analytically. update rules shown algorithm m-step optimizing summarized algorithm appendix. test approach time series data obtained electronic health records postsurgical cardiac patients stored database test performance prediction model randomly select patients least complete blood count tests ordered hospitalizations. three tests used experiment mean corpuscular hemoglobin concentration mean corpuscular hemoglobin mean corpuscular volume time series data noisy signals ﬂuctuate time observations obtained varied time-interval period. order regularly sampled multivariate time series dataset apply -hour discretization original multivariate time series dataset linear interpolation missing gaps discretization. compare sparse ordinary multivariate time series dataset. evaluate performance slds approach split time series patients training testing sets times series form training data used testing. evaluation metric. objective test predictive performance approach ability predict future value observation patient future time given sequence patient’s past observations. judge quality prediction using average mean absolute error multiple test data predictions. speciﬁcally amae deﬁned follows true observation time series ˆyij corresponding predicted value yij. number time series length time series. conduct evaluation test dataset generate various prediction tasks follows. patient complete time series patient calculate number observations time series generate different pairs indices patient index last observation assumed seen index observation would like predict. adding time stamp reading index indices help deﬁne possible prediction tasks formulate time series. time series patient proceed randomly picking different pairs indices total predictions tasks method repeat random sampling predictions times average mean absolute error tasks judge quality test predictions. prediction results shown table figure figure figure figure olds achieves lowest prediction error amae number hidden states varying number states errors olds ﬁrst improve increase number hidden states exceeds optimal point. clearly shows overﬁtting problem. slds performs similarly; performance ﬁrst impoves deteriorates. however errors deteriorate slower pace shows robust overﬁtting problem. comparing methods slds always outperfroms olds indicating additional sparsity term included optimization helps better underlying structure transition matrix. conclusion paper presented sparse linear dynamical system multivariate time series predictions. comparing traditional linear state-space systems slds model tries prevent overﬁtting problem represent additional structure transition matrix. experimental results real world clinical data electronic health records systems demonstrated novel model achieves errors statistically signiﬁcantly lower errors ordinary linear dynamical system. would like note results presented work preliminary include three time series. investigation complex higher dimensional timeseries data needed conducted future. addition would like study group lasso regularization techniques believe would able better control dimensionality hidden state space. finally plan study extensions model switching-state controlled dynamical systems acknowledgement supported grants national institutes health. content solely responsibility authors necessarily represent ofﬁcial views nih. would like thank eric heim mahdi pakdaman useful discussions comments work. references iyad batal dmitriy fradkin james harrison fabian moerchen milos hauskrecht. mining recent temporal patterns event detection multivariate time series data. sigkdd pages iyad batal hamed valizadegan gregory cooper milos hauskrecht. pattern mining approach adam charles muhammad salman asif justin romberg christopher rozell. sparsity penalties dynamical system estimation. information sciences systems annual conference pages ieee bernard ghanem narendra ahuja. sparse coding linear dynamical systems application dynamic texture recognition. pattern recognition international conference pages ieee james douglas hamilton. time series analysis volume cambridge univ press hauskrecht valko batal clermont visweswaran g.f. cooper. conditional outlier detection clinical alerting. amia annual symposium proceedings volume page american medical informatics association katayama. subspace methods system identiﬁcation. springer branislav kveton milos hauskrecht. solving factored mdps exponential-family transition lennart ljung torkel glad. modeling dynamic systems. p.v. overschee moor d.a. hensher j.m. rose w.h. greene train greene krause gere hibbeler. subspace identiﬁcation linear systems theory–implementation. boston kluwer academicpublishers", "year": 2013}