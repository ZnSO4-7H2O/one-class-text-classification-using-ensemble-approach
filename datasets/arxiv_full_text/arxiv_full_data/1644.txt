{"title": "Zero-Shot Relation Extraction via Reading Comprehension", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relation types that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relation types with high accuracy, and that zero-shot generalization to unseen relation types is possible, at lower accuracy levels, setting the bar for future work on this task.", "text": "show relation extraction reduced answering simple reading comprehension questions associating natural-language questions relation slot. reduction several advantages learn relationextraction models extending recent neural reading-comprehension techniques build large training sets models combining relation-speciﬁc crowd-sourced questions distant supervision even zero-shot learning extracting relation types speciﬁed test-time labeled training examples. experiments wikipedia slot-ﬁlling task demonstrate approach generalize questions known relation types high accuracy zero-shot generalization unseen relation types possible lower accuracy levels setting future work task. relation extraction systems populate knowledge bases facts unstructured text corpus. type facts predeﬁned crowdsourcing distant supervision collect examples train extraction model relation type. however approaches incapable extracting relations speciﬁed advance observed training. paper propose alternative approach relation extraction potentially extract facts types neither speciﬁed observed priori. show possible reduce relation extraction problem answering simple reading comprehension questions. relation type least parametrized natural-language question whose answer example relation educated mapped where study? which university graduate from?. given particular entity text mentions non-null answer questions asserts fact also ﬁlls slot figure illustrates examples. reduction enables ways framing learning problem. particular allows perform zero-shot learning deﬁne relations model already trained. speciﬁcally zero-shot scenario assumes access labeled data relation types. data used train reading comprehension model reduction. however test time asked previously unseen relation type rather providing labeled data relation simply list questions deﬁne relation’s slot values. assuming learned good reading comprehension model correct values extracted. data models. distant supervision relatively large number relations wikidata easily gathered practice wikireading dataset also introduce crowdsourcing approach gathering verifying questions relation. process produced questions relation average yielding dataset questionsentence-answer examples total. questions paired relation types instances overall procedure modest costs. modeling challenge existing reading-comprehension problem formulations assume answer question always present given text. however relation extraction premise hold model needs reliably determine question answerable. show recent state-of-the-art neural approach reading comprehension directly extended model answerability trained dataset. modeling approach another advantage reduction machine reading models improve time ability extract relations. experiments demonstrate approach generalizes paraphrases questions training incurring minor loss performance furthermore translating relation extraction realm reading comprehension allows extract signiﬁcant portion previously unseen relations virtually zero analysis suggests model able generalize cases learning typing information occurs across many relations well detecting relation paraphrases certain extent. also many feasible cases model quite master providing interesting challenge future work. interested particularly harsh zero-shot learning scenario given labeled examples relation types training extract relations type test time. information parametrized questions. setting differs prior relation extraction. bronstein explore similar zero-shot setting event-trigger identiﬁcation speciﬁed trigger words test time. generalize measuring similarity potential triggers given seed using unsupervised methods. focus instead slot ﬁlling questions suitable descriptions trigger words. open information extraction schemaless approach extracting facts text. open systems need relation-speciﬁc training data often treat different phrasings different relations. work hope extract canonical slot value independent original text phrased. universal schema represents open extractions knowledge-base facts single matrix whose rows entity pairs columns relations. redundant schema enables knowledge-base population matrix completion techniques. verga predict facts entity pairs observed original matrix; equivalent extracting seen relation types unseen entities rockt¨aschel demeester inference rules predict hidden knowledge-base relations observed naturallanguage relations. setting akin generalizing across different manifestations relation since natural-language description target relation appears training data. moreover information unseen relations explicit inference rules opposed implicit natural-language questions. zero-shot scenario manifestation test relation observed training substantially challenging universal-schema terminology empty column plus columns single entry columns share entities existing columns making rest matrix irrelevant. empty column others match descriptions. toutanova proposed similar approach decomposes natural-language relations computes similarity universal schema setting; however extend method knowledge-base relations attempt recover out-of-schema relations consider slot-ﬁlling challenge relation extraction given knowledgebase relation entity sentence example consider relation occupation entity steve jobs sentence steve jobs american businessman inventor industrial designer. goal text spans holds example {businessman inventor industrial designer}. empty also valid answer contain phrase satisﬁes observe given natural-language question expresses solving reading comprehension problem answering equivalent solving slot-ﬁlling challenge. challenge becomes queriﬁcation translating rather querify every entity propose method querifying relation treat variable querify parametrized query question template instantiate template relevant entities creating tailored natural-language question entity process schema queriﬁcation order magnitude efﬁcient querifying individual instances annotating relation type automatically annotates instances. applying schema queriﬁcation relations pre-existing relation-extraction dataset converts reading-comprehension dataset. dataset train readingcomprehension model given sentence question returns text spans within answer zero-shot scenario given relation test-time neither speciﬁed observed beforehand. example deciphered relation turing colleagues came method efﬁciently deciphering enigma domainspeciﬁc exist common knowledge-bases. querify reading-comprehension model sentence document interest instantiating question template different entities ultimately need relation deﬁne information need form question. approach provides naturallanguage application developers interested incorporating relation-extraction component programs; linguistic knowledge pre-deﬁned schema needed. implement approach require components training data reading-comprehension model. section construct large relationextraction dataset querify using efﬁcient crowdsourcing procedure. adapt existing state-of-the-art reading-comprehension model suit problem formulation collect reading-comprehension examples figure ﬁrst gather labeled examples task relation-slot ﬁlling. slot-ﬁlling examples similar reading-comprehension examples contain knowledge-base query instead natural-language question; e.g. spouse instead angela merkel married to?. collect many slot-ﬁlling examples distant supervision convert queries natural language. slot-filling data wikireading dataset collect labeled slot-ﬁlling examples. wikireading collected aligning wikidata relation corresponding wikipedia article entity reasonable assumption relation derived article’s text. instance dataset contains relation entity document answer used distant supervision select speciﬁc sentences manifests. speciﬁcally took ﬁrst sentence contain grouped instances merge answers given answer this implemented efﬁciently constraining potential entities existing facts knowledge base. example entity satisﬁes occupation entity subclass holds. leave exact implementation details system future work. educated albert einstein’s alma mater? albert einstein awarded university z¨urich dissertation titled... steve jobs american businessman inventor industrial designer. angela merkel’s second current husband quantum chemist professor joachim sauer largely... figure examples reading-comprehension dataset. instance contains relation question sentence answer question explicitly mentions entity also appears brevity answers underlined instead displayed separate column. wine produced region france. capital mexico populous city north america. unincorporated organized territory united states. mountain range stretches across united states canada. schema queriﬁcation crowdsourcing queriﬁcation schema level straightforward task encourage workers ﬁgure relation’s semantics lexicallycreative asking questions. therefore apply combination crowdsourcing tactics mechanical turk annotation phases collection veriﬁcation. relation present annotator example sentences entity sentence masked variable addition underline extractable answers appear annotator must come question whose answer given sentence underlined span within sentence. example country captures exact answers sentence figure asking general question where might return false positives worker produced different question templates example set. relation sampled different example sets hired different annotators set. instance annotation phase workers also given addition example name relation another instance hidden. potential question templates unique average. veriﬁcation phase measure question templates’ quality sampling additional sentences instantiating question template example entity annotators asked answer question sentence mark unanswerable; annotators’ answers match question template valid. discarded templates answered correctly majority examples overall applied schema queriﬁcation relations least examples costing roughly veriﬁcation phase left high-quality question templates spanning relations. join templates slot-ﬁlling dataset along relations instantiating template matching entities. process yields reading-comprehension dataset examples instance contains original relation question sentence answers negative examples support relation extraction dataset deviates recent reading comprehension formulations introduces negative examples question-sentence pairs answers following methodology infoboxqa generate negative examples matching question pertains relation sentence expresses another relation. also assert sentence contain answer instance match used relatively lenient measure many annotators selected correct answer slightly incorrect span; e.g. american businessman instead businessman. therefore used token-overlap secondary ﬁlter requiring average score least angela merkel married sentence occupation angela merkel german politician currently chancellor germany. process generated million negative examples. relatively naive method generating negative examples analysis shows third negative examples contain good distractors discussion recent datasets collected expressing knowledge-base assertions natural language. simple dataset created annotating questions individual freebase facts collecting roughly natural-language questions support knowledge graph. morales used similar process collect questions wikipedia infoboxes yielding -example infoboxqa dataset. task identifying predicate-argument structures qasrl proposed open schema semantic roles relation argument predicate expressed natural-language question containing predicate whose answer argument authors collected question-answer pairs sentences. efforts costs scale linearly number instances requiring signiﬁcant investments large datasets. contrast schema queriﬁcation generate enormous amount data fraction cost labeling relation level; evidence able generate dataset times larger simple best knowledge ﬁrst robust method collecting question-answering dataset crowd-annotating schema level. text models assume exists correct answer span. therefore modify existing model allows decide whether answer exists. ﬁrst give high-level description original model describe modiﬁcation. start bidaf model whose input sequences words sentence question model predicts start positions ystart yend answer span bidaf uses recurrent neural networks encode contextual information within alongside attention mechanism align parts vice-versa. outputs bidaf model conﬁdence scores ystart yend potential start end. denote scores zstart zend number words sentence words zstart indicates likely answer start position sentence similarly zend indicates likely answer index. assuming answer exists transform conﬁdence scores pseudo-probability distributions pstart pend softmax. probability i-to-j-span context therefore deﬁned allow model signal answer concatenate trainable bias conﬁdences score vectors zstart zend. score vectors ˜zstart ˜zend deﬁned ˜zstart similarly zend indicates row-wise concatenation. hence last elements ˜zstart ˜zend indicate model’s conﬁdence answer start respectively. apply softmax augmented vectors obtain pseudo-probability distributions ˜pstart ˜pend. means probability model assigns null answer enables model sensitive absolute values conﬁdence scores zstart zend. essentially setting learning threshold decides whether model sufﬁciently conﬁdent best candidate answer span. threshold provides dynamic per-example decision whether instance answerable also global conﬁdence threshold pmin; best answer’s conﬁdence threshold infer answer. section global threshold broader picture model’s performance. evaluation metrics instance evaluated comparing tokens labeled answer predicted span. precision true positive count divided number times system returned non-null answer. recall true positive count divided number instances answer. hyperparameters experiments initialized word embeddings glove ﬁne-tune them. typical training order million examples epochs enough convergence. training sets ratio positive negative examples chosen match test sets’ ratio. comparison systems experiment sevkb relation eral variants model. feed model relation indicator instead question. expect variant generalize reasonably well unseen entities fail unseen relations. second variant uses relation’s name instead question also consider weakened version queriﬁcation approach where training question template relation observed. full variant model multiple templates also evaluate asking relation multiple ways improves performance create ensemble sampling questions test instance predicting answer each. choose answer highest conﬁdence scores. addition model compare three systems. ﬁrst random baseline chooses named entity sentence appear question also reimplement labeler shown good results extractive portion wikireading lastly retrain off-the-shelf relation extraction system shown promising results number benchmarks. system represents relations indicators cannot extract unseen relations. setup partitioned dataset along entities question randomly clustered entity three groups train test. instance alan turing examples appear training steve jobs examples exclusive test. sampled examples train test. partition also ensures sentences test time different train since sentences gathered entity’s wikipedia article. results table shows model generalizes well entities texts little variance performance relation relation multiple templates question ensemble. single template performs signiﬁcantly worse variants; conjecture simpler relation descriptions allow easier parameter tying across different examples whereas learning multiple questions allows model acquire important paraphrases. variants model outperform off-the-shelf relation extraction systems setting demonstrating reducing relation extraction reading analysis examples multiple templates mispredicted shows errors attributed annotation errors additional result inaccurate span selection model fully penalized. total sample pure system errors suggesting model close performance ceiling setting setup created folds train/dev/test samples data question template relation held test another development set. instance what living? appear training what job? exclusive test set. split stratiﬁed sampling examples question template process created training sets examples matching development test sets examples each. trained tested multiple templates folds yielding performance unseen templates. replicated existing test sets replaced unseen question templates templates training yielding performance seen templates. revisiting example convert test-set occurrences what job? what living?. results table shows approach able generalize unseen question templates. system’s performance unseen questions nearly strong previously observed templates setup created folds train/dev/test samples partitioned along relations relations train test. example educated allocated test educated examples appear train. using stratiﬁed sampling relations created training sets examples matching test sets examples fold. results table shows system’s performance; figure extends results variants model applying global threshold answers’ conﬁdence scores generate precision/recall curves expected representing knowledge-base relations indicators insufﬁcient zero-shot setting; must interpreted natural-language expressions allow andr´as dombai... ...currently plays goalkeeper tatab´anya. airport closely associated royal jordanian? royal jordanian airlines... ...from main base queen alia international airport... f¨urstenberg china factory founded... ...by johann georg langen... voice type ´etienne lainez have? etienne lainez... ...was french operatic tenor... generalization. difference using single question template relation’s name appears minor. however training variety question templates substantially increases performance. conjecture multiple phrasings relation allows model learn answer-type paraphrases occur across many relations also advantage multiple questions test time negative examples checked whether distractor incorrect answer correct answer type appears sentence. example question john mccain married answer john mccain chose sarah palin running mate sarah palin correct answer type. noticed negative examples contain distractors. pairing examples results unseen relations experiment section found method answered distractor examples incorrectly compared easier examples. appears negative examples easy signiﬁcant portion trivial. positive examples observed instances solved matching relation sentence question others rely answer’s type. moreover notice categorized according type information needed detect part question appears verbatim text phrasing text deviates question typical relations well phrasing text deviates question unique relation name categories verbatim global speciﬁc respectively. figure illustrates different types cues discuss analysis. selected important solving instance. important cues counted half. table shows distribution. type cues appear somewhat dominant relation cues half cues relation-speciﬁc whereas global cues account third cases verbatim cues sixth. encouraging result potentially learn accurately recognize verbatim global cues relations. however method able exploit cues partially. templates setting. hand model appears agnostic whether relation verbatim global speciﬁc able correctly answer instances similar accuracy examples rely typing information trend much clearer; model much better detecting global type cues speciﬁc ones. based observations think primary sources model’s ability generalize relations global type detection acquired training many different relations relation paraphrase detection probably relies pre-trained word embeddings. showed relation extraction reduced reading comprehension problem allowing generalize unseen relations deﬁned on-the-ﬂy natural language. however problem zero-shot relation extraction solved poses interesting challenge information extraction machine reading communities. research machine reading progresses tasks beneﬁt similar approach. support future work avenue make code data publicly available. research supported part darpa deft program gifts google tencent nvidia allen distinguished investigator award. also thank mandar joshi victoria group helpful conversations comments work.", "year": 2017}