{"title": "Nothing Else Matters: Model-Agnostic Explanations By Identifying  Prediction Invariance", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "At the core of interpretable machine learning is the question of whether humans are able to make accurate predictions about a model's behavior. Assumed in this question are three properties of the interpretable output: coverage, precision, and effort. Coverage refers to how often humans think they can predict the model's behavior, precision to how accurate humans are in those predictions, and effort is either the up-front effort required in interpreting the model, or the effort required to make predictions about a model's behavior.  In this work, we propose anchor-LIME (aLIME), a model-agnostic technique that produces high-precision rule-based explanations for which the coverage boundaries are very clear. We compare aLIME to linear LIME with simulated experiments, and demonstrate the flexibility of aLIME with qualitative examples from a variety of domains and tasks.", "text": "core interpretable machine learning question whether humans able make accurate predictions model’s behavior. assumed question three properties interpretable output coverage precision effort. coverage refers often humans think predict model’s behavior precision accurate humans predictions effort either up-front effort required interpreting model effort required make predictions model’s behavior. approach interpretable machine learning designing inherently interpretable models. visualizations models usually perfect coverage trade-off accuracy model effort required comprehend especially complex domains like text images input space large accuracy usually sacriﬁced models compact enough comprehensible humans. experiments usually involve showing humans visualizations measuring human precision predicting model’s behavior random instances time required make predictions model-agnostic explanations avoid need trade accuracy treating model black box. explanations sparse linear models gradients still exhibit high precision effort even complex models providing explanations local scope however coverage explanations explicit lead human error. take example figure explain prediction complex model predicts person described figure makes less linear lime explanation sheds light clear whether apply insights explanation instances. words even explanation faithful locally easy know local region furthermore clear linear approximation less faithful even within local region. paper introduce anchor local interpretable model-agnostic explanations system explains individual predictions if-then rules model-agnostic manner. rules intuitive humans usually require effort comprehend apply. particular alime explanation rule sufﬁciently anchors prediction changes rest instance matter example anchor figure states model almost always predict salary person educated beyond high school regardless features. explanations make coverage clear apply conditions rule met. propose method compute explanations guarantees high precision high probability. further present empirical comparison linear lime qualitative evaluation variety tasks demonstrate anchors intuitive high precision clear coverage boundaries. model explained denoted explain individual predictions anchor deﬁned constraints possible constraints example figure {education high school}. assume distribution interest sample sample inputs constraints anchor met. reason condition depend instance explained precision anchor deﬁned expected accuracy applying anchor instances meet constraints formalized equation argued before high precision requirement model-agnostic explanations. trivial perfectly precise anchor constraint speciﬁc example explained meets order balance precision coverage effort optimize objective equation shortest anchor high precision. length anchor used proxy effort speciﬁc anchors naturally less coverage. algorithm solving equation exactly unfeasible precision cannot computed exactly arbitrary ﬁnding best combinatorial complexity. address former approximate precision sampling solve probably approximately correct version equation chosen anchor high precision high probability. latter employ algorithm similar spirit lazy decision trees construct greedily. particular step want pick constraint dominates constraints terms precision stopping criterion equation met. efﬁciency want sample instances possible make greedy decision. hoeffding bounds differences precision decide constraint dominates constraints high probability. uses insight hoeffding trees difference control sampling distribution thus bounds sample regions input space reduce uncertainty precision estimates samples possible. lack space omit details algorithm. simulated experiments order evaluate difference linear lime anchor lime terms coverage precision perform simulated experiments datasets adult hospital readmission. latter -class classiﬁcation problem task predict patient readmitted hospital inpatient encounter within days days never. dataset learn gradient boosted tree classiﬁer trees generate explanations instances validation dataset. evaluate coverage precision explanations separate test dataset. anchor unless noted otherwise consider linear lime explanation covers every instance within distance parameter vary. alime sample sampling whole rows dataset except features constrained evaluate explanations chosen either random submodular pick procedure picks explanations maximize coverage validation. show precision-coverage plots single explanation figures vary linear lime alime. results show level coverage alime better precision linear lime. furthermore using submodular pick greatly increases coverage precision level. linear lime performs particularly worse dataset highest number dimensions distance degrades. note main advantages alime linear lime making coverage clear humans without human experiments know plots look like linear lime expect alime. vary number explanations simulated user sees figures order keep results comparable picked average precision linear lime least datasets alime able maintain higher precision regardless many explanations shown coverage dominates linear lime. worth noting datasets data type behavior model simple large part input space demonstrated anchors maximize coverage dataset figure models complex behavior input space covered simple rules high precision. figure image classiﬁcation explaining prediction inception examples qualitative examples part-of-speech tagging black state-of-the-art tagger explain predictions word play different contexts table anchors demonstrate picks correct patterns. furthermore short easy understand. anchors particularly suited task dimensionality small behavior good models easily captured if-then rules linear models. image classiﬁcation alime explain prediction inception classiﬁer image zebra figure ﬁrst split image superpixels. anchor figure means non-grey superpixels substitute greyed-out superpixels random image model predict zebra around time. illustrate this display figure images model predicts zebra. choice distribution produces images look nothing like real images makes robust explanations distributions hide parts image gray dark patches anchor demonstrates model picks pattern require zebra four legs even head pattern different patterns humans detect zebras. visual question answering visual models multi-modal thus explained terms image question both. here anchors questions leaving image ﬁxed bigram language model trained input questions select questions explain rows figures anchors respectively what many show questions drawn original question. ﬁrst anchor states what question answer banana time latter states many respectively explanations clearly indicate undesirable behavior model. again kind explanation intuitive easier understand linear model even high weight words what banana knows exactly applies not. work argued high precision clear coverage bounds desirable properties model-agnostic explanations. introduced alime system designed produce rule-based explanations exhibit properties. if-then rules intuitive easy understand identifying parts input result prediction invariance similar humans explain many choices. demonstrated alime’s ﬂexibility explaining predictions variety classiﬁers myriad domains outperforming linear explanations lime simulated experiments. references stanislaw antol aishwarya agrawal jiasen margaret mitchell dhruv batra lawrence zitnick devi parikh. visual question answering. international conference computer vision david baehrens timon schroeter stefan harmeling motoaki kawanabe katja hansen klaus-robert müller. explain individual classiﬁcation decisions. journal machine learning research pedro domingos geoff hulten. mining high-speed data streams. proceedings sixth sigkdd international conference knowledge discovery data mining pages york acm. isbn ---. ./.. jerome friedman. lazy decision trees. proceedings thirteenth national conference artiﬁcial intelligence volume aaai’ pages aaai press isbn ---x. johan huysmans karel dejaeger christophe mues vanthienen bart baesens. empirical evaluation comprehensibility decision table tree rule based predictive models. decis. support syst. april issn ./j.dss. cynthia rudin julie shah. bayesian case model generative approach case-based reasoning prototype classiﬁcation. ghahramani welling cortes n.d. lawrence k.q. weinberger editors advances neural information processing systems pages curran associates inc. himabindu lakkaraju stephen bach jure leskovec. interpretable decision sets joint framework description prediction. proceedings sigkdd international conference knowledge discovery data mining pages york acm. isbn ----. grégoire montavon sebastian bach alexander binder wojciech samek klaus-robert müller. explaining nonlinear classiﬁcation decisions deep taylor decomposition. december marco tulio ribeiro sameer singh carlos guestrin. model-agnostic interpretability machine learning. human interpretability machine learning workshop icml", "year": 2016}