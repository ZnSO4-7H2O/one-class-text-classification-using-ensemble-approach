{"title": "Efficient Multi-Start Strategies for Local Search Algorithms", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Local search algorithms applied to optimization problems often suffer from getting trapped in a local optimum. The common solution for this deficiency is to restart the algorithm when no progress is observed. Alternatively, one can start multiple instances of a local search algorithm, and allocate computational resources (in particular, processing time) to the instances depending on their behavior. Hence, a multi-start strategy has to decide (dynamically) when to allocate additional resources to a particular instance and when to start new instances. In this paper we propose multi-start strategies motivated by works on multi-armed bandit problems and Lipschitz optimization with an unknown constant. The strategies continuously estimate the potential performance of each algorithm instance by supposing a convergence rate of the local search algorithm up to an unknown constant, and in every phase allocate resources to those instances that could converge to the optimum for a particular range of the constant. Asymptotic bounds are given on the performance of the strategies. In particular, we prove that at most a quadratic increase in the number of times the target function is evaluated is needed to achieve the performance of a local search algorithm started from the attraction region of the optimum. Experiments are provided using SPSA (Simultaneous Perturbation Stochastic Approximation) and k-means as local search algorithms, and the results indicate that the proposed strategies work well in practice, and, in all cases studied, need only logarithmically more evaluations of the target function as opposed to the theoretically suggested quadratic increase.", "text": "levente kocsis data mining search research group informatics laboratory computer automation research institute hungarian academy sciences budapest hungary local search algorithms applied optimization problems often suﬀer getting trapped local optimum. common solution deﬁciency restart algorithm progress observed. alternatively start multiple instances local search algorithm allocate computational resources instances depending behavior. hence multi-start strategy decide allocate additional resources particular instance start instances. paper propose multi-start strategies motivated works multi-armed bandit problems lipschitz optimization unknown constant. strategies continuously estimate potential performance algorithm instance supposing convergence rate local search algorithm unknown constant every phase allocate resources instances could converge optimum particular range constant. asymptotic bounds given performance strategies. particular prove quadratic increase number times target function evaluated needed achieve performance local search algorithm started attraction region optimum. experiments provided using spsa kmeans local search algorithms results indicate proposed strategies work well practice cases studied need logarithmically evaluations target function opposed theoretically suggested quadratic increase. local search algorithms applied optimization problems often suﬀer getting trapped local optimum. moreover local search algorithms guaranteed converge global optimum conditions usually converge slow pace conditions satisﬁed. hand algorithms employed aggressive settings much faster convergence local optima achievable guarantee global optimum. common solualternatively start multiple instances local search algorithm allocate computational resources particular processing time instances depending behavior. instances started time number instances grow time depending allocation strategy. type problems computational cost usually measured total number steps made search algorithm instances often reﬂects situation evaluation target function optimized expensive costs related determine algorithms next negligible compared former paper address problem dynamically starting several instances local search algorithms allocating resources instances based performance. knowledge solutions problem either based heuristics assumption local optima search algorithms converge extreme value distribution paper propose multi-start strategies mild conditions target function attractive theoretical practical properties supposing convergence rate local search algorithms unknown constant strategies continuously estimate potential performance algorithm instance every phase allocate resources instances could converge optimum particular range constant. selection mechanism analogous direct algorithm optimizing lipschitz-functions unknown constant preference given rectangles contain global optimum. optimum within rectangle estimated optimistic estimate depends size rectangle. strategies function describing convergence rate local search algorithms similar size rectangles used direct algorithm. since proposed multi-start strategies potential performance local search algorithm continuously estimated currently best value target function returned algorithm method restricted work local search algorithms return best known value target function step. case example certain meta-learning problems goal good parameter setting learning algorithm. search space parameter space learning algorithm step local search methods means running learning algorithm completely possibly large data set. hand local search algorithm sort gradient search optimizing error function training data value target function usually available case batch learning gradient estimated samples. paper described analyzed section section deal selection mechanism among ﬁxed number instances local search algorithm while addition simple schedules starting instances also considered section natural extensions case ﬁnitely many local search algorithm instances. section concludes discussion results section simulation results real synthetic data provided section conclusions future work described section problem allocating resources among several instances search algorithms comfortably handled generalized version maximum k-armed bandit problem. original version problem consists several rounds round chooses arms receives reward depending choice goal maximizing highest reward received several rounds. model easily used problem considering local search algorithm instance pulling means taking additional step corresponding algorithm evaluating target function point suggested algorithm reward received value target function sampled point. generic algorithm standard maximum k-armed bandit problem reward assumed independent identical distribution provided adam so-called reservation price instance introduced gives maximum amount resources worth spend instance instance achieves reservation price useless select again. computation reservation price depends model algorithm learnt speciﬁc constraints. consider scenario several instances some possibly randomized local search algorithms goal maximizing expected performance. instance terminates. scenario natural assume values returned instances independent. furthermore since good search algorithms follow procedures yield substantially better results pure random guessing cicirello smith suggested rewards search instances viewed maximum many random variables hence modeled extreme value distributions. several algorithms based assumption hence developed maximum k-armed bandit problem returns following generalized extreme value distributions cicirello smith apply methods extreme-value-distribution assumption decision point meta-learning algorithm streeter smith model obtain upper conﬁdence bounds performance estimate type algorithms used algorithms best expected result. latter theoretically justiﬁed example natural strategy probe algorithm instances while estimate future performance based results trial phase promising algorithm time remaining. streeter smith proposed distribution free approach standard maximum k-armed bandit problem rewards round assumed independent clearly case situation algorithm instances parallel reward evaluating target function point improvement upon current maximum since samples chosen local search algorithm usually depend previous samples. nevertheless ideas lessons learnt maximum k-armed bandit problems used case well example algorithm threshold ascent streeter smith gives reasonably good solutions case principle probing instances using promising time remaining also carries situation easily algorithms ﬁrst exploration exploitation phase referred sequel explore-and-exploit algorithms. class algorithms simple rules suggested beck freuder predict future performance algorithm carchrae beck employ bayesian prediction. another related problem fast algorithms among several ones solve problem. precisely several algorithm instances available produce correct answer certain question suﬃciently long time. time needed algorithm instance answer assumed random quantity independent identical distributions instances goal combine given algorithms minimize expected running time answer found. distribution running time known optimal non-adaptive time-allocation strategy perform sequence runs certain cut-oﬀ time depends distribution distribution unknown particular running time sequence chosen results expected total running time logarithmic factor larger optimum achievable distribution known. note strategy among provide schedule increases number algorithm instances. set-up specialized problem goal ε-optimal approximation optimum running time number steps needed given search algorithm achieve approximation. note case running time algorithm instance providing ε-suboptimal solution deﬁned inﬁnity results luby remain valid ε-optimal solution found positive probability. problem kautz horvitz ruan gomes selman proposed allocation strategy based updating dynamically belief run-time distribution. concerning latter hoos st¨utzle found empirically run-time distributions approximately exponential certain problems ribeiro rosseti vallejos dealt comparison diﬀerent run-time distributions. finally time allocation strategies available optimization problem solved several times standard multi-armed bandit framework done gagliolo schmidhuber learning automatic algorithm conﬁguration underlying problem similar cases automatic algorithm conﬁguration usually refers tuning search algorithms meta-learning used subset problems tuning machine learning algorithms main problem allocate time slices particular algorithms maximizing best result returned. allocation depend intermediate performance algorithms. automatic algorithm conﬁguration metalearning systems various heuristics explore space algorithms parameters finally important note that although multi-start local search strategies solve global optimization problems concentrate maximizing performance given underlying family local optimization methods. since choice latter major eﬀect achievable performance compare results vast literature global optimization. denotes maximum simplicity assume continuous continuity implies existence particular bounded. therefore without loss generality assume non-negative. form known explicitly search algorithms usually evaluate several locations return estimate based observations. obvious trade-oﬀ number samples used quality estimate performance search strategy measured accuracy achieves estimating constraint number samples used. given local search algorithm general strategy ﬁnding good approximation optimum several instances initialized diﬀerent starting points approximate maximum value observed. concentrate local search algorithms deﬁned formally sequence possibly randomized sampling functions simplify notation following unless stated explicitly otherwise adopt convention argmax denote maximizing sample smallest index results remain valid choice break ties. simplicity consider starting single local search algorithm diﬀerent random points although results work extended allow varying parameters well allow dependence among initializations maximum probability number instances tends inﬁnity hand algorithm favorable properties possible design multi-start strategies still keep random search based consistency provide much faster convergence optimum terms number evaluations certain cases shown nesterov optimization convex functions gerencs´er v´ag´o noise-free spsa convex functions kieﬀer k-means clustering dimension log-concave densities. results pertain simple situation local optimum global many results extended general situations observed exponential rate convergence experiments functions many local maxima. alternative using multiple instances local search algorithms algorithms parallel round decide algorithms take extra step. approach based estimating potential performance local search algorithm based lemma note known obvious would instance possible performances become separated high probability sense margin performance actually best second best algorithm large actually best algorithm guaranteed best long high probability. could pick best instance given computational budget exhausted practice however usually known certain problem classes local search algorithms known belong function class example known constant factor even latter case best instance still cannot selected high probability matter large margin however using ideas general methodology lipschitz optimization unknown constant around problem estimate certain optimistic potential performance algorithm instance round step promising ones. main idea resulting strategy summarized follows. assume instances algorithm denoted denote location evaluated time take step starting point estimate location maximum algorithm samples since best achievable estimate maximum given actual algorithms gives high probability convergence rate algorithms provide best estimate maximum long then evaluates points round converges best achievable estimate estimate still optimistic rate convergence slower pessimistic rate convergence slower latter desirable sense negatively biased estimate expected performance algorithm want practice usually known exactly estimate often cannot constructed. hand known constant factor construct family estimates scales denote normalized version gδ/g′δ constant construct family estimates ranges positive reals. reasonable choose round algorithms take another step provide largest estimate values around fact know real scaling factor certainly algorithms provide largest value gδ/g′δ discussed later waste many samples algorithms maximize values using optimistic estimate similar spirit optimistic estimates standard upper conﬁdence bound-type solution multi-armed bandit problem well-known search algorithm however exact convergence rate known even constant factor many local search algorithms even corresponding bounds usually meaningful asymptotic regime often practical interest. therefore give freedom design algorithm going estimate form where similarly requirements positive monotone decreasing function limn→∞ also assume without loss generality actual form based theoretical analysis resulting algorithms heuristic considerations. essentially functions converge zero exponentially fast agreement exponentially fast local convergence rates examples given lemma optimal choice given example known left future work. particular closed interval containing also maximizes therefore round strategy metamax selects local search algorithms corresponding point ˆfinir−) algorithm selected uniformly random apart case least used algorithms provides currently best estimate happens surely ﬁrst round usually happen later guaranteed round least algorithms largest estimate ˆfinir− ˆfr− smallest step number njr− thus usually half total number function calls used optimal local search algorithm. observation gives practical lower bound proportion function calls made optimal local search algorithms; surprisingly theorem shows lower bound achieved algorithm asymptotically. randomization step precludes using multiple instances step number introduced speed algorithm certain pathological cases. example converges correct estimate algorithms produce estimate round independently samples inferior estimates randomization half calls compute made without randomization would drop round would algorithm. furthermore could take step algorithms convex hull similar pathological examples constructed beneﬁcial algorithms corners. hand almost never happens practice three algorithms line algorithms typically never fall non-corner points convex hull. remainder section analyze performance metamax algorithm. proposition shows algorithm consistent sense performance asymptotically achieves best algorithm instance number rounds increases. understand algorithm better lemma provides general suﬃcient condition algorithm instance advanced given round while based result lemma provides conditions ensure suboptimal algorithm instances used round stepped many times before. lemma gives upper bound number algorithm instances used round. results lemmas used show theorems remark optimal algorithm instances used least minimum frequency that turn yields asymptotic rate convergence metamax algorithm. described above round select exactly algorithms made least number steps. thus algorithms minimum step number algorithm increase rounds completes proof. lemma provides conditions using algorithm instances certain round depend actual performance instances. next result gives similar conditions however based best estimates achievable algorithms. ˆf∗i limr→∞ finir asymptotic estimate algorithm max≤i≤k ˆf∗i denote best estimate achievable using algorithms optimal algorithms converge best estimate denote cardinality half function calls evaluate correspond optimal algorithm instances. probability least |o|δ given ˆf∗k. max{g− since round algorithms used pi∈o c+kr′ high probability. since latter bounded r′+r′/ε high probability. algorithm used least happen although decay rate exponential quite fast enough satisfy optimal scenarios theorem hold. case turns number algorithms converging local maximum plays role determining usage frequency optimal algorithms. round least algorithms used suboptimal one. assume least suboptimal algorithms. assume converge suboptimal local maxima large enough optimal algorithm better suboptimal ones converges ˆfinir ˆfjnjr ˆfknkr assume without loss generality fjnjr ˆfknkr clearly chosen round assume nkr. since ˆfjnjr ˆfknkr convergent sequences large enough clearly implies grow linearly since diﬀerences bounded limr→∞ njr/nkr therefore suboptimal algorithm limn→∞ njr/r /kmax then despite favorable properties metamax strategy inconsistent too. however increase number algorithms inﬁnity consistency random search still keeping reasonably fast convergence rate metamax. tension metamax able inﬁnitely many local search algorithm instances. major issue local search algorithms started time time implemented modifying step metamax algorithm randomly initialized local search algorithm introduced round obviously skip initialization step metamax start algorithm samples. better control length round round allow diﬀerent function denoted depend value measured round before assume monotone decreasing limn→∞ typically make dependent nir− made algorithms round number algorithm instances used round note start exactly algorithm algorithm approaches inﬁnity reasonably fast. somewhat relaxing random initialization condition imagine situation local search algorithms initialized clever deterministic ﬁrst steps better value initial guesses. algorithms optimal provide example identical estimates ﬁrst three steps. easy algorithm stepped exactly twice thus convergence optimum achieved. although random initialization search algorithms guarantees metamax conceptually diﬀer place step extended step algorithm. result technical modiﬁcation also appears step simplify presentation metamax algorithm slight insigniﬁcant modiﬁcation also introduced step discussion below. metamax overtake usually introduces short rounds leading algorithm becomes also used one. goal modiﬁcation step synchronize choice optimal algorithms steps equally good solution would choose case step achieve actually best algorithm dominates others terms accuracy number calls made algorithms compute target function. type dominance used hutter slightly diﬀerent context. algorithm used current round. also note that result modiﬁcations currently best algorithm taken steps extra number steps taken step indeed positive. important consequence modiﬁcations that round number steps taken local search algorithm best round metamax following lines analysis provided metamax. first proposition shown algorithm consistent solution found algorithm actually converges lemma shows suboptimal algorithms make ﬁnitely many steps lemma gives upper bound length round. main theoretical results section apply metamax algorithm theorem gives lower bound number steps taken actually best algorithm given round while consequence theorem shows rate convergence algorithm function total number steps used algorithm instances turns quadratically steps needed generic local search algorithm instance converges optimum. local search algorithms converge local optima assumption usually satisﬁed situation hold pathological case inﬁnitely many local maxima maxima dense global maximum. assumption prove similarly lemma suboptimal algorithm selected limited number times increases particular large enough suboptimal algorithm chosen ﬁnitely many times. finally derive convergence rate algorithm metamax. first bound number steps taken currently best algorithm terms number rounds total number steps taken local search algorithms. prove second part notice round nir−r− algorithms stepped step algorithm used taken steps currently best one. also step extra samples used overtaking. case overtaking advanced step well air− nir−r− extra steps taken therefore nir−r− note proof used crude estimate length usual round relative example lemma this however aﬀects result constant factor long able bound number rounds number extra steps taken overtaking happens since eﬀect overtakings introduces quadratic dependence proof experimental results section show number algorithm instances usual growth rate which taken account sharpen bound often best algorithm chosen. assumption random search component metamax implies eventually optimal algorithm best. point convergence rate optimal local search algorithms determine performance search number steps taken best local search algorithm bounded theorem theorem suppose assumption holds. almost surely ﬁnite random index rounds estimate metamax algorithm total number steps taken local search algorithms round satisﬁes remark value bounded high probability using properties uniform random search actual problem; would yield similar bounds theorems metamax algorithm. note exploration-exploitation trade-oﬀ metamax algorithm value potentially decreased introduce algorithms often nirr reduced time. theorems imply that asymptotically metamax algorithm needs quadratically function evaluations local search algorithm ensured converge optimum. fiisi form partition denotes indicator function belong nicely behaving function class local search algorithm started converges maximum preserve performance local search algorithm original function class price asymptotically quadratic increase number function calls evaluate metamax follows consistency random search. performance bounds provided disadvantage asymptotic sense hold possibly large number rounds fact quite easy construct scheduling strategies consistent asymptotically arbitrarily large fraction function evaluations used optimal local search algorithms explore-and-exploit algorithms achieve goals number function evaluations used known ahead arbitrarily small fraction evaluations target function exploration. compare performance algorithms explore-and-exploit algorithms section particular match performance guarantees metamax family algorithms spend half time exploration half exploitation exploration part uniform allocation strategy used ﬁnite number local search algorithms schedule luby used inﬁnitely many local search algorithms. although theoretical guarantees proved paper metamax family also hold explore-and-exploit algorithms experiments metamax family seems behave superior compared algorithms expected. theoretical results also give suﬃcient guidance chose parameter algorithm simplicity ease presentation). results require suﬃciently fast exponential decay problem dependent cannot determined advance. suﬃciently fast decay rate would ensure example metamax algorithm could always stronger results theorem would never deal conditions exploration would performed algorithms would actually best local search algorithm). practice always found appropriate chose decay exponentially. furthermore found even eﬀective gradually decrease decay rate enhance exploration time elapses finally connection investigated. keeping limitations theoretical results mind still believe theoretical analyses given provide important insight algorithms guide potential user practical applications especially since properties metamax family proved asymptotic regime usually observed practice well. furthermore think possible improve analysis bound thresholds results become valid reasonable values would require diﬀerent approach therefore left future work. following present results metamax metamax. first demonstrate performance algorithm optimizing synthetic function next behavior algorithm tested standard data sets. show metamax applied tuning parameters machine learning algorithms classiﬁcation task solved neural network parameters training algorithm ﬁne-tuned metamax combined spsa. metamax also applied boost performance k-means clustering. section compare results experiments theoretical bounds obtained section experiments accordance simplifying assumptions introduced section main diﬀerence individual runs particular local search algorithm starting point. obviously general diversiﬁcation techniques exist example parameters local search algorithm could also vary instance instance initialization instance could also depend results delivered example relative diﬀerence average error emetamax metamax emetamax metamax optimizing parameters multi-layer perceptron learning letter data standard deviation relative diﬀerence deﬁned emetamax emetamax existing instances. although metamax strategies could also applied general scenarios behavior better studied simpler scenario; hence experiments correspond setup. section compare versions metamax algorithm multi-start strategies including three constant three variable number algorithm instances. strategies ﬁxed time steps target function evaluated times together local search instances used spsa base local search algorithm cases. spsa local search algorithm sampling function uses gradient descent stochastic approximation derivative actual location spsa estimates partial derivative implementation algorithm followed guidelines provided gain sequence perturbation size values vary diﬀerent experiments; chosen heuristically based experience similar problems addition evaluations required perturbed points also evaluate function current point starting point chosen randomly function evaluated ﬁrst point. thrasc threshold ascent algorithm streeter smith algorithm begins selecting ﬁxed number instances once. phase time step thrasc selects best estimates produced algorithm instances previous time steps counts many estimates produced denoting latter value time algorithm selects instance index argmaxi parameters algorithm experiments best value appeared note threshold ascent developed maximum k-armed bandit problem; nevertheless provides suﬃciently good performance setup test experiments. rand random search algorithm. seen running sequence spsa algorithms instance used exactly step evaluation random starting point spsa algorithm. ee-unif algorithm instance explore-and-exploit algorithms. ﬁrst steps unif algorithm used exploration subsequently exploration phase spsa instance achieved highest value exploration phase selected. versions metamax algorithm tested. motivated fact spsa known converge global optimum exponentially fast satisﬁes restrictive conditions chose decays exponentially fast. control exploration suboptimal algorithm instances allowed time-varying function changes total number function calls evaluate algorithms far. thus round used algorithms ﬁxed number local search instances unif ee-unif thrasc) number instances simulations choice provided reasonably good performance problems analyzed. parameters spsa -dimensional case -dimensional case. performance search algorithms measured error deﬁned diﬀerence maximum value function best result obtained search algorithm given number steps. results multi-start strategies twodimensional test functions shown figure error curve averaged runs strategy steps observe cases versions metamax algorithm converge fastest. thrasc better unif luby seems fairly competitive two. exploreand-exploit-type algorithms similar performance dimensional function clearly better ‘non-exploiting’ base algorithms -dimensional function behavior somewhat pathological sense values performances best among algorithms increasing error actually increases respective base algorithms achieve smaller errors values random search seems option -dimensional function. similar results obtained dimensions pathological behavior explore-and-exploit algorithms start appear gradually starting -dimensional function pronounced dimensions onwards. limited experimental data obtained higher dimensions shows superiority metamax preserved high-dimensional problems well. reason pathological behavior explore-and-exploit strategies illustrated follows. assume spsa instances converging global optimum another converging suboptimal local optimum. assume ﬁrst steps optimal algorithm gives better result suboptimal algorithm takes reaches local maximum algorithms even further optimal algorithm beats suboptimal one. exploration stopped ﬁrst last regime explore-and-exploit algorithm choose ﬁrst optimal local search instance whose performance quite close global optimum exploitation phase exploration stopped middle regime suboptimal search instance selected exploitation whose performance even close global optimum. scenario error exploitation phase lower small increases higher values decrease error increasing assured optimal instance converges exploration phase past suboptimal local optima results selecting optimal local search instance exploitation. scenario error decrease fast figure average error multi-start strategies -dimensional -dimensional modiﬁed griewank function. conﬁdence intervals shown color corresponding curves. note intervals small. figure average error multi-start strategies tuning parameters multilayer perceptron vehicle data letter data conﬁdence intervals also shown color corresponding curves. initially increase decrease till converges quite similar observe figure right. pathological behavior becomes transparent many local search algorithms length exploitation phase scales number local search instances length exploration instance kept ﬁxed. analyzing experimental data shows complex versions scenario outlined occurred simulations main cause observed pathological behavior letter multilayer perceptron learning algorithm weka parameters tuned learning rate momentum range size hidden layer multilayer perceptron number epochs parameters spsa algorithm -dimensional griewank function rate correctly classiﬁed items test vehicle using multilayer perceptron varying values parameters shown figure highest rate similarly classiﬁcation rate letter shown figure highest rate error rates optimized multilayer perceptron data sets vehicle letter shown figure parameters learning algorithm tuned multi-start strategies above. error cases diﬀerence best classiﬁcation rate obtained best classiﬁcation rate obtained multi-start strategies given number steps. results shown averaged runs. observe metamax algorithm converged fastest average three strategies ﬁxed number algorithm instances nearly identical results luby slightly worse these random search slowest although performed nearly badly synthetic functions. reason random search relatively better performance could twofold large parts error surface oﬀer fairly small error error surface less smooth therefore spsa less successful using gradient information. explore-and-exploit variants performed well vehicle data initially performance worsened larger values this coupled observation figure right would suggest explore-and-exploit variants competitive small values despite asymptotic guarantees. summary metamax algorithm provided best performance tests usually requiring signiﬁcantly fewer steps optimum algorithms. e.g. letter data metamax algorithm found global optimum runs time steps. conclude metamax converged faster multi-start strategies investigated four test cases notable advantage diﬃcult surfaces induced classiﬁcation tasks. acusual choice squared euclidean distance case cording necessary conditions k-means algorithm alternates partitioning data according centers ﬁxed recomputing centers partitioning kept ﬁxed. easily seen cost cannot increase steps hence algorithm converges local minimum cost function. practice algorithm stops decrease cost function. however k-means algorithm often trapped local optimum whose value inﬂuenced initial centers. spsa restarting k-means diﬀerent initialization result ﬁnding global optimum. consider initialization techniques ﬁrst termed k-means chooses centers uniformly random data points; second k-means++ chooses initial center uniformly random data chooses centers data points probability proportional distance data point closest center already selected. k-means algorithm usually terminates relatively small number steps thus multi-start strategies bounded number instances would active local search algorithms therefore appear particularly attractive. however natural domain consider strategy starts instance previous ﬁnished. strategy referred subsequently serial. mentioned considerations test metamax variant algorithms applicable unbounded number instances. experiments spsa used note theoretical results indicate k-means converge exponential rate multi-start strategies serial metamax tested data cloud machine learning repository data employed arthur vassilvitskii well. number clusters ten. performance multi-start strategies deﬁned diﬀerence smallest cost function obtained strategy given number steps smallest cost seen experiments results averaged runs plotted figure initialization methods metamax strategy converges faster serial strategy. note data k-means++ clever initialization procedure yields faster convergence standard k-means uniform initialization consistent results presented arthur vassilvitskii note metamax algorithm practical modiﬁcation local search algorithm terminated chosen anymore. clearly improves performance algorithm chosen anymore improvement observed. contrast quadratic penalty suggested theorem plugging estimate theorem would logarithmic factor calls evaluate needed achieve performance search algorithm started attraction region optimum. finally perhaps main practical question concerning metamax family multistart algorithms decide them. rule thumb suﬃciently large performance diﬀerence average local search algorithm best one. clearly single local search produces acceptable result worth eﬀort several instances local search especially complicated schedule. many real problems often case relatively easy close optimum acceptable applications approaching optimum greater precision hard; latter importance metamax algorithm variants useful. last wonder computational costs algorithms. discussed before consider case evaluation target function expensive clearly case griewank function used demonstrate basic properties algorithm holds many optimization problems practice including experiments considered paper. problems function evaluation indeed expensive overhead introduced metamax algorithms depends number rounds. metamax algorithm upper convex hull points round; worst case take long calculations practice usually figure number algorithm instances metamax. average number instances shown benchmarks griewank function parameter tuning multilayer perceptron clustering k-means k-means++. maximum minimum number instances runs benchmarks much cheaper upper convex hull determined point corresponds actually best estimate point corresponds least used algorithm requires computations even less special ordering tricks introduced. since target function evaluated least twice round average computational overhead needed evaluation worst case practically reduced even less. similar considerations hold overhead call closer even less practice. examples considered amount overhead negligible relative computational resources needed evaluate single point. paper provided multi-start strategies local search algorithms. strategies continuously estimate potential performance algorithm instance optimistic supposing convergence rate local search algorithms unknown constant every phase resources allocated instances could converge optimum particular range constant. three versions algorithm presented able follow performance best ﬁxed number local search algorithm instances that gradually increasing number local search algorithms achieve global consistency. theoretical analysis asymptotic behavior algorithms also given. speciﬁcally mild conditions function maximized best algorithm metamax preserves performance local search algorithm original function class quadratic increase number times target function needs evaluated simulations demonstrate algorithms work quite well practice. theoretical bound suggests target function evaluated quadratic factor times achieve performance search algorithm started attraction region optimum experiments found logarithmic penalty. clear whether diﬀerence result slightly conservative analysis choice experimental settings. also ﬁnite sample analysis algorithm interest experiments indicate metamax algorithm provides good performance even relatively small number steps taken local search algorithms sense provides speed-up compared approaches even number times target function evaluated relatively small. finally future work needed clarify connection convergence rate optimal algorithms function used exploration. authors would like thank anonymous referees numerous insightful constructive comments. research supported part mobile innovation center hungary national development agency hungary research technological innovation fund pascal network excellence parts paper presented ecml", "year": 2014}