{"title": "Improved Techniques for Training GANs", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.", "text": "present variety architectural features training procedures apply generative adversarial networks framework. focus applications gans semi-supervised learning generation images humans visually realistic. unlike work generative models primary goal train model assigns high likelihood test data require model able learn well without using labels. using techniques achieve state-of-the-art results semi-supervised classiﬁcation mnist cifar- svhn. generated images high quality conﬁrmed visual turing test model generates mnist samples humans cannot distinguish real data cifar- samples yield human error rate also present imagenet samples unprecedented resolution show methods enable model learn recognizable features imagenet classes. generative adversarial networks class methods learning generative models based game theory. goal gans train generator network produces samples data distribution pdata transforming vectors noise training signal provided discriminator network trained distinguish samples generator distribution pmodel real data. generator network turn trained fool discriminator accepting outputs real. recent applications gans shown produce excellent samples however training gans requires ﬁnding nash equilibrium non-convex game continuous highdimensional parameters. gans typically trained using gradient descent techniques designed value cost function rather nash equilibrium game. used seek nash equilibrium algorithms fail converge work introduce several techniques intended encourage convergence gans game. techniques motivated heuristic understanding non-convergence problem. lead improved semi-supervised learning peformance improved sample generation. hope form basis future work providing formal guarantees convergence. several recent papers focus improving stability training resulting perceptual quality samples build techniques work. instance dcgan architectural innovations proposed radford discussed below. proposed techniques feature matching discussed sec. similar spirit approaches maximum mean discrepancy train generator networks another proposed techniques minibatch features based part ideas used batch normalization proposed virtual batch normalization direct extension batch normalization. primary goals work improve effectiveness generative adversarial networks semi-supervised learning like many deep generative models gans previously applied semi-supervised learning work seen continuation reﬁnement effort. training gans consists ﬁnding nash equilibrium two-player non-cooperative game. player wishes minimize cost function discriminator generator. nash equilibirum point minimum respect minimum respect unfortunately ﬁnding nash equilibria difﬁcult problem. algorithms exist specialized cases aware feasible apply game cost functions non-convex parameters continuous parameter space extremely high-dimensional. idea nash equilibrium occurs player minimal cost seems intuitively motivate idea using traditional gradient-based minimization techniques minimize player’s cost simultaneously. unfortunately modiﬁcation reduces increase modiﬁcation reduces increase gradient descent thus fails converge many games. example player minimizes respect another player minimizes respect gradient descent enters stable orbit rather converging desired equilibrium point previous approaches training thus applied gradient descent player’s cost simultaneously despite lack guarantee procedure converge. introduce following techniques heuristically motivated encourage convergence feature matching feature matching addresses instability gans specifying objective generator prevents overtraining current discriminator. instead directly maximizing output discriminator objective requires generator generate data matches statistics real data discriminator specify statistics think worth matching. speciﬁcally train generator match expected value features intermediate layer discriminator. natural choice statistics generator match since training discriminator features discriminative real data versus data generated current model. letting denote activations intermediate layer discriminator objective generator deﬁned ||ex∼pdataf ez∼pzf discriminator hence trained usual way. regular training objective ﬁxed point exactly matches distribution training data. guarantee reaching ﬁxed point practice empirical results indicate feature matching indeed effective situations regular becomes unstable. minibatch discrimination main failure modes generator collapse parameter setting always emits point. collapse single mode imminent gradient discriminator point similar directions many similar points. discriminator processes example independently coordination gradients thus mechanism tell outputs generator become dissimilar other. instead outputs race toward single point discriminator currently believes highly realistic. collapse occurred discriminator learns single point comes generator gradient descent unable separate identical outputs. gradients discriminator push single point produced generator around space forever algorithm cannot converge distribution correct amount entropy. obvious strategy avoid type failure allow discriminator look multiple data examples combination perform call minibatch discrimination. concept minibatch discrimination quite general discriminator model looks multiple examples combination rather isolation could potentially help avoid collapse generator. fact successful application batch normalization discriminator radford well explained perspective. however restricted experiments models explicitly identify generator samples particularly close together. successful speciﬁcation modelling closeness examples minibatch follows denote vector features input produced intermediate layer discriminator. multiply vector tensor ra×b×c results matrix rb×c. compute l-distance rows resulting matrix across samples apply negative exponential output minibatch layer sample deﬁned cb’s samples next concatenate output minibatch layer intermediate features input feed result next layer discriminator. compute minibatch features separately samples generator training data. before discriminator still required output single number example indicating likely come training data task discriminator thus effectively still classify single examples real data generated data able examples minibatch side information. minibatch discrimination allows generate visually appealing samples quickly regard superior feature matching interestingly however feature matching found work much better goal obtain strong classiﬁer using approach semi-supervised learning described section historical averaging applying technique modify player’s cost include term value parameters past time historical average parameters updated online fashion learning rule scales well long time series. approach loosely inspired ﬁctitious play algorithm equilibria kinds games. found approach able equilibria low-dimensional continuous non-convex games minimax game player controlling player controlling value function otherwise. one-sided label smoothing label smoothing technique recently independently re-discovered szegedy replaces targets classiﬁer smoothed values like recently shown reduce vulnerability neural networks adversarial examples replacing positive classiﬁcation targets negative targets optimal discriminator becomes αpdata+βpmodel pdata+pmodel presence pmodel numerator problematic because areas pdata approximately zero pmodel large erroneous samples pmodel incentive move nearer data. therefore smooth positive labels leaving negative labels virtual batch normalization batch normalization greatly improves optimization neural networks shown highly effective dcgans however causes output neural network input example highly dependent several inputs minibatch. avoid problem introduce virtual batch normalization example normalized based statistics collected reference batch examples chosen ﬁxed start training itself. reference batch normalized using statistics. computationally expensive requires running forward propagation minibatches data generator network. generative adversarial networks lack objective function makes difﬁcult compare performance different models. intuitive metric performance obtained human annotators judge visual quality samples automate process using amazon mechanical turk using interface ﬁgure fig. annotators distinguish generated data real data. resulting quality assessments models described section downside using human annotators metric varies depending setup task motivation annotators. also results change drastically give annotators feedback mistakes learning feedback annotators better able point ﬂaws generated images giving pessimistic quality assessment. left column fig. presents screen annotation process right column shows inform annotators mistakes. alternative human annotators propose automatic method evaluate samples correlate well human evaluation apply inception model every generated image conditional label distribution images contain meaningful objects conditional label distribution entropy. moreover expect combining requirements metric propose exp||p)) exponentiate results values easier compare. inception score closely related objective used training generative models catgan although less success using objective training good metric evaluation correlates pretrained inception model http//download.tensorflow.org/models/ image/imagenet/inception---.tgz. code compute inception score model made available time publication. consider standard classiﬁer classifying data point possible classes. model takes input outputs k-dimensional vector logits turned class probabilities applying softmax pmodel exp. supervised learning model trained minimizing cross-entropy observed labels model predictive distribution pmodel. semi-supervised learning standard classiﬁer simply adding samples generator data labeling generated class correspondingly increasing dimension classiﬁer output pmodel supply probability fake corresponding original framework. also learn unlabeled data long know corresponds classes real data maximizing pmodel. assuming half data consists real data half generated loss function training classiﬁer becomes decomposed total cross-entropy loss standard supervised loss function lsupervised unsupervised loss lunsupervised fact standard game-value becomes evident substitute pmodel expression exp] cp∀j<k+ exp] undetermined scaling function unsupervised loss thus consistent supervised loss sense sutskever hope better estimate optimal solution data minimizing loss functions jointly. practice lunsupervised help trivial minimize classiﬁer thus need train approximate data distribution. training minimize game-value using discriminator deﬁned classiﬁer. approach introduces interaction classiﬁer fully understand empirically optimizing using feature matching works well semi-supervised learning training using minibatch discrimination work all. present empirical results using approach; developing full theoretical understanding interaction using approach left future work. finally note classiﬁer outputs over-parameterized subtracting general function output logit i.e. setting change output softmax. means equivalently case lsupervised becomes standard supervised loss function original classiﬁer classes discriminator given besides achieving state-of-the-art results semi-supervised learning approach described also surprising effect improving quality generated images judged human annotators. reason appears human visual system strongly attuned image statistics help infer class object image represents presumably less sensitive local statistics less important interpretation image. supported high correlation quality reported human annotators inception score developed section explicitly constructed measure objectness generated image. discriminator classify object shown image bias develop internal representation puts emphasis features humans emphasize. effect understood method transfer learning could potentially applied much broadly. leave exploration possibility future work. performed semi-supervised experiments mnist cifar- svhn sample generation experiments mnist cifar- svhn imagenet. provide code reproduce majority experiments. mnist mnist dataset contains labeled images digits. perform semi-supervised training small randomly picked fraction these considering setups labeled examples. results averaged random subsets labeled data chosen balanced number examples class. remaining training images provided without labels. networks hidden layers each. weight normalization gaussian noise output layer discriminator. table summarizes results. figure samples generated model during semi-supervised training. samples clearly distinguished images coming mnist dataset. samples generated minibatch discrimination. samples completely indistinguishable dataset images. samples generated generator semi-supervised learning using feature matching look visually appealing using minibatch discrimination instead improve visual quality. mturk annotators able distinguish samples cases would obtained random guessing. similarly researchers institution able artifacts would allow distinguish samples. however semi-supervised learning minibatch discrimination produce good classiﬁer feature matching. cifar- cifar- small well studied dataset natural images. data study semi-supervised learning well examine visual quality samples achieved. discriminator layer deep convolutional network dropout weight normalization. generator layer deep batch normalization. table summarizes results semi-supervised learning task. presented real fake data generated best cifar- model mturk users correctly categorized images correctly. however mturk users sufﬁciently familiar cifar- images sufﬁciently motivated; able categorize images accuracy. validated inception score described observing mturk accuracy drops data ﬁltered using samples according inception score. performed series ablation experiments demonstrate proposed techniques improve inception score presented table also present images ablation experiments—in opinion inception score correlates well subjective judgment image quality. samples dataset achieve highest value. models even partially collapse relatively scores. caution inception score used rough guide evaluate models trained independent criterion; directly optimizing inception score lead generation adversarial examples table table inception scores samples generated various models images. score highly correlates human judgment best score achieved natural images. models generate collapsed samples relatively score. metric allows avoid relying human evaluations. methods includes techniques described work except feature matching historical averaging. remaining experiments ablation experiments showing techniques effective. -vbn+bn replaces generator dcgans. causes small decrease sample quality cifar. important imagenet. -l+ha removes labels training process adds historical averaging compensate. makes possible still generate recognizable objects. without sample quality considerably reduced removes label smoothing incurs noticeable drop performance relative methods. -mbf removes minibatch features incurs large drop performance greater even drop resulting removing labels. adding cannot prevent problem. tested techniques dataset unprecedented scale images ilsvrc dataset categories. knowledge previous publication applied generative model dataset large resolution large number object classes. large number object classes particularly challenging gans tendency underestimate entropy distribution. extensively modiﬁed publicly available implementation dcgans using tensorflow achieve high performance using multi-gpu implementation. dcgans without modiﬁcation learn basic image statistics generate contiguous shapes somewhat natural color texture learn objects. using techniques described paper gans learn generate objects resemble animals incorrect anatomy. results shown fig. figure samples generated imagenet dataset. samples generated dcgan. samples generated using techniques proposed work. techniques enable gans learn recognizable features animals eyes noses features correctly combined form animal realistic anatomical structure. generative adversarial networks promising class generative models held back unstable training lack proper evaluation metric. work presents partial solutions problems. propose several techniques stabilize training allow train models previously untrainable. moreover proposed evaluation metric gives basis comparing quality models. apply techniques problem semi-supervised learning achieving state-of-the-art results number different data sets computer vision. contributions made work practical nature; hope develop rigorous theoretical understanding future work.", "year": 2016}