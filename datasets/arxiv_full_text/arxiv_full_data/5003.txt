{"title": "TensorFlow Agents: Efficient Batched Reinforcement Learning in  TensorFlow", "tag": ["cs.LG", "cs.AI"], "abstract": "We introduce TensorFlow Agents, an efficient infrastructure paradigm for building parallel reinforcement learning algorithms in TensorFlow. We simulate multiple environments in parallel, and group them to perform the neural network computation on a batch rather than individual observations. This allows the TensorFlow execution engine to parallelize computation, without the need for manual synchronization. Environments are stepped in separate Python processes to progress them in parallel without interference of the global interpreter lock. As part of this project, we introduce BatchPPO, an efficient implementation of the proximal policy optimization algorithm. By open sourcing TensorFlow Agents, we hope to provide a flexible starting point for future projects that accelerates future research in the field.", "text": "introduce tensorflow agents eﬃcient infrastructure paradigm building parallel reinforcement learning algorithms tensorflow. simulate multiple environments parallel group perform neural network computation batch rather individual observations. allows tensorflow execution engine parallelize computation without need manual synchronization. environments stepped separate python processes progress parallel without interference global interpreter lock. part project introduce batchppo eﬃcient implementation proximal policy optimization algorithm. open sourcing tensorflow agents hope provide ﬂexible starting point future projects accelerates future research ﬁeld. introduce uniﬁed interface reinforcement learning agents accompanying infrastructure integrating tensorflow allows eﬃciently develop algorithms. deﬁning standard algorithm interface reinforcement learning allows reuse common infrastructure algorithms change algorithm easily environment. accelerate future reinforcement learning research releasing project public. important goals project make easy implement algorithms train fast. many reinforcement learning methods spend time interacting environment compared time required compute apply gradient updates. bottlenecks simulation algorithm environment forward pass neural network progressing environment. address limitations simulate multiple environments parallel extend openai interface batched environments stepped parallel. separate processes every environment environments implemented python freely without limited global interpreter lock. integrating batched environment tensorflow graph combine reinforcement learning algorithm leaving single operation call within training loop. release tensorflow agents project open source community together batchppo optimized implementation recently introduced proximal policy gradient algorithm simple powerful reinforcement learning baseline showed impressive results locomotion tasks believe releasing project speed progress reinforcement learning research. present uniﬁed agent interface together eﬃcient tensorflow framework parallel reinforcement learning implementation recently introduced proximal policy gradient algorithm openai brockman introduced standardized interface environments since gained wide adoption. extend work providing interface allows combine multiple environments step together. moreover introduce common interface learning algorithms implemented tensorflow. allow make environments algorithms easily exchangeable part research project. duan released rllab framework implements several reinforcement learning algorithms provides common infrastructure training evaluation. numerical computations mainly implemented numpy theano. work also provides tools training evaluation optimized parallel simulation resulting signiﬁcantly accelerated training. openai baselines aims provide high-quality implementations reinforcement learning algorithms. among algorithms includes implementation using tensorflow neural network computation. implementation relies python algorithm logic accelerated using mpipy tensorflow agents similar goal provides reusable infrastructure future vectorized implementations reinforcement learning algorithms. schulman provides implementation written numpy keras. addition gradient-based variant described paper includes version uses lbfgs updating neural networks. project served reference algorithmic part batchppo implementation. build upon work parallelizing experience collection providing tensorflow infrastructure reusable research projects. observable markov decision process deﬁnes stochastic sequence states observations actions rewards starting initial state draw observations actions policy rewards reward function following state transition funcestimate expectation collecting multiple episodes environment. collected episodes multiple gradient steps need correct changing action distribution updated policy data collecting policy using importance sampling stochasticity transition function reward function policy would need collect many episodes suﬃcient estimate expectation. reduce variance estimate without introducing bias subtracting state values γirt+i] returns eﬀectively removes stochastic inﬂuences following time steps estimate practice collect small number episodes perform several updates full-batch gradient descent discard collected data repeat process. schulman also proposed alternative objective clips importance sampling ratio found objective using penalty work well practice explore option. many cases bottleneck reinforcement learning algorithms collecting episodes environment. time consuming computations process forward pass neural network stepping environments. fortunately parallelize processes using multiple environments time. previous implementations asynchronous workers fully utilize system simpler architecture saving communication overhead. implementation completely deﬁned within tensorflow graph. first parallelize forward pass neural network vectorizing agent computation produce batch actions batch observations. similar facebook’s agents also process multiple observations step. using batch size inference allows leverage internal thread pool tensorflow session hardware accelerators gpus tpus. second simulate environments parallel. relying tensorflow session suﬃcient purpose since many environments implemented python restricted global interpreter lock instead spawn separate process environment available cores step environments parallel. communication main process environment processes introduces constant overhead alleviated ability many environments parallel. parallelize forward passes neural network environment stepping allow environments step faster others sync. makes algorithm implementations conceptually simple eﬃcient. training cpus implementation fully utilizes available resources. using gpus system switches full load cpus gpus. process could potentially parallelized introducing action environments step together algorithm. introduce interface infrastructure algorithms eﬃciently interacting parallel environments. ﬁrst describe algorithm interface followed batched environment interface simulation operation combining two. tensorflow agents algorithm deﬁnes inference learning computation batch agents. implements following functions deﬁne computation graph algorithm operations additionally return string tensors containing tensorflow summaries. infrastructure combines summaries writes jointly. tensors empty strings summaries written current step. provide batchppo implementation deﬁned ppo.ppoalgorithm example implementation interface. constructor expects in-graph batch environment tensor global step tensor indicating whether compute summaries current time step conﬁguration object. neural networks used batchppo implemented tf.contrib.rnn .rnncell classes speciﬁed conﬁguration object. custom neural network structures deﬁned implementing tf.contrib.rnn.rnncell interface returning tuple containing tensors action mean action standard deviation state value. chose interface integrates nicely existing tensorflow ecosystem providing freedom model optimized algorithm. provide example implementations feed forward policy recurrent policy. eﬃciently simulate environments parallel provide agents.tools. wrappers.externalprocess environment wrapper. wrapper constructs openai environment inside external process. calls step reset well attribute accesses forwarded environment wait result. environment wrapper compatible existing python environments comply openai interface long rely shared global state. agents.tools.batchenv class extends openai interface vectorized environments. combines multiple openai environments step accepting batch actions returning batches observations rewards done ﬂags info objects. individual environments live external processes stepped parallel. observation action spaces combined environments must match accessed without modiﬁcation. addition batch environment allows access individual environments index. alternatively custom environments implemented vectorized format directly. integrate environment algorithm tensorflow graph. allows prevent copying data perform simulation small number tensorflow session runs. class agents.tools.ingraphbatchenv integrates batch environment tensorflow graph makes step reset functions accessible operations. current batch observations last actions rewards done ﬂags stored variables made figure batchppo episode returns environment steps. blue line indicates performance using mean action line indicated performance sampling action distribution. results better published results using algorithm tensorflow operation tools.simulate fuses together in-graph batch environment algorithm. optional reset tensors indicate whether algorithm compute tensorflow summaries current step whether environments reset agent indices start episodes. used training protocol switches training evaluation phases discontinue ongoing episodes. operation provides single tensorflow session call inside training loop training loop resembles case supervised learning. demonstrate performance infrastructure implementation train batchppo algorithm control tasks mujoco domain. agent uses neural networks compute mean action current observation provide estimate state value. also experimented recurrent neural networks generally observed slower learning similar ﬁnal performance. standard deviation action elements learned independent parameter vector. actions sampled gaussian distribution parameterized predicted mean standard deviation used deﬁne diagonal elements covariance matrix. elements constitute action vector therefore independent. evaluation policy’s mean action rather sampling distribution. used hyper parameter conﬁguration considered tasks. specifically collect batches episodes update perform gradient steps policy value networks using adam optimizers ﬁxed learning rates respectively. networks layers figure batchppo episode returns training time hours using cores. blue line indicates performance using mean action line indicated performance sampling action distribution. implementation quickly solve challenging locomotion tasks single machine. units relu non-linearities apply tanh action mean range. following schulman heess streaming statistics observations rewards normalize them. adopt additional cut-oﬀ penalty enable overshooting desired change divergence factor results batchppo implementation shown figures environment steps hours training time using cores respectively. blue line shows evaluation performance acting using mean action line shows training performance sampling policy’s action distribution. shaded areas indicate percentile three seeds. observe reliably high performance better published results existing implementations introduced infrastructure paradigm implementation using tensorflow parallel reinforcement learning algorithms including batchppo eﬃcient implementation proximal policy optimization algorithm. reasoning design choices result simple extendable performant implementation. hope providing infrastructure public accelerate research reinforcement learning provides powerful framework algorithm implementations. future custom environments could implemented vectorized possibly within tensorflow leverage parallel hardware without introducing inter-process communication overhead. thank nicolas heess josh merel deepmind insightful discussions. furthermore thank tensorflow team community developing tensorflow thus making project possible.", "year": 2017}