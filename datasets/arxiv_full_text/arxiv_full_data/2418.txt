{"title": "On the Number of Samples Needed to Learn the Correct Structure of a  Bayesian Network", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Bayesian Networks (BNs) are useful tools giving a natural and compact representation of joint probability distributions. In many applications one needs to learn a Bayesian Network (BN) from data. In this context, it is important to understand the number of samples needed in order to guarantee a successful learning. Previous work have studied BNs sample complexity, yet it mainly focused on the requirement that the learned distribution will be close to the original distribution which generated the data. In this work, we study a different aspect of the learning, namely the number of samples needed in order to learn the correct structure of the network. We give both asymptotic results, valid in the large sample limit, and experimental results, demonstrating the learning behavior for feasible sample sizes. We show that structure learning is a more difficult task, compared to approximating the correct distribution, in the sense that it requires a much larger number of samples, regardless of the computational power available for the learner.", "text": "bayesian networks useful tools giving natural compact representation joint probability distributions. many applications needs learn bayesian network data. context important understand number samples needed order guarantee successful learning. previous works studied sample complexity mainly focused requirement learned distribution close original distribution generated data. work study diﬀerent aspect learning task namely number samples needed order learn correct structure network. give asymptotic results probability learning wrong structure valid large sample limit experimental results demonstrating learning behavior feasible sample sizes. bayesian networks generative models well suited representing knowledge uncertainty. compact representation modularity intuitive causal interpretation made popular today used various ﬁelds expert systems economics computational biology. speciﬁed components qualitative part represents conditional dependencies random variables quantitative part representing exact joint probability distribution related r.v.s. network. often building according expert knowlcases needs learn edge. data i.e. sample realizations important assess sample size needs order learn approximates true given quality. subject sample complexity drawn attention past decade. framework used known structure without hidden variables various authors discrete unknown structure. works used kullblack-leibler distance between original learned model’s distributions measure approximation quality. diﬀerent criterion presented measured performance learned model answering queries study concentrated diﬀerent aspect learning task number samples needed order learn correct network structure. intuition practice indicate learning correct structure much harder task approximating original distribution hence requires larger number samples. relation number causal errors sample size studied using computer simulations speciﬁc learning algorithms. derive rigorous upper lower-bounds error probability well experimental results when refer correct structure actually mean correct equivalence class speciﬁc structure within equivalence class cannot distinguished based observational data solely. tion specifying conditional probabilities θi|pag parents graph parents excluding denoted values denoted pag. keeping parent’s values setting denoted probability distributions values -dimensional simplex denoted joint probability distribution associated denoted satisﬁes distribution denote conditional independence relations form hold disjoint sets r.v.s. graph denote independence assertions implied called i-map called p-map denote distributions i-map assume samples generated probability distribution p-map. generating samples denoted corresponding distribution denoted pb∗. unique p-map graph-equivalent structures. moreover |g∗| i-map refer ’correct’ structure purpose recover data. simplicity concentrate boolean r.v.s techniques applied discretenode bns. give bounds error probability i.e. probability learning wrong structure bayesian-information-criteria speciﬁc case minimum-description-length score. results easily modiﬁed give asymptotic behavior scores penalty terms. obtain diﬀerent bounds structures cannot represent true distribution structures represent true distribution minimal number parameters tasks respectively large moderate deviations results. results correct large sample limit also give experimental results moderate number samples. address problem learning exact correct structure consider case learning structure approximates original one. relaxation requirement leads lower sample complexity. hand assume learner computationally unbounded learns scoring possible models exhaustively selecting best scoring one. obviously number samples needed real-life algorithms structure learning higher. next section give basic deﬁnitions results score relative entropy used later. section give asymptotic sample complexity bounds learning correct structure. section present computer simulations demonstrating behavior error probability moderate number samples. conclusions future directions. binary random variables joint probability distribution general uppercase denote random variables lowercase denote realizations. sometimes omit latter example formally bayesian network pair n-vertex directed acyclic graph called structure represents dependencies xi’s variable independent non-descenders given parents {θi}n parametrizastrategy cope overﬁtting problem based minimal description length principle score ’penalizes’ complex models thus giving trade-oﬀ data-ﬁtting model complexity. usually takes form assumptions score asymptotically consistent particular interest choice since case score known asymptotically equivalent bayesian score also termed bayesian information criterion working relative entropy distance measure often possesses technical diﬃculties since satisfy requirements norm. prove results relative entropy analogous symmetry triangle inequality properties norm. results rely fact reference distribution bounded away zero involve constants depend proximity zero. graph probability denote always write uniquely unit vector. moreover since probability distributions also satisﬁes taking taylor expansion relative entropy gives fully-connected consistent ordering denoted graphs obtained fully-connected dags removing edge denoted notation used dags. denote number realizations learning set. samples denoted value r.v. i-th sample. assume samples i.i.d. pb∗. distribution denote joint product measure i.i.d. r.v.s. distributed according thus example sample size estimate simply counting number occurrences value sample. resulting distribution called sample distribution given log-likelihood data entropy conditional entropy variable respect similarly denotes mutual information variables. denote sample distribution assuming correct structure probability distribution associated best scoring structure score called asymptotically consistent ’correct’ model attain highest score probability approaching one. here ’correct’ model refers graph minimal number parameters i-map pb∗. easily shown log-likelihood consistent score thus useful comparing structures. adding edges graph always improves likelihood makes complete graph highest scoring regardless true generating data. common diﬀerent graph structures curved exponential families thought riemannian manifolds diﬀerent dimensions subsets simplex ’competing’ data casted simply distance sample probability manifold ’pays’ penalty proportional dimension. analyzing models together seems complicated approach studying error probability model time i.e. following divide models disjoint subsets graphs i-maps graphs i-maps higher dimension |g∗|. section devoted problem identifying correct network structure data. problem also known statistics literature ’model selection’. exponential families problem studied asymptotic consistency established asymptotic results error-probability function number samples obtained. shown directed graphical models hidden variables fact curved exponential families thus haughton’s results applicable them. nevertheless explicit constants appearing haughton’s asymptotical analysis proof ﬁrst inequality follows directly proposition second inequality note therefore event happens according sanov theorem probability completes proof. apply lemma explicit lower-bound error exponent theorem. unfortunately theorem give explicit upperbound error exponent. rest section devoted ﬁnding bound. following derive upper-bound steps. first assume observed distribution ideal pb∗. although assumption unrealistic many times even feasible helps understand trade-oﬀ data-ﬁtting model complexity score. next study eﬀect sampling noise bound. using concentration measure arguments show high probability close allowing upper-bound presence sampling noise. deriving results bring useful relation pmaps. deﬁnition p-map strictly positive following stronger relation holds proposition p-map positivity means edge ’redundant’ respect distribution magnitude thought measure much information captured compared lower-dimensional models. intuitively expect higher value easier separate lower-dimensional models based data sampled bounds obtained next section show indeed dependence well dependence purpose achieve tightest bounds possible merely demonstrate dependence convergence rate parameters. note weaker form lemma still valid noisy case assuming require maxg|g|≤|g∗| method proof showing likelihood diﬀerence noisy case close high probability ideal version shown order show proximity series concentration lemmas proof recall log-likelihood written apply lemma used along union bound bound |lln similarly taking lemma used bound combining bounds |lln using triangle inequality gives proof ﬁrst part simply possible realizations lemma apply union bound noting second part chain rule entropy apply ﬁrst part sets apply union bound desired result. previous section seen graph i-map ’bounded away’ simplex used relate error probability large deviation event. graph i-map event choosing overparameterized i-map moderate deviation event next theorem characterizes asymptotic probability theorem assume moreover then note nested models asymptotic error depends speciﬁc parametrization determining pb∗. computer simulations performed dependence parametrization also non-nested models. substantially diﬀerent under-ﬁtting error shown previous section seen error exponents depends parametrization. constant proportional diﬀerence dimensions |g|−|g∗| over-parameterized faster decay error probability. previous section noted qualitative diﬀerence under-ﬁtting errors. under-ﬁtting error decays exponentially fast over-ﬁtting probability decays slower power show diﬀerence relevant mainly large limit small might expect intuitively situation opposite under-ﬁtting likely. examined errors learning process small four r.v.s. diﬀerent dags four nodes divided equivalence classes. chose graph edge parametrization equivalence classes i-maps under-ﬁt also took speciﬁc structures over-parameterized i-map i-map pb∗. since errors become rare events large importance-sampling methods order estimate probability. rather generating samples directly used distributions events likely applied appropriate correction needed estimate probability given pb∗. error probabilities presented ﬁgure error probability dominated under-ﬁtting structure larger over-ﬁtting probability higher. qualitatively similar results also obtained networks started choices competing wrong graphs. figure log-error probabilities learning wrong graphs shown functions sample size over-parameterized i-map i-map pb∗. used importance sampling averaged diﬀerent distributions samples drawn each. log-error exhibits excellent straight line. error ﬁtted better straight line loglog plot accordance power-law asymptotic behavior.", "year": 2012}