{"title": "Modelling, Visualising and Summarising Documents with a Single  Convolutional Neural Network", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Capturing the compositional process which maps the meaning of words to that of documents is a central challenge for researchers in Natural Language Processing and Information Retrieval. We introduce a model that is able to represent the meaning of documents by embedding them in a low dimensional vector space, while preserving distinctions of word and sentence order crucial for capturing nuanced semantics. Our model is based on an extended Dynamic Convolution Neural Network, which learns convolution filters at both the sentence and document level, hierarchically learning to capture and compose low level lexical features into high level semantic concepts. We demonstrate the effectiveness of this model on a range of document modelling tasks, achieving strong results with no feature engineering and with a more compact model. Inspired by recent advances in visualising deep convolution networks for computer vision, we present a novel visualisation technique for our document networks which not only provides insight into their learning process, but also can be interpreted to produce a compelling automatic summarisation system for texts.", "text": "capturing compositional process maps meaning words documents central challenge researchers natural language processing information retrieval. introduce model able represent meaning documents embedding dimensional vector space preserving distinctions word sentence order crucial capturing nuanced semantics. model based extended dynamic convolution neural network learns convolution ﬁlters sentence document level hierarchically learning capture compose level lexical features high level semantic concepts. demonstrate effectiveness model range document modelling tasks achieving strong results feature engineering compact model. inspired recent advances visualising deep convolution networks computer vision present novel visualisation technique document networks provides insight learning process also interpreted produce compelling automatic summarisation system texts. encoding symbolic concepts distributed representations dream excited researchers decades recent years idea re-emerged successes neural networks language modelling machine translation natural language processing tasks chunking named entity recognition particularly fruitful vein recent research tackled compositional models vector space semantics. algebraic approaches proved popular simplicity arguably approaches based deep neural networks generated recent interest. exempliﬁed work recursive neural networks used great effect embedding sentences tasks sentiment analysis work fusing neural networks based upon fully connected networks either entirely feed forward manner including recurrent connections notable exception model kalchbrenner uses convolutional neural network build continuous distributed representations sentences. fundamental building block nearly applications neural networks creation continuous representations words. since text fundamentally symbolic neural networks operate continuous inputs necessary create mapping symbolic representation text continuous space networks operate. creation word embeddings received much attention several excellent methods exist creating mappings. addition providing suitable representation neural networks word embeddings shown capture many semantic relationships concepts represent. word embeddings learned neural networks also serve excellent general purpose representation words used non-neural models. paper show convnets used build distributed representations documents. model compositional; combines word embeddings sentence embeddings combines sentence embeddings document embeddings. combinations every level convnets inspired convolution networks seen great success computer vision. since model based convolutions able preserve ordering information words sentence sentences document. information lost bag-of-words n-gram models. going beyond classiﬁcation also show visualisation techniques developed computer vision literature understanding activation patterns convnets applied directly convolutional document model. visualisations give direct insights models learn. furthermore also used identify important sections sentence document. novel application show visualisation technique simonyan automatically generate summaries movie reviews. model divided levels sentence level document level implemented using convnets. sentence level convnet transform embeddings words sentence embedding entire sentence. document level another convnet transform sentence embeddings ﬁrst level single embedding vector represents entire document. model trained feeding document embeddings second level model softmax classiﬁer convnets levels trained jointly backpropogation entire model. sentence level weights convnets process different sentences tied every sentence document embedding produced convnet. levels model work different levels abstraction ﬁrst level operates words sentence second level operates sentences document. transformations level performed convnets; convnet contains layers convolution pooling tanh transformations. architecture model forces information pass intermediate sentence based representation. architecture inspired gulechere bengio show learning appropriate intermediate representations helps generalisation also hinton show forcing information pass carefully chosen bottlenecks possible control types intermediate representations learned. levels model modiﬁed version dynamic convolutional neural network kalchbrenner model similar convolutional networks used computer vision cascade convolution pooling tanh transformations adapted text modelling. detailed schematic sentence level model shown figure overview full model shown figure following sections describe convolutional layers operate within level model. input level model embedding matrix. sentence level columns matrix correspond embeddings words sentence processed document level columns correspond sentence embeddings produced sentence level model. figure word embeddings concatenated columns form sentence matrix. convnet applies cascade convolution pooling nonlinearity operations transform projected sentence matrix embedding sentence. sentence embeddings concatenated columns form document matrix. document model applies cascade convolution pooling nonlinearity operations form embedding whole document softmax classiﬁer. sentence level embedding matrix built concatenating embeddings word columns matrix. words drawn ﬁxed vocabulary represent using matrix word embeddings rd×|v column matrix dimensional vector gives embedding single word vocabulary. word embedding vectors parameters model optimised using backpropogation. sentence level model produces embedding vector sentence document. input document level obtained assembling sentence embeddings document matrix word embeddings assembled sentence matrix sentence level. convolution convolutional layer contains ﬁlter bank rd×wf×nf refer width number feature maps respectively. ﬁrst dimension feature rd×wf equal number dimensions embeddings generated layer below. convolution operation model dimensional. align ﬁrst axis feature embedding axis convolve along rows. sentence level corresponds convolving across words document level corresponds convolving across sentences. figure left comparison convolutions model model kalchbrenner using feature maps model feature looks every dimension layer generates single value position along sentence matrix model kalchbrenner feature generates output layer above feature maps convolved embeddings dimension generate four outputs. right schematic full model. layer convolution pooling shown level part model made deeper. feature generates numbers value obtained applying feature different location along sentence matrix. outputs different feature maps stacked form matrix latent representations input next layer. cases wide convolutions order weights feature maps reach every word/sentence including ones edges. unlike dcnn treat embedding dimensions channels. dcnn dimension feature generates hidden representation i.e. feature generates representation size d×|s| model feature generates representation size ×|s| approach also obviates need folding operation appears dcnn. difference illustrated figure change also means model substantially fewer parameters dcnn since output convolution layer smaller factor approach corresponds typical setup computer vision feature small spatial domain spans channels input image since work text spatial dimension different embedding dimensions correspond channels. since different sentences documents different lengths embedding matrices width. issue convolutional layers since convolutions handle inputs arbitrary width problematic input fully connected layer want generate sentence embeddings another model expects ﬁxed size inputs. solution k-max pooling applied embedding matrix separately. apply k-max pooling single keep largest values along discard rest. since ﬁxed parameter always generates ﬁxed size output example applying -max pooling yields procedure also illustrated graphically figure necessary impose ﬁxed length representations next layer convolution layer; however pooling desirable decrease size representation proved effective computer vision. compromise proposed kalchbrenner called dynamic k-max pooling fraction length input sentence. pooling operation discards half inputs instead all-but-k them help information long sentences documents propagate model. table left number test errors twitter sentiment dataset. ﬁrst block three entries second block kalchbrenner right error rates imdb movie review data set. ﬁrst block maas second dahl third wang manning fourth mikolov learning single model capable solving multiple tasks holy grails ﬁeld machine learning. convnet approach strongly motivated vision. section demonstrate learn single convnet model visualise saliency words sentences documents classify sentences documents summarise documents. preceding section argued convnet model substantially fewer parameters convnet model kalchbrenner parameter parsimony important feature scaling models embedding mobile devices. natural paid price classiﬁcation performance attain feature. ﬁrst experiment closely reproduces tweet sentiment classiﬁcation setting kalchbrenner shows models achieve comparable results. training contains million tweets weak sentiment labels automatically derived presence emoticons tweet text test contains tweets sentiment label assigned human annotator. model layers convolution pooling tanh transformations using feature maps widths respectively. word embeddings dimensional. exactly follows used kalchbrenner except used version convolution operation model pooling. consequence model substantially fewer parameters model kalchbrenner al.. results reported table also lists selection results previous work data comparison. model errors kalchbrenner’s errors close performance signiﬁcantly better competitors. central goal paper develop novel convnet methods visualisation summarisation reviews. however well tasks also want convnet work well review sentiment classiﬁer. also line aspiration building models deployed solve multiple tasks. focus imdb movie review sentiment data originally introduced maas benchmark sentiment analysis. dataset contains total movie reviews posted imdb. unlabelled reviews remaining reviews divided review training review test set. labelled pre-process review ﬁrst stripping html markup breaking review sentences breaking sentence words. nltk perform tasks. also numbers generic number token symbol symbol word appears fewer times training unknown. leaves word vocabulary. best model data uses layer convnet model process sentence followed layer convnet model process document. word embeddings dimensional sentence model uses feature maps width followed k-max pooling layer width leads sentence embeddings dimensions. document model uses feature maps look adjacent sentences followed k-max pooling layer width leads document embeddings dimensions. results experiment shown table model achieves best knowledge third best published result data set. found result encouraging dataset small train convnet model well would like too. main challenge achieving good performance task regularising model strongly enough would overﬁt. best result data achieved paragraph vector mikolov however inference model expensive since must perform optimisation order infer paragraph vector unseen document. contrast able compute document embedding using single feed forward pass. shown train convnet model good sentiment classiﬁer following section capitalise show model also enables visualise salient features documents provide users compact summaries reviews. section show recent work visualising activations convnets computer vision also applied visualising convnets text. addition providing insights model learned techniques used extract automatic summaries texts. deconvolutional networks used great effect generate interpretable visualisations activations deep layers convolutional neural networks used computer vision recent work shown good visualisations obtained using single backpropogation pass network. fact procedure formally quite similar operations carried deconvolutional visualisation backpropogation generalisation deconvolutional approach since backpropogate non-convolutional layers. ﬁrst step summarisation procedure create saliency document assigning importance score sentence. generate saliency given document adopt technique simoyan modiﬁed objective function. ﬁrst perform forward pass network generate class prediction document. construct pseudo-label inverting network predictions feed training loss function true label. choice pseudo-label allows induce greatest loss. infer saliency words take ﬁrst order taylor expansion loss function using pseudo-label. formally network function denotes words compose document summarised. approximate loss linear function vector entry word document |wi| measure saliency word saliency scores easily computed performing single pass backpropogation network. intuition behind using gradient magnitudes saliency measure magnitude derivative indicates words need changed least affect score most. explained generate saliency maps words technique generate saliency maps sentences model clear separation sentence document level representations. generate sentence level saliency simply perform partial backpropogation pass model equivalently take taylor expansion respect partial evaluation network. denotes evaluation network sentence level. saliency scores sentence rank sentences document. generate summary ﬁxed length simply take highly ranked sentences review form summary. order evaluate automatic summaries produced model train na¨ıve bayes classiﬁer imdb movie review sentiment data classify review summaries. tf/idf weighted unigram features processing train na¨ıve bayes model. results experiment shown table compare accuracy na¨ıve bayes summaries different sizes created taking ranked sentences using visualisation technique. even keeping review accuracy classiﬁer trained full reviews drops less test set. baseline also report accuracy classiﬁer summaries created choosing random sentences review clear results summaries create preserve signiﬁcant amount information lost choosing random sentences. also compare common summarisation heuristic building summary choosing ﬁrst last sentence review. heuristic performs particularly badly data explained fact many reviews begin sentences plot summary generally relevant sentiment review. show several examples summaries created model figure seen examples many reviews begin short descriptions reviewer brief summary plot. sentences useful part summaries since express opinion movie reviewed. model learns ignore background sentences consistently. paper introduced convolutional neural network model able represent meaning documents embedding dimensional vector space preserving distinctions word sentence order crucial capturing nuanced semantics. model builds document representation compositional manner combining word embeddings sentence embeddings combining sentence embeddings representation full document. shown single model used accomplish wide variety document modelling tasks including classiﬁcation summarisation visualisation document structure. tasks accomplished single model trained classiﬁcation task re-training needed apply model beyond task created. structure model allows learn word sentence document representations simultaneously. important avenue future work work exploring representations three table results classifying summaries na¨ıve bayes. results labelled proportion indicate selecting indicated percentage sentences review results labelled ﬁxed show result selecting ﬁxed number sentences each. summary column shows accuracy na¨ıve bayes summaries produced model. random column shows model classifying summaries created selecting sentences random. margin column shows difference accuracy model random summaries. caught movie sci-fi channel recently. actually turned pretty decent b-list horror/suspense ﬁlms guys take road trip stop wedding worst possible luck maniac freaky make-shift tank/truck hybrid decides play cat-and-mouse them. things complicated pick ridiculously whorish hitchhiker. makes unique combination comedy terror actually work movie unlike many others. guys likable enough good chase/suspense scenes. nice pacing comic timing make movie passable horror/slasher buff. definitely worth checking out. local independent station york city area. cast showed promise director george cosmotos became suspicious. sure enough every every pointless stupid every george cosmotos movie ever saw. he’s like stupid man’s michael awfulness accolade promises. there’s point conspiracy burning issues urge conspirators left connect dots grafﬁti various walls next. thus current budget crisis iraq islamic extremism fate social security million americans without health care stagnating wages death middle class subsumed sheer terror grafﬁti. truly stunningly idiotic ﬁlm. graphics best part game. number best game series. next underground. deserves strong love. insane game. massive levels massive unlockable characters... it’s massive game. waste money game. kind money wasted properly. even though graphics suck thats doesn’t make game good. actually graphics good time. today graphics crap. cares? canada game aye. well don’t know that might. knows. well canadian people wait minute getting topic. game rocks. play enjoy love it’s pure brilliance. ﬁrst good original. horror/comedy movie. heard second made watch really makes movie work judd nelson’s character sometimes clever script. pretty good script person wrote final destination ﬁlms direction okay. sometimes there’s scenes looks like ﬁlmed using home video camera grainy look. great made movie. worth rental probably worth buying nice eerie feeling watch judd nelson’s stanley best. suggest newcomers watch ﬁrst watching sequel you’ll idea stanley like little history background. movie released biggest soon became blockbuster. honestly movie ridiculous watch plot gloriﬁes loser. movie line preeti madhura tyaga amara means love’s sweet sacriﬁce immortal. movie hero movie sacriﬁces love leading lady even though loved other justiﬁcation meaning line. movie inﬂuenced many young broken hearts found loser like sacriﬁcial attitude thoughtful hence became cult movie could moved lives. ganesh’s acting movie amateurish crass childishly stupid. actually looks funny song he’s supposed look stylish cool. looks don’t help leading role either. hair style badly done part movie. pooja gandhi cant act. costumes horrendous movie inconsistent. good part movie excellent cinematography brilliant music mano murthy actually true saving graces movie. also lyrics jayant kaikini well penned. director yograj bhat lauded picturization songs tasteful manner. anyway except songs movie ordinary friend went phase years selecting crappest horror ﬁlms video shop evening’s entertainment. reason ended buying cheap synth soundtrack classic time genre. there’s also amusing scenes. among scene man’s attacked defends number unlikely objects made laugh time apart it’s total crap mind you. probably worth watch like ﬁlms like chopping mall. i’ve seen too. tried restarting movie twice. three machines wrong steven seagal’s voice change? ﬁlming studio sound someone doesn’t even resemble him? sound destroyed? minutes ﬁnally hear actor’s real voice. though throughout sounds like audio recorded bathroom. would ashamed donate copy movie goodwill owned copy. rented never again. check database renting movies good movies. usually knew getting watched steven seagal movie. guess more. vertigo stars stewart novak elevate this stewart’s christmas movie movie level entertainment. chemistry stars makes fairly moving experience revelation gleaned movie witchcraft seen metaphor private pain hampers many people’s relationships. nice diversion legendary stars figure several example summaries created convnet. full text review shown black sentences selected convnet appear colour. summarising review ﬁrst sentence popular pragmatic approach clear examples heuristic effective convnet summarisation scheme. summary created selecting sentences review.", "year": 2014}