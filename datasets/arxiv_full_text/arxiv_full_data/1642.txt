{"title": "Mixed Membership Word Embeddings for Computational Social Science", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Word embeddings improve the performance of NLP systems by revealing the hidden structural relationships between words. Despite their success in many applications, word embeddings have seen very little use in computational social science NLP tasks, presumably due to their reliance on big data, and to a lack of interpretability. I propose a probabilistic model-based word embedding method which can recover interpretable embeddings, without big data. The key insight is to leverage mixed membership modeling, in which global representations are shared, but individual entities (i.e. dictionary words) are free to use these representations to uniquely differing degrees. I show how to train the model using a combination of state-of-the-art training techniques for word embeddings and topic models. The experimental results show an improvement in predictive language modeling of up to 63% in MRR over the skip-gram, and demonstrate that the representations are beneficial for supervised learning. I illustrate the interpretability of the models with computational social science case studies on State of the Union addresses and NIPS articles.", "text": "word embeddings improve performance systems revealing hidden structural relationships words. despite success many applications word embeddings seen little computational social science tasks presumably reliance data lack interpretability. propose probabilistic model-based word embedding method recover interpretable embeddings without data. insight leverage mixed membership modeling global representations shared individual entities free representations uniquely diﬀering degrees. show train model using combination state-of-the-art training techniques word embeddings topic models. experimental results show improvement predictive language modeling skip-gram demonstrate representations beneﬁcial supervised learning. illustrate interpretability models computational social science case studies state union addresses nips articles. word embedding models learn encode dictionary words vector space representations shown valuable variety natural language processing tasks statistical machine translation part-of-speech tagging chunking named entity recogition provide nuanced representation words simple indicator vector dictionary. models follow long line research data-driven semantic representations text including latent semantic analysis probabilistic extensions particular topic models found broad applications computational social science digital humanities interpretable representations reveal meaningful insights. despite widespread success tasks word embeddings supplanted topic models method choice computational social science applications. speculate primary factors perceived reliance data lack interpretability. work develop models address limitations. word embeddings risen popularity applications success models designed speciﬁcally data setting. particular mikolov showed simple word embedding models high-dimensional representations scale massive datasets allowing outperform sophisticated neural network language models process fewer documents. work oﬀer somewhat contrarian perspective currently prevailing trend data optimism exempliﬁed work mikolov collobert others argue massive datasets suﬃcient allow language models automatically resolve many challenging tasks. note datasets always available particularly computational social science applications data interest often obtained large scale sources internet social media sources press releases academic journals books transcripts recorded speech wikipedia embeddings tasks target dataset however shall here standard practice might always eﬀective size dataset correspond degree relevance particular analysis. even large corpora idiosyncrasies make embeddings invalid domains. instance suppose would like word embeddings analyze scientiﬁc articles machine learning. table report similar words word learning based word embedding models trained corpora. embeddings trained articles nips conference similar words related machine learning desired embeddings trained massive generic google news corpus similar words relate learning teaching classroom. evidently domain-speciﬁc data important. even concerningly bolukbasi show word embeddings encode implicit sexist assumptions. suggests trained large generic corpora could also encode hegemonic worldview inappropriate studying e.g. black female hip-hop artists’ lyrics poetry syrian refugees could potentially lead systematic bias minorities women people color applications real-world consequences automatic essay grading college admissions. order proactively combat kinds biases large generic datasets address computational social science tasks need eﬀective word embeddings small datasets relevant datasets used training even small. make word embeddings viable alternative topic models applications social sciences desire embeddings semantically meaningful human analysts. paper introduce interpretable word embedding model associated topic model designed work well trained small medium-sized corpus interest. primary insight data-eﬃcient parameter sharing scheme mixed membership modeling inspiration topic models. mixed membership models provide ﬂexible eﬃcient latent representation entities associated shared global representations uniquely varying degrees. identify skipgram wordvec model mikolov corresponding certain naive bayes topic model leads mixed membership extensions allowing fewer vectors words. show leads better modeling performance without data measured predictive performance well interpretable latent representations highly valuable computational social science applications. interpretability representations arises deﬁning embeddings words terms embeddings topics. experiments also shed light relative merits training embeddings generic data corpora versus domain-speciﬁc data. section provide necessary background word embeddings well topic models mixed membership models. traditional language models predict words given contexts found thereby forming joint probabilistic model sequences words language. bengio developed improved language models using distributed representations words represented neural network synapse weights equivalently vector space embeddings. later authors noted word embeddings useful semantic representations words independently whether full joint probabilistic language model learned alternative training schemes beneﬁcial learning embeddings. particular mikolov proposed skip-gram model inverts language model prediction task aims predict context given input word. skip-gram model log-bilinear discriminative probabilistic classiﬁer parameterized input word embedding vectors input words output word embedding vectors context words context shown table top-left. topic models latent dirichlet allocation another class probabilistic language models used semantic representation straightforward model text corpora unsupervised multinomial naive bayes latent cluster assignment document selects multinomial distribution words referred topic documents’ words assumed generated. topic models improve naive bayes using mixed membership model assumption words document belong topic relaxed replaced distribution topics model’s assumed generative process word document topic assignment drawn word drawn chosen topic mixed membership formalism provides useful compromise model ﬂexibility statistical eﬃciency topics shared across documents thereby sharing statistical strength document free topics unique degree. bayesian inference aids data eﬃciency uncertainty managed shorter documents. recent papers aimed combine topic models word embeddings address small data problem computational social science focus here. provide detailed discussion related work supplementary. design interpretable word embedding model small corpora identify novel connections word embeddings topic models adapt advances topic modeling. following distributional hypothesis skip-gram’s word embeddings parameterize discrete probability distributend semantically coherent property leveraged gaussian model suggests discrete distributions reinterpreted topics thus reinterpret skip-gram parameterization certain supervised naive bayes topic model topic model input words fully observed cluster assignments words wi’s contexts document. skip-gram diﬀers supervised topic model parameterization topics word vectors encode distributions log-bilinear model. note although skip-gram discriminative sense jointly model input words equivalently interpreting encoding conditionally generative process context given words order develop probabilistic models extend skip-gram. model improved replacing naive bayes assumption mixed membership assumption. applying mixed membership representation topic model version skip-gram obtain model bottom-right table parameterizing model word embeddings obtain ﬁnal model mixed membership skip-gram model input word distribution topics topic vector-space embedding output word vector topic superscripts denote current parameter estimates. m-step optimizes lower bound log-likelihood obtained substituting equation however involves complexity em-steps token number topics/dictionary words respectively even token considered impractical word embeddings instead propose approximation sublinear time ﬁrst impute using reparameterization technique thereby reducing task standard word embedding. done sublinear time using metropolis-hastings-walker algorithm. oracle log-likelihood tion eﬃciently learn topic word embeddings noise-contrastive estimation enough computation exactly optimizes cdll objective function avoids computing expensive normalization constants provides adjustable computational eﬃciency knob. details described below. course model also requires parameters learned namely mixed membership proportions based topic modeling hypothesized care added parameters need adversely aﬀect performance small-medium data regime reasons bayesian approach eﬀectively manage uncertainty them marginalize prevents being bottleneck training; test time using posterior given context instead obtain vector word type word token leverage context posterior mean ˆvwi embeddings convex combinations topic vectors fewer vectors words model capacity lost ﬂexibility mixed membership representation allows model compensate. number shared vectors equals number words mixed membership skip-gram strictly representationally powerful skip-gram. vectors words expect increased representational power would beneﬁcial data regime. goal leave future work. ﬁrst describe idealized impractical training algorithm mmsg introduce practicable procedure mmsg principle trained maximum likelihood estimation using optimizing log-likelihood hinmodel networks learning neural bayesian data models approach network framework belief learning framework models methods markov function bayesian based inference model models bayesian prior data parameters likelihood priors structure graphical monte carlo chain markov sampling mcmc method methods model bayesian neural networks weigend bayesian data mackay learning computation practical probability model data models priors algorithm bayesian likelihood parameters carlo monte mcmc chain reversible sampling model posterior derive algorithm insight mmsg model equivalent topic model version parameterization. suﬃciently high dimensional embeddings log-bilinear model capture hood embeddings would encode exact word distributions topics topic model however topic model admits collapsed gibbs sampler eﬃciently resolves cluster assignments cause bottleneck therefore propose reparameterize mmsg corresponding topic model purposes imputing z’s. then ﬁxed estimate learning word topic vectors corresponds ﬁnding optimal vectors encoding φ’s. topic model pre-clustering step reminiscent reisinger mooney huang apply oﬀ-the-shelf clustering algorithm initially identify diﬀerent clusters contexts apply word embedding algorithms cluster assignments. however clustering learned based word embedding model itself clustering test time performed bayesian reasoning equation rather ad-hoc method. dirichlet priors parameters collapsed gibbs update parameter vectors dirichlet priors topic word distributions input output word/topic counts number occurrences word word context. scale algorithm thousands topics using adapted version recently proposed metropolishastings-walker algorithm high-dimensional topic models scales sublinearly method uses data structure called alias table allows amortized time sampling discrete distributions. metropolis-hastings update used correct approximating update proposal distribution based samples. interpret product context dominates collapsed gibbs update product experts word context expert weighs multiplicatively update. order approximate alias tables proposals approximate product experts mixture experts. select word uniformly context proposal draws candidate topic proportionally chosen context word’s contribution update expect proposals bear resemblance target distribution ﬂatter property we’d generally like proposal distribution. proposal implemented eﬃciently sampling experts alias table data structure amortized time rather time linear sparsity pattern since proposal involve sparse term perform simulated annealing optimize posterior natural metropolis-hastings. interpreting negative posterior energy function boltzmann distribution temperature iteration achieved raising model part metropolis-hastings acceptance ratio power annealing also helps mixing standard gibbs updates become nearly deterministic. temperature schedule target ﬁnal temperature controls initial temperature therefore mixing early computed time constant sampling amortized constant time iteration amortized constant time rao-blackwellized estimates mixed membership proportions obtained ﬁnal sample finally topic assignments imputed estimated topic model must learn embeddings still expensive context maximum likelihood estimation i.e. optimizing vector word topic embeddings. complexity also issue standard skip-gram mnih mnih kavukcuoglu addressed using noisecontrastive estimation algorithm gutmann hyv¨arinen avoids expensive normalization step making algorithm scale sublinearly vocabulary size algorithm solves unsupervised learning tasks transforming supervised learning task distinguishing data randomly sampled noise samples logistic regression. supposing samples noise distribution word-pair example objective function context diﬀerence log-likelihood model noise distributions. learn embeddings stochastic gradient ascent objective. number noise samples tends inﬁnity method increasingly well approximates maximum likelihood estimation i.e. stationary points equation converge equation ﬁrst measured eﬀectiveness embeddings skip-gram’s training task predicting context words given input words task measures methods’ performance predictive language modeling. used four datasets sociopolitical scientiﬁc literary interest corpus nips articles nealing performed iterations performed million minibatches size dimensional embeddings used used nips state union smaller datasets. methods able leverage remainder context either adding context’s vectors posterior helped methods except naive skip-gram. identify several noteworthy ﬁndings. first generic data vectors outperformed skip-gram datasets large margin indicating domain-speciﬁc embeddings often important. second mixed membership models using posterior inference beat matched naive bayes counterparts word embedding models topic models. hypothesized posterior inference test time important good performance. finally topic models beat corresponding word embedding models prediction. therefore recommend mmsg topic model variant predictive language modeling small data regime. goals experiments study relative merits data domain-speciﬁc small data validate proposed methods study applicability computational social science research. tested performance representations features document categorization regression tasks. results given table document categorization used three standard benchmark datasets table mean reciprocal rank held-out context words. skip-gram topic model mixed membership. bold indicates statistically signiﬁcant improvement versus newsgroups reuters- newswire articles ohsumed medical abstracts cardiovascular diseases held test documents newsgroups used standard train/test splits literature corpora obtained document embeddings mmsg latent space topic embeddings summing posterior mean vectors token. vector addition similarly used construct document vectors embedding models. vectors normalized unit length. also considered tf-idf baseline. logistic regression models trained features extracted training method. across three datasets several clear trends emerged first generic google vectors consistently substantially outperformed classiﬁcation performance skipgram mmsg vectors highlighting importance corpus-speciﬁc embeddings. second despite mmsg’s superior performance language modeling small datasets features outperformed mmsg’s document categorization task. encoding vectors topic level instead word level mmsg loses word level resolution embeddings turned valuable particular classiﬁcation tasks. however restricted type embedding construct features classiﬁcation. interestingly mmsg features concatenated improved classiﬁcation performance vectors individually. suggests topic-level mmsg vectors wordlevel vectors encode complementary information beneﬁcial performance. finally further concatenating generic google vectors’ features improved performance again despite fact vectors performed poorly own. noted tf-idf notoriously eﬀective document categorization outperformed embedding methods datasets. also analyzed regression task predicting year state union address based text information. used lasso-regularized linear regression models evaluated leave-one-out cross-validation experimental setup. root-mean-square error results reported table unlike tasks google data vectors best individual features case outperforming domain-speciﬁc mmsg embeddings individually. hand sg+mmsg+google performed best overall showing domain-speciﬁc embeddings improve performance even data embeddings successful. tf-idf baseline beaten embedding models task. also performed several case studies. obtained document embeddings latent space topic embeddings summing posterior mean vectors ˆvwi token visualized dimensions using t-sne state union addresses embedded almost linearly year major jump around deal well separated party given time period. embedded topics allow interpret space. george bush addresses embedded near terror topic addresses. colored circles t-sne projected embeddings sotu addresses. color party green george washington) size recency gray circles correspond topics. objects visual object recognition model recognition segmentation character speech recognition system hybrid computer vision ieee image pattern university science colorado department error training data performance gaussian distribution model matrix nips corpus input word bayesian naive bayes skip-gram models learned topic words refer bayesian networks probabilistic models neural networks. mixed membership models able separate coherent speciﬁc topics including bayesian modeling bayesian training neural networks monte carlo methods. performing additive composition word vectors obtain ﬁnding prior mean vector word type obtain relevant topics nearest neighbors similarly additive composition topic word proposed model-based method training interpretable corpus-speciﬁc word embeddings computational social science using mixed membership representations metropolis-hastings-walker sampling nce. experimental results prediction supervised learning case studies state union addresses nips articles indicate high-quality embeddings topics obtained using method. results highlight fact data always best domain-speciﬁc data valuable even small. plan approach substantive social science applications address algorithmic bias fairness issues.", "year": 2017}