{"title": "An Evolving Cascade Neural Network Technique for Cleaning Sleep  Electroencephalograms", "tag": ["cs.NE", "cs.AI"], "abstract": "Evolving Cascade Neural Networks (ECNNs) and a new training algorithm capable of selecting informative features are described. The ECNN initially learns with one input node and then evolves by adding new inputs as well as new hidden neurons. The resultant ECNN has a near minimal number of hidden neurons and inputs. The algorithm is successfully used for training ECNN to recognise artefacts in sleep electroencephalograms (EEGs) which were visually labelled by EEG-viewers. In our experiments, the ECNN outperforms the standard neural-network as well as evolutionary techniques.", "text": "abstract. evolving cascade neural networks training algorithm capable selecting informative features described. ecnn initially learns input node evolves adding inputs well hidden neurons. resultant ecnn near minimal number hidden neurons inputs. algorithm successfully used training ecnn recognise artefacts sleep electroencephalograms visually labelled eeg-viewers. experiments ecnn outperforms standard neural-network well evolutionary techniques. build feed-forward neural networks cascade-correlation learning algorithm suggested generates hidden neurons needed. several authors explored applied cascade neural networks real-world problems cascade network differs fully connected feed-forward neural networks exploiting fixed architecture contrast fnns cascade networks start learning neuron learning algorithm automatically adds trains neurons creating multi-layer structure. number hidden neurons complexity network increases step-by-step training error decreases. result training algorithm grows neural network near optimal complexity generalise well. fan-in small number hidden layers controlling connectivity. results reveal trade-off connectivity depth number independent parameters learning time etc. number inputs small relative size training higher connectivity usually leads faster learning fewer independent parameters also results unbounded fan-in depth. practice training algorithms over-fit cascade networks noise training data. noise affects features assumed presenting training data makes irrelevant classification problem. overcome over-fitting problem data preprocessing techniques developed aimed select informative features however results feature selection algorithms depend special conditions example order features processed method based combination algorithms early stopping ensemble averaging. shown method improves predictive ability cascade networks. pruning methods described developed networks trained cascade-correlation learning algorithm. methods used estimate importance input variables characterising quantitative relationships. cascade-correlation networks compared neural networks fixed structure terms performance. selected input variables improved predictive accuracy cascade neural networks. handling suggested ivakhnenko using evolutionary principle gmdh effectively used training neural networks growing architecture gmdh algorithms capable selecting relevant features learning neural networks. within gmdh approach complexity neural networks grows predefined criterion met. result gmdh-type neural networks achieve near optimal complexity. paper mainly focus selecting informative features cascade networks learn data. believe effective prevent networks over-fitting. indeed cascade network starts learn small number inputs neurons inputs neurons added network improve performance. networks evolve learning name evolving cascade neural networks clinical electroencephalograms recorded newborns sleep hours. data characterises features calculated spectral domain. however features irrelevant redundant makes recognition problem difficult number different human sleep states using high order kalman filter coefficients averaged one-second window. coefficients clustered kohonen selforganizing network. three types transition trajectories found corresponding states wakefulness dreaming sleep deep sleep. following idea schl√∂gl used inverse filtering identify artefacts scored types sleep eegs. kalman filtering used estimate adaptive autoregressive parameters. concluded variance prediction error used indicator muscle movement artefacts. roberts suggested technique detecting outlying patterns existing real data deteriorate analysis sleep states. suggested using artificial class located outside class boundaries pattern assigned probability novelty test largest. describes ecnn training algorithm developed. section describes application ecnn algorithm recognise artefact segments clinical eegs. sections describe comparison ecnn standard feed-forward neural-network evolutionary techniques eegs finally section concludes paper. ideas behind cascade-correlation architecture follows. first build cascade architecture adding neurons together connections inputs well previous hidden neurons. configuration changed following layers. second idea learn newly created neuron fitting weights minimise residual error network. neurons added network performance increases. common cascade-correlation technique assumes variables characterising training data relevant classification problem. beginning cascade network inputs output neuron starts learn without hidden neurons. output neuron connected every input weights adjustable learning. follows training neuron algorithm adjusts weights reduce residual error network. algorithm adds trains neurons residual error decreases. networks predefined network automatically built training data. second cascade network learns fast neurons trained independently other. define cascade network architecture consisting neurons whose number inputs increased layer next. first layer neuron connected inputs input input single-input neuron provides minimal error. output previous neuron. third input neuron connected input provides maximal decrease output error. neuron layer connected manner. function output neuron written follows output error feature involved combination previous features. natural assume output error evaluated validation dataset irrelevant well redundant features unlikely become involved resultant network. point view selection criterion operates regularity criterion gmdh mentioned above. used fitting synaptic weights neuron. case values dependent generalisation ability neuron given connections value increases proportionally number misclassified validation examples. words neuron irrelevant connections cannot classify unseen examples correctly reason value expected high. idea behind algorithm criterion select neurons relevant connections. criterion says value calculated neuron less value calculated previous neuron connection neuron relevant previous layer else less relevant. formally criterion used define following acceptance rule rule connections weights neuron added network. case neuron accepted rule given number failed attempts algorithm stops neuron minimal value assigned output neuron. real-world problems structure parameters noise affecting data unknown. without information standard evaluation methods e.g. assuming gaussian noise yield biased estimates weights neurons. however projection method described yield unbiased estimates presence noise unknown structure. based method developed method fitting weights described below. training data evaluated regularity criterion. implement criterion dividing whole training dataset subsets first fitting second validation weights. clearly case outputs neurons relevant irrelevant connections calculated dataset significantly different. training ecnn used following heuristics. first exploit best feature provides minimal value calculated single-input neurons. feature connected neuron. second heuristic incrementally involve features cascade network. third heuristic rule accept neuron connections relevant previous neuron. weights candidate-neurons updated rule condition met. step neurons start learn input. following steps cascade network involves features well neurons value decreases. neurons. networks know able generalise well. recognition artefacts sleep eegs recorded clinical conditions still difficult problem described several researchers recognising artefacts eegs recorded newborns sleep hours standard electrodes breidbach developed neural network technique taking account spectral statistical features calculated segment frequency bands sub-delta delta theta alpha beta beta following experiments used structure data recorded newborns sleep hours. spectral powers frequency bands calculated channels well c+c. features extended statistical features presenting relative absolute powers well variances. finally data normalised zero mean unit variance. eeg-viewer. segments merged dataset divided training testing subsets containing randomly selected segments. rates artefacts datasets respectively. fig. depicts structure best ecnn includes four inputs three hidden neurons output neuron. original features training algorithm selected four features make important contribution classification outcome. first hidden neuron connected inputs output neuron connected outputs hidden neurons well inputs calculated variables remaining variables. variables strongly correlated others correlation coefficients close fact explains variety structures ecnns capable recognising artefacts effectiveness. runs. variable used frequently input variables. however rigorously analysing frequencies cannot conclude reflect contribution features classification outcome. indeed ecnn became stuck local maxima performance missed best combinations sequences features. ecnns reached deepest maximum performance. ecnns depicted fig. frequencies using variables rather reflect biased contribution them. comparison used standard neural-network technique train fnns hidden layer output neuron. number hidden neurons varied neurons. neurons implemented standard sigmoid transfer function order remove contribution correlated inputs improve performance fnns applied standard technique principal component analysis different fractions total variation. matlab. learning parameters algorithm listed table net.trainparam.epochs net.trainparam.goal net.trainparam.lr net.trainparam.max_fail net.trainparam.min_grad net.trainparam.mu net.trainparam.mu_dec net.trainparam.mu_inc net.trainparam.mu_max net.trainparam.time comments layers hidden output randomly initialised. active regions layer neurons distributed roughly evenly input space maximum number epochs performance goal learning rate maximum validation failures minimum gradient parameter algorithm multiplier multiplier maximum training time thus conclude first ecnn slightly outperforms standard testing data. second best ecnn involves relevant features selected original features consists neurons. ecnn training algorithm able automatically discover neural network structure appropriate training data. third discovered structure standard technique allows achieving increase performance shown fnn*. ecnn algorithm used preprocessing technique effective standard pca. second comparative experiments used eegs recorded newborns described first experiments. eegs containing segments form training validation data sets containing form testing data set. artefact rates training testing data respectively. record normalised zero mean unit variance. experiments used fold crossvalidation. evolutionary technique gmdh mentioned section well standard decision tree technique techniques listed table times fold order find classifiers providing best performance validation data. parameters classifiers initialised randomly. performances best classifiers validation data evaluated testing data table provides average values standard variances calculated folds. gmdh using evolutionary search strategy learn polynomial networks near optimal complexity data. transfer function neurons assigned shortterm non-linear polynomial method described important note that class boundaries data overlap heavily coefficients randomly selected training examples provides better performance validation data. experiments found best performance achieved selecting training examples. single-input neurons. transfer function described algorithm mates randomly selected neurons adds offspring population performance validation data becomes better offspring-neurons created generation predefined. gmdh-type network evolved created offspring-neurons improve performance. number failed attempts improving current performance exceeded gmdh algorithm stops selects resultant network providing best performance validation data. best performance achieved several networks resultant network assigned consisting minimal number neurons. experiments initial population assigned consisting single-input neurons number offspring-candidates number serial failed attempts predefined respectively. standard variance shown table average number twoinput neurons transfer function variance respectively. average number input variables involved gmdh-type networks respectively provided best performance testing data. consist nodes involve correspondingly input variables. average performance variance shown table gmdh-type neural network outperforms average average performance fnns variance best performance fnns achieved principal components hidden neurons. components contributed least classification outcomes. performances gmdh-type neural network table same. variance shown table slightly better gmdh techniques. performance worse first experiment used records labelled eeg-expert. happen second experiments used eegs recorded different conditions labelled several experts. moreover artefact rate eegs higher. equal number found best fnns. however eccns involve average input variables selected original variables. ecnn technique seems better gmdh techniques. developed algorithm allowing cascade neural networks evolve learn presence noise redundant features. ecnn starts learn neuron inputs well neurons added network performance increases. result ecnn near optimal complexity. newborns sleep hours. artefacts visually labelled eegviewers. spectral statistical features calculated present eegs automated recognition noisy redundant. data slightly outperformed standard fnns fixed structure. ecnn also outperformed evolutionary gmdh-type well standard decision tree techniques. neural networks presence noise redundant features. believe eccn technique improve performance cascade neural networks applied real-world problems pattern recognition classification. research supported university jena part epsrc grant gr/r/. author grateful frank pasemann joachim schult fruitful enlightening discussions joachim frenzel burghart scheidt pediatric clinic university jena providing recordings jonathan fieldsend university exeter useful comments.", "year": 2005}