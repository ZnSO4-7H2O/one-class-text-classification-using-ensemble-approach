{"title": "RETURNN: The RWTH Extensible Training framework for Universal Recurrent  Neural Networks", "tag": ["cs.LG", "cs.CL", "cs.NE"], "abstract": "In this work we release our extensible and easily configurable neural network training software. It provides a rich set of functional layers with a particular focus on efficient training of recurrent neural network topologies on multiple GPUs. The source of the software package is public and freely available for academic research purposes and can be used as a framework or as a standalone tool which supports a flexible configuration. The software allows to train state-of-the-art deep bidirectional long short-term memory (LSTM) models on both one dimensional data like speech or two dimensional data like handwritten text and was used to develop successful submission systems in several evaluation campaigns.", "text": "software functional layers. software comes dependencies furthermore tightly integrated rasr software package also developed institute paper provide overview important aspects software. paper organized follows section give overview software returnn based well competing implementations used tasks asr. section describes design neural network training setup within returnn. section gives overview components tool. section provides information extend returnn additional functional layers. finally demonstrate efﬁciency returnn empirically comparing tensorflow torch. theano python based framework symbolic mathematical tensor expressions support automatic differentiation. expressions modeled computational dependency graph augmented automatic optimization procedure. implementation graph node abstract deﬁned various types hardware like cpus gpus. properties make theano particularly useful neural network training tasks. providing required building blocks theano allows deﬁne complex connectionist structures fully differentiable. keras high-level theano based framework data-driven machine learning. maybe similar software package returnn. keras started pure theano based framework also supports tensorflow back-end minimal restrictions. similar projects built theano library include lasagne blocks tensorflow recent open source machine learning package google actively developed comes already many predeﬁned solutions lstms endto-end systems others. tensorflow similar theano also works symbolic computation graphs automatic differentiation. torch uses programming language consists many ﬂexible modular components developed community. contrast theano torch symbolic expressions calculations done explicitly. work release extensible easily conﬁgurable neural network training software. provides rich functional layers particular focus efﬁcient training recurrent neural network topologies multiple gpus. source software package public freely available academic research purposes used framework standalone tool supports ﬂexible conﬁguration. software allows train state-of-the-art deep bidirectional long short-term memory models dimensional data like speech dimensional data like handwritten text used develop successful submission systems several evaluation campaigns. index terms recurrent neural networks lstm speech recognition software package multi-gpu introduction recurrent neural networks particular lstms dominate sequential learning tasks including automatic speech recognition statistical machine translation image caption generation training deep recurrent neural networks considerably harder compared pure feed-forward structures accumulation gradients time. long time implementations methods topologies required training. changed rapidly solutions automatic differentiation symbolic representations combined powerful computing libraries machine learning community prominent example time theano provides extensive python package compute derivatives using symbolic mathematical expressions. packages allow comfortably design neural network architectures level serve ready-to-use solutions large scale tasks. instead primary focus generality order allow various system designs without introducing constraints performance usability issues. also naturally much interest getting best performance particular hardware setup instead keep compatibility maximum. returnn draws theano additional layer theano library aims research oriented software packages application driven machine learning software like caffe software provides highly optimized lstm kernels written cuda well efﬁcient training multi-gpu setup. simplify construction topologies using json based network conﬁguration also providing extend class direction class direction class direction class direction output class softmaxfrom fig. example network speciﬁcation json realizes bidirectional lstm-rnn layers containing nodes forward backward direction correspondingly. task speciﬁc software packages like rasr kaldi developing speech recognition systems contain modules train decode systems including neural networks. eesen package extends kaldi adding rudimentary support lstms returnn extends rasr support various recurrent neural networks architectures systems. returnn provides fully functional training software includes user interaction multi-batch trainer possibility extract network activations processing. dependencies besides theano required network topologies always gpu. execution returnn writes useful information conﬁgurable verbosity standard output ﬁle. network activations forwarded directly passed rasr decoder described section possible execute returnn daemon mode allows access model evaluation using services. conﬁguration network architectures described using json format. network hereby layer identiﬁcation names layer descriptions. layer description simply dictionary containing class parameter speciﬁes layer class optional list incoming layers layer speciﬁc parameters. constructing network returnn looks layer speciﬁed loss recursively instantiates layers directly indirectly connected figure example bidirectional lstm network. remaining conﬁguration parameters provided simple parameters. typical conﬁguration contains task path descriptor input data learning rate together suitable adjustment methods information batches crafted. conﬁguration parameters also merged network json description even provided fully python format single conﬁguration used. python format allows deﬁne custom layer types functions conﬁguration network. provide demo setups conﬁguration ﬁles together release software. layers layers fundamental building blocks returnn. layer named class callable json description specifying constructor parameters. already provide rich feed-forward layers including convolutional operations support common activation functions. convolutional layers hereby make fig. returnn processing pipeline. sequences generated passed main engine. engine combines sequences batches performs epoch-wise training network parameters using devices. device computes error signal batch generates updates according optimizer. efﬁcient kernel implementations within cudnn. network behavior augmented using functional components like sampling windowing. output layers various loss functions available including crossentropy mean-squared-error connectionist temporal classiﬁcation main focus however lies recurrent layers. different cell implementations including lstm gated recurrent units associative lstm many variants available. recurrent layers connected passing ﬁnal state another allowing encoder decoder topologies attention described conﬁgurable attention mechanism available calculate expected inputs encoder networks. using similar method recurrent layers allow basic language modeling. example seen figure several layers composed sub-networks used regular layers allows model high order circular dependencies layers. large vocabulary speech recognition tasks memory requirements signiﬁcantly larger memory limitations operating hardware. particularly true gpus. therefore implemented data caching technique minimizes hard disk usage keeping amount allocated memory conﬁgurable threshold. also many tasks lengths sequences deviate large amount combining sequences batches requires process many additional frames added zero-padding. software therefore provides option chunk sequences segments constant length. sacriﬁcing contextual information provided states chunks chunking allows make much efﬁcient memory returnn also supports generic pre-training scheme simpler network topologies automatically generated based given network topology. currently experimental torch-theano bridge released software allows torch code within returnn. ward computations performed single matrix multiplication whole mini-batch sequences. applies back propagation step respect weights inputs recurrent part back propagated time. furthermore reuse memory wherever possible custom cuda kernels lstm gating mechanism. best knowledge provide ﬁrst publicly available gpu-based implementation multidimensional long short-term memory mdlstm layer hidden state position calculated based predecessor hidden activations means computed predecessor states known. consequence previous implementations mdlstm process pixel time traversing image column-wise outer loop row-wise inner loop. noticed activations positions common diagonal computed time allows exploit massive parallelism offered modern gpus. additionally process multiple images also four directions multi-directional mdlstm layer simultaneously using batched cublas operations custom cuda kernels. optionally stable cell described used improve convergence. optimization optimizing deep rnns regular stochastic gradient descent allow network converge ﬁxed point weight space sophisticated methods needed. cases learning rate scaling schedules estimate better parameter dependent update step. returnn many well known learning rate schedules implemented including adagrad adadelta adam furthermore returnn allows classical momentum term also simpliﬁed nesterov accelerated gradient decreasing learning rate training done based validation error. particular noise addition norm constraints outlier detection mechanisms allow better convergence avoid numerical instabilities. note batches gradients scaled returnn dimension batch direct inﬂuence norm gradients. regularization possible using dropout layer inputs layer penalizing large norms weight matrices. returnn mostly written python parts extended modules using cuda follows object-oriented design. layer described section used base class extend package functional elements. layer hereby considered black reads batch sequences writes batch sequences possibly different shape. order avoid inﬂuences zero-padding multiple sequences different lengths processed together index tensor indicates time step batch whether frame considered part sequence not. layer deﬁnition kind theano expression. layer class provided list incoming layers index tensor. layer expected create tensor time ﬁrst batch index second layer output size third dimension. likewise layer list incoming layers provide member fig. processing pipeline multi-gpu training. initial parameter passed workers. workers make consecutive updates without synchronizing parameters every update. processing three batches workers send current parameter estimate back process combined single parameters. used perform feature extraction within rasr passing resulting inputs network real-time. also provide method send output activations network rasr order perform decoding retrieve error signal calculated based discriminative training criteria available rasr. multi-gpu training modern machines consist several gpus cards deﬁnes isolated computation system. computation systems used independent sub-batch processors. unfortunately library internally allows handle single device context. therefore chose implement multi-gpu functionality interaction several independent system processes similar attached sub-process. main process scheduled access real network parameters. data batches processed user speciﬁed number batches assigned device corresponding data copied memory. main process provides image current network parameters workers apply updates asynchronously batch batch. processing speciﬁc amount batches workers send modiﬁed network parameters back main process combined single parameters averaging. overall process depicted figure processes communicate sockets using simple self-deﬁned protocol. weight matrices transferred serialized arrays signiﬁcantly slows training workers synchronized often. however experiments observe stable convergence even synchronize epoch. fact often observe regularizing effect measure better generalization error keeping gpus asynchronous several hundred batches. cuda kernels lstm layers noticed straightforward lstm implementation theano using scan efﬁcient terms speed memory. therefore chose implement lstm kernels directly using cuda cublas non-recurrent part lstm fortable comparison runtime memory requirement different software packages. numbers averaged training epochs. note precise memory usage estimates torch tensorflow obtained internal memory management. toolkit returnn theano lstm keras tensorflow torch newly written layer class directly executed using json description variables corresponding json object passed arguments constructor layer. data handling dataset abstracted generic interface. dataset provide multiple inputs output targets variable dimensionality shape inputs outputs encoded sparsely. wide range dataset implementations. prominently support hierarchical data format also used format models produced returnn. moreover features rasr directly used within returnn described section release software contains several examples dataset usages. demonstrate performance returnn framewise labeled speech data chime dataset show returnn successively converges training compare implementations. frame consists consecutive speaker-adapted -dimensional mfcc vectors reduced dimensions lda. vectors labeled tied allophone states using viterbi alignment obtained previously trained hidden markov model. segments average length frames variance frames. ease processing recurrent neural networks divided observation sequences constant chunks frames padding zero-frames required. compare returnn keras torch tensorflow. torch recently published cudnn based lstm implementation also planned migrated theano based frameworks. package measure average runtime average memory consumption relative number misclassiﬁed frames training data. training performed chunks parallel nvidia three bidirectional lstm layers used experiments containing units direction. similar conﬁguration achieved word error rate development evaluation corpus. seen table internal lstm kernel returnn outperforms competitors except cudnn implementation runtime memory usage. order provide direct comparison lstm implementations also present runtime returnn lstm version make optimized lstm kernels internal memory management tensorflow torch make difﬁcult obtain exact measurements memory usage less memory required lstm kernel compared theano based kernels including keras. also conducted experiments evaluate runtime classiﬁcation performance returnn multiple gpus. here training time epoch seconds four nvidia gpus respectively. evolution frame error rate corresponding minima shown figure convergence time signiﬁcantly reduced using multiple devices. observe smoothing effect model averaging system trained four gpus achieved lowest frame error experiment. presented returnn highly conﬁgurable training framework neural networks. software based theano cuda provides fast training procedures recurrent neural networks upon others. includes rich functional layers applied network designs using convenient json syntax. returnn successfully used several recent evaluation campaigns including read handwriting competition icfhr iwslt german speech transcription task chime speech separation recognition challenge ranked ﬁrst ﬁrst second respectively. providing training framework allows train neural networks minimal conﬁguration effort hope increase interest research area allow people access methods. returnn downloaded institute’s website freely available academic research purposes. work partially supported intelligence advanced research projects activity department defense u.s. army research laboratory contract wnf--c-. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon. disclaimer views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied iarpa dod/arl u.s. government. additionally research partially supported ford motor company deutsche forschungsgemeinschaft contract schl/-. also would like thank authors theano tools enabling work. has¸im andrew senior franc¸oise beaufays long shortterm memory based recurrent neural network architectures large vocabulary speech recognition arxiv preprint arxiv. http//arxiv.org/pdf/.. albert zeyer patrick doetsch paul voigtlaender ralf schl¨uter hermann comprehensive study deep bidirectional lstm rnns acoustic modeling speech recognition international conference acoustics speech signal processing oriol vinyals alexander toshev samy bengio dumitru erhan show tell neural image caption generator proceedings ieee conference computer vision pattern recognition fr´ed´eric bastien pascal lamblin razvan pascanu james bergstra goodfellow arnaud bergeron nicolas bouchard yoshua bengio theano features speed improvements deep learning unsupervised feature learning nips workshop james bergstra olivier breuleux fr´ed´eric bastien pascal lamblin razvan pascanu guillaume desjardins joseph turian david wardefarley yoshua bengio theano math expression compiler proceedings python scientiﬁc computing conference june yangqing evan shelhamer jeff donahue sergey karayev jonathan long ross girshick sergio guadarrama trevor darrell caffe convolutional architecture fast feature embedding arxiv preprint arxiv. david rybach stefan hahn patrick lehnen david nolden martin sundermeyer zoltan t¨uske simon wiesler ralf schl¨uter hermann rasr rwth aachen university open source speech recognition toolkit ieee asru waikoloa dec. simon wiesler alexander richard pavel golik ralf schl¨uter hermann rasr/nn rwth neural network toolkit speech recognition ieee international conference acoustics speech signal processing florence italy bart merri¨enboer dzmitry bahdanau vincent dumoulin dmitriy serdyuk david warde-farley chorowski yoshua bengio blocks fuel frameworks deep learning arxiv preprint arxiv. mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg man´e rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda vi´egas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng tensorflow large-scale machine learning heterogeneous systems software available tensorﬂow.org. yangqing evan shelhamer jeff donahue sergey karayev jonathan long ross girshick sergio guadarrama trevor darrell caffe convolutional architecture fast feature embedding proceedings international conference multimedia. daniel povey arnab ghoshal gilles boulianne lukas burget ondrej glembek nagendra goel mirko hannemann petr motlicek yanmin qian petr schwarz silovsky georg stemmer karel vesely kaldi speech recognition toolkit ieee workshop automatic speech recognition understanding marconi martigny dec. number idiap-rr-- ieee signal processing society ieee catalog cfpsrw-usb. group hierarchical data format version paul voigtlaender patrick doetsch hermann handwriting recognition large multidimensional long short-term memory recurrent neural networks international conference frontiers handwriting recognition shenzhen china oct. patrick doetsch michal kozielski hermann fast robust training recurrent neural networks ofﬂine handwriting recognition international conference frontiers handwriting recognition crete greece sept. jeffrey dean greg corrado rajat monga chen matthieu devin mark marc'aurelio ranzato andrew senior paul tucker yang quoc andrew large scale distributed deep networks advances neural information processing systems pereira burges bottou weinberger eds. alex graves j¨urgen schmidhuber ofﬂine handwriting recognition multidimensional recurrent neural networks neural information processing systems foundation ilya sutskever james martens george dahl geoffrey hinton importance initialization momentum deep learning proceedings international conference machine learning icml june barker ricard marxer emmanuel vincent shinji watanabe third ’chime’ speech separation recognition challenge dataset task baselines ieee workshop automatic speech recognition understanding scottsdale", "year": 2016}