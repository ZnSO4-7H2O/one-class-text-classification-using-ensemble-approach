{"title": "On the relation between accuracy and fairness in binary classification", "tag": ["cs.LG", "cs.AI"], "abstract": "Our study revisits the problem of accuracy-fairness tradeoff in binary classification. We argue that comparison of non-discriminatory classifiers needs to account for different rates of positive predictions, otherwise conclusions about performance may be misleading, because accuracy and discrimination of naive baselines on the same dataset vary with different rates of positive predictions. We provide methodological recommendations for sound comparison of non-discriminatory classifiers, and present a brief theoretical and empirical analysis of tradeoffs between accuracy and non-discrimination.", "text": "study revisits problem accuracyfairness tradeoff binary classiﬁcation. argue comparison non-discriminatory classiﬁers needs account different rates positive predictions otherwise conclusions performance misleading accuracy discrimination naive baselines dataset vary different rates positive predictions. provide methodological recommendations sound comparison nondiscriminatory classiﬁers present brief theoretical empirical analysis tradeoffs between accuracy non-discrimination. discrimination-aware machine learning emerging research area studies make predictive models free discrimination historical data built biased incomplete even contain past discriminatory decisions. research assumes protected grounds discrimination forbidden given legislation. goal machine learning develop algorithmic techniques incorporating non-discriminatory constraints predictive models. discrimination-aware manumber chine learning data mining focus achieving equal acceptance rates favored protected groups individuals binary classiﬁcation. forcing acceptance rates equal without taking account characteristics individuals seen afﬁrmative action introduces positive discrimination promoting protected community. desired legal political reasons. machine learning identify pitfalls avoid comparing performance classiﬁers comparison misleading proportions positive predictions classiﬁers different. provide methodological recommendations sound comparison present brief theoretical empirical analysis tradeoffs accuracy non-discrimination. given dataset contains discrimination goal build classiﬁer would accurate possible obey non-discrimination constraints. example model could decide upon granting loan given demographic information ﬁnancial standing considering ethnicity applicant protected ground. assume values target variable historical dataset objectively correct e.g. whether loan repaid not. discrimination happen target variable needs polar outcome preferred denote input variables denote protected characteristic foreign denote target variable reject classiﬁer maps even though among input variables variables correlated result classiﬁer capture protected characteristics induce indirect discrimination decision making. discrimination measured difference rates acceptance suppose discrimination historical dataset desired discrimination classiﬁer output proportion favored individuals data prior probability acceptance data rate acceptance classiﬁer output sion) random classiﬁer assigns labels random. therefore better observed accuracy necessarily mean better classiﬁcation ability acceptance rates classiﬁers different. order able compare classiﬁers could normalize accuracy respect therefore suggest using comparison normalized accuracy cohen’s kappa indicates much classiﬁer question better random classiﬁer right plot discrimination varies different acceptance rates. discrimination everybody accepted nobody accepted closer acceptance rate gets extremes smaller better fairness classiﬁer classiﬁer exactly same output same classiﬁcation threshold varies. would like assess fairness classiﬁer therefore similarly accuracy need normalize result respect propose normalize maximum possible dmax discrimination would maximum classiﬁer ranks candidates ﬁrst everyone favored community accepted candidates protected community start accepted. case maximum discrimination could consider accuracy measures imbalanced data f-score. prefer cohen’s kappa since f-score behave consistently extreme acceptance rates therefore difﬁcult interpret. f-score classiﬁer accepts everybody would equal varies depending dataset kappa always gives case. compared evacuation procedure titanic. passengers queue ﬁrst class passengers priority third class passengers. many passengers evacuated boats. threshold default typically positive decision. considering available resources decision maker choose different threshold. suppose objective keep discrimination desired level time maximize prediction accuracy. effectively choosing threshold decision maker chooses acceptance rate performance discrimination-aware classiﬁers typically compared plotting discrimination accuracy. attempt remove discrimination easily produce classiﬁers different acceptance rates original dataset especially using off-the-shelve classiﬁer implementations simply round numerical probability scores without constraints positive output rates. evaluation nonmain message discriminatory classiﬁers must take account rates acceptance otherwise classiﬁer performance comparable changing acceptance rate changes baseline accuracy baseline discrimination. small experiment benchmark dataset illustrates situation. target variable describes whether person high income low. protected characteristic among inputs. randomly split dataset halves training testing. train logistic regression train output class probability scores test vary classiﬁcation threshold changes acceptance rate also plot accuracy random classiﬁer inputs randomly decides upon outcome given probability acceptance figure presents results. value means worst possible discrimination favored community complete priority means discrimination people favored protected communities fully queue. negative indicating reverse discrimination. figure plots normalized accuracy normalized discrimination logistic regression experiment. large part discrimination appears closely line discrimination data. results make sense since classiﬁer experiment mechanisms discrimination removal. extreme ends everybody accepted everybody rejected intuitively discrimination normalized measure correctly shows discrimination. observed that assuming labels data correct discrimination removal comes cost reduces prediction accuracy. authors found given constraints acceptance rates maximum possible accuracy decreases linearly reducing difference rates acceptance. revisit problem accuracy-fairness tradeoff normalized measures would show similar relations. oracle ﬁctional baseline classiﬁer maximum possible intelligence strives satisfy non-discrimination constraints. random classiﬁer opposite intelligence. individual random classiﬁer makes random prediction probability acceptance accuracy oracle kappa discrimination would data random classiﬁer deﬁnes baseline performance random classiﬁer turns majority class classiﬁer. acceptance rate favored community increase acceptance rate protected community resulting decrease classiﬁcation accuracy would linearly proportional discrimination data rate acceptance need ﬁxed optimal strategy still either reduce acceptance favored community increase acceptance protected community choice depends also closed form solution moment figure presents simulated results oracle classiﬁer benchmark dataset change both solution acceptance rate kept original data. experiments show maximum possible accuracy given discrimination constraints. using normalized measures accuracy discrimination upper bounds remain linear. experiment compares performance three classiﬁers trained using three different strategies including protected characteristic among classiﬁer inputs excluding protected characteristic classiﬁer inputs excluding protected characteristic classiﬁer inputs plus massaging labels training data. massaging perhaps simplest discrimination removal strategy introduced training labels converted binary numeric using ranker function logistic regression discrimination. calls revision massaging possibly discrimination removal techniques taking consideration possibility different acceptance rates normalized measures discrimination. evaluation non-discriminatory classiﬁers needs take account positive output rates otherwise comparison misleading conclusions comparative performance invalid. introduced normalization factor discrimination measure considering maximum possible discrimination given acceptance rate. maximum discrimination present protected individuals start accepted everybody favored community accepted. acceptance rates constrained resources freely available choose decision makes. acceptance rate data classiﬁer outputs ﬁxed classiﬁers comparable terms otherwise need compared terms training data. number lowest ranked males positive label changed negative number highest ranked females negative label changed positive positive rate remains original data discrimination zero. classiﬁer learned modiﬁed training data. testing data modiﬁed. table presents results measured testing data. make several interesting observations. first classiﬁers tend output lower acceptance rates original data. time protected characteristic used discrimination measure show decrease nominal discrimination compared original data normalized discrimination three classiﬁers even higher data. apparently classiﬁer learned discriminatory data without protective measures ampliﬁes discrimination. removing protected characteristic indicates little improvement discrimination. called redlining effect. number features data correlated protected characteristic therefore discrimination still captured cases logistic regression decision tree still higher original dataset. interestingly massaging strategy outputs higher acceptance rates removing protected characteristic. acceptance rates massaging closer positive rates original data discrimination lower expected. suggests discrimination present training data usage protected characteristic allowed classiﬁers tend decrease acceptance rate show better nominal discrimination ﬁgures real underlying discrimination remains. finally figure presents normalized accuracies discriminations different acceptance rates. overall massaging remove discrimination many acceptance rates removal precise sometimes even overshoots introducing reverse", "year": 2015}