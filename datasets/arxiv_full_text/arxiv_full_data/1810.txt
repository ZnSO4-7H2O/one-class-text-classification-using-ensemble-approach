{"title": "Does Neural Machine Translation Benefit from Larger Context?", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "We propose a neural machine translation architecture that models the surrounding text in addition to the source sentence. These models lead to better performance, both in terms of general translation quality and pronoun prediction, when trained on small corpora, although this improvement largely disappears when trained with a larger corpus. We also discover that attention-based neural machine translation is well suited for pronoun prediction and compares favorably with other approaches that were specifically designed for this task.", "text": "paper ﬁrst attempt investigating potential implicitly incorporating discourse-level structure neural machine translation. initial attempt focus incorporating small number preceding and/or following source sentences attention-based neural machine translation model speciﬁcally instead modelling conditional distribution translations given source sentence build network models conditional distribution i-th preceding source sentence i-th following source sentence. propose novel larger-context neural machine translation model based recent works larger-context language modelling multi-way multilingual neural machine translation ﬁrst evaluate proposed model baseline model without context sentence using bleu ribes measure translation quality averaged sentences corpus. evaluation strategy reveals beneﬁt larger context always apparent evaluation metric average translation quality conﬁrming earlier instance hardmeier observation then turn focused evaluation based pronoun prediction shared task wmt’. cross-lingual pronoun prediction task notice beneﬁts incorporating larger context training models small corpora larger ones. interestingly also observe neural machine translation predict pronouns propose neural machine translation architecture models surrounding text addition source sentence. models lead better performance terms general translation quality pronoun prediction trained small corpora although improvement largely disappears trained larger corpus. also discover attention-based neural machine translation well suited pronoun prediction compares favorably approaches speciﬁcally designed task. major strength neural machine translation recently become facto standard machine translation research capability seamlessly integrating information multiple sources. nature continuous representation used within neural machine translation system information addition tokens source target sentences integrated long information projected vector space. allowed researchers build nonstandard translation system multilingual neural translation systems multimodal translation systems syntax-aware neural translation systems core recent extensions idea using context extend attention-based neural machine translation described including additional encoder attention model. additional encoder similarly bidirectional recurrent network encodes context sentence case source sentence immediately current source sentence similarly original source additional attention model different original one. goal incorporating larger context translation provide additional discourse-level information necessary translating given source token phrase. implies attention over selection tokens larger context done respect source token phrase considered. thus propose make attention model take input previous target symbol previous decoder hidden state context annotation vector well source vector main attention model. translation attention-based proposed bahdanau become facto standard recent years academia industry attention-based translation system consists decoder three components; attention model. encoder ofrecurrent network bidirectional gated recurrent unit encodes source sentence annotation vectors fatt attention model implemented feedforward network taking input previous target symbol ˆyt′− previous decoder hidden state zt′− annotation vector attention scores used compute αtt′ based decoder’s hidden state output distribution possible target symbols computed although single preceding sentence paper proposed method easily handle multiple preceding and/or following sentences either multiple sets encoder attention mechanism concatenating context sentences long single sequence. standard metric automatically evaluating translation quality machine translation system bleu bleu computed validation test corpus inspecting overlap n-grams reference generated corpora. bleu become facto standard after found correlate well human judgement phrase-based neural machine translation systems. metrics meteor often used together bleu also measure average translation quality machine translation system entire validation test corpus. well-known much positive negative effect larger context machine translation. understood larger context allows machine translation system capture properties apparent single source sentence style genre topical patterns discourse coherence anaphora degree impact average translation quality unknown. impact measured metric speciﬁcally designed evaluate speciﬁc effect larger coninstance discourse coherence text. used metrics analyzing larger-context language modelling recent years context machine translation cross-lingual pronoun prediction established tasks effect larger-context modelling ability machine translation system incorporating larger-context information evaluated. paper therefore compare vanilla neural machine translation model proposed larger-context model based average translation quality measured bleu pronoun prediction accuracy measured macro-averaged recall. order investigate relationship average translation quality pronoun prediction accuracy single corpus language pair provided part shared task crosslingual pronoun prediction pronoun prediction train models speciﬁcally pronoun prediction task train maximize average translation quality. model trained conduct pronoun prediction en-fr en-de experiments. target side parallel corpus language pair heavily preprocssed including tokenization lemmatization. although corpora come tags them. case en-fr pronouns includes elle elles cela other. consists case en-de. macroaverage recall used main evaluation metric. sentence pairs en-fr en-de training corpora respectively. pronoun prediction input model source sentence corresponding target sentence pronouns replaced special token replace. goal ﬁgure pronoun replaced replace token done ﬁnding combination maximizes log-probability multiple replace tokens single example exhaustively possible combinations feasible size pronoun small. addition data/tasks crosslingual pronoun prediction shared task also check average translation quality using iwslt’ en-de training set. iwslt’ iwslt’ test development test respectively. ensure naive model train naive attentionbased neural machine translation system based code publicly available online. dimensionalities word vectors encoder recurrent network decoder recurrent network respectively. one-layer feedforward network tanh hidden units attention model. regularize models dropout. larger-context model largercontext model closely follows conﬁguration naive model. additional encoder gru’s thus outputs -dimensional timedependent context vector time. learning train types models maximize log-likelihood given training corpus using adadelta early-stop bleu validation set. anything particular cross-lingual pronoun prediction task. varying training corpus sizes experiment varying size training corpus meaningful difference performance vanilla larger-context models w.r.t. size training set. corpora pronoun prediction task using original training set. results presented table observe larger-context models generally outperform vanilla ones terms bleu ribes macro-average recall. however improvement vanishes size training grows. conﬁrm lemmatization target side pronoun task corpora observing proposed larger-context model also outperforms vanilla iwslt ende training corpus size approxpaper proposed novel extension attention-based neural machine translation seamlessly incorporates context surrounding sentences. extensive evaluation measured terms average translation quality cross-lingual pronoun prediction revealed beneﬁt larger context moderate training sentence pairs. able observe similar level beneﬁt larger training corpus. suspect large corpus allows model capture subtle word relations source sentence alone. believe better more-focused evaluation metric necessary order properly evaluate inﬂuence discourse-level information translation.", "year": 2017}