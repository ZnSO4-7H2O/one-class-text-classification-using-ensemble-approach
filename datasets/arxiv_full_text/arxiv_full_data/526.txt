{"title": "Globally Normalized Transition-Based Neural Networks", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. We discuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models.", "text": "introduce globally normalized transition-based neural network model part-ofspeech tagging dependency parsing sentence compression results. model simple feed-forward neural network operates task-speciﬁc transition system achieves comparable better accuracies recurrent models. discuss importance global opposed local normalization insight label bias problem implies globally normalized models strictly expressive locally normalized models. taken neural processing ﬁeld storm. particular variants long short-term memory networks produced impressive results part-of-speech classic tasks tagging syntactic parsing semantic role labeling might speculate recurrent nature models enables results. work demonstrate simple feed-forward networks without recurrence achieve comparable better accuracies lstms long globally normalized. section uses transition system feature chen manning recurrence perform beam search maintaining multiple hypotheses introduce global normalization conditional random ﬁeld objective overcome label bias problem locally normalized models suffer from. since beam inference approximate partition function summing elements beam early updates compute gradients based approximate global normalization perform full backpropagation training neural network parameters based loss. section revisit label bias problem implication globally normalized models strictly expressive locally normalized models. lookahead features partially mitigate discrepancy cannot fully compensate it—a point return later. empirically demonstrate effectiveness global normalization evaluate model part-of-speech tagging syntactic dependency parsing sentence compression model achieves state-of-the-art accuracy tasks matching outperforming lstms signiﬁcantly faster. particular dependency parsing wall street journal achieve best-ever published unlabeled attachment score section also outperform previous structured training approaches used neural network transitionbased parsing. ablation experiments show outperform weiss alberti global backpropagation training model parameters neural network parameters training global part model. also outperform zhou despite using smaller beam. shed additional light label bias problem practice provide sentence compression example local model completely fails. demonstrate globally normalized parsing model without lookahead features almost accurate best model locally normalized model loses absolute accuracy cannot effectively incorporate evidence becomes available. finally provide open-source implementation method called syntaxnet integrated popular tensorflow framework. also provide pre-trained state-of-the english dependency parser called parsey mcparseface tuned balance speed simplicity accuracy. throughout work transition systems complete structures input number decisions dependency parsing example true arc-standard arc-eager transition systems sentence length number deciassume one-to-one mapping decision sequences states essentially assume state encodes entire history decisions. thus state reached unique decision sequence decision sequences states interchangeably slight abuse notation deﬁne equal state reached decision sequence dj−. parameters neural network excluding parameters ﬁnal layer. ﬁnal layer parameters decision representation state computed neural network parameters note score linear parameters next describe softmax-style normalization performed local global level. signiﬁcant practical advantange locally normalized cost local partition function derivative usually computed efﬁciently. contrast term contains many cases intractable. make learning tractable globally normalized model beam search training sequence decoded keep track location gold path beam. gold path falls beam step stochastic gradient step taken following objective contains paths beam step together gold path preﬁx straightforward derive gradients loss back-propagate gradients levels neural network deﬁning score gold path remains beam throughout decoding gradient step performed using beam decoding. intuitively would like model able revise earlier decision made search later evidence becomes available rules earlier decision incorrect. ﬁrst glance might appear locally normalized model used conjunction beam search exact search able revise earlier decisions. however label bias problem collins pages lafferty bottou lecun smith johnson means locally normalized models often weak ability revise earlier decisions. section gives formal perspective label bias problem proof globally normalized models strictly expressive locally normalized models. theorem originally proved smith johnson training data consists inputs paired gold decision sequences stochastic gradient descent negative log-likelihood data model. locally normalized model negative log-likelihood global models strictly expressive local models consider tagging problem task input sequence decision sequence first consider locally normalized model restrict scoring function access ﬁrst input symbols scoring decision return restriction soon. scoring function otherwise arbitrary function tuple hdi− deﬁne possible distributions local model obtained scores vary. similarly deﬁne possible distributions under global model. distribution function pair probability main result following theorem smith johnson subset cite michael collins source example underlying proof. note theorem refers conditional models form global local normalization. equivalence results joint models form quite different example results abney imply weighted context-free grammars probabilistic context-free grammars equally expressive. scores depend ﬁrst input symbols globally normalized model still able model data locally normalized model fails ambiguity input symbol naturally resolved next symbol observed locally normalized model able revise prediction. easy locally normalized model example allowing scores take account input symbol xi+. generally model form integer speciﬁes amount lookahead model. lookahead common practice insufﬁcient general. every amount lookahead construct examples cannot modeled locally normalized model duplicating middle input times. local model scores considers entire input capture distribution case decomposition makes independence assumptions. however increasing amount context used input comes cost requiring powerful learning algorithms potentially training data. detailed analysis tradeoffs structural features crfs powerful local classiﬁers without structural constraints liang experiments local classiﬁers unable reach performance crfs problems parsing named entity recognition structural constraints important. note nothing preclude approach makes global normalization powerful scoring functions obtaining best worlds. experiments follow make both. directly optimizing global model deﬁned works well found training model steps achieves precision much faster ﬁrst pretrain network using local objective given perform additional training steps using global objective given pretrain layers except softmax layer way. purposefully abstain complicated hand engineering input features might improve performance recipe training stage weiss model. speciﬁcally averaged stochastic gradient descent momentum tune learning rate learning rate schedule momentum early stopping time using separate held-out corpus task. tune different hyperparameters training global objective. data evaluation. conducted experiments number different datasets part english wall street journal penn treebank standard tagging splits; english treebank union multi-domain corpus containing data ontonotes corpus english version treebank updated corrected question treebank weiss conll multi-lingual shared task model conﬁguration. inspired integrated tagging parsing transition system bohnet nivre employ simple transition system uses shift action predicts current word buffer gets shifted stack. extract following features window tokens centered current focus token word cluster character n-gram length also extract predicted previous tokens. results. table compare model linear compositional characterto-word lstm model ling ﬁrst-order linear model exact inference emission features model. additionally also transition features word cluster character n-gram length endpoints transition. results ling solicited authors. local model already compares favorably methods average. using beam search locally normalized model help global normalization leads reduction relative error empirically demonstrating effect label bias. character ngrams feature important increasing average accuracy conll’ datasets absolute. shows characterlevel modeling also done simple feed-forward network without recurrence. data evaluation. corpora tagging experiments except standard parsing splits wsj. avoid over-ﬁtting development sec. tuning hyperparameters models. convert english constituency trees stanford style dependencies using version converter. english predicted tags exclude punctuation evaluation standard. conll datasets follow standard practice include punctuation evaluation. follow alberti predicted tags include k-best feature supplied predicted morphological features. report unlabeled labeled attachment scores model conﬁguration. model conﬁguration basically originally proposed chen manning reﬁned weiss particular arc-standard transition system extract features prior work words part speech tags dependency arcs labels surrounding context state well k-best tags proposed alberti hidden layers dimensions each. results. tables show ﬁnal parsing results comparison best systems literature. obtain best ever published results almost datasets including wsj. main results pretrained word embeddings weiss alberti tri-training. artiﬁcially restrict pretrained word embeddings observe modest drop uas; example training yields global model beam size even though tri-training model compares favorably reported weiss tri-training. show sec. gains attributed full backpropagation training differentiates approach weiss alberti results also signiﬁcantly outperform lstm-based approaches dyer data follow filippova large news collection used heuristically generate compression instances. ﬁnal corpus contains compression instances examples training development ﬁnal test. report per-token score per-sentence accuracy i.e. percentage instances fully match golden compressions. following filippova also human evaluation sentences raters score compressions readability informativeness scale model conﬁguration. transition system sentence compression similar tagging scan sentences left-to-right label token keep drop. extract features words tags dependency labels window tokens centered inresults. table shows sentence compression results. globally normalized model signiﬁcantly outperforms local model. beam search locally normalized model suffers severe label bias issues discuss concrete example section also compare sentence compression system filippova -layer stacked lstm uses dependency label information. lstm global model perform automatic evaluation well human ratings model roughly faster. derived proof label bias problem advantages global models. emprirically veriﬁed theoretical superiority demonstrating state-of-the-art performance three different tasks. section situate compare model previous work provide examples label bias problem practice. neural network models combined conditional random ﬁelds globally normalized models before. bottou describe global training neural network models structured prediction problems. peng non-linear neural network layer linearchain artires apply similar approach general markov network zheng introduce recurrence model huang ﬁnally combine crfs lstms. neural models limited sequence labeling tasks exact inference possible model works well exact inference intractable. transition-based parsing henderson work closest weiss watanabe sumita approaches global normalization added local model chen manning empirically weiss achieves best performance even though model keeps parameters locally normalized neural network ﬁxed trains perceptron uses activations features. model therefore limited ability revise predictions locally normalized model. table show full backpropagation training word embeddings important signiﬁcantly contributes performance model. also compared training objective perceptron-like hinge loss gold best elements beam. limited backpropagation depth training layer found negligible differences accuracy objective hinge loss respectively. however training full backpropagation accuracy higher training converged faster. zhou perform full backpropagation training like even much larger beam performance signiﬁcantly lower ours. also apply model additional tasks experiment dependency parsing. finally watanabe sumita introduce recurrent components additional techniques like maxviolation updates corresponding constituency parsing model. contrast model require recurrence specialized training. observed several instances severe label bias sentence compression task. although using beam search local model outperforms greedy inference average beam search leads local model occasionally produce empty compressions important note search errors empty compression higher probability prediction greedy inference. however expressive globally normalized model suffer limitation correctly gives empty compression almost zero probability. pakistan former leader pervez musharraf appeared court ﬁrst time treason charges. pakistan former leader pervez musharraf appeared court ﬁrst time treason charges. table example sentence compressions label bias locally normalized model leads breakdown beam search. probability compression local global models shows global model properly represent zero probability empty compression. considering tokens hence unlike full parsing model ability look ahead sentence making decision. result greedy model constraint uas; locally normalized model beam search globally normalized model thus globally normalized model gets close performance model full lookahead locally normalized model beam gives dramatically lower performance. ﬁnal experiments full lookahead globally normalized model achieves accuracy compared accuracy local model beam search. thus adding lookahead allows local model close performance global model; however still signiﬁcant difference accuracy large part label bias problem. number authors considered modiﬁed training procedures greedy models locally normalized models. daum´e introduce searn algorithm allows classiﬁer making greedy decisions become robust errors made previous decisions. goldberg nivre describe improvements greedy parsing approach makes methods imitation learning augment training methods focused set. note greedy models unlikely solve label bias problem used conjunction beam search given problem expressivity underlying model. recent work augmented locally normalized models correctness probabilities error states effectively adding step every decision probability correctness resulting structure evaluated. gives conpresented simple powerful model architecture produces state-of-the-art results tagging dependency parsing sentence compression. model combines ﬂexibility transition-based algorithms modeling power neural networks. results demonstrate feed-forward network without recurrence outperform recurrent models lstms trained global normalization. support empirical ﬁndings proof showing global normalization helps model overcome label bias problem locally normalized models suffer. would like thank ling wang training part-of-speech tagger setup emily pitler ryan mcdonald greg coppola fernando pereira tremendously helpful discussions. finally grateful members google parsing team.", "year": 2016}