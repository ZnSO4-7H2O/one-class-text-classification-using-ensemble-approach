{"title": "Learning Mixed Graphical Models", "tag": ["stat.ML", "cs.CV", "cs.LG", "math.OC"], "abstract": "We consider the problem of learning the structure of a pairwise graphical model over continuous and discrete variables. We present a new pairwise model for graphical models with both continuous and discrete variables that is amenable to structure learning. In previous work, authors have considered structure learning of Gaussian graphical models and structure learning of discrete models. Our approach is a natural generalization of these two lines of work to the mixed case. The penalization scheme involves a novel symmetric use of the group-lasso norm and follows naturally from a particular parametrization of the model.", "text": "consider problem learning structure pairwise graphical model continuous discrete variables. present pairwise model graphical models continuous discrete variables amenable structure learning. previous work authors considered structure learning gaussian graphical models structure learning discrete models. approach natural generalization lines work mixed case. penalization scheme involves novel symmetric group-lasso norm follows naturally particular parametrization model. many authors considered problem learning edge structure parameters sparse undirected graphical models. focus using regularizer promote sparsity. line work taken separate paths learning continuous valued data learning discrete valued data. however typical data sources contain continuous discrete variables population survey data genomics data url-click pairs etc. genomics data addition gene expression values attributes attached sample gender ethniticy etc. work consider learning mixed models continuous variables discrete variables. continuous variables previous work assumes multivariate gaussian model mean inverse covariance estimated graphical lasso minimizing regularized negative found friedman banerjee graphical lasso problem computationally challenging several authors considered methods related pseudolikelihood nodewise regression discrete models previous work focuses estimating pairwise markov ranr≤j φrj. maximum likelihood problem intractable models moderate large number variables requires evaluating partition function derivatives. previous work focused pseudolikelihood approach main contribution propose model connects discrete continuous models previously discussed. conditional distributions model widely adopted well understood models multiclass logistic regression gaussian linear regression. addition case discrete variables model pairwise markov random ﬁeld; case continuous variables gaussian graphical model. proposed model leads natural scheme structure learning generalizes graphical lasso. parameters occur singletons vectors blocks penalize using group-lasso norms respects symmetry model. since parameter block diﬀerent size also derive calibrated weighting scheme penalize edge fairly. also discuss conditional model allows output variables mixed viewed multivariate response regression mixed output variables. similar ideas used learn covariance structure multivariate response regression continuous output variables witten tibshirani rothman section introduce mixed graphical model discuss previous approaches modeling mixed data. section discusses pseudolikelihood approach parameter estimation connections generalized linear models. section discusses natural method perform structure learning mixed model. section presents calibrated regularization scheme section discusses consistency estimation procedures section discusses methods solving optimization problem. finally section discusses conditional random ﬁeld extension section presents empirical results census population survey dataset synthetic experiments. discrete takes states. model parameters continuouscontinuous edge potential continuous node potential continuousdiscrete edge potential discrete-discrete edge potential. model simpliﬁes multivariate gaussian case continuous variables simpliﬁes usual discrete pairwise markov random ﬁeld case discrete variables. conditional distributions graphical model critical importance. absence edge corresponds variables conditionally independent. conditional independence read conditional distribution variable others. example multivariate gaussian model conditionally independent partial correlation coeﬃcient partial correlation coeﬃcient also regression coeﬃcient linear regression variables. thus conditional independence structure captured conditional distributions regression coeﬃcient variable others. mixed model desirable property type conditional distributions simple gaussian linear regressions multiclass logistic regressions. follows pairwise property joint distribution. detail thus continuous variables conditioned discrete multivariate gaussian common covariance means depend value discrete variables. means depend additively values ρsj. marginal known form models number discrete variables sample eﬃciently. proposed model special case lauritzen’s mixed model following assumptions common covariance additive mean assumptions marginal factorizes pairwise discrete markov random ﬁeld. three assumptions full model simpliﬁes mixed pairwise model presented. although full model general number parameters scales exponentially number discrete variables conditional distributions convenient. state discrete variables mean covariance. consider example binary variables continuous variables; full model requires estimates mean vectors covariance matrices dimensions. even homogeneous constraint imposed lauritzen’s model still mean vectors case binary discrete variables. full mixed model complex cannot easily estimated data without additional assumptions. comparison mixed pairwise model number parameters allows natural regularization scheme makes appropriate high dimensional data. alternative regularization approach take paper limited-order correlation hypothesis testing method castelo authors develop hypothesis test likelihood ratios conditional independence. however restrict case discrete variables marginally independent maximum likelihood estimates well-deﬁned line work regarding parameter estimation undirected mixed models decomposable path discrete variables cannot contain continuous variables. models allow fast exact maximum likelihood estimation node-wise regressions applicable structure known also related work parameter learning directed mixed graphical models. since primary goal learn graph structure forgo exact parameter estimation pseudolikelihood. similar exact maximum likelihood decomposable models pseudolikelihood interpreted node-wise regressions enforce symmetry. negative log-likelihood convex standard gradient-descent algorithms used computing maximum likelihood estimates. major obstacle involves high-dimensional integral. since pairwise mixed model includes discrete continuous models special cases maximum likelihood estimation least diﬃcult special cases ﬁrst well-known computationally intractable problem. defer discussion maximum likelihood estimation appendix gressions used meinshausen b¨uhlmann gaussian graphical model ravikumar ising model. method thought asymmetric form pseudolikelihood since pseudolikelihood enforces parameters shared across conditionals. thus number parameters estimated separate regression approximately double pseudolikelihood expect pseudolikelihood outperforms sample sizes regularization regimes. node-wise regression used baseline method since straightforward extend mixed model. predicted pseudolikelihood joint procedure outperforms separate regressions; left figures ihler conﬁrm separate regressions outperformed pseudolikelihood numerous synthetic settings. concurrent work yang extend separate node-wise regression model special cases gaussian categorical regressions generalized linear models univariate conditional distribution categorical gaussian). specifying conditional distributions besag show joint distribution also speciﬁed. thus another justify mixed model deﬁne conditionals continuous variable gaussian linear regression conditionals categorical variable multiple logistic regression results besag arrive joint distribution however neighborhood selection algorithm yang restricted models form cannot applied edge selection pairwise mixed model categorical model greater states. baseline method separate regressions closely related neighborhood selection algorithm proposed; baseline considered generalization yang allow general pairwise interactions appropriate regularization select edges. unfortunately theoretical results yang apply baseline nodewise regression method joint pseudolikelihood. section show incorporate edge selection maximum likelihood pseudolikelihood procedures. graphical representation probability distributions absence edge corresponds conditional independency statement variables conditionally independent given variables would like maximize likelihood subject penalization number edges since results sparse graphical model. pairwise mixed model type edges figure symmetric matrix represents parameters model. example square corresponds continuous graphical model coeﬃcients solid square scalar βst. blue square corresponds coeﬃcients solid blue square vector parameters parameters correspond edge grouped indicator function. problem non-convex replace sparsity group sparsity penalties appropriate convex relaxations. scalars absolute value vectors norm matrices frobenius norm. choice corresponds standard relaxation group group norm group penalties treated equals irrespective size group. suggest calibration weighting scheme balance load equitable way. introduce weights group parameters represents parameter groups corresponding weight. viewed generalized residual diﬀerent groups diﬀerent dimensions—e.g. scalar/vector/matrix. even independence model might expect section study model selection consistency correct edge selected parameter estimates close truth pseudolikelihood maximum likelihood. consistency established using framework ﬁrst developed ravikumar later extended general m-estimators proofs section omitted since follow straightforward application results results stated mixed model show certain conditions estimation procedures model selection consistent. also consider uncalibrated regularizers simplify notation straightforward adapt calibrated regularizer case. diﬃcult establish consistency results problem equation parameters non-identiﬁable. constant respect change variables similarly cannot hope recover popular issue drop last level indicators levels instead levels. allows model identiﬁable results asymmetric formulation treats last level diﬀerently levels. instead maintain symmetric formulation introducing constraints. consider problem group regularizer implicitly enforces constraints optimization problems equation equation solutions. theoretical results constrained formulation equation since identiﬁable. restricted strong convexity standard assumption ensures parameter uniquely determined value likelihood function. without this hope accurately estimating stated subspace much smaller irrepresentable condition stringent condition. intuitively requires active variables overly dependent inactive variables. although exact form condition enlightening known almost necessary model selection consistency lasso common assumption works establish model selection consistency also deﬁne constants appear theorem lipschitz constants log-partition function. twice continuously diﬀerentiable functions gradient hessian locally lipschitz continuous ball radius around remark theorem applies maximum likelihood pseudolikelihood estimators. maximum likelihood constants tightened; everywhere appears replaced theorem remains true. however values diﬀerent methods. maximum likelihood gradient log-partition hessian log-likelihood depend samples. thus constants completely determined likelihood. pseudolikelihood values depend samples theorem applies assumptions made sample quantities; thus theorem less useful practice applied pseudolikelihood. similar situation yang assumptions made sample quantities. section discuss algorithms solving proximal gradient proximal newton methods. convex optimization problem decomposes form smooth convex convex possibly non-smooth. case negative log-likelihood negative log-pseudolikelihood group sparsity penalties. block coordinate descent frequently used method non-smooth function group especially easy apply function quadratic since block coordinate update solved closed form many diﬀerent non-smooth smooth particular case quadratic block update cannot solved closed form. however certain problems update approximately solved using appropriate inner optimization routine group sparsity penalties considered proximal operator takes familiar form soft-thresholding group soft-thresholding since groups non-overlapping proximal operator simpliﬁes scalar soft-thresholding group soft-thresholding φrj. class proximal gradient accelerated proximal gradient algorithms directly applicable problem. algorithms work solving ﬁrstorder model current iterate determined line search. theoretical convergence rates properties proximal gradient algorithm accelerated variants well-established accelerated proximal gradient method achieves linear convergence rate objective strongly convex sublinear rate non-strongly convex problems. tfocs framework package allows experiment diﬀerent variants accelerated proximal gradient algorithm. tfocs authors found auslender-teboulle algorithm exhibited less oscillatory behavior proximal gradient experiments next section done using auslender-teboulle implementation tfocs. section borrows heavily schmidt schmidt class proximal newton algorithms order analog proximal gradient algorithms quadratic convergence rate attempts incorporate order information smooth function model function. iteration minimizes quadratic model centered norm. simpliﬁes proximal operator general case positive deﬁnite closed-form solution many common nonsmooth however proximal operator available sub-problems solved eﬃciently proximal gradient. case separable coordinate descent also applicable. theoretical analysis suggests proximal newton methods generally require fewer outer iterations ﬁrstorder methods providing higher accuracy incorporate order information. conﬁrmed empirically proximal newton methods faster large gradient expensive compute since objective quadratic coordinate descent also applicable subproblems. hessian matrix replaced quasi-newton approximation bfgs/l-bfgs/sr. implementation pnopt implementation frequently machine learning statistics regularization parameter heavily dependent dataset. generally chosen cross-validation holdout performance convenient provide solutions interval start algorithm λmax solve using previous solution warm start λmin. reduces cost ﬁtting entire path solutions λmax chosen smallest value parameters using equations suggested theoretical shown figure chosen results section experimental results recovery correct edge undergoes sharp phase transition expected. samples pseudolikelihood recovering correct edge probability nearly phase transition experiments done using proximal newton algorithm discussed section census survey dataset consider consists variables continuous discrete log-wage year sexmarital status race education level geographic region class health health insurance dataset assembled steve miller openbi.com march supplement current population survey data. evaluations done using holdout test size survey experiments. regularization parameter varied interval figure figure shows graph used synthetic experiments experiment used blue nodes continuous variables nodes binary variables orange green dark blue lines represent types edges. figure plot probability correct edge recovery given sample size using maximum likelihood pseudolikelihood. results averaged trials. figure model selection diﬀerent training sizes. circle denotes lowest test negative pseudolikelihood number parentheses number edges model lowest test negative pseudolikelihood. saturated model edges. figure study model selection performance learning graphical model variables diﬀerent training samples sizes. sample size increases optimal model increasingly dense less regularization needed. sensible baseline method compare separate regression algorithm. algorithm linear gaussian logistic regression variable conditioned rest. evaluate performance optimizing loss function expected better. pseudolikelihood objective similar half number parameters separate regressions since coeﬃcients shared conditional likelihoods. figures pseudolikelihood performs similarly separate regressions sometimes even outperforms regression. beneﬁt pseudolikelihood learned parameters using conditional model model variables logwage education jobclass. variables used features. conditional model trained using pseudolikelihood. compare generative model learns joint distribution variables. figure conditional model outperforms generative model except small sample sizes. expected since conditional distribution models less variables. small sample sizes small generative model outperforms conditional model. likely generative models converge faster discriminative models optimum. maximum likelihood estimates computable small models conditional model previously studied. pseudolikelihood originally motivated approximation likelihood computationally tractable. compare maximum likelihood maximum pseudolikelihood diﬀerent evaluation criteria negative likelihood negative figure separate regression pseudolikelihood y-axis appropriate regression loss response variable. levels regularization small training sizes pseudolikelihood seems overﬁt less; global regularization eﬀect ﬁtting joint distribution opposed separate regressions. figure separate regression pseudolikelihood y-axis appropriate regression loss response variable. large sample sizes separate regressions pseudolikelihood perform similarly. expected since nearing asymptotic regime. figure conditional model generative model various sample sizes. y-axis test performance evaluated negative pseudolikelihood conditional model. conditional model outperforms full generative model except smallest sample size figure maximum likelihood pseudolikelihood. y-axis negative pseudolikelihood. y-axis bottom negative likelihood. pseudolikelihood outperforms maximum likelihood across experiments. pseudolikelihood. figure pseudolikelihood outperforms maximum likelihood negative likelihood negative pseudolikelihood. would expect pseudolikelihood trained model better pseudolikelihood evaluation maximum likelihood trained model better likelihood evaluation. however found pseudolikelihood trained model outperformed maximum likelihood trained model evaluation criteria. although asymptotic theory suggests maximum likelihood eﬃcient pseudolikelihood analysis applicable ﬁnite sample regime misspeciﬁed model. liang jordan asymptotic analysis pseudolikelihood maximum likelihood well-speciﬁed model. also observed pseudolikelihood slightly outperforming maximum likelihood synthetic experiment figure would like thank percy liang rahul mazumder helpful discussions. work consistency follows collaboration yuekai jonathan taylor. jason supported department defense national defense science engineering graduate fellowship program national science foundation graduate research fellowship program stanford graduate fellowship. trevor hastie partially supported grant dms- national science foundation grant ro-eb- national institutes health.", "year": 2012}