{"title": "Optimizing Differentiable Relaxations of Coreference Evaluation Metrics", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Coreference evaluation metrics are hard to optimize directly as they are non-differentiable functions, not easily decomposable into elementary decisions. Consequently, most approaches optimize objectives only indirectly related to the end goal, resulting in suboptimal performance. Instead, we propose a differentiable relaxation that lends itself to gradient-based optimisation, thus bypassing the need for reinforcement learning or heuristic modification of cross-entropy. We show that by modifying the training objective of a competitive neural coreference system, we obtain a substantial gain in performance. This suggests that our approach can be regarded as a viable alternative to using reinforcement learning or more computationally expensive imitation learning.", "text": "deal challenge optimize directly non-differentiable metrics using reinforcement learning example relying reinforce policy gradient algorithm however approach successful which suggested clark manning possibly discrepancy sampling decisions training time choosing highest ranking ones test time. successful alternative using ‘roll-out’ stage associate cost possible decisions clark manning computationally expensive. imitation learning though also exploiting metrics requires access expert policy exact policies directly computable metrics interest. work combining best worlds proposing simple method turn popular coreference evaluation metrics differentiable functions model parameters. show function computed recursively using scores individual local decisions resulting simple efﬁcient estimation procedure. idea replace nondifferentiable indicator functions corresponding posterior probabilities computed model. consequently non-differentiable funcset size tions used within metrics become differp). though assume scores underlying statistical model used deﬁne probability model show serious limitation. speciﬁcally baseline probabilistic version neural mention-ranking coreference evaluation metrics hard optimize directly nondifferentiable functions easily decomposable elementary decisions. consequently approaches optimize objectives indirectly related goal resulting suboptimal performance. instead propose differentiable relaxation lends gradient-based optimisation thus bypassing need reinforcement learning heuristic modiﬁcation cross-entropy. show modifying training objective competitive neural coreference system obtain substantial gain performance. suggests approach regarded viable alternative using reinforcement learning computationally expensive imitation learning. coreference resolution task identifying mentions refer entity document. shown beneﬁcial many natural language processing applications including question answering information extraction often regarded prerequisite text understanding task. coreference resolution regarded clustering problem cluster corresponds single entity consists mentions given text. consequently natural evaluate predicted clusters comparing ones annotated human experts exactly standard metrics contrast state-of-theart systems optimized make individual comodel wiseman outperforms original achieves similar performance global version importantly introduced differentiable relaxations training observe substantial gain performance probabilistic baseline. interestingly absolute improvement higher reported clark manning using using reward rescaling suggests method provides viable alternative using reward rescaling. outline paper follows introduce neural resolver baseline metrics section method turn mention ranking resolver entity-centric resolver presented section proposed differentiable relaxations section section shows experimental results. section introduce neural mention ranking framework underpins current stateof-the-art models speciﬁcally consider probabilistic version method proposed wiseman experiments baseline. list mentions document. mention index mention coreferent standard coreference resolution literature refer antecedent then mention ranking goal score antecedents mention higher mentions i.e. scoring function require coreferent not. reward rescaling technique computes error values heuristic loss function based reward difference best decision according current model decision leading highest metric score. error types false anaphor false wrong link mistake respectively. experiments borrow values durrett klein subsequent discussion refer loss mention-ranking heuristic cross entropy. evaluation. however least discriminative metric whereas ceaf slow compute popular metrics incorporate loss addition integrate shown provide good balance discriminativity interpretability. mention-ranking resolvers explicitly provide information entities/clusters required lea. therefore propose simple solution turn mention-ranking resolver entity-centric one. first note document containing mentions potential entities ﬁrst mention. probability mention corresponds entity show computed recursively based follows words consider possible coreferent correspond entity link considered mi’s self-link. probability zero impossible assigned entity introduced later. figure extra information. ﬁrst question answered proposition second question important because intuitively mention anaphoric potential entity exist. show answer proving proposition probability anaphoric always higher probability refers proposition valid probability entity-centric heuristic cross entropy loss computed consider coreference resolution multiclass prediction problem. entity-centric heuristic cross entropy loss thus given below correct entity belongs peγ). similar mention-ranking heuristic loss section cost function used manipulate contribution four different error types functions used computing size function link function link. non-differentiable metrics nondifferentiable. thus need make functions differentiable. su|d linkd computed similarly constraint mentions taken account. plugging functions precision recall section obtain differentiable ˆfβb fβlea used loss functions hyper-parameter regularization terms. worth noting that ˆfβb ˆfβlea fβlea. therefore training model proposed losses start high temperature anneal small non-zero temperature. however experiments annealing left future work. demonstrate proposed differentiable train coreference resolver. source code trained models available https//github.com/ lephong/diffmetric_coref. experiments english portion conll data consists documents various domains formats. split provided conll shared task used. resolvers original features wiseman slight modiﬁcation described wiseman where deﬁned section potential entity ﬁrst mention. replacing indicator function probability distribution differentiable version size function link function train resolvers adagrad minimize loss functions learning rate tuned development one-document mini-batches. note baseline initialization point train three resolvers. results ﬁrstly compare resolvers wiseman wiseman results shown ﬁrst half table baseline surpasses wiseman likely using features wiseman using entity-centric heuristic cross entropy loss relaxations clearly beneﬁcial slightly better baseline global model wiseman lβ=b lβ=lea outperform baseline global model wiseman lec. however best values respectively lβlea. among .lea achieves highest comparing clark manning absolute improvement baselines higher reward rescaling much shorter training time conll metric clark manning ours respectively. worth noting absolute scores weaker clark manning build similar stronger mention-ranking baseline employs deeper neural networks requires much larger number epochs train purpose illustrating proposed losses started simpler model wiseman requires figure example predictions subscript before mention index. superscript subscript mention indicates antecedent predicted baseline lβ=b mentions color true coreferents. mark incorrect decisions. table shows breakdown errors made baseline resolvers development set. proposed resolvers make fewer false anaphor wrong link errors false errors compared baseline. suggests loss optimization prevents over-clustering driving precision antecedents difﬁcult detect self-link chosen. increases make false anaphor wrong link errors less false errors. figure baseline lβ=b mistakenly links under-clustering hand problem resolvers example lβ=b missed behaviour results reduced recall recall damaged severely still obtain better score. conjecture behaviour consequence using score objective undesirable used instead. correctly instance also figure detects non-anaphoric links discussion resolvers evaluated score metrics lβlea perform best figure table however conﬁrm that values little larger hypotheses. first statistical difference training development leads case optimal suboptimal set. second experiments meaning relaxations might close true evaluation metrics enough. future work conﬁrm/reject this annealing i.e. gradually decreasing tial however expect would outperform lβlea metric would around metric. turns that behave quite similarly non-extreme cases. figure moosavi strube mention ranking entity centricity main streams coreference resolution literature. mention ranking considers local independent decisions choosing correct antecedent mention. approach computationally efﬁcient currently dominant state-of-the-art performance wiseman propose simple neural networks compute mention ranking scores heuristic loss train model. wiseman extend employing lstms compute mention-chain representations used compute ranking scores. call representations global features. clark manning build similar resolver wiseman much stronger thanks deeper neural networks better mention detection effective hyperparameters epochs training. furthermore using reward rescaling achieve best performance literature english chinese portions conll dataset. work built upon mention ranking turning mentionranking model entity-centric one. worth noting although model proposed wiseman mentionranking models employed. entity centricity hand incorporates entitylevel information solve problem. approach top-down haghighi klein propose generative model. also bottom-up merging smaller clusters bigger ones clark manning method proposed greedily incrementally adds mentions previously built clusters using prune-and-score technique. importantly employing imitation learning methods optimize resolvers directly evaluation metrics. work similar sense resolvers incrementally mentions previously built clusters. however different clark manning resolvers discrete decisions instead seamlessly compute probability mention refers entity mentionranking probabilities optimized differentiable relaxations evaluation metrics. using differentiable relaxations evaluation metrics work related line research reinforcement learning nondifferentiable action-value function replaced differentiable critic critic trained close true action-value function possible. technique applied machine translation evaluation metrics non-differentiable. disadvantage using critics guarantee critic converges true evaluation metric given ﬁnite training data. contrast differentiable relaxations need train convergence guaranteed would like thank raquel fern´andez wilker aziz naﬁse sadat moosavi anonymous reviewers suggestions comments. project supported european research council dutch national science foundation amazon services grant.", "year": 2017}