{"title": "Scene-centric Joint Parsing of Cross-view Videos", "tag": ["cs.CV", "cs.AI"], "abstract": "Cross-view video understanding is an important yet under-explored area in computer vision. In this paper, we introduce a joint parsing framework that integrates view-centric proposals into scene-centric parse graphs that represent a coherent scene-centric understanding of cross-view scenes. Our key observations are that overlapping fields of views embed rich appearance and geometry correlations and that knowledge fragments corresponding to individual vision tasks are governed by consistency constraints available in commonsense knowledge. The proposed joint parsing framework represents such correlations and constraints explicitly and generates semantic scene-centric parse graphs. Quantitative experiments show that scene-centric predictions in the parse graph outperform view-centric predictions.", "text": "views. multi-camera setups common real-word surveillance systems large-scale cross-view activity dataset available privacy security reasons. makes data-demanding deep learning approaches infeasible. joint parsing framework computes hierarchy spatio-temporal parse graphs establishing cross-reference entities among different views inferring semantic attributes scene-centric perspective. example fig. shows parse graph hierarchy describes scene people playing ball. ﬁrst view person action grounded cluttered background detected second view. viewcentric parse graph contains local recognition decisions individual view scene centric parse graph summaries comprehensive understanding scene coherent knowledge. structure individual parse graph fragment induced ontology graph regulates domain interests. parse graph hierarchy used represent correspondence entities multiple views cross-view video understanding important underexplored area computer vision. paper introduce joint parsing framework integrates view-centric proposals scene-centric parse graphs represent coherent scene-centric understanding cross-view scenes. observations overlapping ﬁelds views embed rich appearance geometry correlations knowledge fragments corresponding individual vision tasks governed consistency constraints available commonsense knowledge. proposed joint parsing framework represents correlations constraints explicitly generates semantic scene-centric parse graphs. quantitative experiments show scene-centric predictions parse graph outperform view-centric predictions. past decades remarkable progress made many vision tasks e.g. image classiﬁcation object detection pose estimation. recently comprehensive visual tasks probe deeper understanding visual scenes under interactive multi-modality settings visual turing tests visual question answering addition discriminative tasks focusing binary categorical predictions emerging research involves representing ﬁnegrained relationships visual scenes unfolding semantic structures contexts including caption description generation question answering paper present framework uncovering semantic structure scenes cross-view camera network. central requirement resolve ambiguity establish cross-reference among information multiple cameras. unlike images videos shot single static point view cross-view settings embed rich physical geometry constraints overlap ﬁelds ∗hang yuanlu yuan contributed equally paper. work supported muri project darpa award n--- copyright association advancement artiﬁcial intelligence rights reserved. scene. probabilistic model incorporate various constraints parse graph hierarchy formulate joint parsing inference problem. mcmc sampling algorithm dynamic programming algorithm used explore joint space scene-centric viewcentric interpretations optimize optimal solutions. quantitative experiments show scene-centric parse graphs outperforms initial view-centric proposals. contributions. contributions work threefold uniﬁed hierarchical parse graph representation cross-view person action attributes recognition; stochastic inference algorithm explores joint space scene-centric view-centric interpretations efﬁciently starting initial proposals; joint parse graph hierarchy interpretable representation scene events. multi-view video analytics. typical multi-view visual analytics tasks include object detection cross-view tracking action recognition person re-identiﬁcation reconstruction heuristics appearances motion consistency constraints used regularize solution space methods focus speciﬁc multi-view vision task whereas propose general framework jointly resolve wide variety tasks. semantic representations. semantic expressive representations developed various vision tasks e.g. image parsing scene reconstruction human-object interaction pose attribute estimation paper representation also falls category. difference model deﬁned upon cross-view spatio-temporal domain able incorporate variety tasks. interpretability. automated generation explanations regarding predictions long rich history artiﬁcial intelligence. explanation systems developed wide range applications including simulator actions robot movements object recognition images approaches rule-based suffer generalization across different domains. recent methods including proxy models data interpret black models scene-centric parse graphs explicit representations knowledge deﬁnition. jects captured network cameras. ﬁrst introduce concept ontology graph domain deﬁnitions describe parse graphs parse graph hierarchy view-centric scene-centric representations respectively. ontology graph. deﬁne scope representation scenes events ontology used describe plausible objects actions attributes. deﬁne ontology graph contains nodes representing objects parts actions attributes respectively edges representing relationships nodes. specifically every object part node concrete type object detected videos. edges object part nodes encodes part-of relationships. action attribute nodes connected object part node represent plausible actions appearance attributes object take. example fig. shows ontology graph describes domain including people vehicles bicycles. object decomposed parts enriched actions attributes edges among action nodes denote incompatibility. ontology graph considered compact without compositional relationships event hierarchy. paper focus restricted domain inspired larger ontology graphs easily derived large-scale visual relationship datasets open-domain knowledge bases parse graphs. ontology describes plausible elements subset concepts true given instance given time. example person cannot standing sitting time plausible actions person take. distinguish plausible facts satisﬁed facts node grounded associated data. therefore subgraph ontology graph contains grounded nodes used represent speciﬁc instance speciﬁc time. paper refer subgraphs parse graphs. view-centric parse graph camera time nodes parse graph. p|v) node likelihood concept represented node grounded data fragment practice probability approximated normalized detection classiﬁcations scores prior distribution parse graphs regulates combination nodes transitions probability scene-centric parse graphs across time. probability distributions estimated training se|gt) deﬁned gibbs distribution quences. models compatibility scene-centric view-centric parse graphs hierarchy energy decomposed four different terms described detail subsection below. weights tuning parameters learned crossvalidation. consider view-centric parse graphs videos different cameras independent conditioned scene-centric parse graph assumption cameras ﬁxed known locations. view captures incomplete facts scene spatio-temporal hierarchy parse graphs represent collective knowledge scene individual views. concrete view-centric parse graph contains nodes grounded video sequence captured individual camera whereas scene-centric parse graph aggregation view-centric parse graphs therefore reﬂects global understanding scene. illustrated fig. time step scene-centric parse graph connected corresponding view-centric parse graphs indexed views scene-centric graphs regarded markov chain temporal sequence. terms notations paper tilde notation represent view-centric concepts corresponding scene-centric concepts task joint parsing infer spatio-temporal parse graph hierarchy input frames video sequences captured network cameras object identity mapping scene-centric parse graph view-centric parse graphs camera deﬁnes structure parse graph hierarchy. section discuss formulation assuming ﬁxed structure defer discussion traverse solution space section formulate inference parse graph hierarchy inference problem posterior distribution follows inference process consists sub-steps matching object nodes scene-centric view-centric parse graphs estimating optimal values parse graphs ˜g}. overall procedure follows ﬁrst obtain viewcentric objects actions attributes proposals pretrained detectors video frames. forms initial view-centric predictions ˜g}. next markov chain monte carlo sampling algorithm optimize parse graph structure given ﬁxed parse graph hierarchy variables within scene-centric viewcentric parse graphs efﬁciently estimated dynamic programming algorithm. steps performed iteratively convergence. stochastic algorithm traverse solution space parse graph hierarchy satisfy detailed balance condition deﬁne three reversible operators follows. merging. merging operator groups view-centric parse graph view-centric parse graph creating scene-centric parse graph connects two. operator requires operands describe objects type either different views view non-overlapping time intervals. swapping. swapping operator swaps viewcentric parse graphs. view swapping operator shortcut merging splitting combined. deﬁne proposal distribution uniform distribution. iteration generate structure proposal applying three operators respect probability respectively. generated proposal accepted respect acceptance rate metropolis-hastings algorithm p|x) posterior deﬁned eqn. given ﬁxed parse graph hierarchy need estimate optimal value node within parse graph. illustrated fig. frame scene-centric node corresponding view-centric nodes form star model whole scene-centric nodes regarded markov chain temporal order. therefore proposed model essentially directed acyclic graph infer optimal node values simply apply standard factor graph belief propagation algorithm. appearance similarity. object node parse graph keep appearance descriptor. appearance energy regulates appearance similarity object scene-centric parse graph view-centric parse graphs. appearance feature vector object. view-level feature vector extracted pretrained convolutional neural networks; scene level mean pooling view-centric features. spatial consistency. time point every object scene ﬁxed physical location world coordinate system appears image plane camera according camera projection. object node parse graph hierarchy keep scene-centric location object scene-centric parse graphs view-centric location image plane viewcentric parse graphs. following energy deﬁned enforce spatial consistency perspective transform maps person’s view-centric foot point coordinates world coordinates ground plane scene camera homography obtained intrinsic extrinsic camera parameters. action compatibility. among action object part nodes scene-centric human action predictions shall agree human pose observed individual views different viewing angles action node scene-centric parse graphs positions human parts view-centric parse graph. practice separately train action classiﬁer predicts action classes joint positions human parts uses classiﬁcation score approximate probability. attribute consistency. cross-view sequences entities observed multiple cameras shall consistent attributes. energy term models commonsense constraint scene-centric human attributes shall agree observation individual views setup datasets evaluate scene-centric joint-parsing framework tasks including object detection multi-object tracking action recognition human attributes recognition. object detection multi-object tracking tasks compare published results. action recognition human attributes tasks compare performance view-centric proposals without joint parsing scene-centric predictions joint parsing well additional baselines. following datasets used cover variety tasks. campus dataset contains video sequences four scenes captured four cameras. different multi-view video datasets focusing solely multi-object tracking task videos campus dataset contains richer human poses activities moderate overlap ﬁelds views cameras. addition tracking annotation campus dataset collect annotation includes action categories attribute categories evaluating action attribute recognition. kitchen dataset action recognition dataset contains video sequences captured cameras overlapping views. focusing imagery inputs framework modalities motion capturing rfid reader signals magnetic sensor signals used inputs experiments. evaluate detection tracking task compute human bounding boxes motion capturing data projecting human poses image planes cameras using intrinsic extrinsic parameters provided dataset. evaluate human attribute tasks annotate human attribute categories every subject. evaluation object detection tracking. fasterrcnn create initial object proposals video frames. detection scores used likelihood term eqn. joint parsing objects initially detected certain views projected object’s scene-centric positions camera matrices. joint parsing extract bounding boxes grounded object nodes view-centric parse graph compute multi-object detection accuracy precision concretely accuracy measures faction correctly detected objects among ground-truth objects precision computed fraction true-positive predictions among output predictions. predicted bounding considered match ground-truth intersection union score greater prediction overlaps ground-truth extracting bounding boxes viewcentric parse graphs grounded grouping according identity correspondence different views obtain object trajectories identity matches across multiple videos. evaluation compute four major tracking metrics multi-object tracking accuracy multi-object track precision number identity switches number fragments higher value lower value idsw frag indicate tracking method works better. report quantitative comparisons several published methods table results performance measured tracking metrics comparable published results. conjecture appearance similarity main drive establish cross-view correspondence additional semantic attributes proved limited gain tracking task. pickup putdown throw catch overall reach taking lower release opendoor closedoor opendrawer closedrawer overall methods view-centric baseline-vote baseline-mean scene-centric obtained fully-connected neural network hidden layers neurons predicts action labels using human pose. campus dataset collect additional annotations human action classes pickup putdown throw catch total examples. kitchen dataset evaluate action categories reaching takingsomething lowering releasing opendoor closedoor opendrawer closedrawer. measure individual accuracies category well overall accuracies across categories. table shows performance scene-centric predictions view-centric proposals additional fusing strategies baselines. concretely baseline-vote strategy takes action predictions multiple views outputs label majority voting baseline-mean strategy assumes equal priors cameras outputs label highest averaged probability. evaluating scenecentric predictions project scene-centric labels back individual bounding boxes calculate accuracies following procedure evaluating view-centric proposals. joint parsing framework demonstrates improved results aggregates marginalized decisions made individual views also encourages solutions comply tasks. fig. compares confusion matrix view-centric proposals scene-centric predictions joint parsing campus dataset. understand effect multiple views break classiﬁcation accuracies number cameras persons observed observing entity cameras generally leads better performance many conﬂicting observations also cause degraded performance. fig. shows success failure examples. human attribute recognition. follow similar procedure action recognition case above. additional annotations different types human attributes collected campus kitchen dataset. viewcentric proposals score obtained attribute grammar model measure performance average precisions attribute categories well mean average precision human attribute literatures. scene-centric predictions projected bounding boxes views calculating precisions. table shows quantitative comparisons view-centric scene-centric predictions. baseline fusing strategies action recognition task used. scene-centric prediction outperforms original proposals categories remains comparable others. notably campus dataset harder standard human attribute datasets occlusions limited scales humans irregular illumination conditions. runtime initial view-centric proposals precomputed minute scene shot cameras containing round entities algorithm performs frames second average. optimization proposed method real-time. note although proposed framework uses sampling-based method using view-based proposals initialization warm-starts sampling procedure. therefore figure success failure examples view-centric scene-centric predictions action attribute recognition tasks. failure examples true labels bracket. occluded means locations objects parts projected scene locations therefore viewcentric proposals generated. better viewed color. overall runtime signiﬁcantly less searching entire solution space scratch. problems larger size efﬁcient mcmc algorithms adopted. example mini-batch acceptance testing technique demonstrated several order-of-magnitude speedups. represent joint parsing framework computes hierarchy parse graphs represents comprehensive understanding cross-view videos. explicitly specify various constraints reﬂect appearance geometry correlations among objects across multiple views correlations among different semantic properties objects. experiments show joint parsing framework improves view-centric proposals produces accurate scenecentric predictions various computer vision tasks. explicit parsing end-to-end training paradigm appealing many data-rich supervised learning scenarios extension leveraging loosely-coupled pre-trained modules exploring commonsense constraints helpful large-scale training data available expensive collect practice. example many applications robotics human-robot interaction domains share underlying perception units scene understanding object recognition etc. training every scenarios entirely could exponential number possibilities. leveraging pre-trained modules explore correlation constraints among treated factorization problem space. therefore explicit joint parsing scheme allows practitioners leverage pre-trained modules build systems expanded skill scalable manner. interpretable interface. joint parsing framework provides comprehensive scene-centric understanding scene moreover sence-centric spatio-temporal parse graph representation interpretable interface computer vision models users. particular consider following properties explainable interface shall apart correctness answers relevance agent shall recognize intent humans provide information relevant humans’ questions intents. self-explainability agent shall provide information interpreted humans answers derived. criterion promotes humans’ trust intelligent agent enables sanity check answers. consistency answers provided agents shall consistent throughout interaction humans across multiple interaction sessions. random nonconsistent behaviors cast doubts confusions regarding agent’s functionality.", "year": 2017}