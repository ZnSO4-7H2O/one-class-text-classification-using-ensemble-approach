{"title": "Optimal Convergence for Distributed Learning with Stochastic Gradient  Methods and Spectral-Regularization Algorithms", "tag": ["stat.ML", "cs.AI", "cs.LG", "math.FA"], "abstract": "We study generalization properties of distributed algorithms in the setting of nonparametric regression over a reproducing kernel Hilbert space (RKHS). We first investigate distributed stochastic gradient methods (SGM), with mini-batches and multi-passes over the data. We show that optimal generalization error bounds can be retained for distributed SGM provided that the partition level is not too large. We then extend our results to spectral-regularization algorithms (SRA), including kernel ridge regression (KRR), kernel principal component analysis, and gradient methods. Our results are superior to the state-of-the-art theory. Particularly, our results show that distributed SGM has a smaller theoretical computational complexity, compared with distributed KRR and classic SGM. Moreover, even for non-distributed SRA, they provide the first optimal, capacity-dependent convergence rates, considering the case that the regression function may not be in the RKHS.", "text": "study generalization properties distributed algorithms setting nonparametric regression reproducing kernel hilbert space ﬁrst investigate distributed stochastic gradient methods mini-batches multi-passes data. show optimal generalization error bounds retained distributed provided partition level large. extend results spectral-regularization algorithms including kernel ridge regression kernel principal component analysis gradient methods. results superior state-of-the-art theory. particularly results show distributed smaller theoretical computational complexity compared distributed classic sgm. moreover even non-distributed provide ﬁrst optimal capacity-dependent convergence rates considering case regression function rkhs. statistical learning theory input-output pairs unknown distribution observed. learn function used predict future outputs given corresponding inputs. quality predictor often measured terms mean-squared error. case conditional mean called regression function optimal among measurable functions nonparametric regression problems properties function estimated known priori. nonparametric approaches adapt complexity problem hand good results. kernel methods common nonparametric approaches learning based choosing rkhs hypothesis space design learning algorithms. appropriate reproducing kernel rkhs used approximate smooth function. classical algorithms perform learning task regularized algorithms kernel principal component regression generally sra. point view inverse problems approaches amount solving empirical linear operator equation empirical covariance operator replaced regularized here regularization term used controlling complexity solution over-ﬁtting ensuring best generalization ability. statistical results generalization error developed another type algorithms perform learning tasks based iterative procedure kind algorithms empirical objective function optimized iterative explicit constraint penalization regularization overﬁtting realized early-stopping empirical procedure. statistical results generalization error regularization roles number iterations/passes investigated gradient methods accelerated gradient methods conjugate gradient methods incremental gradient methods sgm. interestingly viewed special instances statistical results well studied algorithms; however algorithms suﬀer computational burdens least order nonlinearity kernel methods sample size. indeed standard execution requires space time -iterations requires space general. approaches would prohibitive dealing large-scale learning problems especially case data cannot stored single machine. thus motivate study distributed learning algorithms basic idea distributed learning simple randomly divide dataset size subsets equal size compute independent estimator using ﬁxed algorithm subset average local solutions global predictor. interestingly distributed learning technique successfully combined generally shown statistical results generalization error retained provided number partitioned subsets large. moreover highlighted distributed allows handle large datasets restored multiple machines also leads substantial reduction computational complexity versus standard approach performing samples. paper study distributed multi-passes data mini-batches. algorithm combination distributed learning technique randomly partitions dataset size subsets equal size computes independent estimator subset averages local solutions global predictor. several free parameters step-size mini-batch size total number iterations partition level show appropriate choices algorithmic parameters optimal generalization error bounds achieved provided partition level large. proposed conﬁguration certain advantages computational complexity. example without considering benign properties studied problem regularity regression function capacity assumption rkhs even implementing single machine distributed optimal convergence rate order computational complexity space time compared space time classic performing samples space time distributed krr. moreover approach dovetails naturally parallel distributed computation guaranteed superlinear speedup parallel processors proof main results based similar error decomposition decomposes excess risk three terms bias sample computational variance. error decomposition allows study distributed distributed simultaneously. diﬀerent rely heavily intrinsic relationship square loss paper integral operator approach used combining novel reﬁned analysis. byproduct derive optimal statistical results generalization error non-distributed improve results extend analysis distributed derive similar optimal results generalization error distributed sra. proof distributed involves estimation statistical error bounds distributed using integral operator approach. thus extend analysis distributed distributed based fact special instance bauer derived results distributed improve literature less strict condition number partitioned subsets. moreover ﬁrst statistical result distributed supervised learning setting considering non-attainable case results ﬁrst ones optimal capacity-dependent rates even nondistributed kpcr thus ﬁlls theoretical since remainder paper organized follows. section introduces supervised learning setting. section describes distributed numerical realization presents theoretical results generalization error distributed following simple comments discussions. section introduces distributed gives statistical results generalization error. section discusses compares results related work. section provides proofs distributed sgm. finally proofs auxiliary lemmas results distributed provided appendix. kernel methods based choosing hypothesis space rkhs. recall reproducing kernel symmetric function positive semideﬁnite ﬁnite points {ui} reproducing kernel deﬁnes rkhs completion linear span respect inner product section ﬁrst state distributed study discuss numerical realization. present theoretical results generalization properties distributed nondistributed following simple discussions. throughout paper assume that sample size positive integers randomly decompose |zm| write study following distributed mini-batches multi-pass data. ﬁrst positive integers denoted algorithm b-minibatch stochastic gradient methods sample current solution subtracting scaled gradient estimate. easy gradient estimate iteration s-th local estimator unbiased estimate full gradient empirical risk global predictor average local solutions. special case algorithm reduces classic multi-pass studied parameters aﬀect algorithm’s generalization properties computational complexity. coming subsection show parameters chosen algorithm generalize optimally long number subsets large. diﬀerent choices correspond diﬀerent regularization strategies. paper particularly interested cases ﬁxed universal constants depend local sample size tuned. total number iterations local estimator bigger local sample size means algorithm data once another words algorithm multiple passes data. follows number ‘passes’ data referred iterations algorithm. ﬁnite subsets denote kernel matrix )]x∈xx∈x kxx. obviously using inductive argument prove algorithm equivalent simulations rkhs associated gaussian kernel exp| mini-batch size number partitions step-size suggested corollary coming subsection executed simulation times. trial approximated generalization error computed empirical measure points. mean standard deviation computed generalization errors trials respect number passes depicted figure ﬁgures distributed performs well number passes achieves minimal generalization error. number subsets increases error number passes reach minimal error also slightly increase. note computational cost iteration global estimator thus total computational cost algorithm reach minimal error would reduced section state theoretical results generalization error distributed following simple discussions. need introduce basic assumptions. throughout paper make following basic assumptions. result provides generalization error bounds distributed diﬀerent choices step-size mini-batch size total number iterations/passes. convergence rate optimal logarithmic factor sense matches minimax rate convergence rate number passes achieve optimal error bounds cases roughly one. result asserts distributed generalizes optimally pass data diﬀerent choices step-size mini-batch size provided according computational complexities space time comparing space time classic sgm. learning problem regularity regression function capacity rkhs. follows show convergence rate improved make benign assumptions learning problem. ﬁnite rank condition holds case refereed capacity independent case. smaller allows deriving faster convergence rates studied algorithms shown following results. result consider setting ﬁxed step-size. results decaying step-size directly derived following proofs coming sections combining basic estimates derived error bound depends number iteration step-size mini-batch size number sample points right-hand side inequality composed three terms. ﬁrst term related regularity parameter regression function results estimating bias. second term depends sample size results estimating sample variance. last term results estimating computational variance random choices sample points. comparing error bounds derived classic performed local machine averaging local solutions reduce sample computational variances keeps bias unchanged. number iteration increases bias term decreases sample variance term increases. so-called trade-oﬀ problem statistical learning theory. solving trade-oﬀ problem leads best choice number iterations. notice computational variance term independent number iterations depends step-size mini-batch size partition level. derive optimal rates necessary choose small step-size and/or large minibatch size suitable partition level. follows provide diﬀerent choices algorithmic parameters corresponding diﬀerent regularization strategies leading optimal convergence rates. comments theorem. first convergence rate optimal also matches minimax rate logarithmic factor. second distributed saturates reason averaging local solutions reduce sample computational variances bias. similar saturation phenomenon also observed analyzing distributed third condition equivalent assuming learning problem diﬃcult. believe condition necessary applying distributed learning technique reduce computational costs means reduce computational costs learning problem easy. fourth learning problem becomes easier faster convergence rate moreover larger number partition finally diﬀerent parameter choices leads diﬀerent regularization strategies. ﬁrst regimes step-size mini-batch size ﬁxed prior constants number iterations depends unknown distribution parameters. case regularization parameter number iterations practice tuned using cross-validation methods. besides step-size number iterations third regime mini-batch size number iterations last regime depend unknown distribution parameters regularization eﬀects. theorem asserts distributed diﬀerently suitable choices parameters generalize optimally provided partition level large. results provide generalization error bounds multi-pass trained single dataset. derived convergence rate optimal minimax sense note saturation eﬀect section ﬁrst state distributed sra. present theoretical results generalization error distributed following simple discussions. finally give convergence results single dataset. above regularization parameter appropriately chosen order achieve best performance. practice tuned using cross-validation methods. associated given ﬁlter functions. diﬀerent ﬁlter functions correspond diﬀerent regularization algorithms. following examples provide several speciﬁc choices ﬁlter functions leads diﬀerent types regularization methods e.g. results provide generalization error bounds distributed sra. upper bound depends number partition regularization parameter total sample size regularization parameter setting derived error bounds simpliﬁed terms upper bound. raised estimating bias sample variance. note trade-oﬀ bias term sample variance term. solving trade-oﬀ leads best choice regularization parameter. note also similar distributed distributed also saturates convergence rate corollary optimal matches exactly minimax rate better rate distributed theorem latter extra logarithmic factor. according corollary distributed appropriate choice regularization parameter generalize optimally number partitions large. best knowledge corollary ﬁrst optimal statistical result distributed considering nonattainable case moreover requirement number results assert generalizes optimally regularization parameter well chosen. best knowledge derived result ﬁrst optimally capacitydependent rates non-attainable case general sra. note unlike distributed classic saturation eﬀect. ﬁrst brieﬂy review convergence results generalization error generally sra. statistical results diﬀerent convergence rates shown e.g. without considering capacity assumption. caponnetto vito rely special separable properties square loss. statistical results generalization error diﬀerent convergence rates shown e.g. best convergence rate shown widely used convex optimization machine learning e.g. references therein follows brieﬂy recall recent works generalization error nonparametric regression rkhs considering square loss. term online learning algorithm mean one-pass sample used once. diﬀerent variants either without regularization studied. take form expectation shown. convergence rates capacity-independently optimal take capacity assumption account. considering averaging step proof technique motivated dieuleveut bach proved meet challenge large-scale learning line research focus designing learning algorithms nystr¨om subsampling generally sketching. interestingly latter also applied compressed sensing rank matrix recovery kernel methods e.g. references therein. basic idea nystr¨om subsampling replace standard large matrix smaller matrix obtained subsampling kernel methods nystr¨om subsampling successfully combined generalization error bounds order derived provided subsampling level suitably chosen considering case computational advantages algorithms highlighted. here summarize convergence rates computational costs table distributed advantages memory time. another line research large-scale learning focus distributed learning. distributed learning based divide-and-conquer approach used e.g. perceptron-based algorithms parametric smooth convex optimization problems sparse regression recently approach successfully applied learning algorithms kernel methods zhang ﬁrst provided partition level large. number partition retain optimal rate shown distributed depends conditions less well understood thus potentially leads suboptimal partition number. provided alternative reﬁned analysis distributed leading less strict condition partition number. extended analysis distributed comparison condition partition number theorem distributed less strict. moreover theorem shows distributed retain optimal rate even non-attainable case. according corollary distributed appropriate choices parameters achieve optimal rate partition number large. comparison derived results distributed distributed table latter advantages memory time. related works zinkevich studied distributed optimization problems ﬁnite-dimensional domain proved convergence results assuming objective function strongly convex. jain considered distributed averaging least square regression problems ﬁnite-dimension space proved certain convergence results depend smallest eigenvalue covariance matrix. results apply cases consider distributed multi-pass nonparametric regression rkhs objective function strongly convex. ﬁnally remark using partition approach also scale kernel methods computational advantage similar using distributed learning technique. introduce inclusion operator covariance operator furthermore consider adjoint operator given easily proved kxdρx operators proved positive trace class operators fact denotes expectation random variable denotes supreme norm respect given bounded operator denotes operator norm i.e. supf∈hfh lfh. could another separable hilbert space diﬀerent denote random variables {jsi}b+≤i≤bt jsbt} {j··· note js··· jsbt conditionally indeerror decomposition similar given classic multi-pass sgm. three terms right-hand side ﬁrst term depends regularity regression function called bias. second term depends noise level called sample variance. last term caused random estimates full gradients called computational variance. following subsections estimate three terms separately. total error bounds thus derived substituting estimates error decomposition. throughout paper assume step-size sequence satisﬁes non-negative notational simplicity throughout rest subsection drop index ﬁrst local estimator whenever using inequality according lemma ﬁlter function indexed regularization parameter qualiﬁcation positive number using lemma spectral theorem following results. proofs lemmas based concentration result hilbert space valued random variable proof lemma based concentration inequality norms self-adjoint operators hilbert space completeness give proofs appendix. a|x|− notational simplicity denote a|x|δγ prove choice ensures condition satisﬁed thus probability least holds. obviously easily prove using therefore according lemma know sample variance averaging local estimators well controlled terms sample variance local estimator. throughout rest subsection shall drop index ﬁrst local estimator whenever shows i.e. rewrite etc. proposition assumption follows estimate local computational variance i.e. subsections drop index s-th local estimator whenever shows ﬁrst introduce following lemmas whose proof found also received funding european research council european union’s horizon research innovation programme lemma x··· sequence independently identically distributed selfadjoint hilbert-schmidt operators separable hilbert space. assume almost surely positive trace-class operator constant bounded assumption hypothesis space sequence generated local sample given sequence generated distributed i.e. minibatch size maximal number iterations random index uniform distribution performing s-th local sample random indices t-th iteration performing s-th local sample random indices performing s-th local sample iterations random indices distributed iterations expectation respect random variables expectation respect random variables expectation respect random variables sequence step-sizes positive constants assumption hilbert space square integral functions respect regression function deﬁned parameters related ‘regularity’ parameters related eﬀective dimension sequence generated respect s-th local sample sequence generated distributed sequence generated pseudo s-th local sample sequence generated distributed pseudo sequence generated population inclusion adjoint operator sρs∗ operator covariance operator sampling operator r|x| adjoint operator empirical covariance operator πk=t+ pseudo regularization parameter tx˜λ estimator deﬁned s-th local sample algorithm estimator deﬁned distributed algorithm estimator deﬁned pseudo s-th local sample estimator deﬁned distributed pseudo function deﬁned population", "year": 2018}