{"title": "Inferring Generative Model Structure with Static Analysis", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Obtaining enough labeled data to robustly train complex discriminative models is a major bottleneck in the machine learning pipeline. A popular solution is combining multiple sources of weak supervision using generative models. The structure of these models affects training label quality, but is difficult to learn without any ground truth labels. We instead rely on these weak supervision sources having some structure by virtue of being encoded programmatically. We present Coral, a paradigm that infers generative model structure by statically analyzing the code for these heuristics, thus reducing the data required to learn structure significantly. We prove that Coral's sample complexity scales quasilinearly with the number of heuristics and number of relations found, improving over the standard sample complexity, which is exponential in $n$ for identifying $n^{\\textrm{th}}$ degree relations. Experimentally, Coral matches or outperforms traditional structure learning approaches by up to 3.81 F1 points. Using Coral to model dependencies instead of assuming independence results in better performance than a fully supervised model by 3.07 accuracy points when heuristics are used to label radiology data without ground truth labels.", "text": "obtaining enough labeled data robustly train complex discriminative models major bottleneck machine learning pipeline. popular solution combining multiple sources weak supervision using generative models. structure models aﬀects training label quality diﬃcult learn without ground truth labels. instead rely weak supervision sources structure virtue encoded programmatically. present coral paradigm infers generative model structure statically analyzing code heuristics thus reducing data required learn structure signiﬁcantly. prove coral’s sample complexity scales quasilinearly number heuristics number relations found improving standard sample complexity exponential identifying degree relations. experimentally coral matches outperforms traditional structure learning approaches points. using coral model dependencies instead assuming independence results better performance fully supervised model accuracy points heuristics used label radiology data without ground truth labels. deep neural networks complex discriminative models rely large amount labeled training data success. many real-world applications obtaining magnitude labeled data expensive time consuming aspects machine learning pipeline. recently generative models used create training labels various weak supervision sources heuristics knowledge bases modeling true class label latent variable. necessary parameters generative models learned using unlabeled data distribution true labels inferred. properly specifying structure generative models essential estimating accuracy supervision sources. traditional structure learning approaches focused supervised case previous works related weak supervision assume structure user-speciﬁed recently bach showed possible learn structure models sample complexity scales sublinearly number possible binary dependencies. however sample complexity scales exponentially higher degree dependencies limiting ability learn complex dependency structures. moreover time required learn dependencies also grows exponentially degree dependencies hindering development user-deﬁned heuristics. poses problem many domains high degree dependencies common among heuristics operate shared inputs. inputs interpretable characteristics extracted data. example various approaches computer vision bounding segmentation attributes like location size weakly supervise complex image-based learning tasks another example comes medical imaging domain attributes include characteristics area intensity perimeter tumor shown figure note attributes heuristics written encoded programmatically. typically relatively small interpretable characteristics heuristics often share attributes. results high order dependency structures among sources crucial model generative model learns accuracies sources. eﬃciently learn higher order dependencies present coral paradigm statically analyzes source code weak supervision sources infer rather learn complex relations among heuristics. coral’s sample complexity scales quasilinearly number relevant dependencies. moreover time identify relations constant degree dependencies since requires looking source code heuristic. speciﬁcally coral analyzes code used generate weak supervision heuristics inputs heuristics share inputs. information used generate dependency structure heuristics generative model learns proper weights structure assign probabilistic labels training data. experimentally validate performance coral across various domains show outperforms traditional structure learning various conditions signiﬁcantly computationally eﬃcient. show modeling dependencies leads improvement points compared standard structure learning approaches. additionally show coral assign labels data ground truth labels augmented training results improving discriminative model performance points. complex relation-based image classiﬁcation task heuristic functions written using object label location primitives able train model comes within points score achieved fully-supervised model trained rich hand-labeled attribute relation information visual genome database coral paradigm takes input domain-speciﬁc primitives programmatic user-deﬁned heuristic functions operate primitives. formally deﬁne abstractions section coral runs static analysis source code creates primitives heuristic functions identify sets heuristics related virtue sharing primitives coral identiﬁes dependencies uses factor graph model relationship heuristics primitives true class label. describe conditions coral learn structure generative model signiﬁcantly less data traditional approaches section show aﬀects generative model accuracy simulations. finally discuss coral learns accuracies heuristic outputs probabilistic labels training data figure running example coral paradigm. users apply standard algorithms segment tumors x-ray extract domain-speciﬁc primitives image segmentation. write heuristic functions primitives output noisy label image. generative model takes inputs provides probabilistic training labels discriminative model. coral abstractions domain-speciﬁc primitives domain-speciﬁc primitives coral simplest elements heuristic functions take input operate over. dsps coral semantic meaning making interpretable users. akin concept language primitives programming languages smallest unit processing meaning. motivation making dsps domain-speciﬁc instead general construct various data modalities allow users take advantage existing work ﬁeld extracts meaningful characteristics data. figure shows example pipeline bone tumor classiﬁcation malignant benign inspired real experiments. first automated segmentation algorithm used generate binary mask tumor then extract three dsps segmentation area perimeter total intensity segmented area. complex characteristics texture shape edge features also used deﬁne formal construct programmatically encoding dsps. users generate dsps coral primitive speciﬁer function create_primitives figure speciﬁcally function takes input single unlabeled data point returns instance primitiveset maps primitive names primitive values like integers note p.ratio composed primitives rest generated independently image segmentation. coral heuristic functions viewed mapping subset heuristic functions dsps noisy label training data shown figure experience user-deﬁned note mostly nested if-then statements statement checks whether value single primitive combination user-set threshold shown figure take input ﬁelds object return label based value input primitives. running example focuses single data point generation construction procedures applied entire training assign noisy labels data point. static dependency analysis since number dsps domains relatively small multiple operate dsps shown figure figure share least primitive trivially related other. prior work learns dependencies using labels assign data points probability success scales amount data available however learn dependencies among pairs eﬃciently since data required grows exponentially degree relation. turn limits complexity dependency structure method accurately learn model. heuristic function inputs coral takes advantage fact users write known ﬁnite primitives. infers dependencies exist among simply looking source code dsps constructed. process requires data successfully learn dependencies making computationally eﬃcient standard approaches. order determine whether share least coral looks input since take input operate over simply grouping primitives share eﬃcient approach recognizing dependencies. shown running example would result coral recognizing dependencies among since input three diﬀerent this however would incorrect since primitive p.ratio composed p.perimeter p.intensity makes related. therefore along looking primitives takes input also essential model primitives composed. primitive compositions running example figure explain coral gathers information compositions. coral builds abstract syntax tree represent computation create_primitives function performs. represents operations involving primitives tree shown figure primitive compositions coral ﬁrst ﬁnds expressions primitives then assignment expression coral traverses subtree rooted assignment expression adds encountered primitives dependency p.name. primitives encountered subtree primitive registered independent rest. composition structure results traversing shown figure p.area p.intensity p.perimeter independent p.ratio composition. heuristic function dependency structure knowledge dsps composed return original method looking inputs hfs. before identify p.area p.perimeter respectively. however know uses p.ratio composition p.intensity p.perimeter. implies related takes either p.intensity p.perimeter inputs. proceed build relational structure among dsps. shown figure structure shows independent dsps operates over. relational structure implicitly encodes dependency information edge points primitive n-way relation virtue sharing primitive. dependency information formally encoded factor graph shown figure discussed next section. note chose particular programmatic setup creating dsps explain static analysis infer dependencies; however process modiﬁed work setups encode dsps well. figure shows encoded hfs. shows dependency structure nodes edge going inputs shows factor graph coral uses model relationship dsps latent class label creating generative model describe generative model used predict true class labels. coral model uses factor graph model relationship primitives heuristic functions latent class label show incorporating information primitives shared across static analysis factor graph infers dependencies heuristics guaranteed present. next section describe coral recovers additional dependencies heuristics studying empirical relationships primitives. modeling heuristic function dependencies dependencies inferred static analysis goal learn accuracies assign labels training data accordingly. factor graph thus consists types factors accuracy factors φacc factors static analysis φhf. static analysis factors ensure heuristics correctly evaluated based dependencies found previous section. ensure probability zero given conﬁguration correct value given primitives depends static analysis factors deﬁned occur takes input dsps takes input dsps always value. although static analysis would pick share primitive dependency true independent. occur depend diﬀerent primitives primitives happen always value. case impossible static analysis infer dependency primitives diﬀerent names generated independently described section realistic example comes running example would expect area perimeter tumor related. account cases necessary capture possible dependencies occur among dsps ensure dependencies static analysis misspecify factor graph. introduce factor account additional dependencies among primitives φdsp. many possible choices dependency factor simple choice model pairwise similarity primitives. allows dependency factor represented inferring dependencies without data factors capture dependencies among heuristic functions represented φdsp factor. dependencies represented φdsp factor precisely dependencies cannot inferred static analysis fact factor depends solely content primitives. therefore impossible determine factor without data. assuming true φdsp seems like strong condition real-world experiments including φdsp factor rarely leads improvements case include φacc factors. experiments bounding location size object labels domain-speciﬁc primitives image video querying tasks. since primitives correlated modeling primitive dependency lead improvement modeling dependencies static analysis. moreover experiments modeling relation among primitives helps observe relatively small beneﬁts modeling dependencies provides therefore without data possible model important dependencies among lead signiﬁcant gains case dependencies modeled. generating probabilistic training labels given probability distribution factor graph goal learn proper weights θacc θsim coral adopts structure learning approaches described recent work learns dependency structures weak supervision setting maximizes -regularized marginal pseudolikelihood primitive learn weights relevant factors. learn weights generative model contrastive divergence maximum likelihood estimation routine maximize marginal likelihood observed primitives. gibbs sampling used estimate intractable gradients used stochastic gradient descent. typically deterministic functions primitives standard gibbs sampling able properly. result modify gibbs sampler simultaneously sample primitive along heuristics depend despite fact true class label latent process still converges correct parameter values additionally amount data necessary learn parameters scales quasilinearly number parameters. case number parameters number heuristics number relevant primitive similarity dependencies formally state conditions result match ratner provide sample complexity method. first assume exists feasible parameter known contain parameter models true distribution data next must able accurately learn provided labeled samples true distribution. speciﬁcally must asymptotically unbiased estimator takes labeled data independently sampled proposition suppose stochastic gradient descent produce estimates weights setup satisfying conditions then ﬁxed error number unlabeled data points least log] expected parameter error bounded maximizing marginal likelihood observed primitives fully speciﬁed factor graph complete generative model used predict latent class label. data point domain-speciﬁc primitives heuristic functions compute noisy labels. accuracy factors estimate distribution latent class label noisy labels train discriminative model. figure present simulation empirically compare sample complexity structure learning simulation accuracy explore settings exists binary -ary -ary dependency among hfs. dependent share exactly primitive primitives independent increases methods improve performance improved estimates heuristic accuracies dependencies. case binary dependency structure learning recovers necessary dependency samples similar performance coral. contrast second third settings high-order dependencies structure learning struggles recover relevant dependency consistently performs worse coral even training data provided. seek experimentally validate following claims approach. ﬁrst claim dependencies inferred static analysis improve signiﬁcantly model take dependencies account. second compare structure learning approach weak supervision show outperform variety domains. finally show case primitive dependencies coral learn model well classiﬁcation tasks range specialized medical domains natural images video include details dsps appendix. compare approach generative models learn accuracies diﬀerent heuristics speciﬁcally assumes heuristics independent bach learns inter-heuristic dependencies also compare majority vote fully supervised case measure performance discriminative model trained labels generated using methods. visual genome activitynet classiﬁcation explore extract complex relations images videos given object labels bounding boxes. used subsets datasets visual genome activitynet deﬁned task ﬁnding images person biking road ﬁnding basketball videos respectively. tasks small dsps shared heavily among modeling dependencies observed static analysis signiﬁcant improvement independent case. since dependencies involved groups heuristics coral improved signiﬁcantly points structure learning well unable model dependencies lack enough data. moreover modeling primitive dependencies help since primitives indeed independent report results tasks terms score harmonic mean precision recall. bone tumor classiﬁcation used labeled bone tumor x-ray images along radiologist-drawn segmentations. deﬁned task diﬀerentiate aggressive non-aggressive tumors generated combination hand-tuned rules decision-tree generated rules discriminative model utilized hand-tuned features domain-speciﬁc primitive statistics. discriminative model improvement hf+dsp methods. *improvements shown terms score rest terms accuracy. activitynet model using vggnet embeddings features. overlap features dsps) encoded various shape texture edge intensity-based characteristics. although explicitly shared primitives dataset generative model still able model training labels accurately knowledge many primitives heuristics operated over thus improving independent case signiﬁcantly. moreover small dataset size hindered structure learning gave minimal boost independent case used heuristics coral label additional images ground truth labels beat previous score points mammogram tumor classiﬁcation used ddsm-cbis dataset consists scanned mammograms associated segmentations tumors form binary masks. deﬁned task identify tumor malignant benign. heuristic operated primitive resulting dependencies static analysis could identify. case structure learning performed better coral used static analysis infer dependencies however including primitive dependencies allowed match structure learning resulting point improvement independent case need labeled training data grows common alternative utilize weak supervision sources distant supervision multi-instance learning heuristics speciﬁcally images weak supervision using object detection segmentation visual databases popular technique well estimating accuracies sources without access ground truth labels classic problem methods crowdsourcing boosting co-training learning noisy labels popular approaches combine various sources weak supervision assign noisy labels data however coral require labeled data model dependencies among domain-speciﬁc primitives heuristics interpreted workers classiﬁers views methods. recently generative models also used combine various sources weak supervision speciﬁc example data programming proposes using multiple sources weak supervision text data order describe generative model subsequently learns accuracies source. coral also focuses multiple programmatically encoded heuristics weakly label data learns accuracies assign labels training data. however coral adds additional layer domain-speciﬁc primitives generative model allows generalize beyond text-based heuristics. also infers dependencies among heuristics primitives rather requiring users specify them. previous work also assume structure generative models user-speciﬁed recently bach showed possible learn dependency structure among sources weak supervision sample complexity scales sublinearly number possible pairwise dependencies. coral instead identiﬁes dependencies among heuristic functions inspecting content programmable functions therefore relying signiﬁcantly less data learn generative model structure. moreover coral also pick higher-order dependencies structured learning needs large amounts data detect. paper introduced coral paradigm models dependency structure weak supervision heuristics systematically combines outputs assign probabilistic labels training data. described coral takes advantage programmatic nature heuristics order infer dependencies among static analysis requires sample complexity quasilinear number heuristics relations found. showed coral leads signiﬁcant improvements discriminative model accuracy traditional structure learning approaches across various domains. coral scratches surface possible ways weak supervision borrow ﬁeld programming languages especially applied large magnitudes data need encoded programmatically. look natural extension treating process encoding heuristics writing functions hope explore interactions systematic training creation concepts programming language ﬁeld. acknowledgments thank shoumik palkar stephen bach helpful conversations feedback. grateful darvin assistance ddsm dataset based experiments associated deep learning models. acknowledge bone tumor dataset annotated drs. christopher beaulieu carefully collected career late henry jones m.d. material based research sponsored defense advanced research projects agency agreement number fa---. gratefully acknowledge support darpa simplex program n--c- darpa fa--- fa--- national science foundation graduate research fellowship dge- joseph goodman stanford graduate fellowship moore foundation national institute health oﬃce naval research awards moore foundation okawa research grant american family insurance accenture toshiba intel. research supported part aﬃliate members supporters stanford dawn project intel microsoft teradata vmware. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright notation thereon. views conclusions contained herein authors interpreted necessarily representing oﬃcial policies endorsements either expressed implied darpa u.s. government. opinions ﬁndings conclusions recommendations expressed material authors necessarily reﬂect views darpa afrl u.s. government.", "year": 2017}