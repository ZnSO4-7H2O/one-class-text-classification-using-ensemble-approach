{"title": "Adaptive Representation Selection in Contextual Bandit with Unlabeled  History", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We consider an extension of the contextual bandit setting, motivated by several practical applications, where an unlabeled history of contexts can become available for pre-training before the online decision-making begins. We propose an approach for improving the performance of contextual bandit in such setting, via adaptive, dynamic representation learning, which combines offline pre-training on unlabeled history of contexts with online selection and modification of embedding functions. Our experiments on a variety of datasets and in different nonstationary environments demonstrate clear advantages of our approach over the standard contextual bandit.", "text": "consider extension contextual bandit setting motivated several practical applications unlabeled history contexts become available pre-training online decisionmaking begins. propose approach improving performance contextual bandit setting adaptive dynamic representation learning combines ofﬂine pre-training unlabeled history contexts online selection modiﬁcation embedding functions. experiments variety datasets different nonstationary environments demonstrate clear advantages approach standard contextual bandit. introduction sequential decision making common problem many practical applications agent must choose best action perform iteration order maximize cumulative reward period time. challenges achieving good trade-off exploration actions exploitation known actions. exploration exploitation trade-off sequential decision making problems often formulated multi-armed bandit problem given bandit arms associated ﬁxed unknown reward probability distribution agent selects play iteration receives reward drawn according selected arm’s distribution independently previous actions. particularly useful version contextual multi-armed bandit simply contextual bandit problem iteration choosing agent observes n-dimensional context feature vector. time goal learn relationship context vectors rewards order make better prediction action choose given context example contextual bandit approach commonly used various practical sequential decision problems side information clinical trials recommender system patient’s information online user’s proﬁle provide context making better decision treatment propose show reward represents outcome selected action example success failure particular treatment option. however certain real-life applications online decision-making starts agent access unlabeled context history potentially used prior knowledge improve subsequent online decision-making. instance medical decision-making settings doctor access medical records different patients used gain better understanding patients population. different example unlabeled context history occur online recommender setting system previous information users although reward feedback might missing. access unlabeled data makes possible pre-train model input ofﬂine mode later improve online decision making. example learn autoencoder inputs potentially better representations. moreover inputs non-homogeneous want cluster unlabeled data learn separate representations cluster. then online mode decide representation given context; context-driven representation selection potential improve subsequent decision-making. representation models continue updated online contexts become available especially nonstationary environments abundant practical applications context distribution reward distribution change various ways. motivated scenarios consider contextual bandit setting called contextual bandit representation learning unlabeled history setting assumed unlabeled contexts available pre-training online decision-making starts bandit’s performance improved learning good context representation rather using input embedding functions pre-trained unlabeled history adaptively selected based context online decision-making. next propose algorithm cbrh setting called adaptive bandit context-driven embeddings implements online clustering-based embedding selection learning coupled thompson-sampling contextual bandit approach. demonstrated empirical results multiple datasets approach consistently outperforms standard contextual bandit appears particularly beneﬁcial nonstationary environments several types involving context reward nonstationarity. related work multi-armed bandit problem extensively studied. optimal solutions provided using stochastic formulation robbins auer bayesian formulation thompson kaufmann agrawal goyal using adversarial formulation however approaches take account context affect arm’s performance. linucb contextual thompson sampling authors assume linear dependency expected reward action context; representation space modeled using linear predictors. however algorithms assume agent observe reward iteration case many practical applications including discussed earlier paper. authors studies considering kind incomplete feedback called \"partial monitoring general framework sequential decision making problems incomplete feedback allows learner possible retrieve expected value actions analysis feedback matrix assumed known learner. authors study variant stochastic multi-armed bandit problem rewards corrupted. framework motivated privacy preserving online recommender systems goal maximize rewards based observation transformation rewards stochastic corruption process known parameters. setting similar on-line semisupervised learning ﬁeld machine learning studies learning labeled unlabeled examples on-line setting. however setting receive true label iteration receive bandit feedback. features deﬁning context. denote reward vector reward time associated herein primarily focus bernoulli bandit binary reward i.e. denote policy. also denotes joint distribution assume expected reward linear function context i.e. associated contextual thompson sampling setting consider general thompson sampling reward choosing time follows parametric likelihood function following posterior distribution time given multivariate gaussian distribution vbi−) size every step algorithm generates d-dimensional sample vbi−) selects maximizes using notation introduced previous section deﬁne novel bandit setting contextual bandit representation learning unlabeled history based following assumptions. first assume context mapped representation using embedding function selected currently available embedding functions. second assume embedding functions modiﬁed online. third access unlabeled contexts i.e. contexts withassociated rewards assumed. dataset used example pre-training embedding functions deﬁne ∪ei∈e{π compound-function policies function maps action objecˆ tive learn hypothesis iterations maximizing cumulative reward. describe adaptive context-driven embedding selection approach solving cbru problem introduced previous section. variants based onlineofﬂine clustering respectively; choice controlled boolean input parameter isonline algorithm inputs include unlabeled pre-training dataset well number embeddings algorithm processes input contexts sequentially mini-batch data updates embeddings reﬂect possible changes data distribution. algorithm adaptive bandit context-dependent embeddings input unlabeled dataset unlabeled contexts pre-training; number clusters boolean variable isonline. cluster clusters cluster train autoencoder construct initialization step consists clustering pre-training dataset clusters training autoencoder cluster results encoding functions initializing parameters contextual thompson sampling bandit used later make classiﬁcation decisions based embedded context within data mini-batch next input sample arrives ﬁrst assigned existing clusters associated corresponding embedding function next online clustering performed isonline true i.e. centroid cluster recomputed changes made clusters otherwise changes clusters batch shortly. based cluster assignment corresponding embedding function used compute representation vector given input given context contextual bandit makes decision obtains reward updates parameters using contextual thompson sampling described previous section. mini-batch reached isonline false clusters recomputed scratch using data points received finally embeddings updated respectively using updated clusters universal embedding universal embedding denotes single embedding computed based data always recomputed include data recent mini-batch; clustering performed. empirical evaluation datasets evaluated approach four imaging datasets mnist stl- cifar- caltech- silhouettes- warfarin simulate online data stream draw samples dataset sequentially starting beginning time draw last sample. round algorithm receives reward instance classiﬁed correctly otherwise. compute total number classiﬁcation errors performance metric. important keep mind bandit feedback makes classiﬁcation problem signiﬁcantly challenging compared standard supervised learning since true label never revealed bandit setting unless classiﬁcation correct. thus describe details experiments. mnist took samples original test dataset pre-train encodings samples training dataset simulate online bandit arms corresponding different digits. stl- samples unlabeled data used pre-train encodings; test samples together training samples combined simulate online bandit different arms corresponding image classes. caltech- silhouettes- dataset original samples used pre-training online learning different arms cifar- dataset test samples used pre-training training samples left online bandit arms warfarin dataset test samples used pre-training training samples left online bandit arms nonstationary environments simulated several types nonstationarity using datasets. mentioned before assume input data arrive batches data distribution change across batches remaining stationary within batch. used batch size varied number embeddings using presenting average results nonstationary context varying cluster distribution simulate changes context distribution ﬁrst clustered samples corresponding pre-training data subset clusters. next generate sequence batches batch contained certain fraction samples different clusters fractions changing across batches i.e. probability distribution cluster membership changing simulating nonstationary input. nonstationary context negative images another type input nonstationarity involved introducing negative images inputs semantics different textures. namely probability negative image original image presented input. experiments performed settings half rand stationary nonstationary context conditions shufﬂed unshufﬂed rewards nonstationary reward multi-task environment another type nonstationarity assuming input samples come different domains thus associated different subsets labels example combined randomly selected training samples selected domains mnist warfarin datasets extended possible labels include labels mnist labels warfarin. used linear stretching make input dimensions equal across domains. algorithm assign label input without information domain input came from. nonstationary reward shufﬂed class labels explored multi-task setting introducing different type nonstationary reward class labels shufﬂed i.e. randomly permuted batch. results explored different combination nonstationarities. table summarizes results nonstationary context varying cluster distribution mixeddomain settings unshufﬂed reward function. three datasets baseline still outperforming embeddings. however consider mean accuracy entire experiments three algorithms were universal embedding baseline mini-batch embedding respectively suggesting advantage representation learning moreover take look whole iteration history example mnist dataset observe initially baseline considerably worse embedding-based approaches requires large number iteration ﬁnally catch them. figures show history reward accumulation stl- cifar- demonstrating baseline consistently outperformed embedding selection methods. datasets mnist half-stat mnist rand-stat mnist half-nonstat mnist rand-nonstat stl- half-stat stl- rand-stat stl- half-nonstat stl- rand-nonstat caltech- half-stat caltech- rand-stat caltech- half-nonstat caltech- rand-nonstat datasets mnist half-stat mnist rand-stat mnist half-nonstat mnist rand-nonstat stl- half-stat stl- rand-stat stl- half-nonstat stl- rand-nonstat caltech- half-stat caltech- rand-stat caltech- half-nonstat caltech- rand-nonstat next table summarizes results shufﬂed reward function nonstationary context varying cluster distribution mixed-domain settings. based mean accuracy entire experiment three algorithms were universal embedding mini-batch embedding online embedding respectively. furthermore experiment embedding-based approaches always outperformed baseline suggesting setting reward functions nonstationary addition nonstationary input environment advantage representation learning quite signiﬁcant compared standard note that nonstationary labels reward accumulated baseline remains signiﬁcantly reward embedding-based approaches iterations thus challenging setting context reward nonstationarities embedding-based approaches clearly outperform standard contextual bandit. table summarizes results nonstationary online learning setting negative environments unshufﬂed reward. based mean accuracy entire experiment three algorithms were online embedding universal embedding mini-batch embedding respectively. again embedding-based approaches always superior baseline online embedding achieved best performance among methods mnist universal batch embeddings taking turns outperforming baseline datasets settings. finally table summarizes results nonstationary online learning setting negative environments shufﬂed reward function. based mean accuracy entire experiment three algorithms were universal embedding online embedding mini-batch embedding respectively conﬁrming advantage adaptive encoding standard addition difference textures semantics introduced experiments demonstrated embedding selection outperforms single universal embedding nonstationary cases. figures visualize details reward accumulation time different methods mnist data settings tables performance between embedding-based approaches baseline especially large settings. furthermore adaptive context-dependent embedding approaches consistently ourperform single-embedding approach online embedding emerging best especially increasing number iterations. conclusions introduced extension contextual bandit problem motivated several real-world applications non-stationary environments including recommendation systems health monitoring medical diagnosis others. setting refer contextual bandit representation learning unlabeled history unlabeled contexts available prior online decision making allows instead using context learn context representations. next online phase embeddings selected adaptively depending context updated based contexts observed far. propose speciﬁc algorithms cbrh problem based online ofﬂine clustering combine online embedding selection learning contextual thompson sampling bandit. algorithms evaluated several types nonstationary environments compared standard contextual bandit well universal embedding several datasets. overall observe clear advantages embedding-based approaches standard contextual bandit; moreover proposed adaptive embedding selection learning methods frequently outperform universal embedding multiple nonstationary settings. references shipra agrawal navin goyal. analysis thompson sampling multi-armed bandit problem. colt annual conference learning theory june edinburgh scotland pages sofía villar jack bowden james wason. multi-armed bandit models optimal design clinical trials beneﬁts challenges. statistical science review journal institute mathematical statistics yver. online semi-supervised learning application dynamic learning radar data. international radar conference \"surveillance safer world\" pages gábor bartók dean foster dávid alexander rakhlin csaba szepesvári. partial monitoring—classiﬁcation regret bounds algorithms. mathematics operations research lihong reyzin robert schapire. contextual bandits linear payoff functions. geoffrey gordon david dunson miroslav dudik editors aistats volume jmlr proceedings pages jmlr.org adam coates andrew importance encoding versus training sparse coding vector quantization. proceedings international conference machine learning pages adam coates andrew honglak lee. analysis single-layer networks unsupervised feature learning. proceedings fourteenth international conference artiﬁcial intelligence statistics pages international warfarin pharmacogenetics consortium estimation warfarin dose clinical pharmacogenetic data. engl emilie kaufmann nathaniel korda rémi munos. thompson sampling asymptotically optimal finite time algorithmic learning theory proc. analysis. international conference volume lncs pages lyon france springer. jérémie mary romaric gaudel philippe preux. bandits recommender systems. machine learning optimization data first international workshop pages ororbia alexander giles david reitter. online semi-supervised learning deep hybrid boltzmann machines denoising autoencoders. arxiv preprint arxiv.", "year": 2018}