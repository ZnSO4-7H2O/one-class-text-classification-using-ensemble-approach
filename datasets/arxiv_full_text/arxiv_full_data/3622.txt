{"title": "Natural data structure extracted from neighborhood-similarity graphs", "tag": ["stat.ML", "cond-mat.dis-nn", "cs.CV", "cs.LG"], "abstract": "'Big' high-dimensional data are commonly analyzed in low-dimensions, after performing a dimensionality-reduction step that inherently distorts the data structure. For the same purpose, clustering methods are also often used. These methods also introduce a bias, either by starting from the assumption of a particular geometric form of the clusters, or by using iterative schemes to enhance cluster contours, with uncontrollable consequences. The goal of data analysis should, however, be to encode and detect structural data features at all scales and densities simultaneously, without assuming a parametric form of data point distances, or modifying them. We propose a novel approach that directly encodes data point neighborhood similarities as a sparse graph. Our non-iterative framework permits a transparent interpretation of data, without altering the original data dimension and metric. Several natural and synthetic data applications demonstrate the efficacy of our novel approach.", "text": "sentence summary direct construction sparse ‘neighborhood-similarity’ graphs overcomes many limitations traditional high-dimensional data analysis. present data typically consist simultaneous measurements many distinct characteristics. interpretation data high dimensional ‘data spaces’ pose challenge particular data cannot directly visualized solution data dimensionality reduction approaches generally used step however inherently distorts original distance relation data points uncontrollable effect subsequent low-dimensional analysis methods. alternative clustering methods applied directly high dimensional data methods often either priori model cluster form model-free iterative optimization scheme based local distances. model-based algorithms bias results searching e.g. gaussian clouds convex sets manifold structures regions constrained density change inherent given data. model-free approaches avoid shortcoming effects iterative modiﬁcations data point distances generally seizable bias results unpredictable manner. novel general framework circumvents problems. build sparse graph data data points comprise nodes. nodes linked edges close occupy structurally ‘similar’ regions dataset. problem interpreting data structure high dimensions reduced problem interpreting structure sparse graph vast literature methods already available. importantly time-complexity methods applied sparse graphs largely independent original dimensionality dataset. precisely start directed k-nearest-neighbor graph deﬁned respect distance measure chosen suit particular data science application structural contours graph sharpened calculating measure ‘structural similarity’ pairs data points joined edge removing edges according user-deﬁned thresholding criterion. retain full control bias introduced edge ﬁltering full distance information k-neighborhood point independently follows. distances given point k-nearest neighbors deﬁnes empirical distribution distance. distribution quantitatively compared corresponding distributions neighboring points graph. statistical literature contains many methods comparing empirical distributions well-documented speciﬁc properties. proof concept chose well-known kolmogorov-smirnov statistic maximum absolute difference cumulative distributions data points empirical distance distributions statistic likely reside structurally similar regions dataset. emphasize however quantitative measure distribution similarity suited meet speciﬁc goals data scientist could chosen. nevertheless simply comparing distance distributions misleading remedy shortcoming combine latter measure measure overlap k-neighborhoods nodes joined edge graph. again done many ways; demonstrate concept using jaccard similarity index ﬁrst show ﬁltering edges graph described above natural data clusters emerge connected graph components independently subsequent graph-based interpretation steps. illustrate strength approach begin challenging twodimensional examples adapting approach high dimensional data. spirals commonly used test challenge explore strength algorithms. even beginning graph possible reveal edge ﬁltering dimensional chain structures closely interwoven sparsely sampled spirals without making explicit manifold assumption. moreover similarity measures based comparative rather absolute measures largely density invariant permits edge removal arbitrarily shaped high density regions preserving edges within high-dimensional data propose sorting nodes exhibits data structure wide range scales visually graph’s adjacency matrix. upon removing edges original graph means edge ﬁltering obtain nested sets separate connected components. order natural topological clusters emerge expresses fundamental relationship data structure similarity criteria used ﬁltering. algorithmically edges ﬁltered large sweep averaged value similarity measures starting sweep strict edge ﬁltering criterion sort nodes according size connected component belong edge ﬁltering criterion becomes lenient connected components merge together nodes re-sorted according size connected components preserving previous orderings subordering process continues original graph conﬁguration regained. procedure look similar hierarchical clustering fact different validate approach high dimensions polynomial transformation puts original low-dimensional data clear manifold structure highdimensional space data celebrated t-sne algorithm standard parameter settings substantial difﬁculties method able identify even smallest closely spaced sets highlights strength adjacency matrix sorting absence background noise many major partitions already captured graph ﬁltering structure clearly emerge upon sorting scrutinize performance real-world high-dimensional data carefully curated standard image datasets coil- mnist examined. datasets contain labeled images single objects where rather working higher level features often done concurrent approaches take pixel values data. coil- dataset contains images different objects photographed different angles already sorted adjacency matrix approach successfully distinguishes majority objects however even ﬁltering objects remain within connected component. closer inspection however reveals that upon rotation horizontal rectangular objects occupy frame similar additionally asymmetric object number partitioned distinct components left facing right front facing images associated similarly shaped object number instead simple explanation naive representation data space pixel value coordinate distances light gray objects black background dominated object’s shape. mnist ‘training’ dataset images handwritten digits signiﬁcant overlap digit classes presents even greater challenge. data connected component criterion partitioning graph strict permit breakdown correct digit classes demonstrate potential subsequent graph-based interpretation steps. partitioning largest connected component ﬁltered graph using standard spectral clustering approach digit classes immediately revealed graph topology errors spectral clustering process remedied removing spurious partitions here role edge ﬁltering evident filtering removes majority graph edges digit classes permitting dominant clusters revealed. reassignment nodes nearest large cluster provides accurate partition digit classes compares favorably stateof-art unsupervised methods infogan that however require number digit classes known request training large neural network. approach also offers signiﬁcant computational advantage construction graph edge-ﬁltering operations performed computational cost even exceedingly large datasets analyzed real-time interpretive exploration. construction graph computationally expensive step accelerated using approximate substantial scope development speciﬁc algorithms transparent tools based general framework. results presented small hint potential approach suggesting substantial role neighborhood similarity graphs future data science. fig. method overview dimensional examples. similarity calculation edge connecting blue nodes showing cumulative distributions distance neighborhoods distance schematic. dimensional synthetic data examples adjacency matrix sorting algorithm rows colored boxes indicate sort order nodes corresponding connected components. step previous connected component orderings preserved sub-orderings shown eight dimensional synthetic example. original dimensional dataset eight dimensional projection t-sne results tsne parameter perplexity respectively. graph adjacency matrix sorting. sorted adjacency matrix ﬁltering colored boxes indicating strongly connected components corresponding exactly sets colors fig. testing coil- image dataset sorted adjacency matrix ﬁltering. gray dots removed edges; boxes strongly connected components. comparison connected components ﬁltering given object labels. mismatch examples. fig. testing mnist handwritten digits dataset filtered adjacency matrix normalized clustering cluster re-merging boxes strongly connected components clusters; blue dots edges preserved ﬁltering; gray dots edges removed ﬁltering. bottom comparison clusters true labels normalized cluster re-merging reassignment points clusters smaller points rightcolumn shows color key; clusters composed fewer points shown white. plotted. colors indicate largest distinct strongly connected components filtered graph. labels indicate edge removal criteria. fig. parameter settings center panel.", "year": 2018}