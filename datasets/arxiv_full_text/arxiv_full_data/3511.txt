{"title": "Incomplete Dot Products for Dynamic Computation Scaling in Neural  Network Inference", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "We propose the use of incomplete dot products (IDP) to dynamically adjust the number of input channels used in each layer of a convolutional neural network during feedforward inference. IDP adds monotonically non-increasing coefficients, referred to as a \"profile\", to the channels during training. The profile orders the contribution of each channel in non-increasing order. At inference time, the number of channels used can be dynamically adjusted to trade off accuracy for lowered power consumption and reduced latency by selecting only a beginning subset of channels. This approach allows for a single network to dynamically scale over a computation range, as opposed to training and deploying multiple networks to support different levels of computation scaling. Additionally, we extend the notion to multiple profiles, each optimized for some specific range of computation scaling. We present experiments on the computation and accuracy trade-offs of IDP for popular image classification models and datasets. We demonstrate that, for MNIST and CIFAR-10, IDP reduces computation significantly, e.g., by 75%, without significantly compromising accuracy. We argue that IDP provides a convenient and effective means for devices to lower computation costs dynamically to reflect the current computation budget of the system. For example, VGG-16 with 50% IDP (using only the first 50% of channels) achieves 70% in accuracy on the CIFAR-10 dataset compared to the standard network which achieves only 35% accuracy when using the reduced channel set.", "text": "fig. contrasting computation using complete product standard networks incomplete product proposed paper convolutional layer. standard ﬁlter given layer input channels used product computation compute corresponding output channel. under example ﬁlter uses ﬁrst input channels compute corresponding output channel approximation cdp. furthermore ﬁlters used since output channels ﬁlters utilized next layer. leads reduction computation idp. abstract—we propose incomplete products dynamically adjust number input channels used layer convolutional neural network feedforward inference. adds monotonically non-increasing coefﬁcients referred proﬁle channels training. proﬁle orders contribution channel non-increasing order. inference time number channels used dynamically adjusted trade accuracy lowered power consumption reduced latency selecting beginning subset channels. approach allows single network dynamically scale computation range opposed training deploying multiple networks support different levels computation scaling. additionally extend notion multiple proﬁles optimized speciﬁc range computation scaling. present experiments computation accuracy trade-offs popular image classiﬁcation models datasets. demonstrate that mnist cifar- reduces computation signiﬁcantly e.g. without signiﬁcantly compromising accuracy. argue provides convenient effective means devices lower computation costs dynamically reﬂect current computation budget system. example vgg- achieves accuracy cifar- dataset compared standard network achieves accuracy using reduced channel set. inference deep convolutional neural networks edge devices received increasing attention applications begin sensor data input models running directly device however trained model ﬁxed accuracy size latency proﬁle determined number layers parameters layer. static nature models problematic dynamic contexts running mobile device power latency requirements inference change based current battery life computation latency allowance device. approach address dynamic contexts train multiple models ranging sizes varying number parameters layer mobilenet selecting appropriate model based current system requirements. however kind approach requires storing multiple models different sizes support desired computation ﬂexibilities system. ideally would fig. proﬁles evaluated paper. channel coefﬁcient index x-axis corresponds input channel y-axis shows value coefﬁcient proﬁle. all-one proﬁle corresponds standard network layer withchannel coefﬁcients. proﬁles provide different schemes coefﬁcients. instance half-exp exponential decay second half coefﬁcients used. compute target percentage beginning components starting accumulated target percentage components reached. contribution remaining components smaller coefﬁcients ignored making product incomplete. mathematically equivalent setting unused coefﬁcients proﬁle corresponding various policies favor dynamic computation scaling certain ranges expense regions. section iv-c discuss performance implications various proﬁles. proﬁles evaluated paper shown figure all-one corresponds standard network layer without channels given layer. coefﬁcients provide ordering channels allow channels small coefﬁcients dropped inference save computation. training proﬁle products performed during forward propagation beginning channels ﬁlter without compromising accuracy signiﬁcantly. call product incomplete product show evaluation section using networks trained properly chosen proﬁle achieves much higher accuracy relative using standard cnns using scheme able train single dynamically scale across computation range. figure contrasts forward propagation standard layer using input channels refer complete product using half input channels sources computation reduction output channel computed ﬁlter using input channels half ﬁlters unused since corresponding output channels never consumed next layer. therefore leads reduction computation cost. general computation cost times original computation cost percentage channels used idp. show analysis section reduces computations several conventional network structures still maintaining similar accuracy channels used. next section describe integrated multiple layer types order train networks subset channels used forward propagation. call networks incomplete neural networks. incomplete neural network consists incomplete layers. incomplete layer similar standard neural network layer except incomplete product operations replace conventional complete product operations forward propagation. begin section describing forward propagation provide explanation speciﬁcally added fullyconnected convolutional depthwise separable convolutional layers. adds proﬁle consisting monotonically nonincreasing coefﬁcients components product computations forward propagation. coefﬁcients order importance components decreasing order least important. mathematically incomplete product vectors fig. computation ﬁlter consisting kernels patch input consisting slices input channels highlighted dashed black lines. right ﬁlter input shown vector form. coefﬁcients correspond input channel. vertical dashed line signiﬁes ﬁrst channels used compute product number output channels number input channels i-th channel j-th pointwise ﬁlter i-th channel depthwise ﬁlter i-th input channel j-th output channel. incomplete depthwise convolution layer following computation instead number output components number input components layer weight corresponding j-th output component i-th input component i-th input component j-th output component. incomplete linear layer following computation instead incomplete block consists incomplete layers batch normalization activation functions. paper conv denotes incomplete convolution block containing incomplete convolution layer batch normalization activation function. similarly s-conv denotes incomplete depthwise separable convolution block. denotes incomplete fully connected block. b-conv denotes incomplete binary convolution block layer weight values either b-fc denotes incomplete binary fully connected block. number output channels number input channels i-th channel j-th ﬁlter i-th input channel j-th output channel. input data kernel convolution. paper input data experiments. incomplete convolution layer following computation instead fig. incomplete depthwise separable convolution consists depthwise convolution followed pointwise convolution. illustration applied pointwise convolution. note also applied depthwise convolution illustrated figure multiple proﬁles focusing speciﬁc range used single network efﬁciently cover larger computation range. figure provides overview training process network proﬁles example ﬁrst proﬁle operates range second proﬁle operates range. training phase performed multiple stages ﬁrst training proﬁle ﬁxing learned weights training proﬁle speciﬁcally training proﬁle channels corresponding range learned. training proﬁle proﬁle trained similar manner learns weights range still utilizing previously learned weights proﬁle inference time proﬁle selected based current percentage requirement. percentage chosen application power management policy dictates computation scaling based e.g. current battery budget device. two-proﬁle example figure proﬁle used otherwise proﬁle used. middle layers shared across proﬁles proﬁle maintains separate ﬁrst last layer. design choice optional. experimentally found maintaining separate ﬁrst last layer helps improve accuracy proﬁle adapting proﬁle subset channels present range. generally separate layers small amount memory overhead base model. instance vgg- model used evaluation section additional proﬁle adds ﬁlter convolutional layer neuron fully connected layer proﬁle classiﬁer case second proﬁle layers translate increase fig. training incomplete neural network proﬁles. first proﬁle trained using ﬁrst channels ﬁlter every layer. proﬁle trained using channels every layer updates latter channels ﬁlter. proﬁle independent ﬁrst last layer. example linear proﬁles used. mobilenet network structure interesting convolution layer depth dimension computationally expensive depth large making natural target idp. cifar- dataset evaluating network structure. binarized networks binarized networks useful low-power embedded device applications. combine networks device cloud layer segmentation shown figure mnist dataset evaluating network structure. fig. networks used evaluate idp. complete product layers standard convolutional layers. layers trained proﬁle described section ii-b order subset product components inference across dynamic computation scaling range. multiplier denotes multiple repeated layers type. mobilenet s-conv layer depthwise separable convolution layer described binarized network model b-conv b-fc refer binary convolution layer binary fully connected layer respectively. model segment layer device cloud lower layers device higher layers cloud evaluation percentage speciﬁed layer shown percentage input channels forward propagation. choice proﬁle large impact range trained model. figure shows effect different proﬁles mlp. used comparison since smaller model size magniﬁes differences behavior networks trained various proﬁles. using allone proﬁle equivalent training standard network without idp. all-one model achieves highest accuracy result product complete falls quickly poorer performance decreased. shows standard model support large dynamic computation range. equally weighting contribution channel quality network deteriorates quickly fewer channels used. proﬁles weight early channels latter channels less. allows network deteriorate gradually channels smaller contribution unused idp. example model trained linear proﬁle able maintain higher accuracy idp. linear proﬁle model still achieves accuracy standard model. compare standard network linear proﬁle remaining analysis paper. compared linear proﬁle harmonic proﬁle places much higher weight beginning subset channels. translates larger dynamic computation range model trained harmonic proﬁle. however model performs worse standard model channels used fortunately described section iv-d generally multiple proﬁles single network mitigate problem fig. comparison classiﬁcation accuracy structure figure trained using four proﬁles mnist dataset. x-axis shows percentage components used layer forward propagation. all-one proﬁle equivalent standard network section evaluate performance networks presented figure trained linear proﬁle compared standard network. figure compares dynamic scaling incomplete products standard networks incomplete networks four different networks vgg- mobilenet binarized networks. fig. comparison performance dynamic scaling standard networks incomplete networks network structure described section iv-a. top-left mlp. top-right vgg-. bottom-left mobilenet bottom-right binarized network. network structure observe using linear proﬁle allows network achieve larger dynamic computation range compared standard network. vgg- model model linear proﬁle achieves accuracy opposed all-one model. additionally network linear network able achieve accuracy standard network channels used. section explore performance multipleproﬁle incomplete neural networks described section iii. figure shows proﬁle multiple-proﬁle incomplete network scales across percentage range vgg- networks. three proﬁle scheme used ﬁrst proﬁle trained range second proﬁle range third proﬁle range. vgg- proﬁle scheme used ﬁrst proﬁle trained range second proﬁle range. proﬁles trained incrementally starting ﬁrst proﬁle. proﬁle learns weights speciﬁed range utilizes weights learned earlier proﬁles later proﬁles weights learned earlier proﬁles without affecting performance earlier proﬁles. training network multi-stage fashion enables single weights support multiple proﬁles. ﬁrst proﬁle vgg- model observe accuracy improve greater used. since maximum proﬁle learn channels range therefore cannot higher ranges inference. second proﬁle able achieve higher ﬁnal accuracy ﬁrst proﬁle performs worse lower part range. proﬁles able achieve higher accuracy across entire range compared single proﬁle network shown figure fig. comparison performance dynamic scaling three proﬁles proﬁles vgg- proﬁles given model share network weights layers. networks trained using linear coefﬁcients. lower regions compared single-proﬁle case. instance proﬁle achieves classiﬁcation accuracy compared accuracy single proﬁle case. proﬁle update channels learned proﬁle still utilizes training. note proﬁle still achieve similar performance single-proﬁle model region. model observe similar trends applied three proﬁle case. proﬁles added ﬁnal proﬁle still able achieve high ﬁnal accuracy even though beginning input channels layer shared three proﬁles. section ﬁrst compare methods similar style objective preventing overﬁtting instead providing dynamic mechanism scale computation range. dropout popular technique dealing overﬁtting using subset features model given training batch. test time dropout uses full network whereas allows network dynamically adjust computation using subset channels. dropconnect similar dropout instead dropping output channels drops network weights. approach similar also mechanism removing channels order support dynamic computation scaling. however adds proﬁle directly order contribution channels randomly drop training. decov recent technique reducing overﬁtting directly penalizes redundant features layer training. high level work shares similar goal multiple-proﬁle aiming create non-redundant channels generalizes well given restricted computation range single proﬁle. growing body work models smaller memory footprint power efﬁcient. driving innovations depthwise separable convolution decouples convolutional layer depthwise pointwise layer shown figure approach generalization inception module ﬁrst introduced googlenet mobilenet utilized depthwise separable convolutions objective performing state inference mobile devices. shufﬂenet extends concept depthwise separable convolution divides input groups improve efﬁciency model. structured sparsity learning constrains structure learned ﬁlters reduce overall complexity model. orthogonal approaches used conjunction them show incorporating mobilenet. proﬁle targeted speciﬁc application number channels given range. current implementation approach described section paper aims proﬁles share coefﬁcients overlapping channels. instance figure ﬁrst half proﬁle coefﬁcients proﬁle coefﬁcients. train weights incrementally starting innermost proﬁle extending outermost proﬁle. future want study general setting coefﬁcient sharing constraint could relaxed jointly training network weights proﬁle coefﬁcients. paper proposes incomplete product novel dynamically adjust inference costs based current computation budget conserving battery power reducing application latency device. enables szegedy sermanet reed anguelov erhan vanhoucke rabinovich going deeper convolutions proceedings ieee conference computer vision pattern recognition szegedy vanhoucke ioffe shlens wojna rethinking inception architecture computer vision proceedings ieee conference computer vision pattern recognition introducing proﬁles monotonically non-increasing channel coefﬁcients layers cnns. allows single network scale amount computation inference time adjusting number channels illustrated figure sources computation saving effects multiplicative. example lead reduce computation additionally improve ﬂexibility effectiveness inference time introducing multiple proﬁles. proﬁle trained target speciﬁc range. inference time current proﬁle chosen target range selected application according power management policy. using multiple proﬁles able train network wide computation scaling range maintaining high accuracy dynamic adaptation approach well notion multiple proﬁles network training proﬁles novel. cnns increasingly running devices ranging smartphones devices believe methods provide dynamic scaling become important. hope paper inspire work dynamic neural network reconﬁguration including designs training associated methodologies. howard chen kalenichenko wang weyand andreetto adam mobilenets efﬁcient convolutional neural networks mobile vision applications arxiv preprint arxiv. krizhevsky nair hinton cifar- dataset teerapittayanon mcdanel kung distributed deep neural networks cloud edge devices international conference distributed computing systems courbariaux hubara soudry el-yaniv bengio binarized neural networks training deep neural networks weights activations constrained arxiv preprint arxiv.", "year": 2017}