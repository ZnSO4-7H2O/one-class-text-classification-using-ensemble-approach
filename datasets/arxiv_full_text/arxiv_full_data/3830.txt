{"title": "Separating Answers from Queries for Neural Reading Comprehension", "tag": ["cs.CL", "cs.NE"], "abstract": "We present a novel neural architecture for answering queries, designed to optimally leverage explicit support in the form of query-answer memories. Our model is able to refine and update a given query while separately accumulating evidence for predicting the answer. Its architecture reflects this separation with dedicated embedding matrices and loosely connected information pathways (modules) for updating the query and accumulating evidence. This separation of responsibilities effectively decouples the search for query related support and the prediction of the answer. On recent benchmark datasets for reading comprehension, our model achieves state-of-the-art results. A qualitative analysis reveals that the model effectively accumulates weighted evidence from the query and over multiple support retrieval cycles which results in a robust answer prediction.", "text": "present novel neural architecture answering queries designed optimally leverage explicit support form queryanswer memories. model able reﬁne update given query separately accumulating evidence predicting answer. architecture reﬂects separation dedicated embedding matrices loosely connected information pathways updating query accumulating evidence. separation responsibilities effectively decouples search query related support prediction answer. recent benchmark datasets reading comprehension model achieves state-of-the-art results. qualitative analysis reveals model effectively accumulates weighted evidence query multiple support retrieval cycles results robust answer prediction. recent advances many tasks achieved utilizing neural architectures employ form external memory. making explicit memories enables models bridge long-range dependencies solve complex reasoning tasks might involve multiple observations. neural architectures equipped explicit memories able achieve impressive results variety tasks. memory networks example able answer questions require higher level readuse form external memory appears essential tackling complex queries require comprehension given context memory module stores explicit contextual information support either contains correct answer clues lead instance attention-based architectures encode supporting contexts typically recurrent neural networks h-dimensional latent representations jointly serve form memory. end-to-end memory networks similar although split support individual parts separately encoded form memories. systems utilize learned query representation selecting memories predicting actual answer affect overall performance. recent work successfully addressed issue directly using retrieved hidden states attention weights pointers answer prediction. work propose novel end-to-end neural architecture answering queries. explicitly separates queries answers reﬂected representation supporting knowledge queryanswer pairs general architecture. particular employ dedicated embedding matrices loosely connected information pathways updating query answer representation. separation responsibilities increases capabilities model search support selectively accumulating evidence answer parallel. representation support reﬂects task answering queries directly facilitates utilization model. evaluate approach reading comprehension tasks involve answering cloze-style queries namely cnn/dailymail task named entity subtask children’s book test datasets provide document support query restriction model also handle multiple documents. contributions following introduce representation supporting memories form query-answer pairs based develop neural architecture answering queries leverages representation iii) evaluate system reading comprehension benchmark datasets competitive systems achieving state-of-the-art results give insights systems ability utilize multiple support retrieval cycles improving reading comprehension performance utilizing explicit memory end-to-end differentiable neural architectures enabled models solve complex tasks require learning simple algorithms processing reasoning large amounts contextual information. traditional architectures rnns like lstm suited kind tasks limited memory capacity difﬁculties learn long-range dependencies large contexts. graves introduced neural turing machines ntms augment traditional rnns external memory written read from. memory composed predeﬁned number writable slots. addressable content position shifts allows solving simple algorithmic tasks. capacity also limited external memory slots carry information long ranges easily traditional rnns. ntms inspired subsequent work using attention-based architectures store information typically form hidden states dynamically time-step processing given context. states retrieved attention mechanism softly selects state matches given query state. viewed keeping encoded context memory. architectures achieved impressive results tasks involve larger contexts reading comprehension machine translation recognizing textual entailment based ideas attention mechanism endto-end memory networks select explicit memories query answering. memories encoded representations input representation query matching output representation subsequent utilization. distinction important representation used match original query different responsibility representation used answer update query. thus attentionbased approaches answering queries using supporting documents considered special case memory networks hidden states form inputoutput representation individual memories jointly encoded. variants memory networks achieved good results various tasks language modeling reading comprehension question answering important contribution memory networks idea reﬁning updating query memories multiple memory retrieval cycles answering query. idea lead signiﬁcant improvements architectures employ attention mechanism iteratively reading comprehension tasks suggested neural architectures. instance associative memory used effectively compress multiple memories redundant copies single memory array. shown promising results e.g. language modeling recognizing textual entailment might therefore suited compress large amounts external memories used conjunction model. query-answer neural network utilizes supporting knowledge form explicit query-answer pairs predict answer given query answers support weighted matching scores corresponding support query actual query weighted query-answer pair retrieved used update current query predicted answer representation subsequent support retrieval cycle process repeated speciﬁed number hops finally predicted answer representation hops input answer classiﬁcation given possible answer candidates. note approach require supporting answers correspond answer candidates. model stores supporting knowledge pairs queries answers given supporting documents -pairs formed detecting task-speciﬁc spans-of-interest forming -pairs soi. work consider cloze-style -pairs. thus given pair corresponds entire document particular ﬁller gap. consider following example encoding queries given document symbols spans-of-interest ﬁrst symbols embedded embedding matrix next entire document encoded bi-directional resulting representations backward-rnn forward-rnn document position afterwards form following query representation trainable parameter-matrix initialized additional random noise identity matrix. thus initially corresponds roughly forward state left context backward state right context. order ensure query representation considers outer context respective required encoding supporting queries advantage entire context encoded contrast restricting figure illustration architecture demonstrates support retrieval cycle along corresponding update query answer utilizing supporting queries answers -pairs). query representation initialized encoding query string initial answer representation computed based initial query representation. context ﬁxed-size context window sentence. furthermore word-order positional information captured naturally employing rnns. encoding actual query identical encoding supporting queries encoding answers work consider answers individual symbols. however approach extended sequences symbols well. answer candidates answer candidates query embedded second embedding matrix sois within support. encodings different applications corresponding output embedding corresponding input embedding used update current answer model used update query. intuition behind using updating query representation want answer word original query would thus embedded corresponds embeddings used answer prediction. utilized infer answer speciﬁc task not. answer representation represented updated adding gated retrieved answer scalar answer accumulation gate depends similarity current query weighted support queries similarity original query encoded answer weighted support answer representation retrieved support iii) measures highest answer candidate probability ﬁnal answer representation following dimensions rh×h; rh×h; answer scoring training maximum number hops scores answer candidates calculated using inner product respective embeddings ﬁnal answer representation setup dataset evaluate architecture recently proposed benchmarks reading comprehension. benchmarks require system answer cloze-style query solely based single supporting answer retrieval supporting answer selected softly supporting -pairs softmax-weights based similarity scores query support weights viewed attention weights respective supporting -pairs. query answer update query representation predicted answer representation consecutively updated using supporting -pairs realizing multi-hop support retrieval. instance example model might support original query best retrieve answer reasonable update original query support includes answer ukraine. subsequent updated query eventually leads correct answer germany support figure illustrates process. supporting document. hermann created datasets news articles. article queries created respective summaries removing named entity summary sentence predicted. articles dataset pre-processed named entity recognition co-reference resolution entity anonymization. similar mind hill created children’s book test dataset passages children’s books sentences extracted. within last sentence passage word removed predicted. dataset split subtasks depending part-of-speech word predicted. evaluate model named entity subtask challenging subtask traditional language models. input presentation encoding input model consists context document query. actual query cloze-text position removed named entity replaced placeholder symbol. entire input encoded bi-directional query answer representations computed described supporting -pairs extracted occurrences answer candidate context document form cloze-text corresponding ﬁller training experiments hidden dimension train models without pre-trained word vectors. input embedding matrix partially initialized -dimensional glove-embeddings randomly rest using pre-trained word vectors. general embeddings initialized gaussian -mean .-stddev matrices described glorot bengio biases except encoder update-gate bias initialized dropout applied rate embedded input words regularization. train system using minibatch adam optimization using initial learning-rate halved whenever accuracy development drops checkpoints ﬁrst entire epoch passed. accuracy drops entire epochs training stopped. mini-batch sizes/respective checkpoint iterations dailymail datasets dataset. trained single models ensembles models. note similar chen consider words entities answer candidates cnn/dailymail dataset. models trained consecutive support retrieval cycles support performed better using models implemented tensorflow results model benchmarks presented table show trained single models ensembles achieve stateof-the-art results dailymail dataset perform similar recent work dataset. important observation model outperforms memory networks large margin. even self-supervision explicitly introduces training objective selecting correct memory memory networks clearly outperformed system. attribute query-answer representation supporting memory related architectural changes separate end-to-end memory networks qanns. models observe using instead support retrieval cycles makes difference glove used initialization. using glove initialize embeddings gives boost performance datasets. would like point systems comparable results either attention-weights context-words retrieved hidden state predict ﬁnal answer. works follow similar idea work separates answer used prediction query. table accuracies different models benchmark test datasets reading comprehension. hermann chen trischler hill dhingra sordoni note works recent. accuracies qanns testsets employing model trained support retrieval cycles applied varying number retrieval cycles epireader achieves slightly better results dataset employs second neural network re-ranks output attention based model improve prediction. idea orthogonal work used improve qanns similarly. iterative attentive reader alternates attention speciﬁc parts query attention context document. authors found attending query useful however attention usually placeholder symbol similar approach. achieve similar results dataset outperformed models dataset. attribute improvement answer update mechanism accumulates dedicated answer embeddings support multiple hops original query gates. thus ﬁnal answer prediction depends attention weights also query answer embeddings. advantage cannot exploited much dailymail datasets entities thus answers anonymized. illustration figure provides example correctly predicted answer align computed support weights trained models different numbers support retrieval cycles found using least hops leads signiﬁcantly better performance using hops. indicates multiple consecutive support retrievals respective query updates important robust performance reading comprehension tasks. figure examples support weights models trained ﬁxed hops. legends show activity respective answer gates brackets. predicted answer underlined correct answer displayed bold-face. mance varying number hops using model trained hops. evaluation gives insights accuracy gains stability answer prediction increasing number hops. results presented table demonstrate model gains hops results quite stable. pronounced difference occurs using hops. relative stability performance hops indicates system learns utilize gating mechanisms decide keep update current query accumulate answers successfully. even though model trained hops best results testsets achieved utilizing model additional hops surprisingly found rather large improvement percentage points accuracy. analysis qualitative analysis system sampled documents dataset found correct answer retrieved already after ﬁrst kept prediction cases however interesting exceptions rule displayed figure shows example documents respective attention weights supporting spans-of-interest hop. general observation support weights pronounced ﬁrst spread increasing number positions additional hop. highly weighted positions vary signiﬁcantly hops shows query updated. shown empirically positive effect example figure however sometimes also result incorrect prediction although answer correctly found ﬁrst demonstrated figure interesting example validation displayed figure shows system puts high support weights different positions document never correct answer. nevertheless surprise model predicts answer correctly anyway. explanation might model learned general words like people good answers dataset another explanation query puts strong bias ﬁnal answer. test latter hypothesis query-gate effectively removes query representation predicted answer representation. found prediction changed people explained support weights. ﬁnding conﬁrms premise query able bias ﬁnal answer query maybe beneﬁcial answer prediction. presented type neural network architecture answering queries. end-to-end trainable learns utilize knowledge form supporting query-answer pairs infer answer given query. explicitly separates query representation used selecting support answer representation used prediction. results recently proposed benchmark datasets task reading comprehension show model outperforms approaches various state-ofthe-art baselines. shows idea explicitly separating query answer important tasks involve answering queries. future work involves extension architecture able properly handle kinds queries e.g. list-queries queries expecting generated answers. furthermore believe architecture suited successful application variety tasks area information extraction. thank thomas demeester thomas werkmeister sebastian krause rockt¨aschel sebastian riedel comments early draft work. research supported german federal ministry education research projects sides bbdc software campus references mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg man´e rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda vi´egas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous systems. software available tensorﬂow.org. junyoung chung caglar gulcehre kyunghyun yoshua bengio. empirical evaluation gated recurrent neural networks sequence modeling. arxiv preprint arxiv.. danihelka greg wayne benigno uria kalchbrenner alex graves. associative long short-term memory. arxiv preprint arxiv.. bhuwan dhingra hanxiao william cohen gated-attention arxiv preprint karl moritz hermann tomas kocisky edward grefenstette lasse espeholt mustafa suleyman phil blunsom. teaching machines read comprehend. nips pages felix hill antoine bordes sumit chopra jason weston. goldilocks principle reading children’s books explicit memory representations. iclr volume abs/.. ankit kumar ozan irsoy jonathan james bradbury robert english brian pierce peter ondruska ishaan gulrajani richard socher. anything dynamic memory networks natural language processing. corr abs/.. alexander miller adam fisch jesse dodge amirhossein karimi antoine bordes jason weston. key-value memory networks directly reading documents. arxiv preprint arxiv.. jeffrey pennington richard socher christopher manning. glove global vectors word representation. emnlp volume pages rockt¨aschel sebastian riedel. learning knowledge base inference neural theorem provers. akbc adam trischler zheng xingdi yuan kaheer suleman. natural language comprehension epireader. arxiv preprint arxiv.. oriol vinyals meire fortunato navdeep jaitly.", "year": 2016}