{"title": "Visual Explanations from Hadamard Product in Multimodal Deep Networks", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "The visual explanation of learned representation of models helps to understand the fundamentals of learning. The attentional models of previous works used to visualize the attended regions over an image or text using their learned weights to confirm their intended mechanism. Kim et al. (2016) show that the Hadamard product in multimodal deep networks, which is well-known for the joint function of visual question answering tasks, implicitly performs an attentional mechanism for visual inputs. In this work, we extend their work to show that the Hadamard product in multimodal deep networks performs not only for visual inputs but also for textual inputs simultaneously using the proposed gradient-based visualization technique. The attentional effect of Hadamard product is visualized for both visual and textual inputs by analyzing the two inputs and an output of the Hadamard product with the proposed method and compared with learned attentional weights of a visual question answering model.", "text": "visual explanation learned representation models helps understand fundamentals learning. attentional models previous works used visualize attended regions image text using learned weights conﬁrm intended mechanism. show hadamard product multimodal deep networks well-known joint function visual question answering tasks implicitly performs attentional mechanism visual inputs. work extend work show hadamard product multimodal deep networks performs visual inputs also textual inputs simultaneously using proposed gradient-based visualization technique. attentional effect hadamard product visualized visual textual inputs analyzing inputs output hadamard product proposed method compared learned attentional weights visual question answering model. multimodal joint function hadamard product widely used multimodal learning tasks. many state-of-the-art models used hadamard product joint function achieve competitive performance visual question answering recent challenge characteristic hadamard product studied deep neural networks. mi-rnn uses integrate different information ﬂows within rnn. show hidden activations saturated toward comparing addition implies gradient tanh function vanished. show hadamard product performs low-rank bilinear pooling deep neural networks. paper show analysis input output hadamard product multimodal deep networks sufﬁcient visualize cross-grounding modalities. unlike previous works method need annotated label attentional weights. results suggest hadamard product multimodal joint function gives excellent performance task also visual explanations vision-language input modalities. class activation mapping proposed identify discriminative regions image classiﬁcation tasks utilizes global average pooling layers representations localize regions. however limitation architecture modiﬁcation architecture requires re-training. grad-cam generalizes various models. grad-cam also visualize tasks e.g. image captioning visual question answering similar approach. unlike previous methods method unsupervised visualization direct visualization vision-language cross-groundings occurred joint function. visual explanations hadamard product work visualize difference intermediate visual input output hadamard product intermediate visual input intermediate textual input space image three layers using standard back-propagation. input image resnet- function however though also function treated constant visualization purpose. speculate constant virtual target joint representation mean squared error indicates amount deviation empirically visualization effective ﬁxing deﬁne visual explanation vision language using inputs hadamard product output product unlike work guided back-propagation relu activations uses positive gradient output back-propagation better visualization. imputed version gradient calculated using resnet- feature extractor. equation newly introduced gist. here embedded word vectors given question looked indices tokens. notice dimensions since output element-wise multiplication moreover deﬁnitions generalized models uses hadamard product multimodal joint function. multimodal low-rank bilinear attention networks model visualization compare visual explanation visual input attentional weights show visual explanation textual input. provides efﬁcient attention mechanism visual questionanswering tasks based interpretation hadamard product operator low-rank bilinear pooling. inputs question embedding vector output learnable skip-thought vectors model visual feature vectors lattice space output ﬁxed resnet- model section brieﬂy describe structure mlb. attention mechanism uses attention probability distribution lattice space. here using low-rank bilinear pooling deﬁned expressed fukui conceptually similar jaderberg softmax function applies vector bias terms omitted simplicity. attended visual feature linear combination corresponding coefﬁcients αgi. attention probability distribution glimpse concatenation resulting vectors denotes predicted answer candidate answers aggregation entire model parameters. method uses intermediate representations figure indicates analyzing points replicate module copies question embedding vector match visual feature vectors. conv modules indicate convolution project channel dimension computationally equivalent linear projection channel. post-processing using equation gradients rc×h×w rρ×d. size image number tokens question dimension word embedding vector. then pixel normalized figure shows example visual explanations. ﬁrst column shows input image second third columns show ﬁrst second attention maps representing notice represents concatenation attended visual features. this model learns generate appropriate attention probability distributions parallel. ﬁrst second rows show similar distributions; however third shows difference. fourth column shows visualization proposed method. although visualization comes analysis hadamard product shows similar result attention maps. ﬁrst second attend donut plate scarf neck respectively. third proposed visualization seems represent ﬁrst second attention maps additive way. visual explanation textual input shows plausible result nouns signiﬁcantly attended whereas words verbs adjectives articles less attended bottom figure visualization attentional weights visual explanations visual textual inputs. attention maps represented lattice space proposed method represents attended region image pixels. plate donut visualized proposed method scarf row. take-home message two-fold results competitive explicit textual attention models explicit textual attention giving unprecedented attention text rather might working regularization using selective weights. sufﬁcient evidence visual attention based comprehension question. textual attention expanding verbs propositions connected nouns phenomena seem consistent co-attention models although work tried mitigate problem using word phrase question-level features. work show hadamard product multimodal deep networks implicitly performs attentional mechanism visual inputs also textual inputs simultaneously using proposed gradient-based visualization technique visual question answering model. though technique based analysis hadamard product multimodal deep networks shows competitive results explicit visualization learned attentional weights. results suggest explicit textual attention providing unique attentional mechanism textual input. instead might regularization using selective weights learned training. moreover cautiously argue textual attention biased toward noun words appeared given text limits inferential capability model. references aishwarya agrawal jiasen stanislaw antol margaret mitchell lawrence zitnick devi parikh dhruv batra. visual question answering. international journal computer vision akira fukui dong park daylen yang anna rohrbach trevor darrell marcus rohrbach. multimodal compact bilinear pooling visual question answering visual grounding. arxiv preprint arxiv. jaderberg karen simonyan andrew zisserman koray kavukcuoglu. spatial transformer networks. advances neural information processing systems pages jin-hwa sang-woo donghyun kwak min-oh jeonghee jung-woo byoung-tak zhang. multimodal residual learning visual advances neural information processing systems pages jin-hwa kyoung woon woosang jeonghee jung-woo byoung-tak zhang. hadamard product low-rank bilinear pooling. international conference learning representations ryan kiros yukun ruslan salakhutdinov richard zemel antonio torralba raquel urtasun sanja fidler. skip-thought vectors. advances neural information processing systems pages jiasen jianwei yang dhruv batra devi parikh. hierarchical question-image coattention visual question answering. advances neural information processing systems pages ramprasaath selvaraju abhishek ramakrishna vedantam michael cogswell devi parikh dhruv batra. grad-cam that? visual explanations deep networks gradient-based localization. international conference computer vision yuhuai saizheng zhang ying zhang yoshua bengio ruslan salakhutdinov. multiplicative integration recurrent neural networks. advances neural information processing systems pages bolei zhou aditya khosla agata lapedriza aude oliva antonio torralba. learning deep features discriminative localization. ieee conference computer vision pattern recognition pages supplementary examples method shown figure example emphasizes importance visual explanation. ﬁrst question ‘what color toddler’s hair?’ corresponding answer ‘blonde’. without visualization know whether model purely biased data distribution not. although possibility model biased toward ‘blonde hair’ attends hair given image visual explanation helps assess model. figure another examples visualization. second third rows forth column attended area bench bike slightly different. interestingly attention maps third different other ﬁrst attention shows part bike whereas second attention shows salient objects. *which unclear beagle charles spaniel. figure another examples visualization. ﬁrst second rows fourth column subtle difference umbrella backpack backpack blue color side area know relationship infer pose hand hand.", "year": 2017}