{"title": "Curriculum Dropout", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Dropout is a very effective way of regularizing neural networks. Stochastically \"dropping out\" units with a certain probability discourages over-specific co-adaptations of feature detectors, preventing overfitting and improving network generalization. Besides, Dropout can be interpreted as an approximate model aggregation technique, where an exponential number of smaller networks are averaged in order to get a more powerful ensemble. In this paper, we show that using a fixed dropout probability during training is a suboptimal choice. We thus propose a time scheduling for the probability of retaining neurons in the network. This induces an adaptive regularization scheme that smoothly increases the difficulty of the optimization problem. This idea of \"starting easy\" and adaptively increasing the difficulty of the learning problem has its roots in curriculum learning and allows one to train better models. Indeed, we prove that our optimization strategy implements a very general curriculum scheme, by gradually adding noise to both the input and intermediate feature representations within the network architecture. Experiments on seven image classification datasets and different network architectures show that our method, named Curriculum Dropout, frequently yields to better generalization and, at worst, performs just as well as the standard Dropout method.", "text": "dropout effective regularizing neural networks. stochastically dropping units certain probability discourages over-speciﬁc co-adaptations feature detectors preventing overﬁtting improving network generalization. besides dropout interpreted approximate model aggregation technique exponential number smaller networks averaged order powerful ensemble. paper show using ﬁxed dropout probability training suboptimal choice. thus propose time scheduling probability retaining neurons network. induces adaptive regularization scheme smoothly increases difﬁculty optimization problem. idea starting easy adaptively increasing difﬁculty learning problem roots curriculum learning allows train better models. indeed prove optimization strategy implements general curriculum scheme gradually adding noise input intermediate feature representations within network architecture. experiments seven image classiﬁcation datasets different network architectures show method named curriculum dropout frequently yields better generalization worst performs well standard dropout method. since deep neural networks become ubiquitous computer vision applications. reason generally ascribed powerful hierarchical feature representations directly learnt data usually outperform classical hand-crafted feature descriptors. figure left right training curriculum dropout gradually increases amount bernoulli multiplicative noise generating multiple partitions within dataset feature representation layers differently original dropout mainly focuses hardest partition only complicating learning beginning potentially damaging network classiﬁcation performance. non-convex optimization intensive computations learning network parameters. relying availability massive data hardware resources aforementioned training challenges empirically tackled deep architectures effectively trained end-to-end fashion exploiting parallel computation. seminal work argues overﬁtting occurs result excessive co-adaptation feature detectors manage perfectly explain training data. leads overcomplicated models unsatisfactory unseen testing data points. address issue dropout algorithm proposed investigated nowadays extensively used training neural networks. method consists randomly suppressing neurons training according values sampled bernoulli distribution. speciﬁcally unit kept unchanged unit suppressed. effect suppressing neuron value output zero forward pass training weights updated backward pass. forward-backward pass completed sample drawn neuron another forward-backward pass done till convergence. testing time neuron suppressed activations modulated mean value bernoulli distribution. resulting model fact often interpreted average multiple models argued improves generalization ability leveraging dropout idea many works proposed variations original strategy however still unclear variation improves respect original dropout formulation many works real theoretical justiﬁcation proposed approach favorable empirical results. therefore providing sound justiﬁcation still remains open challenge. addition lack publicly available implementations make fair comparisons problematic. point departure work intuition excessive co-adaptation feature detectors leads overﬁtting unlikely occur early epochs training. thus dropout seems unnecessary beginning training. inspired considerations work propose dynamically increase number units suppressed function number gradient updates. speciﬁcally introduce generalization dropout scheme consisting temporal scheduling curriculum expected number suppressed units. adapting time parameter bernoulli distribution used sampling smoothly increase suppression rate training evolves thereby improving generalization model. address problem overﬁtting deep neural networks proposing novel regularization strategy called curriculum dropout dynamically increases expected number suppressed units order improve generalization ability model. draw connections original dropout framework regularization theory curriculum learning provides improved justiﬁcation dropout training relating existing machine learning methods. complement foundational analysis broad experimental validation compare curriculum dropout versus original anti-curriculum paradigms neural network-based image classiﬁcation. evaluate performance standard datasets therein method detailed evaluated different types deep learning models datasets conﬁrming effectiveness approach overﬁtting. since then many works investigated topic. propose drop-connect general version dropout. instead directly setting units zero network connections suppressed. generalization proven better performance slower train respect introduce data-dependent evolutional-dropout shallow deep learning respectively. versions based sampling neurons form multinomial distribution different probabilities different units. results show faster training sometimes better accuracies. wang accelerate dropout. method hidden units dropped using approximated sampling gaussian distribution. results show leads fast convergence without deteriorating accuracy. bayer carry analysis showing dropout proﬁciently applied recurrent neural networks. analyze effect dropout convolutional layers deﬁne probabilistic weighted pooling effectively acts regularizer. zhai zhang investigate idea dropout applied matrix factorization. frey introduce binary belief network overlaid neural network selectively suppress hidden units. networks jointly trained making overall process computationally expensive. wager apply dropout generalized linear models approximately prove equivalence data-dependent regularization dropout training adagrad optimizer. rennie propose adjust dropout rate linearly decreasing unit suppression rate training network experiences dropout. respect licly released code similar work. papers beyond bare experimental evaluation proposed dropout variation omitting justify soundness approach. conversely works much formal rely approximations carry analysis biased towards shallow models linear regression matrix factorization differently paper addition experimental effectiveness provide several natural justiﬁcations corroborate proposed dropout generalization deep neural networks. deep neural networks display co-adaptations units terms concurrent activations highly organized clusters neurons. training latter specialize detecting certain details image classiﬁed shown zeiler fergus visualize high sensitivity certain ﬁlters different layers detecting dogs people’s faces wheels general ordered geometrical patterns moreover co-adaptations highly generalizable across different datasets proved torralba’s work indeed ﬁlter responses provided alexnet within conv pool/ layers similar despite images used training different objects imagenet versus scenes places datasets. arguments support existence positive co-adaptations neurons network. nevertheless soon training keeps going coadaptations also negative excessively speciﬁc training images exploited updating gradients. consequently exaggerated co-adaptations neurons weaken network generalization capability ultimately resulting overﬁtting. prevent dropout precisely contrasts negative co-adaptations. latter removed randomly suppressing neurons architecture restoring improved situation neurons independent. empirically reﬂects better generalization capability network training dynamic process. despite previous interpretation totally sound original dropout algorithm cannot precisely accommodate indeed suppression neuron given layer modeled bernoulli random variable employing distribution natural since statistically models binary activation/inhibition processes. spite that seems suboptimal ﬁxed whole differently intuition that beginning training co-adaptation units displayed preserved positively representing selforganization network parameters towards optimal conﬁguration. understand considering random initialization network’s weights. statistically independent actually co-adapted all. also quite unnatural neural network random weights overﬁt data. hand risk overdone co-adaptations increases training proceeds since loss minimization achieve small objective value overcomplicating hierarchical representation learnt data. implies overﬁtting caused excessive co-adaptations appears while. since ﬁxed parameter able handle increasing levels negative co-adaptations work tackle issue proposing temporal dependent parameter. here denotes training time measured gradient updates since models probability given neuron retained count average number units remain active total number given layer. intuitively quantity must higher ﬁrst gradient updates starting decreasing soon training gears. late stages training decrease stopped. thus constrain limit value taken prescribed original dropout scheme starting initial condition unit suppression performed dropout gradually introduced eventually convergence models fact retrieve original formulation particular case curriculum. considering figure provide intuitive straightforward motivations regarding choice. blue curves fig. polynomials increasing degree despite fulﬁlling initial constraint manually thresholded impose introduces parameters respect quantity selected argument discourages replacement variable moreover evaluating area curve intuitively measure aggressively green curves behave delaying dropping scheme eventually converge precisely convergence faster moving green curves left fastest achieved scheduling function could still argue parameter annoying since requires cross validation. necessary fact actually ﬁxed according following heuristics. despite def. considers limit condition operatively replaced total number gradient updates needed optimization. thus totally reasonable assume order magnitude priori known ﬁxed power therefore curriculum function def. interested furthermore imposing actually rule thumb implies |θcurriculum used experiments additionally figure grab intuitions fact asymptotic convergence indeed realized quite consistent part training well means during portion training actually dropping neurons prescribed addressing overﬁtting issue. addition arguments provide complementary insights scheduled implementation dropout training. smarter initialization network weights. problem optimizing deep neural networks non-convex non-linearities pooling steps. spite that theoretical papers investigated issue sound mathematical perspective. instance mild assumptions haeffele vidal derive sufﬁcient conditions ensure local minimum also global guarantee former found starting initialization. theory presented cannot straightforwardly applied dropout case pure deterministic framework theoretical analysis carried out. therefore still open question whether initializations equivalent sake dropout training ones preferable. providing theoretical insight ﬂavor posit curriculum dropout interpreted smarter initialization. indeed implement soft transition classical dropout-free training network versus dropout perspective curriculum seems equivalent performing dropout training network whose weights already slightly optimized evidently resulting better initialization them. naive approach think perform regular training certain amount gradient updates apply dropout remaining ones. call switch-curriculum. actually induces discontinuity objective value damage performance respect smooth transition performed curriculum check fig. curriculum dropout adaptive regularization. several connections established dropout model training noise addition common trend discovered unregularized loss function optimized artiﬁcially corrupted data actually equivalent minimize loss augmented data dependent penalizing term. linear/logistic regression least squares proved dropout induces regularizer scaled impact regularization ﬁxed therefore rising potential overunder-ﬁtting issues θcurriculum small regularizer zero perform regularization all. indeed latter simply necessary network weights still values close random statistically independent initialization. hence overﬁtting unlikely occur early training steps. differently expect occur soon training proceeds using regularizer weighted increasing function therefore gradient updates heavier effect regularization. reason overﬁtting better tackled proposed curriculum. despite overall idea adaptive selection parameters novel either regularization theory tuning network hyper-parameters best knowledge ﬁrst time concept timeadaptive regularization applied deep neural networks. compendium. conclude general comments. posit overﬁtting beginning network training. therefore differently allow scheduled retain probability gradually drops neurons out. among plausible curriculum functions def. proposed choice introduces additional parameter tuned implicitly provides smarter weight initialization dropout training. throughout interpretations retrieve common idea smoothly changing difﬁculty training applied network. fact better understood ﬁnding connections curriculum learning explain next section. sake clarity remind concept curriculum learning within classical machine learning algorithm training examples presented model unordered manner frequently applying random shufﬂing. actually different happens human training process education. indeed latter highly structured level difﬁculty concepts learn proportional people managing easier knowledge babies harder adults. start small paradigm likely guide learning process following intuition proposes subdivide training examples based difﬁculty. then learning conﬁgured easier examples come ﬁrst eventually complicating processing hardest ones training. concept formalized introducing learning time training begins ends time denotes distribution training example drawn from. notion curriculum learning formalized requiring ensures sampling examples easier ones sampled qλ+ε mathematically formalized assuming target training distribution accounting examples easy hard ones. sampling corrected factor interpretation measure difﬁculty training example maximal complexity training example ﬁxed reached training i.e. i.e. relationship order prove scheduled dropout fulﬁlls deﬁnition simplicity consider applied input layer only. restrictive since considerations apply intermediate layer considering layer trains feature representation used input subsequent one. images exploited training consider partitions dataset including clean data possible ways corrupting bernoulli multiplicative noise denote probability sampling uncorrupted d-dimensional image within image dataset gradient update case sampling dropped-out equivalent sampling corresponding uncorrupted image overlapping binary mask entry zero probability mapping number zeros conclude give additional interpretation curriculum dropout. entry zero. clearly corresponds easiest available example since learning starts considering possible available visual information. start decreasing suppressed still almost information original dataset available training network. grows decreases bigger number entries zero. complicates task requiring improved effort model capitalize reduced uncorrupted information available stage training process. connection dropout curriculum learning possible thanks generalization def. consequently original dropout interpreted considering single speciﬁc value constant retain probability means that previously found adaptive regularization level difﬁculty training examples ﬁxed original dropout. encounters concrete risk either oversimplifying overcomplicating learning detrimental effects model’s generalization capability. hence proposed method allows setup progressive curriculum complicating examples smooth adaptive manner opposed complication ﬁxed equal maximal beginning conclude note aforementioned work proposes linear increase retain probability. according equations implements calls anti-curriculum shown perform slightly better worse no-curriculum strategy always worse curriculum implementation. experiments conﬁrm ﬁnding. section applied curriculum dropout neural networks image classiﬁcation problems different datasets using convolutional neural network architectures multi-layer perceptrons particular used different architectures lenet deeper called cnn- cnn- respectively. following detail datasets used network architectures adopted case. digits) training test sets contain images respectively. training set’s images generated using mnist training images test set’s images generated using mnist test images. used cnn-. svhn real world images street view house numbering. used cropped images representing single digit exploited subset dataset consisting images training images testing randomly selected. used cnn- also case. cifar- cifar- datasets collect tiny natural images reporting elements classes respectively. datasets training test sets contain images respectively. used cnn- datasets. caltech- resolution images classes. them variable size instances available balanced dataset used images class training testing respectively. images reshaped pixels. used cnn- here. applied curriculum dropout using function picked using heuristics ﬁxed follows. cnn- cnn- retain probability input layer θinput selecting θconv convolutional fully connected layers respectively. θinput θhidden cases adopted recommended values reporting results emphasize improve standard dropout framework compete state-of-the performance image classiﬁcation tasks. reason engineering tricks data augmentation particular pre-processing neither tried complex network architectures. figure curriculum dropout compared regular dropout anti-curriculum regular training network units suppression cases plot mean test accuracy function gradient updates. shadows represent standard deviation errors. best viewed colors. figure switch-curriculum. compare curriculum regular dropout three cases switch regular dropout training beginning middle iii) almost learning. left right curriculum functions cross-entropy loss test accuracy curves. qualitatively compared curriculum dropout versus original dropout anti-curriculum dropout unregularized i.e. dropout training network since cnn- trained scratch order ensure robust experimental evaluation rework guess simpler learning task less effective curriculum learning. task relatively easy itself less need starting easy. case done additional cost training time requirements. expected anti-curriculum improved signiﬁcant scheduling. also sometimes anti-curriculum strategy even performs worse nonregularized network coherent ﬁndings discussion concerning annealed dropout anticurriculum represents generalization. addition neither regular curriculum dropout ever need early stopping anti-curriculum often does. paper propose scheduling dropout training applied deep neural networks. softly increasing amount units suppressed layerwise achieve adaptive regularization provide better smooth initialization weight optimization. allows implement mathematically sound curriculum justiﬁes proposed generalization broad experimental evaluation image classiﬁcation tasks proposed curriculum dropout proved effective original dropout annealed latter example anti-curriculum therefore achieving inferior performance disciplined approach ease dropout training. globally always outperform original dropout using various architectures improve idea margin. tested curriculum dropout image classiﬁcation tasks only. however guess that standard dropout method general thus applicable different domains. future work apply scheduling computer vision tasks also extending case inter-neural connection inhibitions recurrent neural networks.", "year": 2017}