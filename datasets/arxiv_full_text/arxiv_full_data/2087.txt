{"title": "Block-Wise MAP Inference for Determinantal Point Processes with  Application to Change-Point Detection", "tag": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "abstract": "Existing MAP inference algorithms for determinantal point processes (DPPs) need to calculate determinants or conduct eigenvalue decomposition generally at the scale of the full kernel, which presents a great challenge for real-world applications. In this paper, we introduce a class of DPPs, called BwDPPs, that are characterized by an almost block diagonal kernel matrix and thus can allow efficient block-wise MAP inference. Furthermore, BwDPPs are successfully applied to address the difficulty of selecting change-points in the problem of change-point detection (CPD), which results in a new BwDPP-based CPD method, named BwDppCpd. In BwDppCpd, a preliminary set of change-point candidates is first created based on existing well-studied metrics. Then, these change-point candidates are treated as DPP items, and DPP-based subset selection is conducted to give the final estimate of the change-points that favours both quality and diversity. The effectiveness of BwDppCpd is demonstrated through extensive experiments on five real-world datasets.", "text": "figure -sec part -min speech recording shown change-point candidates. segments different speakers noises plotted different colors. bwdpp kernel constructed whole -min recording change-point candidates bwdpp items. white denotes non-zero entries black indicates zero. dpps maximum posteriori problem argmaxy aiming ﬁnding subset highest probability attracted much attention broad range potential applications. noting np-hard problem number approximate inference methods purposed including greedy methods optimizing submodular function optimization continuous relaxation minimum bayes risk decoding minimizes applicationspeciﬁc loss function existing methods need calculate determinants conduct eigenvalue decomposition. computations taken scale kernel size cost around time become intolerably high become large e.g. thousands. nevertheless class dpps kernel almost block diagonal inference whole kernel could replaced series sub-inferences subkernels. since sizes sub-kernels become smaller overall computational cost signiﬁcantly reduced. dpps often deﬁned line items similar neighbourhoods line signiﬁcantly different away. since inference dpps conducted block-wise manner refer bwdpps rest paper. observation mainly motivated probexisting inference algorithms determinantal point processes need calculate determinants conduct eigenvalue decomposition generally scale full kernel presents great challenge real-world applications. paper introduce class dpps called bwdpps characterized almost block diagonal kernel matrix thus allow efﬁcient block-wise inference. furthermore bwdpps successfully applied address difﬁculty selecting change-points problem change-point detection results bwdpp-based method named bwdppcpd. bwdppcpd preliminary change-point candidates ﬁrst created based existing well-studied metrics. then change-point candidates treated items dpp-based subset selection conducted give ﬁnal estimate change-points favours quality diversity. effectiveness bwdppcpd demonstrated extensive experiments real-world datasets. determinantal point processes elegant probabilistic models subset selection problems quality diversity considered. formally given items {··· deﬁnes probability measure subsets every subset l-ensemble kernel positive semideﬁnite matrix. writing gram matrix could viewed squared volume spanned column vectors deﬁning qiφi popular decomposition kernel given measures quality item viewed angle vector diversity features measures similarity items shown probability including increases quality diversity result assigns high probability subsets good quality diverse change-point detection aims detecting abrupt changes time-series data period time consecutive change-points often referred segment state homogeneous properties interest behaviour human activity data after choosing number change-point candidates without much difﬁculty treat change-point candidates items select subset ﬁnal estimate change-points. change-point candidate quality change-point. moreover true locations change-points along timeline tend diverse since states would change rapidly. therefore preferred conduct changepoint selection incorporates quality diversity. dpp-based subset selection clearly suits purpose well. meanwhile corresponding kernel become alblock diagonal neighbouring items less diversiﬁed items apart diversiﬁed case becomes bwdpp. problem actively studied decades various methods could broadly classiﬁed bayesian frequentist approach. bayesian approach problem reduced estimating posterior distribution change-point locations given time-series data posteriors estimated include indicator sequence length although many improvements made e.g. using advanced monte carlo method efﬁciency estimating posteriors still challenge real-world tasks. frequentist approach core idea hypothesis testing general strategy ﬁrst deﬁne metric considering observations past present windows. windows move forward change-points selected metric value exceeds threshold. widely-used metrics include cumulative generalized likelihood-ratio bayesian information criterion kullback leibler divergence recently subspace-based metrics kernel-based metrics density-ratio various metrics explored choose thresholds perform change-point selection also determining factor detection performance relatively less studied. heuristic-based rules procedures dominant well-performed e.g. selecting local peaks threshold discarding lower peaks close requiring metric differences change-points neighbouring valleys threshold studied metrics create preliminary changepoint candidates without much difﬁculty. then treat change-point candidates items conduct dpp-based subset selection obtain ﬁnal estimate change-points favours quality diversity. contribution paper two-fold. first introduce class called bwdpps characterized almost block diagonal kernel matrix thus allow efﬁcient block-wise inference. second bwdpps successfully applied address difﬁcult problem selecting change-points results bwdppbased method named bwdppcpd. rest paper organized follows. describing brief preliminaries introduce bwdpps give theoretical result bwdpp-map method. next introduce bwdppcpd present evaluation experiment results number real-world datasets. finally conclude paper discussion potential future directions. diagonal sub-matrices rli×li subkernels containing items mutually similar off-diagonal sub-matrices rli×li+ sparse submatrices non-zero entries bottom left representing connections adjacent sub-kernels. fig. gives good example matrices. indices y··· l··· correspondingly. indices denote square sub-matrix indexed lcicj |ci| |cj| sub-matrix rows indexed columns following general notations diag mean block diagonal matrix consisting sub-matrices means positive semi-deﬁnite. strictly block diagonal kernel ﬁrst consider motivating case kernel strictly block diagonal i.e. elements off-diagonal sub-matrices zero. easily seen following divide-and-conquer theorem holds. theorem block diagonal kernel theorem tells inference strictly block diagonal kernel decomposed series sub-inferences sub-kernels. overall computation cost largely reduced. noting exact dpp-map algorithms available approximate dpp-map algorithms could used plug-and-play sub-inferences. almost block diagonal kernel analyze inference bwdpp alblock diagonal kernel deﬁned hypothesized subset selected y··· l··· correspondingly without loss generality assume invertible deﬁning recursively ˜lci represents zero matrix appropriate size corresponding area zeros. second equation lcci since almost block diagonal kernel. continuing recursion det. ˜lci depends c··· cannot optimize det··· separately. alternatively provide approximate method optimize c··· sequentially named bwdpp-map method depth-ﬁrst greedy search method essence. bwdpp-map described table argmaxci;cj j=··· denotes optimizing value ﬁxed sub-kernel ˜lyi given similarly ˜lci namely ˜lyi conclusion similar inference strictly block diagonal kernel using bwdpp-map inference almost block diagonal kernel decomposed series sub-inferences sub-kernels well. four comments conclusion. first noted bwdpp-map method approximate optimization method even sub-inference step conducted exactly. ˜lci depends c··· provide empirical evaluation later showing block-wise operation greedy search bwdpp-map achieve computation speed-up marginal sacriﬁce accuracy. second following lemma show sub-kernel ˜lyi positive semi-deﬁnite theoretically guaranteed conduct sub-inference existing dpp-map algorithms e.g. greedy dppmap algorithm proof lemma appendix. lemma ˜lyi third order apply bwdpp-map need ﬁrst partition given kernel form almost block diagonal matrix deﬁned partition unique. trivial partition arbitrary kernel partition i.e. regarding whole matrix single block. leave study ﬁnding optimal partition work. provide heuristic rule partition called γ-partition performs well experiments. deﬁnition γ-partition deﬁned partitioning kernel almost block diagonal form deﬁned maximum number blocks every off-diagonal matrix size non-zero area bottom left exceed heuristic obtain γ-partition kernel ﬁrst identify series non-overlapping dense square submatrices along main diagonal many possible. next adjacent square sub-matrices main diagonal merged size non-zero area corresponding off-diagonal sub-matrix exceeds noted kernel could subject γpartition ways different values taking γ-partition kernel different values obtain balance computation cost optimization accuracy. smaller implies smaller achievgenerally speaking partition kernel size sub-kernels approximately reduce computational complexity times. larger implies larger computation reduction. able γ-partition thus smaller computation reduction. hand smaller means smaller degree interaction adjacent sub-inferences thus better optimization accuracy. fourth empirical illustration bwdpp-map given fig. greedy algorithm used sub-inferences bwdpp-map. synthetic kernel size ﬁxed realization area non-zero entries kernel ﬁrst speciﬁed uniformly randomly choosing size sub-kernels size non-zero areas off-diagonal sub-matrices next vector generated item separately following standard normal distribution. finally non-zero entries speciﬁed previous step entry value given fig. provides example synthetic kernel. generate synthetic kernels described above. synthetic kernel take γ-partition bwdpp-map. performance directly applying greedy algorithm original unpartitioned kernel used baseline. results fig. show bwdpp-map runs much faster baseline. increase runtime drops inference accuracy degrades within tolerable range. connection bwdpp-map sub-inference algorithm dpp-map inference algorithm used plugand-play fashion sub-inference procedure bwdpp. natural connection bwdpp-map corresponding dpp-map algorithm. relation given following result. theorem dpp-map algorithm bwdppmap sub-inference maps positive semi-deﬁnite matrix subset indices i.e. bwdpp-map equivalent applying following steps successively almost block diagonal kernel deﬁned proof theorem appendix. theorem states bwdpp-map essentially series bayesian belief updates update conditional kernel contains information previous selection conditional distribution figure top-left entries synthetic kernel. log-probability ratio runtime ratio t/tref obtained using bwdpp-map kernel different γ-partition pref tref baseline performance directly applying greedy algorithm original unpartitioned kernel. results averaged kernels. error represents conﬁdence level. result. equivalent form allows compare bwdppmap directly method applying entire kernel. latter inference entire time former inference sequence smaller subsets ...ym. concretely i-th update subset added kernel j=yj information previous selection result incorporated kernel generate conditional kernel. finally dpp-map inference performed conditional kernel select x··· time-series observations represents d-dimensional observation time denote segment observations time interval represent different segments observations different intervals explicitly denoting beginning ending times intervals necessary. method build existing metrics. dissimilarity metric denoted measures dissimilarity between arbitrary segments quality-diversity decomposition kernel given items {··· kernel written gram matrix columns vectors representing items popular decomposition kernel deﬁne qiφi measures quality item viewed angle vector diversity features measures similarity items therefore deﬁned diag diag quality vector consisting similarity matrix consisting qualitydiversity decomposition allows construct separately address different concerns utilized construct kernel cpd. bwdppcpd bwdppcpd two-step method described follows. step based dissimilarity metric preliminary change-point candidates created. consider moving pair adjacent windows xt−w+t xt+t+w along w··· size local windows. then large value adjacent windows i.e. suggests change-point likely occur time obtain series values local peaks mean values marked corresponding locations t··· selected form preliminary change-point candidates {··· change-point candidates step treat {··· bwdpp items select subset ﬁnal estimate change-points. next bwdpp similarity matrix deﬁned address fact true locations change-points along timeline tend diverse since states would change rapidly. done assigning high similarity score items close other. speciﬁcally deﬁne parameter representing position diversity level. finally taking γ-partition kernel almost block diagonal form bwdpp-map used select change-points favours quality diversity discussion rich studies metrics problem. choice dissimilarity metric ﬂexible could well-tailored characteristics data. present examples used experiments. symmetric kullback-leibler divergence numerator likelihood segments follows different models respectively denominator segments together follows single model practice plug maximium likelihood estimates parameters e.g. assume timeseries segment {x··· follows homogeneous poisson process occurring time i-th event log-likehood bwdppcpd method evaluated real-world time-series data. firstly three classic datasets examined namely well-log data coal mine disaster data jones industrial average return data small data size. accurate model characterize data making task harder. experiments numbers items varies hundreds thousands where except bwdpp-map algorithms perform inference within reasonable cost time large kernel scale. human activity detection speech segmentation provide comparison. dissimilarity metric poisson processes used coal mine disaster experiments gaussian models symkl used. well-log data well-log contains measurements nuclear magnetic response taken drilling well. example varying gaussian mean changes reﬂect stratiﬁcation earth’s crust outliers removed prior experiment. shown fig. changes detected bwdppcpd. coal mine disaster data coal mine disaster standard dataset testing method consists accidents occurring rates accidents believed changed times task detect them. bwdppcpd detection result shown fig. agrees jones industrial average return djia contains daily return rates jones industrial average example varying gaussian variance changes caused events potential macroeconomic effects. four changes data detected bwdppcpd matched well important events compared change detected corresponds date stock market crash ended. shows bwdppcpd discovers information data. human activity detection hasc contains human activity data collected portable three-axis accelerometers task segment data according human behaviour changes. fig. shows example hasc. performance best algorithm number correctly found changes number detected changes number ground-truth changes. score could viewed overall score balances rcl. result shown table parameters attain best results algorithms. receiver operating characteristic curve often used evaluate performance different precision recall true positive rate false positive rate given −prc. bwdppcpd different levels obtained tuning position diversity parameter rulsif tuning threshold speech segmentation tested datasets speech segmentation. ﬁrst dataset called hubm subset mandarin broadcast news speech released ldc. second dataset called telrecord consists telephone conversations around -min long collected real-world call centres. acoustic features -order mfccs extracted time-series data. speech segmentation segment audio data acoustically homogeneous segments e.g. utterances single speaker non-speech portions. datasets contain utterances hesitations variety changing background noises presenting great challenge cpd. bwdppcpd method different kernel partition tested classic segmentation methods distbic used comparison. post-processing step based values also taken reduce false alarms bwdppcpd. experiment results table shows bwdppcpd outperforms distbic datasets. addition comparing results obtained using found faster slightly worse performance. agrees analysis bwdpp-map using different γ-partition tradeoff speed accuracy. demonstrated useful change-point detection problem. bwdpp-based change-point detection method bwdppcpd shows superior performance experiments several real-world datasets. almost block diagonal kernels suit change-point detection problem well bwdpps achieve that. theoretically bwdpp-map could applied block tridiagonal matrices without modiﬁcation. remains studied theoretical issues regarding exact approximate partition kernel form alblock diagonal matrix potential bwdpp applications also worth exploration. acer kayaaslan aykanat recursive bipartitioning algorithm permuting sparse square matrices block diagonal form overlap. siam journal scientiﬁc computing c–c. buchbinder feldman naor schwartz tight linear time approximation unconstrained submodular maximization. foundations computer science ieee annual symposium ieee. chen gopalakrishnan speaker environment channel change detection clustering bayesian information criteproc. darpa broadcast news transcription rion. understanding workshop virginia usa. kawahara yairi machida change-point detection timeseries data based subspace identiﬁcation. data mining icdm seventh ieee international conference ieee. lavielle lebarbier application mcmc methods multiple change-points problem. signal processing yamada collier sugiyama change-point detection time-series data relative density-ratio estimation. neural networks", "year": 2015}