{"title": "Intriguing Properties of Randomly Weighted Networks: Generalizing While  Learning Next to Nothing", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "Training deep neural networks results in strong learned representations that show good generalization capabilities. In most cases, training involves iterative modification of all weights inside the network via back-propagation. In Extreme Learning Machines, it has been suggested to set the first layer of a network to fixed random values instead of learning it. In this paper, we propose to take this approach a step further and fix almost all layers of a deep convolutional neural network, allowing only a small portion of the weights to be learned. As our experiments show, fixing even the majority of the parameters of the network often results in performance which is on par with the performance of learning all of them. The implications of this intriguing property of deep neural networks are discussed and we suggest ways to harness it to create more robust representations.", "text": "training deep neural networks results strong learned representations show good generalization capabilities. cases training involves iterative modiﬁcation weights inside network back-propagation. extreme learning machines suggested ﬁrst layer network ﬁxed random values instead learning paper propose take approach step almost layers deep convolutional neural network allowing small portion weights learned. experiments show ﬁxing even majority parameters network often results performance performance learning them. implications intriguing property deep neural networks discussed suggest ways harness create robust representations. deep neural networks create powerful representations successively transforming inputs multiple layers computation. much expressive power attributed depth; theory shows complexity computed function grows exponentially depth renders deep networks expressive shallower counterparts number parameters. moreover data representation eﬃcient information-theoretic point view increasingly deeper network designs thousand layers deep modern architectures contain millions billions parameters often exceeding number training samples millions suggests networks could prone over-ﬁtting otherwise highly-overparameterized could much compact; supported network pruning methods able retain network accuracy removing many weights re-training. counter-intuitively even shown network pruned selecting arbitrary subset ﬁlters still recover original accuracy hinting large redundancy parameter space. large parameter space explain current methods machine learning tend data-hungry. could weights require updating equally useful common optimization pipeline involves iterative gradient-based method used update weights network minimize target loss function. instead training weights suggest almost extreme opposite network weights initialized randomly certain fraction updated optimization process. experiments show negative eﬀect network performance magnitude surprisingly small respect number parameters learned. eﬀect holds range architectures conditions datasets ruling option speciﬁc peculiar combination thereof. discuss explore various ways selecting subsets networks parameters learned. best knowledge others shown analytic properties randomly weighted networks ﬁrst explore eﬀects keeping weights randomly initialized values multiple layers. claim successfully training mostly-random networks implications current understanding deep learning speciﬁcally moreover ability opens interesting possibilities overloading networks keeping ﬁxed backbone subset parameters re-training small set. used create cheap ensemble models nevertheless diverse enough outperform single model. rest paper organized follows. section describe related work. followed description method extensive experiments limit learned parameters various ways. discussion concluding remarks. reproducibility make code publicly available. random features long line research revolving around randomly drawn features machine learning. extreme learning machines show utility keeping layer neural ﬁxed usually done layers within layers across multiple layers. shown picking random features merits matching kernels data. analytically shown useful properties random nets gaussian weights. mentioned work many theoretical works deep neural networks assume speciﬁc conditions known hold practice; show empirically happens weights selected randomly throughout various layer network within layers. fixed features recent result showing quite surprisingly using ﬁxed hadamard matrix ﬁnal classiﬁcation layer hinder performance classiﬁer. contrast impose constraints values ﬁxed weights evaluate eﬀect ﬁxing many diﬀerent subsets weights throughout network. compression/filter pruning many works attempt learn compact representation pruning unimportant ﬁlters example compressing network learning performing tensor-decompositions ﬁlter representations regularizing structure sparse representation designing networks compact begin with either architectural changes learning discrete weights e.g. network interpretability many works attempting dissect representation learned within networks order develop intuition inner workings improve debugging representation extract meaningful explanation ﬁnal output network. work analyzes feature selectivity network depth progresses. also attempt activities speciﬁc ﬁlters back pixel-space. methods mapping output network speciﬁc image regions suggested either using gradient-based methods biologically inspired ones others either generate images maximize response image speciﬁc category attempt invert internal representations order recover input image another line work shows emergent representations relate interpretable concepts. tried supervise nets become interpretable interpretability network often deﬁned correlation unit within concept semantic meaning. keep features random argue similarly powerful features emerge withnecessarily interpretable. admittedly interpretability deﬁned examining population neurons e.g. kind interpretation depends function learns outputs neurons given concept. regard supervised network construction interpretable. standard settings weights neural network learned order minimize loss function goal test many parameters actually learned eﬀect ﬁxing parameters ﬁnal performance. work concern visionrelated tasks dealing image classiﬁcation though future intend show non-vision tasks well. setting network typically deﬁned series convolutional layers interleaved non-linearities ﬁnal layer usually fully-connected though cast form convolutional layer well. splitting disjoint sets experiment weights allow updated optimizer. either randomly initialized zero. i∈wli partition selected weights layer roi×ii×ki×ki simplicity treat ﬁlters homogeneous manner e.g. ﬁlters shortcut ﬁlters used resnets treated special way. addition selecting subset coeﬃcients dimension always implemented choosing ﬁrst coeﬃcients dimension depend speciﬁcs scenario currently testing. admittedly could lead suboptimal results however goal learn optimal sparse connections rather show power inherent choosing arbitrary subset given size. figure training parameters deep networks generalize surprisingly well small fraction parameters learned. y-axis validation top-% accuracy. unshaded area speciﬁes fraction ﬁlters learned convolutional layer. shaded areas specify ﬁxed integer number ﬁlters learned conv. layer. blue lines represent training except speciﬁed frozen fraction. green lines represent speciﬁed fraction weights zeroed out. figure training subsets weights slicing diﬀerent dimensions weight tensor fraction total number weights selected dimension. slice limiting output ﬁlters; limiting input weights subsets weights according spatial location inside kernel. fraction along selected dimension size slice along selected dimension learning subset ﬁlters outperforms learning subset weights ﬁlter. batch normalization always batch normalization layers part original model. however layers also optionally learnable parameters addition running estimates mean variance. multiplication bias term ﬁlter. hence number total weights layers amount usually tens thousands dependent number convolutional layers followed layers number output channels each. example densenets although layers usually thought auxiliary layers improve network convergence note beneﬁts learning parameters task dependent manner beyond stabilization optimization process. exempliﬁed work parameters layers trained task. experiments take step further show learned tuning parameters network vgg- well vgg- without batch-normalization baselines modify reference implementation. evaluate many diﬀerent conﬁgurations limit number training epochs experiments epochs cases also reduce widening factor default experiments perform full experiments performed using ubuntu machine single titan-x pascal using pytorch deep-learning framework. unless speciﬁed otherwise models optimized using momentumsgd. ﬁrst step establish dimension better slice ﬁlter tensor layer. recall learning subset ﬁrst dimension translates learning ﬁlters training parameters ﬁlter learning second fourth dimension means learning ﬁlters choosing subset ﬁlter coeﬃcients each. intuitively allowing subset ﬁlters fully learned allows learned ﬁlters make full-use features incoming connections. hand learning ﬁlters selecting inputs ﬁxed seems like worse option cause important features missed; speciﬁcally happen ﬁxed bias term cause non-linearities zero eﬀect incoming features rendering invisible subsequent layers. tested densenet architecture cifar- sliced convolutional layers along four diﬀerent dimensions limited number epochs one. tested fraction dimension well ﬁxed-sized slice dimension minimum element slice. figure figure performance absolute number parameters training epochs. much performance preserved learning relatively smaller parameters even zeroing rest densenet eﬃcient sense degrades gradually decrease number learned parameters. ﬁtting lines performance fraction learned parameters follows logarithmic curve various models. plot resulting performance total number parameters. dimension minimal size selected slice this slicing third fourth dimensions results much larger number learned parameters. figure show results learning slices given size along ﬁrst second dimensions layer. conclusion section above turn experiment various architectures conﬁgurations limiting selecting subset ﬁlters using ways described section discuss results below. figure shows top- accuracy training epochs. best performer densenets. alexnet failed learn non-trivial fractions ﬁlters layer. note ﬁxed weights zeroed-out. zeroing weights eﬀectively reduces number ﬁlters network. using ﬁlters zeroing rest achieves performance densenets. shaded areas figure specify learning constant integer number ﬁlters layer. interestingly learning single ﬁlter layer result non-trivial accuracy. fact zeroing nonlearned weight resulting single-ﬁlter layer still able much chance around cifar- resnets. vgg- without layers failed converge performance better chance settings. parameter eﬃciency another view results seen figure plots performance obtained total number parameters logarithmic scale. even zeroing non-learned parameters densenets attain decent performance less parameters roughly eighth original amount. straight line performance fraction learned parameters performance grows logarithmically number parameters larger slope scenarios cifar- cifar dense wide residual networks. table summarizes experiment. non-trivial accuracy reached learning single layer subsets. furthermore cases learning last fully-connected layer proves inferior another layer. example wide-resnets learning layer attains top- accuracy cifar- much less either middle blocks. number parameters indeed much lower note grows linearly number classes middle blocks remains constant practical point view indicate ﬁne-tuning single layer network towards task last layer necessarily best choose. nevertheless ﬁne-tuning additional layer middle prove useful additional parameter cost quite modest. wide dense residual network. learning parameters layers non-trivial performance using densenets e.g. cifar cifar- longer case please refer table integer number ﬁlters layer) used layers make diﬀerence. however starting diﬀerence becomes smaller. likely representative power introduced parameters becomes less signiﬁcant parameters introduced ﬁgure also performance attainable training weights architecture notably using percent parameters induces little loss accuracy densenet achieve full-accuracy ﬁlters layer. various ways distribute fraction parameters trained network achieve similar accuracies. achieving high accuracy much ﬁlters learned also consistent result full training runs full runs also full training sessions densenet limited parameters cifar-. speciﬁcally ﬁlters arbitrarily zeroed achieve almost baseline performance please refer table numerical results experiment. note learning single ﬁlter layer densenets outperform large cifar- cifar-. figure learning diﬀerent fractions network without batch-norm layers. batchnormalization accounts much accuracy number weights reduced become less signiﬁcant using large fraction weights. black dashed lines using all-weights batch-norm. figure learning fraction weights full runs learning cifar- done eﬀectively weights ﬁxed random initialized state. cifar- larger portion parameters needed green blue dashed lines represent results weight learned densenets respectively. finally perform larger scale tiny-imagenet experiment. choose tiny-imagenet dataset variant larger imagenet dataset. contains two-hundred image categories training validation images each. images downscaled pixels. experiment recent yellowfin optimizer found less need manual tuning sgd. train total epochs initial learning rate lowered factor iterations. architecture widen-factor architecture shown perform quite reasonably well another downs-scaled version imagenet train fully-parameterized version partial versions fractions ﬁlters conv. layer. results experiment summarized table value fully-parameterized version little importance. note parameters trained lose top- accuracy another convolutional layers. training network keeping weights ﬁxed enables creation cheap ensemble models share many weights vary remaining portion. example training densenet model learning weights requires roughly parameters model. total cost e.g. ensemble model size independent models. resulting ensemble diverse independently trained models? using densenets testing cifar- trained three ensemble models elements each. ﬁrst train fully-parametrized model epoch starting point train ensemble element additional epoch. experiment tried adam optimizer reported ensemble type best results. table ensemble models. ensemble models convolutional weight sharing elements perform similarly independently trained models though signiﬁcantly compact. relearning conv. weights report ensemble mean accuracy elements accuracy attained averaging ensemble’s predictions total number learned parameters trained ensembles fully parameterized models models varying layer models shared fraction convolutional weights table summarizes results. ﬁxing parameters already outperforms re-training layers. fully-parameterized ensemble indeed shows better variability solutions though using portion shared conv. weights signiﬁcantly less weights. perform analysis magnitude distribution weights within learned ﬁxed layers. motivated observation relatively small weights lead better generalization bounds analyze magnitudes weights convolutional layers experiment section convolutional layer record mean absolute weight value variance weights. layers ﬁxed weights report value well. mean variance ﬁxed weights determined random initialization. performance resulting network. network architectures robust ﬁxing weights others. learning also possible extremely small number ﬁlters learned convolutional layers little single ﬁlter layers. three simple applications described phenomena cheap ensemble models backbone ﬁxed network learning multiple representations small number parameters added task transferlearning learning middle layer ﬁnal classiﬁcation layer. intend explore directions future work well testing reported phenomena additional non-vision related tasks natural language processing reinforcement-learning.", "year": 2018}