{"title": "Population-Contrastive-Divergence: Does Consistency help with RBM  training?", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Estimating the log-likelihood gradient with respect to the parameters of a Restricted Boltzmann Machine (RBM) typically requires sampling using Markov Chain Monte Carlo (MCMC) techniques. To save computation time, the Markov chains are only run for a small number of steps, which leads to a biased estimate. This bias can cause RBM training algorithms such as Contrastive Divergence (CD) learning to deteriorate. We adopt the idea behind Population Monte Carlo (PMC) methods to devise a new RBM training algorithm termed Population-Contrastive-Divergence (pop-CD). Compared to CD, it leads to a consistent estimate and may have a significantly lower bias. Its computational overhead is negligible compared to CD. However, the variance of the gradient estimate increases. We experimentally show that pop-CD can significantly outperform CD. In many cases, we observed a smaller bias and achieved higher log-likelihood values. However, when the RBM distribution has many hidden neurons, the consistent estimate of pop-CD may still have a considerable bias and the variance of the gradient estimate requires a smaller learning rate. Thus, despite its superior theoretical properties, it is not advisable to use pop-CD in its current form on large problems.", "text": "estimating log-likelihood gradient respect parameters restricted boltzmann machine typically requires sampling using markov chain monte carlo techniques. save computation time markov chains small number steps leads biased estimate. bias cause training algorithms contrastive divergence learning deteriorate. adopt idea behind population monte carlo methods devise training algorithm termed population-contrastivedivergence compared leads consistent estimate signiﬁcantly lower bias. computational overhead negligible compared variance gradient estimate increases. experimentally show pop-cd signiﬁcantly outperform many cases observed smaller bias achieved higher log-likelihood values. however distribution many hidden neurons consistent estimate pop-cd still considerable bias variance gradient estimate requires smaller learning rate. thus despite superior theoretical properties advisable pop-cd current form large problems. estimating log-likelihood gradient respect parameters undirected graphical model restricted boltzmann machine challenging task. analytic calculation gradient infeasible often approximated using markov chain monte carlo techniques. getting unbiased samples model distribution requires markov chain convergence practice chain iterated ﬁxed amount steps quality samples often unknown. arguably k-step contrastive divergence algorithm often used training. training iteration cd-k markov chain initialized sample dataset steps block-gibbs-sampling performed. practice often obviously results considerable bias gradient approximation. even though bias bounded reported cause divergence long optimization difﬁcult detect heuristics moreover shown gradient ﬁeld approximation belong objective function introduction sampling schemes proposed gradient approximation prominent ones persistent contrastive divergence parallel tempering former uses persistent monte carlo chain training. initially started training example chain discarded gradient update successively reused following learning iterations. done hope chain stays close model distribution performing pcd-based gradient ascent. however requires small learning rate guarantee model distribution changes slowly enough compensate small mixing rate gibbs-sampling. solve problem suggested training sampling method runs multiple tempered replica chains. metropolis-based swapping operator allows samples swap chains order achieve faster mixing. paid higher computational cost. inherently parallel algorithm applied simultaneously full data performs step parallel markov chains every sample. makes serial algorithm difﬁcult transfer gpus whereas implementing straightforward. paper introduces different direction training based population monte carlo method sampling samples re-weighted using importance sampling application transition operator weighted samples unbiased. furthermore weights used re-sample points. present algorithm combining importance sampling learning. implemented efﬁciently without suffering high bias rbms small number hidden neurons. undirected graphical model bipartite structure. standard binary consists visible variables taking states hidden variables taking states joint distribution gibbs distribution energy weight matrix visible hidden bias vectors respectively. normalization conh typically unknown calculated summing possible states hidden visible units exponential min{n paper focus problem maximum log-likelihood ﬁtting distribution data gradient w.r.t. model parameters given ﬁrst term derivative computed analytically calculation second term requires intractable. instead samples used obtain estimate expectation distribution. training algorithm often used literature approximates expectation sample gained steps gibbs-sampling starting sample however distribution results running markov chain k-steps usually unknown therefore weighting scheme equation applied. instead known conditional distribution κ|x) proposal distribution importance sampling since limit thus estimator consistent. method makes equations create markov chain step states sampled. states importance weights estimated states re-sampled accordingly. re-sampling procedure results samples corresponds t-th state chain. chain thus depends choice chosen heavy tails reduce variance estimate. section introduce main contribution paper population-contrastivedivergence algorithm training. algorithm exploits features population-mcmc combining efﬁcient implementation cd-k bias reweighting scheme introduced pmc. rewrite log-likelihood gradient equation setting ep|v) estimating based re-weighting scheme equation this choose distribution hidden variables generated -steps block-wise gibbs-sampling starting i-th training example call following. proposal distribution conditional distribution visible variables given state hidden i.e. yields based considerations previous section follows estimate second term gradient given training results following estimate log-likelihood gradient refer pop-cd-k. setting leads cd-k. actual implementation advisable work increased numerical stability. algorithm appendix describes pop-cd pseudo code. following analyse properties pop-cd-k detail. analyse runtime costs well bias method take closer look behavior weights discuss alternative weighting schemes. equation produce noteworthy computational overhead compared entities compute additionally weights weights turn inexpensive compute distribution already computed calculating reuse already computed determining gradient update. thus cost compute negligible compared sampling cost. known equation biased reasons. ﬁrst reason samples used estimate gradient estimation second that even assuming unbiased estimator lead unbiased estimate easy remove bias using samples rewriting practice advisable employ instead usually small number samples used. case estimate high variance length gradient obtained vary several orders magnitude. variance leads small effective sample size becomes clear weight sample large also high probability weight small otherwise. therefore puts probability mass samples less likely partly counterbalanced weighting scheme. case probability sample could lead large weight sample large however would require much smaller probability sample thus probability drawing would small. alternative formulations approach weights therefore little information distribution enters learning process. could question whether beneﬁcial full sample sampling distribution instead deﬁned equation proposal distribution case weights become ˜p/κ typical choice block-wise gibbssampler ˜p/p) depends single different approach several samples together mixture proposal distribution advantage approach proposal τ→∞−→ distribution takes distribution information account. latter good proposal distribution assuming however computational cost computing importance weights rises linearly number samples mixture. compared pop-cd-k algorithm experimentally previously proposed methods. implemented algorithms machine learning library shark make code experiments available case acceptance manuscript. chose small artiﬁcial datasets bars stripes data used desjardins br¨ugge refer artiﬁcial modes well bigger real world datasets mnist letters. artiﬁcial modes created generate distributions gibbs-sampling problems setting control figure training curves bars stripes artiﬁcial modes hidden neurons different learning rates. pop-cd variants cannot distinguished. furthermore curve cannot distinguished pop-cd artiﬁcial modes. parameter datasets bars stripes artiﬁcial modes known lead bias causing log-likelihood diverge makes good benchmarks analysing bias different gradient estimation strategies. experiments except ones bars stripes performed mini batch learning. priori size batches thus also number samples generated estimation model mean divisor full dataset size. batch sizes bars stripes artiﬁcial modes letters mnist. momentum term experiments. ﬁrst experiments considered behaviour algorithm settings normalisation constant computed analytically. selected number hidden neurons compared algorithms pcd- pop-cd- pop-cd- parallel chains choose learning rates datasets. algorithm iterations bars stripes artiﬁcial modes respectively. mnist trained iterations letters trained iterations iterations evaluated log-likelihood every iterations. experiments repeated times reported log-likelihood values means trials. second experiments trained rbms large number hidden neurons. normalisation constant estimated using annealed importance sampling samples intermediate distributions. learning parameters iterations iterations letters mnist third experiments investigated bias variance methods. ﬁrst trained using hidden neurons four problems. number training iterations bars stripes artiﬁcial modes letters mnist. furthermore repeated large experiments training hidden units iterations mnist hidden units iterations letters. that calculated ground truth estimate true gradient. done intermediate chains. chain given iterations burn-in time afterwards samples taken compute estimate. then generated gradient estimates cd-k pop-cd-k finally estimated bias methods squared distance empirical average methods estimated ground truth variance expectation squared distance single samples mean. results ﬁrst experiments seen figure artiﬁcial datasets figure mnist letters. figure shows pop-cd- well pop-cd- outperformed cd-. proposed algorithm reached signiﬁcantly higher log-likelihood values diverged pop-cd show sign divergence. using sampling steps lead almost performance pop-cd. efﬁcient pop-cd performed better computationally expensive pt-. real datasets cause divergence. however algorithms performed almost letters pop-cd- still reaches higher likelihood values mnist stopped results second experiments given figure plots letters data look qualitatively same aside scaling abscissa. cases performed better pop-cd- pop-cd-. mnist diverged initial learning phase. popcd showed slow learning progress four algorithms exhibit similar behaviour. algorithms diverge long divergence less pronounced popcd-k cd-k. table shows measured bias variance third experimental setup. experiments hidden neurons cd-k always smaller variance pop-cd-k often order magnitude more. bias pop-cd-k contrast much smaller explains experimental results observed. experiments hidden units pop-cd-k larger bias well. experiments method performed even better compared computational much expensive sampling techniques however problems bias notably impair learning pop-cd slower require smaller learning rates stochastic gradient descent. fact pop-cd fares better smaller learning rates explained larger variance measured experiments table bias-variance-trade-off well known issue importance sampling based estimators. potentially bias variance importance sampling based estimators depends well proposal function approximates target distribution. thus results indicate used proposal function lead high variance thus optimal choice complex target distributions. working small sample size compared difﬁculty estimation problem proposal distribution estimate normalisation constant large variance leads biased estimate inverse normalisation constant. supported results experiments large number hidden units well bias estimates table observation steps gibbs-sampling help using pop-cd larger models explained analysis section variance estimator depends well sampled h—and thus proposal distribution p)—approximates distribution. higher closer gets reduces variance weights affects estimator gradient well estimator normalisation constant. supported results table different values proposed variant contrastive divergence algorithm inspired population monte carlo method. learning algorithm termed population-cd incorporates importance weights sampling scheme makes bias loglikelihood gradient estimate independent markov chain sampling operators used negligible computational overhead compared however comes cost increased variance estimate. bias method depends well empirical mean importance weights estimates normalisation constant distribution. contrast pop-cd consistent. problems small number hidden neurons clearly outperforms computational cost leading higher likelihood values. however rbms many hidden neurons pop-cd require large number samples achieve smaller bias cd—therefore still recommended case. reason current estimator relies simple proposal distributions information distribution samples thus estimator seen replacing unknown distribution known distribution worse distribution samples available markov chain. hence future work must investigate whether exists better choice proposal distribution bridging true distribution desjardins courville bengio vincent delalleau tempered markov proceedings chain monte carlo training restricted boltzmann machines. international conference artiﬁcial intelligence statistics volume jmlr salakhutdinov learning markov random ﬁelds using tempered transitions. bengio schuurmans lafferty williams culotta advances neural information processing systems schulz m¨uller behnke investigating convergence restricted boltzmann machine learning. ranzato bengio hinton lecun nips workshop deep learning unsupervised feature learning smolensky information processing dynamical systems foundations harmony theory. rumelhart mcclelland parallel distributed processing explorations microstructure cognition vol. foundations press sutskever tieleman convergence properties contrastive divergence. proceedings international conference artiﬁcial intelligence statistics volume jmlr tieleman training restricted boltzmann machines using approximations likelihood gradient. proceedings international conference machine learning", "year": 2015}