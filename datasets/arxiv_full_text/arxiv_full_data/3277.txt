{"title": "A simple coding for cross-domain matching with dimension reduction via  spectral graph embedding", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Data vectors are obtained from multiple domains. They are feature vectors of images or vector representations of words. Domains may have different numbers of data vectors with different dimensions. These data vectors from multiple domains are projected to a common space by linear transformations in order to search closely related vectors across domains. We would like to find projection matrices to minimize distances between closely related data vectors. This formulation of cross-domain matching is regarded as an extension of the spectral graph embedding to multi-domain setting, and it includes several multivariate analysis methods of statistics such as multiset canonical correlation analysis, correspondence analysis, and principal component analysis. Similar approaches are very popular recently in pattern recognition and vision. In this paper, instead of proposing a novel method, we will introduce an embarrassingly simple idea of coding the data vectors for explaining all the above mentioned approaches. A data vector is concatenated with zero vectors from all other domains to make an augmented vector. The cross-domain matching is solved by applying the single-domain version of spectral graph embedding to these augmented vectors of all the domains. An interesting connection to the classical associative memory model of neural networks is also discussed by noticing a coding for association. A cross-validation method for choosing the dimension of the common space and a regularization parameter will be discussed in an illustrative numerical example.", "text": "data vectors obtained multiple domains. feature vectors images vector representations words. domains diﬀerent numbers data vectors diﬀerent dimensions. data vectors multiple domains projected common space linear transformations order search closely related vectors across domains. would like projection matrices minimize distances closely related data vectors. formulation cross-domain matching regarded extension spectral graph embedding multi-domain setting includes several multivariate analysis methods statistics multiset canonical correlation analysis correspondence analysis principal component analysis. similar approaches popular recently pattern recognition vision. paper instead proposing novel method introduce embarrassingly simple idea coding data vectors explaining mentioned approaches. data vector concatenated zero vectors domains make augmented vector. cross-domain matching solved applying single-domain version spectral graph embedding augmented vectors domains. interesting connection classical associative memory model neural networks also discussed noticing coding association. cross-validation method choosing dimension common space regularization parameter discussed illustrative numerical example. keywords phrases multiple domains common space matching weight multivariate analysis canonical correlation analysis spectral graph embedding associative memory sparse coding. consider multiple domains getting data vectors. number domains denote domain. example images words. domain data vectors number data vectors dimension data vector. image feature vectors word vectors computed wordvec texts typically hundreds thousands millions. would like retrieve relevant words image query alternatively retrieve images word query. nothing related graph theory.) assume weight symmetric example subject certain constraints. supervised learning matching weights training data. handles problem semi-supervised learning missing observation simply letting unobserved weights formulation cross-domain matching regarded extension spectral graph embedding multi-domain setting similar approaches popular recently pattern recognition vision particular formulation reduces classical multivariate analysis statistics known multiset canonical correlation analysis letting connecting vectors across domains index class labels coded indicator variables treated domains; appear canonical discriminant analysis correspondence analysis. formulation becomes classical canonical correlation analysis hotelling letting becomes principal comfield sense nonzero elements domains overlap other. vectors domains represented points solution optimization problem applying single-domain version spectral graph embedding section review spectral graph embedding methods. section show coding solves minimization interesting connection classical associative memory model neural networks also discussed noticing coding section relations multivariate analysis methods explained. section show illustrative numerical example cross-domain matching. particular discuss crossvalidation method choosing dimension common space regularization parameter; resample matching weights discussing cross-domain matching review spectral graph theory consider extra constraints section result used cross-domain matching section following argument based spectral clustering particular normalized graph laplacian spectral embedding reduces problem section letting solution optimization problem denote rp×p matrices satisfying inverse matrix denoted easily computed cholesky decomposition spectral decomposition symmetric simple form. data matrices rnd×pd domains deﬁned rn×p concatenate transformation matrices deﬁne rp×k vectors common space also concatenated deﬁne rnd×k rn×k matching weight matrices rn×n using matrices transformation written error function written section adding regularization term error function objective function becomes solution thus cross-domain matching solved single-domain version spectral graph embedding. important point large data matrix expressed would better rewrite constraints terms cross-domain matching. notice diag diagn diagt simplicity assume regularization matrix written block diagonal matrix diag. work edges graph data analysis. data vector coded deﬁne data matrix re×p concatenating matching weight order since minimization equivalent maximization tra). therefore cross-domain matching interpreted kind input patterns coded interestingly idea found classical neural network models. part memorized vector used recalling whole vector auto-associative correlation matrix memory associative memory recall would subject future research work domain matching becomes version mcca connections sets variables speciﬁed coeﬃcients another version mcca discussed extensively takane hwang abdi simplicity assumed regularization matrix written block diagonal constraints expressed matrix diagin. constraints correspond takane hwang abdi generated grid point respectively deﬁning underlying true associations linked vectors other except within domain. true weights vectors ¯wde weights across grid points zero. numbers nonzero elements respectively happens wrong results shown fig. observed distances common space disturbed fig. situation becomes worse fig. possible make reasonable data-retrieval more. important choose appropriate unknown reality compute error observed however ﬁtting error fig. work well. ﬁtting error minimized prediction unlinked pairs vectors good seen fig. another issue notice fig. ﬁtting error monotone increasing becomes monotone rescale estimating true error performed cross-validation analysis follows. nonzero elements resampled make words elements deﬁned w∗de generated bernoulli trial number nonzero elements remaining matrix average error. cross-validation error shown fig. plot similar fig. successfully choose fact shimodaira showed asymptotically cross-validation error unbiasedly estimates true error", "year": 2014}