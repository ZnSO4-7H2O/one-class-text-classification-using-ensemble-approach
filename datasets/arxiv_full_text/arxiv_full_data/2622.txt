{"title": "Low-dimensional Data Embedding via Robust Ranking", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We describe a new method called t-ETE for finding a low-dimensional embedding of a set of objects in Euclidean space. We formulate the embedding problem as a joint ranking problem over a set of triplets, where each triplet captures the relative similarities between three objects in the set. By exploiting recent advances in robust ranking, t-ETE produces high-quality embeddings even in the presence of a significant amount of noise and better preserves local scale than known methods, such as t-STE and t-SNE. In particular, our method produces significantly better results than t-SNE on signature datasets while also being faster to compute.", "text": "freedom mapping intrinsically high-dimensional representation lower-dimensional embedding. simple example mapping uniformly distributed points two-dimensional circle one-dimensional line; regardless embedding points line always violate similarity constraints. paper cast triplet embedding problem joint ranking problem. embedding object remaining object naturally ranked distance triplet expresses object ranked higher object ranking therefore triplet embedding viewed mapping objects euclidean space joint rankings belonging query objects consistent possible. order embedding dene loss triplet minimize losses triplets. initially triplet loss unbounded. however order make method robust noise apply novel robust transformation caps triplet loss constant. method t-exponential triplet embedding inherits heavy-tail properties t-ste producing high-quality embeddings signicantly robust noise method. figure illustrates examples embeddings subset data points mnist dataset using t-ste proposed method. triplets synthetically generated sampling random point -nearest neighbors point another point located away embeddings similar noise triplets however ‘reversing’ triplets t-ste fails produce meaningful embedding t-ete almost unaected noise also apply t-ete method dimensionality reduction develop technique samples subset triplets high-dimensional space low-dimensional representation satises corresponding ranking. quantify importance triplet non-negative weight. show even small carefully chosen subset triplets capture sucient information local well global structure data produce high-quality embeddings. proposed method outperforms commonly used t-sne dimensionality reduction many cases much lower complexity. triplet embedding ranking section formally dene triplet embedding problem. denote objects. suppose feature representation objects unknown. however abstract describe method called t-ete nding low-dimensional embedding objects euclidean space. formulate embedding problem joint ranking problem triplets triplet captures relative similarities three objects set. exploiting recent advances robust ranking t-ete produces high-quality embeddings even presence signicant amount noise better preserves local scale known methods t-ste t-sne. particular method produces signicantly better results t-sne signature datasets also faster compute. introduction learning metric embedding objects based relative similarities central problem human computation crowdsourcing. application domain includes variety dierent elds recommender systems psychological questionnaires. relative similarities usually provided form triplets triplet expresses object similar object object similarity function unknown even quantied. object referred query object objects test objects. triplets typically gathered human evaluators data-collecting mechanism amazon mechanical turk. types constraints also used side information semi-supervised metric learning clustering given relative similarity comparisons objects goal triplet embedding representation objects metric space constraints induced triplets satised much possible. words embedding reect underlying similarity function constraints generated. earlier methods triplet embedding include generalized non-metric multidimensional scaling crowd kernel learning stochastic triplet embedding extension t-distributed major drawback previous methods triplet embedding performance drop signicantly small amount noise introduced data. noise arise dierent reasons. instance human evaluator dierent similarity function comparing objects result might exist conicting triplets reversed test objects. another type noise could insucient degree https//www.mturk.com information relative similarities objects available form triplets. triplet ordered tuple represents constraint relative similarities objects type object similar object object denote triplets available objects given triplets triplet embedding problem amounts nding metric representation objectsy similarity constraints imposed triplets satised much possible given distance function embedding. instance case euclidean distance want reason require constraints satised embedding exist inconsistent and/or conicting constraints among triplets. common phenomenon triplets collected human evaluators crowdsourcing consider triplet embedding problem ranking problem imposed constraints specically triplet seen partial ranking result query given results namely triplet constraint species result relatively higher rank setting order closeness test objects query object determines ranking objects. dene non-negative loss associated triplet constraint reect ranking constraint loss monotonically increasing function pairwise distance properties ensure whenever dene triplet embedding problem minimizing ranking losses triplets corrupted noise loss even single inconsistent triplet dominate total objective result poor performance. order avoid eect introduce robust transformation individual loss triplet constant. capping helps avoid noisy triplets produce high-quality embeddings even presence signicant amount noise. robust loss transformations introduce generalized logt expt functions generalization standard functions respectively. generalized logt function temperature parameter dened max. similarly standard recovered limit figure illustrate expt logt functions several values major dierence standard functions familiar distributive properties hold general expt expt expt logt logt logt. important property expt decays zero slower values motivates dening heavy-tailed distributions using expt function. specically t-exponential family distributions dened generalization exponential family using expt function place standard function note desired. moreover derivative transformed loss along additional property loss function converges constant i.e. transformation develop robust ranking approach problem triplet embedding presence noise constraints. loss ranking associated triplet loss non-negative satises properties valid loss ranking discussed earlier. note heavy-tail expt function loss function encourages relatively higher-satisfaction ranking compared e.g. standard function. dening loss triplet ranking loss formulate objective triplet embedding problem minimizing robust transformations individual losses which call method t-exponential triplet embedding note loss triplet summation capped additionally gradient objective function respect positions objects dened probability triplet satised. setting recovers t-ste formulations respectively. maximize joint probability triplets satised embedding poor performance t-ste presence noise explained fact capping logsatisfaction probabilities triplet therefore figure generalization nearest-neighbor performance mnist scenes generalization error nearest-neighbor error generalization accuracy presence noise nearest-neighbor accuracy presence noise. experiments generalization nearest-neighbor error experiments start smaller number dimensions increases noise experiments figures best viewed color. consider case high-dimensional representation {xi}n provided objects. t-ete method hand following question given high-dimensional representation objects possible lower-dimensional representation objects satisfying ranking constraints formed based relative similarities representation note total number triplets formed objects trying satisfy possible triplets computationally expensive. however argue triplets redundant contain amount information relative similarity objects. instance consider triplets located away neighbors other. given provides extra information placements long located close together note object nearby objects relatively short distance specify local structure object whereas located away determine global placement space. matter query object would like consider triplets preserve local global structure data. following discussion above emphasize preserving local information explicitly choosing test object among nearest-neighbors query object global information object preserved considering small number objects uniformly sampled located farther away. leads following procedure sampling informative triplets. object choose object m-nearest neighbors then sample outlier object uniformly located farther away object. equivalent sampling triplet uniformly random conditioned test object chosen among m-nearest neighbors equal number nearest-neighbors outliers point results triplets total. figure embedding food dataset using t-ste t-ete methods. appear clear separation clusters three dierent clusters food evident vegetables meals creams deserts breads cookies original t-ete formulation aims satisfy triplet equally likely. would reasonable cases side information extent constraint provided. however given high-dimensional representation objects assumption accurate. words ratio pairwise similarities objects specied triplet vary signicantly among triplets. account variation introduce notion weight triplet reect extent triplet needs satised. formally ωijk denote weight associated triplet {ωijk} denote triplet weights. weighted t-ete formulated minimizing weighted capped losses triplets finally assign weights sampled triplets note loss ratio inversely proportional well triplet satised embedding. suggests using inverse loss ratios triplets high-dimensional space weights associated triplets. formally constant scaling factor pair distance nearest neighbor. choice scaling adaptively handles dense well sparse regions data distribution. finally choice function rather using expt emphasis distances objects high-dimensional space. pseudo-code algorithm shown algorithm practice dividing weight maximum weight adding constant positive bias weight improves results. note sampling weighting triplets using calculating gradient loss requires calculating pairwise distances objects high-dimensional space cient methods calculate m-nearest neighbors low-dimensional embedding. many cases results huge computational advantage complexity t-sne. experiments section conduct experiments evaluate performance t-ete triplet embedding well application weighted t-ete non-linear dimensionality reduction. experiments compare t-ete following triplet embedding methods gnmds t-ste. evaluate generalization performance dierent methods means satisfying unseen triplets nearest-neighbor error well robustness constraint noise. also provide visualization results real-world datasets. next apply weighted tete method non-linear dimensionality reduction compare result t-sne method. code t-ete method well experiments publicly available upon acceptance. comparisons. results shown figure seen t-ete performs good best performing method even better generalization nearest-neighbor error. indicates t-ete successfully captures underlying structure data scales properly number dimensions. generalization nearest-neighbor error evaluate performance dierent methods means generalization unseen triplets well preserving nearestneighbor similarity. part experiments consider mnist digits scenes datasets. synthetic triplets generated mentioned earlier evaluate generalization performance perform -fold cross validation report fraction held-out triplets unsatised function number dimension. quantity indicates well method learns underlying structure data. additionally calculate nearest-neighbor error function number dimensions. nearest-neighbor error measure well embedding captures pairwise similarity objects based relative robustness noise next evaluate robustness dierent methods triplet noise. evaluate performance generate dierent test datasets number triplets training set. noise level randomly subsample subset training triplets reverse order objects. generating embedding evaluate performance test report fraction test triplets satised well nearestneighbor accuracy. results shown figure seen performance methods starts drop immediately small amount noise added data. hand t-ete robust triplet noise performance almost unaected noise. veries t-ete eectively applied real-world datasets large portion triplets corrupted noise. figure dimensionality reduction results using t-sne gure) weighted t-ete gure) wine sphere swiss roll faces coil- mnist usps letters datasets. experiments. figures best viewed color. visualization results provide visualization results food music datasets. figures illustrate results food dataset using t-ste t-ete respectively. initialization data points used methods. seen clear clusters evident using t-ste method. hand t-ete reveals three main clusters data visualization music dataset using t-ete method shown figure result compared using t-ste method. distribution artists neighborhood structure similar methods jason davis brian kulis prateek jain suvrit inderjit dhillon. information-theoretic metric learning. proceedings international conference machine learning. daniel ellis brian whitman adam berenzweig steve lawrence. quest ground truth musical artist similarity. proceedings international conference music information retrieval paris france ville hyvönen teemu pitkänen sotiris tasoulis elias jääsaari risto tuomainen liang wang jukka corander teemu roos. fast k-nn search. arxiv preprint arxiv. eric zhishan xiang zhang vladimir jojic wang. metric learning relative comparisons minimizing squared residual. ieee international conference data mining. ieee journal machine learning research naudts. deformed exponentials logarithms generalized thermostatistics. physica http//arxiv.org/pdf/cond-mat/ naudts. estimators escort probabilities phi-exponential families statistical physics. journal inequalities pure applied mathematics naudts. generalized thermostatistics based deformed exponential logarithmic functions. physica maaten weinberger. stochastic triplet embedding. ieee international workshop machine learning signal processing. doihttp//dx.doi.org/./mlsp.. hyokun parameswaran raman vishwanathan. ranking robust binary classication. proceedings international conference neural information processing systems cambridge http//dl.acm.org/citation.cfm?id=. meaningful regions using t-ete method. noise triplets collected human evaluators. additionally t-ete results nearest-neighbor error data points compared error using t-ste. dimensionality reduction results apply weighted triplet embedding method dimensional visualization following datasets wine sphere swiss roll faces erent pose lighting) coil- mnist usps compare results obtained using t-sne method. experiments method bias results shown figure seen method successfully preserves underlying structure data produces high-quality embedding datasets underlying low-dimensional manifold clusters points hand cases t-sne over-emphasizes separation points therefore tears manifold. eect happens clusters e.g. usps dataset. embedding forms multiple separated sub-clusters objective function also enjoys better convergence properties converges good solution using simple gradient descent. eliminates need complex optimization tricks momentum early over-emphasis used t-sne. conclusion introduced ranking approach embedding objects low-dimensional space given relative similarity constraints form triplets. showed method t-ete robust high level noise triplets. generalized method weighted version incorporate importance triplet. applied weighted triplet embedding method develop dimensionality reduction technique outperforms commonly used t-sne method many cases lower complexity better convergence behavior. references sameer agarwal josh wills lawrence cayton gert lanckriet david kriegman serge belongie. generalized non-metric multidimensional scaling. proceedings eleventh international conference articial intelligence statistics. juan puerto rico. ehsan amid aristides gionis antti ukkonen. kernel-learning approach semi-supervised clustering relative distance comparisons. joint european conference machine learning knowledge discovery databases. springer ehsan amid antti ukkonen. multiview triplet embedding learning attributes multiple maps. proceedings international conference machine learning http//jmlr.org/proceedings/papers/ v/amid.pdf repository. research.cs.aalto./pml/software/dredviz/ web.mit.edu/cocosci/isomap/datasets.html www.cs.columbia.edu/cave/software/softlib/coil-.php www.cs.nyu.edu/~roweis/data.html", "year": 2016}