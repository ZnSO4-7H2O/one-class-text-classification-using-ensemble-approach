{"title": "Optimizing expected word error rate via sampling for speech recognition", "tag": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "abstract": "State-level minimum Bayes risk (sMBR) training has become the de facto standard for sequence-level training of speech recognition acoustic models. It has an elegant formulation using the expectation semiring, and gives large improvements in word error rate (WER) over models trained solely using cross-entropy (CE) or connectionist temporal classification (CTC). sMBR training optimizes the expected number of frames at which the reference and hypothesized acoustic states differ. It may be preferable to optimize the expected WER, but WER does not interact well with the expectation semiring, and previous approaches based on computing expected WER exactly involve expanding the lattices used during training. In this paper we show how to perform optimization of the expected WER by sampling paths from the lattices used during conventional sMBR training. The gradient of the expected WER is itself an expectation, and so may be approximated using Monte Carlo sampling. We show experimentally that optimizing WER during acoustic model training gives 5% relative improvement in WER over a well-tuned sMBR baseline on a 2-channel query recognition task (Google Home).", "text": "dalen gales found practical limit around words conventional determinization algorithm around words improved memory-efﬁcient algorithm time complexity also means error marking unlikely compatible on-the-ﬂy lattice generation. paper propose alternative approach latticebased word-level embr training. gradient expected loss optimized training written expectation allowing gradient approximated sampling. samples paths lattice used conventional smbr training. approach extremely ﬂexible places almost restriction form loss used. approach perform word-level embr training speech recognition tasks show improves compared smbr training. knowledge also ﬁrst work investigate word-level embr training state-of-the-art acoustic models based neural nets. similar forms sampled training proposed previously. graves performed sampled embr training special case model noting special structure allows trivial sampling specialized approaches reduce variance samples speech recognition viewed simple reinforcement learning problem action consists outputting given word sequence. view training learning stochastic policy sampling-based approach described similar reinforce algorithm though sample probability distribution globally normalized instead locally normalized differences form variance reduction used. noted previous investigations reported better optimizing state-level phonemelevel criterion word-level criterion however reported opposite word-level criterion effective experiments investigate question systematically. remainder paper review sequence-level probabilistic model used training describe training discuss embr training describe sampling-based approach training describe experimental results section review weighted ﬁnite state transducer globally normalized obtain probabilistic model. model used training deﬁned way. weighted deﬁned real semiring naturally turned probabilistic model sequence output labels edge weighted real-valued weight optional output label taken speciﬁed output alphabet optional input label taken speciﬁed input alphabet. absence input output label usually destate-level minimum bayes risk training become facto standard sequence-level training speech recognition acoustic models. elegant formulation using expectation semiring gives large improvements word error rate models trained solely using crossentropy connectionist temporal classiﬁcation smbr training optimizes expected number frames reference hypothesized acoustic states differ. preferable optimize expected interact well expectation semiring previous approaches based computing expected exactly involve expanding lattices used training. paper show perform optimization expected sampling paths lattices used conventional smbr training. gradient expected expectation approximated using monte carlo sampling. show experimentally optimizing during acoustic model training gives relative improvement well-tuned smbr baseline -channel query recognition task index terms acoustic modeling sequence training sampling minimum bayes risk training shown effective train neural net–based acoustic models widely used state-of-the-art speech recognition systems training minimizes expected loss loss measures distance reference hypothesis. state-level training popular form training uses loss deﬁned frame level based clustered context-dependent phonemes subphonemes. computationally tractable elegant lattice-based formulation using expectation semiring shown perform favorably terms word error rate compared alternative losses proposed given prevalence word error rate evaluation metric natural consider using training. refer word-level edit-based training. however expected word edit distance lattice harder compute expected frame-level loss used smbr training. result many approximations true edit distance proposed lattice-based training gibson provides systematic review strengths weaknesses various approximations heigold show possible exactly compute expected word edit distance lattice ﬁrst expanding lattice using approach sometimes termed error marking impressive error marking computationally implementionally non-trivial increases size lattices. memory required determinization during error marking increases rapidly utterance length function loss speciﬁes output reference word sequence yref. broadly speaking concentrates probability mass sufﬁciently ﬂexible model trained convergence assign probability word sequence smallest loss. state-level minimum bayes risk training deﬁnes loss instead number frames current cluster index differs reference cluster index. sequence time-aligned reference cluster indices typically obtained using forced alignment. loss number disadvantages compared minimizing number word errors however advantage expected loss gradient computed tractably using elegant formulation based expectation semiring crucial property loss makes tractable loss path decomposed additively edges path. typically feasible perform smbr training full unrolled decoder graph lattice containing subset paths used instead interest recently performing exact lattice-free sequence training using simpliﬁed decoder graph given prevalence word error rate evaluation metric natural loss training edit distance reference hypothesized word sequences. refer training using levenshtein distance loss edit-based training using number word errors word-level embr training. number word errors result dynamic programming computation decompose additively edges path meaning expectation semiring approach cannot applied without modiﬁcation. discussed fact possible exactly compute expected number word errors lattice expanding lattice using error marking approach produces larger lattices practice limits maximum utterance length compatible on-the-ﬂy lattice generation section look simple sampling-based approach computing loss value gradient. approach essentially agnostic form loss used allowing perform embr training simply efﬁciently. consider expected loss function rather since provides noted special epsilon label. path initial state ﬁnal state consists sequence edges weight path deﬁned product weights edges. probability path markovian distribution. output label sequence associated path sequence non-epsilon output labels along path. probability sequence output labels deﬁned probabilities paths consistent acoustic model speciﬁes mapping acoustic acoustic feature vector sequence logit sequence given model parameters dimension logit vector typically associated cluster context-dependent phonemes subphonemes paper acoustic model stacked long short-term memory network given logit sequence deﬁne weighted composition score decoder graph decoder graph composition incorporates weighted information context-dependence pronunciation word sequence plausibility score input output alphabets decoder graph input alphabet decoder graph output alphabet vocabulary. refer unrolled decoder graph since composition effectively unrolls decoder graph time. score simple sausage structure states edge state state input output label weight frame cluster index conditional probabilistic model word given acoustic feature vector sequence obtained applying procedure described helpful know gradient weight probability respect acoustic logits. score constructed sequence non-epsilon input labels encountered along path consists cluster index frame overall ztqt consists figure example python implementation sampled training. sample path returns sampled path lattice collapse path takes path word sequence gammas takes path matrix occurs path loss computes levenshtein distance case embr training. compared performance smbr embr training -channel query recognition task using ctc-style model clustered context-dependent phonemes includes blank symbol. acoustic feature vector sequence -dimensional frame step consisted stack three frames ﬁlterbanks channel. stacked unidirectional lstm acoustic model layers cells used followed linear layer output dimension logits post-processed applying normalization softmaxthen-log adding logit blank scaling logits remaining clusters mimic often done decoding models; postprocessing seem critical good performance. initial system trained random initialization using million steps asynchronous stochastic gradient descent using context-dependent phonemes reference label sequence. smbr embr systems trained million steps best systems selected small held-out obtained total roughly million million steps respectively. training step computed gradient whole utterance. lattices training generated on-the-ﬂy. alignments smbr training computed on-the-ﬂy. decoder graph used training constructed weak bigram language model estimated training data ctc-style context-dependent transducer blank symbol used embr training used samples step. used embr training computed language-speciﬁc capitalization punctuation normalization. learning rate embr training gradients required implement sampled modular graph-of-operations framework tensorflow samples drawn efﬁciently using backward ﬁltering–forward sampling first conventional backward algorithm used compute weights partial paths state ﬁnal state. reweighted using potential function i.e. replacing weight edge goes state state weβj results equivalent locally normalized stochastic i.e. edge weights leaving state samples reweighted drawn using simple ancestral sampling i.e. sample edge leaving initial state edge leaving state sampled edge repeat reach ﬁnal state. implementation values computed draw multiple samples reweighting performed on-the-ﬂy sampling. refer using gradient-based training sampled training. intuitive interpretation samples worse-than-average loss weight corresponding path reduced samples better-than-average loss weight corresponding path increased. subtraction makes estimated gradient invariant overall additive shift loss seen performing variance reduction often regarded extremely important sampling-based policy optimization reinforcement learning times larger smbr training number word errors typically smaller number frame errors typical gradient norm values smbr embr training reﬂected this. veriﬁed using times larger learning rate smbr detrimental. training data voice search–speciﬁc subset corpus around million anonymized utterances artiﬁcial room reverbation noise added. evaluated three corpora anonymized -channel google home utterances. training presented figure somewhat contrary common wisdom smbr training converges quickly observe continuing improve around million steps. billion total steps smbr training gets worse stays roughly constant training meaning suffering overﬁtting. embr training takes long time achieve full gains; indeed clear converged even billion total steps. embr performance appears good better smbr number steps. embr step samples set-up took around time smbr step. embr little overhead sampling paths given beta probabilities cheap overall computation time smbr embr typically dominated stacked lstm computations lattice generation. consisted stack four frames ﬁlterbanks. stacked lstm layers cells each. initial cross-entropy system trained million steps. smbr embr systems trained futher million steps models best held-out around million million total steps respectively. systems trained corpus around million anonymized voice search dictation utterances artiﬁcial reverb noise added evaluated four voice search test sets including noise added. training presented figure results presented table experiments used learning rate smbr embr training since fully aware mismatch dynamic range; using smaller learning rate improve smbr performance. observed consistent gains word-level embr training task. seen sampled word-level embr training provides simple effective optimize expected word error rate training improves empirical performance speech recognition tasks disparate architectures. many thanks erik mcdermott patient helpful feedback successive drafts manuscript has¸im many fruitful enjoyable discussions implementing smbr embr training tensorflow. dahl deng acero context-dependent pre-trained deep neural networks large-vocabulary speech recognition ieee transactions audio speech language processing vol. abadi barham chen chen davis dean devin ghemawat irving isard tensorflow system large-scale machine learning proc. osdi h.-a. loeliger molkaraie estimating partition function ﬁelds capacity constrained noiseless channels using tree-based gibbs sampling proc. information theory workshop macherey haferkamp schl¨uter investigations error minimizing training criteria discriminative training automatic speech recognition proc. interspeech", "year": 2017}