{"title": "Learning Transformations for Clustering and Classification", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "A low-rank transformation learning framework for subspace clustering and classification is here proposed. Many high-dimensional data, such as face images and motion sequences, approximately lie in a union of low-dimensional subspaces. The corresponding subspace clustering problem has been extensively studied in the literature to partition such high-dimensional data into clusters corresponding to their underlying low-dimensional subspaces. However, low-dimensional intrinsic structures are often violated for real-world observations, as they can be corrupted by errors or deviate from ideal models. We propose to address this by learning a linear transformation on subspaces using matrix rank, via its convex surrogate nuclear norm, as the optimization criteria. The learned linear transformation restores a low-rank structure for data from the same subspace, and, at the same time, forces a a maximally separated structure for data from different subspaces. In this way, we reduce variations within subspaces, and increase separation between subspaces for a more robust subspace clustering. This proposed learned robust subspace clustering framework significantly enhances the performance of existing subspace clustering methods. Basic theoretical results here presented help to further support the underlying framework. To exploit the low-rank structures of the transformed subspaces, we further introduce a fast subspace clustering technique, which efficiently combines robust PCA with sparse modeling. When class labels are present at the training stage, we show this low-rank transformation framework also significantly enhances classification performance. Extensive experiments using public datasets are presented, showing that the proposed approach significantly outperforms state-of-the-art methods for subspace clustering and classification.", "text": "guillermo sapiro department electrical computer engineering department computer science department biomedical engineering duke university durham low-rank transformation learning framework subspace clustering classiﬁcation proposed. many high-dimensional data face images motion sequences approximately union low-dimensional subspaces. corresponding subspace clustering problem extensively studied literature partition highdimensional data clusters corresponding underlying low-dimensional subspaces. however low-dimensional intrinsic structures often violated real-world observations corrupted errors deviate ideal models. propose address learning linear transformation subspaces using nuclear norm modeling optimization criteria. learned linear transformation restores low-rank structure data subspace time forces maximally separated structure data diﬀerent subspaces. reduce variations within subspaces increase separation subspaces robust subspace clustering. proposed learned robust subspace clustering framework signiﬁcantly enhances performance existing subspace clustering methods. basic theoretical results presented help support underlying framework. exploit low-rank structures transformed subspaces introduce fast subspace clustering technique eﬃciently combines robust sparse modeling. class labels present training stage show low-rank transformation framework also signiﬁcantly enhances classiﬁcation performance. extensive experiments using public datasets presented showing proposed approach signiﬁcantly outperforms state-of-the-art methods subspace clustering classiﬁcation. learned cost transform also applicable classiﬁcation frameworks. handwritten images digit trajectories moving object well-approximated lowdimensional subspace high-dimensional ambient space. thus multiple class data often union low-dimensional subspaces. ubiquitous subspace clustering problem partition high-dimensional data clusters corresponding underlying subspaces. standard clustering methods k-means general applicable subspace clustering. various methods recently suggested subspace clustering sparse subspace clustering soltanolkotabi candes soltanolkotabi wang local subspace aﬃnity local best-ﬁt flats generalized principal component analysis agglomerative lossy compression locally linear manifold clustering spectral curvature clustering recent survey subspace clustering found vidal low-dimensional intrinsic structures enable subspace clustering often violated real-world data. example assumption lambertian reﬂectance basri jacobs show face images subject obtained wide variety lighting conditions accurately approximated -dimensional linear subspace. however real-world face images often captured pose variations; addition faces perfectly lambertian exhibit cast shadows specularities therefore critical subspace clustering handle corrupted underlying structures realistic data such deviations ideal subspaces. data low-dimensional subspace arranged columns single matrix matrix approximately low-rank. thus promising handle corrupted data subspace clustering restore low-rank structure. recent eﬀorts invested seeking transformations transformed data decomposed low-rank matrix component sparse error shen zhang peng zhang proposed image alignment extension multiple-classes applications cryo-tomograhy) shen discussed context salient object detection. methods build recent theoretical computational advances rank minimization. paper propose improve subspace clustering classiﬁcation learning linear transformation subspaces using matrix rank nuclear norm convex surrogate optimization criteria. learned linear transformation recovers low-rank structure data subspace time forces maximally separated structure data diﬀerent subspaces reduce variations within subspaces increase separations subspaces accurate subspace clustering classiﬁcation. example shown fig. faces detected aligned e.g. using ramanan approach learns linear transformations face images restore subject low-dimensional structure. comparing last ﬁrst fig. easily notice faces subject across diﬀerent proposed approach considered learning data features features learned order reduce within-class rank increase class separation encourage robust subspace clustering. such framework criteria introduced incorporated data classiﬁcation clustering problems. section formulate analyze low-rank transformation learning problem. sections discuss low-rank transformation subspace clustering classiﬁcation respectively. experimental evaluations given section public datasets commonly used subspace clustering evaluation. finally section concludes paper. {sc}c n-dimensional subspaces pose models deﬁned ramanan enable optional crop-andﬂip step retain informative side face third row. proposed approach learns linear transformations face images restore subject low-dimensional structure shown last row. comparing last ﬁrst easily notice faces subject across diﬀerent poses visually similar transformed space enabling better face clustering recognition across pose rd×d global linear transformation data points ||·|| denotes matrix induced -norm rank encourages consistent representation transformed data subspace; minimizing second discrimination term −rank encourages diverse repreexplain pedagogical formulation using rank however optimal simultaneously reduce variation within class subspaces introduce separations diﬀerent class subspaces motivating nuclear norm optimization reasons modeling ones well. matrices dimensions objective function reaches minimum matrices independent applying learned transformation however independence infer maximal separation important goal robust clustering classiﬁcation. example lines intersecting origin independent regardless angle between maximally separated angle becomes intuition mind proceed describe proposed formulation based nuclear norm. figure learned transformation using nuclear norm criterion. three subspaces denoted denote angle between subspaces using transform respectively data points associated random noises denote root mean square deviation points true subspace observe learned transformation maximizes distance every pair subspaces towards reduces deviation points true subspace noise present note individual subspaces nuclear norm signiﬁcantly reduced. note that rank values rank rank rank) diﬀerent nuclear norm values manifesting improved between-subspaces separation. adopting diﬀerent normalization interesting subject future research. throughout paper keep particular form normalization already proven lead excellent results. important note simply relaxation replacement rank nuclear norm critical optimization considerations reducing variation within class subspaces show next learned transformation using objective function also maximizes separation diﬀerent class subspaces leading improved clustering classiﬁcation performance. based theorem proposed objective function reaches minimum column spaces every pair matrices orthogonal applying learned transformation equivalently reaches minimum separation every pair subspaces maximized transformation i.e. smallest principal angle subspaces equals note improved separation obtained rank used second term thereby justifying nuclear norm instead. then intuitively theoretically justiﬁed selection criteria learning transform illustrate properties learned transformation using synthetic examples fig. adopt projected subgradient method described appendix search transformation matrix minimizes shown fig. learned transformation maximizes separation every pair subspaces towards reduces deviation data points true subspace noise present. note that comparing fig. fig.d learned transformation using maximizes angle subindependent subspaces transformation renders pairwise orthogonal obtained closed-form follows take basis column space subspace form matrix obtain orthogonalizing transformation u)−u. elaborate properties learned intersecting planes shown fig. though subspaces neither independent disjoint closed-form orthogonalizing transformation still signiﬁcantly increases angle planes towards fig. note also closed-form orthogonalizing transformation size dimension subspace plot ﬁrst dimensions visualization. comparing orthogonalizing transformation leaned transformation fig. introduces similar subspace separation enables signiﬁcantly reduced within subspace variations indicated decreased nuclear norm values experiments diﬀerent samples subspace shown second fig. formulation maximizes separations diﬀerent classes subspaces also simultaneously reduces variations within class subspaces. learned transformation shares similar methodology i.e. minimizing intra-class variation maximizing inter-class separation. classes shown fig. class consisting lines. learned transformation fig. shows smaller intra-class variation fig. merging lines class simultaneously maximizes angle classes towards note usually reduce data dimension number intersecting planes cannot improved shown fig. however learned transformation fig. prepares data separable using subspace clustering. shown sapiro property demonstrated makes learned transformation better learner binary classiﬁcation tree. figure comparisons closed-form orthogonalizing transformation. intersecting planes shown plane contains points. closed-form orthogonalizing transformation signiﬁcantly increase angle planes towards leaned transformation introduces similar subspace separation simultaneously enables signiﬁcantly reduced within subspace variation indicated smaller nuclear norm values experiments points subspace shown second row. closed-form orthogonalizing approach valid independent subspaces fails producing framework limited that even additional theoretical foundations come. learned transformation make immediate observations first angles signiﬁcantly increased within figure comparisons linear discriminant analysis classes shown class consisting lines. notice learned transformation shows smaller intra-class variation merging lines class simultaneously maximizes angle classes towards shows example non-linearly separable classes i.e. intersecting planes cannot improved however learned transformation prepares data separable using subspace clustering. valid range second +θbc +θac though point clean interpretation angles balanced pair-wise orthogonality possible strongly believe theories behind persistent observations currently exploring this. note based rank reaches minimum subspaces independent necessarily maximally distant. propositions show property nuclear norm theorem holds induced -norm frobenius norm. however replace rank function induced -norm norm frobenius norm objective function minimized trivial solution dimension reduction along transformation. connects proposed framework literature compressed sensing though goal learn sensing matrix subspace classiﬁcation reconstruction carson nuclear-norm minimization provides metric compressed sensing design paradigm. results reduced dimensionality presented section move classiﬁcation learned transform training labeled data clustering training data available. particular address subspace clustering problem meaning partition data clusters corresponding underlying subspaces. ﬁrst present general procedure enhance performance existing subspace clustering methods literature. propose speciﬁc fast subspace clustering technique fully exploit low-rank structure transformed subspaces. clustering tasks data labeling course known beforehand practice. proposed algorithm algorithm iterates stages ﬁrst assignment stage obtain clusters using subspace clustering methods e.g. particular paper often improved technique introduced section second update stage based current clustering result compute optimal subspace transformation minimizes algorithm repeated clustering assignments stop changing. lrsc algorithm general procedure enhance performance subspace clustering methods don’t enforce overall objective function present form versatility purpose. study convergence adopt subspace clustering method lrsc assignment step optimizing lrsc update criterion given cluster assignment transformation current lrsc iteration take point current cluster place cluster tion onto tyc. lrsc iterative algorithm optimize alternative minimization formally studying convergence subject future research experimental validation presented already demonstrates excellent performance lrsc possible applications proposed learned transform. experiments observe signiﬁcant clustering error reduction ﬁrst lrsc iterations proposed lrsc iterations enable signiﬁcantly cleaner subspaces subspace clustering benchmark data literature. intuition behinds observed empirical convergence update step lrsc iteration decreases second term small value close discussed section time updated transformation tends reduce intra-subspace variation reduces ﬁrst cluster deviation term even assignments derived various subspace clustering methods. though algorithm adopt subspace clustering methods fully exploit lowrank structure learned transformed subspaces propose following speciﬁc technique clustering step lrsc framework called robust sparse subspace clustering predeﬁned sparsity value explained elhamifar vidal data point linear aﬃne subspace dimension written linear aﬃne combination points subspace. thus represent point linear aﬃne combination points sparse linear aﬃne combination obtained choosing nonzero coeﬃcients. optimization process computationally demanding simplify using local linear embedding wang transformed point represented using nearest neighbors denoted solves system linear equations suggested roweis saul correlation matrix nearly singular conditioned adding small multiple identity matrix. experiments observe simpliﬁcation step dramatically reduces running time without sacriﬁcing accuracy. given sparse representation transformed data point denote sparse representation matrix noted written -sized vector non-zero values pairwise aﬃnity matrix deﬁned |xt| based experimental results presented section proposed r-ssc outperforms state-of-the-art subspace clustering techniques accuracy running time e.g. times faster original using implementation provided elhamifar vidal performance enhanced r-scc used internal step lrsc algorithm note learned transform encourages low-rank sub-space outliers might still exists. moreover iterations algorithm intermediate learned desired one. justiﬁes incorporation low-rank decomposition. section learning global transformation classes discussed incorporated clustering framework section availability data labels training enables consider instead learning individual class-based linear transformation. problem class-based linear transformation learning formulated global transformation matrix learned perform classiﬁcation transformed space simply considering transformed data features. example nearest neighbor classiﬁer used testing sample uses feature searches nearest neighbors among learned perform recognition similar way. however apply learned transforms testing data point pick best using criterion minimal reconstruction error sparse decomposition section ﬁrst presents experimental evaluations subspace clustering using three public datasets mnist handwritten digit dataset extended yaleb face dataset hopkins database motion segmentation. mnist dataset consists -bit grayscale handwritten digit images available http//www.vision.jhu.edu/data/hopkins contains video sequences along extracted feature trajectories videos motions videos three motions. subspace clustering methods compared based studies elhamifar vidal vidal zhang three methods exhibit state-of-the-art subspace clustering performance. adopt implementations provided elhamifar vidal http//www.vision.jhu.edu/code/ implementation provided zhang http//www.ima.umn. edu/~zhang/lbf/. adopt similar setups described zhang experiments subspace clustering. section presents experimental evaluations classiﬁcation using public face datasets dataset extended yaleb dataset. dataset consists subjects imaged simultaneously diﬀerent poses illustration purposes conduct ﬁrst experiments subset mnist dataset. adopt similar setup described zhang using sets digits randomly choose images digit. sparsity value r-ssc perform iterations subgradient updates learning transformation subspaces. subgradient update step unless otherwise stated perform dimension reduction random projections preprocess data thereby saving computations literature e.g. elhamifar vidal vidal zhang projection dimension usually performed enhance clustering performance. however often obvious determine correct projection dimension real data many subspace clustering methods show sensitive choice projection dimension. dimension reduction step needed framework proposed. fig. shows misclassiﬁcation rate running time clustering subspaces digits. misclassiﬁcation rate ratio misclassiﬁed points total number points. visualization purposes data plotted dimension reduced using laplacian eigenmaps belkin niyogi diﬀerent clusters represented diﬀerent colors ground truth plotted using true cluster labels. proposed rssc outperforms state-of-the-art methods terms clustering accuracy running time. clustering error r-ssc reduced using proposed lrsc framework algorithm learned low-rank subspace transformation. clustering figure misclassiﬁcation rate running time clustering digits. methods compared elhamifar vidal pollefeys zhang visualization data plotted dimension reduced using laplacian eigenmaps belkin niyogi diﬀerent clusters represented diﬀerent colors ground truth plotted true cluster labels. iter indicates number lrsc iterations algorithm proposed r-ssc outperforms stateof-the-art methods terms clustering accuracy running time e.g. times faster ssc. clustering performance r-ssc improved using proposed lrsc framework. note data clearly clustered clean subspaces transformed domain figure misclassiﬁcation rate clustering digits. methods compared pollefeys zhang adopted proposed lrsc framework denoted r-lbf. convergence r-lbf signiﬁcantly outperforms state-of-the-art methods. fig. shows misclassiﬁcation rate clustering subspaces three digits. adopt lrsc framework denoted robust illustrate performance existing subspace clustering methods enhanced using proposed lrsc algorithm. convergence r-lbf uses proposed learned subspace transformation signiﬁcantly outperforms state-of-the-art methods. table shows misclassiﬁcation rate clustering diﬀerent number digits denotes subset digits digit randomly pick samples digit compare performance fewer number data points class present. cases proposed lrsc method signiﬁcantly outperforms state-of-the-art methods. images digit randomly partition mini-batches. ﬁrst perform iteration lrsc algorithm selected data various values. shown fig. always observe empirical convergence subspace transformation learning projected subgradient method presented appendix converges table misclassiﬁcation rate clustering diﬀerent numbers digits mnist dataset denotes subset digits digit randomly pick samples digit. cases proposed lrsc method signiﬁcantly outperforms state-of-the-art methods. figure convergence objective function using online batch learning subspace transformation. always observe empirical convergence online batch learning. vary value norm constraint discussions convergence found appendix converge objective function value takes sec. online learning sec. batch learning. starting ﬁrst mini-batch perform iteration lrsc minibatch time subspace transformation learned previous mini-batch warm restart. adopt iterations subgradient descent updates. shown fig. observe similar empirical convergence online transformation learning. converge objective function value takes sec. online learning sec. batch learning. extended yaleb dataset subjects imaged lighting conditions shown fig. assumption lambertian reﬂectance face images subject diﬀerent lighting conditions accurately approximated -dimensional linear subspace conduct face figure misclassiﬁcation rate running time clustering subjects using diﬀerent subspace clustering methods. proposed r-ssc outperforms state-of-the-art methods accuracy running time. improved using learned transform lrsc reduces error fig. fig. shows error rate running time clustering subspaces subjects using diﬀerent subspace clustering methods. proposed r-ssc techniques outperforms state-of-the-art methods accuracy running time. shown fig. using proposed lrsc algorithm misclassiﬁcation errors rssc reduced signiﬁcantly example subjects. fig. shows convergence updating step ﬁrst lrsc iterations. dramatic performance improvement explained fig. observe expected theory presented before learned subspace transformation increases distance subspaces time reduces nuclear norms subspaces. results clustering subspaces subjects shown fig. table shows misclassiﬁcation rate clustering subspaces diﬀerent number subjects denotes ﬁrst subjects extended yaleb dataset. cases proposed lrsc method signiﬁcantly outperforms state-of-the-art methods. note without low-rank decomposition step obtain misclassiﬁcation rate figure misclassiﬁcation rate clustering subjects using proposed lrsc framework. adopt proposed r-ssc technique clustering step. proposed lrsc framework clustering error r-ssc reduced signiﬁcantly e.g. -subject case. note classes clustered clean subspaces transformed domain. figure smallest mean principal angles pairs subject subspaces nuclear norms subject subspaces transformation. note entry denotes smallest principal angle entry denotes average cosine principal angles. observe learned subspace transformation increases angles subspaces also reduces nuclear norms subspaces. overall average smallest principal angles subspaces increased average subspace nuclear norm decreased fig. fig. using synthetic examples previously compared learned transformation closed-form orthogonalizing transformation lda. table compare three transformations using real data. perform supervised transformation learning subjects extended yaleb dataset using three diﬀerent transformation learning algorithms perform subspace clustering transformed data. proposed transformation learning signiﬁcantly outperforms methods. figure misclassiﬁcation rate running time clustering subjects. proposed r-ssc outperforms state-of-the-art methods accuracy running time. proposed lrsc framework clustering error r-ssc reduced signiﬁcantly. note classes clustered clean subspaces transformed domain hopkins dataset consists three types videos checker traﬃc articulated videos motions videos three motions. main task segment video sequence multiple rigidly moving objects multiple spatiotemporal regions correspond diﬀerent motions scene. motion dataset contains much cleaner subspace data digits faces data evaluated above. enable fair comparison project data lower dimensional subspace using explained vidal zhang results comparing methods taken vidal shown vidal zhang method signiﬁcantly outperforms previous state-of-the-art methods dataset. table method shows comparable results motions table misclassiﬁcation rate clustering diﬀerent number subjects extended yaleb face dataset denotes ﬁrst subjects dataset. cases proposed lrsc method signiﬁcantly outperforms state-of-the-art methods. table misclassiﬁcation rate clustering subjects extended yaleb dataset using supervised transformation learning. proposed transformation learning outperforms closed-form orthogonalizing transformation clustering transformed data. extended yaleb dataset adopt similar setup described jiang zhang split dataset halves randomly selecting lighting conditions training half testing. learn global low-rank transformation matrix training data. report recognition accuracies table make following observations. first recognition accuracy increased simply applying learned transformation matrix original face images. second best accuracy obtained ﬁrst recovering low-rank subspace subject e.g. third fig. then transformed testing face e.g. second fig. sparsely decomposed low-rank subspace subject classiﬁed subject minimal reconstruction error. sparsity value used omp. shown fig. low-rank representation subject shows reduced variations caused illumination. third global transformation performs better class-based transformations fact illumination dataset varies globally coordinated across subjects. last least method outperforms state-of-the-art sparse representation based face recognition methods. table misclassiﬁcation rate motions three motions segmentation hopkins dataset. shown vidal zhang method signiﬁcantly outperforms previous state-of-the-art methods dataset. proposed lrsc shows comparable results motions outperforms three motions. note method orders magnitude faster ssc. table recognition accuracies illumination variations extended yaleb dataset. recognition accuracy increased simply applying learned low-rank transformation matrix original face images. example learn class-based low-rank transformation matrix subject training data. noted goal learn transformation matrix help classiﬁcation necessarily correspond real geometric transform. table shows face recognition accuracies pose variations dataset make following observations. first recognition accuracy dramatically increased applying learned transformations. second best accuracy obtained recovering low-rank subspace subject e.g. third fig. fig. then transformed testing face e.g. fig. fig. sparsely decomposed low-rank subspace subject classiﬁed subject minimal reconstruction error section third class-based transformation performs better global transformation case. choice settings data dependent. last least method outperforms best knowledge reported best recognition performance experimental setup. however unsupervised method proposed method requires training still illustrating simple learned transform signiﬁcantly improve performance. figure face recognition accuracy combined pose illumination variations dataset. proposed methods denoted g-lrt color clrt color blue. proposed methods signiﬁcantly outperform comparing methods especially extreme poses enable comparison adopt setup face recognition combined pose illumination variations dataset. subjects poses illumination conditions training; classify subjects poses illumination conditions. three face recognition methods adopted comparisons eigenfaces turk pentland wright dadl dadl state-of-the-art sparse representation methods face recognition dadl adapts sparse dictionaries actual visual domains. shown fig. proposed methods global class-based signiﬁcantly outperform comparing methods especially extreme poses testing examples using global transformation shown fig. notice transformed faces subject exhibit reduced variations caused pose illumination. performing subspace transformation experiments notice peak clustering accuracy usually obtained smaller dimension ambient space. example fig. exhaustive search optimal observe misclassiﬁcation rate reduced subjects before provides framework sense clustering classiﬁcation connecting work presented extensive literature compressed sensing particular sensing design e.g. carson plan study detail optimal size learned transformation matrix subspace clustering classiﬁcation including potential connection number subspaces data investigate connections compressive sensing. introduced subspace low-rank transformation approach subspace clustering classiﬁcation. using matrix rank optimization criteria nuclear norm convex surrogate learn subspace transformation reduces variations within subspaces increases separations subspaces. demonstrated proposed approach signiﬁcantly outperforms state-of-the-art methods subspace clustering classiﬁcation provided theoretical support experimental results. numerous venues research opened framework introduced. theoretical level extending analysis noisy case needed. furthermore understanding virtues global class-dependent transform important interesting study framework compressed dimensionality form. beyond this considering proposed approach feature extraction technique combination successful clustering classiﬁcation techniques subject current research. matrices obtained concatenating matrices achieve minimum computing nuclear norm necessarily ones achieve corresponding minimum nuclear norm computation simple projected subgradient method search transformation matrix minimizes describing note problem nondiﬀerentiable non-convex deserves proper study eﬃcient optimization keeping mind development advanced optimization techniques improve performance proposed framework. selected simple subgradient approach since goal paper present framework already simple optimization leads fast convergence excellent performance detailed section signiﬁcant improvements performance compared state-of-the-art. objective function d.c. program yuille rangarajan sriperumbudur lanckriet provide simple convergence analysis projected subgradient approach proposed above. ﬁrst term convex term added second term linear term using subgradient concave term evaluated current iteration. solve sub-objective function using subgradient method i.e. using constant step size iteratively take step negative direction subgradient subgradient evaluated though subgradient step guarantee decrease cost function recht using constant step size subgradient method guaranteed converge within range optimal value convex problem simpliﬁed version d.c. procedure performing iteration subgradient method solving subobjective function discussion simpliﬁcation later). objective bounded decreases iteration d.c. procedure thus convergence local minimum guaranteed. eﬃciency considerations solving convex sub-objective function perform iteration subgradient method obtain simpliﬁed method still observe empirical convergence experiments fig. fig.n. solution shown above minimization without norm constraint always converges local minimum thus initialization becomes critical dropping norm constraint. initializing identity matrix observe trivial solution convergence experiments normalization case fig.. gradient-based algorithm using various alternatives e.g. lagrange multipliers coeﬃcient normalization gradients tangent space. implement coeﬃcient normalization method i.e. obtaining normalize ||t|| words normalize length without changing direction. discussed douglas problem minimizing cost function subject norm constraint forms basis many important tasks gradient-based algorithms often used along norm constraint. though expected norm constraint change convergence behavior gradient algorithm fuhrmann fig. best knowledge open problem analyze norm constraint choice aﬀect convergence behavior gradient/subgradient method.", "year": 2013}