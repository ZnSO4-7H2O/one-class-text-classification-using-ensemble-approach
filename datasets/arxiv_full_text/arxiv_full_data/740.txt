{"title": "Evolutionary Training of Sparse Artificial Neural Networks: A Network  Science Perspective", "tag": ["cs.NE", "cs.AI", "cs.LG"], "abstract": "Through the success of deep learning, Artificial Neural Networks (ANNs) are among the most used artificial intelligence methods nowadays. ANNs have led to major breakthroughs in various domains, such as particle physics, reinforcement learning, speech recognition, computer vision, and so on. Taking inspiration from the network properties of biological neural networks (e.g. sparsity, scale-freeness), we argue that (contrary to general practice) Artificial Neural Networks (ANN), too, should not have fully-connected layers. We show how ANNs perform perfectly well with sparsely-connected layers. Following a Darwinian evolutionary approach, we propose a novel algorithm which evolves an initial random sparse topology (i.e. an Erd\\H{o}s-R\\'enyi random graph) of two consecutive layers of neurons into a scale-free topology, during the ANN training process. The resulting sparse layers can safely replace the corresponding fully-connected layers. Our method allows to quadratically reduce the number of parameters in the fully conencted layers of ANNs, yielding quadratically faster computational times in both phases (i.e. training and inference), at no decrease in accuracy. We demonstrate our claims on two popular ANN types (restricted Boltzmann machine and multi-layer perceptron), on two types of tasks (supervised and unsupervised learning), and on 14 benchmark datasets. We anticipate that our approach will enable ANNs having billions of neurons and evolved topologies to be capable of handling complex real-world tasks that are intractable using state-of-the-art methods.", "text": "abstract success deep learning artiﬁcial neural networks among used artiﬁcial intelligence methods nowadays. anns major breakthroughs various domains particle physics reinforcement learning speech recognition computer vision taking inspiration network properties biological neural networks argue artiﬁcial neural networks fully-connected layers. show anns perform perfectly well sparsely-connected layers. following darwinian evolutionary approach propose novel algorithm evolves initial random sparse topology consecutive layers neurons scale-free topology training process. resulting sparse layers safely replace corresponding fully-connected layers. method allows quadratically reduce number parameters fully conencted layers anns yielding quadratically faster computational times phases decrease accuracy. demonstrate claims popular types types tasks benchmark datasets. anticipate approach enable anns billions neurons evolved topologies capable handling complex real-world tasks intractable using state-of-the-art methods. success deep learning artiﬁcial neural networks among used artiﬁcial intelligence methods nowadays. anns major breakthroughs various domains particle physics reinforcement learning speech recognition typically anns layers fully-connected neurons contain network parameters leading quadratic number connections respect number neurons. turn network size severely limited obvious computational limitations. contrast anns biological neural networks demonstrated sparse topology also hold important properties instrumental learning efﬁciency. extensively studied include scale-freeness small-worldness nevertheless anns evolved mimic topological features practice lead extremely large models. previous studies demonstrated that following training phase models weights histograms peak around zero moreover previous work hinted similar fact. machine learning state-of-the-art sparse topological connectivity pursued aftermath training phase bears beneﬁts inference phase. claim topological sparsity must pursued since design phase leads substantial reduction connections turn memory computational efﬁciency. time able make standard training algorithms e.g. stochastic gradient descent structured multi-layer architecture anns preserved. otherwise would able train large anns complete random sparse topology difﬁculty ﬁnding suitable optimization algorithms. recent paper introduced complex boltzmann machines sparse variant restricted boltzmann machines conceived sparse scale-free topology xbms outperform fully-connected rbms counterparts much faster training inference phases. based ﬁxed sparsity pattern xbms fail properly model data distribution. overcome limitation paper introduce sparse evolutionary training procedure takes consideration data distributions creates sparse bipartite layers suitable replace fully-connected bipartite layers type anns. follows natural simplicity darwinian evolutionary approach explored successfully previous work evolutionary function approximation also explored network connectivity mcdonnell waagen layers architecture deep neural networks bipartite layers start random sparse topology evolving random process training phase towards scale-free topology. remarkably process incorporate constraints force scale-free topology. evolutionary algorithm arbitrary follows phenomenon takes place real-world complex networks starting erd˝os-rényi random graph topology throughout millenia natural evolution networks structured connectivity i.e. scale-free small-world topologies. remainder paper organized follows. section presents background knowledge mainly beneﬁt less specialist reader. section introduces proposed method set. section describes experiments performed discusses results. finally section concludes chapter proposes future research directions. artiﬁcial neural networks mathematical models inspired biological neural networks used three machine learning paradigms unsupervised learning reinforcement learning make versatile powerful quantiﬁable remarkable success registered recently last generation anns many ﬁelds computer vision gaming like biological counterparts anns composed neurons weighted connections neurons. based purposes architectures many models anns restricted boltzmann machines multi layer perceptron convolutional neural networks recurrent neural networks many models contain fully-connected layers. fully-connected layer neurons means neurons connected neurons belonging adjacent layer architecture. purpose paper section brieﬂy describe models contain fully-connected layers i.e. restricted boltzmann machines multi layer perceptron restricted boltzmann machine two-layer generative stochastic neural network capable learn probability distribution inputs unsupervised manner. topological perspective allows interlayer connections. layers visible layer neurons represent input data; hidden layer neurons represent features automatically extracted model input data. visible neuron connected hidden neurons weighted undirected connection leading fully-connected topology layers. thus information bidirectional rbms visible layer hidden layer hidden layer visible layer respectively. rbms beside successful providing good initialization weights supervised training deep artiﬁcial neural network architectures also successful stand alone models variety tasks density estimation model human choice collaborative ﬁltering information retrieval multi-class classiﬁcation multi layer perceptron classical feed-forward model maps input data corresponding output data. thus used supervised learning. composed input layer neurons represent input fig. illustration procedure. sparse connected layer training epoch fraction weights ones closest zero removed then weighs added randomly amount ones previously removed training epoch performed procedure remove weights repeated. process continues ﬁnite number training epochs usual anns training. data output layer neurons represent output data arbitrary number hidden layers between neurons representing hidden features input data information mlps unidirectional starting input layer towards output layer. thus connections unidirectional exist consecutive layers. consecutive layers mlps fullyconnected. connections neurons belonging layer neurons belonging layers consecutive. demonstrated mlps universal function approximators used model type regression classiﬁcation problems. general working models involves phases training weighted connections neurons optimized using various algorithms used mlps contrastive divergence used rbms) minimize loss function deﬁned purpose; inference optimized model used fulﬁll purpose. complex networks everywhere different forms different ﬁelds formally complex network graph nontrivial topological features humannature-made. well-known deeply studied type topological features complex networks scale-freeness fact wide range real-world complex networks topology. network scale-free topology sparse graph approximately power-law degree distribution fraction total nodes network connections nodes parameter usually stays range detailed algorithm exempliﬁed figure formally deﬁned sparse connected layer ann. layer neurons collected vector neuron connected arbitrary number neurons linear number connections |wk| respect number neurons sparse layers. case fully-connected layers number connections quadratic i.e. nknk−. however random generated topology suited particularities data model tries learn. overcome situation training process training epoch fraction smallest positive weights highest negative weights removed. removed weights ones closest zero thus expect removal notably change model performance next topology evolve data amount random connections equal amount weights removed previously added number connections remains constant training process. training ends keep topology obtained last weight removal step without adding random connections. please note removal important connections corresponds selection phase natural table datasets characteristics. data used paper chosen cover wide range ﬁelds anns potential advance state-of-the-art including biology physics computer vision data mining economics. worth highlighting initial phase conceiving procedure weight-removal weight-addition steps training epoch introduced intuitively. still last phases preparing paper found similarity phenomenon takes place biological brains named synaptic shrinking sleep. phenomenon demonstrated recently papers published science journal february short found sleep weakest synapses brain shrink strongest synapses remain unaltered supporting hypothesis core functions sleeping renormalize overall synaptic strength increased awake keeping analogy happens also anns procedure. evaluate types anns restricted boltzmann machine multi layer perceptron experiment unsupervised supervised learning. total evaluate benchmark datasets detailed table covering wide range ﬁelds anns employed biology physics computer vision data mining economics. also assess combination different training methods i.e. contrastive divergence stochastic gradient descent first analyzed performance bipartite undirected stochastic model i.e. restricted boltzmann machine popular unsupervised learning capability high performance feature extractor density estimator model derived procedure dubbed set-rbm. experiments performing small random search mnist dataset able assess meta-parameters dataset speciﬁc values general enough perform well also different datasets. studies connectivity sparsity still good estimation set-rbm capabilities compared rbmf fully-connected rbms state-of-the-art results xbms performed experiments benchmark datasets coming various domains depicted table using splitting training testing data models trained epochs using contrastive divergence steps learning rate momentum weight decay discussed evaluated generative performance scrutinized models computing log-probabilities test data using annealed importance sampling setting parameters used matlab experiments. implemented set-rbm rbmf ourselves; adapted code provided figure depicts model’s performance datasets using varying numbers hidden neurons; table summarizes results presenting best performer type model dataset. datasets set-rbm outperforms fullyconnected rbms reducing parameters orders magnitude. instance mnist dataset set-rbm reaches nats .-fold improvement fully-connected parameters reduction datasets set-rbm outperforms represents state-of-the-art results datasets sparse variants figure shows striking results stability. fully-connected rbms show instability over-ﬁtting issues procedure stabilizes set-rbms avoids over-ﬁtting. situation observed often high number hidden neurons chosen instance look dataset independently values observe setrbms stable reach around nats almost learning behavior point. contrary dataset fully-connected rbms short initial good learning behavior that epochs analyzed reaching minimum performance nats mention good stability over-ﬁtting avoidance capacity induced procedure also sparsity itself rbmf stable behavior almost cases. ﬁnally veriﬁed initial hypothesis sparse connectivity set-rbm. figure shows connectivity naturally evolves towards scale-free topology. assess fact used null hypothesis statistics assumes relation measured phenomena. null hypothesis degree distribution hidden neurons power-law distribution rejected computed p-value them. reject null hypothesis p-value lower statistically signiﬁcant threshold cases looking p-values beginning learning phase null hypothesis rejected. expected initial degree distribution hidden neurons binomial randomness erd˝os-rényi random graphs used initialize fig. experiments variants using benchmark datasets. model studied considered three cases number contrastive divergence steps three cases number hidden neurons ﬁrst datasets used last three datasets used x-axes show training epochs; left y-axes show average log-probabilities computed test data right y-axes reﬂect fraction given model three models. overall set-rbm outperforms models cases. also interesting set-rbm rbmf much stable present over-ﬁtting problems rbm. fig. set-rbm evolution towards scale-free topology. considered three cases number contrastive divergence steps three cases number hidden neurons ﬁrst datasets used last three datasets used x-axes show training epochs; left y-axes show average logprobabilities computed set-rbms test data right y-axes show p-values computed degree distribution hidden neurons set-rbm power-law distribution. observe models high enough number hidden neurons set-rbm topology always tends become scale-free. table summarization experiments variants. dataset report best average log-probabilities obtained test data model. represents number hidden neurons number steps number weights model. set-rbms topology. subsequently learning phase that many cases p-values decrease considerably statistical signiﬁcant level threshold. situations occur means degree distribution hidden neurons set-rbm starts approximate power-law distribution. expected cases fewer neurons fail evolve scale-free topologies cases neurons always evolve towards scale-free topology summarize cases studied set-rbms topology evolves clearly learning phase erd˝os-rényi topology towards scale-free one. better explore capabilities also assessed performance classiﬁcations tasks based supervised learning. developed variant multi layer perceptron dubbed set-mlp fully-connected layers replaced sparse layers obtained procedure kept parameter previous case set-rbms parameter performed small random search mnist dataset. compared set-mlp standard fully-connected sparse variant ﬁxed erd˝os-rényi topology dubbed mlpf rob. assessment used three benchmark datasets coming computer vision domain particle physics cases used data processing techniques network architecture training method ﬁxed learning rate momentum weight decay dropout rate difference mlpf set-mlp consisted topological connectivity. used python keras library theano back-end experiments. used standard keras implementation implemented set-mlp mlpf standard keras libraries. results depicted figure show set-mlp outperforms mlpf rob. moreover set-mlp always outperforms orders magnitude fewer parameters. looking cifar dataset weights set-mlp leads signiﬁcant gains. time set-mlp comparable results state-of-the-art models carefully ﬁne-tuned. fig. experiments variants using benchmark datasets. plots left side reﬂect models performance terms classiﬁcation accuracy training epochs right y-axes left plots give p-values computed degree distribution hidden neurons set-mlp models power-law distribution showing set-mlp topology becomes scale-free training epochs. plots right side depict number weights three models dataset. striking situation happens cifar dataset set-mlp model outperforms drastically model approximately times fewer parameters. table summarization experiments variants. dataset report best classiﬁcation accuracy obtained model test data. represents number weights model. quantify second best model literature cifar reaches classiﬁcation accuracy million parameters set-mlp reaches better accuracy million parameters. moreover best model literature cifar accuracy million parameters also beneﬁting pre-training phase although pre-trained models studied here mention set-rbm easily used pre-train set-mlp model improve performance. regarding topological features figure that similarly found set-rbm experiments hidden neuron connections set-mlp rapidly evolve towards power-law distribution. considering different datasets scrutiny stress assessed image-intensive non-image sets. image datasets convolutional neural networks typically outperform mlps. these fact matches perfectly procedure. instance used replace cnns fully connected layers sparse evolutionary counterparts. beneﬁt would two-fold reduce total number parameters cnns permit larger models. however cnns viable types high-dimensional data biological data theoretical physics data cases mlps better choice. fact case higgs dataset set-mlp achieves classiﬁcation accuracy parameters. whereas best models literature achieved accuracy three many times many parameters last least experiments performed observed quite stable respect choice meta-parameters choices offered best possible performance even ﬁne-tuned dataset i.e. mnist evaluated performance datasets. still both set-rbm set-mlp speciﬁc model type set-rbm set-mlp good enough outperform state-of-the-art. paper introduced simple procedure replace anns fully-connected bipartite layers sparse layers. validated approach datasets widely used models i.e. rbms mlps. evaluated combination different training methods i.e. contrastive divergence stochastic gradient descent unsupervised supervised learning. showed capable quadratically reduce number parameters bipartite neural networks layers decrease accuracy. cases set-rbms set-mlps outperform fully-connected counterparts. moreover always outperform non-evolutionary counterparts i.e. rbmf mlpf rob. conclude procedure coherent real-world complex networks whereby nodes connections tend evolve scale-free topologies feature important implications anns could envision computational time reduction reducing number training epochs would instance preferential attachment algorithms evolve faster topology bipartite layers towards scale-free one. course possible improvement treated carefully forcing model topology evolve unnaturally faster scale-free topology prone errors instance data distribution perfectly matched. widely adopted reduce fully-connected layers sparse topologies types anns e.g. convolutional neural networks recurrent neural networks deep reinforcement learning networks prove basis much larger anns possibly billion-node scale supercomputers. also lead building small powerfull anns could directly trained low-resource devices without need ﬁrst training supercomputers move trained models low-resource devices currently performed state-of-the-art powerfull capabilities enabled linear relation number neurons amount connections yielded set. anns built much representational power better adaptive capabilities current state-of-the-art anns push artiﬁcial intelligence well beyond current boundaries.", "year": 2017}