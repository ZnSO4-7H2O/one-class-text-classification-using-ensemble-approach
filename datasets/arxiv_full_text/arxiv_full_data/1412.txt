{"title": "Introspective Classification with Convolutional Nets", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "We propose introspective convolutional networks (ICN) that emphasize the importance of having convolutional neural networks empowered with generative capabilities. We employ a reclassification-by-synthesis algorithm to perform training using a formulation stemmed from the Bayes theory. Our ICN tries to iteratively: (1) synthesize pseudo-negative samples; and (2) enhance itself by improving the classification. The single CNN classifier learned is at the same time generative --- being able to directly synthesize new samples within its own discriminative model. We conduct experiments on benchmark datasets including MNIST, CIFAR-10, and SVHN using state-of-the-art CNN architectures, and observe improved classification results.", "text": "propose introspective convolutional networks emphasize importance convolutional neural networks empowered generative capabilities. employ reclassiﬁcation-by-synthesis algorithm perform training using formulation stemmed bayes theory. tries iteratively synthesize pseudo-negative samples; enhance improving classiﬁcation. single classiﬁer learned time generative able directly synthesize samples within discriminative model. conduct experiments benchmark datasets including mnist cifar svhn using state-of-the-art architectures observe improved classiﬁcation results. great success achieved obtaining powerful discriminative classiﬁers supervised training decision trees support vector machines neural networks boosting random forests however recent studies reveal even modern classiﬁers like deep convolutional neural networks still make mistakes look absurd humans common improve classiﬁcation performance using data particular hard examples train classiﬁer. different types approaches proposed past including bootstrapping active learning semi-supervised learning data augmentation however approaches utilize data samples either already present given training additionally created humans separate algorithms. paper focus improving convolutional neural networks endowing synthesis capabilities make internally generative. past attempts made build connections generative models discriminative classiﬁers self supervised boosting algorithm proposed train boosting algorithm sequentially learning weak classiﬁers using given data self-generated negative samples; generative discriminative learning work generalizes concept unsupervised generative modeling accomplished learning sequence discriminative classiﬁers self-generated pseudonegatives. inspired self-generated samples utilized well recent success deep learning propose introspective convolutional network classiﬁer study internal generative aspect beneﬁt cnn’s discriminative classiﬁcation task. recent line work using discriminator help external generator generative adversarial networks different objective here. building single model simultaneously discriminative generative. introspective convolutional networks introduced number properties. introduce introspection convolutional neural networks show signiﬁcance supervised classiﬁcation. reclassiﬁcation-by-synthesis algorithm devised train iteratively augmenting negative samples updating classiﬁer. stochastic gradient descent sampling process adopted perform efﬁcient synthesis icn. propose supervised formulation directly train multi-class classiﬁer. show consistent improvement state-of-the-art classiﬁers benchmark datasets experiments. related work method directly related generative discriminative learning framework also connection self-supervised learning method focused density estimation combining weak classiﬁers. previous algorithms connecting generative modeling discriminative classiﬁcation fall category hybrid models direct combinations two. existing works introspective learning different scope problem tackled here. generative modeling schemes minimax entropy inducing features auto-encoder recent cnn-based generative modeling approaches discriminative classiﬁcation single model generative discriminative. discuss methods related namely generative discriminative learning generative adversarial networks relationship generative discriminative learning largely inspired follows similar pipeline developed however also large improvement summarized below. boosting. builds convolutional neural networks explicitly supervised classiﬁcation unsupervised modeling. focuses supervised classiﬁcation task competitive results benchmark datasets whereas originally applied generative modeling power classiﬁcation task addressed. automatic feature learning manually speciﬁed features. greater representational power end-to-end training whereas relies manually designed features. comparison generative adversarial networks recent efforts adversarial learning also interesting worth comparing with. introspective adversarial. emphasizes introspective synthesizing samples classiﬁer focuses adversarial using distinct discriminator guide generator. supervised classiﬁcation unsupervised modeling. main focus develop classiﬁer introspection improve supervised classiﬁcation task whereas mostly building high-quality generative models unsupervised learning. single model separate models. retains discriminator generator whereas maintains models generator discriminator discriminator trained classify real fake samples. reclassiﬁcation-by-synthesis minimax. engages iterative procedure reclassiﬁcationby-synthesis stemmed bayes theory whereas minimax objective function optimize. training classiﬁer standard cnn. multi-class formulation. gan-family work semi-supervised learning task devised adding additional not-real class standard classes multi-class classiﬁcation; results different setting standard multi-class classiﬁcation additional model parameters. instead aims directly supervised multi-class classiﬁcation task maintaining parameter setting within softmax function without additional model parameters. later developments alongside share similar aspects also achieve goal does. since discriminator meant perform generic two-class/multi-class classiﬁcation task special settings semi-supervised learning created. instead single model generative discriminative thus improvement icn’s generator leads direct means ameliorate discriminator. work like motivated observation adding small perturbations image leads classiﬁcation errors absurd humans; approach however taken augmenting positive samples existing input whereas able synthesize samples scratch. recent work proposed family focuses unsupervised image modeling using cascade cnns. method pipeline shown figure immediate improvement several aspects described previous section. particular gain representation power efﬁcient sampling process backpropagation variational sampling strategy. formulation start discussion introducing basic formulation borrow notation data sample label indicating either negative positive sample study binary classiﬁcation ﬁrst. discriminative classiﬁer computes probability positive negative. generative model instead models captures underlying generation process class binary classiﬁcation positive samples primary interest. bayes rule figure schematic illustration reclassiﬁcation-by-synthesis algorithm training. top-left ﬁgure shows input training samples circles positive samples crosses blue negatives. bottom ﬁgures samples progressively self-generated classiﬁer synthesis steps ﬁgures show decision boundaries progressively updated reclassiﬁcation steps. pseudo-negatives gradually generated help tighten decision boundaries. make interesting important observations eqn. dependent faithfulness classiﬁer report made simultaneously generative discriminative. however requirement informative distribution negatives samples drawn good coverage entire space especially samples close positives allow classiﬁer faithfully learn seems exist dilemma. supervised learning given limited amount training data classiﬁer focused decision boundary separate given samples classiﬁcation unseen data accurate. seen left plot figure motivates implement synthesis part within learning make learned discriminative classiﬁer generate samples pass classiﬁcation different generated samples given positive samples. allows attain single model aspects time generative model positive samples improved classiﬁer classiﬁcation. suppose given training ..n} directly train discriminative classiﬁer e.g. convolutional neural networks learn always approximation various reasons including insufﬁcient training samples generalization error classiﬁer limitations. previous attempts improve classiﬁcation data augmentation mostly done positive samples instead argue importance adding negative samples improve classiﬁcation performance. dilemma ..n} limited given data. clarity represent goal augment negative training generating confusing pseudo-negatives improve classiﬁcation become hard distinguish given positive samples. cross-validation used determine using pseudo-negatives reducing validation error). call samples drawn pseudo-negatives expand ..n} tl}. includes pseudo-negative samples self-generated model time indicates number pseudo-negatives generated round. deﬁne reference distribution gaussian distribution independently). carry learning ...t iteratively obtain updating classiﬁer reports discriminative probability reason using approximation true limited samples drawn time compute name speciﬁc training algorithm introspective convolutional network classiﬁer reclassiﬁcation-by-synthesis described algorithm adopt convolutional neural networks classiﬁer build end-to-end learning framework efﬁcient sampling process reclassiﬁcation-by-synthesis present reclassiﬁcation-by-synthesis algorithm section. schematic illustration shown figure single classiﬁer trained progressively simultaneously discriminator generator. pseudo-negatives gradually generated classiﬁcation boundary gets tightened hence yields improvement classiﬁer’s performance. reclassiﬁcation-by-synthesis method described algorithm algorithm includes steps reclassiﬁcation-step synthesis-step discussed detail below. reclassiﬁcation-step reclassiﬁcation-step viewed training normal classiﬁer training ..n} base classiﬁer. training classiﬁer learned high-dimensional vector parameters. input given training data ..n} initialization obtain reference distribution t=..t update model synthesis-step sample pseudo-negative samples current model augment pseudo-negative reclassiﬁcation-step update classiﬁer back step convergence synthesis-step goal draw fair samples various markov chain monte carlo techniques including gibbs sampling iterated conditional modes adopted often slow. motivated deepdream code neural increasing artistic style work update random sample drawn using backpropagation. note partition function constant dependent sample starting drawn using stochastic gradient ascent backpropagation allows obtain fair samples subject eqn. gaussian noise added eqn. along line stochastic gradient langevin dynamics gaussian distribution step size annealed sampling process. sampling strategies. conducting experiments carry several strategies using stochastic gradient descent algorithm lagenvin including early-stopping sampling process becomes positive short markov chain simulated); stopping large conﬁdence positive iii) sampling ﬁxed large number steps. table shows results different options major differences classiﬁcation performance observed. building connections mcmc active area machine learning combining additional gaussian noise annealed stepsize results simulation langevin dynamics mcmc. recent work shows similarity constant mcmc along analysis using momentum updates. progressively learned discriminative classiﬁer viewed carving feature space essentially becomes equivalent class positives; volume equivalent class satisﬁes condition exponentially large analyzed probability landscape positives makes sampling process particularly biased towards small limited modes. results figure illustrates large variation sampled/synthesized examples. analysis t=∞→ derived inspired convergence proof kl||p− denotes kullbackleibler divergence assumption classiﬁer improves remark. particular attention negative samples live space often much larger positive sample space. negative training samples distribution given negative examples original training set. reclassiﬁcation-by-synthesis algorithm essentially constructs mixture model sequentially generating pseudo-negative samples augment training set. distribution augmented negative sample thus becomes encodes pseudo-negative samples confusing similar positives. adding pseudo-negatives might degrade classiﬁcation result since become similar positives. crossvalidation used decide adding pseudo-negatives helping classiﬁcation task. better pseudo-negative samples increasingly faithful positives interesting topic worth exploring. overall algorithm thus capable enhancing classiﬁcation self-generating confusing samples improve cnn’s robustness. multi-class classiﬁcation one-vs-all. section discussed binary classiﬁcation case. dealing multi-class classiﬁcation problems mnist cifar- need adapt proposed reclassiﬁcation-by-synthesis scheme multi-class case. done directly using one-vs-all strategy training binary classiﬁer using i-th class positive class combine rest classes negative class resulting total binary classiﬁers. training procedure becomes identical binary classiﬁcation case. classes algorithm train individual binary classiﬁers advantage using one-vs-all strategy algorithm made nearly identical binary case price training different neural networks. softmax function. also desirable build single classiﬁer perform multi-class classiﬁcation directly. propose formulation train end-to-end multiclass classiﬁer directly. since directly dealing classes pseudo-negative data slightly different introduce negatives individual class ﬁrst term eqn. encourages softmax loss original training second term eqn. encourages good prediction individual pseudo-negative class generated k-th class hyperparameter balancing terms. note need build single sharing classes. particular introducing additional model parameters perform direct k-class classiﬁcation parameter setting identical standard multi-class classiﬁcation task; compare additional not-real class created classiﬁcation task thus becomes class classiﬁcation. figure synthesized pseudo-negatives mnist dataset classiﬁer. shows training examples. increases classiﬁer gradually synthesize pseudo-negative samples become increasingly faithful training samples. conduct experiments three standard benchmark datasets including mnist cifar- svhn. mnist running example illustrate proposed framework using shallow cnn; show competitive results using state-of-the-art classiﬁer resnet mnist cifar- svhn. experiments reclassiﬁcation step optimizer mini-batch size momentum equal synthesis step adam optimizer momentum term equal results obtained averaging multiple rounds. training test time. general training time around double baseline cnns experiments times mnist dataset times cifar- dataset times svhn dataset. added overhead training mostly determined number generated pseudo-negative samples. test time introduces additional overhead baseline cnns. mnist standard mnist dataset consists training validation test samples. adopt simple network containing convolutional layers ﬁlter size channels respectively. convolutional layers stride pooling layers used. leakyrelu activations used convolutional layer. last convolutional layer ﬂattened sigmoid output reclassiﬁcation step current training data including previously generated pseudo-negatives. initial learning rate decreased factor synthesis step backpropagation sampling process discussed section table compare different sampling strategies. time synthesize ﬁxed number pseudo-negative samples. show synthesized pseudo-negatives mnist dataset figure samples original training dataset. gradually synthesizes pseudo-negatives increasingly faithful original data. pseudo-negative samples continuously used improving classiﬁcation result. table test errors mnist dataset. compare method baseline deep belief network label smoothing moreover two-step experiments combining combining dcgan also reported descriptions text details. method comparison different sampling strategies. perform langevin several options backpropagation sampling strategies. option early-stopping generated samples classiﬁed positive; option stopping high conﬁdence samples positive; option stopping large number steps. table shows results observe signiﬁcant differences choices. ablation study. experiment using random noise synthesized pseudo-negatives ablation study. table observe outperforms baseline icn-noise method one-vs-all softmax cases. effects varying training sizes. better understand effectiveness method carry experiment varying number training examples. training sets different sizes including examples. results reported figure shown particularly effective training relatively small since capability synthesize pseudo-negatives training. comparison gan. focuses unsupervised learning; dcgan show results unsupervised learning semi-supervised classiﬁcation. apply supervised classiﬁcation setting design experiment perform two-step implementation. code obtained pseudo-negative samples individual digit; pseudo-negatives used augmented negative samples train individual one-vs-all classiﬁers combined form multi-class classiﬁer end. compare dcgan follow procedure generator trained dcgan using tensorflow implementation used generate positive samples augmented negative train individual one-vs-all classiﬁers combined create overall multi-class classiﬁer. cnn+gdl achieves test error cnn+dcgan achieves test error mnist dataset whereas reports error using architecture. supervised learning task directly speciﬁed dcgan care needed design optimal setting utilize generated samples dcgan two-step approach made discriminative classiﬁer utilizing given negative samples ﬁrst boosting manually designed features adopted produce competitive results classiﬁer does. nevertheless advantage integrated end-to-end supervised learning single-model framework observed. compare generative model based deep learning approach report classiﬁcation result table achieves test error using softmax function. also compare label smoothing used regularization technique encouraging model less conﬁdent. training example ground-truth label label distribution replaced mixture original ground-truth distribution ﬁxed distribution. achieves test error softmax case. addition also adopt resnet- another baseline model achieves test error mnist dataset. resnet- based achieves improved result robustness external adversarial examples. show improved robustness dealing confusing challenging examples compare baseline classiﬁer adversarial examples generated using fast gradient sign method fast gradient sign method cause maxout network misclassify adversarial examples generated mnist test experiment starting mnist test examples ﬁrst determine correctly classiﬁed baseline order generate adversarial examples them. generated adversarial examples successfully fool baseline however examples fool classiﬁer reduction error adversarial examples. note improvement achieved without using additional training data knowing prior adversarial examples generated speciﬁc fast gradient sign method contrary adversarial examples generated classiﬁer side fool using method still fool baseline classiﬁer. two-way experiment shows improved robustness baseline cnn. cifar- dataset consists color images size images split sets images training images testing. adopt resnet baseline model data augmentation follow standard procedure augmenting dataset zero-padding pixels side; also perform cropping random ﬂipping. results reported table one-vs-all softmax cases outperforms baseline resnet classiﬁers. proposed method orthogonal many existing approaches various improvements network structures order enhance performance. also compare convolutional resnet- label smoothing resnet-+dcgan methods described mnist experiments. shown improve baseline worse method cases except mnist dataset. svhn standard svhn dataset. combine training data extra data form training test data test set. data augmentation applied. result reported table shown achieve competitive results. paper proposed introspective convolutional nets algorithm performs internal introspection. observe performance gains within supervised learning using state-of-the-art architectures standard machine learning benchmarks. acknowledgement work supported iis- iis- northrop grumman contextual robotics grant. thank saining weijian kwonjoon shuai tang sanjoy dasgupta helpful discussions.", "year": 2017}