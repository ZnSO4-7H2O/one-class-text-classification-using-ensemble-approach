{"title": "Learning ELM network weights using linear discriminant analysis", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "We present an alternative to the pseudo-inverse method for determining the hidden to output weight values for Extreme Learning Machines performing classification tasks. The method is based on linear discriminant analysis and provides Bayes optimal single point estimates for the weight values.", "text": "abstract. present alternative pseudo-inverse method determining hidden output weight values extreme learning machines performing classification tasks. method based linear discriminant analysis provides bayes optimal single point estimates weight values. extreme learning machine multi-layer feedforward neural network offers fast training flexible non-linearity function classification tasks. principal benefit network parameters calculated single pass during training process standard form input layer fully connected hidden layer non-linear activation functions. hidden layer fully connected output layer linear activation functions. number hidden units often much greater input layer fan-out hidden units input frequently used. feature elms weights connecting input layer hidden layer random values. simplifies requirements training determining hidden output unit weights achieved single pass. randomly projecting inputs much higher dimensionality possible find hyperplane approximates desired regression function represents linear separable classification problem common calculating hidden output weights moorepenrose pseudo-inverse applied hidden layer outputs using labelled training data. paper present alternative method hidden output weight calculation networks performing classification tasks. advantage method pseudo-inverse method weights best single point estimates bayesian perspective linear output stage. using network architecture random values input hidden layer weights applied input layer index number input features respectively hidden layer index number hidden units respectively output layer index number output units respectively. weights associated input hidden layer hidden output layer linear sums respectively. desired outputs series hidden layer outputs represent desired outputs value corresponding desired class elements. example indicates desired target class restate desired targets using matrix column contains desired targets network instance series. substituting desired outputs optimization problem involves solving following linear equation refer pseudo-inverse method output weight calculation pi-elm. note cases classification problem ill-posed necessary regularize solution using standard methods tikhonov regularization paper develop alternative approach estimating based maximum likelihood estimator assuming linear model. refer lda-elm method equivalent applying linear discriminant analysis hidden layer outputs. presentation based notation ripley quation provides easy combine outputs multiple classifiers. posterior probabilities calculated class classifier form combined posterior probability choose class highest combined posterior probability. schemes unweighted averaging across posterior probability outputs simple schemes. applied lda-elm pi-elm weight calculation method mnist handwritten digit recognition problem authors previously reported good classification results using database database training testing examples. example pixel level grayscale image handwritten digit classes approximately equally distributed training testing sets. algorithms applied directly unprocessed images trained networks providing data batch mode. random values input layer weights uniformly distributed prior probabilities classes lda-elm results show lda-elm outperforms pi-elm every fan-out value. average performance benefit decrease error rate lda-elm larger benefit smaller fan-out values. table shows little extra computational requirement lda-elm method. last experiment performed investigated combining multiple networks using lda-elm averaging posterior probabilities. investigated using ensemble number repeated training testing times ensemble number. averaged results shown fig. results shown fig. demonstrate benefit combining multiple ldaelm networks mnist database. combining networks reduced error rate adding networks reduced error. best error rate achieved networks combined. results mnist database shown fig. suggest performance benefit gained using lda-elm output weight calculation pi-elm method. small extra computation overhead believe viable alternative pseudo-inverse method especially small fan-out values. nother benefit lda-elm ability combine outputs networks combining posterior probabilities estimates individual networks. applied mnist database able reduce error rate result comparable best performance layer neural networks processing data work include comparing weight calculation methods publicly available databases abalone iris data sets presented method weight calculation hidden output weights networks performing classification tasks. method based linear discriminant analysis requires modest amount extra calculation time compared pseudo-inverse method applied mnist database average misclassification rate improvement comparison pseudo-inverse method identically configured initialized networks. b.d. ripley pattern recognition neural networks cambridge univ. press l.i. kuncheva combining pattern classifiers methods algorithms wiley press", "year": 2014}