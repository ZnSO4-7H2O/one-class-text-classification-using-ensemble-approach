{"title": "Convergence Analysis of Optimization Algorithms", "tag": ["stat.ML", "cs.AI", "cs.LG", "math.OC"], "abstract": "The regret bound of an optimization algorithms is one of the basic criteria for evaluating the performance of the given algorithm. By inspecting the differences between the regret bounds of traditional algorithms and adaptive one, we provide a guide for choosing an optimizer with respect to the given data set and the loss function. For analysis, we assume that the loss function is convex and its gradient is Lipschitz continuous.", "text": "regret bound optimization algorithms basic criteria evaluating performance given algorithm. inspecting diﬀerences regret bounds traditional algorithms adaptive provide guide choosing optimizer respect given data loss function. analysis assume loss function convex gradient lipschitz continuous. minimizing argument iterative methods update current parameter vector current step method gradient step size also denotes gradient objective function current parameter time step respect parameter vector generally deﬁne loss function convex instant loss convex regularization function analysis deﬁne regret analysis mainly focuses regret bound algorithms. choice optimizer results diﬀerence performance training procedure neural network. roughly classify optimization algorithm convergence rate. ﬁrst order method stochastic gradient descent momentum method nesterov accelerated gradient method. adaptive method adagrad adadelta adam well known. optimizing tasks training neural adaptive methods usually preferred. recent research shows traditional ﬁrst order algorithms stochastic gradient method momentum method give better convergence results adaptive methods. possible reason structure estimating hessian matrix adaptive algorithms. estimation issue mentioned later section brieﬂy. section show regret bound gradient descent algorithm full batch bounded constant. also show stochastic gradient descent method shares regret bound. guarantee exact decreasing direction. since assume cost function convex constant bound implies error certain step bounded inverse iteration number. theorem convex gradient l-lipschitz continuous sequence {θt} generated update satisﬁes accelerate convergence gradient descent method momentum method past steps update current step. intuitively past steps relevant next update using information seems natural. called momentum parameter learning rate. momentum update follows since momentum method modiﬁes basic structure gradient descent approach share convergence rate. similar previous analysis assume convex gradient l-lipschitz continuous. rather updating update minimize objective function. main idea known gamble ﬁrst correct later. estimates next point jump previous gradient direction calculates gradient position correct estimated point. including adagrad method adaptive method next sections follow newton’s method known second-order method. since methods minimize objective function estimated hessian matrix apply newton’s method approach generally perform better algorithms. usually cost exact calculation hessian matrix extremely expensive therefore adagrad algorithm estimates hessian matrix following idea. according consider mean squared error function second term hessian equation goes zero approximation close real value implies estimate hessian matrix outer product gradient vector. approximation quite reasonable given mean squared error functions. approximation always proper arbitrary designed cost functions. especially classiﬁcation tasks often non-smooth cost functions cross entropy loss. consequently mention introduction estimation causes potential limitation adaptive methods applied various loss functions. additionally beneﬁt adagrad author mentioned since method updates parameter vector element-wisely adagrad perform better previous methods like momentum method loss function sparse. compare dense cases sparse relatively chance sparse gradient vector. adagrad method gives larger step size gradient direction highly aﬀects optimization process. therefore rarely occurring factor importance frequently occurring factors. since convexity imply diﬀerentiability import concept sub-gradient. sub-gradient applied algorithms covered here. sub-diﬀerentiable function evaluated denoted particular gradient vector sub-gradient denoted function diﬀerentiable directly implies ∇θj. also denote concatenated important feature adagrad calculate outer product sub-gradient denoted rd×d", "year": 2017}