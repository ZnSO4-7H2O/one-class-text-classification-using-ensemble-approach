{"title": "Ensemble of Generative and Discriminative Techniques for Sentiment  Analysis of Movie Reviews", "tag": ["cs.CL", "cs.IR", "cs.LG", "cs.NE"], "abstract": "Sentiment analysis is a common task in natural language processing that aims to detect polarity of a text document (typically a consumer review). In the simplest settings, we discriminate only between positive and negative sentiment, turning the task into a standard binary classification problem. We compare several ma- chine learning approaches to this problem, and combine them to achieve the best possible results. We show how to use for this task the standard generative lan- guage models, which are slightly complementary to the state of the art techniques. We achieve strong results on a well-known dataset of IMDB movie reviews. Our results are easily reproducible, as we publish also the code needed to repeat the experiments. This should simplify further advance of the state of the art, as other researchers can combine their techniques with ours with little effort.", "text": "sentiment analysis common task natural language processing aims detect polarity text document simplest settings discriminate positive negative sentiment turning task standard binary classiﬁcation problem. compare several machine learning approaches problem combine achieve state art. show task standard generative language models slightly complementary state techniques. achieve strong results well-known dataset imdb movie reviews. results easily reproducible publish also code needed repeat experiments. simplify advance state researchers combine techniques little effort. sentiment analysis among popular simple useful tasks natural language processing. aims predicting attitude text typically sentence review. instance movies restaurant often rated certain number stars indicate degree reviewer satisﬁed. task often considered simplest basic machine learning techniques yield strong baselines often beating much intricate approaches simplest settings task seen binary classiﬁcation positive negative sentiment. however several challenges towards achieving best possible accuracy. obvious represent variable length documents beyond simple words approaches lose word order information. advanced machine learning techniques recurrent neural networks variations however clear provide signiﬁcant gain simple bag-of-words bag-of-ngram techniques work compared several different approaches realized without much surprise model combination performs better individual technique. ensemble best beneﬁts models complementary thus diverse techniques desirable. vast majority models proposed literature discriminative nature parameters tuned classiﬁcation task directly. work boost performance ensemble considering generative language model. train language models positive reviews negative ones likelihood ratio models evaluated test data additional feature. example assume positive review higher likelihood generated model trained large positive reviews lower likelihood given negative model. paper constrained work binary classiﬁcation trained generative models positive negative. could consider higher number classes since approach scales linearily number models train i.e. class. large pool diverse models simple implement yields state performance largest publicly available benchmarks movie reviews stanford imdb dataset reviews. code reproduce experiments available https//github.com/mesnilgr/iclr. section describe detail approaches considered study. novelty paper consists combining generative discriminative models together sentiment prediciton. generative model deﬁnes distribution input. training generative model class bayes rule predict class test sample belongs formally given dataset pairs y}i=...n i-th document training corresponding label number training samples train models subject subject then given input test time compute ratio p+/p− p/p. assigned positive class otherwise negative class. different choices distribution choose from. common n-gram count-based non-parametric method compute order compute likelihood document markov assumption simply multiply n-gram probabilities words document mentioned before train n-gram language model using positive documents model using negative ones. experiments used srilm toolkit train n-gram language models using modiﬁed kneser-ney smoothing furthermore language models trained different datasets mismatch vocabularies words appear training sets. problem scoring test data contain novel words seen least training datasets. avoid problem needed penalty scoring vocabulary word. n-grams simple data-driven build language models. however suffer data sparsity large memory requirement. since number word combinations grows exponentially length context always little data accurately estimate probabilities higher order n-grams. contrast n-grams languages models recurrent neural networks parametric models address issues. inner architecture rnns gives potentially inﬁnite context window allowing perform smoother predictions. know practice context window limited exploding vanishing gradients still rnns outperform signiﬁcantly n-grams state statistical language modeling. review techniques beyond scope short paper point reader depth discussion topic. using n-grams rnns compute probability test document belonging positive negative class bayes’ rule. scores averaged ensemble models explained section among purely discriminative methods popular choice linear classiﬁer bagof-word representation document. input representation usually tf-idf weighted word counts document. order preserve local ordering words better representation would consider also position-independent n-gram counts document ensemble used supervised reweighing counts naive bayes support vector machine approach approach computes log-ratio vector average word counts extracted positive documents average word counts extracted negative documents. input logistic regression classiﬁer corresponds log-ratio vector multiplied binary pattern word document vector. note logictic regression replaced linear svm. implementation slightly improved performance reported adding tri-grams shown table recently proposed unsupervised method learn distributed representations words paragraphs. idea learn compact representation word paragraph predicting nearby words ﬁxed context window. captures co-occurence statistics learns embeddings words paragraphs capture rich semantics. synonym words similar paragraphs often surrounded similar context therefore mapped nearby feature vectors embeddings used represent document ﬁxed size feature vector. authors document descriptor input hidden layer neural network sentiment discrimination. work combine probability scores mentioned models linear interpolation. formally deﬁne overall probability score weighted geometric mean baseline models pkαk best setting weights brute force grid search quantizing coefﬁcient values interval increments search evaluated validation avoid overﬁtting. focus smarter since consider models approach consider scope paper. using models would make method prohibitive. larger number models might want consider random search coefﬁcients even bayesian approaches techniques give better running time performance. section report results largest publicly available sentiment analysis datasets imdb dataset movie reviews. dataset consists movie reviews categorized either positive negative. reviews training rest table reports results individual model. found generative models performed worst rnns slightly better n-grams. competitive method method based reweighed bag-of-words favoring simplicity reproducibility performance results reported paper produced linear classiﬁer. finally table reports results combining previous models ensemble. interpolate scores sentence vectors nb-svm achieve state-of-the-art performance compared reported notice implementation sentence vectors method alone yielded order measure contribution model ﬁnal ensemble classiﬁer remove model time ensemble. observe removal generative model affects least ensemble performance. overall three models contribute success overall ensemble suggesting three models pick complimentary features useful discrimination. table show test reviews misclassiﬁed single models classiﬁed accurately ensemble. proposed simple powerful ensemble system sentiment analysis. combine three rather complementary conceptually different baseline models based generative approach based continuous representations sentences based clever reweighing tf-idf bag-of-word representation document. model contributes success overall system achieving state performance challenging imdb movie review dataset. code reproduce experiments available https//github.com/mesnilgr/iclr. hope researchers take advantage code include results ensemble focus improving state sentiment analysis. experiments match results followed suggestion quoc hierarchical softmax instead negative sampling. however produces accuracy result training test data shufﬂed. thus consider result invalid. sentences really realistic sensible movie ramgopal verma stupidity like songs hindi movies class acting nana patekar much similarities real ’encounters’ leslie nielson talented actor made huge mistake doesn’t even come close funny best word describe stupid good funny good ernest ﬁlms real hoot unintentionally sidney portier’s character sweet lovable want smack nothing movie rings true it’s boring boot movie based novel island moreau version john frankenheimer wasn’t terriﬁc music would hesitate give cinematic underachievement music actually makes like certain passages give maas andrew daly raymond pham peter huang andrew potts christopher. learning word vectors sentiment analysis. proceedings annual meeting association computational linguistics. association computational linguistics socher richard pennington jeffrey huang eric andrew manning christopher semisupervised recursive autoencoders predicting sentiment distributions. conference empirical methods natural language processing wang sida manning christopher baselines bigrams simple good sentiment topic classiﬁcation. proceedings annual meeting association computational linguistics short papers-volume association computational linguistics", "year": 2014}