{"title": "Fisher Kernel for Deep Neural Activations", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Compared to image representation based on low-level local descriptors, deep neural activations of Convolutional Neural Networks (CNNs) are richer in mid-level representation, but poorer in geometric invariance properties. In this paper, we present a straightforward framework for better image representation by combining the two approaches. To take advantages of both representations, we propose an efficient method to extract a fair amount of multi-scale dense local activations from a pre-trained CNN. We then aggregate the activations by Fisher kernel framework, which has been modified with a simple scale-wise normalization essential to make it suitable for CNN activations. Replacing the direct use of a single activation vector with our representation demonstrates significant performance improvements: +17.76 (Acc.) on MIT Indoor 67 and +7.18 (mAP) on PASCAL VOC 2007. The results suggest that our proposal can be used as a primary image representation for better performances in visual recognition tasks.", "text": "compared image representation based low-level local descriptors deep neural activations convolutional neural networks richer mid-level representation poorer geometric invariance properties. paper present straightforward framework better image representation combining approaches. take advantages representations propose efﬁcient method extract fair amount multi-scale dense local activations pre-trained cnn. aggregate activations fisher kernel framework modiﬁed simple scale-wise normalization essential make suitable activations. replacing direct single activation vector representation demonstrates signiﬁcant performance improvements indoor pascal results suggest proposal used primary image representation better performances visual recognition tasks. image representation important factors affect performance visual recognition tasks. barbu introduced interesting experiment simple classiﬁer along human brain-scan data substantially outperforms state-of-the-art methods recognizing action video clips. success local descriptors many researches devoted deep study global image representation based bag-of-word model aggregates abundant local statistics captured hand-designed local descriptors. representation improved vlad fisher kernel adding higher order statistics. major beneﬁt global representations based local descriptors invariance property scale changes location changes occlusions background clutters. recent computer vision researches drastic advances visual recognition achieved deep convolutional neural networks jointly learn whole feature hierarchies starting image pixels ﬁnal class posterior stacked non-linear processing layers. deep representation quite efﬁcient since intermediate templates reused. however deep nonlinear millions parameters estimated. requires strong computing power optimization large training data generalized well. recent presence large scale imagenet database raise parallel computing contribute breakthrough visual recognition. krizhevsky achieved impressive result using large-scale image classiﬁcation. instead training speciﬁc task intermediate activations extracted pre-trained independent large data successfully applied generic image representation. combining activations classiﬁer shown impressive performance wide visual recognition tasks object classiﬁcation object detection scene classiﬁcation ﬁne-grained classiﬁcation attribute recognition image retrieval domain transfer utilizing activations generic image representation straightforward extract responses ﬁrst second fully connected layer pretrained feeding image represent image responses however representation vulnerable geometric variations. techniques address problem. common practice exploiting multiple jitterred images data augmentation. though data augmentation used prevent over-ﬁtting recent researches show average pooling augmenting data averaging multiple activation vectors test stage also helps achieve better geometric invariance cnns improving classiﬁcation performance figure pipeline proposed method. given pre-trained replace ﬁrst fully connected layers equivalent convolutional layers efﬁciently obtain large amount multi-scale dense activations. activations followed multi-scale pyramid pooling layer suggest. consequent image representation combined linear target classiﬁcation task. different experiment enhancing geometric invariance activations also presented. gong proposed method exploit multi-scale activations order achieve geometric invariance characteristic improving recognition accuracy. extracted dense local patches three different scales local patch pre-trained cnn. activations aggregated ﬁner scales vlad encoding introduced encoded activations concatenated single vector obtain ﬁnal representation. paper introduce multi-scale pyramid pooling improve discriminative power activations robust geometric variations. pipeline proposed method illustrated figure similar also utilize multi-scale activations present different pooling method shows better performance experiments. speciﬁcally suggest efﬁcient obtain abundant amount multi-scale local activations aggregate using state-of-the-art fisher kernel simple important scale-wise normalization called multi-scale pyramid pooling. proposal demonstrates substantial improvements scene object classiﬁcation tasks compared previous representations including single activation average pooling vlad activations also demonstrate object conﬁdence maps useful object detection/localization though category-level labels without speciﬁc object bounding boxes used training. according empirical observations replacing vlad kernel fisher kernel present signiﬁcant impact however shows meaningful performance improvements pooling mechanism takes average pooling scale-wise normalization applied. implies performance improvement representation come superiority fisher kernel careful consideration neural activation’s property dependent scales. fisher kernel framework visual vocabulary proposed perronnin extends conventional bag-of-words model probabilistic generative model. models distribution low-level descriptors using gaussian mixture model represents image considering gradient respect model parameters. although number local descriptors varies across images consequent fisher vector ﬁxed-length therefore possible discriminative classiﬁers linear svm. denote d-dimensional local descriptor ...k} denote pre-trained gaussians ...k}. visual word gradient vectors computed aggregating gradients local descriptors extracted image respect mean standard deviation gaussian. then ﬁnal image representation fisher vector obtained concatenating gradient vectors. accordingly fisher kernel framework represents image kddimensional fisher vector figure obtaining multi-scale local activations densely pre-trained cnn. ﬁgure target layer ﬁrst fully connected layer equally implemented convolutional layer containing ﬁlters size obtain activation spatial ordering local descriptors conserved. single pre-trained shared scales. representing image ﬁrst generate scale pyramid input image minimum scale image ﬁxed size scale image times larger resolution previous scale image. feed scaled images pre-trained extract dense activation vectors. then activation vectors merged single vector multi-scale pyramid pooling. consider activation vector local descriptor straightforward aggregate local activations fisher vector explained sec. however activations different scale properties compared sift-like local descriptors explained sec. adopt fisher kernel suitable activation characteristics introduce adding multi-scale pyramid pooling layer modiﬁed follows. given scale pyramid containing scaled image local activation vectors extracted scale ﬁrst apply reduce dimension activation vectors obtain then aggregate local actiscale fisher vector vation vectors fisher encoding fisher vectors merged global vector average pooling -normalization denotes cardinality set. average pooling since natural pooling scheme fisher kernel rather vector concatenation. following improved fisher kernel framework ﬁnally apply power normalization -normalization fisher vector overall pipeline illustrated figure table average time extracting multi-scale dense activations image. caffe reference model activations extracted random images pascal timings based server .ghz intel xeon titan black. directions model parameters best local descriptors image gmm. ﬁsher kernel framework improved additional two-stage normalizations power-normalization factor followed -normalization. refer theoretical proofs details. obtain multi-scale activations without modiﬁcation previous approach cropped local patches patches network resizing patches ﬁxed size input. however extract multi-scale local activations densely approach quite inefﬁcient since many redundant operations performed convolutional layers overlapped regions. extract dense activations without redundant operations simply replace fully connected layers existing equivalent multiple convolution ﬁlters along spatial axises. image larger ﬁxed size modiﬁed network outputs multiple activation vectors vector activations corresponding local patch. procedure illustrated fig. method thousands dense local activations multiple scale levels extracted reasonable extraction time shown table empirical analysis scale-wise classiﬁcation scores pascal analysis ﬁrst diversify dataset seven different scale levels smallest scale resolution biggest scale resolution extract dense sift descriptors local activation vectors seventh layer cnn. then follow standard framework encode fisher vectors train independent linear scale respectively. fig. show results classiﬁcation performances using sift-fisher cnn-fisher according scale. ﬁgure demonstrates clear contrast siftfisher cnn-fisher. cnn-fisher performs worst largest image scale since local activations come small image regions original image siftfisher performs best scale since sift properly captures low-level contents within small regions. aggregate activations scales fisher vector poorly performing activations dominant inﬂuence large weight image representation. possible strategy aggregating multi-scale activations choose activations scales relatively performing well. however selection good scales dependent dataset activations large image scale also contribute geometric invariance property balance inﬂuence scale. empirically examined various combinations pooling shown sec. found scale-wise fisher vector normalization followed simple average pooling effective balance inﬂuence. compare pooling method naive fisher pooling experiment apply pooling methods different numbers scales perform classiﬁcation pascal despite simplicity multi-scale pyramid pooling demonstrates superior figure classiﬁcation performance sift-fisher cnnfisher according image scale pascal tick labels horizontal axis denote image scales average number local descriptors. compare scale characteristics traditional local features activations. tells suitable directly adopt fisher kernel framework multi-scale local activations representing image. investigate best aggregating activations global representation perform empirical studies conclude applying scale-wise normalization fisher vectors important. naive obtain fisher vector given multiscale local activations aggregate better combine fisher kernel mid-level neural activations property activations according patch scale took consideration. traditional fisher kernel visual classiﬁcation tasks hand-designed local descriptors sift often densely computed multi-scale. local descriptor encodes low-level gradient information within local region captures detailed textures shapes within small region rather global structure within larger region. contrast mid-level neural activation extracted higher layer cnns represents higher level structure information closer class posteriors. shown visualization proposed zeiler fergus image regions strongly activated certain ﬁlter ﬁfth layer usually capture category-level entire object. performances depicted fig. performance naive fisher kernel pooling deteriorates rapidly ﬁner scale levels involved. indistinctive neural activations ﬁner scale levels become dominant forming fisher vector. representation however exhibits stable performance accuracy constantly increasing ﬁnally saturated. verify pooling method aggregates multi-scale activations effectively. indoor used scene classiﬁcation task. dataset contains images indoor scene classes total. challenging dataset many indoor classes characterized objects contain rather spatial properties. performance measured top- accuracy. pascal used object classiﬁcation task. consists images object classes total. task quite difﬁcult since scales objects ﬂuctuate multiple objects different classes often contained image. performance measured mean average precision. oxford flowers used ﬁne-grained object classiﬁcation task distinguishes sub-classes object class. dataset consists images ﬂower classes. class consists various numbers images performance measured top- accuracy. cnns pre-trained ilsvrc’ dataset extract multi-scale local activations. caffe reference model composed convolutional layers three fully connected layers. model performed top- error single center-crop validation image used evaluation ilsvrc’ dataset. henceforth denote model alex since nearly architecture krizhevsky al.’s overfeat also composed convolutional layers three fully connected layers. shows top- error ilsvrc’ dataset center-crop. compared alex uses smaller ﬁlters dense stride ﬁrst convolutional layer. experiments conducted mostly alex default. cnns used pascal dataset compare method demonstrates excellent performance cnns. pre-trained models available online overall procedure image representation follows. given image make image pyramid containing seven scaled images. image pyramid twice resolution previous scale starting standard size deﬁned feed scale image obtain vectors dimensional dense activations seventh layer. dimensionality activation vector reduced projection trained activation vectors sampled training images. visual vocabulary also trained samples. consequently dimensional fisher vector computed power-normalization follow. one-versus-rest linear svms quadratic regularizer hinge loss trained ﬁnally. perform comprehensive experiments compare various methods three recognition tasks. ﬁrst show performance method baseline methods. then compare result state-of-the-art methods dataset. simplicity notation protocol denotes pooling method denotes descriptors pooled notations summarized table compare method several baseline methods. baseline methods include intermediate activations pre-trained standard input average pooling multiple jittered images modiﬁed versions method. comparison results dataset summarized table expected basic representation alex-fc performs worst datasets. average pooling improves performance +.%∼+% however improvement bounded regardless number data augmentation. baseline methods exploit multi-scale activations show better results single-scale representations. compared performance gains multi-scale activations exceed dataset. shows image representation based activations enriched utilizing multi-scale local activations. even though baseline methods exploiting multi-scale activations show substantial improvements compared single-scale baselines also verify handling multi-scale activations important improvement. compared naive fisher kernel pooling achieves extra signiﬁcant performance gain dataset. instead pooling multi-scale activations concatenating encoded fisher vectors another option done gong al.’s method concatenation also improves performance however without additional dimension reduction raises dimensionality proportional number scales still outperforms datasets. comprehensive test various pooling strategies shows proposed image representation used primary image representation wide visual recognition tasks. also apply spatial pyramid kernel representation. construct spatial pyramid four sub-regions increases dimensionality representation four times. results unequable differences marginal datasets. result surprising rich activations smaller image scales already cover global layout. makes kernel redundant. table compare result various stateof-the-art methods indoor similar ours gong proposed pooling method multi-scale activations. performed vlad pooling scale concatenated them. compared representation largely outperforms method gain +.%. performance possibly comes large number scales superiority fisher kernel details pooling strategy. three scales extract seven-scale activations quite efﬁcient though adding local activations ﬁner scales naive harm performance actually contribute better invariance property proposed mpp. addition experiment shown suitable aggregating multi-scale activations concatenation. implies better performance come superior fisher kernel better handling multi-scale neural activations. record holder indoor dataset combined alex-fc complementary features called dsfl. dsfl learns discriminative shareable ﬁlters target dataset. stack additional pool layer already achieve state-of-the-art performance pretrained alex only. also stack dsfl feature representation result shows performance shows representation also improved combining complementary features. results summarized table methods alex network. razavian performed target data augmentation oquab used multi-layer perceptron instead linear ground truth bounding boxes. representation outperforms methods using pre-trained alex without data augmentation bounding annotations. gains respectively. recent methods outperforming method. adopting better cnns source task target task spatial pyramid pooling network multi-label cnns basic demonstrates slightly lower precisions compared them however basic alex without ﬁnetuning representation equipped superior cnns ﬁne-tuned representation reaches nearly statof-the-art performance method improved stacking mpp. performance still lower conduct target data augmentation ﬁne-tuning. believe method improved additional techniques ﬁne-tuning target data augmentation ground truth bounding boxes leave issue future work major focus generic image representation pre-trained cnn. table shows classiﬁcation performances flowers. method outperforms previous state-of-the-art method without powerful representation various previous methods show much lower performances. table shows per-class performances compared state-of-the-art methods method performs best classes among classes. interesting classes include bottle pottedplant tvmonitor representative small objects dataset. results clearly demonstrates beneﬁt aggregates activations ﬁner-scales well prone harm performance handled inappropriately. method cnn-fc average pooling crops ﬂips given size input image. average pooling crops ﬂips given size input image. naive fisher kernel pooling without scale-wise vector normalization given multi-scale image pyramid. concatenation scale-wise normalized fisher vectors given multi-scale image pyramid. proposed representation given multi-scale image pyramid. acc. method description yes. baseline alex-fc yes. baseline yes. baseline yes. baseline yes. baseline yes. yes. mpp+sp yes. mpp+dsfl yes. baselines methods. method acc. description singh part+gist+dpm+sp juneja ifk+bag-of-parts doersch ifk+midlevelrepresent. dsfl yes. dsfl+alex-fc zhou yes. alex-fc zhou alex-fc yes. razavian ap+pt+targetaug. yes. gong vlad concat. yes. table classiﬁcation performances pascal classiﬁcation. represents ﬁne-tuning pre-trained denotes ground truth object bounding boxes training. interesting feature method present object conﬁdence maps object classiﬁcation tasks though train classiﬁers without bounding annotation class-level labels. recover conﬁdence maps trace much weight given local patch accumulate weights local activations. tracing weight local activations possible ﬁnal representation formed regardless number scales number local activation vectors. trace weight patch compute ﬁnal representation patch using corresponding single activation vector compute score pre-trained classiﬁers used object classiﬁcation. fig. fig. show several examples object conﬁdence test images. ﬁgures verify image representation encodes discriminative image patches well despite large within-class variations well substantial geometric changes. discussed sec. images containing small-size objects also present accurate conﬁdence maps. maps utilized considerable object detection/localization also useful analyzing image representation. proposed multi-scale pyramid pooling better neural activations pre-trained cnn. several conclusions derive study. first take scale characteristic neural activations consideration successful combination fisher kernel cnn. activations become uninformative patch size becomes smaller however contribute better scale invariance meet simple scale-wise normalization. second dense deep neural activations multiple scale levels extracted reasonable computations replacing fully connection equivalent multiple convolution ﬁlters. enables pool truly multi-scale activations achieve signiﬁcant performance improvements visual recognition tasks. third reasonable object-level conﬁdence maps obtained image representation even though class-level labels given supervision applied object detection localization tasks. comprehensive experiments three different recognition tasks results suggest proposal used primary image representation better performances various visual recognition tasks.", "year": 2014}