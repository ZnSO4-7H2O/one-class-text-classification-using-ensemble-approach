{"title": "Representation Learning for Visual-Relational Knowledge Graphs", "tag": ["cs.LG", "cs.AI"], "abstract": "A visual-relational knowledge graph (KG) is a multi-relational graph whose entities are associated with images. We introduce ImageGraph, a KG with 1,330 relation types, 14,870 entities, and 829,931 images. Visual-relational KGs lead to novel probabilistic query types where images are treated as first-class citizens. Both the prediction of relations between unseen images and multi-relational image retrieval can be formulated as query types in a visual-relational KG. We approach the problem of answering such queries with a novel combination of deep convolutional networks and models for learning knowledge graph embeddings. The resulting models can answer queries such as \"How are these two unseen images related to each other?\" We also explore a zero-shot learning scenario where an image of an entirely new entity is linked with multiple relations to entities of an existing KG. The multi-relational grounding of unseen entity images into a knowledge graph serves as the description of such an entity. We conduct experiments to demonstrate that the proposed deep architectures in combination with KG embedding objectives can answer the visual-relational queries efficiently and accurately.", "text": "abstract. visual-relational knowledge graph multi-relational graph whose entities associated images. introduce imagegraph relation types entities images. visual-relational lead novel probabilistic query types images treated ﬁrst-class citizens. prediction relations unseen images multi-relational image retrieval formulated query types visual-relational approach problem answering queries novel combination deep convolutional networks models learning knowledge graph embeddings. resulting models answer queries unseen images related other?\" also explore zero-shot learning scenario image entirely entity linked multiple relations entities existing multi-relational grounding unseen entity images knowledge graph serves description entity. conduct experiments demonstrate proposed deep architectures combination embedding objectives answer visual-relational queries eﬃciently accurately. several application domains modeled knowledge graphs entities represented nodes object attributes node attributes relationships entities directed edges nodes. instance product recommendation system represented knowledge graph nodes represent customers products typed edges represent customer reviews purchasing events. medical domain several knowledge graphs model diseases symptoms drugs genes interactions increasingly entities knowledge graphs associated visual data. instance online retail domain product advertising images medical domain patient-associated imaging data sets ability knowledge graphs compactly represent domain attributes relations make important component numerous systems. facilitate integration organization retrieval structured data support various forms reasoning. recent years playing increasingly crucial role ﬁelds question answering language modeling text generation even though large body work learning reasoning setting visual-relational entities associated visual data received much attention. visual-relational represents entities relations entities large number images associated entities imagenet visualgenome datasets based wordnet predominantly used either object classiﬁcation data case imagenet facilitate scene understanding single image. imagegraph propose problem reasoning visual concepts across large images organized knowledge graph. core idea treat images ﬁrst-class citizens relational completion queries. combination multi-relational structure numerous complex queries possible. main objective work understand extent visual data associated entities used conjunction deep learning methods answer visual-relational queries. allowing images arguments queries facilitates numerous novel query types. figure list query types address paper. order answer queries built embedding methods well deep representation learning approaches visual data. ﬂurry machine learning approaches tailored speciﬁc problems link prediction knowledge graphs. examples knowledge base factorization embedding approaches random-walk based models combine approaches deep neural networks facilitate visual-relational query answering. numerous application domains could beneﬁt query answering visual kgs. instance online retail visual representations novel products could leveraged zero-shot product recommendations. crucially instead able retrieve similar products visual-relational would support prediction product attributes speciﬁcally attributes customers might interested instance fashion industry visual attributes crucial product recommendations general believe able ground novel visual concepts existing attributes various relation types reasonable approach zero-shot learning. make following contributions. first introduce imagegraph visual-relational relations images associated diﬀerent entities. second introduce visual-relational query types. third propose novel neural architectures objectives answering novel query types. ﬁrst time deep cnns embedding learning objectives combined joint model. fourth show proposed class deep neural networks also successful zero-shot learning creating relations entirely unseen entities using visual data query time. relational visual data answering queries visual-relational knowledge graph main objective. previous work combining relational visual data focused object detection scene recognition required complex visual-relational reasoning. recent years witnessed surge reasoning human-object object-object object-attribute relationships visualgenome project knowledge base integrates language vision modalities. project provides knowledge graph based wordnet provides annotations categories attributes relation types image. recent work used dataset focus scene understanding single images. instance proposed model detect relation types objects depicted image inferring sentences riding bicycle.\" veit propose siamese learn metric representation pairs textile products learn products similar styles. large body work metric learning objective generate image embeddings pairwise distance-based loss minimized recent work extended idea directly optimize clustering quality metric zhou propose method based bipartite graph links depictions meals ingredients. johnson propose visualgenome data recover images text queries. imagegraph diﬀerent data sets relation types hold diﬀerent images image annotated entities. deﬁnes novel class problems seeks answer queries images related?\" work address problems ranging predicting relation types image pairs multi-relational image retrieval. zero-shot learning focus exploring ways used relationships unseen images images depicting novel entities part visual depictions known entities. form zero-shot learning objective generalize novel visual concepts without seeing training examples. generally methods rely underlying embedding space attributes order recognize unseen categories. however paper assume availability common embedding space assume existence external visual-relational similar approach explicit knowledge encoded underlying embedding space works rely ﬁnding similarities linguistic space leveraging distributional word representations capture notion taxonomy similarity. works address scene understanding single image i.e. models able detect visual relationships given image. contrary models able relationships diﬀerent images entities. imagegraph visual-relational whose relational structure based freebase speciﬁcally based subset freebase used benchmark data since include visual data perform following steps enrich entities image data. implemented crawler able parse query results image search engines google images bing images yahoo image search. minimize amount noise polysemous entity labels extracted entity wikipedia uris billion triple freebase dump. instance springﬁeld massachusetts obtained uris springfield_ springfield_. uris processed used search queries disambiguation purposes. used crawler download images removed corrupted quality duplicate images used images returned image search engines whenever results. images scaled maximum height width pixels maintaining aspect ratio. resulted images associated diﬀerent entities ﬁltering triples either head fig. plots distribution relation type frequencies. y-axis represents number occurrences x-axis relation type index. shows common relation types. tail entity could associated image visual consists triples expressing diﬀerent relation types entities. provide three sets triples training validation testing plus three image splits also training validation test. table lists statistics resulting visual derived fbk- also associated crawled images. since providing images would violate copyright provide code distributed crawler list image urls crawled experiments paper.. distribution relation types depicted figure show logarithmic scale number times relation occurs observe relationships like award_nominee profession occur quite frequently others ingredient occur times. relation types symmetric asymmetric neither distinct entity types person athlete city. figure plot frequent entity types. figure plot entity frequencies example entities. best knowledge imagegraph visual-relational entities relation types entity-level images. main diﬀerences imagegraph imagenet following. imagenet based wordnet lexical database synonymous words lexical category grouped synsets. relations expressing connections synsets. freebase hand orders magnitudes relations. subset focus relations expressing location places positions basketball players gender entities. moreover entities imagenet exclusively represent entity types cats cars whereas entities either entity types instances entity types albert einstein paris. renders computer vision problems associated imagegraph challenging existing datasets. moreover imagegraph focus learning knowledge graph given triples statements form head tail entities respectively relation type. figure depicts small fragment relations entities images associated entities. prior work included image data therefore focused following types queries. first query type asks relations given pair head tail entities. second query types asks entities correctly completing triple. latter query type often referred knowledge base completion. here focus queries involve visual data query objects objects either contained queries answers queries both. entities associated image data several completely novel query types possible. figure lists query types focus paper. refer images used training seen images unseen. fig. proposed architecture query answering. illustration possible approaches visual-relational query answering. predict relation types images directly combine entity classiﬁer embedding model relation prediction query types sought-after relations underlying entities never observed training. query types form zero-shot learning since neither entity’s relationships entities images observed training. considerations illustrate novel nature visual query types. machine learning models able learn relational semantics simply classiﬁer assigns images entities. query types also motivated fact typical number entities orders magnitude greater number relations. ﬁrst discuss state embedding methods translate concepts query answering visual-relational kgs. rawi feature representation entity diﬀerentiable functions. completion methods learn embedding entities vector space scoring function trained assign high scores correct triples scores incorrect triples. scoring functions often form relation d-dimensional vectors embedding function maps input representation entities embedding space. case without additional visual data representation entity simply one-hot encoding. embedding vectors head tail entities combined parameter vector diﬀerence −||eh et|| multiplication circular correlation concatenation concatenation instances matrix vectors learned training. general parameters trained high true triples triples assumed hold training objective often based logistic loss shown superior composition functions tpos tneg positive negative training triples respectively parameters trained learning regularization hyperparameter. objective process creating corrupted triples tneg required. often involves sampling random entity either head tail entity. answer queries types training form possible completions queries compute ranking based scores assigned trained model completions. visual-relational input consists image data instead one-hot encodings entities. approach propose builds ideas methods developed completion. instead simple embedding function multiplies input weight matrix however deep convolutional neural networks extract meaningful visual features input images. composition function evaluate four operations used completion literature diﬀerence multiplication concatenation circular correlation. figure depicts basic architecture trained query answering. weights parts neural network responsible embedding image input denoted tied. also experimented additional hidden layers indicated dashed dense layer. composition operation either diﬀerence multiplication concatenation conduct series experiments evaluate proposed approach visualrelational query answering. first describe experimental set-up applies experiments. second report interpret results diﬀerent types visual-relational queries. used caffe deep learning framework designing training evaluating proposed models. embedding function based model introduced pre-trained ilsvrc data derived imagenet removed softmax layer original vgg. added -dimensional layer last dense layer vgg. output layer serves embedding input images. reason reducing embedding dimensionality motivated objective obtain eﬃcient compact latent representation feasible billion entities. composition function performed either four operations diﬀerence multiplication concatenation circular correlation. also experimented additional hidden layer relu activation. figure depicts generic network architecture. output layer architecture softmax sigmoid activation cross-entropy loss. initialized weights newly added layers xavier method used batch size maximal possible ﬁtting memory. create training batches sample random triple uniformly random training triples. given triple randomly sample image head tail training images. applied learning rate parameters learning rate remaining parameters. crucial diﬀerent learning rates since large gradients newly added layers would lead unreasonable changes pretrained part network. weight decay reduced learning rate factor every iterations. models trained iterations. since answers query types either rankings images rankings relations utilize metrics measuring quality rankings. particular report results hits measuring percentage times correct relation ranked highest also compute median ranks correct entities relations mean reciprocal rank entity relation rankings respectively test triples rankr rank correct relation rankimg rank highest ranked image entity query remove triples also correct answers query ranking. experiments commodity hardware single nvidia given pair unseen images want determine relations underlying unknown entities. expressed figure illustrates query type refer visual relation prediction. train deep architectures using training validation triples images respectively. triple training data sample training image uniformly random head tail entity. architecture depicted figure softmax activation categorical cross-entropy loss. test triple sample image uniformly random test images head tail entity respectively. pair images query trained deep neural networks. robust statistical estimate evaluation measures repeat process three times test triple. again none test triples images seen training training images used testing. computing answer query takes model compare proposed architectures diﬀerent baselines based entity classiﬁcation followed embedding method relation prediction probabilistic baseline entity classiﬁcation baseline consists ﬁne-tuning pretrained classify images entities imagegraph. obtain relation type ranking test time predict entities head tail using embedding method distmult return ranking relation types given pair. distmult embedding method achieves state results completion therefore experiment substitute original output layer pretrained imagenet output layer suitable problem. train join train validation splits learning rate layers train following strategy experiments. system trained test model classifying entities images test set. train distmult sample negatives triples positive triple used embedding size figure illustrates vgg+distmult baseline contrasts proposed approach. second baseline computes probability relation type using training validation triples. baseline ranks relation types based prior probabilities. table lists results baselines diﬀerent proposed architectures. probabilistic baseline outperforms vgg+distmult baseline metrics. highly skewed distribution relation types training validation test triples. small number relation types makes large fraction triples. figure plots counts relation types entities. moreover despite distmult achieving hits value relation prediction problem entity pairs baseline vgg+distmult performs poorly. poor entity classiﬁcation performance remainder experiments therefore compare probabilistic baseline. lower part table lists results experiments. diff mult stand diﬀerent possible composition operations. omitted composition operation circular correlation since able make corresponding model converge despite trying several diﬀerent optimizers hyperparameter settings. post-ﬁx stands architectures added additional hidden layer relu activation softmax. concatenation operation clearly outperforms multiplication diﬀerence operations. contrary ﬁndings completion literature mult diff outperformed concatenation operation. models additional hidden layer perform better shallower counterparts exception diff model. hypothesize diﬀerence linear composition operation beneﬁting additional non-linearity. proposed models outperforms baselines. given unseen image know underlying entity relation type want retrieve existing images complete query. image head entity given return ranking images tail entity; tail entity image given return ranking images head entity. problem corresponds query type figure note equivalent performing multi-relational metric learning which best knowledge done before. performed experiments three composition functions diﬀerent activation/loss functions. first used models trained softmax activation categorical cross-entropy loss rank images. second took models trained softmax activation substituted softmax activation sigmoid activation corresponding binary cross-entropy loss. training triple created negative triples sampling head tail entity entities. negative triples used conjunction binary cross-entropy loss equation reﬁne pretrained weights. directly training model binary cross-entropy loss possible since model converge properly. pretraining softmax activation categorical cross-entropy loss crucial make binary loss work. testing used test triples ranked images based probabilities returned respective models. instance given query substituted imgt? training validation images time ranked images according probabilities returned models. rank highest ranked image belonging true entity compute values evaluation measures. repeat experiment three times report average values. again compare results diﬀerent architectures probabilistic baseline. baseline however compute distribution head tail entities relation types. example relation type locatedin compute table lists results experiments. relation prediction best performing models based concatenation operation followed diﬀerence multiplication operations. architectures additional hidden layer improve performance. also provide results concatenation-based model softmax activation reﬁned weights using sigmoid activation negative sampling described before. model best performing model. neural network models signiﬁcantly better baseline respect median hits. however baseline slightly superior results mrr. skewed distribution entities relations shows baseline highly competitive given figure visualizes answers cat-sig model provided four example queries. queries left model performed well ranked correct entity examples right illustrate queries model returned inaccurate ranking. perform query answering highly eﬃcient manner precomputed stored image embeddings once compute scoring function query time. answering multi-relational image retrieval query took last experiments addresses problem zero-shot learning visual relation prediction. query types given image entirely entity part ﬁrst query type asks relations given image unseen image know underlying entity. second query type asks relations given image existing entity. believe creating multi-relational links existing entities reasonable approach zero-shot learning since unseen entity category integrated existing relations existing visual concepts attributes provide characterization fig. example results zero-shot learning. pair images three relation types listed. pair images ﬁrst relation type correct. pair images bottom correct relation type taxonomyhasentry among three relation types. entity/category. problem cannot addressed embedding methods since entities need part training models work. zero-shot experiments generated training validation test triples. randomly sampled entities occur head test triples. removed training validation triples whose head tail entities. finally kept test triples entities either head tail both. query type know target entity sample images models times compute probability. average probabilities rank relations. query type image sampled randomly. previous experiments repeated procedure three times averaged results. baseline compute probabilities relation training validation probabilities relations conditioned target entity again competitive baselines skewed distribution relations entities. table lists results experiments. model based concatenation operation outperforms baseline performs surprisingly well. deep models able generalize unseen images since performance comparable performance relation prediction task entity part training figure depicts example queries zero-shot query type ﬁrst query example model ranked correct relation type ﬁrst second example challenging correct relation type part ranked relation types. understanding single image. present novel visual-relational entities enriched visual data. proposed several novel query types introduce neural architectures suitable probabilistic query answering. propose novel approach zero-shot learning problem visually mapping image entirely entity observed relation types proposed models tend learn ﬁne-grained visual type typically occurs head tail relation type. cases conditioning either head tail entity inﬂuence predictions models substantially. potential shortcoming proposed methods believe room improvement probabilistic query answering visual-relational kgs.", "year": 2017}