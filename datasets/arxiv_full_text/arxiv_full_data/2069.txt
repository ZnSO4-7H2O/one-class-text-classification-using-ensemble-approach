{"title": "Structured Priors for Structure Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Traditional approaches to Bayes net structure learning typically assume little regularity in graph structure other than sparseness. However, in many cases, we expect more systematicity: variables in real-world systems often group into classes that predict the kinds of probabilistic dependencies they participate in. Here we capture this form of prior knowledge in a hierarchical Bayesian framework, and exploit it to enable structure learning and type discovery from small datasets. Specifically, we present a nonparametric generative model for directed acyclic graphs as a prior for Bayes net structure learning. Our model assumes that variables come in one or more classes and that the prior probability of an edge existing between two variables is a function only of their classes. We derive an MCMC algorithm for simultaneous inference of the number of classes, the class assignments of variables, and the Bayes net structure over variables. For several realistic, sparse datasets, we show that the bias towards systematicity of connections provided by our model yields more accurate learned networks than a traditional, uniform prior approach, and that the classes found by our model are appropriate.", "text": "traditional approaches bayes structure learning typically assume little regularity graph structure sparseness. however many cases expect systematicity variables real-world systems often group classes predict kinds probabilistic dependencies participate capture form prior knowledge hierarchical bayesian framework exploit enable structure learning type discovery small datasets. speciﬁcally present nonparametric generative model directed acyclic graphs prior bayes structure learning. model assumes variables come classes prior probability edge existing variables function classes. derive mcmc algorithm simultaneous inference number classes class assignments variables bayes structure variables. several realistic sparse datasets show bias towards systematicity connections provided model yield accurate learned networks traditional approach using uniform prior classes found model appropriate. unsupervised discovery structured predictive models sparse data central problem artiﬁcial intelligence. bayesian networks provide useful language describing large class predictive models much work unsupervised learning focused discovering structure data. approaches bayes structure learning assume generic priors graph structure sometimes encoding sparseness bias otherwise expecting regularities learned graphs. however many cases expect systematicity variables real-world systems often play characteristic roles usefully grouped classes predict kinds probabilistic dependencies participate systematicity provides important constraints many aspects learning inference. consider domain medical learning reasoning. knowledge engineers area historically imposed strong structural constraints; qmr-dt network example segregates nodes diseases symptoms permits edges former latter. recent attempts medical knowledge engineering continued tradition; example kraaijeveld explicitly advocate organizing bayes nets diagnosis three layers inﬂuence ﬂowing context fault inﬂuence nodes. similar divisions classes pervade literature probabilistic graphical models biological interactions; example learn gene networks discovering salient example structuring problem learning gene networks around discovery small regulators responsible controlling activation genes also inﬂuence other. knowledge relationships classes provide inductive constraints allow bayesian networks learned much less data would otherwise possible. example consider structure learner faced database medical facts including list patients series otherwise undiﬀerentiated conditions provided. learner knew that ﬁrst conditions diseases rest symptoms inﬂuence possible diseases symptoms learner would need consider dramatically reduced hypothesis space structures. various forms less speciﬁc knowledge could also quite useful knowing variables played similar causal role even without precise knowledge role entailed. causal roles also support transfer medical expert system knowledgeable lung conditions faced data liver conditions deﬁned entirely variables would able transfer speciﬁc probabilistic dependencies could potentially transfer abstract structural patterns finally abstract structural knowledge often interesting right aside inductive bias contributes. example biologist might interested learn certain genes regulators learning speciﬁc predictive causal links particular genes. present hierarchical bayesian approach structure learning captures sort abstract structural knowledge using nonparametric block-structured priors bayes graph structures. given observations variables simultaneously infer speciﬁc bayes variables abstract structural features dependencies. assume variables come classes prior probability edge existing between variables function classes. general assume knowledge number classes class assignments variables probabilities edges existing given classes; aspects prior must inferred estimated time learn structure parameters bayes net. derive approach simultaneous inference features data real-valued parameters integrated analytically mcmc used infer number classes class assignments variables bayes structure variables. show bias towards systematicity connections provided model yield accurate recovery network structures uniform prior approach underlying structure exhibits systematicity. also demonstrate randomly generated sparse dags nonparametric form inductive bias protect incurring substantial penalty prior mismatch. number previous approaches learning inference exploited structural constraints abstract levels speciﬁc bayesian network. approaches typically show tradeoﬀ representational expressiveness ﬂexibility abstract knowledge ease knowledge learned. tradeoﬀ approaches like assume bayes learned must respect strict constraints case variables divided modules variables module share parents conditional probability table method yields powerful inductive bias learning network structure module assignments data especially appropriate study gene regulatory networks appropriate modeling many domains less regular structure. qmr-dt hepar networks example contain highly regular largely nonmodular structure would like able discover sort network good characterization sort regularity. ﬂexibilitylearnability tradeoﬀ frameworks probabilistic relational models provide expressive language abstract relational knowledge constrain space bayesian networks appropriate given domain signiﬁcant challenge learn including class structure undiﬀerentiated event data. work aims valuable intermediate point ﬂexibility-learnability spectrum. consider hypotheses abstract structure less expressive prms simultaneously learnable data along speciﬁc networks probabilistic dependencies. nonparametric models also yield stronger inductive bias structure learning uniform prior weaker ﬂexible module networks. intuition nodes bayes predictive classes formalized several ways. paper focus structure priors obtained similar formalizations ordered blockmodel blockmodel. joint distributions work described meta-model consisting three major pieces prior structure bayes nets prior parameterizations bayes nets given structure data likelihood induced parameterization. simplicity work discrete-state bayesian networks known domains standard conjugate dirichlet-multinomial model conditional probability tables note variable figure refers graph structure elaboration full bayesian network conditional probability tables. inference model figure specializations characterizes bayes structure learning problem. step towards representation learning causal roles. starting point inﬁnite blockmodel nonparametric generative model directed graphs modify ways produce acyclic graphs. ﬁrst describe generative process ordered blockmodel generate class-assignment vector containing partition nodes graph chinese restaurant process hyperparameter represents partition terms restaurant countably inﬁnite number tables table corresponds group partition seating assignment person class object. people seated existing table probability proportional number previous occupants table. tables created probability proportional hyperparameter exchangeable distribution partitions induces invariant order entry people restaurant. speciﬁcally have note graphs possible process though ones lacking salient block structure typically require classes. also note hyperparameters process intuitive interpretable. controls prior partitions nodes classes; smaller values favor fewer groups priori. controls matrix values favoring clean blocks increases yielding biases towards sparseness denseness respectively. ordering encodes causal depth nodes based classes. believe speciﬁc directed dependencies important outputs structure discovery causal systems constrain priors assign equal mass members particular markov equivalence class. blockmodel prior include ordering graph generation essentially above probability edge node class node class entries permitted nonzero. cyclic graphs rejected afterwards renormalization. formally blockmodel version acyclic otherwise. priors appropriate depending situation. interested classes correspond diﬀerent stages causal process ordered blockmodel appropriate. expect nodes given class include connections others class study gene regulatory networks regulators inﬂuence others blockmodel perform better. additionally models generate directed acyclic graph. blockmodel either assigning nodes class learning sparsity graph assigning nodes diﬀerent classes ordered blockmodel option assigning nodes diﬀerent classes corresponding layers topological ordering arbitrary dag. data likelihood standard marginal likelihood complete discrete observations assuming cpts integrated out; plays role pseudo-counts setting degree expected determinism cpts overall then model three free parameters give opportunity encode weak prior knowledge diﬀerent levels abstraction. generate square graph template matrix size equal number classes generated partition ηoaob represents probability edge node class node class ensure acyclicity entries except strictly diagonal; possibly nonzero entries ηoaob drawn beta distribution normalizing constant) determining contribution given structure predictive distribution explicitly represent posterior parameters structure using conjugacy. reduce size hypothesis space experiments paper also integrate matrix. number edges nodes class nodes class corresponding number missing edges number classes before. have inference general entail ﬁnding posterior beliefs organize inference around markov chain monte carlo procedure consisting gibbs metropolis-hastings kernels retain theoretical correctness guarantees limit long simulations. course straightforward anneal markov chain periodically restart interested value high-scoring states selective model averaging overall mcmc process decouples moves graphs relevant moves latent states prior graph moves simple gibbs sample potential edge conditioned rest hidden state. involves scoring states current graph graph given edge toggled joint renormalizing sampling therefore quite computationally cheap. priors place probability mass cyclic graphs cyclic graphs never accepted. furthermore much likelihood cached since nodes whose parent changed contribute diﬀerent term likelihood. found gibbs moves eﬀective classic neighborhood based graph moves discussed though general expect combinations moves mhmoves randomly propose reversing edges neighborhood-based moves eﬀective. conditionally sampling latent states describe block structure also straightforward. unordered case simply standard gibbs sampler chinese restaurant process ﬁxing class assignments nodes call running free node graphical meta-model ordered blockmodel. dashed line indicates components meta-model corresponding traditional bayes structure learning uniform prior. latent variables representing abstract structural knowledge. nonparametric hierarchical approach attractive several reasons. gracefully handles spectrum class granularities generally preferring produce clumps permitting object class necessary. provide convenient locations insertion strong weak prior knowledge ranging complete template knowledge expectation reasonable template found. also represent additional outputs learning interest cognitive applications scientiﬁc data analysis throughout paper compare approach bayes structure learning given conventional uniform prior dags. formally corresponds subcomponent meta-model figure indicated dashed prior graph structures given equation single class ﬁxed figure results three models -node layered topology data samples provided training. upper displays marginal probabilities edge graph model obtained selective model averaging samples. bottom displays edge marginals adjacency matrix along marginal probabilities indicating inferences class structure. speciﬁcally process exchangeable gibbs sample given entry taking last person enter restaurant sampling class assignment normalized product conditionals equation ordered case must careful. there relative ordering classes objects except whose resampling. considering creation class consider insertions possible free spots relative order. leaves exhaustive mutually exclusive possibilities score joint distribution renormalize gibbs sampler. large problems obtaining even approximate posteriors beyond current computational capabilities search techniques like would doubt useful approximate estimation. furthermore additional temperature parallelization schemes well sophisticated moves including splitting merging classes could used improve mixing. evaluate approach three ways comparing throughout uniform model baseline uses uniform prior graph structures. first consider simple synthetic examples withstrong block structure. second explore networks topologies parameterizations inspired networks real-world interest including qmrdt network gene regulatory network. finally report model performance data sampled hepar engineered network captures knowledge liver disorders. interest using structured priors learn predictive structure small sample sizes usually cannot hope identify structural features deﬁnitively. thus evaluate learning performance bayesian setting looking marginal posterior probabilities edges class assignments. focused initial studies fairly small networks approximate bayesian inferences network structure done quickly scaling larger networks important goal future work. cases used mcmc scheme explore space graphs classes orders report results based approximate posterior constructed relative scores best models found. pool models chose typically constructed searching times iterations each; found generally suﬃcient state least matched score ground truth structure. sampled several training test sets structure; here report representative results. hyperparameters values throughout real-world applications might adjust parameters capture prior knowledge true graph likely sparse number underlying classes either large small ﬁxing hyperparameters allows fairer comparison uniform model. compared uniform model expect block models perform well true graph block-structured. figure shows results given samples three-layered structure left. cpts network sampled symmetric dirichlet distribution hyperparameter true graph strongly block structured even though number samples small block models discover three classes make accurate predictions figure learning results data sampled graph without block structure. adjacency matrix representing true graph entry black edge edge class assignment marginal probabilities subset sample sizes figure estimated kullbackleibler divergences posterior predictive distributions model ground truth. edges appear true graph. inferences made uniform model less accurate particular relatively conﬁdent existence edges violate feed-forward structure true network. prior appropriate datasets expect uniform model beat block models cases true graph block-structured. ideally however learning algorithm ﬂexible enough cope many kinds data might hope performance block models degrade badly true graph match priors. explore setting generated data graphs sparse non-block-structured connectivity like shown figure graph sampled including edge probability rejected cyclic graphs graphs inout-degrees greater cpts sampled dirichlet distribution hyperparameter expected block model discovers block structure data remaining conﬁdent throughout nodes belong single class. surprise block model performed better uniform model making fewer mistaken predictions edges appear true structure matching true distribution closely uniform model. even though block model found class learned density connections winning uniform model sparse datasets. since ordered block model allows connections within classes cannot oﬀer advantage figure shows performance comparable uniform model. approach modeling abstract structure motivated part common-sense medical knowledge networks like qmr-dt. network proprietary created qmr-like network connectivity shown figure network classes corresponding diseases edges appear diseases symptoms. cpts network generated using noisy-or parameterization. results figure show block models recover classes given small examples particularly striking unordered model begins make accurate predictions class membership examples provided. time examples provided blockmodel makes accurate predictions edges true graph achieves predictive distribution superior uniform model. ticular expect genes belonging class regulator sometimes regulate other. test models simple setting structure generated data network shown figure cpts sampled dirichlet distribution hyperparameter samples block model conﬁdent correct class structure considering correct within-regulator edges although uncertain orientation edge. uniform model similar beliefs edge samples equally conﬁdent incorrect beliefs inﬂuences regulators. ordered blockmodel begins infer class diﬀerences earlier considering grouping various subsets regulators. already suggested knowledge medical conditions sometimes organized knowledge interactions three classes variables risk factors diseases symptoms. hepar network captures knowledge liver diseases structure close three-part template. structure hepar elicited medical experts cpts learned database medical cases. generated training data subset full network includes variables appear figure onisko edges nodes true model. network structure shown figure provided training examples unordered block model ﬁnds classes includes diseases second includes remaining variables. note missing disease pattern connections rather unusual unlike diseases single outgoing edge edge sent another disease rather symptom. model fails distinguish risk factors symptoms perhaps connectivity risk factors diseases sparse. although neither block model recovers three part structure described creators network discover suﬃcient structure allow match generating distribution better uniform model. paper explored formalisms representing abstract structural constraints bayes nets shown knowledge support structure learning sparse data. formalisms based nonparametric hierarchical bayesian models discover characterize graph regularities terms node classes. seen approach succeeds block structure present without incurring signiﬁcant cost not. also seen approach time learning bayes structure recover abstract classbased patterns characterizing aspect causal roles variables play. sions. first blockmodel prior without modiﬁcations acyclicity directly applicable discovering latent types undirected graphical models markov models time series dynamic bayesian networks continuous-time bayesian networks. second expect approach provide additional beneﬁts observational data incomplete prior knowledge likely roles becomes signiﬁcant. data could incorporated approximations marginal likelihood exploration richer pattern languages also useful. example notion nodes certain class connect nodes another type might appropriate domains. literature social networks e.g. provides many examples similar interesting relational patterns ripe probabilistic formalization. closer dependence parameterization class would also interesting explore example classes might project excitatory inhibitory edges. finally note transfer systems variables important function abstract knowledge could implemented adding another layer hierarchical model. particular could ﬂexibly share causal roles across entirely diﬀerent networks replacing chinese restaurant franchise central feature work attempts negotiate principled tradeoﬀ expressiveness space possible abstract patterns possibility learning abstract patterns themselves. nonparametric approach inspired striking capacity human learning also desideratum intelligent agent ability learn certain kinds simple natural structures quickly still able learn arbitrary arbitrarily complex structures given enough data. expect exploring tradeoﬀs detail promising area future research. thank goodman murphy koller helpful discussions gordon graphics assistance darpa calo project james mcdonnell foundation causal learning collaborative communication sciences laboratory ﬁnancial support.", "year": 2012}