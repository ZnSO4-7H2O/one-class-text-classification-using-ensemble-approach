{"title": "Neural network ensembles: Evaluation of aggregation algorithms", "tag": ["cs.AI", "cs.NE"], "abstract": "Ensembles of artificial neural networks show improved generalization capabilities that outperform those of single networks. However, for aggregation to be effective, the individual networks must be as accurate and diverse as possible. An important problem is, then, how to tune the aggregate members in order to have an optimal compromise between these two conflicting conditions. We present here an extensive evaluation of several algorithms for ensemble construction, including new proposals and comparing them with standard methods in the literature. We also discuss a potential problem with sequential aggregation algorithms: the non-frequent but damaging selection through their heuristics of particularly bad ensemble members. We introduce modified algorithms that cope with this problem by allowing individual weighting of aggregate members. Our algorithms and their weighted modifications are favorably tested against other methods in the literature, producing a sensible improvement in performance on most of the standard statistical databases used as benchmarks.", "text": "ensembles artiﬁcial neural networks show improved generalization capabilities outperform single networks. however aggregation eﬀective individual networks must accurate diverse possible. important problem then tune aggregate members order optimal compromise conﬂicting conditions. present extensive evaluation several algorithms ensemble construction including proposals comparing standard methods literature. also discuss potential problem sequential aggregation algorithms non-frequent damaging selection heuristics particularly ensemble members. introduce modiﬁed algorithms cope problem allowing individual weighting aggregate members. algorithms weighted modiﬁcations favorably tested methods literature producing sensible improvement performance standard statistical databases used benchmarks. regression classiﬁcation problems combining outputs several predictors improves performance single generic formal support property provided so-called bias/variance dilemma based suitable decomposition prediction error. according ideas good ensemble members must accurate diverse poses problem generating predictors reasonably good individual performances independently distributed predictions test points. diverse individual predictors obtained several ways. include using diﬀerent algorithms learn data changing internal structure given algorithm iii) learning diﬀerent adequately-chosen subsets data set. probability success strategy iii) frequently used directly tied instability learning algorithm method must sensitive small changes structure data and/or parameters deﬁning learning process. again classical examples sense classiﬁcation regression trees artiﬁcial neural networks particular case anns instability comes naturally inherent data training process randomness also intrinsic non-identiﬁability model. combination strong instability learning algorithm tradeoﬀ predictors’ diversity good individual generalization capabilities requires adequate selection ensemble members. attempts achieve good compromise mentioned properties include elaborations general techniques bagging boosting standard methods ensemble construction follow diﬀerent strategies bagging variants thereof train independent predictors bootstrap re-samples available data usually employing unused examples d−ln validation purposes. predictors aggregated according diﬀerent rules boosting variants stagewise procedures that starting predictor trained sequentially train aggregate members bootstrap re-samples drawn modiﬁed probabilities. according general approach example given diﬀerent chance appear training prioritizing patterns poorly learnt previous stages. predictions diﬀerent members generated weighted decreasing function error predictor makes training data. regression problems focus here boosting still construction area algorithm emerged ’the’ proper implementing technique consequently bagging common method aggregation. hand intermediate alternatives bagging boosting optimize directly ensemble generalization performance instead seeking best individual members much explored work compare diﬀerent strategies ensemble construction restricting work regression setting using anns learning method. restrictions essential; principle analysis extended classiﬁcation problems regression/classiﬁcation methods. furthermore discuss stepwise algorithms build best aggregate network training thus incorporating condition optimal ensemble performance. main purpose establish rules general possible build accurate regression aggregates. this organization work following section re-discuss several bagging-like methods proposed literature considering diﬀerent strategies selecting termination point training processes ensemble members. section also present algorithm optimal point view. section introduce synthetic real-world databases considered study describe experimental settings used learn them. section obtain empirical evidence relative eﬃcacy methods discussed section applying databases. then section present modiﬁed weighted version best algorithms test performances comparison results section also boosting techniques. finally section summarize work done main results obtained draw conclusions. simplest generating regressor aggregate bagging according method data containing examples generates bootstrap re-samples drawing replacement training patterns. thus training contain average diﬀerent examples repeated times remaining examples generally used validation purposes regressor learning phase generates diﬀerent members ensemble whose outputs test point ﬁnally averaged produce aggregate prediction weights usually taken equal options discussed next section. notice that according method regressors trained independently performances individually optimized using out-of-bag bagging viewed ﬁrst stage sequence increasingly sophisticated algorithms building composite regressor. understand this let’s consider ﬁrst situation common validation subset dataset kept unseen networks training phases. let’s also consider training convergence anns bootstrap re-samples obtained saving intermediate states training epoch model whose weights biases take values obtained epoch training process). building ensemble translated task selecting combination state runs create optimal ensemble ensemble smallest error light bagging solves problem choosing state using information given advanced algorithms regressors optimized individually part aggregate. anns simplest choosing optimal number training epochs networks optimizing ensemble performance internal parameters epoch thus instead validating ensemble members maximize individual performances bagging algorithm selects common optimal stopping point networks ensemble. practice ﬁnds general larger individual stopping points found bagging i.e. controlled degree single network overﬁtting improves aggregate’s performance. following refer algorithm epoch. increases. repeated many times considering diﬀerent networks annealing parameter conveniently increased step; algorithm runs settles deep local minimum. practice taken max/ maximum number training epochs random number interval annealing temperature decreased according .qe/ annealing step. point minimization problem simple enough depend critically choices. know algorithm —which call simann— previously discussed literature constitutes main contributions work. notice implementation well simplest implementation epoch forced store intermediate networks fn]. however given large storage capacity computers nowadays applications requirement severe. common situation scarcity data need keep independent validation serious drawback limits eﬃcacy methods discussed above. alternative approach resort out-of-bag patterns unseen network optimize respect number training epochs error otherwise. notice validation procedure generated amounts eﬀectively optimizing performances several subsets trained anns subset including average networks. advantage that like description bagging beginning section sub-utilization data validation purposes necessary. strategies aggregation discussed minimize particular error function global way. diﬀerent approach adapt typical hill-climbing search method problem. previous work proposed simple generating ensemble sequential aggregation individual predictors learning process ensemble member validated previous-stage aggregate prediction performance. early-stopping method applied monitoring generalization capability n-stage aggregate predictor plus network currently trained. retain simplicity independent network training validation process becomes slightly involved leading controlled overtraining individual networks. notice that despite stepwise characteristic algorithm implemented parallel training networks desirable. alternatively implemented sequentially avoids completely burden storing networks intermediate training times like algorithms described above. step generate training bootstrap re-sample dataset validation collecting instances included produce model training network minimum generalization error reached. step generate training validation sets respectively using procedure described step produce model training network generalization error aggregate predictor reaches minimum step parameters model kept constant model trained usual cost function algorithm individual networks directly trained latestopping method based current ensemble generalization performance. method seems reduce aggregate generalization error without paying much attention whether improvement related enhancing members’ diversity not. however actually ﬁnds diverse models reduce ensemble error looking every stage model anticorrelated current ensemble. notice seca also implemented using external validation case bootstrap complements replaced ﬁxed set. described methods constitute chain increasingly optimized algorithms ensemble building starting simplest bagging idea optimizing networks independently simann produce optimal ensemble let’s consider simple analysis computational cost involved implementation algorithms. anns independently trained networks saved along training evolution common algorithms bagging requires computational time select best combination epoch requires exactly computational eﬀort optimal stopping point networks. neuralbag uses instead evaluations best aggregate. finally seca simann require network evaluations respectively. written number simulated annealing steps arbitrary integer facilitate comparison. following take fair comparison neuralbag seca simann. notice however major demand computational point view training network selection build ensemble. practice algorithms’ evaluations section taken networks trained maximum epochs depending database. mentioned introduction completely diﬀerent strategy building composite regression/classiﬁcation machines boosting. classiﬁcation problems main diﬀerence bagging modiﬁed probabilities re-sample training sets stage weights associated examples larger examples poorly learnt previous stages eventually appear several times predictor trained specializes hard examples. finally inclusion ensemble suitably-chosen weight allows exponential decrease boosting rounds ensemble’s training error whole dataset notice that addition mentioned modiﬁcation re-sampling probabilities diﬀerences bagging boosting essentially stage-wise approach requires sequential training aggregate members ﬁnal ensemble members weighted according performances respective training sets consideration last characteristic done section discuss weighting scheme bagged regressors alternative simple average considered section. boosting explained above well deﬁned procedure classiﬁcation setting regression problems several ways implementing basic ideas. unfortunately none emerged proper boosting regressors. without intention exhausting proposed implementations distinguish boosting strategies solving regression problems forward stage-wise additive modelling modiﬁes target values eﬀectively residual errors reducing regression problem classiﬁcation essentially changing example weights emphasize poorly learnt previous stages ﬁtting process order compare bagging-like algorithms described above work implement boosting techniques examples diﬀerent strategies. sections show heuristic algorithms described section work real synthetic data. provide fairly extensive comparison already known methods test simann algorithm methods. next section brieﬂy describe databases experimental settings considered comparison. evaluated algorithms described previous section applying several benchmark databases synthetic friedman data sets chaotic ikeda real-world abalone boston housing ozone servo data sets. cases friedman data sets control noise level allows investigate inﬂuence diﬀerent algorithm’s performances. present results ikeda together real-world sets level noise problem ﬁxed intrinsic dynamics. addition next section present results mackey-glass equation allows general comparison regression methods literature previously applied problem following give brief descriptions databases architectures used. cases number hidden units selected trial error using validation looking minimum generalization error function network architecture chosen kept calculations. notice particularly important point since want compare eﬃcacy diﬀerent aggregation methods trained networks. gaussian noise uniformly distributed interval notice enter deﬁnition included check prediction method’s ability ignore inputs. order explore algorithm’s performances diﬀerent situations considered diﬀerent noise levels training lengths. gaussian noise component alternatively normal distribution normal distribution generated sample vectors noise level data sets randomly split training test sets. training sets alternatively patterns test contained always examples. considered anns architectures number hidden units increasing number patterns training set. abalone determined cutting shell cone staining counting number rings microscope. avoid boring task measurements easier obtain used predict age. considered data downloaded machine learning repository containing attributes examples without missing values. these patterns used testing training anns used learn architecture. data consists training vectors input variables target output. inputs mainly socioeconomic information census tracts greater boston area output median housing price tract. data also downloaded machine learning repository. ozone data correspond meteorological information related maximum daily ozone location angeles area. removing missing values left training vectors containing inputs target output one. data downloaded department statistics university california berkeley. databases trained independent networks storing intermediate weights biases long training experiments convergence considered number networks checking preliminary evaluations sensible performance improvements bigger ensembles. anns implemented diﬀerent bagging-like ensemble construction algorithms changing training stopping points individual networks according criteria discussed previous section. following diﬀerent validation scenarios keeping external validation randomly selected data training anns diﬀerent bootstrap re-samples d−v. considered partitions case ﬁrst number indicates fraction data points case bagging-like algorithms discussed previous section considered. validating training process directly out-of-bag data explained previous section. procedure makes full available data general produce better results previous situation. case tested bagging-like techniques also boosted anns according friedman drucker algorithms considering maximum boosting rounds comparison. results given following section correspond average independent runs above-described procedures without discarding anomalous case indicate variance average errors since deviations characterize dispersion performances diﬀerent realizations training test sets. direct relevance comparing average performances diﬀerent methods procedure guarantees diﬀerences ﬁnal ensemble performances aggregation methods and/or validation settings. order compare results downloaded database used authors considered like works embedding dimension patterns training patterns testing purposes. problem took deﬁned mean-squared error test divided variance total data according deﬁnition nmse constant predictor equal data average perfect one. then value allows appraise predictor’s performance relative complexity diﬀerent regression tasks. notice that indicated table captions results given units errors much smaller consequently predictions much better trivial data average. tables present results synthetic real databases respectively situation external validation containing data used. tables correspond case validation data. mentioned previous section bagging-like algorithms compared. tables present corresponding results bagging-like algorithms out-of-bag validation first external validation experiments indicate epoch performs better bagging cases corresponding synthetic databases none cases real-world databases depending validation size. poor performance becomes slightly better out-of-bag validation consequently advantage using algorithm instead bagging. something similar happens neuralbag which spite good results presented experiments improves bagging roughly half cases. contrary seca simann clearly better bagging average methods outperform bagging approximately friedman problems case real databases independently validation used. considering methods together table shows that learning problems associated friedman synthetic data cases best method either seca network selection simulated annealing pattern conﬁrmed results table seca simann best performers cases. real-world databases methods outperform bagging-like algorithms cases particularly good performance simann. out-of-bag validation table shows that consistently previous results experimental situations seca simmann best performers. stress however good performance epoch friedman data particularly noise-free data. real-world databases table shows seca simann produced best results regression problems investigated. results obey expected behaviors noise level data length. furthermore synthetic friedman problems general test error larger data held validation although case real-world abalone boston ozone datasets. moreover bagged regressors independently method used ensemble them cases out-of-bag validation eﬃcient keeping external agreement works literature. notice however last observation valid noisy friedman servo problems single ann. case out-of-bag validation performed paired t-test check whether seca simann signiﬁcantly outperform bagging. following procedure considered binary variable assumes value seca/simann better bagging otherwise. average variable diﬀers diﬀerence statistically signiﬁcant aﬃrm methods better other. results t-test given tables friedman seca simann better bagging cases; friedman clear differences methods friedman seca simann signiﬁcantly better bagging except high noise patterns training set. real-world databases seca simann always better bagging statistical signiﬁcance several cases. results complete agreement nmse comparison tables order gain insight seca simann’s behaviors might explain good performances shown tables investigated standard bias-variance decomposition generalization error. consider general regression problems vectors predictor variables obtained distribution regression targets generated according true regression function random noise zero mean. estimate learning data obtain model generalization error test point averaged possible realizations decomposed point view single estimator interpret equation saying good method biased little variance possible diﬀerent realizations. learning methods ﬁrst condition reasonably well second satisﬁed since small changes even changes diﬀerent learning experiments lead distinct predictors estimator generalization error reduced produce fairly accurate models output diverse predictions test point course trade-oﬀ conditions ﬁnding good compromise regressors’ mean accuracy diversity seems particularly feasible largely unstable methods like anns. results shown table estimated separately accuracy diversity components error according table present results obtained; easier comparison give normalized mean accuracy diversity bagging ensemble members. expected bagging produces accurate less diverse predictors; instead aggregation methods resign accuracy gain diversity. interestingly that despite similar performance seca simann table aggregation methods select diﬀerent ensemble members. particular simann seeks mainly diverse predictors although accurate seca introduces diversity balanced accuracy. epoch strategy general intermediate methods eﬀective enough outperform them. previous section evaluated several ensemble construction algorithms essentially diﬀer select particular stopping points independently-trained anns. ﬁnal aggregate prediction test point simply mean individual predictions without weighting outputs ensemble members particularly wise seca since members poor generalization capabilities. seca stepwise optimization technique known problem heuristics optimization process cannot review choices made past. figure shows typical example problem given realization friedman data set. open circles represent evolution training test errors construction ensemble using seca. example fourth added network clearly fig. evolution training test errors ensemble construction arbitrary units. open circles correspond seca; dots indicate evolution ensemble members weighted according w-seca. previous work explored possible cope problem using slightly diﬀerent seca algorithm accepts networks improve ensemble performance. unfortunately results showed algorithm also produces overﬁtting unable clearly outperform bagging small noisy data sets. possible intermediate solution weighting ensemble members instead rejecting improve overall ensemble performance. allows reduce inﬂuence choices made past simply giving smaller weights troublesome networks. then following general ideas boosting propose modify algorithm output ensemble m-th stage becomes decreasing function n-th member i.e. weight ensemble member according individual performance whole dataset. boosting reduces importance overﬁtted members ﬁnal ensemble. practice explored figure shows results obtained seca friedman weighting schemes. small intermediate values weighting produces better results simply averaging individual predictions. large values overﬁtting observed since particular networks eﬀectively contribute ensemble. expected acute exponential weighting major diﬀerences laws. hand smaller noise larger take overﬁtting observed. also considered friedman data sets behavior figure representative general trend. figure included results weighting seca using power case discussed above. problematic fourth network given small weight practically ignored ensemble. performed t-test described establish whether performance obtained w-seca signiﬁcantly better seca. fair evaluation algorithm described used anns considered previous section. tables show corresponding results indicate w-seca outperforms seca statistical also applied weighting scheme bagging simann investigate eﬀective elimination ensemble members also impact algorithms. question answer improvement wash diﬀerences observed tables diﬀerent algorithms? order fair evaluation algorithms used anns considered previous section. results obtained collected tables tables show regression problems corresponding friedman data sets w-seca w-simann outperform bagging cases respectively. moreover real-world databases ikeda w-simann always better w-bagging w-seca looses servo database w-bagging. although weighting general beneﬁcial algorithms member selection strategy still important obtain good performances. also supported results paired t-tests w-seca w-simann w-bagging also interest mention weighted algorithms outperform nonweighted ones cases investigated remaining cases correspond high noisescarce data situations. notice also cases best performers seca simann. finally stress problems considered w-bagging performs better bagging cases w-simann performs better simann cases w-seca performs better seca cases. remark important improvements seca expected according discussion connection figure tables also present results obtained boosting algorithms proposed comparison. algorithms used maximum boosting rounds produce fair test considering anns ensembled bagging-like methods. notice friedman datasets boosting algorithms perform better w-seca w-simann three cases cases d-boosting implementation always best performer. real-world databases ikeda implementation w-simann performers. ﬁnal investigation w-seca w-simann considered mackey-glass problem. allows make comparison seven regression methods based support vector machines regularized boosting using radial basis function networks described following works introduced three levels uniform noise training signal-to-noise ratios respectively gaussian noise signal-to-noise ratios respectively. test kept noiseless measure true prediction error. mentioned section fair comparison experimental settings table presents corresponding results show wseca w-simann among performers cases. stress perform worse methods largest gaussian noise case performed thorough evaluation simple methods construction neural network ensembles. particular considered algorithms implemented independent training ensemble members introduced framework suggests naturally simann algorithm optimal one. taking ensemble prediction simple average outputs shown seca simann best performers large majority cases. include synthetic data diﬀerent noise levels training sizes also real-world databases. also shown methods resolve diﬀerently compromise accuracy diversity particular search strategies. greedy method termed seca seeks every stage member least partially anticorrelated previous-stage ensemble estimator. achieved applying late-stopping method learning phase individual networks leading controlled level overtraining ensemble members. principle algorithm retains simplicity independent network training although necessary avoid computational burden saving intermediate networks phase since implemented sequential way. implementation method stepwise construction ensemble network selected time parameters saved. showed comparison several algorithms literature strategy eﬀective exempliﬁed results tables simann algorithm ﬁrst proposed work uses simulated annealing minimize error unseen data respect number training epochs individual ensemble member. method also eﬀective competitive seca databases. furthermore implementation minimization step anns training process practice time consuming computational point also discussed known problem stepwise selection procedures like seca proposed modiﬁcation algorithm overcome modiﬁed algorithm called w-seca weights predictions ensemble members depending individual performances. showed improves results obtained seca practically cases. moreover since weighting general beneﬁcial methods considered investigated whether procedure overrides diﬀerences ensemble construction algorithms. found weighted versions seca simann best performers indicating intrinsic eﬃciency construction methods. finally also performed comparison w-seca w-simann several regression methods including methods based svms regularized boosting. used published results literature corresponding mackey-glass equation. case found algorithms proposed among performers almost situations considered given competitive behavior weighted bagging-like algorithms tempted speculate that regression success boosting ideas might mainly related modiﬁcation resampling probabilities ﬁnal error weighting ensemble members. want comment performance improvement obtained aggregation algorithms discussed work. found general seca simann either weighted non-weighted versions produce better results algorithms literature although holds true several cases statistical signiﬁcance performance improvement obtained depends largely problem considered. instance respect bagging common algorithm ﬁnds following friedman databases improvement high noise large noise-free cases. databases ﬁxed noise level improvement ranges less nearly answer question whether performances justify algorithms proposed instead bagging would depend then concrete application particularly critical however even non-critical ones always chance using w-seca w-simann might obtain fairly large improvements. case best justiﬁcation perhaps fact much additional computational time required implement algorithms. suitable subset trained nets build ensemble. this train number anns optimal validation point like bagging assign networks importance weight strategy. finally networks weights larger given threshold kept ensemble. algorithm –that termed gasen– predictions retained anns combined simple average leads good generalization capabilities compared bagging boosting. strategy readily implemented within simann algorithm simply allowing appropriate random change number aggregated anns addition stochastic search optimal training epochs. notice procedure would extend gasen optimization anns trained arbitrary number epochs might important since degree single-network overtraining known improve ensemble performance. combined approach would presumably bring best simann gasen single algorithm however beyond scope work.", "year": 2005}