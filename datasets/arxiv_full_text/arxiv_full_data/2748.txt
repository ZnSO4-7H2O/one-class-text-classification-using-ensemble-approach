{"title": "Deep Neural Network Approximation using Tensor Sketching", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Deep neural networks are powerful learning models that achieve state-of-the-art performance on many computer vision, speech, and language processing tasks. In this paper, we study a fundamental question that arises when designing deep network architectures: Given a target network architecture can we design a smaller network architecture that approximates the operation of the target network? The question is, in part, motivated by the challenge of parameter reduction (compression) in modern deep neural networks, as the ever increasing storage and memory requirements of these networks pose a problem in resource constrained environments.  In this work, we focus on deep convolutional neural network architectures, and propose a novel randomized tensor sketching technique that we utilize to develop a unified framework for approximating the operation of both the convolutional and fully connected layers. By applying the sketching technique along different tensor dimensions, we design changes to the convolutional and fully connected layers that substantially reduce the number of effective parameters in a network. We show that the resulting smaller network can be trained directly, and has a classification accuracy that is comparable to the original network.", "text": "deep neural networks powerful learning models achieve state-of-the-art performance many computer vision speech language processing tasks. paper study fundamental question arises designing deep network architectures given target network architecture design smaller network architecture approximates operation target network? question part motivated challenge parameter reduction modern deep neural networks ever increasing storage memory requirements networks pose problem resource constrained environments. work focus deep convolutional neural network architectures propose novel randomized tensor sketching technique utilize develop uniﬁed framework approximating operation convolutional fully connected layers. applying sketching technique along different tensor dimensions design changes convolutional fully connected layers substantially reduce number effective parameters network. show resulting smaller network trained directly classiﬁcation accuracy comparable original network. deep neural networks become ubiquitous machine learning applications ranging computer vision speech recognition natural language processing. recent successes convolutional neural networks computer vision applications have part enabled recent advances scaling networks leading networks millions parameters. networks keep growing number parameters reducing storage computational costs become critical meeting requirements practical applications. possible train deploy deep convolutional neural networks modern clusters storage memory bandwidth computational requirements make prohibitive embedded mobile applications. hand computer vision applications growing importance mobile platforms. dilemma gives rise following natural question given target network architecture possible design smaller network architecture approximates original network architecture operations inputs? paper present approach answering network approximation question using idea tensor sketching. network approximation powerful construct allows replace original network smaller training subsequent deployment completely eliminates need ever realizing original network even initial training phase highly desirable property working storage computation constrained environments. approximating network using smaller network computationally hard problem paper study problem network approximation convolutional neural networks. approximate convolutional neural network focus parametrized layers consider layer ∗amazon work done author samsung research america mountain view usa. kasiviswgmail.com. †equal contributions. ‡vmware research palo alto usa. n.narodytskagmail.com. §samsung research america mountain view usa. hongxia.jinsamsung.com clarity distinguish terms network model paper network refers network architecture describes transformation applied input whereas model refers trained network ﬁxed parameters obtained training network training set. function words unbiased estimator additionally establish theoretical bounds variance estimator. ideally want representation length much smaller construction introduce novel randomized tensor sketching idea. rough idea create multiple sketches tensor space performing random linear projections along different dimensions perform simple combination sketches. operation deﬁnes layer approximates functionality layer since input output dimensionality replace layer network layer. convolutional fully connected layers maintaining rest architecture leads smaller networknn approximates next issue efﬁciently train smaller networknn? show that changes running time needed various operations smaller network. allows trainnn directly reduced modelnnd. extensive experimental evaluations different datasets architectures corroborate standard training procedure parameters constructed smaller network learnt space efﬁciently training set. also compared original network also slight improvement excellent performance approach showing increases limits achievable parameter number reduction almost preserving original model accuracy compared several existing approximation techniques. fact technique succeeds generating smaller networks provide good accuracy even large datasets places state-of-the-art network approximation techniques seem succeed preliminaries denote vectors column-wise fashion denoted boldface letters. vector denotes transpose euclidean norm. matrix denotes froebnius norm. random matrices create sketches matrices/tensors involved fully connected/convolutional layers. paper simplicity random scaled sign matrices. note families distributions subsampled randomized hadamard transforms probably lead additional computational efﬁciency gains used sketching. deﬁnition rk×d random sign matrix independent entries probability deﬁne random scaled sign matrix here parameter adjustable algorithm. generally assume note identity matrix. also linearity expectation matrix columns tensor preliminaries. denote matrices uppercase letters higher dimensional tensors euler script letters e.g. real order tensor member tensor product euclidean spaces case vectors matrices identify order tensor p-way array real numbers. different dimensions tensor referred modes. entry tensor denoted tii...ip. mode-n matrix product tensor rd×···×dp matrix rk×dn denoted dimensions elementwise note operation also applied simultaneously multiple modes. general given matrices rki×di resulting tensor tensor rk×k···×kp. matrix rd×d matrix follows that ﬁber obtained ﬁxing indices tensor. ﬂattening tensor along mode matrix whose columns correspond mode-n ﬁbers example fourth order tensor rd×d×d×d rddd×d matrix deﬁned t+dd)i tiiii i.e. entry tensor assigned location matrix weights ﬁlters convolutional layer denoted -dimensional tensor rd×w×h×d represent number output input feature maps represent height width ﬁlter kernels. network approximation based idea tensor sketching. data sketching ideas successfully used designing many machine-learning algorithms especially setting streaming data e.g. generally sketching used construct compact representation data certain properties data preserved. usage sketching however slightly different instead sketching input data apply sketching parameters function. also want design sketching techniques work uniformly matrices higher order tensors. this deﬁne tensor sketch operation deﬁned follows. deﬁnition given tensor sign matrix rk×dn deﬁned tensor since generally pick space needed storing sketch factor dn/k smaller storing case matrices sketches created prepost-multiplying matrix random scaled sign matrices appropriate dimensions. example given matrix rd×d construct mode- sketch given sketch matrix another matrix rd×d natural estimator matrix product easy estimators unbiased. second part following proposition analyzes variance estimators. result motivate construction sketch-based convolutional fully connected layers next section. proposition rd×d. rk×d rk×d independent random scaled sign matrices. notice variance terms decrease variance bound also plugged chebyshev’s inequality probability bound. also variance bounds quantitatively different based whether sketch used. particular depending variance bounds could substantially smaller e.g. columns null space zero matrix bound gives tight zero variance not. describe idea approximating network using tensor sketching. approach almost identical fashion used reduce number parameters involved convolutional fully connected layers without signiﬁcantly affecting resulting accuracy. sketching convolutional layers typical convolutional layer transforms -dimensional input tensor rh×w×d output tensor iout rh×w×d convolving kernel tensor rd×h×w×d depends possibly parameters stride spatial extent zero padding denote convolution operation iout exact deﬁnition convolution operator depends mentioned additional parameters important rely fact convolution operation realized using matrix multiplication explain below. also convolutional layer could optionally followed application non-linear activation function generally parameter free affect construction. tensor sketch operation reduce either dimensionality input feature output feature kernel tensor practice dimensions individual ﬁlters small integers therefore reduce. motivation sketching along different dimensions comes mathematical analysis variance bounds proposition based relationship variance could substantially smaller case other. another trick works simple boosting technique utilize multiple sketches associated independent random matrix. formally deﬁne sk-conv layer follows deﬁnition sk-conv layer parametrized sequence tensor-matrix pairs rd×h×w×k rk×h×w×d rk×d rkhw×dhw independent random scaled sign matrices input rh×w×d constructs iout follows running multiple sketches parallel input taking average also results stable performance across different choices random matrices number free parameters overall tensors together equals hwk. therefore sk-conv layer reduction number parameters compared traditional convolutional layer dd/. reduction time computing iout ignoring dependence reduces convolution operation reduced matrix multiplication idea exploited many deep learning frameworks idea reformulate kernel tensor ﬂattening along dimension representing output feature setting represented along fourth dimension input tensor used form matrix rhw×dhw. construction quite standard refer reader details. follows iout deﬁned iinmat rhw×d reshaping output tensor iout theoretical guarantees sk-conv layer. given traditional convolutional layer kernel tensor independent random scaled sign matrices form corresponding sk-conv layer constructing tensors matu following theorem based proposition analyzes expectation variance using sketches estimator iin∗k since random matrices independent other drop subscript perform analysis single instantiation sketches. theorem rd×h×w×d kernel tensor mat. rk×d rkhw×dhw independent random scaled sign matrices. tensors input matrix rhw×dhw unbiased estimation iink section discuss procedure training sk-conv layer. loss denote loss function network. computational space efﬁciency goal perform training without ever needing construct complete kernel tensor focus deriving gradient loss respect parameters sk-conv layer used back-propagating gradient information. neurons fully connected layer full connections activations previous layer. layers apply linear transformation input. rd×d represent weight matrix represent bias vector. operation layer input described typically layer followed application non-linear activation function. case convolutional layers construction independent applied activation function omit discussion functions. matrix. deﬁnition sk-fc layer parametrized bias vector sequence matrix pairs rk×d rd×k rk×d rk×d independent random scaled sign matrices input performs following operation another advantage time needed computing pre-activation value sk-fc layer smaller time needed traditional setting values satisfy condition. theoretical guarantees sk-fc layer. given traditional layer weight matrix independent random scaled sign matrices form corresponding sk-fc layer setting analyze certain properties construction. following theorem based proposition analyzes expectation variance using sketches estimator vector since random matrices independent other drop subscript perform analysis single instantiation sketches. theorem rd×d. rk×d rk×d independent random scaled sign matrices. unbiased estimation variance bound deep neural networks typically over-parametrized signiﬁcant redundancy deep learning networks several previous attempts reduce complexity deep neural networks variety contexts. approximating fully connected layers. techniques focused approximating fully connected layers reduced form. yang fastfood transformation technique approximate fully connected layers. hashednets architecture proposed chen uses hash function enforce parameter sharing random groups parameters fully connected layer reduce number effective parameters. cheng achieve parameter reduction imposing circulant matrix structure fully connected layers. sindhwani generalize construction proposing broad family structured parameter matrix structure showing effectiveness fully connected layers. choromanska provide theoretical justiﬁcations using structured hashed projections layers. techniques highly effective fully connected layers fall short achieving signiﬁcant reduction number parameters modern cnns dominated convolutional layers therefore effective technique parameter reduction cnns also convolutional layers. approximating convolutional fully connected layers. relevant paper line work approximating fully connected convolutional layers. denil suggested approach based learning low-rank factorization matrices involved within layer cnn. instead learning factors factorization training authors suggest techniques carefully constructing factors learning one. sketching-based approach related low-rank factorization however using sketching eliminate overhead carefully constructing dictionary. achieve parameter reduction using tensor decomposition technique based replacing convolutional kernel consecutive kernels lower rank. issue approach increased depth resulting network training becomes challenging authors rely batch normalization overcome issue. proposed approach depth reduced network remains equal original network reduced network trained without batch normalization. recently garipov building upon work used tensor factorization technique called tensor train decomposition uniformly approximate fully connected convolutional layers. however constructing exact tensor factorization general challenging np-hard problem whereas approach relies simple linear transformations. chen combine hashing idea along discrete cosine transform compress ﬁlters convolutional layer. architecture called freshnets ﬁrst converts ﬁlter weights frequency domain using discrete cosine transform uses hashing idea randomly group resulting frequency parameters buckets. sketches created using random projections related hashing trick used results however techniques naturally attractive convolutional neural networks known preserve spatial locality property preserved simple hashing. also contrast freshnets architectures require simple linear transformations fully connected convolutional layers require special routines inverse etc. additionally provide theoretical bounds quality approximation missing previous studies. related work. long line work reducing model memory size based post-processing trained network techniques pruning binarization quantization low-rank decomposition etc. intermingled training network dataset construct reduced model. results achieve direct network approximation training happens original network. practice combine approach proposed model post-processing techniques reduce storage requirements trained model hinton proposed approaches learn distilled model training compact neural network reproduce output larger network. general idea train large network original training labels learn much smaller distilled model weighted combination original labels softmax output larger model. note network approximation approach need train original large network. also unlike distillation-based approaches separate distilled model formed dataset approach produces single reduced network trained dataset. techniques proposed parameter reduction include inducing zeros parameter matrices sparsity regularizers storing weights ﬁxed-precision formats ideas readily incorporated approach potentially yielding reductions model memory size. daniely generate sketches input show lead compact neural networks. approach based sketching parameters deep network complementary idea approaches used conjunction. line work parameter reduction rather decreasing evaluation time testing. results resulting storage reduction comes side effect. techniques speeding convolutional neural networks include winograd fft-based convolutions again unlike here parameter reduction focus results. section experimentally demonstrate effectiveness proposed network approximation approach. goal experiments test limits reduction possible deep neural networks rather demonstrate tensor sketching approach possible design substantially smaller network achieves almost performance original network wide-range datasets. used torch machine learning framework experiments performed cluster gpus using single run. additional experimental results presented appendix metrics. deﬁne compression rate ratio number parameters reduced network architecture number parameters original network architecture. compression rate indicates compression smaller values indicating higher compression. top- error trained model test captures percentage images test misclassiﬁed model. stable picture model performance errtop- computed averaging test error last training epochs. datasets. popular image datasets cifar svhn imagenet objects recognition dataset images subset imagenet dataset created places note that places challenging dataset used recent ilsvrc scene classiﬁcation challenge. network architectures. experiments four different network architectures. choice architectures done keeping mind limited computational resources disposal recent trend moving away fully connected layers cnns. common observation area reducing number parameters convolutional layers seems much challenging problem fully connected layers. ﬁrst network architecture experiment popular network-in-network minor adjustments corresponding image sizes network-in-network moderately sized network attains good performance medium sized datasets e.g. cifar network employ batch normalization dropout uniform experiments across different techniques. second network consider ninn change last convolution layer replaced fully connected layer following third network experiment simple shallow network refer testnet convolution layers fully connected layers allows easily test efﬁcacy approximation technique layer individually. describe construction testnet detail appendix table shows original top- error ninn ninn+fc. number parameters ninn ninn+fc datasets. statistics testnet presented figure ﬁnal network consider googlenet batch normalization places dataset. network top- error places dataset. compare proposed approach four state-of-the-art techniques approximate convolutional fully connected layers freshnets technique uses hashing frequency domain approximate convolutional layer low-rank decomposition technique tensor decomposition technique using freshnets technique also hashednets technique feature hashing compressing fully connected layers suggested used open-source implementations techniques hashednets freshnets lowrank lowrank required parameters ensure compared approaches achieve compression rate. figure top- error ninn+fc architecture. size layer half total size convolutional layers conv conv. compress fully connected layer factor similar experimental setup figure reducing number parameters convolutional layers factor x-axis scale. compression convolutional layers. performed experiments evaluate performance scheme convolutional layers. used ninn architecture purpose. ninn essentially sequence nine convolution layers compress layers starting conv ﬁnishing conv reducing number parameters layer factor convolution layers compressed achieved network compression rate approximately equal figures shows results experiments. point missing plots corresponding network training failed. expect error decrease compression rate i.e. increase parameter reduction. observe general trend almost plots minor ﬂuctuations figure svhn dataset. make main observations plots. first method always able better compression rate compared techniques comparative techniques started failing sooner kept decreasing compression rate. example approach consistently achieves compression rate none techniques even close achieving. second approach also almost always achieves better accuracy compared techniques. explained section approach advantages compared techniques especially terms ability approximate convolutional layers. effects become pronounced decrease compression rate. cases gain lose accuracy compared original network accuracy. fact sometimes reduced network able gain accuracy original network suggests randomized technique probably also adds regularization effect training. compression convolutional fully connected layers. fully connected layers mix. used modiﬁed ninn architecture experiments replaced last convolution layer fully connected layer size followed classiﬁer layer size figure present results experiments. approach outperforms techniques terms accuracy maximum achievable compression rate. results demonstrate effectiveness proposed approach convolutional fully connected layers. places dataset. evaluate approach large dataset additional experiments places dataset used googlenet architecture batch normalization. limited computational resources single experiment compressed ﬁrst layer achieve compression rate compression level training none competitor methods succeeded whereas approach gave top- error note top- error original googlenet dataset demonstrates approach manages generate smaller networks perform well even large datasets. here cases model storage sizes reduced taking reduced model using certain post-processing operations detailed section outside scope evaluation. parameter sensitivity. appendix present experiments highlight role parameters proposed approach. general observe accuracy compressed models improve increase also averaging effect increasing decreases variance top- error respect randomization arises random matrices. computational efﬁciency. primary focus network approximation added bonus networks generated tensor sketching approach also computationally efﬁcient. example compression rate wall-clock testing time reduced ninn average smaller compared original network across tested datasets. since sketch tensors construction dense efﬁciency gains possible better exploiting dense matrix capabilities modern gpus.", "year": 2017}