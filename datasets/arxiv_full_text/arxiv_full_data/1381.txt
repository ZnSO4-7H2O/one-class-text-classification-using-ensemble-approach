{"title": "Swapout: Learning an ensemble of deep architectures", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "We describe Swapout, a new stochastic training method, that outperforms ResNets of identical network structure yielding impressive results on CIFAR-10 and CIFAR-100. Swapout samples from a rich set of architectures including dropout, stochastic depth and residual architectures as special cases. When viewed as a regularization method swapout not only inhibits co-adaptation of units in a layer, similar to dropout, but also across network layers. We conjecture that swapout achieves strong regularization by implicitly tying the parameters across layers. When viewed as an ensemble training method, it samples a much richer set of architectures than existing methods such as dropout or stochastic depth. We propose a parameterization that reveals connections to exiting architectures and suggests a much richer set of architectures to be explored. We show that our formulation suggests an efficient training method and validate our conclusions on CIFAR-10 and CIFAR-100 matching state of the art accuracy. Remarkably, our 32 layer wider model performs similar to a 1001 layer ResNet model.", "text": "describe swapout stochastic training method outperforms resnets identical network structure yielding impressive results cifar- cifar. swapout samples rich architectures including dropout stochastic depth residual architectures special cases. viewed regularization method swapout inhibits co-adaptation units layer similar dropout also across network layers. conjecture swapout achieves strong regularization implicitly tying parameters across layers. viewed ensemble training method samples much richer architectures existing methods dropout stochastic depth. propose parameterization reveals connections exiting architectures suggests much richer architectures explored. show formulation suggests efﬁcient training method validate conclusions cifar- cifar- matching state accuracy. remarkably layer wider model performs similar layer resnet model. paper describes swapout stochastic training method general deep networks. swapout generalization dropout stochastic depth methods. dropout zeros output individual units random training stochastic depth skips entire layers random training. comparison general swapout network produces value output unit independently reporting randomly selected subset current previous layer outputs unit. result units layer like normal feedforward units others produce skip connections others produce several earlier outputs. effect method averages large architectures includes architectures used dropout used stochastic depth. experimental work focuses version swapout natural generalization residual network show results improvements accuracy residual networks number layers. improvements accuracy often sought increasing depth leading serious practical difﬁculties. number parameters rises sharply although recent works addressed reducing ﬁlter size another issue resulting increased depth difﬁculty training longer chains dependent variables. difﬁculties addressed architectural innovations introduce shorter paths input loss either directly additional losses applied intermediate layers time writing deepest networks successfully trained residual networks show increasing depth swapout networks increases accuracy. compelling experimental evidence large depths helpful though architectural innovations introduced make networks trainable reduce capacity layers. theoretical evidence depth required practical problems thin. bengio dellaleau argue circuit efﬁciency constraints suggest increasing depth important functions require exponentially large shallow networks compute less experimental interest displayed width networks show increasing width swapout networks leads signiﬁcant improvements accuracy; appropriately wide swapout network competitive deep residual network orders magnitude deeper parameters. contributions swapout novel stochastic training scheme sample rich architectures including dropout stochastic depth residual architectures special cases. swapout improves performance residual networks model depth. wider much shallower swapout networks competitive deep residual networks. convolutional neural networks long history intensively studied result recent successes increasing number layers network improves performance network trained. variety signiﬁcant architectural innovations improve trainability including relu batch normalization allowing signals skip layers. method exploits skipping process. highway networks gated skip connections allow information gradients pass unimpeded across several layers residual networks identity skip connections improve training extremely deep residual networks trained perform well contrast architectures method skips unit level randomly. method employs randomness training time. review history random methods introduction shows entirely randomly chosen features produce generalizes well. randomly dropping unit values discourages coadaptation units. randomly skipping layers training reliably leads improvements test time likely regularizes network. precise details regularization remain uncertain appears stochastic depth represents form tying layers; layer dropped layers encouraged able replace method seen training network averages family architectures inference. dropout averages architectures missing units stochastic depth averages architectures missing layers. successful recent randomized methods include dropconnect generalizes dropout dropping individual connections instead units stochastic pooling contrast method skips layers randomly unit level enjoying beneﬁts method. recent results show stochastic gradient descent sufﬁciently steps stable dropout enhances property reducing value lipschitz constant lemma show method enjoys behavior dropout framework. like dropout network trained swapout depends random variables. reasonable strategy test time network evaluate multiple instances average. reliable improvements accuracy achievable training distinct models averaging predictions thereby forming explicit ensemble. contrast instances network average would draw parameters srivastava argue that test time random values dropout network replaced expectations rather taking average multiple instances considerations include runtime test; number samples required; variance; experimental accuracy results. model accurate values expectations available. section show swapout networks estimates expectations outperform strong comparable baselines turn outperformed swapout networks implicit ensemble. figure visualization architectural differences showing computations block using various architectures. circle unit grid corresponding spatial layout circles colored indicate report. given input units feed forward block emit units residual network block emit skipforward network randomly chooses reporting unit finally swapout randomly chooses reporting wise product boldface represent tensors respectively. network block simple layers speciﬁc conﬁguration e.g. convolution followed relu residual network block several potentially different blocks connected form directed acyclic graph form full network model. dropout kills individual units randomly; stochastic depth skips entire blocks units randomly. swapout allows individual units dropped skip blocks randomly. implementing swapout straightforward generalization dropout. input network block computes u’th unit produces output. tensor i.i.d. bernoulli random variables. dropout computes output block natural think dropout randomly selecting output u’th unit. swapout generalizes dropout expanding choice write {θi} distinct tensors bernoulli random variables indexed corresponding parameters {θi}. {fi} corresponding tensors consisting values already computed somewhere network. note however restricted function drop indicate this. natural choices outputs earlier layers. swapout computes output layer question computing since swapout network clearly imitate residual network since residual networks currently best-performing networks various standard benchmarks perform exhaustive experimental comparisons them. accepts view dropout stochastic depth averaging architectures swapout extends architectures used. appropriate random choices yield architectures covered dropout; architectures covered stochastic depth; block level skip connections. choices yield unit level skip residual connections. swapout retains important properties dropout. swapout discourages co-adaptation dropping units also occasion presenting units inputs come earlier layers. dropout shown enhance stability stochastic gradient descent lemma applies swapout general form too. extend notation paper write lipschitz constant applies network gradient network parameters gradient dropped version network. crucial point relevant enabling lemma e||] e||] write gradient swapout network gradient swapout network achieves largest lipschitz constant choice first lipschitz constant applies network; second swapout makes stability worse; third speculate light conditions provide improving model trained swapout represents entire family networks tied parameters members family sampled randomly training. options inference. could either replace random variables expected values recommended srivastava alternatively could sample several members family random average predictions important difference swapout dropout. dropout network estimate expectations exactly random variables thus non-negative). swapout network usually estimate expectations exactly. problem relu]] general. estimates expectations ignore successful experiments show stochastic inference gives signiﬁcantly better results. srivastava argue deterministic inference signiﬁcantly less expensive computation. believe srivastava overestimated many samples required accurate average distinct dropout networks average experience stochastic inference swapout positive number samples needed good behavior small furthermore computational costs inference smaller instance network uses parameters technically delicate point dropout swapout networks interact poorly batch normalization uses deterministic inference. problem estimates collected batch normalization training reﬂect test time statistics. consider random variables bernoulli. shown equality holding thus variance estimates collected batch normalization training represent statistics observed testing expected values used deterministic inference scheme. errors scale estimation accumulate layers stacked. explain reports dropout doesn’t lead improvement used residual networks batch normalization. skipforward equation introduces stochastic parameters also explore compare simpler architecture skipforward introduces parameter samples smaller below. experiment extensively cifar- dataset demonstrate model trained swapout outperforms comparable resnet model. further layer wider model matches performance layer resnet cifar- cifar- datasets. model experiment resnet architectures described however implementation following modiﬁcations improve performance original model blocks different feature sizes subsample using average pooling instead strided convolutions projection shortcuts learned parameters. ﬁnal prediction follow scheme similar network network replace average pooling fully connected layer convolution layer followed global average pooling predict logits softmax. layers resnets arranged three groups convolutional layers group containing equal number ﬁlters. represent number ﬁlters group tuple smallest size cifar-). refer width experiment various multiples base size represented etc. training train using batch size momentum weight decay unless otherwise speciﬁed train models total epochs. starting initial learning rate drop factor epochs epochs. standard augmentation left-right ﬂips random translations four pixels. translation images pixels sides sample random crop. images mini-batch crop. note dropout slows convergence swapout similar reasons. thus using training schedule methods disadvantage swapout. models trained swapout consistently outperform baselines table compares swapout various layer baselines. models trained swapout consistently outperform models similar architecture. stochastic training schedule matters different layers swapout network could trained different parameters bernoulli distributions table shows different stochastic training schedules signiﬁcant affect performance. report performance deterministic well stochastic inference. schedules differ values parameters bernoulli random variables equation different layers. note corresponds maximum stochasticity. schedule less randomness early layers performs best. expected swapout adds unit noise early layers largest number units. thus stochasticity early layers signiﬁcantly reduces randomness system. schedule experiments unless otherwise stated. table comparison fair baselines cifar- swapout always accurate. refer base width others multiples report width along number parameters model. models trained swapout consistently outperform models comparable architecture. stochastic methods trained using linear schedule represent residual block architectures respectively. table choice stochastic training schedule matters. evaluate performance layer swapout model trained different stochasticity schedules cifar-. schedules differ parameters bernoulli random variables equation different layers. linear refers linear interpolation ﬁrst block last others value blocks. report performance deterministic stochastic inference schedule less randomness early layers performs best. swapout improves resnet architecture table evident networks trained swapout consistently show better performance corresponding resnets choices width investigated using deterministic inference. difference indicates performance improvement ensemble effect. stochastic inference outperforms deterministic inference table shows stochastic inference scheme outperforms deterministic scheme experiments. prediction image done averaging results stochastic forward passes. difference widely reported effect ensemble networks better networks ensemble share parameters. instead stochastic inference produces accurate expectations interacts better batch normalization. stochastic inference needs samples good estimate figure shows estimated accuracies function number forward passes image. evident relatively samples enough good estimate mean. compare figure- implies samples required. increase width leads considerable performance improvements number ﬁlters convolutional layer width. table shows performance layer model improves considerably width increased baseline resnet architecture well table wider swapout models work better. evaluate effect increasing number ﬁlters cifar-. resnets contain three groups layers convolutional layers group containing equal number ﬁlters. indicate number ﬁlters group tuple report performance deterministic well stochastic inference samples. size model trained swapout outperforms corresponding resnet model. swapout swapout swapout swapout table swapout outperforms comparable methods cifar-. note layer wider model performs competitively comparison layer resnet model. models trained swapout. swapout better able available capacity corresponding resnet similar architecture number parameters. table compares models trained swapout approaches cifar- table compares cifar-. datasets shallower wider model compares well layer resnet model. swapout uses parameters efﬁciently persistently tables swapout models fewer parameters outperform comparable models. example swapout gets error parameters comparison resnet version parameters. experiments cifar- conﬁrm results table shows swapout effective improves performance layer model widening network reducing stochasticity leads improvements. further wider relatively shallow model trained swapout competitive best performing deep latest resnet model swapout stochastic training method shows reliable improvements performance leads networks parameters efﬁciently. relatively shallow swapout networks give comparable performance extremely deep residual networks. shown different stochastic training schedules produce different behaviors searched best schedule systematic way. possible obtain improvements described extremely general swapout mechanism. straightforward using equation apply swapout inception networks recurrent convolutional networks trained swapout improves upon corresponding layer resnet model further layer wider much shallower model performs competitively comparison layer resnet model figure stochastic inference needs samples good estimate. plot mean error rate left function number samples stochastic training schedules. standard error mean shown shaded interval left magniﬁed right plot. evident relatively samples needed reliable estimate mean error. mean standard error computed using repetitions sample count. choosing form gated networks. experiments focus comparisons residual networks current performers cifar- cifar-. would interesting experiment versions method. dropout batch normalization difﬁcult give crisp explanation swapout works. believe results support idea swapout causes form improvement optimization process. relatively shallow networks swapout reliably work well better quite deep alternatives; swapout notably reliably efﬁcient parameters comparable deeper networks. unlike dropout swapout often propagate gradients still forcing units co-adapt. furthermore swapout networks involve form tying layers. unit sometimes sees layer sometimes layer gradient signal exploited encourage layers behave similarly. reason swapout successful likely involves points. glorot bordes bengio. deep sparse rectiﬁer neural networks. aistats hardt recht singer. train faster generalize better stability stochastic gradient descent. c.-y. gallagher zhang deeply-supervised nets. aistats chen yan. network network. corr abs/. nair hinton. rectiﬁed linear units improve restricted boltzmann machines. icml pinheiro collobert. recurrent convolutional neural networks scene parsing. arxiv preprint prevent neural networks overﬁtting. journal machine learning research srivastava greff schmidhuber. training deep networks. nips szegedy sermanet reed anguelov erhan vanhoucke rabi-", "year": 2016}