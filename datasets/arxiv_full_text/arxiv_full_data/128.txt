{"title": "Describing Multimedia Content using Attention-based Encoder--Decoder  Networks", "tag": ["cs.NE", "cs.CL", "cs.CV", "cs.LG"], "abstract": "Whereas deep neural networks were first mostly used for classification tasks, they are rapidly expanding in the realm of structured output problems, where the observed target is composed of multiple random variables that have a rich joint distribution, given the input. We focus in this paper on the case where the input also has a rich structure and the input and output structures are somehow related. We describe systems that learn to attend to different places in the input, for each element of the output, for a variety of tasks: machine translation, image caption generation, video clip description and speech recognition. All these systems are based on a shared set of building blocks: gated recurrent neural networks and convolutional neural networks, along with trained attention mechanisms. We report on experimental results with these systems, showing impressively good performance and the advantage of the attention mechanism.", "text": "abstract—whereas deep neural networks ﬁrst mostly used classiﬁcation tasks rapidly expanding realm structured output problems observed target composed multiple random variables rich joint distribution given input. focus paper case input also rich structure input output structures somehow related. describe systems learn attend different places input element output variety tasks machine translation image caption generation video clip description speech recognition. systems based shared building blocks gated recurrent neural networks convolutional neural networks along trained attention mechanisms. report experimental results systems showing impressively good performance advantage attention mechanism. structured output problems task input output possesses structure. task therefore input correct output also model structure within output sequence. classic example structured output problem machine translation automatically translate sentence source language target language. accomplish task system need concerned capturing semantic content source language sentence also forming coherent grammatical sentence target language. words given input source sentence cannot choose elements output independently complex joint distribution. structured output problems represent large important class problems include classic tasks speech recognition many natural language processing problems range capabilities deep learning systems increases less established forms structured output problems image caption generation video description generation references therein) considered. important aspect virtually structured output tasks structure output imtimately related structure input. central challenge tasks therefore problem alignment. fundamental problem alignment problem relate subelements input sub-elements output. consider example machine translation. order translate source sentence target language need ﬁrst decompose source sentence constituent semantic parts. need semantic parts counterparts target language. finally need semantic parts compose sentence following grammatical regularities target language. word phrase target sentence aligned word phrase source language. case image caption generation often appropriate output sentence accurately describe spatial relationships elements scene represented image. this need align output words spatial regions source image. paper focus general approach alignment problem known soft attention mechanism. broadly attention mechanisms components prediction systems allow system sequentially focus different subsets input. selection subset typically conditioned state system function previously attended subsets. attention mechanisms employed purposes. ﬁrst reduce computational burden processing high dimensional inputs selecting process subsets input. second allow system focus distinct aspects input thus improve ability extract relevant information piece output thus yielding improvements quality generated outputs. name suggests soft attention mechanisms avoid hard selection subsets input attend instead uses soft weighting different subsets. since subset processed mechanisms offer computation advantage. instead advantage brought soft-weighting readily amenable efﬁcient learning gradient backpropagation. paper present review recent work applying soft attention structured output tasks spectulate future course line research. soft-attention mechanism part growing litterature ﬂexible deep learning architectures embed certain amount distributed decision making. generate exact sentence sample rnnlm iteratively sampling next word distribution instead stochastic sampling possible approximately sentence sample maximizes probability using instance beam search rnn-lm described extended learn conditional language model. conditional language modeling task model distribution sentences given additional input context. context anything image video clip sentence another language. examples textual outputs associated inputs conditional rnn-lm include respectively image caption video description translation. cases transition function take additional input context convolutional neural network special type general feedforward neural network multilayer perceptron speciﬁcally designed work well two-dimensional images often consists multiple convolutional layers followed fullyconnected layers. convolutional layer input image width height color channels ﬁrst convolved local ﬁlters y×c×d. location/pixel recently become common sophisticated recurrent activation functions long short-term memory gated recurrent unit reduce issue vanishing gradient lstm avoid vanishing gradient introducing gating units adaptively control information across time steps. rnn-lm recurrent neural network language modeling task language modeling model learn probability distribution natural language sentences. words given model compute probability sentence consisting multiple words i.e. sentence words long. task language modeling equivalent task predicting next word. clear rewriting sentence probability pooling operation desirable properties. first reduces dimensionality high-dimensional output convolutional spatial maxpooling summarizes activation neighbouring feature activations leading translation invariance. ﬁnal feature last convolutional layer ﬂattened form vector representation input image. vector small number fully-connected nonlinear layers output. recently cnns found excellent task large-scale object recognition. instance annual imagenet large scale visual recognition challenge classiﬁcation track million annotated images classes provided training set. challenge cnn-based entries dominant since deep trained large training provided part ilvrc challenge intermediate representation feature convolutional layer vector representation subsequent fully-connected layers whole network tasks original classiﬁcation. observed intermediate representation deep image descriptor signiﬁcantly boosts subsequent tasks object localization object detection ﬁne-grained recognition attribute detection image retrieval furthermore non-trivial tasks image caption generation found beneﬁt using image descriptors pre-trained deep cnn. later sections discuss detail image representations pre-trained deep used non-trivial tasks image caption generation video description generation task model generates natural language description multimedia input speech image video well text another language take general view. requires model capture underlying complex mapping spatio-temporal structures input complicated linguistic structures output. section describe neural network based approach problem based encoder–decoder framework recently proposed attention mechanism. name suggests neural network based encoder–decoder framework consists encoder decoder. encoder fenc ﬁrst reads input data continuous-space representation choice fenc largely depends type input. two-dimensional image convolutional neural network sec. ii-d used. recurrent neural network sec. ii-a natural choice sentence. decoder generates output conditioned continuous-space representation context input. equivalent computing conditional probability distribution given again choice fdec made based type output. instance image pixel-wise image segmentation conditional restricted boltzmann machine used natural language description input natural able model natural languages described sec. ii-b. fig. graphical illustration simplest form encoder-decoder model machine translation respectively input sentence output sentence continuousspace representation input sentence. encoder–decoder framework successfully used machine translation. work used encoder summarize source sentence conditional rnn-lm sec. ii-a decoded corresponding translation. fig. graphical illustration. authors used pre-trained encoder conditional decoder model generate natural language caption images. similarly simpler feedforward log-bilinear language model context vector summarize certain spatial location image machine translation context vector summarize phrase centered around speciﬁc word source sentence cases number vectors context vary across input examples. choice encoder kind context return governed application type input considered. paper assume decoder conditional rnn-lm sec. ii-b i.e. goal describe input natural language sentence. attention mechanism controls input actually seen decoder requires another neural network refer attention model. main attention model score context vector respect current hidden state decoder fatt computing context vector time step decoder frees encoder compressing variable-length input single ﬁxed-dimensional vector. spatially temporally dividing input encoder used decoder authors applied encoder–decoder framework video description generation used pre-trained extract feature vector frame input video averaged vectors. recent applications encoder–decoder framework continuous-space representation input returned encoder ﬁxeddimensional vector regardless size input. furthermore context vector structured design rather arbitrary vector means guarantee context vector preserves spatial temporal spatio-temporal structures input. henceforth refer encoder–decoder based model ﬁxed-dimensional context vector simple encoder–decoder model. motivation naive implementation encoder– decoder framework simple encoder–decoder model requires encoder compress input single vector predeﬁned dimensionality regardless size amount information input. instance recurrent neural network based encoder used machine translation needs able summarize variablelength source sentence single ﬁxed-dimensional vector. even size input ﬁxed case ﬁxed-resolution image amount information contained image vary signiﬁcantly observed performance neural machine translation system based simple encoder–decoder model rapidly degraded length source sentence grew. authors hypothesized limited capacity simple encoder–decoder’s ﬁxeddimensional context vector. interpretability simple encoder– decoder extremely low. information required decoder generate output compressed context vector without presupposed structure structure available techniques designed inspect representations captured model attention mechanism encoder–decoder models introduction attention mechanism encoder decoder address issues i.e. limited capacity ﬁxed-dimensional context vector lack interpretability. ﬁrst step introducing attention mechanism encoder–decoder framework encoder return structured representation input. achieve allowing continuous-space representation ﬁxed-size vectors refer context i.e. trainable parameters model. maximum likelihood learning weighted used compute context vector whole attention-based encoder–decoder model becomes large differentiable function. allows compute gradient log-likelihood using backpropagation computed gradient instance stochastic gradient descent algorithm iteratively update parameters maximize log-likelihood. variational learning hard attention model attention model makes hard decision time derivatives stochastic decision zero decisions discrete. hence information improve take focusof-attention decisions available back-propagation needed train attention mechanism. question training neural networks stochastic discretevalued hidden units long history starting boltzmann machines recent work studying deal units system trained using back-propagated gradients brieﬂy describe variational learning approach attention-based neural fig. visualization attention weights machine translation model corresponds output symbol column input symbol. brighter higher represent input vectors needs encode ﬁxed amount information focused around particular region input. words introduction attention mechanism bypasses issue limited capacity ﬁxed-dimensional context vectors. furthermore attention mechanism allows directly inspect internal working whole encoder–decoder model. magnitude attention weight positive construction highly correlates predictive spatial temporal spatio-temporal region input j-th context vector corresponds prediction associated t-th output variable easily done visualizing attention matrix fatt. missing {αt− extended hard attention using attention mechanism extended taking intou account past values attention weights general scoring function following approach based purely weights introduced discuss detail three applications/approaches later sections. english-to-french translation task measured bleu ensemble multiple attention-based models. state-of-the-art phrase-based statistical machine translation attention-based model proposed able achieve relative improvement case english-to-french translation task shown table model extended large target vocabulary relative improvement baseline without attention mechanism additionally model recently tested number european language pairs wmt’ translation task.. table results. authors recently proposed method incorporating monolingual language model attentionbased neural machine translation system. method attention-based model shown outperform existing statistical machine translation systems chinese-to-english turkish-to-english translation tasks well european languages tested. image caption generation task model looks input image generates corresponding natural language description. encoder–decoder framework well task. encoder extract continuousspace representation context input image instance deep convolutional network representation conditional rnn-lm based decoder generates natural language description image. recently number research groups independently proposed simple encoder–decoder model solve image caption generation machine translation task sentence language translated corresponding sentence another language neural machine translation aims solving single neural network based model jointly trained end-to-end. encoder–decoder framework described sec. iii-a proposed neural machine translation recently based works attention-based model proposed make neural machine translation systems robust long sentences. here brieﬂy describe model model description attention-based neural machine translation uses bidirectional recurrent neural network encoder. forward network reads input sentence ﬁrst word last resulting sequence state vectors content-based attention mechanism used. content-based attention mechanism eqs. relies solely so-called content-based scoring without context information whole sentence words appear multiple times source sentence cannot distinguished attention model. model description usual encoder–decoder based image caption generation models activation last fully-connected hidden layer continuous-space representation context vector input image authors however proposed activation last convolutional layer pre-trained convolutional network bottom half fig. unlike fully-connected layer case context consists multiple vectors correspond different spatial regions input image attention mechanism applied. furthermore convolution pooling spatial locations pixel space represented context vector overlaps substantially represented neighbouring context vectors helps attention mechanism distinguish similar objects image using context information respect whole image neighbouring pixels. similarly attention-based neural machine translation sec. iv-a decoder implemented conditional rnn-lm. content-based attention mechanism either weighted hard decision tested training model maximum likelihood estimator sec. iii-c variational learning sec. iii-c respectively. authors reported similar performances approaches number benchmark datasets. experimental result attention-based image caption generator evaluated three datasets; flickr flickr coco addition self-evaluation ensemble multiple attention-based models submitted microsoft coco image captioning challenge evaluated multiple automatic evaluation metrics well human evaluators. challenge attention-based approach ranked third based percentage captions evaluated better equal human caption percentage captions pass turing test interestingly model ranked eighth according recently proposed metric cider ninth according widely used metric bleu. means model better relative performance terms human evaluation terms automatic metrics look matching subsequences words directly meaning generated sentence. performance top-ranked systems including attention-based model listed table iii. soon neural machine translation based simple encoder–decoder framework proposed applied video description generation amounts translating video clip natural language description authors used pretrained convolutional network extract feature vector frame video clip average frame-speciﬁc vectors obtain single ﬁxed-dimensional context vector whole video. conditional rnn-lm sec. ii-b used generate description based context vector. since video clip clearly temporal spatial structures possible exploit using attention mechanism described throughout paper. authors proposed approach based attention mechanism exploit global local temporal structures video clips. brieﬂy describe approach. types encoders tested. ﬁrst simple frame-wise application pre-trained convolutional network. however pool per-frame context vectors done simply form context consisting perframe feature vectors. attention mechanism work select per-frame vectors output symbol decoded. authors claimed overall model captures global temporal structure type encoder so-called convolutional network shown fig. unlike usual convolutional network often works spatially two-dimensional image convolutional network applies ﬁlters across spatial dimensions well temporal dimensions. furthermore ﬁlters work pixels local motion statistics enabling model concentrate motion rather appearance. similarly strategy sec. ii-d model trained larger video datasets recognize action video clip activation vectors last convolutional layer used context. authors suggest encoder extracts local temporal structures complementing global structures extracted frame-wise application convolutional network. approach video description generation tested datasets; youtubetext montreal showed beneﬁcial types encoders together attention-based encoder–decoder model attention-based model outperforms simple encoder– decoder model. table summary evaluation. previous applications attention-based model attention mechanism applied task video description also provides straightforward inspect inner workings model. fig. examples. fig. sample videos corresponding generated groundtruth descriptions youtubetext. plot frame corresponds attention weight frame corresponding word generated. reprinted speech recognition task given speech waveform translated corresponding natural language transcription. deep neural networks become standard acoustic part speech recognition systems input speech processed deep neural network based acoustic model another model almost always hidden markov model used correctly much longer sequence speech shorter sequence phonemes/characters/words. recently fully neural network based speech recognition models proposed. here describe recently proposed attention-based fully neural speech recognizer detailed comparison attention-based fully speech recognizer neural speech recognizers e.g. refer reader model description–hybrid attention mechanism basic architecture attention-based model speech recognition similar attention-based models described earlier especially attention-based neural machine translation model sec. iv-a. encoder stacked bidirectional recurrent neural network reads input sequence speech frames frame -dimensional vector consisting mel-scale ﬁlter-bank response energy ﬁrstsecond-order temporal differences. context concatenated hidden states top-level birnn used decoder based conditional rnn-lm generate corresponding transcription case consists sequence phonemes. authors however noticed peculiarity speech recognition compared instance machine translation. first lengths input output differ signiﬁcantly; thousands input speech frames dozen words. second alignment symbols input output sequences monotonic often true case translation. issues especially ﬁrst make difﬁcult content-based attention mechanism described eqs. work well. authors investigated issues carefully proposed attention mechanism location awareness particulary appropriate location awareness case means attention mechanism directly takes account previous attention weights compute next ones. experimental result attention-based speech recognizer evaluated widely-used timit corpus closely following procedure seen table attention-based speech recognizer location-aware attention mechanism recognize sequence phonemes given speech segment perform better conventional fully neural speech recognition. also location-aware attention mechanism helps model achieve better generalization error. similarly previous applications possible inspect model’s behaviour visualizing attention weights. example shown fig. clearly model attends roughly correct window speech time generates phoneme. parsing–grammar foreign language parsing sentence parse tree considered variant machine translation target sentence parse tree. authors evaluate simple encoder– decoder model attention-based model generating linearized parse tree associated natural language sentence. experiments revealed attention-based parser match existing state-of-the-art parsers often highly domain-speciﬁc. discrete optimization–pointer network attention mechanism used solve discrete optimization problems. unlike usual described attention mechanism decoder generates sequence output symbols application discrete optimization decoder predicts source symbols/nodes chosen time step. authors achieve instance case travelling salesperson problem model needs generate sequence cities/nodes cover whole input cities sequence shortest possible route input cover every single city/node. first encoder reads graph instance returns context vectors corresponds city input graph. decoder returns sequence probabilities input cities equivalently context vectors computed attention mechanism. model trained generate sequence cover cities correctly attending city using attention mechanism. question answering–weakly supervised memory network authors applied attention-based model question-answering task. instance task consists facts question fact question natural language sentences. fact encoded continuous-space representation forming context fact vectors. attention mechanism applied context given continuous-space representation question model focus relevant facts needed answer question. neural network location-based attention mechanism opposed content-based attention mechanism described paper. content-based attention mechanism computes relevance spatial temporal spatio-temporally localized region input location-based directly returns region model needs attend often form coordinate -coordinate input image offset current coordinate. location-based attention mechanism successfully used model generate handwritten text. neural network designed locationbased attention mechanism recognize objects image. furthermore generative model images proposed iteratively reads writes portions whole image using location-based attention mechanism. earlier works utilizing attention mechanism contentbased location-based object recognition/tracking found attention-based mechanim described paper variant applied something multimedia input. instance neural turing machine proposed implements memory controller using content-based location-based attention mechanisms. similarly authors used content-based attention mechanism hard decision relevant memory contents futher extended weakly supervised memory network sec. iv-e. paper described recently proposed attentionbased encoder–decoder architecture describing multimedia content. started providing background materials recurrent neural networks convolutional networks form building blocks encoder–decoder architecture. emphasized speciﬁc variants networks often used encoder–decoder model; conditional language model based rnns pre-trained transfer learning. then introduced simple encoder–decoder model followed attention mechanism together form central topic paper attention-based encoder–decoder model. presented four recent applications attention-based encoder–decoder models; machine translation image caption generation video description generation speech recognition gave concise description attention-based model applications together model’s performance benchmark datasets. furthermore description accompanied ﬁgure visualizing behaviour attention mechanism. examples discussed above attention mechanism primarily considered means building model describe input multimedia content natural language meaning ultimate goal attention mechanism encoder–decoder model multimedia content description. however taken possible application attention mechanism. indeed beside superior performance delivers attention mechanism used extract underlying mapping entirely different modalities without explicit supervision mapping. figs. clear attention-based models able infer unsuperivsed alignments different modalities agree well intuition. suggests type attention-based model used solely extract underlying often complex mappings pair modalities much prior/domain knowledge. example attention-based models used neuroscience temporally spatially neuronal activities sequence stimuli authors would like thank following research funding computing support nserc frqnt calcul qu´ebec compute canada canada research chairs cifar samsung. kulkarni premraj ordonez dhar choi berg berg babytalk understanding generating simple image descriptions pattern analysis machine intelligence ieee transactions vol. merrienboer gulcehre bougares schwenk bengio learning phrase representations using encoderdecoder statistical machine translation proceedings empiricial methods natural language processing oct. hochreiter informatik bengio frasconi schmidhuber gradient recurrent nets difﬁculty learning longterm dependencies field guide dynamical recurrent networks kolen kremer eds. mikolov kombrink burget cernocky khudanpur extensions recurrent neural network language model proc. ieee international conference acoustics speech signal processing sermanet eigen zhang mathieu fergus lecun overfeat integrated recognition localization detection using convolutional networks international conference learning representations razavian azizpour sullivan carlsson features off-the-shelf astounding baseline recognition computer vision pattern recognition workshops ieee conference merri¨enboer bahdanau bengio properties neural machine translation encoder–decoder approaches eighth workshop syntax semantics structure statistical translation oct. hinton sejnowski learning relearning boltzmann machines parallel distributed processing explorations microstructure cognition. volume foundations rumelhart mcclelland eds. cambridge press mnih heess graves kavukcuoglu recurrent models visual attention advances neural information processing systems ghahramani welling cortes lawrence weinberger eds. curran associates inc. wehbe murphy talukdar fyshe ramdas mitchell simultaneously uncovering patterns brain regions involved different story reading subprocesses plos vol. nov. durrani haddow koehn heaﬁeld edinburgh’s phrasebased machine translation systems wmt- proceedings ninth workshop statistical machine translation. association computational linguistics baltimore donahue hendricks guadarrama rohrbach venugopalan saenko darrell long-term recurrent convolutional networks visual recognition description arxiv. hodosh young hockenmaier framing image description ranking task data models evaluation metrics journal artiﬁcial intelligence research papineni roukos ward w.-j. bleu method automatic evaluation machine translation proceedings annual meeting association computational linguistics. association computational linguistics denkowski lavie meteor universal language speciﬁc translation evaluation target language proceedings eacl workshop statistical machine translation chen dolan collecting highly parallel data paraphrase evaluation proceedings annual meeting association computational linguistics portland oregon june hinton deng dahl mohamed jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal process. mag. vol. graves fern´andez gomez schmidhuber connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks icml’ pittsburgh hannun case casper catanzaro diamos elsen prenger satheesh sengupta coates deepspeech scaling end-to-end speech recognition arxiv preprint arxiv. garofolo lamel fisher fiscus pallett darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc nasa sti/recon technical report vol. graves a.-r. mohamed hinton speech recognition deep recurrent neural networks icassp’ t´oth combining time-and frequency-domain convolution convolutional neural network-based phone recognition icassp", "year": 2015}