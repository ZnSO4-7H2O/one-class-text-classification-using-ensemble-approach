{"title": "Named Entity Recognition with Bidirectional LSTM-CNNs", "tag": ["cs.CL", "cs.LG", "cs.NE", "68T50", "I.2.7"], "abstract": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information.", "text": "named entity recognition challenging task traditionally required large amounts knowledge form feature engineering lexicons achieve high performance. paper present novel neural network architecture automatically detects wordcharacter-level features using hybrid bidirectional lstm architecture eliminating need feature engineering. also propose novel method encoding partial lexicon matches neural networks compare existing approaches. extensive evaluation shows that given tokenized text publicly available word embeddings system competitive conll- dataset surpasses previously reported state performance ontonotes dataset points. using lexicons constructed publicly-available sources establish state performance score conll- ontonotes surpassing systems employ heavy feature engineering proprietary lexicons rich entity linking information. named entity recognition important task nlp. high performance approaches dominated applying perceptron models hand-crafted features however collobert proposed effective neural network model requires little feature engineering instead learns important features word embeddings trained large quantities unlabelled text approach made possible recent unfortunately many limitations model proposed collobert first uses simple feed-forward neural network restricts context ﬁxed sized window around word approach discards useful long-distance relations words. second depending solely word embeddings unable exploit explicit character level features preﬁx sufﬁx could useful especially rare words word embeddings poorly trained. seek address issues proposing powerful neural network model. well-studied solution neural network process variable length input long term memory recurrent neural network recently rnns shown great success diverse tasks speech recognition machine translation language modeling long-short term memory unit forget gate allows highly non-trivial long-distance dependencies easily learned sequential labelling tasks speech recognition bi-directional lstm model take account effectively inﬁnite amount context sides word eliminates problem limited context applies feed-forward model lstms studied past task hammerton lack computational power blstm tagging named entities. multiple tables look word-level feature vectors. extracts ﬁxed length feature vector character-level features. word vectors concatenated blstm network output layers convolutional neural networks also investigated modeling character-level information among tasks. santos labeau successfully employed cnns extract character-level features pos-tagging respectively. collobert also applied cnns semantic role labeling variants architecture applied parsing tasks requiring tree structures however effectiveness character-level cnns evaluated english ner. considered using character-level bi-directional lstms recently proposed ling postagging preliminary evaluation shows perform signiﬁcantly better cnns being computationally expensive train. figure convolutional neural network extracts character features word. character embedding character type feature vector computed lookup tables. then concatenated passed cnn. learns characterword-level features presenting ﬁrst evaluation architecture well-established english language evaluation datasets. furthermore lexicons crucial performance propose lexicon encoding scheme matching algorithm make partial matches compare simpler approach collobert extensive evaluation shows proposed method establishes state conll- shared task ontonotes datasets. neural network inspired work collobert lookup tables transform discrete features words characters continuous vector representations concatenated neural network. instead feed-forward network bi-directional long-short term memory network. induce character-level features convolutional neural network successfully applied spanish portuguese german pos-tagging also experimented sets published embeddings namely stanford’s glove embeddings trained billion words wikipedia text google’s wordvec embeddings trained billion words google news addition hypothesized word embeddings trained in-domain text perform better also used publicly available glove program in-house re-implementation wordvec program train word embeddings wikipedia reuters datasets well. http//ml.nec-labs.com/senna/ http//nlp.stanford.edu/projects/glove/ https//code.google.com/p/wordvec/ used in-house reimplementation train word vectors uses distributed processing train much quicker publicly-released implementation wordvec performance word analogy task higher reported mikolov stacked bi-directional recurrent neural network long short-term memory units transform word features named entity scores. figures illustrate network detail. extracted features word forward lstm network backward lstm network. output network time step decoded linear layer log-softmax layer log-probabilities category. vectors simply added together produce ﬁnal output. word employ convolution layer extract feature vector percharacter feature vectors character embeddings character type words padded number special padding characters sides depending window size cnn. figure example lexicon features applied. markings indicate token matches begin inside token entry lexicon. indicates token matches single-token entry. randomly initialized lookup table values drawn uniform distribution range output character embedding dimensions. includes unique characters conll- dataset plus special tokens padding unknown. padding token used unknown token used characters random embeddings used experiments. capitalization information erased lookup word embedding evaluate collobert’s method using separate lookup table capitalization feature following options allcaps upperinitial lowercase mixedcaps noinfo method compared character type feature character-level cnns. upper lower case letters numbers punctuations experiment settings english character small enough effective embeddings could learned directly task data. four categories deﬁned conll shared task compiled list known named entities dbpedia extracting descendants dbpedia types corresponding conll categories. construct separate lexicons ontonotes tagset correspondences between dbpedia categories tags could found many instances. addition entry ﬁrst removed parentheses text contained within stripped trailing punctuation ﬁnally tokenized penn treebank tokenization script purpose partial matching. table shows size category lexicon compared collobert’s lexicon extracted senna system. figure shows example lexicon features applied. lexicon category match every n-gram entries lexicon. match successful n-gram matches preﬁx sufﬁx entry least half length entry. high potential spurious matches categories except person discard partial matches less tokens length. multiple overlapping matches within category prefer exact matches partial matches longer matches shorter matches ﬁnally earlier matches sentence later matches. matches case insensitive. token match feature enneural network using torch library training inference done per-sentence level. initial states lstm zero vectors. except character word embeddings whose initialization described previously lookup tables randomly initialized values drawn standard normal distribution. train network maximize sentencelevel log-likelihood collobert first deﬁne tag-transition matrix represents score jumping successive tokens score starting matrix parameters also learned. deﬁne parameters neural network {aij parameters trained. given example sentence length deﬁne score outputted neural network word given parameters score sequence tags given network transition scores section found sophisticated method outperforms method presented collobert treats partial exact matches equally allows preﬁx sufﬁx matches allows short partial matches marks tokens yes/ addition since collobert released lexicon senna system also applied lexicon model comparison investigated using lexicons simultaneously distinct features. found lexicons complement improve performance conll- dataset. table results models various feature sets compared published results. three sections order models published neural network models published non-neural network models. features collobert word embeddings caps capitalization feature lexicon features senna dbpedia lexicons. scores standard deviations parentheses. training done mini-batch stochastic gradient descent ﬁxed learning rate. mini-batch consists multiple sentences number tokens. found applying dropout output nodes lstm layer quite effective reducing overﬁtting explored sophisticated optimization algorithms momentum adadelta rmsprop preliminary experiments improve upon plain sgd. evaluation performed well-established conll- shared task dataset much larger less-studied ontonotes dataset table gives overview different datasets. table score results blstm blstm-cnn models various additional features; collobert word embeddings char character type feature caps capitalization feature lexicon features. note starred results repeated ease comparison. conll dataset conll- dataset consists newswire reuters corpus tagged four types named entities location organization person miscellaneous. dataset small compared ontonotes trained model training development sets performing hyperparameter optimization development set. ontonotes dataset pradhan compiled core portion ontonotes dataset conll- shared task described standard train/dev/test split evaluation. following durrett klein applied model portion dataset gold-standard named entity annotations; testaments portion excluded lacking gold-standard annotations. dataset much larger conll- consists text wide variety sources broadcast conversation broadcast news newswire magazine telephone conversation text. hyper-parameter optimization performed rounds hyper-parameter optimization selected best settings based development performance. table shows ﬁnal hyper-parameters table shows performance best models round. ﬁrst round performed random search selected best hyper-parameters development conll- data. evaluated around hyper-parameter settings. then took settings tuned learning rate epochs ontonotes development set. second round performed independent hyper-parameter searches dataset using optunity’s implementation particle swarm evidence efﬁcient random search evaluated hyper-parameter settings round well. later found training fails occasionally well large variation settings dataset trials selected best based averaged performance. conll- found particle swarm produced better hyper-parameters random search. however surprisingly ontonotes particle swarm unable produce better hyperparameters ad-hoc approach round also tried tuning conll- hyper-parameters round ontonotes better either. training tagging speed intel xeon processor training takes hours tagging test takes seconds conll-. times hours seconds respectively ontonotes. table scores collobert word vectors replaced. tried -dimensional random vectors glove’s released vectors trained billion words google’s released -dimensional vectors trained billion words google news -dimensional glove wordvec skip-gram vectors trained wikipedia reuters rcv- epochs observed models exhibit overtraining instead continued slowly improve development long after reaching near accuracy training set. contrast despite ontonotes much larger conll- training epochs causes performance development decline steadily overﬁtting. conll- dataset blstm models completed training without difﬁculty blstmcnn models fail converge around time depending feature set. similarly ontonotes trials fail. found using lower learning rate reduces failure rate. also tried clipping gradients using adadelta effective eliminating failures themselves. adadelta however made training expensive gain model performance. case experiments excluded trials ﬁnal score subset training data falls certain threshold continued trials obtained successful ones. table shows results datasets. best knowledge best models surpassed previous highest reported scores conll- ontonotes. particular external knowledge word embeddings model competitive conll dataset establishes state ontonotes suggesting given enough data neural network automatically learns relevant features without feature engineering. comparison ffnns re-implemented ffnn model collobert baseline comparison. table shows performing reasonably well conll- ffnns clearly inadequate ontonotes larger domain showing lstm models essential ner. comparison models table shows conll- blstm-cnn models signiﬁcantly outperform blstm models given feature set. effect smaller statistically signiﬁcant ontonotes capitalization features added. adding character type capitalization features blstm-cnn models degrades performance conll mostly improves performance ontonotes suggesting character-level cnns replace hand-crafted character features cases systems weak lexicons beneﬁt character features. wilcoxon rank test comparing four blstm models corresponding blstm-cnn models using feature set. wilcoxon rank test selected robustness small sample sizes distribution unknown. table table show obtain large signiﬁcant improvement trained word embeddings used opposed random embeddings regardless additional features used. consistent collobert results. table compares performance different word embeddings best model table conll- publicly available glove google embeddings point behind collobert’s embeddings. ontonotes glove embeddings perform close collobert embeddings google embeddings point behind. addition dimensional embeddings present signiﬁcant improvement dimensional embeddings result previously reported turian possible reason collobert embeddings perform better publicly available embeddings conll- trained reuters rcv- corpus source conll- dataset whereas embeddings not. hand suspect google’s embeddings perform poorly vocabulary mismatch particular google’s embeddings trained case-sensitive manner embeddings many common punctuations wilcoxon rank test make direct comparison collobert exclude conll- task test data word vector training data. possible difference could responsible disparate performance word vectors conll- training data comprises million words total data; unsupervised training scheme effects likely negligible. symbols provided. test hypotheses performed experiments word embeddings trained using glove wordvec vocabulary list corpus similar collobert shown table glove embeddings improved signiﬁcantly publicly available embeddings conll- wordvec skip-gram embeddings improved signiﬁcantly google’s embeddings ontonotes. time constraints perform hyper-parameter searches word embeddings. word embedding quality depends hyper-parameter choice training also neural network hyper-parameter choice likely sensitive type word embeddings used optimizing likely produce better results provide fairer comparison word embedding quality. effect dropout table compares result various dropout values dataset. models trained using training dataset isolate effect dropout test sets. hyper-parameters features remain best model table datasets test sets dropout essential state performance improvement statistically signiﬁcant. dropout optimized described section hence chosen lexicon features table shows conll- dataset using features senna lexicon proposed dbpedia lexicon provides signiﬁcant improvement allows model clearly surpass previous state art. unfortunately difference minuscule ontonotes likely lexicon match dbpedia categories well. figure shows conll- lexicon coverage reasonable matches tags everything except catchmisc category. example entries lexicon match mostly named entities vice versa. however ontonotes matches noisy correspondence lexicon match category quite ambiguous. example lexicon categories spurious matches unrelated named entities like cardinal language entities matches category lexicon. addition named entities categories like norp product receive little coverage. lower coverage noise ambiguity contribute disappointing performance. suggests dbpedia lexicon construction method needs improved. reasonable place start would dbpedia category ontonotes mappings. order isolate contribution lexicon matching method compare different sources matching methods blstm-cnn model randomly initialized word embeddings compared senna lexicon dbpedia lexicon noisier broader coverage explains applying using method collobert performs worse conll- better ontonotes dataset containing many obscure named entities. however suspect method collobert noise resistant therefore unsuitable lexicon fails distinguish exact partial matches minimum length partial matching. instead apply superior partial matching algorithm bioes encoding dbpedia lexicon gain signiﬁcant improvement allowing lexicon perform similarly senna lexicon. unfortunately could reliably remove partial entries senna lexicon unable investigate whether lexicon matching method would help lexicon. table comparison lexicon matching/encoding methods blstm-cnn model employing random embeddings features. using lexicons best combination matching encoding exact-bioes senna partial-bioes dbpedia. note senna lexicon already contains partial entries exact matching case really primitive form partial matching. analysis ontonotes performance table shows per-genre breakdown ontonotes results. expected model performs best clean text like broadcast news newswire worst noisy text like telephone conversation text model also substantially improves previous work genres except small size training data likely hinders learning. finally performance characteristics model appear quite different previous models likely apply completely different machine learning method. named entity recognition recent approaches characterized perceptron models performance heavily dependent feature engineering. ratinov roth used non-local features gazetteer extracted wikipedia brown-cluster-like word representations achieved score conll. surpassed without using gazetteer instead using phrase features obtained performing k-means clustering private database search engine query logs. passos obtained nearly performance using public data training phrase vectors lexicon-infused skip-gram model. order combat problem sparse features suzuki employed large-scale unlabelled data perform feature reduction achieved score conll- current state systems without external knowledge. training system together related tasks entity linking recently shown improve state art. durrett klein combined coreference resolution entity linking single model added cross-task interaction factors. system achieved state results ontonotes dataset evaluate conll- dataset lack coreference annotations. achieved state results conll training joint model entity linking tasks pair tasks whose interdependencies contributed work durrett klein petasis used feed-forward neural network hidden layer achieved state-of-the-art results dataset. approach used gazetteer tags word word embeddings. hammerton attempted singledirection lstm network combination word vectors trained using self-organizing maps context vectors obtained using principle component analysis. however method optimizes loglikelihood uses softmax used different output encoding optimized unspeciﬁed objective function. hammerton’s reported results slightly baseline models. much later advent neural word embeddings collobert presented senna employs deep ffnn word embeddings achieve near state results tagging chunking srl. build approach sharing word embeddings feature encoding method objective functions. presented charwnn network augments neural network collobert character level cnns reported improved performance spanish portuguese ner. successfully incorporated character-level cnns model. various similar architecture proposed various sequential labeling tasks. huang used blstm pos-tagging chunking tasks employed heavy feature engineering instead using automatically extract characterlevel features. labeau used brnn character-level cnns perform german postagging; model differs powerful lstm unit found perform better rnns preliminary experiments employ word embeddings much important tagging. ling used wordcharacter-level blstms establish current state english tagging. using blstms instead cnns allows extraction sophisticated character-level features found preliminary experiments perform signiﬁcantly better cnns substantially computationally expensive train. shown neural network model incorporates bidirectional lstm character-level beneﬁts robust training dropout achieves state-of-the-art results named entity recognition little feature engineering. model improves previous best reported results major datasets suggesting model capable learning complex relationships large amounts data. preliminary evaluation partial matching lexicon algorithm suggests performance could improved ﬂexible application existing lexicons. evaluation existing word embeddings suggests domain training data important training algorithm. effective construction application lexicons word embeddings areas require research. future would also like extend model perform similar tasks extended tagset entity linking. research supported honda research institute japan ltd. authors would like thank collobert releasing senna word vectors lexicon torch framework contributors andrey karpathy reference lstm implementation.", "year": 2015}