{"title": "Knowledge Graph Completion via Complex Tensor Factorization", "tag": ["cs.AI", "cs.LG", "math.SP", "stat.ML"], "abstract": "In statistical relational learning, knowledge graph completion deals with automatically understanding the structure of large knowledge graphs---labeled directed graphs---and predicting missing relationships---labeled edges. State-of-the-art embedding models propose different trade-offs between modeling expressiveness, and time and space complexity. We reconcile both expressiveness and complexity through the use of complex-valued embeddings and explore the link between such complex-valued embeddings and unitary diagonalization. We corroborate our approach theoretically and show that all real square matrices---thus all possible relation/adjacency matrices---are the real part of some unitarily diagonalizable matrix. This results opens the door to a lot of other applications of square matrices factorization. Our approach based on complex embeddings is arguably simple, as it only involves a Hermitian dot product, the complex counterpart of the standard dot product between real vectors, whereas other methods resort to more and more complicated composition functions to increase their expressiveness. The proposed complex embeddings are scalable to large data sets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.", "text": "statistical relational learning knowledge graph completion deals automatically understanding structure large knowledge graphs—labeled directed graphs— predicting missing relationships—labeled edges. state-of-the-art embedding models propose diﬀerent trade-oﬀs modeling expressiveness time space complexity. reconcile expressiveness complexity complex-valued embeddings explore link complex-valued embeddings unitary diagonalization. corroborate approach theoretically show real square matrices—thus possible relation/adjacency matrices—are real part unitarily diagonalizable matrix. results opens door applications square matrices factorization. approach based complex embeddings arguably simple involves hermitian product complex counterpart standard product real vectors whereas methods resort complicated composition functions increase expressiveness. proposed complex embeddings scalable large data sets remains linear space time consistently outperforming alternative approaches standard link prediction benchmarks. keywords complex embeddings tensor factorization knowledge graph matrix completion statistical relational learning web-scale knowledge graph provide structured representation world knowledge projects google knowledge vault enable wide range applications including recommender systems question answering automated personal agents. incompleteness knowledge graphs—also called knowledge bases—has stimulated research predicting missing entries task known link prediction knowledge graph completion. need high quality predictions required link prediction applications made progressively become main problem statistical relational learning research ﬁeld interested relational data representation modeling. knowledge graphs born advent semantic pushed world wide consortium recommendations. namely resource description framework standard underlies knowledge graphs’ data representation provides ﬁrst time common framework across connected information systems share data paradigm. expressive classical relational databases existing relational data translated knowledge graphs knowledge graphs express data directed graph labeled edges between nodes natural redundancies recorded relations often make possible missing entries knowledge graph. example relation countryofbirth could recorded entities inferred relation cityofbirth known. goal link prediction automatic discovery regularities. however many relations non-deterministic combination facts isbornin islocatedin always imply fact hasnationality. hence natural handle inference probabilistically jointly facts involving relations entities. increasingly popular method state knowledge graph completion task binary tensor completion problem tensor slice adjacency matrix relation knowledge graph compute decomposition partially-observed tensor missing entries completed. factorization models low-rank embeddings popularized netﬂix challenge partially-observed matrix tensor decomposed product embedding matrices much smaller dimensions resulting ﬁxed-dimensional vector representations entity relation graph allow completion missing entries. given fact subject entity linked object entity relation score fact recovered multilinear product embedding vectors sophisticated composition functions binary relations knowledge graphs exhibit various types patterns hierarchies compositions like fatherof olderthan ispartof strict/non-strict orders preorders equivalence relations like issimilarto. characteristics maps diﬀerent combinations following properties reﬂexivity/irreﬂexivity symmetry/antisymmetry transitivity. described bordes relational model able learn combinations properties linear time memory order scale size present-day knowledge graphs keep growth. natural handle possible relations classic canonical polyadic decomposition yields diﬀerent embeddings entity thus prediction performances shown section unique entity embeddings multilinear products scale well naturally handle symmetry -reﬂexivity relations combined appropriate loss function products even handle transitivity however dealing antisymmetric—and generally asymmetric—relations almost always implied superlinear time space complexity making models prone overﬁtting scalable. finding best trade-oﬀ expressiveness generalization complexity keystone embedding models. work argue standard product embeddings eﬀective composition function provided uses right representation instead using embeddings containing real numbers discuss demonstrate capabilities complex embeddings. using complex vectors vectors entries product often called hermitian product involves conjugate-transpose vectors. consequence product symmetric more facts relation receive diﬀerent scores depending ordering entities involved fact. summary complex embeddings naturally represent arbitrary relations retaining eﬃciency product linearity space time complexity. paper extends previously published article extended version adds proofs existence proposed model single multi-relational settings well proofs non-uniqueness complex embeddings given relation. bounds rank proposed decomposition also demonstrated discussed. learning algorithm provided details experiments provided especially regarding training time models. remainder paper organized follows. ﬁrst provide justiﬁcation intuition using complex embeddings square matrix case single type relation entities show existence proposed decomposition possible relations. formulation extended stacked square matrices third-order tensor represent multiple relations stochastic gradient descent algorithm used learn model detailed section present equivalent reformulation proposed model involves real embeddings. help practitioners implementing method without requiring complex numbers software implementation. describe experiments large-scale public benchmark knowledge graphs empirically show representation leads simpler faster algorithms also gives systematic accuracy improvement current state-of-the-art alternatives related work discussed section ﬁrst discuss desired properties embedding models show problem relates spectral theorems discuss classes matrices theorems encompass real complex case. propose matrix decomposition—to best knowledge—and proof existence real square matrices. finally discuss rank proposed decomposition. entities truth single relation holding entities represented sign value represents true facts false facts subject entity object entity. probability entities partially-observed sign matrix indexed identical fashion suitable sigmoid function. throughout paper used logistic inverse link function work pursue three objectives ﬁnding generic structure leads computationally eﬃcient model expressive enough approximation common relations real world knowledge graphs good generalization performances within formulation assumed entities appearing subjects diﬀerent entities appearing objects. netﬂix challenge example corresponds user column corresponds movie extensively studied type model closely related singular value decomposition well case matrix rectangular. however many knowledge graph completion problems entity appear subject object diﬀerent embedding vectors depending whether appears subject object relation. seems natural learn unique embeddings entities initially proposed nickel bordes since used systematically prominent approaches factorization setting using embeddings leftright-side factors boils speciﬁc case eigenvalue decomposition orthogonal diagonalization. spectral theorem symmetric matrices tells matrix orthogonally diagonalizable symmetric therefore often used approximate covariance matrices kernel functions distance similarity matrices. however previously stated paper explicitly interested problems matrices—and thus relation patterns represent—can also antisymmetric even particular symmetry pattern order unique embedding entities extend expressiveness asymmetric relations researchers generalised notion products scoring functions also known table scoring functions state-of-the-art latent factor models given fact along representation relation parameters time space complexity. dimensionality embeddings. entity embeddings subject object model except complex complex conjugate additional latent dimension model. denote respectively fourier transform inverse element-wise product vectors denotes real part complex vector denotes composition functions allow general combinations embeddings. brieﬂy recall several examples scoring functions table well extension proposed paper. rescal expressive scoring functions quadratic complexity rank factorization. recently hole model proposes solution quasi-linear complexity time linear space complexity. distmult seen joint orthogonal diagonalization real embeddings hence handling symmetric relations. conversely transe handles symmetric relations price strong constraints embeddings. canonical-polyadic decomposition generalizes poorly diﬀerent embeddings entities subject object. reconcile expressiveness scalability generalization going back realm well-studied matrix factorizations making complex linear algebra scarcely used tool machine learning community. introduce decomposition real square matrices using unitary diagonalization generalization orthogonal diagonalization complex matrices. allows decomposition arbitrary real square matrices unique representations rows columns. ﬁrst recall notions complex linear algebra well speciﬁc cases diagonalization real square matrices building proposition upon results. easy check real symmetric matrices normal pure real eigenvectors eigenvalues. purely real normal matrices also includes real antisymmetric matrices well real orthogonal matrices many matrices useful represent binary relations assignment matrices represent applied knowledge graph completion setting rows vectorial representations entities corresponding rows columns relation score matrix score relation holding true entities hence consider complex embeddings dimension arbitrary values well real-valued twice-bigger counterparts distmult model score figure represents heatmaps scores function complex-valued case function diagonal real-valued case. real-valued case symmetric subject object entities scores equal value diagonal. whereas complex-valued case variation allows score desired pair values. obtained adding purely imaginary constant eigenvalues. rn×n unitary diagonal. real constant general many possible couples matrices preserve real part decomposition. practice however synonym generalization abilities many eﬀective matrix tensor decomposition methods used machine learning lead non-unique solutions case also learned representations prove useful shown experimental section. addressing knowledge graph completion data-driven approaches assumes suﬃcient regularity observed data generalize unobserved facts. formulated matrix completion problem case section implementing hypothesis make assumption matrix rank approximately rank. ﬁrst discuss rank proposed decomposition introduce sign-rank extend bound developed rank sign-rank. proposed complex-valued decomposition plotted function ﬁxed entity embeddings −+i. right scores corresponding real-valued decomposition number free real-valued parameters plotted function diagonal ﬁxed entity embeddings attribute pair scores whereas since necessarily square replace unitary requirement corollary requirement columns form orthonormal basis smallest dimension also given decomposition always exists dimension make generalization possible hypothesize true matrix signrank thus reconstructed sign low-rank score matrix sign-rank assumption theoretically justiﬁed fact sign-rank natural complexity measure sign matrices linked learnability empirically conﬁrmed wide success factorization models previous attempts approximate sign-rank relational learning complex numbers. showed existence compact factorizations conditions sign matrix speciﬁc cases contrast results show square sign matrix sign-rank exactly decomposed k-dimensional unitary diagonalization. know matrix }n×n rank± example identity matrix rank sign-rank swapping columns identity matrix corresponds relation marriedto relation known hard factorize reals since rank invariant row/column permutations. model express rank finding matches sign-rank corresponds ﬁnding smallest brings loss link prediction seen binary classiﬁcation facts. practice classically done machine learning avoid np-hard problem continuous surrogate loss case logistic loss described section validate models diﬀerent values described section matrix normal also easily check real normal -by- matrix sign-pattern clearly rank matrix since columns linearly dependent hence sign-rank also corollary know normal matrix whose real part sign-pattern whose rank however rank unitary diagonalization real part equals otherwise could -by- complex matrix ew¯e w|e| ew¯e w|e| obviously unsatisﬁable. example generalizes n-by-n square sign matrix ﬁrst hence rank argument holds considering example shows upper bound rank unitary diagonalization showed corollaries strictly greater rank sign-rank decomposed matrix. however might examples addition extension multi-relational data extend previous discussion models multiple relations. relations shall write rm×n×n score tensor rn×n score matrix relation }m×n×n given relation entities probability fact symmetric matrix antisymmetric matrix. this rewrite decomposition score matrix matrix notation. write real part matrices primes imaginary parts double primes symmetric antisymmetric. hence model well suited model jointly symmetric antisymmetric relations pairs entities still using entity representations subjects objects. learning simply needs collapse zero antisymmetric relations indeed symmetric purely real position relates simultaneous unitary decomposition. deﬁnition family matrices cn×n simultaneously unitarily diagonalizable single unitary matrix cn×n ewie∗ cn×n diagonal. deﬁnition family normal matrices cn×n commuting family normal matrices xix∗ theorem suppose family matrices cn×n. commuting family normal matrices apply theorem proposed factorization would make hypothesis relation score matrices commuting family strong hypothesis. actually model slightly diﬀerent since take real part tensor factorization. single-relation case taking real part decomposition rids normality requirement theorem decomposition exist shown theorem multiple-relation case open question whether taking real part simultaneous unitary diagonalization enable decompose families arbitrary real square matrices—that single unitary matrix columns. though seems unlikely could counter-example yet. however letting rank tensor factorization greater show proposed tensor decomposition exists families arbitrary real square matrices simply concatenating decomposition theorem real square matrix though comes relational learning might expect actual rank much lower reasons. ﬁrst discussed above dealing sign tensors hence rank matrices need match sign-rank partially-observed matrices second matrices related other represent entities diﬀerent relations thus beneﬁt sharing latent dimensions. opposed construction exposed proof theorem relations dimensions canceled out. practice rank needed generalize well indeed much lower show experimentally figure also note construction proof theorem matrix unitary more. however unitary constraints matrix case serve proof existence solution among inﬁnite ones rank. practice imposing orthonormality essentially numerical commodity decomposition dense matrices iterative methods example comes matrix tensor completion thus generalisation imposing constraints numerical hassle anything else especially gradient methods. apparent link orthonormality generalisation properties impose constraints learning model following experiments. algorithm describes stochastic gradient descent learn proposed multirelational model adagrad learning-rate updates refer proposed model complex complex embeddings. expose version algorithm uses real-valued vectors order facilitate implementation. separate real-valued representations real imaginary parts embeddings. real imaginary part vectors initialized vectors zero-mean normal distribution unit variance. training contains positive triples negatives generated batch using local closed-world assumption bordes triple randomly change either subject object form negative example. case parameter sets number negative triples generate positive triple. collision positive triples checked occurs rarely real world knowledge graphs largely sparse also computationally expensive. squared gradients accumulated compute adagrad learning rates gradients updated. every iterations parameters evaluated evaluation function algorithm data contains positive negative examples average precision used evaluate model. data contains positives mean reciprocal rank used average precision cannot computed without true negatives. optimization process stopped measure considered decreases compared last evaluation algorithm stochastic gradient descent adagrad complex model input training validation learning rate rank regularization factor negative ratio batch size maximum iteration validate every iterations adagrad regularizer previous score evaluated method proposed paper synthetic real data sets. synthetic data contains symmetric antisymmetric relations whereas real data sets standard link prediction benchmarks based real knowledge graphs. compared complex state-of-the-art models namely transe distmult rescal also canonical polyadic decomposition emphasize empirically importance learning unique embeddings entities. experimental fairness reimplemented models within framework complex model using theanobased implementation transe model results obtained original max-margin loss turned yield better results model only. max-margin loss data sets observed negatives positive triples replicated necessary match number negative triples described garcia-duran models trained negative log-likelihood logistic model following experiments used maximum number iterations batch size validated models early stopping every iterations. assess claim complex accurately model jointly symmetry antisymmetry randomly generated knowledge graph relations entities. relation entirely symmetric completely antisymmetric. data tensor symmetric slice antisymmetric slice decomposed training validation test sets. ensure test values predictable upper triangular parts matrices always kept training diagonals unobserved. conducted -fold cross-validation lower-triangular matrices using uppertriangular parts plus folds training fold validation fold testing. training contains observed triples whereas validation test sets contain triples each. expected distmult able model antisymmetry predicts symmetric relations correctly. although transe symmetric model performs poorly practice particularly antisymmetric relation. rescal large number parameters quickly overﬁts rank grows. canonical polyadic decomposition fails relations push symmetric antisymmetric patterns entity embeddings. surprisingly complex succeeds even simple data. figure parts training validation test sets generated experiment symmetric antisymmetric relation. pixels positive triples blue negatives green missing ones. plots symmetric slice ﬁrst entities. bottom plots antisymmetric slice ﬁrst entities. compare models fully observed data sets contain positive negative triples also called closed-world assumption. kinships data describes diﬀerent kinship relations alyawarra tribe australia among individuals. uniﬁed medical language system data represents medical concepts diseases linked relations describing interactions. metadata data sets summarized table figure average precision factorization rank ranging diﬀerent state-of-the-art models synthetic task. learning performed jointly symmetric relation antisymmetric relation. top-left symmetric relation only. top-right antisymmetric relation only. bottom overall performed -fold cross-validation keeping training validation testing. figure shows best cross-validated average precision ranks ranging error bars show standard deviation runs. regularization data sets complex rescal close slight advantage complex kinships rescal umls. distmult performs poorly many relations antisymmetric umls kinships antisymmetry; power multilinear product—that tensor factorization approach—as transe seen bilinear products importance unique entity embeddings works well. believe separate subject object-entity embeddings works well closed-world assumption amount training data compared number embeddings learn. though fractions positive training examples observed next experiments enforcing unique entity embeddings good generalization. finally evaluated complex data sets well established benchmarks link prediction task. subset freebase curated knowledge graph general facts whereas subset wordnet database featuring lexical relations words. used training validation test splits bordes table summarizes metadata data sets. data sets contain positive triples generated negative samples using local closed-world assumption described section evaluation measure quality ranking test triple among possible subject object substitutions nickel mean reciprocal rank hits standard evaluation measures data sets come ﬂavours ﬁltered. ﬁltered metrics computed removing positive observed triples appear either training validation test ranking whereas metrics remove these. since ranking measures used previous studies generally preferred max-margin ranking loss task chose negative log-likelihood logistic model—as described previous section—as continuous surrogate sign-rank shown learn compact representations several important relations especially transitive relations previously stated tried losses preliminary work indeed training models log-likelihood yielded better results max-margin ranking loss especially fbk—except transe. report ﬁltered ﬁltered hits table evaluated models. hole model recently shown equivalent complex record original results hole reported nickel brieﬂy discuss discrepancy results obtained complex. table filtered mean reciprocal rank models tested data sets. hitsn metrics ﬁltered. *results reported nickel hole model shown equivalent complex score divergence loss function used describes lexical semantic hierarchies concepts contains many antisymmetric relations hypernymy hyponymy part indeed distmult transe models outperformed complex hole respective ﬁltered scores expected models equivalent. table shows ﬁltered reimplemented models relation conﬁrming advantage complex antisymmetric relations losing nothing others. projections relation embeddings visually corroborate results. much pronounced complex model largely outperforms hole ﬁltered hits compared hole. diﬀerence scores models though proved equivalent aforementioned max-margin loss original hole publication performs worse log-likelihood dataset generation negative sample positive experiments. conﬁrmed discussed details trouillon nickel fact distmult yields fairly high scores also task evaluation measures used. dataset involves true facts test never includes opposite facts test fact antisymmetric relations—as opposite fact always false. thus highly scoring opposite fact barely impacts rankings antisymmetric relations. case fully observed experiments opposite fact known complex rescal distmult transe relation name hypernym hyponym member meronym member holonym instance hypernym instance hyponym part part member domain topic synset domain topic member domain usage synset domain usage member domain region synset domain region derivationally related form similar verb group also relations hence many parameters. though probably overﬁts large number relations thus large number parameters learn performs worse less expressive model like distmult. data sets transe largely left behind. illustrates power multilinear product ﬁrst case importance learning unique entity embeddings second. performs especially poorly small number relations magniﬁes subject/object diﬀerence. figure shows ﬁltered complex model quickly converges data sets showing low-rank hypothesis reasonable practice. little gain performances ranks comprised also shows complex perform better twice many parameters rank—the real imaginary parts—compared linear space complexity models indeed thanks better expressiveness. best ranks generally cases scores always close models suggesting need grid-search higher ranks. number negative samples positive sample also large inﬂuence ﬁltered much data sets regularization important found initial learning rate important much think also explain large improvement complex provides data compared previously published results—as distmult results also better previously reported —along log-likelihood objective. seems general adagrad relatively insensitive initial learning rate perhaps causing overconﬁdence ability tune step size online consequently leading less eﬀorts selecting initial step size. defended section linear time space complexity becomes critical dataset grows. illustrate this report figure evolution ﬁltered validation function time best validated hyper-parameters model. convergence criterion used decrease validation ﬁltered mrr—computed every iterations—with maximum number iterations models linear complexity except rescal quadratic rank decomposition learns matrix embedding relation models reach convergence reasonable time minutes hour minutes. diﬀerence rescal models sharp there ﬁrst optimal embedding size lower compared figure evolution ﬁltered validation function time model best hyper-parameters. best rank reported legend. final black marker indicates maximum number iterations reached rescal took four days train whereas models took minutes hours. days might seem manageable could case larger data sets small subset freebase contains figure inﬂuence number negative triples generated positive training example ﬁltered test training time convergence complex model times given relative training time negative triple generated positive training sample proved help generated negative positive. explore proportions increasing number generated negatives leads better results fbk. ﬁxed best validated obtained previous figure shows inﬂuence number generated negatives positive training triple performance complex fbk. generating negatives clearly improves results negative triples ﬁltered hits decreasing negatives probably large class imbalance. model also converges fewer epochs compensates partially additional training time epoch negatives. grows linearly number negatives increases. used principal component analysis visualize embeddings relations wordnet data plotted four ﬁrst components best distmult complex model’s embeddings figures complex model simply concatenated real imaginary parts embedding. figure plots ﬁrst second components relations embeddings using principal component analysis. arrows link labels point. complex embeddings. bottom distmult embeddings. opposite relations clustered together distmult correctly separated complex. figure plots third fourth components relations embeddings using principal component analysis. arrows link labels point. complex embeddings. bottom distmult embeddings. opposite relations clustered together distmult correctly separated complex. relations describe hierarchies thus antisymmetric. hierarchic relations inverse relation data set. example hypernym hyponym part part synset domain topic member domain topic. since distmult unable model antisymmetry correctly represent nature pair opposite relations direction relations. loosely speaking hypernym hyponym pair nature sharing semantics direction entity generalizes semantics other. makes distmult representing opposite relations close embeddings. especially striking third fourth principal component conversely complex manages oppose spatially opposite relations. factorization methods applied representation decomposition generally chosen accordance data despite fact real square matrices eigenvalues complex domain. indeed machine learning community data usually real-valued thus eigendecomposition used symmetric matrices decompositions singular value decomposition non-negative matrix factorization canonical polyadic decomposition comes tensors conversely signal processing data often complex-valued complex-valued counterparts decompositions used. joint diagonalization also much common tool machine learning decomposing sets dense square matrices works recommender systems complex numbers encoding facility merge real-valued relations similarity liking single complex-valued matrix decomposed complex embeddings still unlike work real data decomposed complex domain. deep learning danihelka proposed lstm extended associative memory based complex-valued vectors memorization tasks complex-valued neural network speech synthesis. cases again data ﬁrst encoded complex vectors network. conversely contributions work suggests processing real-valued data complex-valued representation projection onto real-valued subspace simple increasing expressiveness model considered. many knowledge graphs recently arisen pushed recommendation resource description framework data representation. examples knowledge graphs include dbpedia freebase google knowledge vault motivating applications knowledge graph completion include question answering generally probabilistic querying knowledge bases ﬁrst embedding models asymmetry relations quickly seen problem asymmetric extensions tensors studied mostly either considering independent embeddings considering relations matrices instead vectors rescal model direct extensions based uni-bitrigram latent factors triple data well low-rank relation matrix bordes propose two-layer model subject object embeddings ﬁrst separately combined relation embedding intermediate representation combined ﬁnal score. pairwise interaction models also considered improve prediction performances. example universal schema approach factorizes unfolding tensor welbl extend also pairs. riedel also consider augmenting knowledge graph facts exctracting textual data toutanova injecting prior knowledge form horn clauses objective loss universal schema model also considered chang enhance rescal model take account information entity types. recommender systems baruch proposed noncommutative extension decomposition model. recently gaifman models learn neighborhood embeddings local structures knowledge graph showed competitive performances neural tensor network model socher combine linear transformations multiple bilinear forms subject object embeddings jointly feed nonlinear neural layer. non-linearity multiple ways including interactions embeddings gives advantage expressiveness models simpler scoring function like distmult rescal. downside large number parameters make model harder train overﬁt easily. original multilinear distmult model symmetric subject object every relation achieves good performance data sets. however likely absence true negatives data sets discussed section transe model bordes also embeds entities relations space imposes geometrical structural bias model subject entity vector close object entity vector translated relation vector. recent novel handle antisymmetry holographic embeddings hole circular correlation used combining model nickel entity embeddings measuring covariance embeddings diﬀerent dimension though decomposition proposed paper clearly unique able learn meaningful representations. still characterizing possible unitary diagonalizations preserve real part interesting open question. especially approximation setting constrained rank order characterize decompositions minimize given reconstruction error. might allow creation iterative algorithm similar eigendecomposition iterative methods computing decomposition given real square matrix. proposed decomposition could also applications many asymmetric square matrices decompositions applications spectral graph theory directed graphs also factorization asymmetric measures matrices asymmetric distance matrices asymmetric similarity matrices optimization point view objective function clearly non-convex could indeed reaching globally optimal decomposition using stochastic gradient descent. recent results show spurious local minima completion problem positive semi-deﬁnite matrix studying extensibility results decomposition another possible line future work. ﬁrst step would generalizing results symmetric real-valued matrix completion generalization normal matrices straightforward. last steps would extending matrices expressed real part normal matrices ﬁnally joint decomposition matrices tensor. indeed noticed remarkable stability scores across diﬀerent random initialization complex hyper-parameters suggests possibility theoretical property. practically obvious extension merge approach known extensions tensor factorization models order improve predictive performance. example pairwise embeddings together complex numbers might lead improved results many situations involve non-compositionality. adding bigram embeddings objective could also improve results shown models another direction would develop intelligent negative sampling procedure generate informative negatives respect positive sample sampled. would reduce number negatives required reach good performance thus accelerating training time. extension relations entities n-tuples straightforward complex’s expressiveness comes complex conjugation object-entity breaks symmetry subject object embeddings scoring function. stems hermitian product seems standard multilinear extension linear algebra literature question hence remains largely open. described matrix tensor decomposition complex-valued latent factors called complex. decomposition exists real square matrices expressed real part normal matrices. result extends sets real square matrices—tensors— answers requirements knowledge graph completion task handling large variety diﬀerent relations including antisymmetric asymmetric ones scalable. experiments conﬁrm theoretical versatility substantially improves state-of-the-art real knowledge graphs. shows real world relations eﬃciently approximated real part low-rank normal matrices. generality theoretical results eﬀectiveness experimental ones motivate application real square matrices factorization problems. generally hope paper stimulate complex linear algebra machine learning community even especially processing real-valued data. work supported part association nationale recherche technologie cifre grant part paul allen foundation allen distinguished investigator grant part google focused research award. would like thank ariadna quattoni st´ephane clinchant jean-marc andr´eoli soﬁa michel alejandro blumentals l´eo hubert pierre comon helpful comments feedback.", "year": 2017}