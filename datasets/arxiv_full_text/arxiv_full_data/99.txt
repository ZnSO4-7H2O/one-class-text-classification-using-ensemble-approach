{"title": "Long Short-Term Memory Based Recurrent Neural Network Architectures for  Large Vocabulary Speech Recognition", "tag": ["cs.NE", "cs.CL", "cs.LG", "stat.ML"], "abstract": "Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.", "text": "architecture lstms contained special units called memory blocks recurrent hidden layer. memory blocks contain memory cells self-connections storing temporal state network addition special multiplicative units called gates control information. memory block contains input gate controls input activations memory cell output gate controls output cell activations rest network. later address weakness lstm models preventing processing continuous input streams segmented subsequences would allow resetting cell states begining subsequences forget gate added memory block forget gate scales internal state cell adding input cell self recurrent connection cell therefore adaptively forgetting resetting cell’s memory. besides modern lstm architecture contains peephole connections internal cells gates cell learn precise timing outputs lstms conventional rnns successfully applied sequence prediction sequence labeling tasks. lstm models shown perform better rnns learning contextfree context-sensitive languages bidirectional lstm networks similar bidirectional rnns operating input sequence direction make decision current input proposed phonetic labeling acoustic frames timit speech database online ofﬂine handwriting recognition bidirectional lstm networks connectionist temporal classiﬁcation output layer using forward backward type algorithm allows network trained unsegmented sequence data shown outperform state hmm-based system recently following success dnns acoustic modeling deep lstm stack multiple lstm layers combined output layer transducer predicting phone sequences shown state results phone recognition timit database language modeling conventional obtained signiﬁcant reduction perplexity standard n-gram models dnns shown state performance phone recognition large vocabulary speech recognition application lstm networks limited phone recognition timit database required using additional techniques models transducer obtain better results dnns. paper show lstm based architectures obtain state performance large vocabulary speech recognition system thousands context dependent states. proposed architectures modify standard architecture lstm networks make better model parameters addressing computational efﬁciency problems large networks. long short-term memory recurrent neural network architecture designed address vanishing exploding gradient problems conventional rnns. unlike feedforward neural networks rnns cyclic connections making powerful modeling sequences. successfully used sequence labeling sequence prediction tasks handwriting recognition language modeling phonetic labeling acoustic frames. however contrast deep neural networks rnns speech recognition limited phone recognition small scale tasks. paper present novel lstm based architectures make effective model parameters train acoustic models large vocabulary speech recognition. train compare lstm models various numbers parameters conﬁgurations. show lstm models converge quickly give state speech recognition performance relatively small sized models. unlike feedforward neural networks deep neural networks architecture recurrent neural networks cycles feeding activations previous time steps input network make decision current input. activations previous time step stored internal state network provide indeﬁnite temporal contextual information contrast ﬁxed contextual windows used inputs ffnns. therefore rnns dynamically changing contextual window sequence history rather static ﬁxed size window sequence. capability makes rnns better suited sequence modeling tasks sequence prediction sequence labeling tasks. however training conventional rnns gradient-based back-propagation time technique difﬁcult vanishing gradient exploding gradient problems addition problems limit capability rnns model long range context dependencies discrete time steps relevant input signals output. original manuscript submitted icassp conference november rejected content reference page. version slightly edited reﬂect latest experimental results. standard architecture lstm networks input layer recurrent lstm layer output layer. input layer connected lstm layer. recurrent connections lstm layer directly cell output units cell input units input gates output gates forget gates. cell output units connected output layer network. total number parameters standard lstm network cell memory block ignoring biases calculated follows number memory cells number input units number output units. computational complexity learning lstm models weight time step stochastic gradient descent optimization technique therefore learning computational complexity time step learning time network relatively small number inputs dominated factor. tasks requiring large number output units large number memory cells store temporal contextual information learning lstm models become computationally expensive. alternative standard architecture propose novel architectures address computational complexity learning lstm models. architectures shown figure them connect cell output units recurrent projection layer connects cell input units gates recurrency addition network output units prediction outputs. hence number parameters model number units recurrent projection layer. addition recurrent projection layer another non-recurrent projection layer directly connected output layer. model parameters number units non-recurrent projection layer allows increase number units projection layers without increasing number parameters recurrent connections note projection layers regard output units effectively equivalent single projection layer units. terms denote weight matrices terms denote bias vectors logistic sigmoid function respectively input gate forget gate output gate cell activation vectors size cell output activation vector element-wise product choose implement proposed lstm architectures multicore single machine rather gpu. decision based cpu’s relatively simpler implementation complexity ease debugging. implementation also allows easier distributed implementation large cluster machines learning time large networks becomes major bottleneck single machine matrix operations eigen matrix library templated library provides efﬁcient implementations matrix operations using vectorized instructions implemented activation functions gradient calculations matrices using simd instructions beneﬁt parallelization. asynchronous stochastic gradient descent optimization technique. update parameters gradients done asynchronously multiple threads multi-core machine. thread operates batch sequences parallel computational efﬁciency instance matrix-matrix multiplications rather vector-matrix multiplications stochasticity since model parameters updated multiple input sequence time. addition batching sequences single thread training multiple threads effectively chronously processing partition data thread computing gradient step subsequences different utterances. time step used forward-propagate activations backward-propagate gradients using truncated bptt learning algorithm. units hidden layer rnns logistic sigmoid activation function. rnns recurrent projection layer architecture linear activation units projection layer. lstms hyperbolic tangent activation cell input units cell output units logistic sigmoid input output forget gate units. recurrent projection optional non-recurrent projection layers lstms linear activation units. input lstms rnns frame -dimensional log-ﬁlterbank energy features since information future frames helps making better decisions current frame consistent dnns delay output state label frames. truncated backpropagation time learning algorithm update model parameters ﬁxed time step tbptt forward-propagate activations backward-propagate gradients. learning process split input sequence vector subsequences size tbptt. subsequences utterance processed original order. first calculate forward-propagate activations iteratively using network input activations previous time step tbptt time steps starting ﬁrst frame calculate network errors using network cost function time step. then calculate back-propagate gradients crossentropy criterion using errors time step gradients next time step starting time tbptt. finally gradients network parameters accumulated tbptt time steps weights updated. state memory cells processing subsequence saved next subsequence. note processing multiple subsequences different input sequences subsequences shorter tbptt since could reach sequences. next batch subsequences replace subsequences input sequence reset state cells them. networks trained million utterance dataset consisting anonymized hand-transcribed google voice search dictation trafﬁc. dataset represented frames -dimensional log-ﬁlterbank energy features computed every utterances aligned million parameter ffnn states. train networks three different output states inventories obtained mapping states smaller state inventories equivalence classes. state context independent states weights networks training randomly initialized. learning rate speciﬁc network architecture conﬁguration largest value results stable convergence. learning rates exponentially decayed training. training evaluate frame accuracies held development frames. trained models evaluated speech recognition system test hand-transcribed utterances word error rates reported. vocabulary size language model used decoding million. dnns trained minibatch size frames graphics processing unit network fully connected logistic sigmoid hidden layers softmax output layer representing phone states. consistency lstm architectures networks low-rank projection layer dnns inputs consist stacked frames asymmetrical window frames right either frames left figure show frame accuracy results state outputs respectively. ﬁgures name network conﬁguration contains information network size architecture. states number memory cells lstms number units hidden layer rnns. states number recurrent projection units lstms rnns. states number non-recurrent projection units lstms. conﬁguration names state left context right context size number hidden layers number units hidden layers optional low-rank projection layer size number parameters model given parenthesis. evaluated rnns state output conﬁguration since performed signiﬁcantly worse dnns lstms. seen figure rnns also unstable beginning training achieve convergence limit activations gradients exploding gradient problem. lstm networks give much better frame accuracy rnns dnns converging faster. proposed lstm projected architectures give signiﬁcantly better accuracy standard lstm architecture number parameters compare lstm lstm figure lstm network recurrent non-recurrent projection layers generally performs better lstm network recurrent projection layer except state experiment learning rate small. figure show wers models state outputs respectively. note lstm networks converged update results models converge ﬁnal revision paper. speech recognition experiments show lstm networks give improved speech recognition accuracy context independent output state model context dependent output state embedded size model relatively large output state model. seen figure proposed architectures essential obtaining better recognition accuracies dnns. also experiment show depth important dnns compare figure know paper presents ﬁrst application lstm networks large vocabulary speech recognition task. address scalability issue lstms large networks large number output units introduce architecutures make effective model parameters standard lstm architecture. proposed architectures introduces recurrent projection layer lstm layer output layer. introduces another non-recurrent projection layer increase projection layer size without adding recurrent connections decoupling provides ﬂexibility. show proposed architectures improve performance lstm networks signiﬁcantly standard lstm. also show proposed lstm architectures give better performance dnns large vocabulary speech recognition task large number output states. training lstm networks single multi-core machine scale well larger networks. investigate gpudistributed cpu-implementations similar address that. navdeep jaitly patrick nguyen andrew senior vincent vanhoucke application pretrained deep neural networks large vocabulary speech recognition proceedings interspeech tom´aˇs mikolov martin karaﬁ´at luk´aˇs burget ˇcernock´y sanjeev khudanpur recurrent neural network based language model proceedings interspeech. vol. international speech communication association. jeffrey dean greg corrado rajat monga chen matthieu devin quoc mark marc’aurelio ranzato andrew senior paul tucker yang andrew large scale distributed deep networks. nips t.n. sainath kingsbury sindhwani arisoy ramabhadran low-rank matrix factorization deep neural network training high-dimensional output targets proc. icassp alex graves j¨urgen schmidhuber framewise phoneme classiﬁcation bidirectional lstm neural network architectures neural networks vol. alex graves marcus liwicki santiago fernandez roman bertolami horst bunke j¨urgen schmidhuber novel connectionist system unconstrained handwriting recognition pattern analysis machine intelligence ieee transactions vol. abdel rahman mohamed george dahl geoffrey hinton acoustic modeling using deep belief networks ieee transactions audio speech language processing vol.", "year": 2014}