{"title": "Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent  Networks", "tag": ["cs.AI", "cs.LG", "cs.NE", "stat.ML"], "abstract": "A major drawback of backpropagation through time (BPTT) is the difficulty of learning long-term dependencies, coming from having to propagate credit information backwards through every single step of the forward computation. This makes BPTT both computationally impractical and biologically implausible. For this reason, full backpropagation through time is rarely used on long sequences, and truncated backpropagation through time is used as a heuristic. However, this usually leads to biased estimates of the gradient in which longer term dependencies are ignored. Addressing this issue, we propose an alternative algorithm, Sparse Attentive Backtracking, which might also be related to principles used by brains to learn long-term dependencies. Sparse Attentive Backtracking learns an attention mechanism over the hidden states of the past and selectively backpropagates through paths with high attention weights. This allows the model to learn long term dependencies while only backtracking for a small number of time steps, not just from the recent past but also from attended relevant past states.", "text": "rosemary anirudh goyal olexa bilaniuk jonathan binas laurent charlin chris yoshua bengio† mila universit´e montr´eal polytechnique montr´eal montr´eal †cifar senior fellow major drawback backpropagation time difﬁculty learning long-term dependencies coming propagate credit information backwards every single step forward computation. makes bptt computationally impractical biologically implausible. reason full backpropagation time rarely used long sequences truncated backpropagation time used heuristic. however usually leads biased estimates gradient longer term dependencies ignored. addressing issue propose alternative algorithm sparse attentive backtracking might also related principles used brains learn long-term dependencies. sparse attentive backtracking learns attention mechanism hidden states past selectively backpropagates paths high attention weights. allows model learn long term dependencies backtracking small number time steps recent past also attended relevant past states. recurrent neural networks state-of-the-art many machine learning sequence processing tasks. examples models based rnns shine include speech recognition image captioning machine translation speech synthesis common practice train models using backpropagation time wherein network states unrolled time gradients backpropagated unrolled graph. since parameters shared across different time steps bptt prone vanishing exploding gradients equivalent deep feedforward networks many stages. makes credit assignment particularly difﬁcult events occurred many time steps past thus makes challenging practice capture long-term dependencies data wait sequence order compute gradients neither practical machines animals dependencies extend long times. considerably slows training instance consider student taking test outcome depends decision several months back past much study. student using bptt training signal would need travel many time steps student actively thinking test. however human would probably learn change early decision when failing test remembers inspiration proposed approach. truncated bptt gradients backpropagated ﬁxed limited number time steps parameters updated subsequence. truncation often motivated computational concerns memory computation time advantage faster learning obtained making frequent updates parameters rather wait sequence. however makes capturing correlations across distant states even harder. student example before student learning using tbptt gradient signal would hidden states student made relevant decision much study test. regular rnns parametric hidden state vector ﬁxed size. believe critical element classical analysis difﬁculty learning long-term dependencies indeed ﬁxed state dimension becomes bottleneck information forward backward. thus propose semi-parametric next state potentially conditioned previous states making possible—thanks attention—to jump distance time. distinguish three types states proposed semi-parametric many hidden states timesteps sequence analyzed rnn. subset become microstates subset called macrostate. computation next hidden state based whole macrostate addition external input macrostate variable-length must devise special mechanism read ever-growing array. component model propose attention mechanism microstate elements macrostate. attention mechanism setting regarded providing adaptive dynamic skip connections past microstate linked dynamic decision current hidden state. skip connections allow information propagate long sequences. architectures naturally make easier learn long-term dependencies. name algorithm sparse attentive backtracking especially well-suited sequences parts task closely related occur apart time. ideinference involves examining macrostate selecting microstates. ally select microstates instead attending salient relevant ones attention mechanism select number relevant microstates incorporated hidden state. training local backpropagation gradients happens short window time around selected microstates only. allows updates asynchronous respect time steps attend credit assignment takes place globally proposed algorithm. competitive results compared full backpropagation time much better results compared truncated backpropagation time signiﬁcantly shorter truncation windows model. mechanisms also biologically plausible. imagine taken wrong turn roadtrip ﬁnding several miles later. mental focus would likely shift directly location time space made wrong decision without replaying reverse detailed sequence experienced trafﬁc landscape impressions. neurophysiological ﬁndings support existence attention mechanisms involvement credit assignment learning biological systems. particular hippocampal recordings rats indicate brief sequences prior experience replayed awake resting state sleep conditions linked memory consolidation learning moreover observed replay events modulated reward animal receive task sense pronounced presence reward signal less pronounced absent absence reward signal thus mental look back past seems occur exactly credit assignment performed. training long sequences full backpropagation time becomes computationally expensive considerably slows training forcing learner wait making parameter update. common heuristic backpropagate loss particular time step limited number time steps hence truncate backpropagation computation graph truncated backpropagation time heavily used practice inability perform credit assignment longer sequences limiting factor algorithm resulting failure cases even simple tasks copying memory adding task decoupled neural interfaces method replaces full backpropagation time synthetic gradients essentially small networks mapping hidden unit values layer estimator gradient ﬁnal loss respect layer. training synthetic gradient module requires backpropagation layer make approximate gradient updates parameters asynchronous using synthetic gradient module. thus network learns credit assignment particular layer examples gradients backpropagation reducing total number times backpropagation needs performed. online credit assignment rnns without backtracking remains open research problem. approach attempts solve problem estimating gradients using approximation forward mode automatic differentiation instead backpropagation. forward mode automatic differentiation allows computing unbiased gradient estimates online fashion however normally requires storage gradient current hidden state values respect parameters number hidden units. unbiased online recurrent optimization method gets around updating rank- approximation gradient tensor shown keep estimate gradient unbiased potentially risk increasing variance gradient estimator. neural architectures residual networks dense networks allow information skip convolutional processing blocks underlying convolutional network architecture. case residual networks identity connections used skip convolutional processing blocks information recombined using addition. construction provably mitigates vanishing gradient problem allowing gradient given layer bounded. densely-connected convolutional networks alleviate vanishing gradient problem allowing direct path point network output. contrast propose explore might regard form dynamic skip connection modulated attention mechanism. sparse attentive backtracking introduce idea sparse attentive backtracking classical models based lstms grus previous hidden state computation next therefore struggle extremely long-range dependencies. sidesteps limitation additionally allowing model select past microstates computation next hidden state. model potentially reference microstates computed arbitrarily long time. figure ﬁgure illustrates forward pass conﬁguration ktop involves microstate selection summarizakatt ktrunc tion microstates incorporation next microstate evaluated ﬁrst arrows depict attention weights broadcasting current provisional hidden state macrostate presented case katt consists past hidden states) namely weighted summed incorporated compute current ﬁnal hidden state since classic models support operations past make architectural additions. forward pass training step mechanism introduced selects microstates macrostate summarizes them incorporates summary next hidden state. hidden state become microstate. backward pass gradient allowed master chain linking consecutive hidden states also microstates selected forward pass. forward pass microstate selection process denser sparser summarization incorporation less sophisticated. backward pass gating gradient hidden state ancestor microstates also denser sparser although denser forward pass was. instance possible forward pass dense incorporating summary microstates backward pass sparse allowing gradient microstate contributors hidden state another possibility forward pass sparse making hard microstate selections summary. case backward pass necessarily also sparse since microstates contributed hidden state therefore loss noteworthy hidden states need eligible become microstates. practice found restricting pool eligible hidden states every katt’th still works well reducing memory computation expense. increase granularity microstate selection also improve performance preventing model attending exclusively recent hidden states temporally spreading microstates other. algorithm widely applicable compatible numerous architectures including vanilla lstm models. however since necessarily requires altering hiddenfigure ﬁgure illustrates backward pass conﬁguration ktop katt ktrunc gradients passed microstates selected forward pass local truncated backprop performed around microstates. blue arrows show gradient backward pass. crosses indicate tbptt truncation points gradient stops backpropagated. vanilla gru-inspired architectures sab’s selection incorporation mechanisms operate state. lstm architecture adopt experiments operate hidden state cell state. microstate selection microstate selection mechanism determines microstate subset macrostate selected summarization forward pass subset subset receive gradient backward pass training. makes core attention mechanism implementation. selection mechanism hard-coded attention heuristics reason microstate selection mechanism could neural network trained alongside model operates. models here selection mechanism chosen linear transformation computes scalar attention weight eligible microstate vector sparsiﬁer masks ktop greatest attention weights producing sparse attention weights ˜ai. empirically demonstrate even simple mechanism learns focus past time steps relevant current thus successfully performing credit assignment. higher complexity model would interesting avenue future research. summarization microstates selected microstates must somehow combined ﬁxed-size summary incorporation next hidden state. many options exist choose simply perform summation microstates weighted sparsiﬁed attention weight ˜ai. lastly summary must incorporated hidden state. again multiple options exist addition concatenation purposes choose simply summary provisional hidden state output computed lstm cell produce ﬁnal hidden state conditioned upon next timestep. give equations speciﬁc sab-augmented lstm model experiments. time underlying lstm receives vector hidden states vector cell states input computes provisional hidden state vector also serves provisional output. next attention mechanism similar modiﬁed produce sparse discrete attention decisions. first provisional hidden state vector concatenated microstate vector then afﬁne transformation maps concatenated vector attention weight representing salience microstate current time equivalently expressed weights matrices bias vector learned parameters. following this apply piece-wise linear function sparsiﬁes attention making discrete decisions. whose output never sparse). ktop ktopth greatest-valued attention weight time sparsiﬁed attention weights computed effect zeroing attention weights less ktop thus masking ktop salient microstates selected microstates receive gradient information gradient ﬂows rest. summary given time step hidden state selected hard-attention mechanism paths contributing hidden states forward pass. path regular sequential forward path rnn; path dynamic skip connections attention mechanism. perform backpropagation skip connections gradient ﬂows microstates selected attention mechanism experiments report discuss results empirical study analyses performance using different tasks. ﬁrst study synthetic tasks—the copying adding problems designed measure models’ abilities learn long-term dependencies— meant conﬁrm successfully perform credit assignment events occurred many time steps past. study realistic tasks larger datasets. baselines compare quantitative performance model lstm baselines ﬁrst trained backpropagation time second trained using truncated backpropagation time methods trained using teacher forcing also used gradient clipping hyperparameters task-speciﬁc discussed tasks’ respective subsections hyperparameters also used value discussed below. compared standard rnns model additional hyperparameters addition also study impact tbptt truncation length denote ktrunc. determines many timesteps backwards propagate gradients backward pass. effect hyperparameter also studies lstm tbtpp baseline. synthetic adding language modelling task sequential mnist classiﬁcation tasks reaches performance bptt outperforms tbptt. addition adding task outperforms tbptt using much shorter truncation lengths. copying memory task tests model’s ability memorize salient information long time periods. follow setup copying memory problem hochreiter schmidhuber details network given sequence inputs consisting digits followed blank inputs followed special end-ofsequence character followed additional blank inputs. end-of-sequence character network must output copy initial digits. tables report accuracy cross-entropy models’ predictions unseen sequences. note able learn copy task almost perfectly sequence-lengths further outperforms baselines. particularly noticeable longer sequences example best baseline achieves accuracy versus sab’s better understand learning process visualized attention weights learning copying task figure shows attention weights three different learning stages training within ﬁrst epoch. note attention quickly focuses ﬁrst timesteps contain input digits. adding task adding task requires model speciﬁc entries sequence entries spirit copying task larger values require model keep track longer-term dependencies. exact setup follows. example task consists input vectors length ﬁrst vector uniformly generated values second vector encodes binary mask indicates entries ﬁrst input mask randomly generated constraint masked-in entries must different halves ﬁrst input vector. table test accuracy cross-entropy loss performance copying task sequence lengths models tbptt cannot solve task bptt achieve optimal performance. tables report cross-entropy model’s predictions unseen sequences. sab’s performance similar best performance baselines. even longer sequences outperforms tbptt outperformed bptt. evaluate model language modelling task using penn treebank dataset lstm baselines hidden units learning rate used nonoverlapping sequences batches trained epochs. dataset derived text wikipedia consists sequence total characters follow setup mikolov ﬁrst characters training next validation ﬁnal characters testing. train non-overlapping sequences length computational constraints baselines hidden units. trained models using batch size trained maximum epochs. done hyperparameter search model it’s computationally expensive. last task sequential version mnist classiﬁcation dataset. task involves predicting label image given image pixel pixel models lstm hidden units. prediction produced passing ﬁnal hidden state network softmax. used learning rate trained model epochs early stopping based validation set. table shows performs well bptt. future work interesting direction future development sparse attentive backtracking method machine learning standpoint would improving computational efﬁciency sequences question long. since sparse attentive backtracking method uses selfattention every step memory requirement grows linearly length sequence computing attention mechanism requires computing scalar current hidden states previous hidden states might possible reduce memory requirement using hierarchical model done chandar recomputing states lower levels hierarchy attention mechanism looks corresponding higher level hierarchy. might also possible reduce computational cost attention mechanism considering maximum inner product search algorithm instead naively computing inner product hidden states values past. conclusion improving modeling long-term dependencies central challenge sequence modeling exact gradient computation bptt biologically plausible well inconvenient computationally realistic applications. this widely used algorithm training recurrent neural networks long sequences truncated backpropagation time known produced biased estimates gradient focusing short-term dependencies. proposed sparse attentive backtracking biologically motivated algorithm aims combine strengths full backpropagation time truncated backpropagation time. backpropagating gradients paths selected attention mechanism. allows learn long-term dependencies full backpropagation time still allowing backtrack steps truncated backpropagation time thus making possible update weights frequently needed rather wait long sequences. acknowledgments authors would like thank hugo larochelle walter senn alex lamb remi priol matthieu courbariaux gaetan marceau caron useful discussions well nserc cifar google samsung nuance canada research chairs funding compute canada nvidia computing resources. authors would also like thank alex lamb code review. authors would also like express debt gratitude towards contributed theano years making great tool. william chan navdeep jaitly quoc oriol vinyals. listen attend spell neural network large vocabulary conversational speech recognition. acoustics speech signal processing ieee international conference ieee sarath chandar sungjin hugo larochelle pascal vincent gerald tesauro yoshua bengio. hierarchical memory networks. corr abs/. http//arxiv. org/abs/.. kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition sepp hochreiter. vanishing gradient problem learning recurrent neural nets problem solutions. international journal uncertainty fuzziness knowledge-based systems jaderberg wojciech marian czarnecki simon osindero oriol vinyals alex graves koray kavukcuoglu. decoupled neural interfaces using synthetic gradients. arxiv preprint arxiv. soroush mehri kundan kumar ishaan gulrajani rithesh kumar shubham jain jose sotelo aaron courville yoshua bengio. samplernn unconditional end-to-end neural audio generation model. arxiv preprint arxiv. yajie miao mohammad gowayyed florian metze. eesen end-to-end speech recognition using deep models wfst-based decoding. automatic speech recognition understanding ieee workshop ieee george saon hagen soltau ahmad emami michael picheny. unfolded recurrent neural networks speech recognition. fifteenth annual conference international speech communication association anshumali shrivastava ping asymmetric sublinear time maximum inner product search ghahramani welling cortes lawrence weinberger advances neural information processing systems curran associates inc. oriol vinyals alexander toshev samy bengio dumitru erhan. show tell neural image caption generator. proceedings ieee conference computer vision pattern recognition kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhudinov rich zemel yoshua bengio. show attend tell neural image caption generation visual attention. international conference machine learning time complexity forward pass training inference number timesteps size hidden state although current implementation scales space complexity forward pass training unchanged space complexity inference rather however time cost backward pass training cost difﬁcult formulate. hidden states depend sparse subset past microstates past microstates depend several other even earlier microstates. active connections therefore akin directed acyclic graph quite possible worst case backpropagation starting last hidden state touch past microstates several times. however number microstates truly relevant task attention mechanism repeatedly focus exclusion others pathological runtimes encountered. method approximates true gradient sense it’s different kind approximation made truncated gradient except instead truncating last ktrunc time steps truncate skip-step past arbitrarily past. provides combating exploding vanishing gradient problems learning long-term dependencies. verify fact model datasets without gradient clipping. empirically found need gradient clipping text dataset datasets observed little difference gradient clipping. visualize attention weights changes training copying memory task section attention weights averaged batch. salient information copying task ﬁrst steps. ﬁgure shows attention learns move towards concentrate beginning sequence training procedes. note happened ﬁrst epoch training model learns reasonable amount time. figure ﬁgure shows attention weights change time copying task copy length vertical axis time step attending timestep timestep horizontal axis time step attended. subﬁgure attention plot iteration epoch subﬁgure iteration epoch subﬁgure iteration epoch", "year": 2017}