{"title": "Separate Training for Conditional Random Fields Using Co-occurrence Rate  Factorization", "tag": ["cs.LG", "cs.AI"], "abstract": "The standard training method of Conditional Random Fields (CRFs) is very slow for large-scale applications. As an alternative, piecewise training divides the full graph into pieces, trains them independently, and combines the learned weights at test time. In this paper, we present \\emph{separate} training for undirected models based on the novel Co-occurrence Rate Factorization (CR-F). Separate training is a local training method. In contrast to MEMMs, separate training is unaffected by the label bias problem. Experiments show that separate training (i) is unaffected by the label bias problem; (ii) reduces the training time from weeks to seconds; and (iii) obtains competitive results to the standard and piecewise training on linear-chain CRFs.", "text": "standard training method conditional random fields slow large-scale applications. alternative piecewise training divides full graph pieces trains independently combines learned weights test time. paper present separate training undirected models based novel cooccurrence rate factorization separate training local training method. contrast memms separate training unaﬀected label bias problem. experiments show separate training unaffected label bias problem; reduces training time weeks seconds; obtains competitive results standard piecewise training linear-chain crfs. conditional random fields undirected graphical models model conditional probabilities rather joint probabilities. thus crfs need assume unwarranted independence observed variables. crfs deﬁne distribution conditioned whole observed variables. global conditioning allows rich features overlapping global features. crfs successfully applied many tasks natural language processing standard training method calculation global partition function expensive partition function depends model parameters also input data. calculate gradients using forward-backward algorithm intermediate results eﬃciently reused dynamic programming within training instance reused diﬀerent instances. thus calculate scratch instance iteration numerical optimization. linear-chain crfs time complexity standard training method quadratic size label linear number features almost quadratic size training sample tagging experiment standard training time several weeks even though graph simple linear chain. slow training prevents applying crfs large-scale applications. speed training crfs piecewise training decomposes full graph pieces trains independently combines learned weights decoding. training time piecewise training replaces exact global partition function maximum likelihood objective function upper bound approximation. upper bound approximation summation partition functions restricted disjoint pieces edges. pieces generalized edges factors higher arity. whenever pieces tractable partition functions restricted pieces calculated eﬃciently. upper bound piecewise training derived tree-reweighed upper bound upper bound exact global partition function linear combination partition functions restricted tractable subgraphs spanning trees. unsolved problem piecewise training good choice pieces. similar problem choice regions generalized belief propagation welling gives solution using operations region graphs leave free energy sometimes ﬁxed points invariant. experiment results show piecewise training obtains better results three tasks standard training. linear-chain graphs exact surprise approximate training outperform exact training. personal communication cohn attributes exact training overﬁtting data. piecewise training smooth over-ﬁtting model degree. also happens maximum entropy markov models factorize joint distribution small factors. memms suffer label bias problem oﬀsets smoothing eﬀect. paper present separate training. separate training based co-occurrence rate factorization novel factorization method undirected models. separate training ﬁrst factorize full graph small factors using operations cr-f. also means selection factors ﬂexible piecewise training. factors trained separately. contrast directed models memms separate training unaﬀected label bias problem. experiment results show separate training performs comparably standard piecewise training reduces training time radically. co-occurrence rate factorization based elementary probability theory. exponential function pointwise mutual information ﬁrst introduced community church hanks instantiates mutual information speciﬁc events originally deﬁned variables. knowledge present work ﬁrst apply concept factorize undirected graphical models systematic way. singleton contain random variable equal thm. explain reason deﬁne singleton marginal probabilities denominator equals undeﬁned. non-negative quantity clear ﬁrst denotes three random variables contrast second denotes random variables joint random variable following diﬀerent notations distinguish explicitly manipulate variables theorem random variables merged joint random variable xkxk+ factor generated. merge operation used factorize graph inverse partition operation. merging unconnected nodes implies removing conditional independences them. intuitively conditional probability asymmetric concept matches asymmetric properties directed graphs well co-occurrence rate symmetric concept matches symmetric properties undirected graphs well. co-occurrence rate also connects probability factorization graph operations well. training method estimate probabilities factors cr-f frequencies. experiment results show normally method obtains lower accuracy ﬁrst training method method fast method useful large-scale applications. observed training vocabture functions except feature function using word itself. achieve best accuracy method requires additional parameter µoov adweights non-oov probabilities µoov constant parameter oovs. parameter obtained maximizing accuracy held-out dataset. experiments µoov factors learned similar way. parameters denominator local partition function covers possible pairs contrast global normalization local partition functions reused training instances whenever features respect parameters factor learned separately following maximum entropy principle. separate objective function factor. diﬀerent piecewise training learns parameters maximizing single maximum likelihood objective function. estimate parameters training dataset. function convex standard numerical optimization technique e.g. limited-memory bfgs applied achieve global optimum. ﬁrst partial derivative respect given follows generate pairs sequence totally size testing dataset oovs dataset need held-out dataset training µoov. experiment rounds report average accuracy tags correct sequence memms directed models select ﬁrst label accordond observation aﬀect result. observe condition generated data generates probability contrast separate training based co-occurrence rate factorization make correct choice brown corpus tagging. exclude incomplete sentences ending punctuation experimental dataset. results sentences. size space following laﬀerty introduce parameters tagword pair tag-tag pair. also spelling features used laﬀerty whether token begins number upper case letter whether contains hyphen whether ends following suﬃxes -ing -ogy -ion -tion -ity -ies. select sentences held dataset training µoov experiments tagging. ﬁrst experiment subset full corpus sentence corpus three splits results reported tab. tab. tab. respectively. second experiment full coroutgoing transitions others. extreme case possible outgoing transition local conditional probability global normalization proposed laﬀerty keeps crfs away label bias problem. co-occurrence rate factorization also unaﬀected even though local normalized model. reason that contrast memms factors cooccurrence rate factorizations local joint probabilprobabilities transition normalized probability space. transitions treated equally. thus cr-f naturally avoids label bias problem. conﬁrmed experiment results sec. method signiﬁcantly diﬀers memms factorization. implement separate training java. also l-bfgs algorithm packaged mallet numerical optimization. crf++ version piecewise training tool packaged mallet adopted comparison. experiments performed linux workstation single xeon .ghz) working memory. denote ﬁrst separate training method second piecewise training generate paired sequences sequences emits designated symbol probability three symbols probability training generate pairs sequence totally size training dataset testresults show experiments separate training much faster standard training piecewise training achieves better comparable results. tab. shows suﬃcient training data crfs performs better oovs separate training performs slightly better non-oovs. mallet java crf++ time comparison fair also focus paper. split piecewise training converge iterations. named entity recognition also employs linear-chain structure. experiment dutch part conll- named entity recognition corpus. dataset three ﬁles ned.train ned.testa ned.testb. ned.train training ned.testa held-out dataset adjusting µoov ned.testb testing. sentences training. size space sentences held-out dataset http//www.cnts.ua.ac.be/conll/ner/ originally sentences ned.train. piecewise training mallet decode sentences word. exclude single word sentences training testing. task separate training fastest obtains best results. piecewise training obtains slightly better result standard training method consistent results reported sutton mccallum since co-occurrence rate relation includes statement make independence relations surprise rework factorization methods junction tree factorization markov random fields using section sketch obtain factors junction tree mrfs using operations cr-f. separator. result similar obtained shafer-shenoy propagation except factors obtained cr-f local joint probabilities rather positive functions. local joint probabilities normalized locally trained separately. paper proposed novel co-occurrence rate factorization factorizing undirected graphs. based cr-f presented separate training scaling crfs. experiments show separate training unaﬀected label bias problem speeds training radically achieves competitive results standard piecewise training linear-chain graphs. also obtained factors mrfs junction tree using cr-f. shows cr-f general framework factorizing undirected graphs. paper present separate training linearchain graphs. separate training easily extended tree-structured graphs. future generalize separate training loopy graphs. brieﬂy using thm. break loops. node loop partitioned need bring back condition avoid adding edge. keep factorization exact.", "year": 2010}