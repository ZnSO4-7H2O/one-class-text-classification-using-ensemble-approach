{"title": "Ray: A Distributed Framework for Emerging AI Applications", "tag": ["cs.DC", "cs.AI", "cs.LG", "stat.ML"], "abstract": "The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray---a distributed system to address them. Ray implements a dynamic task graph computation model that supports both the task-parallel and the actor programming models. To meet the performance requirements of AI applications, we propose an architecture that logically centralizes the system's control state using a sharded storage system and a novel bottom-up distributed scheduler. In our experiments, we demonstrate sub-millisecond remote task latencies and linear throughput scaling beyond 1.8 million tasks per second. We empirically validate that Ray speeds up challenging benchmarks and serves as both a natural and performant fit for an emerging class of reinforcement learning applications and algorithms.", "text": "next generation applications continuously interact environment learn interactions. applications impose demanding systems requirements terms performance ﬂexibility. paper consider requirements present ray—a distributed system address them. implements dynamic task graph computation model supports task-parallel actor programming models. meet performance requirements applications propose architecture logically centralizes system’s control state using sharded storage system novel bottom-up distributed scheduler. experiments demonstrate sub-millisecond remote task latencies linear throughput scaling beyond million tasks second. empirically validate speeds challenging benchmarks serves natural performant emerging class reinforcement learning applications algorithms. artiﬁcial intelligence currently emerging workhorse technology range real-world applications date however applications largely based fairly restricted supervised learning paradigm model trained ofﬂine deployed serve predictions online. ﬁeld matures necessary consider broader setting standard supervised learning. instead making serving single prediction machine learning applications must increasingly operate dynamic environments react changes environment take sequences actions accomplish goal broader requirements naturally framed within paradigm reinforcement learning deals learning operate continuously within uncertain environment rl-based applications already remarkable results google’s alphago beating human world champion ﬁnding self-driving cars uavs robotic manipulation three characteristics distinguish applications traditional supervised learning applications. first often rely heavily simulations explore states discover consequences actions. simulator could encode rules computer game newtonian dynamics physical system robot hybrid dynamics virtual environments. generally requires massive amounts computation; example realistic application might perform hundreds millions simulations. second computation graph application heterogeneous evolves dynamically. simulation take milliseconds minutes result simulation determine parameters future simulations. third many applications robotic control autonomous driving require actions taken quickly response constantly changing environment. furthermore choose best action application need perform simulations real time. summary need computation framework supports heterogeneous dynamic computation graphs handling millions tasks second millisecond-level latencies. existing cluster computing frameworks fall short adequately satisfying requirements. mapreduce apache spark dryad dask ciel support neither throughput latencies required general applications tensorflow naiad canary generally assume static computation graphs. paper propose cluster computing framework satisﬁes requirements. support heterogeneous dynamic workloads imposed applications implements dynamic task graph computation model similar ciel however also provides actor programming abstraction execution model addition task-parallel abstraction provided ciel. actor abstraction enables support stateful components thirdparty simulators. achieve stringent performance targets supporting dynamic computation graphs employs distributed architecture horizontally scalable. architecture based ideas. first store control state system global control store enables components system stateless. result component easily scaled horizontally restarted case failures. turn global control store scaled sharding made fault tolerant replication. second introduce bottom-up distributed scheduler tasks submitted workers drivers local schedulers local schedulers choose schedule tasks locally forward tasks replicated global scheduler. decreases task latency allowing local decisions increases system throughput reducing burden global scheduler. make following contributions specify systems requirements emerging applications support heterogeneous concurrent computations dynamic task graphs high-throughput low-latency scheduling transparent fault tolerance. support variety workloads—as provides task-parallel actor abstractions— focus workloads paper representative emerging applications primary driver behind ray’s design. here consider system consists agent interacts repeatedly environment goal agent learn policy maximizes reward. policy mapping state environment action take. deﬁnitions environment agent state action reward applicationspeciﬁc figure shows example pseudocode used agent learn policy. typical procedure consists steps evaluate current policy improve policy. evaluate policy pseudocode invokes rollout generate rollouts rollout trajectory states rewards collected interacting environment using environment.step. action computed given current policy environment state policy.compute. trajectories generated train policy uses completed trajectories improve current policy policy.update. process repeated policy converges. flexibility. ﬂexibility system typically measured terms diversity workloads support. consider aspects ﬂexibility heterogeneity concurrently executing tasks generality dynamicity execution graph. functionality. case robot assessing environment’s state involves processing inputs multiple sensors video microphone radar. requires running multiple tasks parallel implementing different computation resource types. computing action evaluating policy many cases implemented deep neural network typically requires gpus. hand computations cpus. note requirements naturally satisﬁed bulk synchronous parallel model implemented many today’s popular cluster computing frameworks tasks within dynamic task graphs. consider train policy function. though shown figure would natural update policy soon subset rollouts ﬁnish launch rollouts maintain pool executing rollouts figure makes execution graph dynamic cannot predict order rollouts complete rollouts used particular policy update. environment compute action matter milliseconds. similarly simulations might take order milliseconds well. such need able schedule tasks less millisecond. otherwise scheduling overhead could signiﬁcant. given clusters tens thousands cores common need able schedule hundreds thousands even millions tasks second. consider cluster consisting servers cores suppose task takes execute. fully utilize cluster would need schedule tasks/sec. ease development. since writing parallel applications non-trivial since developers prefer focus applications rather systems programming simplifying development paramount success system. deterministic replay fault tolerance. ability deterministically replay dramatically simpliﬁes debugging. transparent fault tolerance obviates need users handle faults explicitly. also enables users cheap preemptible resources leading substantial cost savings running public cloud. easy parallelization existing algorithms. involves providing simple supporting existing languages tools libraries. first need provide support python python language choice developers. second need provide tight integration wide range available thirdparty libraries. libraries include simulators openai deepmind mujoco physics simulator well deep learning frameworks like tensorflow theano pytorch caffe requires augmenting task-parallel model actor-like abstraction wrap third-party services. implements dynamic task graph computation model. provides actor taskparallel programming abstraction. dual abstraction differentiates related systems ciel provides task-parallel abstraction orleans primarily provides actor abstraction core provides task-parallel programming model. table shows ray’s api. remote function invoked future represents result task returned immediately. futures retrieved using ray.get composed i.e. future passed argument another remote function. allows user express parallelism capturing data dependencies. remote functions operate immutable objects expected stateless side-effect free outputs determined solely inputs. implies idempotence simpliﬁes fault tolerance function re-execution failure. satisfy requirements heterogeneity ﬂexibility ease development given section augment task-parallel programming model four ways. first handle concurrent tasks heterogeneous durations introduce ray.wait. call takes list futures returns subset whose results available either timeout least available. contrast ray.get blocks requested futures available. highly beneﬁcial applications simulations widely different durations complicates fault tolerance introduced nondeterminism. second handle resource-heterogeneous tasks enable developers specify resource requirements scheduler efﬁciently manage resources. resources speciﬁed remote function allocated function’s execution. third improve ﬂexibility enable nested remote functions meaning remote functions invoke remote functions. also critical achieving high scalability enables multiple processes invoke remote functions parallel finally importantly ease development efﬁciency enhance programming model actor abstraction. limitation encountered early development stateless tasks inability wrap third-party simulators expose internal state. address limitation provides basic support stateful components form actors. actor stateful process exposes methods invoked remote functions executes methods serially. employs dynamic task graph computation model execution remote functions actor methods automatically triggered system inputs become available. section describe computation graph constructed user program program uses table implement pseudocode figure ignoring actors ﬁrst types nodes computation graph data objects remote function invocations tasks. also types edges data edges control edges. data edges capture dependencies data objects tasks. precisely data object output task data edge similarly input data edge control edges capture computation dependencies result nested remote functions task invokes task control edge actor method invocations also represented nodes computation graph. identical tasks difference. capture state dependency across subsequent method invocations actor third type edge stateful edge. method called right method actor stateful edge thus methods invoked actor object form chain connected stateful edges chain captures order methods invoked. stateful edges help embed actors otherwise stateless task graph capture implicit data dependency successive method invocations sharing internal state actor. stateful edges also enable maintain lineage. dataﬂow systems track data lineage enable reconstruction. explicitly including stateful edges lineage graph easily reconstruct lost data whether produced remote functions actor methods description execute function remotely. takes either object values futures arguments returns list futures. non-blocking call. return values associated list futures. blocking call. given list futures return futures whose corresponding tasks completed soon either tasks completed timeout expires. instantiate class class remote actor return reference call method remote actor return list futures. non-blocking call. figure python code implementing example figure ray. note ray.remote indicates remote functions actors. invocations remote functions actor methods return futures passed subsequent remote functions actor methods encode task dependencies. actor environment object self.env shared methods. task graph corresponding invocation train policy.remote. remote function calls actor method calls correspond tasks task graph. ﬁgure shows actors. method invocations actor stateful edges indicating share mutable actor state. control edges train policy tasks invokes. train multiple policies parallel could call train policy.remote multiple times. worker stateless process executes tasks invoked driver another worker. workers started automatically assigned tasks system layer. remote function declared function automatically published workers. worker executes tasks serially. actor stateful process executes invoked methods exposes. unlike worker actor explicitly instantiated worker driver. like workers actors execute methods serially. system layer enables meet performance fault tolerance goals discussed section employing architecture component horizontally scalable fault-tolerant. layer consists three major components global control store distributed scheduler distributed object store. note workers stateless maintain local state across tasks. assuming deterministic execution invoking remote function arguments return results matter whether executed worker not. figure ray’s architecture consists parts application layer system layer. application layer implements computation model described section system layer implements task scheduling data management satisfy performance fault-tolerance requirements. ﬁcation every task code every remote function computation graph current locations objects every scheduling event. also provides publish-subscribe infrastructure facilitate communication components. storing managing entire control state centralized fashion enables every component stateless. simpliﬁes support fault tolerance also makes easy horizontally scale every component state shared component’s replicas shards accessible gcs. scale sharding. since associate pseudo-random virtually every data entry relatively easy balance load across multiple shards. provide fault-tolerance replica shard. centralizing system control information allows easily build debugging proﬁling visualization tools gcs. minimalist tools we’ve built already proven useful development. many existing cluster computing frameworks apache spark ciel dryad hadoop implement centralized scheduler. simpliﬁes design hurts scalability. several approaches improve scheduling scalability batch scheduling scheduler submits tasks worker nodes batches amortize ﬁxed overheads related task submission hierarchical scheduling global scheduler partitions task graph across per-node local figure bottom-up distributed scheduler. tasks submitted bottom-up drivers workers local scheduler forwarded global scheduler needed thickness arrow proportional request rate. schedulers parallel scheduling multiple global schedulers schedule tasks concurrently worker nodes unfortunately none approaches addresses ray’s demands. batch scheduling still requires global scheduler handle every task limits scalability hierarchical scheduling assumes task graph known advance parallel scheduling assumes every global scheduler schedules independent jobs. contrast require highly scalable scheduler handle dynamic task graphs possibly generated single job. like existing hierarchical scheduling solutions employ global scheduler per-node local schedulers. however unlike prior solutions tasks created node submitted node’s local scheduler ﬁrst global scheduler local scheduler schedules tasks locally unless node overloaded cansatisfy task’s requirements task’s inputs remote. local scheduler doesn’t schedule task sends task global scheduler. determine load local scheduler checks current length task queue. length exceeds conﬁgurable threshold concludes local node overloaded. value threshold enables scheduling policy span continuum centralized tasks handed global scheduler decentralized tasks handled locally. local scheduler sends periodic heartbeats containing load information. records information forwards global scheduler. upon receiving task global scheduler uses latest load information every node along locations sizes task’s inputs decide node assign task global scheduler beminimize task latency implement in-memory distributed storage system store inputs outputs every task. allows workers actors share data efﬁciently. node implement object store shared memory. allows zero-copy data sharing tasks running node. additionally apache arrow efﬁcient memory layout becoming facto standard data analytics. task’s inputs local inputs replicated local object store node execution. tasks also write outputs local object store. replication eliminates potential bottleneck data objects minimizes task execution time task reads writes data local memory. increases throughput computation-bound workloads proﬁle shared many applications. existing cluster computing frameworks apache spark dryad object store limited immutable data signiﬁcantly simpliﬁes system design obviating need complex consistency protocols simplifying support fault tolerance. simplicity object store build support distributed objects object single node. distributed objects like large matrices trees implemented higher level collections futures. object reconstruction. component failure result object loss recovers lineage re-execution. tracks lineage recording task dependencies execution. similar solution employed cluster computing systems like apache spark ciel also like systems assumes objects immutable operators deterministic. however unlike systems adds support stateful operator reconstruction. integrating stateful edges directly computation graph leverage reconstruction mechanism remote functions actors. reconstruct lost object walk backward along data stateful edges tasks whose inputs present object store. replay computation subgraph rooted inputs. consider example figure assume rollout figure end-to-end example adds returns solid lines data plane operations dotted lines control plane operations. function registered node invoked executed gets add’s result using ray.get. object table entry created step updated step copied lost. walking backwards along data stateful edges reach input. thus reconstruct rollout need re-instantiate actor executing executing methods order. note object whose lineage includes stateful edges reconstruction require reinstantiating actor replaying possibly long chain stateful edges since actors commonly used wrap third-party simulators ﬁnite lifetime expect chains bounded. however we’ve also found actors useful managing general forms state. improve recovery time cases checkpoint actor’s state periodically allow actor recover checkpoints. latency keep objects entirely memory evict needed disk using least-recentlyused eviction policy. figure illustrates works end-to-end simple example adds objects could scalars matrices returns result remote function automatically registered upon initialization distributed every worker system figure shows step-by-step operations triggered driver invoking add.remote stored nodes respectively. driver submits local scheduler forwards global scheduler next global scheduler looks locations add’s arguments decides schedule task node stores argument local scheduler node checks whether local object store contains add’s arguments since local store doesn’t object looks location learning stored object store replicates locally arguments stored locally local scheduler invokes local worker accesses arguments shared memory figure shows step-by-step operations triggered execution ray.get respectively. upon ray.get’s invocation driver checks local object store value using future returned since local object store doesn’t store looks location gcs. time entry created yet. result object store registers callback object table triggered entry created meanwhile completes execution stores result local object store turn adds entry result triggers callback object store entry next replicates returns ray.get ﬁnally completes task. implemented lines code system layer python application layer. object store zero-copy serialization libraries factored standalone projects used independently ray. bottom-up distributed scheduler .kloc undergo signiﬁcant development reﬁne ray’s scheduling policies. section focus implementation details achieving performance targets dictated real-time applications scheduler performance object store performance end-to-end system performance. bottom-up distributed scheduler. implement local global scheduler event-driven single-threaded processes. internally local schedulers maintain cached state local object metadata tasks waiting inputs tasks ready dispatch worker. object dependencies become available tasks become ready dispatch. worker availability triggers dispatch many tasks possible node’s capacity constraints. local scheduler sends periodic heartbeats global schedulers routed publish-subscriber mechanism containing dispatch queue length resource availability. enables global scheduler balance load across nodes. object store. ray’s object store also implemented single-threaded event loop. uses shared memory workers node read data without copying objects immutable. object made visible worker ﬁnishes creation. minimize object creation overhead store pre-allocates pool large memory-mapped ﬁles. simd-like memory copy maximize throughput copying data worker object store’s shared memory. also parallelize computation object’s content hash used detect non-deterministic computations. uses apache arrow achieve high performance serializing/deserializing python objects. global control store. implement ray’s global control store using redis key-value store shard shard tables object task scale replicate every shard fault tolerance. scale experiment distribute shards across multiple nodes. though implementation uses multiple redis servers performance fault tolerance requirements could also existing systems like ramcloud lastly ray’s monitor tracks system component liveness reﬂects component failures gcs. tasks objects failed cluster nodes marked lost objects later reconstructed lineage information necessary. figure end-to-end scalability system achieved linear fashion leveraging bottom-up distributed scheduler. reaches million tasks second throughput m.xlarge nodes processes million tasks minute. omit cost. section demonstrate three points. first examine scalability system whole well performance individual components second demonstrate robustness fault tolerance third demonstrate natural reinforcement learning applications terms performance ease development experiments amazon services. speciﬁc instance types reported below. end-to-end scalability. beneﬁts global control store ability horizontally scale system. evaluate ability section. fig. benchmark embarrassingly parallel workload increasing cluster size x-axis. observe near-perfect linearity progressively increasing task throughput. exceeds million tasks second throughput nodes continues scale linearly beyond million tasks second nodes. rightmost datapoint shows process million tasks less minute variability minimal. expected increasing task duration reduces throughput proportionally mean task duration overall scalability remains linear. global scheduler’s primary responsibility maintain balanced load throughout system. figure tasks submitted single node rebalanced across available resources. note node load originates processes tasks maximizes utilization local node forwarding tasks global scheduler. figure object store write throughput iops. single client throughput exceeds gb/s large objects iops small objects core instance uses threads copy objects larger thread small objects. plots report throughput threads. results averaged runs. object store performance. track metrics object store performance iops write throughput figure object size increases write throughput single client reaches gb/s. larger objects copying object client dominates time spent object creation. smaller objects completion time dominated serialization overhead client object store. object store peaks iops corresponds operation. recovering object failures. figure demonstrate ray’s ability transparently recover worker node failures elastically scale. driver submits rounds tasks task dependent task previous round. worker nodes killed surviving local schedulers automatically trigger reconstruction lost objects. during periods reconstruction tasks originally submitted driver stall since dependencies cannot satisﬁed. however overall task throughput remains method calls. initial throughput comparable without checkpointing. node failure majority reconstruction done executing checkpoint tasks reconstruct actor’s state result tasks need re-executed method calls stall versus re-executions without checkpointing respectively. future hope reduce actor reconstruction time allowing user annotations read-only methods. overhead replication. make fault tolerant replicate database shards. client writes shards duplicates writes replicas. workloads artiﬁcially make bottleneck reducing number shards overhead twoway replication less real workloads slowdown undetectable. given diverse demanding requirements reinforcement learning applications described section reinforcement learning algorithms today implemented special-purpose ad-hoc systems typically require substantial engineering effort develop generalize algorithms. section implement types reinforcement learning algorithms show able match outperform performance specialized systems built speciﬁcally algorithms. furthermore using distribute algorithms clusters requires changing lines code serial implementations algorithms. evaluate large-scale workloads implement evolution strategies algorithm compare reference implementation special-purpose system built algorithm. uses hierarchy redis servers message buses relies low-level multiprocessing libraries sharing data. shown figure straightforward implementation scalable scaling physical cores whereas special-purpose system stops running cores. implementation runs median time minutes twice fast best published result implementation also substantially simpler develop. parallelizing serial implementation using required modifying lines code. contrast figure fully transparent fault tolerance distributed tasks. dashed line represents number nodes cluster. curves show throughput tasks re-executed tasks driver continually submits retrieves rounds tasks. task takes depends task previous round. task input output size figure fully transparent fault tolerance actor methods. driver continually submits tasks actors cluster. kill nodes causing actors cluster recovered remaining nodes. stable fully utilizing available resources lost dependencies reconstructed. furthermore nodes added back system able fully recover initial throughput. recovering actor failures. next demonstrate ray’s ability transparently recover lost actors. encoding actor’s method calls dependency graph reuse object reconstruction mechanism figure workload figure demonstrates extreme case intermediate actor state saved. previous method calls lost actor must re-executed serially lost actors automatically redistributed across available nodes throughput fully recovers reconstruction. improve reconstruction time long-lived actors provide transparent checkpointing intermediate actor state. figure shows workload automatic checkpoint task actor every ray’s made easy take advantage heterogeneous resources decreasing costs factor tasks actors specify distinct resource requirements allowing cpu-only tasks scheduled cheaper high-cpu instances. contrast applications often exhibit symmetric architectures processes code require identical resources case preventing cpu-only machines scale-out. shown figure implementation outperforms optimized implementation experiments fraction gpus. able parallelize using minimal changes structure serial program. show meet soft real-time requirements controlling simulated robot real time. driver runs simulated robot takes actions ﬁxed time steps varying millisecond milliseconds simulate different real-time requirements. driver submits tasks compute actions taken using policy trained ofﬂine. however actions taken received driver within relevant time step latency budgets real robots order milliseconds even simulation faster real time able produce stable walk. table shows fraction tasks arrive fast enough used robot. dynamic task graphs. closely related ciel support dynamic task graphs nested tasks implement futures abstraction provide lineage-based fault tolerance. however differ important aspects. first extends task model actor abstraction. second employs fully distributed control plane scheduler instead relying single master. addition adds ray.wait method employs in-memory implementation scales well cores. special-purpose system failed beyond cores. cores achieve median time minutes twice fast best published result. faster benchmark exhibits greater runtime variance. figure time reach score humanoid-v task implementation outperforms specialized implementation fewer gpus fraction cost. implementation required every cpus whereas version required gpus reference implementation required several hundred lines code develop custom protocol communicating tasks data workers could easily adapted different algorithms communication patterns. include pseudocode illustrating point section evaluate single node small cluster workloads implement proximal policy optimization compare highly-optimized reference implementation uses openmpi communication primitives. experiments usboth implementations tensorflow deﬁne neural networks rely distributed frameworks communication processes. though algorithm could implemented using tensorflow alone single-machine performance would slow python global interpreter lock distributed versions would look similar implementations ﬁle-based) object store extends existing programming language ciel provides scripting language also closely related dask supports dynamic task graphs including wait-like primitive employs futures abstraction python environment. however dask uses centralized scheduler doesn’t offer actor-like abstraction doesn’t provide fault tolerance. data systems. popular dataﬂow systems mapreduce spark dryad widespread adoption analytics workloads computation model restrictive. spark mapreduce implement execution model assumes tasks within stage perform computation take roughly amount time. dryad relaxes restriction lacks support dynamic task graphs. furthermore none systems provide actor abstraction implement distributed scalable control plane scheduler. finally naiad dataﬂow system provides improved scalability workloads supports static task graphs. actor systems. orleans provides virtual actorbased abstraction. actors perpetual state persists across invocations. scaling orleans also allows multiple instances actor parallel actor operates immutable state state. stateless actors tasks ray. however unlike orleans developer must explicitly checkpoint actor state intermediate responses. addition orleans provides at-least-once semantics. contrast provides transparent fault tolerance exactly-once semantics method call logged arguments results immutable. practice limitations affect performance applications. erlang actor framework actor-based systems also require application explicitly handle fault tolerance. also erlang’s global state store suitable sharing large objects models support data sharing. global control state scheduling. concept logically centralizing control plane previously proposed software deﬁned networks distributed systems resource management distributed frameworks boom name few. draws inspiration pioneering efforts provides signiﬁcant improvements. contrast sdns boom couple control plane data computation decouples storage control plane information logic implementation allows storage computation layers scale independently achieving scalability targets. omega uses distributed architecture schedulers coordinate globally shared state. architecture adds global schedulers balance load across local schedulers targets ms-level second-level task scheduling. implements unique distributed bottom-up scheduler horizontally scalable handle dynamically constructed task graphs. unlike existing cluster computing systems centralized scheduler architecture. sparrow decentralized schedulers make independent decisions limiting possible scheduling policies tasks handled global scheduler. mesos implements two-level hierarchical scheduler top-level scheduler bottleneck. canary achieves impressive performance having scheduler instance handle portion task graph handle dynamic computation graphs. machine learning frameworks. tensorflow mxnet target deep learning workloads efﬁciently leverage cpus gpus. achieve great performance workloads consisting static dags linear algebra operations limited support general workloads. tensorflow fold provides support dynamic task graphs well mxnet internal apis neither fully supports ability modify execution response task progress task completion times faults. tensorflow mxnet principle achieve generality allowing programmer simulate low-level message-passing synchronization primitives pitfalls user experience case similar mpi. openmpi achieve high performance relatively hard program requires explicit coordination handle heterogeneous dynamic task graphs. furthermore forces programmer explicitly handle fault tolerance. api. designing emphasized minimalism. initially started basic task abstraction. later added wait primitive accommodate rollouts heterogeneous durations actor abstraction accommodate third-party simulators amortize overhead expensive initializations. resulting relatively low-level proven powerful simple use. indeed teams report instructing developers ﬁrst write serial implementations parallelize using ray. illustrate point next brieﬂy describe experience algorithms asynchronous advantage actor critic hyperparameter search. state-of-the-art algorithm leverages asynchronous policy updates signiﬁcantly improve training times previous algorithms. scale algorithm simple hierarchical scheme multiple instances trained parallel periodically aggregated form improved model. implementing hierarchical straightforward requiring lines python code extend non-hierarchical version. furthermore simple extension improved performance hardware able implement state-of-the-art hyperparameter search algorithm roughly lines python code using ray. ray’s support nested tasks critical multiple experiments parallel experiment typically used parallelism internally. wait primitive allowed process results experiments order completed adaptively launch ones. actor abstraction allowed pause resume stateful experiments based progress experiments contrast existing implementations wait experiments round complete leading inefﬁcient resource utilization. ray’s still work progress. based early user feedback considering enhancing include higher level primitives simple aggregation map. could also inform scheduling decisions system layer limitations. given workload generality specialized optimizations hard. example must make scheduling decisions without full knowledge computation graph. scheduling optimizations might require complex runtime proﬁling. addition storing lineage task requires implementation garbage collection policies bound storage costs feature actively developing. fault tolerance. often asked fault tolerance really needed applications. statistical nature many algorithms could simply ignore failed rollouts. based experience answer unqualiﬁed yes. first ability ignore failures makes applications much easier write reason about. second particular implementation fault tolerance deterministic replay dramatically simpliﬁes debugging allows easily reproduce errors. particularly important since stochasticity algorithms notoriously hard debug. third fault tolerance helps save money since allows cheap resources like spot instances aws. furthermore workloads scale expect fault horizontal scalability. dramatically simpliﬁed development debugging. basic failure handling horizontal scaling components took less week implement. enabled query entire system state debugging helped numerous bugs generally understand system behavior. instrumental ray’s horizontal scalability. experiments reported section able scale results adding shards whenever became bottleneck. also enables global scheduler scale simply adding replicas. currently manually conﬁguring number shards global schedulers planning develop adaptive algorithms future. advantages believe centralizing control state design component future distributed systems. emerging applications present challenging computational demands. meet demands introduces global control store bottom-up distributed scheduler. together architecture implements dynamic task graph execution turn supports task-parallel actor programming model. programming ﬂexibility particularly important workloads produce tasks diverse resource requirements duration functionality. evaluation demonstrates linear scalability past tasks second transparent fault tolerance substantial performance improvements several contemporary workloads. thus provides powerful combination ﬂexibility performance ease development future applications.", "year": 2017}