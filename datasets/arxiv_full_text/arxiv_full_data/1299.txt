{"title": "Training Binary Multilayer Neural Networks for Image Classification  using Expectation Backpropagation", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "Compared to Multilayer Neural Networks with real weights, Binary Multilayer Neural Networks (BMNNs) can be implemented more efficiently on dedicated hardware. BMNNs have been demonstrated to be effective on binary classification tasks with Expectation BackPropagation (EBP) algorithm on high dimensional text datasets. In this paper, we investigate the capability of BMNNs using the EBP algorithm on multiclass image classification tasks. The performances of binary neural networks with multiple hidden layers and different numbers of hidden units are examined on MNIST. We also explore the effectiveness of image spatial filters and the dropout technique in BMNNs. Experimental results on MNIST dataset show that EBP can obtain 2.12% test error with binary weights and 1.66% test error with real weights, which is comparable to the results of standard BackPropagation algorithm on fully connected MNNs.", "text": "compared multilayer neural networks real weights binary multilayer neural networks implemented efﬁciently dedicated hardware. bmnns demonstrated effective binary classiﬁcation tasks expectation backpropagation algorithm high dimensional text datasets. paper investigate capability bmnns using algorithm multiclass image classiﬁcation tasks. performances binary neural networks multiple hidden layers different numbers hidden units examined mnist. also explore effectiveness image spatial ﬁlters dropout technique bmnns. experimental results mnist dataset show obtain test error binary weights test error real weights comparable results standard backpropagation algorithm fully connected mnns. recent years deep neural networks attracts tremendous attentions wide range research areas related signal information processing. state-of-the-art performances achieved techniques various challenging tasks applications speech recognition object recognition multimedia event detection etc. almost current dnns real-valuedweight mutlilayer neural networks however effective rmnns often massive require large computational energetic resources. example googlenet layers tens thousands hidden units mnns binary weights advantage implemented efﬁciently dedicated hardware. example karakiewicz presented chip enable multiply accumulates second power efﬁciency binary weights. thus attractive develop effective algorithms bmnns achieve comparable performances rmnns. traditional mnns trained backpropagation similar gradient descent methods. however gradient descent methods cannot directly used training binary neural networks. straightforward method problem binarize real-valued weights approach decrease performance signiﬁcantly. recently soudry presented expectation backpropagation algorithm support online training mnns either continuous discrete weight values. experiments several large text datasets show promising performances binary classiﬁcation tasks binary-weighted mnns extension previous work soudry work study performance algorithm image classiﬁcation tasks binary real weights mnns. besides investigate effects different factors network depth layer size dropout strategies performance algorithm image classiﬁcation. study explores possibility using bmnns multimedia supervised classiﬁcation tasks. section review expectation backpropagation introduce implement algorithm binary weights detail. introducing algorithm ﬁrst describe general notations. blodfaced capital letter denotes matrix components xij. blodfaced non-capital letter denotes column vector components besides denotes denotes xijl. indicator function denotes condition holds otherwise. consider general feedforward multilayer neural networks connections adjacent layers. suppose layers number hidden units l-th layer weight matrices layer l-th layer. {wl}l simplicity activation function sign function study. output network therefore algorithm derived within bayesian framework. given labeled dataset weights maximize posterior probability posterior obtain probable weight conﬁguration minimize expected zero-one loss outputs using maximum posteriori estimation. however update generally intractable large networks exponential number values stored updated. solve problem mean-ﬁeld approximation used approximate speciﬁcally approximated simplify summation another approximation performed assuming neuronal fanlarge namely large number units previous layer connected unit next layer. since weights besides wijl independent together large fan-in assumption assume normalized input neural layer gaussian distribution based central limit theorem thus quite common effective approximation. using approximation activation function sign distribution calculated sequentially layers given value wijl. forward pass obtain obtained update wijl computational directly calculate every taylor expansion wijl used approximate ﬁrst order terms expansion calculated using backward propagation derivative terms summarize general expectation backpropagation algorithm introduce implementation algorithm using binary weights real bias. detailed information implementation real weights described soudry given input desire output forward pass ﬁrst performed calculate mean output hvli layer; backward pass conducted update weights. forward pass first initialize inputhvki calculate recursively following values hwkrmi mean posterior distribution variance input layer hvmi resulting mean output layer backward pass backward pass performs bayes update posterior using taylor expansion. based ﬁrst initialize unimportant constant dependent wijl. output based learnt weight conﬁguration output obtained deﬁned deterministic output alternatively output calculated directly performance algorithm evaluated soudry however experiments limited high dimensional text datasets tasks binary classiﬁcation tasks. study examine performance algorithm image datasets multiclass classiﬁcation. check performance algorithm deeper small fan-in architectures image classiﬁcation architectures multiple layers different hidden unites experiments. besides also explore effectiveness dropout techniques algorithms. methods used input image mnns. ﬁrst method directly convert image vector concatenating pixels image certain order concatenating bottom. example standard mnist handwritten digits database input image vector. second method consider spatial conﬁguration images. spatial conﬁguration considered similar convolutional neural networks unit layer receives inputs units located small neighborhood previous layer. shown fig. unit feature inputs connected area input. unit inputs therefore trainable coefﬁcients plus trainable bias. different feature hidden layer study. since feature network constraint connection weights unit feature same. example shown fig. units second layer unit trainable weights. implementation weight matrix ﬁrst second layer weight matrix initialized weights connected units nonzero namely nonzero elements weight matrix. zero elements kept zero whole training process. algorithm assumption large fan-in unit hidden layers connected relative large neighborhood input layer. mnist database contains images test images. training process images training presented sequentially epoch randomized order. task identify label using bmnn classiﬁer trained algorithm. label δklabel+ pre-process training data centralizing normalizing pixels recommended backpropagation standard classiﬁcation real values mnns. output neuron highest value indicates predicted label input pattern. treating image vector constant added input vector allow bias neurons hidden layer spatial ﬁltering method bias added convolving block. neural network architectures used hidden layer hidden layers. type architecture vary number neurons hidden layers. detailed conﬁgurations network architectures methods shown table spatial ﬁltering method different ﬁltering block sizes used hidden layer architecture thus corresponding hidden units feature size hidden layer. taking block size example feature size becomes hidden units. accordingly inputs unit hidden layer inputs unit output layer. selected conﬁgurations large fanin assumption algorithm. conﬁgurations also used learn whether better larger fan-in ﬁrst layer second layer. case two-hidden-layer network select conﬁguration conﬁguration lead smaller fan-in correspond inputs unit ﬁrst layer also inputs unit second hidden layer). also employ dropout technique architectures. dropout technique preventing overﬁtting provides approximately combining exponentially many different network architectures efﬁciently improve performance effectiveness dropout demonstrated neural networks traditional error backpropagation stochastic gradient decent method study investigate effectiveness algorithm. experiments ﬁxed hidden units input units dropout nets. result presentation four abbreviations presentation simplicity b-ebp-d deterministic binary weights; b-ebp-p probabilistic binary weights; r-ebp-d deterministic real weights; r-ebp-p probabilistic real weights. results reported based networks trained epochs. training epochs improve performances network architectures. weight initialization used method soudry effects hidden unite number hidden layer number table shows results mnns mnist dataset using algorithms different network structures without dropout. results observe networks hidden layer increase hidden units clearly improves performance best performance obtained units. two-hidden-layer structure ebp-p outperforms one-hidden-layer structure signiﬁcantly even hidden units layer. results demonstrate works well mnns. another observation ebp-p outperforms ebp-d consistent results shown soudry particularly performance b-ebp-d two-hidden-layer structure worse one-hidden-layer structure. growing size hidden units performance b-ebpdecreases quickly two-hidden-layer models. also algorithm real weights conﬁgurations. performances real weights better performance binary weights structures. r-ebp-p two-hidden-layer slightly better one-hidden-layer. although r-ebp-d two-hidden-layer performs worse one-hidden-layer b-ebp-d performance increase number hidden units increases. standard backprop algorithm one-hidden-layer model units obtain comparable best results obtained r-ebp-p. using binary weight hurt performance table binary weights optimal neural networks hurt performance much effects dropout results algorithms different network structures dropout shown table results show observations without dropout. comparing results table table using dropout improve performance conﬁgurations demonstrates dropout also works algorithms. results using units units one-hidden-layer structure without dropout result hidden units worse hidden units dropout performance continuously increasing increase hidden unit number besides dropout performance b-ebp-d becomes reasonable. results validate dropout effectively prevent overﬁtting bmnns algorithm. effects spatial filtering table shows results mnns using algorithm consideration image spatial conﬁguration. best performance spatial ﬁltering method using binary weights worse results using input vector method shown table contrary performances using real weights improved spatial ﬁltering method performance better network structures using vector input method without dropout best results obtained conﬁguration hidden units results method shed light extension method convolutional neural networks block size connecting unit feature convolutional layer. summary analysis experimental results gives interesting ﬁndings. include bmnns algorithm work well image classiﬁcation task although performance good real mnns; even fan-in size hundreds algorithm still works well bmnns; bmnns ebp-d algorithms networks note using error regularization proper weight initialization standard backpropagation achieve better performance. example achieve error rate using error regularization initializing weight uniformly outperform networks one-hidden-layer; dropout signiﬁcantly improve performance bmnns algorithm; bmnns consideration spatial ﬁltering improve classiﬁcation performance based results mnist. paper report performance binary multilayer neural networks image classiﬁcation tasks. expectation backpropagation algorithm used train bmnns different network architectures performance evaluated standard mnist digits dataset. experimental results demonstrate bmnns algorithm achieve good performance mnist classiﬁcation tasks. results also show dropout techniques signiﬁcant improve bmnns algorithm. image spatial conﬁguration improves performance networks real weights bmnns. study conduct experiments mnist dataset. performance bmnns algorithm image classiﬁcation tasks needs validated image datasets future would like study performance standard convolutional neural networks algorithm explore different weight initialization methods. references hinton geoffrey deng dong dahl george mohamed abdel-rahman jaitly navdeep senior andrew vanhoucke vincent nguyen patrick sainath tara deep neural networks acoustic modeling speech recognition shared views four research groups. signal processing magazine ieee krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. pereira burges c.j.c. bottou weinberger k.q. advances neural information processing systems curran associates inc. zhen-zhong jiang shoou-i rawat shourabh yang chenqiang shicheng shen haoquan xuanchong wang yipei cmu-informedia trecvid multimedia event detection. trecvid workshop volume srivastava nitish hinton geoffrey krizhevsky alex sutskever ilya salakhutdinov ruslan. dropout simple prevent neural networks overﬁtting. journal machine learning research szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. going deeper convolutions. arxiv preprint arxiv.", "year": 2015}