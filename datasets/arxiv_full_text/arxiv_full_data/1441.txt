{"title": "WRPN & Apprentice: Methods for Training and Inference using  Low-Precision Numerics", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Today's high performance deep learning architectures involve large models with numerous parameters. Low precision numerics has emerged as a popular technique to reduce both the compute and memory requirements of these large models. However, lowering precision often leads to accuracy degradation. We describe three schemes whereby one can both train and do efficient inference using low precision numerics without hurting accuracy. Finally, we describe an efficient hardware accelerator that can take advantage of the proposed low precision numerics.", "text": "today’s high-performance deep learning architectures involve large models numerous parameters. low-precision numerics emerged popular technique reduce compute memory requirements large models. however lowering precision often leads accuracy degradation. describe three software-based schemes whereby train efﬁcient inference using low-precision numerics without hurting accuracy. finally describe efﬁcient hardware accelerator take advantage proposed low-precision numerics. introduction background using low-precision numerics promising approach lower compute memory requirements convolutional deeplearning workloads. operating lower precision mode reduces computation well data movement storage requirements. efﬁciency beneﬁts many existing works propose low-precision deep neural networks however majority existing works low-precision dnns sacriﬁce accuracy baseline full-precision models. furthermore prior works target reducing precision model parameters primarily beneﬁts inference batch sizes small. observe activation maps occupy memory compared model parameters typical batch sizes training. observation holds even inference batch size around more. next section elaborates proposals. take advantage software-based optimizations also developed accelerator handle low-precision numerics. discuss details accelerator conclude discussions next steps trade-offs low-precision sparsity overall contributions paper techniques obtain low-precision dnns without sacriﬁcing model accuracy. schemes produces low-precision model surpasses accuracy equivalent low-precision model published date. schemes also helps low-precision model converge faster. training inference using low-precision wide reduced-precision networks based observation activations occupy memory footprint compared weights lower precision activations aggressively weights speed training inference steps well memory requirements. however straightforward reduction precision activation maps leads signiﬁcant reduction model accuracy motivation figure shows memory footprint activation maps ﬁlter maps different networks alexnet inception-resnet-v resnet- resnet-. left plot ﬁgure shows memory footprint occupied weights activations training inference phase batch size batch-size large ﬁlter reuse across batches inputs activation maps occupy signiﬁcantly larger fraction memory compared ﬁlter weights. right plot ﬁgure shows memory footprint inference batch sizes small batch size weights model occupy memory compared activations. training large batch sizes typical lowering activation memory beneﬁcial. case large batch inference common scenario cloud based systems. hand real-time inference deployments batch-sizes relatively small lowering memory requirements weights beneﬁcial. proposal based observation study schemes training inference using low-precision dnns reduce precision activation maps well model parameters without sacriﬁcing network accuracy. investigate three software/algorithm-based schemes precision weights activations lowered without hurting network accuracy. highlight aspect table shows sensitivity study reduce precision activation maps model weights resnet topologies running cifar- dataset train network scratch. weights activations corresponds baseline network full-precision numerics. that general reducing precision activation maps weights hurts model accuracy. further reducing precision activations hurts model accuracy much reducing precision ﬁlter parameters. resnet- weights activations degradation accuracy baseline full-precision network resnet- degradation re-gain model accuracy working reducedprecision operands increase number ﬁlter maps layer scheme compensates surpasses accuracy baseline full-precision network. call approach wrpn table shows effect accuracy precision lowered ﬁlters widened smaller factor ﬁlters ﬁrst layers doubled remaining layers number ﬁlters baseline network. setting accuracy weights activations within baseline fullprecision accuracy. using wrpn technique describe resnet topology cifar- implementation resnet cifar- closely follows conﬁguration ﬁrst layer convolutional layer followed stack layers convolutions feature sizes layers feature size. numbers ﬁlters layers. table resnet- resnet- top- validation error precision activations weight changes. number ﬁlters ﬁrst layers doubled. results end-to-end training network scratch. term cost function training. figure shows schematic training setup. low-precision network converges faster better accuracy trained complex network guides training. table resnet- resnet- top- validation error precision activations weight changes. networks trained supervision full-precision resnet- network using distillation based technique. practice wrpn scheme simple effective starting baseline network architecture change width ﬁlter without changing network design parameter hyper-parameters. reducing precision simultaneously widening ﬁlters keeps total compute cost network at-par baseline cost. thus although number compute operations increase widening ﬁlter maps layer bits required compute operation fraction required using full-precision operations. result appropriate hardware support signiﬁcantly reduce dynamic memory requirements memory bandwidth computational energy speed training inference process. apprentice scheme combine network quantization schemes model compression techniques show accuracies low-precision networks signiﬁcantly improved using knowledge distillation techniques. previous studies model compression large network teacher network small network student network. small student network learns teacher network using technique called knowledge distillation network architecture student network typically different teacher network e.g. investigate student network fewer number neurons hidden layers compared teacher network. work student network similar topology teacher network except student network low-precision neurons compared teacher network neurons operating full-precision. table shows impact lowering precision lowprecision network paired full-precision network. apprentice scheme improve baseline full-precision accuracy. scheme also helps close improved baseline accuracy accuracy lowering precision weights activations. weight activation resnet- resnet- table resnet- resnet- top- validation error precision activations weight changes. networks trained supervision full-precision resnet- network using distillation based technique. ﬁlters ﬁrst layers doubled. wrpn+apprentice scheme combines widening ﬁlters distillation scheme. training process apprentice scheme. table shows results scheme resnet. weights activations resnet- better baseline started similarly resnet- better baseline call approach apprentice low-precision network learning knowledge high precision network. scheme start full-precision trained network transfer knowledge trained network continuously train lowprecision network scratch. knowledge transfer process consists matching logits teacher network softmax scores student network. included addition take advantage low-precision numerics developed accelerator optimized matrix multiplication figure shows schematic accelerator processing engine systolic cross-points. accelerator consists processing engines organized rows columns. ﬁgure matrix corresponds activation tensor consists unsigned integer values operand need ﬂoating-point integer multiplier. weights possible weight values thus multiply integer weight value compare sign operand append operand make zero. accumulator signed integer. compared full-precision accelerator technology process node low-precision accelerator smaller area efﬁcient power. conclusions vendors packing compute bandwidth becoming issue affects scaling large-scale systems. low-precision techniques help scenarios. low-precision networks system-level beneﬁts drawback models degraded accuracy compared full-precision models. present schemes improve accuracy low-precision networks close accuracy models full-precision models. schemes improve accuracy low-precision network conﬁguration compared prior proposals. motivate need smaller model size batch real-time resource constrained inference deployment systems. large batch mode discuss beneﬁts lowering precision activation maps opposed ﬁlter weights. finally discuss design accelerator optimized ternary-precision. references buciluˇa caruana niculescu-mizil model compression proceedings sigkdd international conference knowledge discovery data mining ser. york available http//doi.acm.org/./. mishra marr apprentice using knowledge distillation techniques improve low-precision network accuracy international conference learning representations available https//openreview.net/forum?id=baelzrb mishra nurvitadhi cook marr wrpn wide reduced-precision networks international conference learning representations available https //openreview.net/forum?id=bzvaaeaz rastegari ordonez redmon farhadi xnornet imagenet classiﬁcation using binary convolutional neural networks corr vol. abs/. available http//arxiv.org/abs/. urban geras ebrahimi kahou aslan wang caruana mohamed philipose richardson deep convolutional nets really need deep convolutional? arxiv e-prints mar.", "year": 2018}