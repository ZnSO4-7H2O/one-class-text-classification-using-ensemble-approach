{"title": "Context-Dependent Translation Selection Using Convolutional Neural  Network", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "We propose a novel method for translation selection in statistical machine translation, in which a convolutional neural network is employed to judge the similarity between a phrase pair in two languages. The specifically designed convolutional architecture encodes not only the semantic similarity of the translation pair, but also the context containing the phrase in the source language. Therefore, our approach is able to capture context-dependent semantic similarities of translation pairs. We adopt a curriculum learning strategy to train the model: we classify the training examples into easy, medium, and difficult categories, and gradually build the ability of representing phrase and sentence level context by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points.", "text": "propose novel method translation selection statistical machine translation convolutional neural network employed judge similarity phrase pair languages. speciﬁcally designed convolutional architecture encodes semantic similarity translation pair also context containing phrase source language. therefore approach able capture context-dependent semantic similarities translation pairs. adopt curriculum learning strategy train model classify training examples easy medium difﬁcult categories gradually build ability representing phrase sentence level context using training examples easy difﬁcult. experimental results show approach signiﬁcantly outperforms baseline system bleu points. conventional statistical machine translation system translation model constructed steps first bilingual phrase pairs respecting word alignments extracted word-aligned parallel corpus. second phrase pairs assigned scores calculated using relative frequencies corpus. however ﬁnding utilizing translation pairs based surface forms sufﬁcient conventional approach often fails capture translation pairs grammatically semantically similar. alleviate problems several researchers proposed learning utilizing semantically similar translation pairs continuous space core idea phrases translation pair share semantic meaning similar feature vectors continuous space. matching score computed measuring distance feature vectors phrases incorporated system additional feature. methods however neglect information local contexts proven useful disambiguating translation candidates decoding matching scores translation pairs treated same even different contexts. accordingly methods fail adapt local contexts lead precision issues speciﬁc sentences different contexts. capture useful context information propose convolutional neural network architecture measure context-dependent semantic similarities phrase pairs languages. phrase pair sentence containing phrase source language context. convolutional neural network summarize information phrase pair context compute pair’s matching score multi-layer perceptron. discriminately train model using curriculum learning strategy. classify training examples according difﬁculty level distinguishing positive candidate negative candidate train model learn semantic information easy difﬁcult experimental results large-scale translation task show context-dependent convolutional matching model improves performance bleu points strong phrase-based system. moreover cdcm model signiﬁcantly outperforms context-independent counterpart proving necessary incorporate local contexts smt. contributions. contributions include line work employs local contexts discrete representations words phrases. example marton resnik employed within-sentence contexts consist discrete words guide rule matching. however discrete context features usually suffer data sparseness problem. addition models treated word distinct feature leverage semantic similarity words model. exploited discrete contextual features source sentence learn better bilingual word embeddings smt. however focused frequent phrase pairs induced phrasal similarities simply summing matching scores embraced words. study take account phrase pairs directly compute phrasal similarities convolutional representations local contexts integrating strengths associated convolutional neural networks another line work focuses capturing document-level contexts distributed representations. instance xiao incorporated documentlevel topic information select semantically matched rules. although many sentences share topic document occur sentences actually topics different documents general contexts whole document precise enough speciﬁc sentences contexts different document approach capable learning representations different sentences respectively. moreover learned distributed representations documents rather phrases derived distributed phrase representations corresponding figure architecture cdcm model. convolutional sentence model summarizes meaning tagged sentence target phrase matching model compares representations using multi-layer perceptron. symbol indicates all-zero padding turned gating function. recent years also growing interest bilingual phrase representations group phrases similar meaning across different languages. based translation equivalents share semantic meaning supervise learn semantic phrase embeddings continuous space. example projected phrases source target sides common continuous space language independent. although zhang enforce phrase embeddings sides continuous space exploited transformation semantic embedding spaces. however models focused capturing semantic similarities phrase pairs global contexts neglected local contexts thus ignored useful discriminative information. alternatively integrate local contexts convolutional matching architecture obtain context-dependent semantic similarities. meng zhang proposed independently summary source sentences convolutional neural networks. however extend neural network joint model devlin include whole source sentence focus capturing context-dependent semantic similarities translation pairs. target phrase source sentence contains source phrase aligning ﬁrst project feature vectors convolutional sentence model compute matching score matching model. finally score introduced conventional system additional feature. convolutional sentence model. shown figure model takes input embeddings words iteratively summarizes meaning input layers convolution pooling reaching ﬁxed length vectorial representation ﬁnal layer. layer- convolution layer takes sliding windows respectively models possible compositions neighbouring words. convolution involves ﬁlter produce feature possible composition. given k-sized sliding window example convolution unit composition words generated distinguish phrase pair context additional dimension word embeddings words phrase pair others. transforming words tagged embeddings convolutional sentence model takes multiple choices composition using sliding windows convolution layer. note sliding windows allowed cross boundary source phrase exploit phrasal contextual information. order avoid length variability source sentences target phrases allzero paddings source sentence target phrase maximum length. moreover gate function eliminate effect all-zero padding setting output vector all-zeros input all-zeros. layer- apply local max-pooling non-overlapping windows every convolution max-pooling operations obtain feature vectors source sentence target phrase respectively. matching model. matching score source sentence target phrase measured similarity feature vectors. speciﬁcally multi-layer perceptron nonlinear function similarity compute matching score. first layer combine feature vectors hidden state ideally trained cdcm model expected assign higher matching score positive example lower score negative example employ discriminative training strategy max-margin objective. matching score function deﬁned consists parameters convolutional sentence model mlp. model trained minimizing objective encourage model assign higher matching scores positive examples assign lower scores negative examples. stochastic gradient descent optimize model parameters note cdcm model aims capturing contextual representations distinguish good translation candidates ones various contexts. propose twostep approach. first initialize model context-dependent bilingual word embeddings start strong contextual semantic equivalence word level second train cdcm model curriculum strategy learn context-dependent semantic similarity phrase level easy difﬁcult model initialization plays critical role non-convex problem. initialization cdcm model embeddings words languages real-value dense representation words. typical word embeddings trained monolingual data thus fails capture useful semantic relationship across languages. shown bilingual word embeddings represent substantial step better capturing semantic equivalence word level thus could initialize model strong semantic information. bilingual word embeddings refer semantic embeddings associated across languages similar units language across languages similar representations. utilized word alignments encourage pairs frequently inspired studies propose context-dependent bilingual word embedding model exploits word alignments contextual information shown figure given aligned word pair context extracted nearby window side contextual sequence word pair. vectorial representations similarly calculate matching score y¯ej according bilingual word embedding model also trained minimizing objective negative examples constructed replacing either words randomly chosen corresponding vocabulary. curriculum learning ﬁrst proposed bengio machine learning refers sequence training strategies start small learn easier aspects task gradually increase difﬁculty level. shown curriculum learning beneﬁt nonconvex training giving rise improved generalization faster convergence. point training examples randomly presented organized meaningful order illustrates gradually concepts gradually complex ones. algorithm curriculum training algorithm. denotes training examples initial word embeddings learning rate pre-deﬁned number number training examples. procedure curriculum-training procedure curriculum procedure alg. shows curriculum training algorithm cdcm model. different portions overall training instances different curriculums example training instances consist positive examples easy negative examples easy curriculum latter curriculums gradually increase difﬁculty level training instances curriculum compute gradient loss objective learn using algorithm. note meanwhile update word embeddings better capture semantic equivalence across languages training. loss function section evaluate approach chinese-english translation task. using cdcm model approach achieves signiﬁcant improvement bleu score points. moreover cdcm model signiﬁcantly outperforms context-independent counterpart conﬁrming hypothesis local contexts useful machine translation. section compare model initializations bilingual word embeddings conventional monolingual word embeddings. experimental results show initialization bilingual word embeddings outperforms monolingual counterpart consistently indicating bilingual word embeddings give better initialization cdcm model. carry experiments nist chinese-english translation tasks. training data contains sentence pairs coming dataset. corpus includes ldce ldce ldce hansards portion ldct ldct ldct. train -gram language model xinhua portion gigaword corpus using language toolkit nist evaluation test data development data nist evaluation test data test data. minimum error rate training optimize feature weights. evaluation case-insensitive nist bleu used measure translation performance. training neural networks convolution layers source sentences convolution layers target phrases. them pooling layers used feature maps sliding window learning rate parameters selected based development data. produce high-quality bilingual phrase pairs train cdcm model perform forced decoding bilingual training sentences collect used phrase pairs. obtain unique phrase pairs phrase pairs different contexts. since curriculum training cdcm model requires source phrase least corresponding target phrases obtain phrase pairs remove undesirable ones. baseline baseline system open-source system phrase-based model moses common features including translation models word phrase penalties linear distortion model lexicalized reordering model language model. table evaluation translation quality. cdcmk denotes cdcm model trained curriculum cicm denotes context-independent counterpart combined test sets. superscripts indicate statistically signiﬁcant difference baseline cicm respectively. cicm model following previous works calculate matching degree phrase pair without considering contextual information. unique phrase pair serves positive example randomly selected target phrase phrase table corresponding negative example. matching score also introduced baseline additional feature. table summaries results cdcms trained different curriculums. matter curriculum trained cdcm model signiﬁcantly improves translation quality overall test data best improvement bleu points fully trained cdcm. expected translation performance consistently increased curriculum growing. indicates cdcm model indeed captures desirable semantic information curriculum learning easy difﬁcult. comparing context-independent counterpart cdcm model shows signiﬁcant improvement test data consistently. contribute incorporation useful discriminative information embedded local context. addition performance cicm comparable cdcm. intuitive capture basic semantic similarity source target phrase pair. qualitative analysis. figure lists interesting cases show cdcm model improves performance. analyze phrase pair scores computed cdcm model phrase translation probabilities translation model. first cdcm model scores phrase pairs based rather semantic similarity contextual information co-occurrences corpus. therefore complementary translation model. second growing curriculum model likely capture contextdependent semantic similarities phrase pairs. cases choices translation candidates fully trained cdcm model closer actual translations frequent less frequent phrases. third though cicm model captures semantic similarities phrase pairs fails adapt different local contexts well. contrast cdcm model able provide different translation candidates based discriminative information embedded local contexts. section investigate inﬂuence bilingual word embeddings initialize cdcm model. wordvec train monolingual word embeddings. train bilingual word embeddings using approach described section dimensions bilingual monolingual embeddings table shows comparative results bilingual monolingual word embeddings. seen bilingual word embedding model outperforms monolingual counterpart consistently. reported word-level semantic relationships across languages captured bilingual word embeddings boost machine translation performance. results reconﬁrm ﬁndings. qualitative analysis. figure lists cases show context-dependent bilingual word embeddings produce consistent improvements. seen cdcm model initialized bilingual word embeddings produces discriminative results monolingual counterpart. take cdcm example monolingual word embeddings scenario prefers candidates contain main point bilingual counterpart selects different candidates share semantic meaning. possible reason bilingual contextual information helps capture semantic relationships words across languages thus better phrasal similarities using principle compositionality. convolutional model recursive model. previous works bilingual phrase representations usually employ recurrent neural network recursive autoencoder observed recursive approaches suffer signiﬁcant drop translation quality translating long sentences. contrast kalchbrenner show convolutional model could represent semantic content long sentence accurately. therefore choose convolutional architecture model meaning sentence. limitations. unlike recursive models convolutional architecture ﬁxed depth bounds level composition. task limitation largely compensated network afterwards take global synthesis learned sentence representation. hypotheses tested course research disproved. thought likely difﬁcult curriculum would contribute improvement since circumstance consistent real decoding procedure. turned false shown table possible reason negative examples share semantic meaning positive thus give wrong guide supervised training. constructing reasonable negative examples semantically different positive left future work. paper propose context-dependent convolutional matching model capture semantic similarities phrase pairs sensitive contexts. experimental results show approach signiﬁcantly improves translation performance obtains improvement bleu scores overall test data. integrating deep architecture context-dependent translation selection promising improve machine translation. paper ﬁrst step hope long fruitful journey. future exploit contextual information target side jacob devlin rabih zbib zhongqiang huang thomas lamar richard schwartz john makhoul. fast robust neural network joint models statistical machine translation. federico nicola bertoldi brooke cowan wade shen christine moran richard zens chris dyer ondrej bojar alexandra constantin evan herbst. moses open source toolkit statistical machine translation. haiyang daxiang dong xiaoguang dianhai haifeng wang ting liu. improve statistical machine translation context-sensitive bilingual semantic embedding model. emnlp", "year": 2015}