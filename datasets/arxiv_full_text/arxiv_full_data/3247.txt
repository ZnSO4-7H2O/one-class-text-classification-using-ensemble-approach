{"title": "Sparse Matrix-based Random Projection for Classification", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "As a typical dimensionality reduction technique, random projection can be simply implemented with linear projection, while maintaining the pairwise distances of high-dimensional data with high probability. Considering this technique is mainly exploited for the task of classification, this paper is developed to study the construction of random matrix from the viewpoint of feature selection, rather than of traditional distance preservation. This yields a somewhat surprising theoretical result, that is, the sparse random matrix with exactly one nonzero element per column, can present better feature selection performance than other more dense matrices, if the projection dimension is sufficiently large (namely, not much smaller than the number of feature elements); otherwise, it will perform comparably to others. For random projection, this theoretical result implies considerable improvement on both complexity and performance, which is widely confirmed with the classification experiments on both synthetic data and real data.", "text": "based principle distance preservation gaussian random matrices sparse random matrices sequentially proposed random projection. terms implementation complexity clear sparse random matrix attractive. unfortunately paper indeed propose desired random projection matrix best feature selection performance theoretically analyzing change trend feature selection performance easy reading begin introducing basic notations paper. random matrix denoted rk×d used represent element i-th j-th column indicates vector considering paper concerned binary classiﬁcation following study tend deﬁne samples randomly drawn different patterns high-dimensional datasets respectively. inner product vectors typically written distinguish variable vector written bold. proofs following lemmas typically denote cumulative distribution function minimal integer less maximum integer larger denoted suppose class random matrices rk×d entry distribution lemma formula integer. matrices satisfy lemma different levels sparser matrix implies worse property distance preservation. viewpoint feature selection random projection expected maximize difference arbitrary samples different datasets respectively. usually difference measured euclidean distance denoted terms mutual independence search good random projection equivalent seeking denote feature elements containing discriminative information represent redundant elements tiny variance. subsequently also seperated parts corresponding coordinates feature elements redundant elements respectively. task random projection reduced maximizing implies redundant elements impact feature selection. therefore simpler expression note that better understanding ﬁrst prove relatively simple case {±µ} lemma lemma generalize complicated case distributed formula performance gaussian matrices {±µ} obtained lemma fast decreases µd/π increasing contrast even quickly increases towards µd/π increases. arbitrary adjacent larger average value namely e|s+)/ close µd/π. clarity values remark lemma lemma expands lemma general case |zi| allowed e|s= e|s> since decreases monotonically respect allows variation |zi|. practice real upper bound larger derived sufﬁcient condition lemma. according theoretical condition mentioned above known vector obtain best feature selection ||rf|| quasi-norm counts number nonzero elements desired vector d/df uniformly distributed nonzero elements however practice desired distribution vectors often hard determined since real dataset number words random matrix hold distribution maximizing ratio pr/pr. practice desired distribution implies random matrix exactly nonzero position column simply derived below. assume random matrix rk×d randomly holding nonzero elements column equivalently sd/k nonzero elements derive last equation formula observed increasing sd/k reduce value formula order maximize value indicates desired random matrix nonzero element column. high-dimensional vector. speciﬁc figure observed dense matrices column weight share comparable feature selection performance increases tend sample feature element higher probability. proposed matrix present better performance them ensures ||rf|| high probability equivalently ratio pr/pr relatively large. shown formula condition better satisﬁed also veriﬁed following experiments. roughly estimated lower bound order since proposed matrix column weight leads practice performance advantage seemingly maintained relatively small instance following experiments synthetic data lower bound small phenomenon explained fact obtain performance advantage probability required relatively large rather equal demonstrated remark lemma table classiﬁcation accuracies synthetic data redundant elements suffering three different varying levels best performance highlighted bold. lower bound projection dimension ensures proposal outperforming others datasets highlighted bold well. recall acronyms represent gaussian random matrix sparse random matrix sparse rand matrix redundant elements respectively. close reality introduce unreliability feature elements redundant elements adopting relatively large variances. precisely simulation ﬁxed varies converging zero decrease increases. thus increasing note that probability size dataset data dimension feature dimension table classiﬁcation accuracies face datasets dimension projection dimension best performance highlighted bold. lower bound projection dimension ensures proposal outperforming others datasets highlighted bold well. recall acronyms represent gaussian random matrix sparse random matrix sparse random matrix proposed sparsest random matrix respectively. table classiﬁcation accuracies three datasets dimension projection dimension best performance highlighted bold. lower bound projection dimension ensures proposal outperforming others datasets highlighted bold well. recall acronyms represent gaussian random matrix sparse random matrix sparse random matrix table classiﬁcation accuracies three text datasets dimension projection dimension best performance highlighted bold. lower bound projection dimension ensures proposal outperforming others datasets highlighted bold well. recall acronyms represent gaussian random matrix sparse random matrix sparse random matrix proposed sparsest random matrix respectively. tiny positive number. notational simplicity subscript random variable dropped following proof. ease proof lemma ﬁrst need derive expected value part derives upper bound aforementioned e|s>. simpler expression three factors expression e|s> sequentially represented analyzed respectively.", "year": 2013}