{"title": "Simultaneous Learning of Trees and Representations for Extreme  Classification and Density Estimation", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "We consider multi-class classification where the predictor has a hierarchical structure that allows for a very large number of labels both at train and test time. The predictive power of such models can heavily depend on the structure of the tree, and although past work showed how to learn the tree structure, it expected that the feature vectors remained static. We provide a novel algorithm to simultaneously perform representation learning for the input data and learning of the hierarchi- cal predictor. Our approach optimizes an objec- tive function which favors balanced and easily- separable multi-way node partitions. We theoret- ically analyze this objective, showing that it gives rise to a boosting style property and a bound on classification error. We next show how to extend the algorithm to conditional density estimation. We empirically validate both variants of the al- gorithm on text classification and language mod- eling, respectively, and show that they compare favorably to common baselines in terms of accu- racy and running time.", "text": "consider multi-class classiﬁcation predictor hierarchical structure allows large number labels train test time. predictive power models heavily depend structure tree although past work showed learn tree structure expected feature vectors remained static. provide novel algorithm simultaneously perform representation learning input data learning hierarchical predictor. approach optimizes objective function favors balanced easilyseparable multi-way node partitions. theoretically analyze objective showing gives rise boosting style property bound classiﬁcation error. next show extend algorithm conditional density estimation. empirically validate variants algorithm text classiﬁcation language modeling respectively show compare favorably common baselines terms accuracy running time. several machine learning settings concerned performing predictions large discrete label space. extreme multi-class classiﬁcation language modeling commonly used approach problem reduces series choices tree-structured model leaves typically correspond labels. allows faster prediction many cases necessary make models tractable performance system depend signiﬁcantly structure tree used e.g. york university york york massachussets institute technology cambridge massachussets usa. correspondence yacine jernite <jernitecs.nyu.edu> anna choromanska <acnyu.edu> david sontag <dsontagmit.edu>. sets purely random trees provide efﬁcient datadependent algorithm tree construction training. inspired tree algorithm binary trees present objective function favors high-quality node splits i.e. balanced easily separable. contrast previous work objective applies trees arbitrary width leads guarantees model accuracy. furthermore show successfully optimize setting data representation needs learned simultaneously classiﬁcation tree. finally multi-class classiﬁcation problem closely related conditional density estimation since need consider labels learning prediction time. problems present similar difﬁculties dealing large label spaces techniques present work applied indiscriminately either. indeed show adapt algorithm efﬁciently solve conditional density estimation problem learning language model uses tree structured objective. paper organized follows section discusses related work section outlines necessary background deﬁnes tree-structured objectives multi-class classiﬁcation density estimation section presents objective optimization algorithm section contains theoretical results section adapts algorithm problem language modeling section reports empirical results flickr prediction dataset gutenberg text corpus ﬁnally section concludes paper. supplementary material contains additional material proofs theoretical statements paper. also release implementation algorithm. multi-class classiﬁcation problem addressed literature variety ways. examples include clustering methods later improved sparse output coding iii) variants error correcting output codes variants itrecently proposed tree algorithm differs signiﬁcantly similar hierarchical approaches like example filter trees random trees addresses problem learning good-quality binary node partitions. method results low-entropy trees instead using inefﬁcient enumerate-and-test approach good partition expensive brute-force optimization searches space possible partitions another work uses binary tree example small subset candidate labels makes ﬁnal prediction tractable one-against-all classiﬁer subset identiﬁed proposed recall tree. notable approach based decision trees also include fastxml based optimizing rank-sensitive loss function shows advantage ranking nlp-based techniques context multi-label classiﬁcation. related approaches include sleec classiﬁer extreme multi-label classiﬁcation learns embeddings preserve pairwise distances nearest label vectors ranking approaches based negative sampling another tree approach shows computational speed leads signiﬁcant improvements prediction accuracy. conditional density estimation also challenging settings label space large. underlying problem consists learning probability distribution random variables given context. example language modeling setting learn probability word given previous text either making markov assumption approximating left context last words seen feed-forward neural language models attempting learn low-dimensional representation full history recurrent feed-forward neural probabilistic language models simultaneously learn distributed representation words probability function word sequences expressed terms representations. major drawback models slow train grow linearly vocabulary size make difﬁcult apply number methods proposed overcome difﬁculty. works wordvec reduce model barest bones hidden layer nonlinearities. another proposed approach compute nplm probabilities reduced vocabulary size hybrid neural-n-gram model prediction time. avenues reduce cost computing gradients large vocabularies include using different sampling techniques approximate replacing likelihood objective contrastive spherical loss relying self-normalizing models taking advantage data sparsity using clustering-based methods noted however techniques provide speed test time. similarly classiﬁcation case also signiﬁcant number works tree structured models accelerate computation likelihood gradients various heuristics build hierarchy using ontologies huffman coding algorithm endeavors learn binary tree structure along representation presented iteratively learn word representations given ﬁxed tree structure criterion trades making balanced tree clustering words based current embedding. application present second part paper closely related latter work uses similar embedding context. however setting limited binary trees work arbitrary width provide tree building objective less computationally costly comes theoretical guarantees. section deﬁne classiﬁcation loglikelihood objectives wish maximize. input space label space. joint distribution samples function mapping every input representation parametrized consider objectives. function takes input representation predicts label classiﬁcation objective deﬁned expected proportion correctly classiﬁed examples however also implicitly depend tree structure rest paper provide surrogate objective function determines structure tree show theoretically maximizes criterion equation show empirically maximizes criterion equation denotes proportion nodes reaching node class probability example class reaching sent child probability example class reaching sent child. note have high level maximizing objective encourages conditional distribution class different possible global one; node decision function needs able discriminate examples different classes. objective thus favors balanced pure node splits. call split node perfectly balanced global distribution uniform perfectly pure p·|i takes value either data points class reaching node sent child. section discuss theoretical properties objective details. show maximizing leads perfectly balanced perfectly pure splits. also derive boosting theorem shows number internal nodes tree needs reduce classiﬁcation error arbitrary threshold assumption objective weakly optimized node tree. remark rest paper node functions take input data representation output distribution children used classiﬁcation setting sends data point child highest predicted probability. notation representatree-structured classiﬁcation density estimation show express objectives equations using tree-structured prediction functions illustrated figure figure hierarchical predictor order predict label system needs choose third child node third child node consider tree depth arity leaf nodes internal nodes. leaf corresponds label identiﬁed path root leaf. rest paper following notations correspond node index depth indicates child next path. case classiﬁcation density estimation problems reduced choosing right child node deﬁning probability distribution children given respectively. need replace node decision conditional probability distributions functions respectively. given tree representation function objective functions become oclass section present algorithm simultaneously building classiﬁcation tree learning data representation. maximizing accuracy tree deﬁned equation maximizing objective equation node tree next show lemmas analyze balancedness purity node split isolation i.e. analyze resp. balancedness purity node split resp. purity balancedness ﬁxed perfect. show isolated setting increasing leads balanced pure split. lemma split node perfectly pure next provide bound classiﬁcation error tree. particular show objective weakly optimized node tree weak advantage captured form weak hypothesis assumption algorithm amplify weak advantage build tree achieving desired level accuracy. denote ﬁxed target function domain assigns data point label ﬁxed target distribution together induce distribution labeled pairs label assigned data point tree. denote error tree i.e. ex∼p refers accuracy given equation following theorem holds also want make sure well-formed mary tree step means number labels assigned node always congruent modulo algorithm provides assignment greedily choosing label-child pair still room labels highest value global procedure described algorithm following. start batch re-assign targets node prediction function starting root going tree. node label likely re-assigned child afﬁnity past seen form hierarchical on-line clustering. every example unique path depending label. sample take gradient step node along assigned path lemma algorithm ﬁnds assignment nodes children ﬁxed depth tree increases under well-formedness constraints. remark interesting feature algorithm since representation examples different classes learned together intuitively less risk getting stuck speciﬁc tree conﬁguration. speciﬁcally similar classes initially assigned different children node algorithm less likely keep initial decision since representations examples classes pulled together nodes. start showing maximizing every node tree leads high-quality nodes i.e. perfectly balanced perfectly pure node splits. ﬁrst introduce formal deﬁnitions. deﬁnition split node tree β-balanced instead takes hierarchical approach problem. construct binary tree word corresponds leaf tree thus identiﬁed path root corresponding leaf making sequence choices going left versus right. corresponds treestructured log-likelihood objective presented equation case thus path word deﬁned expression then binary case sigmoid function non-leaf nodes rdr. cost computing likelihood word reduced dr). work authors start training procedure using random tree alternate parameter learning using clusteringbased heuristic rebuild hierarchy. expand upon method providing algorithm allows using hierarchies arbitrary width jointly learns tree structure model parameters. using algorithm algorithm learn good tree structure classiﬁcation model often predicts likely word seeing context however could certainly learn interesting representations tree structure guarantee model would achieve good average log-likelihood. intuitively often several valid possibilities word given immediate left context classiﬁcation objective necessarily take account. another option would learn tree structure maximizes classiﬁcation objective ﬁne-tune model parameters using log-likelihood objective. tried method initial tests approach much better random trees. instead present small modiﬁcation algorithm equivalent log-likelihood training restricted ﬁxed tree setting shown increase value node objectives replacing gradients respect ptarget theorem shows number splits sufﬁce reduce multi-class classiﬁcation error tree below arbitrary threshold shown proof theorem weak hypothesis assumption implies satisfy show tighter version bound assuming node induces balanced split. corollary weak hypothesis assumption says distribution data node tree exists partition weak hypothesis assumption nodes make perfectly balanced splits obtain sufﬁces tree hierarchical bi-linear language model take approach language modeling first using chain rule order markov assumption model probability sentence similarly work also dimensional representation context setting word vocabulary embedding rdr. given context corresponding position represented context embedding vector gain different insights table first although wider trees theoretically slower m-ary tree labels) incomparable time practice always perform better. using algorithm learn structure tree also always leads accurate models gain precision points smaller -ary setting. further importance wider trees learning structure seems less node prediction functions become expressive. high level could imagine setting model learn different dimensions input representation different nodes would minimize negative impact learn representation suited nodes. another thing notice since prediction time depends expected depth label models learned balanced trees nearly fast huffman coding optimal respect given remarks algorithm especially shines settings computational complexity prediction time highly constrained test time mobile devices embedded systems. experiments evaluate classiﬁcation density estimation version algorithm. classiﬁcation used yfccm dataset consists hundred million flickr pictures along captions sets split training validation test examples. focus problem predicting picture’s tags given caption. density estimation learned logbilinear language model gutenberg novels corpus compared perplexity obtained hierarchical losses. experimental settings described greater detail supplementary material. follow setting yfccm prediction task keep tags appear least hundred times leaves label space size compare results obtained fasttext software uses binary hierarchical softmax objective based huffman coding tagspace system uses sampling-based margin loss also extend fasttext software huffman trees arbitrary width. models bagof-word embedding representation caption text; parameters input representation function learn word embeddings caption representation obtained summing embeddings words. experimented embeddings dimension predict caption report precision well training test times table implementation based fasttext open source version added m-ary huffman learned tree objectives. table reports best accuracy obtained hyper-parameter search using version system provide meaningful comparison even though accuracy less reported learn word embeddings using fasttext perform hierarchical clustering vocabulary based these resulting tree learn language model. call approach clustering tree. however hyper-parameter settings tree structure worse random one. conjecture poor performance tree structure means deepest node decisions quite difﬁcult. figure shows evolution test perplexity epochs. appears relevant tree structure learned epoch second epoch learned hierarchical soft-max performs similarly one. figure shows part tree learned gutenberg dataset appears make semantic syntactic sense. paper introduced provably accurate algorithm jointly learning tree structure data representation hierarchical prediction. applied multiclass classiﬁcation density estimation problem showed models’ ability achieve favorable accuracy competitive times settings. notable difference previous task language modeling setting drastically beneﬁt computing make using softmax tractable algorithm requires ﬂexibility thus beneﬁt much gpus small modiﬁcation algorithm allows under maximum depth constraint remain competitive. results presented section obtained using modiﬁed version learns -ary trees depth table presents perplexity results different loss functions along time spent computing learning objective learned tree model nearly three seven times fast train test time respectively objective without losing points perplexity. jason chopra sumit adams keith. tagspace semantic embeddings hashtags. proceedings conference empirical methods natural language processing emnlp october doha qatar meeting sigdat special interest group figure comparison discrete continuous deﬁnitions probabilities binary tree exemplary node e.g. root. denotes sigmoid function. color circles denote data points. remark could deﬁne ratio number examples reach node sent child total number examples reach node ratio number examples reach node correspond label sent child node total number examples reach node correspond label instead look continuous counter-parts discrete deﬁnitions given equations illustrated figure simpliﬁes optimization problem. proof lemma start proving split node perfectly balanced i.e. ∀j={...m}p perfectly pure i.e. ∀j={...m} i={...k} split maximally balanced write thus gradient non-zero. fact fact convex imply maximized extremes interval. thus admits highest value node split perfectly pure. still need show admits highest value node split also perfectly balanced. give proof contradiction thus assume least value least value lets deﬁne sets indices proof theorem weight tree leaf deﬁned probability randomly chosen data point drawn ﬁxed target distribution reaches leaf. suppose time step heaviest leaf weight consider splitting leaf children weight child denoted also ease notation refer pj|i refer furthermore shorthand notice wpj. recall tree leaves. qipj|i andk k-element vector entry equal deﬁne following function expression entropy tree leaves denoted qji. k-element vector entry equal qji. notice contribution same internal node changes splits. next proceed directly proving error bound. denote probability data point reached leaf recall reached label assigned leaf majority label thus lets assume leaf assigned label following true ∀z={...k} ﬁrst inequality taken proof theorem following equality follows fact node balanced. next following exactly steps shown proof theorem obtain corollary. yfccm experiments learned models linearly decreasing rate epochs. hyper-parameter search learning rate learned tree settings learning rate stays constant ﬁrst half training assignlabels routine called times. experiments hogwild data-parallel setting using threads intel xeon .ghz cpu. prediction time perform truncated depth ﬁrst search likely label experiments context window size optimize objectives adagrad hyper-parameter search batch size learning rate hidden representation dimension learned tree settings assignlabels routine called times epoch. used nvidia geforce titan tree-based models -ary. cluster tree learn dimension word embeddings fasttree epochs using hierarchical softmax loss obtain centroids using scikitlearn implementation minibatchkmeans greedily assign words clusters full", "year": 2016}