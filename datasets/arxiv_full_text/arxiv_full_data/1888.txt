{"title": "Learning Eligibility in Clinical Cancer Trials using Deep Neural  Networks", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Interventional clinical cancer trials are generally too restrictive and cancer patients are often excluded from them on the basis of comorbidity, past or concomitant treatments and the fact that they are over a certain age. The efficacy and safety of new treatments for patients with these characteristics are not, therefore, defined. In this work, we build a model with which to automatically predict whether short clinical statements were considered inclusion or exclusion criteria. We used clinical trials protocols on cancer that have been available in public registries for the last 18 years to train word embeddings, and constructed a dataset of 6M short free-texts labeled as eligible or not eligible. We then trained and validated a text classifier, using deep neural networks with pre-trained word-embedding as its inputs, to predict whether or not short free-text statements describing clinical information were considered eligible. The best model achieved an F-measure of 0.92 and an almost perfect agreement when employing a validation set of 800K labeled statements. The trained model was also tested on an independent set of clinical statements mimicking those used in routine clinical practice, yielding a consistent performance. We additionally analyzed the semantic reasoning of the word embedding representations obtained, and were able to identify equivalent treatments for a type of tumor in an analogy with the drugs used to treat other tumors. The present work shows that representation learning using neural networks can be successfully leveraged to extract the medical knowledge available on clinical trial protocols and potentially assist practitioners when prescribing treatments.", "text": "interventional clinical cancer trials generally restrictive cancer patients often excluded basis comorbidity past concomitant treatments fact certain age. eﬃcacy safety treatments patients characteristics therefore deﬁned. work build model automatically predict whether short clinical statements considered inclusion exclusion criteria. used clinical trials protocols cancer available public registries last years train word embeddings constructed dataset short free-texts labeled eligible eligible. trained validated text classiﬁer using deep neural networks pre-trained word-embedding inputs predict whether short free-text statements describing clinical information considered eligible. best model achieved f-measure almost perfect agreement employing validation labeled statements. trained model also tested independent clinical statements mimicking used routine clinical practice yielding consistent performance. additionally analyzed semantic reasoning word embedding representations obtained able identify equivalent treatments type tumor analogy drugs used treat tumors. present work shows representation learning using neural networks successfully leveraged extract medical knowledge available clinical trial protocols potentially assist practiclinical trials provide evidence needed determine safety eﬀectiveness medical treatments. trials basis employed clinical practice guidelines greatly assist clinicians daily practice making decisions regarding treatment. however eligibility criteria used oncology trials restrictive patients often excluded basis comorbidity past concomitant treatments fact certain patients selected therefore mimic clinical practice. signiﬁes results obtained cannot extrapolated patients clinical proﬁles excluded clinical trial protocols. given clinical characteristics particular patients type cancer intended treatment discovering whether represented corpus available requires manual review numerous eligibility criteria impracticable clinicians daily basis. process would therefore greatly beneﬁt evidence-based clinical decision support system work construct dataset using clinical trial protocols published largest public registry available train validate model able predict whether short free-text statements considered eligible eligible criteria trials. model intended inform clinicians whether results obtained therefore whether recommendation standard guidelines conﬁdently applied particular patient. ultimate goal work assess whether representation learning using neural networks could successfully applied extract medical knowledge available clinical trial protocols thus paving toward involved complex projects. figure system architecture objective ﬁnal model predict whether short clinical statements concerning type tumor including molecular proﬁle oncologic treatment medical history concomitant medication included clinical trials. extracting bigrams word-embeddings explore using diﬀerent state-of-the-art neural networks text classiﬁcation. finally validating comparing ﬁnal text classiﬁers best model tested independent testing set. system architecture shown figure remainder paper organized follows. following section provides brief review methods related proposed work. section describes dataset constructed methodology used including details employed train embeddings text classiﬁers. evaluation results detailed section along analysis word embeddings artiﬁcial intelligence methods roughly divided rule-based systems traditional machine learning algorithms representation learning systems include deep learning methods. rule-based approaches natural language processing seek hard-code biomedical knowledge formal-languages computer automatically reason text statements formal languages using logical inference rules. popular rule-based biomedical processing tool broader domain biomedical language probably metamap named entity recognition system identiﬁes concepts uniﬁed medical language system metathesaurus text clinical text analysis knowledge extraction system dnorm many systems built upon tools. example system extracting cancer phenotypes clinical records built describe cancer cases basis mention-annotation pipeline based ontology ctakes system phenotype summarization pipeline based apache unstructured information management architecture regard speciﬁc domain clinical trials prior work focused problem formalizing eligibility criteria using rule-based approaches obtaining computational model could used clinical trial matching semantic reasoning tasks. several languages could applied order express eligibility criteria arden syntax gello ergo among others. weng presented rich overview existing options. semanticct allows formalization eligibility criteria using prolog rules. milian applied ontologies regular expressions express eligibility criteria semantic queries. however problem structuring eligibility criteria clinical trials obtain generalizable model still remains unsolved. devising formal rules representations suﬃcient complexity accurately describe biomedical knowledge problematic. example problem discrete representations biomedical taxonomies ontologies miss nuances words addition subjective require human labor create adapt them traditional machine learning features employed train algorithms support vector machines k-nearest neighbors hand-crafted representation learning features learned nonetheless many factors regarding variation inﬂuence semantic interpretation biomedical language thus making diﬃcult extract high-level abstract features text. deep learning solves central problem means representation learning introducing representations expressed terms simpler representations. deep learning models beginning achieve greater accuracy semantic capabilities prior state regards various biomedical tasks automatic clinical text annotation classiﬁcation. example recent work presented attentional convolutional network predicts medical codes clinical text. aggregates information throughout document using uses attention mechanism select relevant segments thousands possible codes. regard clinical text classiﬁcation tasks proposed approach automatically classify clinical text sentence level using deep cnns represent complex features. best knowledge work ﬁrst reported study explore deep learning techniques order directly achieve semantic interpretation eligibility criteria clinical trials. contrast classic approaches omit constraints limitations previous steps tokenization stemming syntactic analysis named entity recognition tagging concepts ontologies rule deﬁnition manual selection features build model. resulting dataset data preprocessing available. downloaded follows structure ﬁelds deﬁned schema clinical trials relevant data project derived intervention condition eligibility ﬁelds written unstructured free-text language. information eligibility criteria exclusion inclusion criteria sets phrases and/or sentences displayed free format paragraphs bulleted lists enumeration lists etc. none ﬁelds common standards enforce standardized terms medical dictionaries ontologies. moreover language problems polysemy synonymy. original data exploited merging eligibility criteria together study condition intervention subsequently transforming lists short labeled clinical statements processes detailed following section. transformed eligibility criteria sequences plain words separated whitespace. eligibility criterion augmented information concerning study intervention cancer type. done eration diﬀerent kinds bullets lists mistakenly splitting sentences common abbreviations used mutations medical notations include dots semicolons hyphens. symbols separators single-character words extracted text. words lower-cased. decided remove stop words many them semantically relevant clinical statements. scope work deﬁne bigrams commonly found phrases frequent medicine. frequent bigrams detected replaced text. bigrams represent idiomatic phrases compositions individual words. feeding single entity word-embedding rather word separately therefore allows phrase representations learnt. corpus excluding common terms stop words unnecessary generating bigrams. examples bigrams dataset sunitinib malate glioblastoma multiforme immuno histochemistry willebrand dihydropyrimidine dehydrogenase fraumeni etc. dataset several tests suitable threshold discounting coeﬃcient based count discounting factor prevents occurrence many phrases consisting infrequent words. total diﬀerent bigrams retrieved corpus substituted text. preprocessing cleaning data available short clinical statements containing total words. vocabulary consisted diﬀerent words. majority statements contained less words average ground truth automatically labeled clinical statements previously processed eligibility criteria study conditions interventionseligible eligible basis classes unbalanced labeled eligible labeled eligible. dataset suﬃciently large used random balanced undersampling correct resulting reduced dataset labeled samples. eligibility variable containing text criterion expected highly sparse distribution entries repeated. hyperparameter learning rate size word vectors size context window number epochs min. number word occurrences num. negative sampled loss function sampling threshold number buckets minimum length char n-gram maximum length char n-gram rate updates learning rate learns word predicting surrounding context predicts word given surrounding context using gradient descent randomly initialized vectors. work used wordvec skip-gram model. main diﬀerentiating characteristic fasttext embeddings apply char n-grams take account internal structure words learning word representations especially useful morphologically rich languages. fasttext models char n-grams perform signiﬁcantly better carrying syntactic tasks semantic tasks syntactic questions related morphology words. table shows hyperparameters used generate dimensional embeddings fasttext gensim wordvec models. gensim model trained workers ﬁnal vocabulary words using skip-grams cbow models. next stage consists sentence classiﬁcation. this explored diﬀerent neural networks topologies deep convolutional neural networks without pretrained word-embeddings input layer fasttext learning curves built models increasing dataset sizes dataset balanced class label randomly sampled full set. split dataset samples training test set. standard -fold cross validation performed dataset size. accuracy concerning sentence classiﬁcation depends dataset evaluated unable previous reports used present corpus text classiﬁcation clearly deﬁned benchmarks perform comparison. example diﬀerent domains reported accuracy regards classifying hacker news posts diﬀerent categories using similar method case movie reviews reported performance medical domain high performance model potentially useful cdss. used previously published computer systems related work basis deﬁne minimum target accuracy ﬁrst experiment pre-trained word-embeddings used input model ﬁnal dense output layer. diﬀerent experiment also trained word-embeddings classiﬁcation task scratch. training data suﬃciently large vocabulary coverage also appropriate cancer research domain expected model would beneﬁt training embeddings particular domain. convert sentences dataset sequences word indexes. word index simply integer identiﬁer word. considered commonly occurring words dataset truncated sequences maximum length words. description dimensional embedded word sequences convolutions stride relu activation pooling stride convolutions stride relu activation pooling stride convolutions stride relu activation pooling stride fully connected layer relu activation fully connected layer softmax activation fasttext computationally eﬃcient method starts embedding layer maps vocabulary indexes dimensions. adds global average pooling layer averages embeddings words sentence. finally projects onto single unit output layer squashes sigmoid. precision fraction retrieved instances relevant. recall fraction relevant instances retrieved. precision recall harmonic mean precision sensitivity calculated cohen’s kappa statistic measures inter-rater agreement qualitative items. generally considered robust measure simple percent agreement calculation since takes account possibility agreement occurring chance calculated relative observed agreement among raters hypothetical probability chance agreement using observed data calculate probabilities observer randomly yielding category. learning rate size word vectors size context window number epochs minimum number word occurences number negative sampled softmax loss function minimum length char n-gram maximum length char n-gram maximum length word n-gram sampling threshold rate updates learning rate pretrained word vectors supervised learning learning curve shows evolution training number training samples increased curve converged score obtained training sample maximum using full data shown table indicates estimator suﬀered bias error variance issue reported phenomenon able increase performance additional data overcome deep learning models applied complex problems contrast fast thin architecture fasttext contrary model suﬀer variance error. cross-validation used assess well results model generalized unseen data sets obtained robust average validation results results obtained using gensim fasttext generated embeddings studied obtained similar using using pretrained word-embeddings. number dimensions epochs great impact performance computational cost model. eﬃciency. noisiness gradient estimate reduced batch sizes using higher values. explained fact updating single sample noisy sample good representation data. consider batch size representative whole dataset. values higher predictive performance deteriorated earlier epochs training therefore chose value fact reported loss function landscape deep neural networks large-batch methods almost invariably attracted regions sharp minima that unlike small batch methods unable escape basins minimizers. using larger batch consequently signiﬁcant degradation quality model measured ability generalize. nonetheless also bias error case model achieved higher scores training validation sets converging maximum beyond adding data appear beneﬁcial. model whole dataset using training examples bigrams pretrained word embeddings eventually yielded accuracy validation comprised samples. coeﬃcient agreement predicted true labels validation regarded almost perfect agreement implies model reliable. finally order assess potential proposed approach clinical decision support system checked performance using clinical practice simulation. ﬁnal models were therefore tested unseen inputs consisting small short clinical statements would used routine clinical practice. although test size small able draw meaningful conclusions models yielded promising results accuracy favors hypothesis would possible generalize model diﬀerent source data beyond clinical trial protocol eligibility criteria texts source used build validate examples correctly classiﬁed statements would require expert knowledge oncology judge whether cases studied available clinical trials shown below. performance achieved classiﬁer expectations almost perfect agreement outperforming fasttext results. therefore conclude possible address problem predicting whether short clinical statements extracted eligibility criteria considered eligible available corpus clinical cancer trials. word embeddings interesting part work. adding pretrained embeddings classiﬁers alter classiﬁcation results. however embeddings were themselves suﬃciently interesting qualitatively assessed discussed using word space visualizations. word-embeddings obtained fasttext word represented dimensional space used basis visualize subset words reduced space. t-distributed stochastic neighbor embedding purpose dimensionality reduction method particularly well suited visualization high-dimensional datasets. objective algorithm compute probability distribution pairs high dimensional samples similar prototypes high probability clustered together. algorithm subsequently projects probabilities dimensional space optimizes distance respect sample’s location space. deﬁne words complete corpus wish analyze obtain vectors words. t-sne representation figure shows aspects hand words grouped semantic similarities other clusters seem follow spatial distribution diﬀerent regions diagonal direction intrinsic/internal extrinsic/external concepts tensorboard tensorflow provides built-in visualizer called embedding projector interactive visualization analysis high-dimensional data. wordvec embeddings obtained gensim converted tensorﬂow tensor metadata formats embedding visualization. figure shows example results using word ultrasound query. appreciate nearest points ultrasound related explorations mainly medical imaging. nearest neighbor distances also consistent using concepts. example table shows model successfully extracted hormonal therapies breast cancer t-sne nearest neighbors tamoxifen. also used resulting word vectors generate word clusters ﬁtting k-means model number clusters estimated applying reduction factor total number words read implementation resulting clusters found https //github.com/auriml/capstone. upon sampling clusters random total judged relevant regards whether words syntactically semantically related. examples shown table note medical abbreviations lfour copd correctly clustered. word vectors generated also useful regards accurately resolving analogy problems tamoxifen used treat breast cancer used treat prostate cancer?. top-n similar words word raroxifene letrozole anastrozole fulvestrant arimidex antiandrogens exemestane aromatase antiestrogens toremifene serm estrogens agonists fact precise results terms belong hormone-therapy family drugs speciﬁcally used treat prostatic cancer equivalents tamoxifen breast cancer. words model learned abstract concept hormone-therapy family drugs able apply distinctively depending tumor type. work trained validated compared text classiﬁers corpus cancer clinical trial protocols. models classify short free-text sentences describing clinical information largest available balanced million labeled samples total million. overall models proved robust ability generalize. best performance achieved using balanced sampling whole dataset. results expectations agreement model also evaluated independent clinical data source thus paving toward potential -taking account pending improvementsclinical support system oncologists employing clinical notes. experiments word-embedding models achieved high quality clusters addition demonstrating capacity semantic reasoning since able identify equivalent treatments type tumor means analogy drugs used treat tumors. interesting reasoning qualities merit study future work using dataset. evaluation results show clinical trial protocols related cancer freely available meaningfully exploited applying representation learning including deep learning techniques thus opening immediate future work include eﬀectiveness interventions model thus enabling predict whether patients case studied also whether proposed treatment expected eﬀective based indication.", "year": 2018}