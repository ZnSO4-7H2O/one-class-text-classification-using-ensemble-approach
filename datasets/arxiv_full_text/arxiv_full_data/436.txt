{"title": "Exact solutions to the nonlinear dynamics of learning in deep linear  neural networks", "tag": ["cs.NE", "cond-mat.dis-nn", "cs.CV", "cs.LG", "q-bio.NC", "stat.ML"], "abstract": "Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.", "text": "despite widespread practical success deep learning methods theoretical understanding dynamics learning deep neural networks remains quite sparse. attempt bridge theory practice deep learning systematically analyzing learning dynamics restricted case deep linear neural networks. despite linearity input-output networks nonlinear gradient descent dynamics weights change addition hidden layer. show deep linear networks exhibit nonlinear learning phenomena similar seen simulations nonlinear networks including long plateaus followed rapid transitions lower error solutions faster convergence greedy unsupervised pretraining initial conditions random initial conditions. provide analytical description phenomena ﬁnding exact solutions nonlinear dynamics deep learning. theoretical analysis also reveals surprising ﬁnding depth network approaches inﬁnity learning speed nevertheless remain ﬁnite special class initial conditions weights deep networks incur ﬁnite depth independent delay learning speed relative shallow networks. show that certain conditions training data unsupervised pretraining special class initial conditions scaled random gaussian initializations cannot. exhibit class random orthogonal initial conditions weights that like unsupervised pre-training enjoys depth independent learning times. show initial conditions also lead faithful propagation gradients even deep nonlinear networks long operate special regime known edge chaos. deep learning methods realized impressive performance range applications visual object classiﬁcation speech recognition natural language processing successes achieved despite noted difﬁculty training deep architectures indeed many explanations difﬁculty deep learning advanced literature including presence many local minima curvature regions saturating nonlinearities exponential growth decay back-propagated gradients furthermore many neural network simulations observed strikingly nonlinear learning dynamics including long plateaus little apparent improvement followed almost stage-like transitions better performance. however quantitative analytical understanding rich dynamics deep learning remains elusive. example determines time scales deep learning unfolds? training speed retard depth? conditions greedy unsupervised pretraining speed learning? ﬁnal learned internal representations depend statistical regularities inherent training data? provide exact analytical theory learning deep linear neural networks quantitatively answers questions restricted setting. linearity input-output deep linear network always rewritten shallow network. sense linear network gain expressive power depth hence underﬁt perform poorly complex real world problems. lacks important aspect practical deep learning systems deep linear network nonetheless exhibit highly nonlinear learning dynamics dynamics change increasing depth. indeed training error function network weights non-convex gradient descent dynamics non-convex error surface exhibits subtle interplay different weights across multiple layers network. hence deep linear networks provide important starting point understanding deep learning dynamics. answer questions derive analyze nonlinear coupled differential equations describing learning dynamics weight space function statistical structure inputs outputs. exact time-dependent solutions nonlinear equations well conserved quantities weight dynamics arising symmetries error function. solutions provide intuition deep network successively builds information statistical structure training data embeds information weights internal representations. moreover compare analytical solutions learning dynamics deep linear networks numerical simulations learning dynamics deep non-linear networks analytical solutions provide reasonable approximation. solutions also reﬂect nonlinear phenomena seen simulations including alternating plateaus sharp periods rapid improvement. indeed shown previously nonlinear learning dynamics deep linear networks sufﬁcient qualitatively capture aspects progressive hierarchical differentiation conceptual structure seen infant development. next apply solutions investigate commonly used greedy layer-wise pretraining strategy training deep networks recover conditions pretraining speeds learning. show conditions approximately satisﬁed mnist dataset unsupervised pretraining therefore confers optimization advantage deep linear networks applied mnist. finally exhibit class random orthogonal initial conditions weights that linear networks provide depth independent learning times show initial conditions also lead faithful propagation gradients even deep nonlinear networks. show initial conditions also lead faithful propagation gradients even deep nonlinear networks long operate special regime known edge chaos. regime synaptic gains tuned linear ampliﬁcation propagation neural activity weight matrices exactly balances dampening activity saturating nonlinearities. particular show even nonlinear networks operating special regime jacobians involved backpropagating error signals like near isometries. begin analyzing learning three layer network linear activation functions number neurons layer inputoutput network wish train network learn particular input-output yµxµt input-output correlation matrix measures time units iterations; varies network seen examples corresponding iteration. despite linearity network’s input-output gradient descent learning dynamics given constitutes complex coupled nonlinear differential equations cubic interactions weights. fundamental goal understand dynamics learning function input statistics input-output statistics general outcome learning reﬂect interplay input correlations described input-output correlations described begin though simplify analysis focusing case orthogonal input representations assumption hold exactly whitened input data widely used preprocessing step. assumed orthogonal input representations input-output correlation matrix contains information dataset used learning plays pivotal role learning dynamics. consider singular value decomposition central analysis. orthogonal matrix whose columns contain input-analyzing singular vectors reﬂect independent modes variation input orthogonal matrix whose columns contain output-analyzing singular vectors reﬂect independent modes variation output matrix whose nonzero elements diagonal; elements singular values ordered performing change variables synaptic weight space dynamics simplify gain intuition equations note matrix elements connected neurons connecting input mode layer neurons next layer think matrix element hidden neuron matrix element connecting hidden neuron output mode intuitively column vector column synaptic weights presynaptic hidden layer coming input mode column vector display interesting combination cooperative competitive interactions. consider ﬁrst terms equation. terms connectivity modes layers associated input-output mode strength cooperate drive larger magnitudes well point similar directions space hidden units; fashion terms drive product connectivity modes reﬂect input-output mode strength second terms describe competition connectivity modes ﬁrst second layers associated different input modes yields symmetric pairwise repulsive force distinct pairs ﬁrst second layer connectivity modes driving network decoupled regime different connectivity modes become orthogonal. ﬁxed point structure gradient descent learning linear networks worked language connectivity modes necessary condition ﬁxed point sαδαβ zero whenever satisfy relations undercomplete hidden layers nonzero values since rank nonzero values families ﬁxed points. however ﬁxed points unstable except ﬁrst strongest modes i.e. active. thus remarkably dynamics saddle points non-global local minima terms original synaptic variables globally stable ﬁxed points satisfy hence learning converged network represent closest rank approximation true input-output correlation matrix. work interested understanding dynamical weight trajectories learning time scales lead ﬁnal ﬁxed point. difﬁcult though exactly solve starting arbitrary initial conditions competitive interactions different input-output modes. therefore gain intuition general dynamics restrict attention special class initial conditions form connectivity modes zero figure left dynamics learning three layer neural network. curves show strength network’s representation seven modes input-output correlation matrix course learning. traces show analytical curves eqn. blue traces show simulation full dynamics linear network small random initial conditions. green traces show simulation nonlinear three layer network tanh activation functions. generate mode strengths nonlinear network computed nonlinear network’s evolving input-output correlation matrix plotted diagonal elements tanhv time. training consists orthogonal input patterns associated -dimensional feature vector generated hierarchical diffusion process described level binary tree probability modes plotted rest excluded clarity. network training parameters right delay learning competitive dynamics sigmoidal nonlinearities. vertical axis shows difference simulated time half learning analytical time half learning fraction analytical time half learning. error bars show standard deviation simulations random initializations. solutions partially overlapping distinct initial conditions discussed supplementary appendix ﬁxed collection vectors form orthonormal basis synaptic connections input output mode onto hidden units. thus initial conditions point direction alpha differ scalar magnitudes orthogonal connectivity modes. initialization obtained computing taking dart rdbv diagonal arbitrary orthogonal matrix; however show subsequent experiments solutions also excellent approximations trajectories small random initial conditions. straightforward verify starting initial conditions remain parallel future time. furthermore different active modes orthogonal other compete even interact thus ability decouple connectivity modes yields dramatically simpliﬁed dimensional nonlinear system. equations solved noting arise gradient descent error implies product monotonically approaches ﬁxed point initial value. moreover satisﬁes symmetry parameter family scaling transformations symmetry implies noether’s theorem existence conserved quantity namely constant motion. thus dynamics simply follows hyperbolas constant plane approaches hyperbolic manifold ﬁxed points origin also ﬁxed point unstable. fig. shows typical phase portrait dynamics. measure timescale learning interested long takes approach given initial condition. case unequal treated supplementary appendix space constraints. pursue explicit solution assumption reasonable limit starting small random initial conditions. track dynamics obeys time takes travel assume small initial condition within ﬁxed point i.e. learning timescale limit yields result timescale learning input-output mode correlation matrix inversely proportional correlation strength mode. thus stronger input-output relationship quicker learned. time course describes temporal evolution product magnitudes weights input mode hidden layers hidden layers output mode. product starts small value displays sigmoidal rise asymptotes sigmoid exhibit sharp transitions state learning full learning. analytical sigmoid learning curve shown fig. yield reasonable approximation learning curves linear networks start random initial conditions orthogonal decoupled invariant manifold–and therefore exhibit competitive dynamics connectivity modes–as well nonlinear networks solving task. note though nonlinear networks behaved similarly linear case particular task likely problem dependent. network analyzed section minimal example multilayer single layer hidden units. gradient descent much deeper networks? make initial attempt direction based initial conditions yield particularly simple gradient descent dynamics. linear neural network layers hence weight matrices indexed gradient descent dynamics written describe initial conditions suppose orthogonal matrices diagonalize starting weight matrices l+wlrl special case requirement essentially demands output singular vectors layer input singular vectors next layer change mode strength layer propagates output without mixing modes. note formulation restrict hidden layer size; hidden layer different size undercomplete overcomplete. making change variables rl+w along assumption leads decoupled connectivity modes evolve independently other. analogy simpliﬁcation occurring three layer network connectivity mode layered network described scalars anl− whose dynamics obeys gradient descent energy function integrated positive integer though expression complicated. overall strength increases sufﬁciently learning explodes rapidly. eqn. lets study dynamics learning depth limits inﬁnity. particular dynamics learning time measured numremarkably implies that ﬁxed learning rate iterations required tends zero goes inﬁnity. result depends continuous time formulation however. implementation operate discrete time must choose ﬁnite learning rate yields stable dynamics. learning rate derived maximum eigenvalue hessian region interest. linear networks optimal learning rate αopt decays large depth incorporating dependence learning rate depth learning time depth approaches inﬁnity still surprisingly remains ﬁnite optimal learning rate difference learning times network network small emphasize analysis learning speed based number iterations required amount computation–computing iteration deep network require time shallow network. verify predictions trained deep linear networks mnist classiﬁcation task depths ranging used hidden layers size calculated iteration training error fell ﬁxed threshold corresponding nearly complete learning. optimized learning rate separately depth training network twenty rates logarithmically spaced picking fastest. supplementary appendix full experimental details. networks initialized decoupled initial conditions starting initial mode strength fig. shows resulting learning times saturate empirically optimal learning rates scale like predicted. thus learning times deep linear networks start decoupled initial conditions ﬁnite amount slower shallow network regardless depth. moreover delay incurred depth scales inversely size initial strength association. hence ﬁnding initialize mode strengths large values crucial fast deep learning. previous subsection revealed existence decoupled submanifold weight space connectivity modes evolve independently learning learning times independent depth even arbitrarily deep networks long initial composite mode strength denoted above every connectivity mode numerical weight initilization procedures close weight manifold exploit rapid learning properties? breakthrough training deep neural networks started discovery greedy layer-wise unsupervised pretraining could substantially speed improve generalization performance standard gradient descent unsupervised pretraining shown speed optimization deep networks also special regularizer towards solutions better generalization performance time recent results obtained excellent performance starting carefullyscaled random initializations though interestingly pretrained initializations still exhibit faster convergence examine analytically unsupervised pretraining achieves optimization advantage least deep linear networks ﬁnding special class orthogonalized decoupled initial conditions previous section allow rapid supervised deep learning input-output tasks certain precise structure. subsequently analyze properties random initilizations. consider following pretraining ﬁnetuning procedure first using autoencoders unsupervised pretraining module network trained produce input output subsequently network ﬁnetuned ultimate input-output task interest following consider case simplicity. simply input correlation matrix pretraining phase input-output correlation matrix hence qλqt eigenvectors diagonal matrix variances. analysis learning dynamics section directly apply input correlation matrix white. supplementary appendix generalize results handle case. pretraining weights approach since reach ﬁxed point ﬁnite time diagonal matrix approaching identity matrix figure mnist satisﬁes consistency condition greedy pretraining. left submatrix mnist input correlation matrix center submatrix approximately diagonal required. right learning curves mnist layer linear network starting random pretrained initial conditions. pretrained curve starts delay pretraining time. small random initial conditions correspond weights chosen i.i.d. zero mean gaussian standard deviation learning. hence general invertible matrix. starting small random weights though weight matrix roughly balanced contribution overall map. corresponds orthogonal. hence pretraining phase input-to-hidden mapping arbitrary orthogonal matrix. consider ﬁne-tuning phase. weights trained ultimate task interest input-output correlations matrix begins pretrained initial condition ﬁne-tuning task decoupled initial condition written clearly possible initial condition obtained pretraining also decoupled initial condition ﬁnetuning phase initial mode strengths near one. hence state underlying condition required successful greedy pretraining deep linear networks right singular vectors ultimate input-ouput task interest must similar principal components input data quantitatively precise instantiation intuitive idea unsupervised pretraining help subsequent supervised learning task statistical structure input consistent structure input-output learned. moreover quantitative instantiation intuitive idea gives simple empirical criterion evaluated dataset given input-output correlation input correlation compute right singular vectors check approximately diagonal. condition eqn. holds autoencoder pretraining properly decoupled initial conditions appreciable initial association strength near argument also goes straightforwardly layer-wise pretraining deeper networks. fig. shows consistency condition empirically holds mnist pretrained deep linear neural network learns faster started small random initial conditions even accounting pretraining time note analysis unlikely carry completely nonlinear networks. nonlinear networks approximately linear initialization small random initializations hence solutions describe dynamics well early learning. however network enters nonlinear regime solutions expected remain accurate. alternative greedy layerwise pre-training proposed choosing appropriately scaled initial conditions weights would preserve norm typical error vectors backpropagated deep network. context appropriate norm-preserving scaling initial condition connectivity matrix layers corresponds choosing weight i.i.d. figure left learning time function depth different initial conditions weights greedy unsupervised pre-training random orthogonal matrices curve lies green curve. middle optimal learning rates function depth different weight initilizations. right eigenvalue spectrum complex plane random orthogonal matrix. histograms singular values products independent random gaussian matrices whose elements chosen i.i.d. cases histograms taken zero mean gaussian standard deviation realizations random product matrices yielding total singular values histogram. histograms eigenvalue distributions complex plane product matrices width visualization purposes containing origin removed case; would otherwise dominate histogram middle right plots contains eigenvalues respectively. choice zero mean gaussian standard deviation denotes average distribution random matrix moreover distribution concentrates mean large thus scaling linear networks forward propagation activity backpropagation gradients typically norm-preserving. however initialization learning time depth linear networks trained mnist grows depth growth distinct contradiction theoretical prediction made above depth independent learning times starting decoupled submanifold weights composite mode strength suggests scaled random initialization scheme despite norm-preserving nature submanifold weight space. contrast learning times greedy layerwise pre-training grow depth consistent predictions theory whereas greedy pre-training ﬁnds composite mode strength closer simple random initialization scheme enjoy rapid learning properties greedylayerwise pre-training? empirically show choose initial weights layer random orthogonal matrix instead scaled random gaussian matrix orthogonal random initialization condition yields depth independent learning times like greedy layerwise pre-training theoretically random orthogonal initializations yield depth independent learning times scaled random gaussian initializations despite norm preserving nature? answer lies eigenvalue singular value spectra products gaussian versus orthgonal random matrices. single random orthogonal matrix eigenvalue spectra lying exactly unit circle complex plane eigenvalue spectra random gaussian matrices whose elements variance form uniform distribution solid disk radius complex plane moreover singular values orthogonal matrix exactly squared singular values scaled gaussian random matrix well known marcenko-pasteur distribution nontrivial spread even consider product matrices across layers representing total propagation activity across deep linear network random choice weights layer wtot random matrix. average preserves norm typical vector matter whether matrices layer gaussian orthogonal. however singular value spectra wtot differ markedly cases. random orthogonal initilization layer wtot orthogonal matrix therefore singular values equal however random gaussian initialization layer complete theoretical characterization singular value distribution wtot. computed numerically function different depths fig. develops highly kurtotic nature depth increases. singular values become vanishingly small long tail large singular values remain. thus wtot preserves norm typical randomly chosen vector highly anisotropic manner strongly amplifying projection onto small subset singular vectors attenuating directions. intuitively wtot well linear operator would closely related backpropagation gradients early layers amplifying projection operators large depth contrast eigenvalues wtot scaled gaussian case concentrate closer origin depth increases. discrepancy behavior eigenvalues singular values wtot phenomenon could occur eigenvectors wtot highly non-orthogonal reﬂects highly non-normal nature products random gaussian matrices combination ampliﬁcation projection wtot preserve norm clear good backpropagate errors; projection error vectors onto high dimensional subspace corresponding small singular values would strongly attenuated yielding vanishingly small gradient signals corresponding directions early layers. effect present random orthogonal initializations greedy pretraining would naturally explain long learning times starting scaled random gaussian initial conditions relative initilizations fig. left. linear nonlinear networks likely appropriate condition weights generating fast learning times would dynamical isometry. mean product jacobians associated error signal backpropagation near isometry overall global scaling subspace high dimension possible. equivalent many singular values product jacobians possible within small range around constant closely related notion restricted isometry compressed sensing random projections. preserving norms necessary sufﬁcient condition achieving dynamical isometry large depths demonstrated fig. shown linear networks orthogonal initializations achieve exact dynamical isometry singular values greedy pre-training achieves approximately. note discrepancy learning times scaled gaussian initialization orthogonal pre-training initializations modest depths around used large scale applications magniﬁed larger depths explain modest improvement learning times greedy pre-training versus random scaled gaussian initializations observed applications predict modest improvement magniﬁed higher depths even nonlinear networks. finally note recurrent networks thought inﬁnitely deep feed-forward networks tied weights promising approach modiﬁcation training objective partially promotes dynamical isometry gradients currently back-propagated shown deep random orthogonal linear networks achieve perfect dynamical isometry. show nonlinear versions networks also achieve good dynamical isometry properties. consider nonlinear feedforward dynamics random orthogonal connectivity matrix layer scalar gain factor nonlinearity saturates show supplementary appendix exists critical value gain activity decay away zero propagates layers strong linear positive gain combat damping saturating nonlinearity activity propagate indeﬁnitely without decay matter deep network nonlinearity mean activity layer approximately dynamical properties quantitatively captured neural population variance layer thus liml→∞ liml→∞ tanh compute numerically compute fig. supplementary appendix thus nonlinear feedforward networks exhibit phase-transition critical gain; critical gain inﬁnitely deep networks exhibit chaotic percolating activity propagation call critical gain edge chaos analogy terminology recurrent networks. interested errors ﬁnal layer backpropagate back earlier layers whether gradients explode decay depth. quantify this simplicity consider jacobian captures input perturbations propagate output. singular value distribution jacobian well-behaved extremely large small singular values backpropagation gradients also well-behaved exhibit little explosion decay. jacobian evaluated particular point space output layer activations point turn obtained iterating starting initial input layer activation vector thus singular value distribution figure singular value distribution jacobian deﬁned various values gain input layer population variance network architecture consists layers neurons layer linear case fig. jacobian depend gain also initial condition rotational symmetry expect distribution depend population variance thus large singular value distribution end-to-end jacobian linear case) depends parameters gain input population variance numerically computed singular value distribution function parameters fig. single random orthogonal nonlinear network results typical; replotting results different random networks different initial conditions yield similar results. edge chaos linear dampening many layers yields extremely small singular values. edge chaos combination positive linear ampliﬁcation saturating nonlinear dampening yields anisotropic distribution singular values. edge chaos fraction singular value distribution concentrated range remains despite layers propagation reﬂecting appoximate dynamical isometry. moreover nice property remains valid even input variance increased beyond tanh function enters nonlinear regime. thus right column fig. near indicates useful dynamical isometry properties random orthogonal linear networks described survives nonlinear networks even activity patterns enter deeply nonlinear regime input layers. interestingly singular value spectrum robust perturbations increase relative decrease indeed anisotropy singular value distribution relatively mild compared random linear networks scaled gaussian initial conditions thus overall numerical results suggest beyond edge orthogonal chaos good regime learning deep nonlinear networks. summary despite simplicity input-output dynamics learning deep linear networks reveals surprising amount rich mathematical structure including nonlinear hyperbolic dynamics plateaus sudden performance transitions proliferation saddle points symmetries conserved quantities invariant submanifolds independently evolving connectivity modes subserving rapid learning importantly sensitive computable dependence learning time scales input statistics initial weight conditions network depth. right initial conditions deep linear networks ﬁnite amount slower shallow networks unsupervised pretraining initial conditions tasks right structure. moreover introduce mathematical condition faithful backpropagation error signals namely dynamical isometry show surprisingly random scaled gaussian initializations cannot achieve condition despite norm-preserving nature greedy pre-training random orthogonal initialization thereby achieving depth independent learning times. finally show property dynamical isometry survives good approximation even extremely deep nonlinear random orthogonal networks operating beyond edge chaos. cost expressivity deep linear networks gain theoretical tractability prove fertile addressing phenomena deep learning impact carefully-scaled initializations momentum dropout regularization sparsity constraints full analytical treatment learning deep nonlinear networks currently remains open cannot reasonably hope move towards theory without ﬁrst completely understanding linear case. sense work fulﬁlls essential pre-requisite progress towards general quantitative theory deep learning.", "year": 2013}