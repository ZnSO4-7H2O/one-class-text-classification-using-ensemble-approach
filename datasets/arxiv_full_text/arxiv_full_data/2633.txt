{"title": "Provably Optimal Algorithms for Generalized Linear Contextual Bandits", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Contextual bandits are widely used in Internet services from news recommendation to advertising, and to Web search. Generalized linear models (logistical regression in particular) have demonstrated stronger performance than linear models in many applications where rewards are binary. However, most theoretical analyses on contextual bandits so far are on linear bandits. In this work, we propose an upper confidence bound based algorithm for generalized linear contextual bandits, which achieves an $\\tilde{O}(\\sqrt{dT})$ regret over $T$ rounds with $d$ dimensional feature vectors. This regret matches the minimax lower bound, up to logarithmic terms, and improves on the best previous result by a $\\sqrt{d}$ factor, assuming the number of arms is fixed. A key component in our analysis is to establish a new, sharp finite-sample confidence bound for maximum-likelihood estimates in generalized linear models, which may be of independent interest. We also analyze a simpler upper confidence bound algorithm, which is useful in practice, and prove it to have optimal regret for certain cases.", "text": "search problem personalized news recommendation website must recommend news articles interesting users visit website. problem especially challenging breaking news little data available make good prediction user interest. trade-off naturally occurs kind sequential decision making problems. needs balance exploitation—choosing actions performed well past—and exploration— choosing actions potentially give better outcomes. paper study following stochastic karmed contextual bandit problem. suppose rounds agent presented actions associated context choosing action based rewards obtained previous rounds contexts agent receive stochastic reward generated unknown distribution conditioned context chosen action. goal agent maximize expected cumulative rewards rounds. studied model contextual bandits literature linear model expected rewards round linear combination features context vector. linear model theoretically convenient work with. however practice usually binary rewards logistic regression model based algorithms shown substantial improvements linear models therefore consider generalized linear models contextual bandit setting linear logistic probit regression serve three important special cases. celebrated work robbins ﬁrst introduces upper conﬁdence bound approach efﬁcient exploration. later idea conﬁdence bound successfully applied many stochastic bandits problems k-arm bandits problems linear bandits ucb-type algorithms efﬁcient provable optimal k-arm bandits k-armed linear bandits. however study limited linear case. ucb-type algorithms contextual bandits widely used internet services news recommendation advertising search. generalized linear models demonstrated stronger performance linear models many applications rewards binary. however theoretical analyses contextual bandits linear bandits. work propose upper conﬁdence bound based algorithm generalized linear contexing number arms ﬁxed. component analysis establish sharp ﬁnite-sample conﬁdence bound maximumlikelihood estimates generalized linear models independent interest. also analyze simpler upper conﬁdence bound algorithm useful practice prove optimal regret certain cases. contextual bandit problems originally motivated applications clinical trials standard treatment treatment available certain disease doctor needs decide sequetial manner based patient’s proﬁles general physical status medicine history. development modern technologies contextual bandit problems applications especially web-based recommendation advertising microsoft research redmond department statistics yale university haven usa. corlihong <lihonglimicrosoft.com> respondence zhou <denzhomicrosoft.com>. using glms perform well empirically little theoretical study them. natural question arises efﬁcient algorithm achieve optimal convergence rate generalized linear bandits? moreover matches bandits problem’s minimax lower bound indicated linear bandits problem thus optimal. supcb-glm inspired seminal work auer introduced technique construct independence samples linear contextual bandits. observation proving result conﬁdence ball unknown parameter insufﬁcient calculate sharp upper conﬁdence bound need conﬁdence interval directions. thus prove ﬁnite sample normality type conﬁdence bound maximum likelihood estimator glm. best knowledge ﬁrst non-asymptotic normality type result might theoretical value. also analyze simple version algorithm called ucb-glm widely used practice. prove also achieves optimal regret bound reasonable assumption. results shed light explaining good empirical performance bandits practice. related work study bandits problem goes back least sarkar considered discounted regrets rather cumulative regerts. prove myopic rule without exploration asymptotically optimal. recently filippi study stochastic bandit problem considered here. propose glm-ucb algorithm similar algorithm another line research focuses using exp-type algorithms applied almost model classes algorithms choose actions using carefully randomized policy importance sampling reduce bandit problem fullinformation analogue. later variants algorithm give organization section introduces generalized linear bandit problem. section gives brief review statistical properties generalized linear model gives sharp non-asymptotic normality-type result parameter estimation independent value. tool section presents algorithms main theoretical results. section concludes paper discussions including several open problems. proofs given supplementary materials. notations vector denote ℓnorm transpose. kxka √x′ax. minimum maximum singular values matrix written λmin remetric matrices dimensions means positive semi-deﬁnite. realvalued function denote ﬁrst second derivatives. finally feature vectors {xta drawn unknown distribution kxtak feature vector associated unknown stochastic reward agent selects action denoted observes corresponding reward ytat finally make regularity assumption distribution exists constant λmin then agent’s total regret following strategy expressed follows later normality result crucial regret analysis bandits. however best knowledge non-asymptotic normality results glm. following present ﬁnite-sample version classical asymptotic normality results independent interest. practice bounded reward noise also bounded hence satisﬁes appropriate value. addition boundedness assumption rewards feature vectors also need following assumption link function assumption {kxk≤ kθ−θ∗k≤} shall section asymptotic normality maximum-likelihood estimates implies necessity assumption. note assumption weaker assumption filippi requires control local behavior near motivate algorithms proposed paper ﬁrst brieﬂy review classical likelihood theory generalized linear models. canonical generalized linear model conditional distribution given exponential family denidea upper conﬁdence bounds highly effective dealing exploration exploitation trade-off many parametric bandit problems including k-arm bandits linear bandits generalized linear model considered here since strictly increasing function goal equivalent choosing maximize taθ∗ round suppose current estimator round exploitation action take action maximizes estimated mean value exploration action choose largest variance. thus balance exploitation exploration simply choose action maximizes estimated mean variance interpreted upper conﬁdence bound leads algorithm ucb-glm ucb-glm take parameters. initialization stage randomly choose actions ensure unique solution choice theorem statement follows proposition noted assumption contextual needed ensure invertable rest analysis depend stochastic assumption. achieved using regularization condition λmin necessary consistency estimating linear models generalized linear models satisﬁed mild conditions proposition below useful analysis. drawn distribution support unit ball furthermore second moment matrix positive constants. then exist positive universal constants λmin probability least long proof sketch. give proof sketch here full proof found appendix. following simplicity drop subscript ambiguity. therefore denoted need technical lemma existing result random matrix theory. version presented adapted equation theorem vershynin moreover algorithm proposed filippi involves projection step computationally expensive comparing ucb-glm. finally algorithm works well practice. give heuristic argument strong performance section speciﬁc condition sometimes satisﬁed. ﬁxed small. mentioned section technical difﬁculty analyzing ucb-glm dependence samples. inspired technique developed auer create independent samples linear contextual bandits propose another algorithm supcbglm uses algorithm cb-glm sub-routine. algorithm also relies idea conﬁdence bound exploration. round algorithm screens candidate actions based value stages action chosen. stage conﬁdence level stage need exploration thus choose action. otherwise actions ﬁltered step actions passed next stage close enough optimal action. since widths smaller action optimal action. ﬁlter process terminates already accurate taθ∗ level estimate need exploration. thus step choose action maximizes estimated mean value. algorithm different algorithm suplinrel auer directly maximize mean rather upper conﬁdence bound steps modiﬁcation leads simpler algorithm cleaner regret analysis. also would like point that unlike spectraleliminator algorithm easily handle changing action set. lower bound expected regret k-armed linear bandits special bandits considered here. therefore regret supcb-glm algorithm optimal logarithm terms best knowledge ﬁrst algorithm achieves optimal rate bandits. worthwhile compare theorem result theorem small rate supcbglm faster improve previous rates factor. here give brieﬂy illustration extra factor. theorem filippi |x′| upper bounded ususing cauchy-schwartz inequality make fact close sense. however tells actually close every direction. reason able remove extra factor achieve near-optimal regret. also proof theorem facilitate proof ﬁrst present technical lemmas. lemma follows lemma theorem theorem vershynin union bound. proof lemma deferred appendix. theorem provides insights ucb-glm performs well practice. although condition hard check violated cases example k-armed bandits provide heuristic argument justify assumption range problems. large enough estimator close assume positive hxta∗ hxta after example steps. since {xta independent also independent samples. {xta∗ vt/t well-approximated covariance matrix denote many problem pracxta∗ tice especially features dense unlikely feature vector xta∗ lies low-dimensional subspace implies full rank λmin large enough. cautioned that since know distribution feature vectors cannot assume exists. therefore challenging make arguments rigorous. fact studying arima model time series provide example λmin large. second step computed using samples meaning per-step complexity grows least linearly straightforward implementation algorithms. therefore interesting investigate scalable alternatives. possible ﬁrst-order iterative optimization procedure amortize cost analogous approach agarwal paper propose algorithms k-armed bandits generalized linear models. ﬁrst algorithm ucb-glm achieves optimal rate case inﬁnite number arms second algorithm supcbglm provable optimal case ﬁnite number actions round. however remains open whether ucb-glm achieve optimal rate small k-dependent lower bound. currently lower bound results linear bandits dependence number arms. minimax lower bound particularly interest current randomized algorithms optimal regret rate. opposed deterministic ucb-style algorithms studied paper randomized algorithms like thompson sampling advantages certain situations example reward observations delayed recently developed techniques analyzing bayes regret bandits useful analyze cumulative regret considered here.", "year": 2017}