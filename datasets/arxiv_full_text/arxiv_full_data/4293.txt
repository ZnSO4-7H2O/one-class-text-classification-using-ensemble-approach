{"title": "A relativistic extension of Hopfield neural networks via the mechanical  analogy", "tag": ["cond-mat.dis-nn", "cs.NE", "stat.ML"], "abstract": "We propose a modification of the cost function of the Hopfield model whose salient features shine in its Taylor expansion and result in more than pairwise interactions with alternate signs, suggesting a unified framework for handling both with deep learning and network pruning. In our analysis, we heavily rely on the Hamilton-Jacobi correspondence relating the statistical model with a mechanical system. In this picture, our model is nothing but the relativistic extension of the original Hopfield model (whose cost function is a quadratic form in the Mattis magnetization which mimics the non-relativistic Hamiltonian for a free particle). We focus on the low-storage regime and solve the model analytically by taking advantage of the mechanical analogy, thus obtaining a complete characterization of the free energy and the associated self-consistency equations in the thermodynamic limit. On the numerical side, we test the performances of our proposal with MC simulations, showing that the stability of spurious states (limiting the capabilities of the standard Hebbian construction) is sensibly reduced due to presence of unlearning contributions in this extended framework.", "text": "universit`a salento lecce italy binfn istituto nazionale fisica nucleare sezione lecce italy cgnfm-indam gruppo nazionale fisica matematica sezione lecce italy abstract propose modiﬁcation cost function hopﬁeld model whose salient features shine taylor expansion result pairwise interactions alternate signs suggesting uniﬁed framework handling deep learning network pruning. analysis heavily rely hamilton-jacobi correspondence relating statistical model mechanical system. picture model nothing relativistic extension original hopﬁeld model focus low-storage regime solve model analytically taking advantage mechanical analogy thus obtaining complete characterization free energy associated self-consistency equations thermodynamic limit. numerical side test performances proposal simulations showing stability spurious states sensibly reduced presence unlearning contributions extended framework. past years thanks progresses hardware well software assisted rise novel powerful generation artiﬁcial intelligence whose popular contributions perhaps impressive adaptive capability deep learning creative compositional episodes sleeping dreaming regarding former deep learning beyond literally grounding artiﬁcial intelligence within theoretical framework disordered statistical mechanics hopﬁeld recently oﬀered also connectionist perspective high skills deep learning machines could possibly framed route proposes many-body extensions celebrated pairwise model feature retrieval pattern recognition core idea elegant simple already pointed hopﬁeld tank uncorrelated them parenthesized term would small state network retrieves patterns contribution summation would longer negligible pattern would actually attractor reasonable stochastic neural dynamics obeying detailed balance robustness argument lies usage local convexity cost function generalized straightforwardly beyond parabolic approximation coded pairwise interactions instance including also even higher order contributions remembering that connectionist perspective memories connections clearly adding p-spin contributions hamiltonian adds synapses information ﬁled. regarding latter sleeping dreaming idea sleep actually consolidate memories discarding fake information accidentally stored centuries inspection improved signiﬁcantly since discovery sleep ﬁfties pioneered aserinsky kleitman models sleep already framed within neural network perspective crick mitchinson hopﬁeld whole gave rise theory unlearning neural networks nice idea remove spurious states landscape retrievable patterns present work show unlearning hebbian models used pruning networks. nowadays however branches neural networks -deep learning unlearningnever merged main present work obtain uniﬁed model able handle features once. goal mind turn methodologies rather subjects. initially focus spin glasses past years several contributions linking statistical mechanical recipes analytical mechanical approaches appeared literature references therein). ideas consider curie-weiss model simplest example pairwise spin hamiltonian proved free energy obeys standard hamilton-jacobi space coupling external ﬁeld infer statistical properties curie-weiss network thus mechanical analogy study ﬁctitious particle unitary mass classically evolving space-time properties translated back original setting recover sharply results standard route thus possible perform whole statistical mechanical analysis model upon relying solely techniques typical analytical mechanics here framework adapted hopﬁeld model show analogy calls natural generalization hopﬁeld cost-function simply relativistic extension. indeed within mechanical analogy moving classical kinetic energy relativistic counterpart hamilton-jacobi reduces energy-momentum relation free energy -that coincides action equivalenceturns obey relativistic least action principle. classical expression kinetic energy second-order monomial i.e. hopﬁeld cost-function relativistic expression taylor-expanded monomials. remark turn exactly solely even ones alternate signs relativistic extension naturally suggested mechanical analogy accounts higher-order contributions hence potential interest researcher deep learning appealing researches unlearning given alternation signs series. regards worth pointing work always randomly generated pattern’s entries standard amit-gutfreund-sompolinsky theory choice barely resembles real situations however abstract level standard shannon-fano compression argument immediate realize network able cope entirely random patterns certainly able handle amount structured patterns ﬁnal remark stress numerical heuristic explorations largely exploited computer science literature past decade reach sharp statements rigorous well controllable results. indeed along perspective important contributions already appeared mathematical theoretical physics literature references therein). regards ﬁrst paper force focus low-storage analysis model namely study properties model naturally possesses number patterns learn retrieve grows sub-linearly number neurons dedicated task mathematical perspective regime much controllable glassiness hidden model becomes negligible neural networks particular linking learning retrieval simplest representation restricted boltzmann machines prototypes learning generalized hopﬁeld models paradigms retrieval. revise using standard bayes argument features learnt training stage restricted boltzmann machine play patterns recognized future highlighting factopattern recognition happens standard hebbian kernel typical hopﬁeld retrieval recently understood bridge glimpsed disordered systems community still deserves outlined finally brieﬂy comment spurious states intrinsic genesis hopﬁeld framework. move build mathematical framework i.e. mechanical analogy neural networks benchmark section show analogy classical level namely consider standard statistical mechanical package related original pairwise hopﬁeld model give complete picture properties re-obtaining well-known existing results. section extend analogy include higher order contributions hopﬁeld hamiltonian obtain exhaustive picture resulting proprieties generalized model too. section numerical analysis capabilities extended model trough one-to-one comparison among performances classical versus relativistic hopﬁeld model prove systematically extension out-performs w.r.t classical limit particular show spurious states standard hopﬁeld model almost entirely pruned extension. finally section left conclusions brief summary future outlooks. long time machine learning machine retrieval treated separately former mainly addressed statistical inference perspective latter mainly tackled trough statistical mechanics techniques. hereafter instead show restricted boltzmann machines learn stochastic dynamics whose long term relaxation converges gibbs measure eﬀective hamiltonian turns sharply hopﬁeld model pattern recognition; then streamlined summarize properties latter means standard statistical mechanical arguments preliminary step required correctly frame mechanical analogy broad world neural networks. nutshell boltzmann machine two-party system whose parties visible layer hidden units hidden layer. weights connecting form matrix right panel example corresponding hopﬁeld network whose visible neurons retrieve patterns stored hebb matrix three vectors pertaining feature i.e. three original jargon called visible -the receives input outside worldhidden -the dedicated ﬁgure correlations data presented visible layer keeping fig. mind layer composed spins/neurons spins chosen high generality ranging discretevalued e.g. select ising neurons visible layer real-valued e.g. chose gaussian neurons hidden layer analogously entries weight’s matrix real discrete generally speaking continuous weights allow usage stronger learning rules w.r.t. binary counterparts thus convenient learning stage binary weights convenient retrieval stage pattern recognition order trade-oﬀ gave rise broad plethora variations theme references therein) whose best setting lies boltzmann machine equipped gaussian hidden layer boolean visible layer structure typical restricted boltzmann machine thus coded external ﬁelds bias terms statistical inference perspective statistical mechanics viewpoint body interactions always trivial playing role machine learning discarded soon crucially dynamics weights dynamics spins evolve separately adiabatically diﬀerent timescales average away fast neural scale studying slow synaptic dynamics likewise keep synapses quenched interested fast neural dynamics typically data vectors presented visible layer machine assumption data independently generated probability distribution ultimate goal machine make inner representation close possible original one. approximates usually achieved minimization kullback-leibler cross entropy deﬁned analogously response kullback-leibler cross entropy variation biases secure learning rule thanks deﬁnite sign last term above. weights boltzmann machine symmetric suﬃces detailed balance hold latter guarantees long term limit stochastic dynamics always gibbs measure hence mathematical structure probability distribution known allows generating explicit algorithms among following contrastive divergence criterion probably applied standard statistical mechanical ones recipe elementary powerful learning rule roughly speaking simply tries make theoretical correlation functions close possible empirical ones. machine able reproduce statistics stored training data correctly internal weights rearranged machine asked generate vectors according statistical properties coincide input data machine learnt representation reality question retrieval capabilities machine kind features machine discover provided data. since hidden neurons loss generality. understand retrieval capabilities cost function statistical mechanical perspective useful introduce mattis magnetizations deﬁned cover pivotal role order parameters hopﬁeld theory capture relaxation network. happen essentially three diﬀerent ways network’s dynamics ends pure state namely attractor coincides exactly stored patterns becomes neurons aligned pattern entries pure state). network’s dynamics ends spurious state namely unwanted mixture pure states corresponding mattis magnetization becomes strictly diﬀerent zero albeit sensibly smaller one. network’s dynamics failure ends random sample phase space mattis parameters zero write hopﬁeld hamiltonian mattis order parameters minimum energy request follows candidate minima hopﬁeld model states force mattis magnetizations assume value combination mattis magnetization diﬀerent zero remarkably equilibrium states global minima coincide patterns tempted associate spontaneous relaxation network attractor cognitive process information retrieval. quantify statement standard route introduce study free energy related model express extremize terms mattis order parameters brieﬂy summarized hereafter low-storage case namely limn→∞ free energy hopﬁeld model storage regime reads ﬁrst line bare deﬁnition free energy second line used gaussian integral representation exponent extensive factor multiplying terms thus evaluated saddle point argument whose extremization returns level hence extremizing free energy obtain maximum entropy solutions cost-function minimization. coded last equation self-consistent equation order parameter quantiﬁes strength retrieval whatever route starting hebbian prescription original hopﬁeld paper upon marginalization hidden layer boltzmann machine unfortunately always network whose attractors stored patterns excess stock minima constituted so-called spurious states simplest example -pattern mixture deﬁned follows mattis overlap state three patterns -for large networksσi hence smaller amplitude mattis overlap pure state still meta-stable state orbiting surrounding network attracted spurious states converge rather pure ones. unfortunately patterns added linearly memory kernel exponential proliferation unwanted meta-stable states retrieval landscape hopﬁeld network hopﬁeld suggested procedure prune remove coupling matrix nutshell hopﬁeld’s idea transparent elegant brilliant sensibly much spurious states pure states let’s start system random make quench high probability system spurious state iteratively check eﬀectively network becomes progressively cleared nasty attractors procedure called unlearning linked sleep eﬀectiveness random starting point setting quenching procedure consolidating memories maximum entropy neither minimum energy principles indeed observing free energy plays action suitable mechanical analogy import arsenal mathematical tools investigate properties particular show free energy proper actionobeys hamiltonjacobi whose solution returns variational principle free energy minimization expressed context least action principle reduces standard statistical mechanics next observable play generalized free energy hopﬁeld model statistical mechanical perspective covers role action present mechanical analogy. remark thermodynamic limit motion space-time symmetries whose no¨ether currents derived respectively momentum conservation energy conservation further limit determination explicit expression free energy terms mattis magnetizations reduces explicit calculation action free motion. cauchy conditions choose mechanical analogy used note pathologies previous treatment somehow shine transparent calculations natural extension hopﬁeld cost-function actually main observations waiting done. ﬁrst that free energy plays action think exponent maxwell-boltzmann weight product observe underlying metrics euclidean rather minkowskian special relativity. second classical mechanics velocity unbounded magnetization obviously bounded i.e. straight natural relativistic generalization that indeed content section. mechanical analogy hopﬁeld cost-function reads kinetic energy associated ﬁctitious particle natural extension hopﬁeld model constituted relativistic deformation i.e. further r.h.s. alternate-sign series hence contributions learning unlearning prevailing contribution played standard hebbian learning show section beneﬁcial role unlearning lies destabilizing retrieval spurious memories thus resulting enhanced network’s performances w.r.t. classical limit model introduced extend partition function free energy mechanical analogy properly previous sections e.g. summing lorentz factor therefore written observations together determination explicit expression relativistic free energy terms mattis magnetizations reduces calculation mattis self-consistent equations solve recursively obtain theoretical expectations quality retrieval various pure spurious states compare monte carlo simulations network’s performances. order test capabilities relativistic hopﬁeld network compare results classical counterpart task performed diﬀerent kind extensive simulations follows. order test amplitude stability pure spurious attractors performed following series numerical simulation introduced random numbers uniformly sampled following stochastic neural dynamics note that also dynamical evolution system tunes level noise network triggers amplitude hyperbolic tangent time step neuron ﬁeld computed multiplied hyperbolic tangent applied number compared figure first histograms maximum mattis magnetization starting spurious state. results maximum mattis magnetization classical relativistic hopﬁeld models diﬀerent noise values purple bars superposition blue ones. network whose thermalization study built neurons stored patterns. simulations consist diﬀerent runs organized diﬀerent random pattern conﬁgurations randomly selected initial conditions diﬀerent stochastic evolutions noise level. second retrieval performances spurious states initial conditions. results number ﬁnal states classical relativistic models starting spurious state lowest thermal energy supply spurious states stable. however relativistic model starts retrieve properly roughly half runs proper pure state. contrary classical model reach obtaining correct thermalization pure state half runs correctly. mattis magnetizations whose magnitude larger arbitrary threshold coupled ﬁnal patterns recognized retrieved choose mthreshold since best splits spurious state intensities relativistic hopﬁeld models diﬀerent noise values purple bars superposition blue ones. network whose thermalization study built neurons stored patterns. simulations consist diﬀerent runs organized diﬀerent random pattern conﬁgurations randomly selected initial conditions diﬀerent stochastic evolutions noise level. second retrieval performances spurious states initial conditions. results number ﬁnal states classical relativistic models starting spurious state lowest thermal energy supply spurious states stable. however relativistic model starts retrieve properly roughly half runs proper pure state. contrary classical model reach obtaining correct thermalization pure state half runs correctly. mattis magnetizations whose magnitude larger arbitrary threshold coupled ﬁnal patterns recognized retrieved choose mthreshold since best splits spurious state intensities whose existence ensured detailed balance results shown fig. fig. fig. analysis respectively ﬁrst rows show histograms counting many times figure first histograms maximum mattis magnetization starting spurious state results maximum mattis magnetization classical relativistic hopﬁeld models diﬀerent noise values purple bars superposition blue ones. network whose thermalization study built neurons stored patterns. simulations consist diﬀerent runs organized diﬀerent random pattern conﬁgurations randomly selected initial conditions diﬀerent stochastic evolutions noise level. second retrieval performances spurious states initial conditions. results number ﬁnal states classical relativistic models starting spurious state lowest thermal energy supply spurious states stable. however relativistic model starts retrieve properly roughly half runs proper pure state. contrary classical model reach obtaining correct thermalization pure state half runs correctly. mattis magnetizations whose magnitude larger arbitrary threshold coupled ﬁnal patterns recognized retrieved choose mthreshold since best splits spurious state intensities starting initial condition aligned maximal spurious increasing value spurious attractors becomes unstable also classical model therefore causing fast fall-oﬀ relative improvement increasing particular value curve falls also outside range temperature values spurious states possess stability noise levels beyond threshold spurious states longer stable likewise generally reward relativistic extension. markov dynamics collect ﬁnal state relaxation process whose existence ensured detailed balance results shown fig. again ﬁrst show histograms counting times system ended pure state three figure first histograms mattis magnetization random initial conditions results maximum mattis magnetization classical relativistic hopﬁeld models diﬀerent noise values purple bars superposition blue ones. network whose thermalization study built neurons stored patterns. montecarlo simulations consist diﬀerent runs organized diﬀerent random pattern conﬁgurations randomly selected initial conditions diﬀerent stochastic evolutions noise level. second retrieval performances random initial conditions. results retrieval ﬁnal states classical relativistic models mattis magnetizations whose magnitude larger arbitrary threshold coupled ﬁnal patterns recognized retrieved choose mthreshold since best relativistic models network patterns stored. data points averages diﬀerent pattern realizations sampled random initial conditions results compared main branches theoretical solution self-consistency eqs. note presence figure results diﬀerent networks size shown classical hopﬁeld model relativistic extension expected grows curves approach analytic solution smallest values trace phase transition expected. detailed balance hopﬁeld’s hamiltonian plays also lyapounov function hence control convergence minimum whose attainment reached spins share sign ﬁelds acting fig. show behavior mattis order parameter function noise network predicted theory using self-consistent classical hopﬁeld model -left panelself-consistent relativistic counterpart -right panel- well behaviour obtained simulations obtained standard monte carlo runs system built neurons despite redundant stress branches -rather onereported fig. unbroken model’s spin-ﬂip symmetry pattern anti-pattern attractors neural dynamics clearly happen thresholds ﬁring re-introduced theory fig. compare solution various sizes network shines plots classic case well relativistic generalization grows analytical scaling approached. inspired pioneering works gardner estimates depth stability basins attractions pure spurious states introduce percentage random spin-ﬂips impose system underlying idea start system known state reshuﬄe kicking randomly percentage neurons checking grows return network initial attractor. results shown fig. focusing -mixture spurious state order quantify pruning capabilities relativistic model compare always escape spurious state regardless tacitely showing basins attraction states corrupted unlearning contributions nestled relativistic hopﬁeld cost function reasonable cause increased performances relativistic hopﬁeld model decreased energy barrier spurious states maxima surrounding them. check idea compare gaps classical relativistic models still start network’s dynamics spurious state pure state -the target patternway move spurious state target controlled noiseless random walks i.e. ground-state dynamics step select spin already aligned target pattern want network reach move otherwise compute energy repeat process pure state reached. starting spurious state approaching pure energy barrier crossed collected energy gaps averaging statistics consisting diﬀerent runs found energy barrier escape spurious state halved relativistic model compared classical counterpart spin-ﬂips fraction would take initial condition immediately away spurious attractor relevant discussion). left panel noise levels signiﬁcantly neural states models become capable escape spurious state however middle panel already mild noise level relativistic model ability drift away spurious toward pure attractor ﬁnally increasing noise level network right panel appreciate relativistic model out-performs w.r.t. classical counterpart entire analyzed range. would like conclude work possible continuation investigation. relativistic extension hopﬁeld model nice justiﬁcation terms mechanical analogy hamilton-jacobi formalism. side statistical mechanics model possible choices cost-function incorporating principles deep learning network pruning reasonable extend general cost-functions recover hopﬁeld associative memory framework leading contribution taylor expansion. straightforward generalization proposal indeed -parametric hamiltonian gives hopﬁeld model leading order. order pruning corrections alternate signes) choose positive real number. give sketch performances model compared retrieval frequency curves diﬀerent values network choosing initial condition aligned spurious state fig. result higher lead models performing better retrieving network prepared spurious state conﬁguration. good point encourage future study context. particular detailed investigation -parametric model required numerically analitically. leave point study high storage regime open future works. regarding unlearning phenomenon quoting hassibi stork central problem machine learning pattern recognition minimize system complexity consistent training data. begin trained network many weights question become weights eliminated? answering question gave rise pruning machine learning. however pruning algorithms extensively exploited neural networks away detailed balance e.g. radial-basis-function and/or feed-forward neural networks much said stochastic neural dynamics obeying detailed balance namely associative neural networks boltzmann machines and/or hopﬁeld-like models. models however powerful tricksappeared past remove spurious states un-learning procedure inspired figure retrieval performances -parametric model. results -parametric model network size patterns stored. initial condition aligned spurious state sign. values control parameter lower performances gradually worsen respect relativistic hopﬁeld model. side increasing value beyond retrieval frequency accordingly grows network dynamics likely ends pure state. always spurious state initial conditions averaged results glauber dynamics diﬀerent pattern realizations them diﬀerent stochastic evolution. rem-sleep proposed always seen fortiori algorithmic procedure rather intrinsic property model. focusing deep learning instead since seminal review lecun bengio hinton deep learning impressive skills listed alongside limitation understanding deep learning machines achieve scores plethora contributions quickly appeared theme among hopﬁeld’s idea higher-order interactions cost-function optimized. however unlearning intriguing proposal come natural property models. paper tried merge major breakthroughs artiﬁcial intelligence re-obtaining particular features unique natural model relativistic extension hopﬁeld paradigm. accomplish task ﬁrst develop full mechanical analogy statistical mechanical treatment standard hopﬁeld model checking mechanical analogy work correctly even dealing neural networks mirror free energy model plays mechanical action obeys classic hamilton-jacobi equation space parameters extremizing action least square action principle re-obtain correct expression self-consistency mattis order parameter obtained trough standard route amit-gutfreund-sompolinsky storage case. checked mechanical analogy correctly recovers details celebrated hopﬁeld picture noticed pathologies classical mechanical treatment namely underlying minkowskian metric tensor bounded velocity ﬁctitious particle mechanical motion. observations suggested extend mechanical analogy relativistic treatment naturally introduced novel cost-function relativistic generalization original hopﬁeld proposal. model studied details conﬁned storage analytical expression free energy well prescription evolution mattis order parameters explicitly obtained shown total agreement numerical simulations. also considered -parametric generalization relativistic model giving numerical sketch performances increase suitable choice control parameter. result opens possibility study general cost-function choices. remarkably hamiltonians taylor expanded shown include higher order monomials monomials succeed another alternate signs leading term standard pairwise hopﬁeld model next fourneuron coupling reverse sign third term six-neuron coupling correct sign features recovered emergent properties relativistic extension hopﬁeld model clearly deserve detailed inspection paper deepened solely former postponing analysis storage capacity network forthcoming paper. reason behind choice required mathematical treatment completely diﬀerent. dealing high storage concepts borrowed statistical mechanics spin glasses required plan report soon also results high-storage regime. conclusive remark restricted analysis binary weights/patterns approach holds real-valued variable finally conclude paper observing also mechanical analogy perform useful tool possibly beyond associative memory theory problems artiﬁcial intelligence hopﬁeld networks remaining ﬁrst benchmark. acknowledges partial ﬁnancial support gnfm-indam miur miur would like thank elena agliari useful discussions especially concerning numerical analysis.", "year": 2018}