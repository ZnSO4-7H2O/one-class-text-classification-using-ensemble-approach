{"title": "Outrageously Large Neural Networks: The Sparsely-Gated  Mixture-of-Experts Layer", "tag": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "text": "capacity neural network absorb information limited number parameters. conditional computation parts network active per-example basis proposed theory dramatically increasing model capacity without proportional increase computation. practice however signiﬁcant algorithmic performance challenges. work address challenges ﬁnally realize promise conditional computation achieving greater improvements model capacity minor losses computational efﬁciency modern clusters. introduce sparsely-gated mixture-of-experts layer consisting thousands feed-forward sub-networks. trainable gating network determines sparse combination experts example. apply tasks language modeling machine translation model capacity critical absorbing vast quantities knowledge available training corpora. present model architectures billion parameters applied convolutionally stacked lstm layers. large language modeling machine translation benchmarks models achieve signiﬁcantly better results state-of-the-art lower computational cost. exploiting scale training data model size central success deep learning. datasets sufﬁciently large increasing capacity neural networks give much better prediction accuracy. shown domains text images audio typical deep learning models entire model activated every example leads roughly quadratic blow-up training costs model size number training examples increase. unfortunately advances computing power distributed computation fall short meeting demand. various forms conditional computation proposed increase model capacity without proportional increase computational costs schemes large parts network active inactive per-example basis. gating decisions binary sparse continuous stochastic deterministic. various forms reinforcement learning back-propagation proposed trarining gating decisions. figure mixture experts layer embedded within recurrent language model. case sparse gating function selects experts perform computations. outputs modulated outputs gating network. ideas promising theory work date demonstrated massive improvements model capacity training time model quality. blame combination following challenges modern computing devices especially gpus much faster arithmetic branching. works recognize propose turning on/off large chunks network gating decision. large batch sizes critical performance amortize costs parameter transfers updates. conditional computation reduces batch sizes conditionally active chunks network. network bandwidth bottleneck. cluster gpus computational power thousands times greater aggregate inter-device network bandwidth. computationally efﬁcient relative computational versus network demands algorithm must exceed ratio. embedding layers seen form conditional computation handicapped problem. since embeddings generally need sent across network number interactions limited network bandwidth instead computational capacity. depending scheme loss terms necessary achieve desired level sparsity per-chunk and/or example. bengio three terms. issues affect model quality load-balancing. model capacity critical large data sets. existing literature conditional computation deals relatively small image recognition data sets consisting images. hard imagine labels images provide sufﬁcient signal adequately train model millions alone billions parameters. work ﬁrst time address challenges ﬁnally realize promise conditional computation. obtain greater improvements model capacity minor losses computational efﬁciency signiﬁcantly advance state-of-the-art results public language modeling translation data sets. approach conditional computation introduce type general purpose neural network component sparsely-gated mixture-of-experts layer consists number experts simple feed-forward neural network trainable gating network selects sparse combination experts process input parts network trained jointly back-propagation. introduced technique generic paper focus language modeling machine translation tasks known beneﬁt large models. particular apply convolutionally stacked lstm layers figure called position text selecting potentially different combination experts position. different experts tend become highly specialized based syntax semantics language modeling machine translation benchmarks improve best published results fraction computational cost. since introduction decades mixture-of-experts approach subject much research. different types expert architectures proposed svms gaussian processes dirichlet processes deep networks. work focused different expert conﬁgurations hierarchical structure inﬁnite numbers experts adding experts sequentially garmash monz suggest ensemble model format mixture experts machine translation. gating network trained pre-trained ensemble model. works concern top-level mixtures experts. mixture experts whole model. eigen introduce idea using multiple moes gating networks parts deep model. intuitive latter approach powerful since complex problems contain many sub-problems requiring different experts. also allude conclusion potential introduce sparsity turning moes vehicle computational computation. work builds moes general purpose neural network component. eigen uses stacked moes allowing sets gating decisions convolutional application allows different gating decisions position text. also realize sparse gating demonstrate practical massively increase model capacity. structure mixture-of-experts layer mixture-of-experts layer consists expert networks\" e··· gating network\" whose output sparse n-dimensional vector. figure shows overview module. experts neural networks parameters. although principle require experts accept sized inputs produce same-sized outputs initial investigations paper restrict case models feed-forward networks identical architectures separate parameters. denote output gating network output i-th expert network given input output module written follows save computation based sparsity output wherever need compute experiments thousands experts need evaluate handful every example. number experts large reduce branching factor using two-level hierarchical moe. hierarchical primary gating network chooses sparse weighted combination experts\" secondary mixture-of-experts gating network. following focus ordinary moes. provide details hierarchical moes appendix implementation related models conditional computation. whose experts simple weight matrices similar parameterized weight matrix proposed whose experts hidden layer similar block-wise dropout described dropped-out layer sandwiched fully-activated layers. noisy top-k gating components softmax gating network sparsity noise. taking softmax function tunable gaussian noise keep values setting rest sparsity serves save computation described above. form sparsity creates theoretically scary discontinuities output gating function observed problem practice. noise term helps load balancing discussed appendix amount noise component controlled second trainable weight matrix wnoise. training gating network train gating network simple back-propagation along rest model. choose gate values experts nonzero derivatives respect weights gating network. type occasionally-sensitive behavior described respect noisy rectiﬁers. gradients also backpropagate gating network inputs. method differs boolean gates reinforce-style approach train gating network. modern cpus gpus large batch sizes necessary computational efﬁciency amortize overhead parameter loads updates. gating network chooses experts example batch examples expert receives much smaller batch approximately inefﬁcient number experts increases. solution shrinking batch problem make original batch size large possible. however batch size tends limited memory necessary store activations forwards backwards passes. propose following techniques increasing batch size mixing data parallelism model parallelism conventional distributed training setting multiple copies model different devices asynchronously process distinct batches data parameters synchronized parameter servers. technique different batches synchronously combined layer. distribute standard layers model gating network according conventional data-parallel schemes keep shared copy expert. expert layer receives combined batch consisting relevant examples data-parallel input batches. devices function data-parallel replicas model-parallel shards model distributed devices device processes batch size expert receives batch approximately examples. thus achieve factor improvement expert batch size. case hierarchical primary gating network employs data parallelism secondary moes employ model parallelism. secondary resides device. technique allows increase number experts proportionally increasing number devices training cluster. total batch size increases keeping batch size expert constant. memory bandwidth requirements device also remain constant step times amount time necessary process number training examples equal number parameters model. goal train trillionparameter model trillion-word corpus. scaled systems writing paper possible adding hardware. taking advantage convolutionality language models apply time step previous layer. wait previous layer ﬁnish apply time steps together batch. increases size input batch layer factor number unrolled time steps. increasing batch size recurrent suspect even powerful models involve applying recurrently. example weight matrices lstm could replaced moe. sadly models break convolutional trick last paragraph since input timestep depends output previous timestep. gruslys describe technique drastically reducing number stored activations unrolled cost recomputing forward activations. would allow large increase batch size. another major performance concern distributed computing network bandwidth. since experts stationary number gating parameters small communication involves sending inputs outputs experts across network. maintain computational efﬁciency ratio expert’s computation size input output must exceed ratio computational network capacity computing device. gpus thousands one. experiments experts hidden layer containing thousands relu-activated units. since weight matrices expert sizes input_size×hidden_size hidden_size× output_size ratio computation input output equal size hidden layer. conveniently increase computational efﬁciency simply using larger hidden layer hidden layers. observed gating network tends converge state always produces large weights experts. imbalance self-reinforcing favored experts trained rapidly thus selected even gating network. eigen describe phenomenon hard constraint beginning training avoid local minimum. bengio include soft constraint batch-wise average gate. take soft constraint approach. deﬁne importance expert relative batch training examples batchwise gate values expert. deﬁne additional loss limportance added overall loss function model. loss equal square coefﬁcient variation importance values multiplied hand-tuned scaling factor wimportance. additional loss encourages experts equal importance. bengio also include additional losses. controls per-example sparsity need since enforced ﬁxed value third loss encourages diversity gate values. experiments gate values naturally diversify experts specialize need enforce diversity gate values. loss function ensure equal importance experts still receive different numbers examples. example expert receive examples large weights another receive many examples small weights. cause memory performance problems distributed hardware. solve problem introduce second loss function lload ensures balanced loads. appendix contains deﬁnition function along experimental results. previous state-of-the-art best previously published results models consisting stacked long short-term memory layers number parameters lstm layers models vary million million. quality increases greatly parameter count computational costs. results models form line figure -right. models models consist stacked lstm layers layer vary sizes layers number experts. full details model architecture training regimen additional baselines results appendix computation varied capacity investigate effects adding capacity trained series models roughly equal computational costs million multiply-andadds training example timestep forwards pass excluding softmax layer. call metric trained models moes containing experts models hierarchical moes containing experts. expert million parameters. layers experts active input. results models shown figure -left. model always-active experts performed similarly computationally-matched baseline models largest models achieved impressive lower perplexity test set. figure model comparison -billion-word language-modeling benchmark. left plot test perplexity function model capacity models similar computational budgets approximately -million-ops-per-timestep. right plot test perplexity function computational budget. line represents lstm models bottom line represents -billion parameter models different computational budgets. varied computation high capacity addition largest model previous section trained models similarly high capacity higher computation budgets. models larger lstms fewer larger experts. details found appendix results three models form bottom line figure -right. table compares results models best previously-published result dataset even fastest models beats best published result despite requiring computation. computational efﬁciency trained models using tensorflow clusters containing tesla gpus. models determine computational efﬁciency tflops/gpu dividing number ﬂoating point operations required process training batch observed step time number gpus cluster. operation counts used higher ones report ops/timestep numbers include backwards pass include importance-sampling-based training softmax layer count multiply-and-add separate operations. models ﬂoating point operations involved experts represent total. baseline models wtih observed computational efﬁciency ranged tflops/gpu. low-computation models computation efﬁciency ranged tflops/gpu except -expert model make full available parallelism. highest-computation model efﬁcient tflops/gpu likely larger matrices. numbers represent signiﬁcant fraction theoretical maximum tflops/gpu claimed nvidia. detailed results appendix table -billion-word corpus adding additional capacity seems produce diminishing returns number parameters layer exceeds billion seen figure -left. hypothesized larger training even higher capacities would produce signiﬁcant quality improvements. constructed similar training consisting shufﬂed unique sentences google’s internal news corpus totalling roughly billion words. similarly previous section tested series models similar computational costs million ops/timestep. addition baseline lstm model trained models augmented layers containing results figure shows test perplexity function capacity training billion words billion words training full billion words test perplexity improves signiﬁcantly experts dropping lower computationally matched baseline degrades experts possibly result much sparsity. widening lines demonstrates increased model capacity helps larger training sets. model architecture model modiﬁed version gnmt model described reduce computation decreased number lstm layers encoder decoder respectively. inserted layers encoder decoder layer contained experts million parameters adding total billion parameters models. details model architecture testing procedure results found appendix datasets benchmarked method wmt’ en→fr en→de corpora whose training sets sentence pairs sentence pairs respectively. experimental protocols also similar newstest used test compare previous work combination newstest newstest used development set. also tested model google’s production english french data. results tables show results largest models compared published results. approach achieved bleu scores wmt’ en→fr en→de benchmarks. models reﬁnement results constitute signiﬁcant gains bleu score strong baselines perplexity scores also better. google production dataset model achieved higher test bleu score even training sixth time. dataset train single gnmt model large combined dataset twelve language pairs. results somewhat worse separately trained single-pair gnmt models. surprising given twelve models times capacity twelve times aggregate training model. repeat experiment single moe-augmented model. appendix details model architecture. train model dataset process number training examples training time shorter lower computational budget model. results results single-pair gnmt models multilingual gnmt model multilingual model given table model achieves lower perplexity multilingual gnmt model. bleu score model signiﬁcantly beats multilingual gnmt model language pairs even beats monolingual gnmt models language pairs. poor performance english korean seems result severe overtraining rarer language pairs small number real examples highly oversampled training corpus. ops/timestep training time hardware perplexity french english test bleu german english test bleu japanese english test bleu korean english test bleu portuguese english test bleu spanish english test bleu english french test bleu english german test bleu english japanese test bleu english korean test bleu english portuguese test bleu english spanish test bleu work ﬁrst demonstrate major wins conditional computation deep networks. carefully identiﬁed design considerations challenges conditional computing addressed combination algorithmic engineering solutions. focused text conditional computation help domains well provided sufﬁciently large training sets. look forward seeing many novel implementations applications conditional computation years come. would like thank members google brain google translate teams helped project particular zhifeng chen yonghui melvin johnson. thanks also anonymous iclr reviewers helpful suggestions making paper better. references martín abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro gregory corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal józefowicz lukasz kaiser manjunath kudlur josh levenberg mané rajat monga sherry moore derek gormurray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda viégas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorﬂow large-scale machine learning heterogeneous distributed systems. corr abs/. http//arxiv.org/abs/.. dario amodei rishita anubhai eric battenberg carl case jared casper bryan catanzaro jingdong chen mike chrzanowski adam coates greg diamos erich elsen jesse engel linxi christopher fougner tony awni hannun billy patrick legresley libby sharan narang andrew sherjil ozair ryan prenger jonathan raiman sanjeev satheesh david seetapun shubho sengupta wang zhiqian wang chong wang xiao dani yogatama zhan zhenyao zhu. deep speech end-to-end speech recognition english mandarin. arxiv preprint arxiv. yoshua bengio nicholas léonard aaron courville. estimating propagating gradients stochastic neurons conditional computation. arxiv preprint arxiv. ciprian chelba tomas mikolov mike schuster thorsten brants phillipp koehn tony robinson. billion word benchmark measuring progress statistical language modeling. arxiv preprint arxiv. geoffrey hinton deng dong george dahl abdel-rahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara sainath deep neural networks acoustic modeling speech recognition shared views four research groups. ieee signal processing magazine melvin johnson mike schuster quoc maxim krikun yonghui zhifeng chen nikhil thorat fernanda viégas martin wattenberg greg corrado macduff hughes jeffrey dean. google’s multilingual neural machine translation system enabling zero-shot translation. corr abs/. http//arxiv.org/abs/.. quoc marc’aurelio ranzato rajat monga matthieu devin chen greg corrado jeffrey dean andrew building high-level features using large scale unsupervised learning. icml hasim andrew senior françoise beaufays. long short-term memory recurrent neural network architectures large scale acoustic modeling. interspeech yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey jeff klingner apurva shah melvin johnson xiaobing łukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes jeffrey dean. google’s neural machine translation system bridging human machine translation. arxiv preprint arxiv. discussed section load-balancing purposes want deﬁne additional loss function encourage experts receive roughly equal numbers training examples. unfortunately number examples received expert discrete quantity used backpropagation. instead deﬁne smooth estimator load number examples assigned expert batch inputs. smoothness allows back-propagate gradients estimator. purpose noise term gating function. deﬁne probability nonzero given random choice noise element keeping already-sampled choices noise elements. compute note nonzero greater kth-greatest element excluding itself. probability works initial load imbalance avoid out-of-memory errors need initialize network state approximately equal expert load accomplish this initialize matrices wnoise zeros yields signal noise. experiments trained models identical architecture using different values wimportance wload. trained model epochs measured perplexity test set. also measured coefﬁcients variation importance load well ratio load overloaded expert average load. last value signiﬁcant load balancing purposes distributed hardware. metrics averaged several training batches. results results reported table combinations containing least losses similar model quality loss much worse. models higher values wload lower loads overloaded expert. number experts large reduce branching factor using two-level hierarchical moe. hierarchical primary gating network chooses sparse weighted combination experts\" secondary mixture-of-experts gating network. hierarchical consists groups experts each denote primary gating network gprimary secondary gating networks expert networks output given loadprimary loadi deonte load functions primary gating network secondary gating network respectively. denotes subset gprimaryi would seem simpler loadh loadij would gradient respect primary gating network formulation above. model architecture model consists layers word embedding layer recurrent long short-term memory layer layer second lstm layer softmax layer. dimensionality embedding layer number units lstm layer input output dimensionality layer equal every layer softmax apply drouput layer output dropping activation probability dropp otherwise dividing dropout output previous layer added layer output. residual connection encourages gradient layer architecture expert layer feed forward network relu-activated hidden layer size output layer size thus expert contains parameters. output layer passed sigmoid function dropout. varied number experts models using ordinary layers experts hierarchical layers experts. call resulting models moe- moe- moe- moe--h moe--h moe-h. hierarchical layers ﬁrst level branching factor corresponding number gpus cluster. noisy-top-k gating ordinary layers level hierarchical layers. thus example processed exactly experts total ops/timestep. lstm layers contribute ops/timestep desired total computationally-matched baselines moe- model employ sparsity since experts always used. addition trained four computationally-matched baseline models sparsity xlstm- replace layer additional -unit lstm layers. lstm-- model contains -unit lstm layer output lstm projected dimensions next timestep lstm receives projected output. identical models published re-ran account differences training regimen obtained results similar published ones. training models trained cluster gpus using synchronous method described section batch consisted sentences totaling roughly words. interest time limited training epochs training took hours models except moe- took hours used adam optimizer base learning rate increased linearly ﬁrst training steps decreased proportional inverse square root step number. softmax output layer trained efﬁciently using importance sampling similarly models model performed hyper-parmeter search best dropout probability increments ensure balanced expert utilization wimportance wload described section appendix results evaluate model using perplexity holdout dataset used follow standard procedure words including sentence symbol. results reported table model report test perplexity computational budget parameter counts value dropp computational efﬁciency. additional models investigate effects adding computation presence large layer. models computation budgets ops/timestep. similar models above models layer lstm layers. dimensionality embedding layer input output dimensionality layer instead moe-m lstm layers units. moe-m lstm layers units output projection size moe-m uses hierarchical layer experts hidden layer size moe-m uses hierarchical layer experts hidden layer size models parameters layers. searched best dropp model trained model epochs. models achieved test perplexity respectively showing even presence large computation still useful. results reported bottom table larger models similar computational budget best published model literature training times similar. comparing epochs model lower test perplexity model architecture models similar structure -million-operations-per-timestep models described previous section. vary number experts models using ordinary layer experts hierarchical layers experts. hierarchical layers ﬁrst level branching factors respectively. training models trained cluster tesla gpus except last models trained clusters gpus enough memory parameters. models training batch sizes approximately million words. models trained once-through billion words. implement several memory optimizations order billion parameters gpu. first store activations hidden layers experts instead recompute backwards pass. secondly modify optimizer expert parameters require less auxiliary storage adam optimizer keeps ﬁrst second moment estimates perparameter gradients. triples required memory. avoid keeping ﬁrst-moment estimator reduce size second moment estimator replace factored approximation. matrix parameters instead maintaining full matrix second-moment estimators maintain vectors row-wise column-wise averages matrix. step matrix estimators taken outer product vectors divided mean either one. technique could similarly applied adagrad model baseline model. notable measured computational efﬁciency largest model compared models. likely result fact that purposes comparison models increase training batch size proportionally number gpus. comparison include results computationally matched baseline model consisting lstms unpruned -gram model kneser-ney smoothing model architecture single language pair models model modiﬁed version gnmt model described reduce computation decrease number lstm layers encoder decoder respectively. insert layers encoder decoder attention mechanism encoder decoder ﬁrst decoder lstm receiving output providing input attention layers model input output dimensionality lstm layers hidden units -dimensional output projection. residual connections around lstm layers encourage gradient similar gnmt effectively deal rare words used subword units inputs outputs system. train models different numbers experts layers. addition baseline model layers train models layers containing experts models hierarchical layers containing experts. layers hierarchical models level gating network. thus input processed exactly experts layer. expert layer feed forward network hidden layer size relu activation. thus expert contains parameters. output layer passed sigmoid function. strictly-balanced gating function described appendix model architecture multilingual model used model architecture single-language-pair models following exceptions used noisy-top-k gating described section scheme appendix layers encoder decoder non-hierarchical moes experts expert larger hidden layer size doubles amount computation layers raising computational budget entire model ops/timestep. training trained networks using adam optimizer base learning rate increased linearly ﬁrst training steps held constant additional steps decreased proportional inverse square root step number. single-language-pair models similarly applied dropout output embedding lstm layers using dropp training done synchronously cluster gpus described section training batch consisted sentence pairs containing roughly words gpu. ensure balanced expert utilization wimportance wload described section appendix metrics evaluated models using perplexity standard bleu score metric. reported tokenized bleu score computed multi-bleu.pl script downloaded public implementation moses also used while original size corpus billion words neural models trained maximum billion words. reported kneser-ney -gram models trained billion billion words respectively giving slight advantage reported results. results tables section show comparisons results published methods. figure shows test perplexity function number words source sentences processed models different numbers experts. seen figure increased number experts approach test perplexity model continued improve. figure perplexity wmt’ google production datasets function number words processed. large differences models beginning training different batch sizes. models incur computational budget except experts. found experts indeed become highly specialized syntax and/or semantics seen table example expert used indeﬁnite article introduces direct object verb phrase indicating importance leadership. table contexts corresponding experts layer encoder portion wmt’ translation model. expert sort inputs training batch decreasing order show words surrounding corresponding positions input sentences. peculiarities infrastructure since ﬁxed time machine translation experiments models faster every expert received exactly batch size. accommodate this used different gating function describe below. sparse gating obtain sparse gating vector multiply component-wise sparse mask normalize output. mask function speciﬁes experts assigned input example batchwise mask force expert receive exact number examples introduce alternative mask function mbatchwise operates batches input vectors. instead keeping values example keep values expert across training batch k|x| experiments suggest also observed using batchwise function training requires modiﬁcations inference large batch examples. solution train vector per-expert threshold values approximate effects batchwise mask. following mask inference time attention mechanism described gnmt involves learned attention function\" takes source vector\" target vector\" must computed every source time step target time step gnmt attention function implemented feed forward neural network hidden layer size expressed attention function simultaneously compute attention function multiple source time steps multiple target time steps using optimized matrix multiplications. found little difference quality functions.", "year": 2017}