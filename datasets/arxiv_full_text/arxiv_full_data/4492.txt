{"title": "The Bayesian Decision Tree Technique with a Sweeping Strategy", "tag": ["cs.AI", "cs.LG"], "abstract": "The uncertainty of classification outcomes is of crucial importance for many safety critical applications including, for example, medical diagnostics. In such applications the uncertainty of classification can be reliably estimated within a Bayesian model averaging technique that allows the use of prior information. Decision Tree (DT) classification models used within such a technique gives experts additional information by making this classification scheme observable. The use of the Markov Chain Monte Carlo (MCMC) methodology of stochastic sampling makes the Bayesian DT technique feasible to perform. However, in practice, the MCMC technique may become stuck in a particular DT which is far away from a region with a maximal posterior. Sampling such DTs causes bias in the posterior estimates, and as a result the evaluation of classification uncertainty may be incorrect. In a particular case, the negative effect of such sampling may be reduced by giving additional prior information on the shape of DTs. In this paper we describe a new approach based on sweeping the DTs without additional priors on the favorite shape of DTs. The performances of Bayesian DT techniques with the standard and sweeping strategies are compared on a synthetic data as well as on real datasets. Quantitatively evaluating the uncertainty in terms of entropy of class posterior probabilities, we found that the sweeping strategy is superior to the standard strategy.", "text": "abstract--the uncertainty classification outcomes crucial importance many safety critical applications including example medical diagnostics. applications uncertainty classification reliably estimated within bayesian model averaging technique allows prior information. decision tree classification models used within technique gives experts additional information making classification scheme observable. markov chain monte carlo methodology stochastic sampling makes bayesian technique feasible perform. however practice mcmc technique become stuck particular away region maximal posterior. sampling causes bias posterior estimates result evaluation classification uncertainty incorrect. particular case negative effect sampling reduced giving additional prior information shape dts. paper describe approach based sweeping without additional priors favorite shape dts. performances bayesian techniques standard sweeping strategies compared synthetic data well real datasets. quantitatively evaluating uncertainty terms entropy class posterior probabilities found sweeping strategy superior standard strategy. uncertainty classification outcomes crucial importance many safety critical applications medical diagnostics prediction survival patient injuries. applications bayesian model averaging provide reliable estimates classification uncertainty. decision tree classification models within bayesian averaging framework gives experts additional information making classification scheme observable main idea using classification models recursively partition data points axis-parallel manner. models provide natural feature selection uncover schetinin j.e. fieldsend partridge w.j. krzanowski r.m. everson t.c. bailey hernandez school engineering computer science mathematics university exeter generally hierarchical system consisting splitting terminal nodes. binary splitting nodes specific question divide data points disjoint subsets. terminal node assigns data points falling node class whose points prevalent. within bayesian framework class posterior distribution calculated terminal node makes bayesian integration computationally expensive breiman provided bayesian generalization tree models required evaluate posterior distribution. make bayesian averaging feasible approach chipman suggested markov chain monte carlo technique making stochastic sample posterior distribution. sampling across models variable dimensionality technique exploits reversible jump extension suggested green prior information distorted number samples reasonably large mcmc technique making birth death change-question change-rule moves explores posterior distribution result provides accurate estimates posterior. however practice lack prior information brings bias posterior estimates result evaluation classification uncertainty incorrect within mcmc technique prior number splitting nodes given properly. otherwise samples taken posterior calculated located away region containing desired models. likewise prior number splits assigned uniform minimal number data points allowed nodes inappropriately small. case grow excessively samples taken posterior distribution calculated over-fitted dts. result inappropriately assigned priors leads poor results clearly lack prior knowledge favored structure often happens practice increases uncertainty results bayesian averaging dts. paper decrease uncertainty classification outcomes using bayesian strategy sampling models. main idea behind strategy explicitly assign prior probability splitting nodes rule) rule define position predictor rule splitting node respectively. parameters priors specified follows. first define maximal number splitting nodes smax second draw predictors uniform discrete distribution finally candidate value assign drawn uniform discrete splitting variable total number distribution case prior complete described follows aving priors parameters jjjj qqqq determine marginal likelihood data given classification tree. general case likelihood written multinomial dirichlet distribution sampling large decision trees allow sampling models variable dimensionality mcmc technique exploits reversible jump extension extension allows mcmc technique sample large induced real-world data. implement mcmc technique chipman denison suggested exploring posterior probability using following types moves. bayesian techniques standard sweeping strategies evaluated terms entropy class posterior probabilities described evaluations made artificial problem real datasets taken machine learning repository well real trauma dataset taken london emergency centre. remaining part paper organized follows. sections describe standard suggested bayesian techniques. section describes experiments conducted compare performance classification uncertainty techniques. finally section concludes paper. integral analytically calculated simple cases. practice part integrand posterior density qqqq conditioned data cannot usually evaluated. however values qqqq qqqq drawn posterior distribution write basis mcmc technique approximating integrals perform approximation need generate random samples running markov chain converged stationary distribution. draw samples markov chain estimate predictive posterior density define classification problem presented data number data points categorical response. using classification need determine probability datum assigned terminal node class number terminal nodes initially assign -dimensional dirichlet prior terminal node dic- jjjji knowledge favored structure case knowledge favored structure chipman suggested generalization prior assuming prior probability split terminal nodes dependent many splits already made them. example terminal node probability splitting written difficulties sampling large decision trees induced real-world data number splitting nodes large. cases size rationally decrease defining minimal number data points pmin allowed splitting nodes number data points partitions made birth change moves becomes less given number pmin moves assigned unavailable mcmc algorithm resamples moves. moves assigned unavailable distorts proposal probabilities given birth death change moves respectively. larger smaller number data points falling splitting nodes correspondingly larger probability moves become unavailable. resampling unavailable moves makes balance proposal probabilities biased. show balance proposal probabilities biased assume example probabilities equal respectively note large birth change moves assigned unavailable probabilities equal respectively. result birth change moves made probabilities equal respectively. emulate moves resultant probabilities shown fig. figure probabilities birth death moves become equal first moves birth death reversible change dimensionality qqqq described remaining moves provide current dimensionality qqqq. note change-split move included make large jumps potentially increase chance sampling maximal posterior whilst change-rule move local jumps. dts. restricting strategy however requires setting manner additional parameters size number first burn-in samples. sadly practice often happens limitation period grow quickly strategy improve performance. alternatively approach based explicit limitation size search space extended using restarting strategy chipman suggested clearly strategies cannot guarantee sampled model space region maximal posterior. next section describe approach based sweeping dts. iii. bayesian averaging sweeping strategy section describe approach decreasing uncertainty classification outcomes within bayesian averaging models. main idea approach assign prior probability splitting nodes dependent range values within number data points less given number points pmin. prior explicit current partition range values unknown. partition levels hand partition level number data points becomes less given number pmin. therefore conclude prior probability splitting ranges variable partition levels follows first level partition probability equal variable assume first partition split original data non-empty parts. parts contains less data points original data consequently partition either splitting variable case numerator decreases probability becomes less partition makes values numerator consequently probability smaller. probability splitting nodes dependent level partitioning data set. prior favors splitting terminal nodes contain large number data points. clearly desired property mcmc technique allows accelerating convergence markov chain. result using prior mcmc technique sampling fig. proposal probabilities birth death change moves denoted first second third groups respectively. left hand bars group denote proposal probabilities equal respectively. right hand bars groups denote resultant probabilities birth death change moves made reality birth change moves assigned unavailable probabilities respectively. disproportion balance probabilities birth death moves dependent size averaged samples. clearly beginning burn-in phase disproportion close zero burn-in phase size form stabilized value becomes maximal. grow quickly first burn-in samples increase likelihood value birth moves much larger others. reason almost every partition data accepted. grown change moves accepted small probability result mcmc algorithm tends stuck particular structure instead exploring possible structures. hierarchical structures changes nodes located upper levels significantly change location data points lower levels. reason small probability changing accepting located near root node. therefore mcmc algorithm collects splitting nodes located root node changed. nodes typically contain small numbers data points. subsequently value likelihood changed much moves frequently accepted. result mcmc algorithm cannot explore full posterior distribution properly. extend search space restrict sizes given number first burn-in samples described indeed restriction strategy gives chances finding smaller size could competitive term likelihood values larger within approach terminal node making birth change moves contains less pmin data points removed clearly removing unacceptable nodes turns random search direction mcmc algorithm chances find maximum posterior amongst shorter dts. process unacceptable nodes removed named strategy sweeping. example fig. provides resultant probabilities estimated moves case original probabilities birth death change moves equal respectively assumed example given unacceptable birth change moves equal values less previous example induced sweeping strategy shorter induced standard strategy. shorter data points fall splitting nodes less probabilities are. addition unacceptable change moves assigned third option mentioned above partitions contain less pmin data points. unacceptable birth moves reassigning unacceptable change moves resultant probabilities birth death moves become equal approximately i.e. values probabilities similar shown fig. fig. proposal probabilities birth death change moves denoted first second third groups respectively. left hand bars group denote proposal probabilities equal respectively. right hand bars groups denote resultant probabilities birth death change moves made reality unacceptable birth moves redone unacceptable change moves reassigned probabilities respectively. however prior dependent level partition also distribution data points partitions. analyzing data partition value probability dependent distribution data. reason prior cannot implemented explicitly without distribution data points partition. prior uninformative used rule available. information preferable values uniform distribution drawing rule rulenew proposed level cause partitions containing less data points pmin. however within technique proposals avoided. hierarchical structure change moves applied first partition levels heavily modify shape result bottom partitions contain less data points pmin. mentioned section within bayesian techniques moves assigned unavailable. within approach birth change move arise three possible cases. first case number data points partition larger pmin. second case number data points partition larger pmin. third case number data points partitions larger pmin. three cases processed follows. second case node containing unacceptable number data points removed resultant move birth type mcmc resamples otherwise algorithm performs death move. classification uncertainty techniques compared terms entropy described entropy summed class posterior probabilities calculated test datum class follows experimental evaluation proposal bayesian mcmc strategy used following data sets. first artificial exclusive problem output sign noise variable. three problems taken machine learning repository last real problem required predict survival probability patient injury. table lists number classes number variables number patterns data sets. first problem resolved consisting three nodes. pruning factor pmin equal proposal probabilities death birth changesplit change-rules respectively. number burn-in post burn-in samples sampling rate respectively. resultant bayesian perform quite well recognizing test examples. acceptance rate burn-in post burn-in. average number nodes interval within fold cross-validation respectively. fig. depicts samples likelihood numbers nodes well densities nodes burn-in post burn-in phases. left plot figure markov chain quickly converges stationary value likelihood near zero. post burn-in values likelihood slightly oscillate around zero depicted fig. table provides performance rates standard bayesian strategy bayesian technique sweeping strategy calculated data sets within fold cross-validation. techniques proposal probabilities value pmin. table strategies reveal performance test data. important prior information preferable number nodes minimal number pmin available. number pmin equal first four domain problems ionosphere votes wisconsin equal last green reversible jump markov chain monte carlo computation bayesian model determination biometrika denison holmes malick smith bayesian methods domingos bayesian averaging classifiers overfitting problem proceedings seventeenth international conference machine learning. stanford morgan kaufmann mcmc methodology stochastic sampling posterior distribution makes bayesian techniques feasible. however exploring space parameters existing techniques prefer sampling local maxima posterior instead properly representing posterior. affects evaluation posterior distribution result causes increase classification uncertainty. negative effect reduced averaging obtained different starts restricting size burn-in phase alternative reducing negative effect suggested bayesian technique using sweeping strategy. within strategy modified birth change moves removing splitting nodes containing fewer data points acceptable. compared performances bayesian techniques standard sweeping strategies synthetic dataset well datasets machine learning repository real injury data. quantitatively evaluating uncertainty terms entropy found bayesian technique using sweeping strategy superior standard bayesian technique. also observe sweeping strategy provides much shorter dts. thus conclude bayesian strategy averaging able decreasing classification uncertainty without affecting classification accuracy problems examined. clearly desirable property classifiers used safety-critical systems classification uncertainty crucial importance.", "year": 2005}