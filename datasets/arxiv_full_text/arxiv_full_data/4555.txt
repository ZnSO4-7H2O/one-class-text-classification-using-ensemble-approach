{"title": "Inverse-Category-Frequency based supervised term weighting scheme for  text categorization", "tag": ["cs.LG", "cs.AI"], "abstract": "Term weighting schemes often dominate the performance of many classifiers, such as kNN, centroid-based classifier and SVMs. The widely used term weighting scheme in text categorization, i.e., tf.idf, is originated from information retrieval (IR) field. The intuition behind idf for text categorization seems less reasonable than IR. In this paper, we introduce inverse category frequency (icf) into term weighting scheme and propose two novel approaches, i.e., tf.icf and icf-based supervised term weighting schemes. The tf.icf adopts icf to substitute idf factor and favors terms occurring in fewer categories, rather than fewer documents. And the icf-based approach combines icf and relevance frequency (rf) to weight terms in a supervised way. Our cross-classifier and cross-corpus experiments have shown that our proposed approaches are superior or comparable to six supervised term weighting schemes and three traditional schemes in terms of macro-F1 and micro-F1.", "text": "term weighting schemes often dominate performance many classifiers centroid-based classifier svms. widely used term weighting scheme text categorization i.e. tf.idf originated information retrieval field. intuition behind text categorization seems less reasonable paper introduce inverse category frequency term weighting scheme propose novel approaches i.e. tf.icf icf-based supervised term weighting schemes. tf.icf adopts substitute factor favors terms occurring fewer categories rather fewer documents. icf-based approach combines relevance frequency weight terms supervised way. cross-classifier cross-corpus experiments shown proposed approaches superior comparable supervised term weighting schemes three traditional schemes terms macro-f micro-f. text categorization task labeling natural language texts thematic categories predefined natural language text often converted machine friendly format computer classifiers ―understand‖ content text. step called text representation. vector space model content text represented vector term term size term weight space i.e. document term weighting schemes often affect effectiveness classifiers. example leopold kindermann pointed performance classifiers dominated term weighting schemes rather kernel functions. therefore well-defined term weighting scheme assign appropriate weighting value term. present types term weighting schemes unsupervised term weighting schemes supervised term weighting schemes utws widely used task utws variations borrowed information retrieval field famous tf.idf proposed jones robertson however task differs task. task categorical information terms training documents available advance. categorical information importance task. debole sebastiani proposed supervised term weighting methods used known categorical information training corpus. adopted values three feature selections substitute factor weighting terms. thorough experiments exhibit uniform superiority respect standard tf.idf supervised term weighting schemes however seem reasonable unsupervised ones task. recently proposed probability based supervised term weighting scheme solve imbalanced text classification problem. similarly used relevance frequency substitute also proposed novel supervised term weighting i.e. tf.rf. however supervised term weighting schemes common shortcoming simplification multiclass classification problem multiple independent binary classification problems. process simplification distribution term among categories disappears because positive category negative category. previous researches shown important using alone achieve good performance task therefore researchers focus factor instead. discriminating power term document related also related distribution term among categories. intuition fewer term appears categories discriminative term text categorization. similar task. paper inspect role inverse category frequency introduce term weighting scheme task propose novel term weighting schemes based i.e. tf.icf icf-based supervised term weighting schemes applied multi-class binary respectively. three widely used corpora i.e. skewed reuters- balanced newsgroup tf.icf outperforms three traditional schemes terms macro-averaging micro-averaging three different classifiers multi-class classification tasks. moreover icf-based adopting merits outperforms seven supervised term weighting schemes standard tf.idf binary classification tasks. rest paper organized follows. briefly review related work section present overview term weighting schemes including unsupervised term weighting schemes supervised term weighting schemes section section propose tf.icf icf-based supervised term weighting scheme separately give detailed explanation. text collection text process benchmark methodology classifiers evaluation measures presented section discuss detailed experimental results three corpora section conclude paper section recent years widely used many applications instance spam email classification question categorization online news classification. generally tasks divided single-label learning task multi-label learning task many techniques explored literature e.g. centroid-based classifier naïve bayes decision tree support vector machines since paper focuses term weighting functions interested techniques find details sebastiani term weighting functions originate field famous tf.idf huge success salton buckley discussed many typical term weighting schemes field found best document weighting function. tf.idf’s success researchers retain function task many tasks usually take tf.idf defaulted term weighting function. improved schemes. instance zhou found distributional features useful text categorization. guan applied inter-class inner-class term indices construct centroid vectors better initial values traditional methods. inter-class index variation factor. however debole sebastiani proposed supervised term weighting schemes idea replacing factor three feature selection functions i.e. information gain chi-square gain ratio. proposed tf.rf improve performance text categorization related relevant documents. experiments showed tf.rf outperformed supervised term weighting schemes traditional ones moreover proposed probability based supervised term weighting scheme improve performance imbalanced text classification. quan adopted three supervised term weighting schemes question classification. proposed improved tf.idf method combined tf.idf information gain. many variations used document classification despite vast research efforts term weighting schemes study still needed reveal whether inverse category frequency beneficial term weighting schemes realize study paper tries make contributions problem. mentioned above unsupervised term weighting schemes borrowed field. intuition filed query term occurrence many documents good discriminator given less weight occurred less documents. debole sebastiani concluded three assumptions tf.idf model i.e. assumption rare terms less important frequent terms; assumption multiple occurrences term document less important single occurrence; normalization assumption long documents important short documents quantity term matching. according assumptions researchers proposed many variations tf.idf model. present standard ―ltc‖ variation many researchers believe term weighting schemes form tf.idf also applied task. intuition filed seems reasonable. however needs carefully weighed task task different task. task term occurs many documents maybe good discriminator especially occurs categories category. example corpus patents patent category ―computer science‖ contain term ―computer‖ ―software‖. document frequency term corpus high mean term less discrimination? contrast term regards powerful feature category ―computer science‖. therefore consider distribution term among categories rather among documents. first motivation revise term weighting scheme task. researchers noted tf.idf best term weighting model task. therefore debole sebastiani proposed supervised term weighting schemes. introduced term selection term weighting schemes made phases term weighting activity supervised learning information membership training documents categories used. later research showed models using feature selection metrics weight term superior traditional tf.idf model give detailed formulations readers interested methods refer debole sebastiani related relevant documents contain term four different factors including base logarithmic operation factor whereas base constants respectively; factor divided whereas term frequency; considers ratio know prob-based scheme proposed deal imbalanced text classification supposed balance rare categories. instance given catego used variations factor question categorization problem. guan adopted construct centroid vector category centroid-based classifier outperformed classifiers closed tests. intuition behind fewer categories term occurs discriminating power term contributes text categorization. assumption named assumption favors rare terms biases popular terms category level. therefore introduce factor term weighting scheme propose novel approaches i.e. tf.icf icf-based supervised term weighting schemes. formulae expressed respectively. respect tf.idf tf.icf mixture model term weighting. factors tf.idf model estimated document level factor tf.icf estimated document level factor estimated category level. tf.icf different previous tf.icf methods name different meanings. example reed abbreviation dealing stream data inverse document frequency whole corpus. lertnatteed also proposed tficf whose factor estimated category level. kimura proposed tf.icf weight terms cross-language retrieval system factor defined ratio term frequency number terms category mation acquired saved analyzing training corpus thus used calculate term weight training corpus partitioned positive category negative category. select four terms real-world text corpus verify effectiveness icf-based method. first terms related category last terms related category earn. table list values terms using nine term weighting schemes. number nearby term category frequency corpus. table shown weighting values four terms using methods consider positive negative categories. term ―payout‖ filtered thod assigns weighting value term ―dividend‖ wrongly tagged positive category. remaining methods i.e. icf-based discriminate terms categories correctly. worthy noting category frequency ―payout‖ term appears category earn. weighting value ―payout‖ enlarged emphasize role euters- reuters- data set* widely used benchmarking collection data based trinity college dublin version changed documents original sgml format format. according ―modapte‖ split subset categories removing unlabeled documents documents class labels denoted reuters- single-label multi-class corpus training documents test documents. imbalance problem reuters- serious common category accounts whole training whereas bottom categories contain several training instances category. stop words punctuation numbers removed; letters converted lowercase; word stemming applied. final vocabulary words. newsgroup data set† consists documents uniformly distributing twenty categories. data also famous benchmark tasks randomly select instances category test instances rest texts training instances. training instances test instances. keep ―subject‖ ―keywords‖ ―content‖. information ―path‖ ―from‖ ―message-id‖ ―sender‖ ―organization‖ ―references‖ ―date‖ ―lines‖ email addresses filtered out. stop words list words keep words occur least twice. letters converted lowercase word stemming applied. final vocabulary words. la：the corpus‡ larger data text-data collection. derived trec consists examples. randomly choose examples test rest training set. total number words data sets feature selection used three classifiers handle high-dimensional data. reuters- trial provides separated training test sets according ―modapte‖ split; newsgroup trials since given test acquire averaged macro-f micro-fof trials evaluate proposed approaches. efore applying classifiers text represented vector. text process utws stws different thus give detailed explanations follows. utws compare performance multi-class classification task. first process training save value term files used calculate weighting value term test set. l-normalization performed normalize term weight. stws evaluate nine methods binary classification task suited binary classification experiment chosen category tagged positive category rest categories training corpus combined together negative category. obtain statistical information term positive negative classes save files. analyzing document test calculate weighting value term combining statistical information training set. achieves top-notch performance among widely used classification algorithms libsvm adopt kernel functions i.e. linear function radial basis function multi-class classification tasks -fold cross validation find optimal parameters penalty parameter kernel parameter rbf. cross validation time-consuming thus binary classification tasks select linear kernel defaulted parameters order save training time. meanwhile centroid-based classifier outperforms naïve bayes text categorization according previous research centroid-based classifier affected term weighting scheme also adopted order compare previous research experiment similarity measure classifiers cosine function. unless otherwise specified default parameter values classifier experiments. measure effectiveness terms precision recall defined usual measures popular performance measures tasks. measure effectiveness combines contributions well-known function defined usually estimated ways multi-class problem i.e. macro-averaging micro-averaging macro-f gives weight categories thus mainly influenced rare categories skewed reuters- corpus. contrary micro-f dominated performance common categories skewed reuters- corpus. therefore macro-f micro-f reuters- give quite different results. balance newsgroup macro-f micro-f newsgroup quite similar. binary classification problem obtain true-positive number calculate score positive class calculate macro-f micro-f entire data set. irstly compare tf.icf unsupervised term weighting schemes multi-class classification tasks. report overall performance tf.icf method three corpora multi-class classification task. tables show performance four term weighting schemes terms macro-f micro-f skewed reuters- four classifiers best result highlighted bold. shown table tf.icf performs consistently best experiments especially centroid-based classifiers. example compared tf.idf macro-f micro-f tf.icf using centroid-based classifier improve respectively; using improvements respectively. using classifier performance tf.idf close tf.icf. reason performance dominated kernel function rather term weighting function. shown second third columns sigresults observed table reports performance balanced newsgroup corpus. macro-f micro-f close report micro-f table compared traditional term weighting methods find tf.icf achieves best performance. respect tf.idf micro-f tf.icf improved using centroid-based classifier; improvement using knn. table show performance comparisons four term weighting schemes data set. observe tf.icf outperforms three methods terms macro-f micro-f. improvement significant centroid-based classifiers used. example comparing tf.idf macro-f micro-f tf.icf improved employing knn. achieve worse performance four classifiers verified previous research also find performance superior classifiers used conclusion opposite centroid-based classifier adopted. employ mcnemar’s significance test verify difference performance term weighting schemes. reuters- corpus using classifier ―>>‖ denotes better significance level however tf.icf using classifier tf.idf employing centroid-based classifier. according cross-classifier cross-corpus experiments tf.icf used standard term weighting scheme multi-class task tf.icf seems reasonable tf.idf tf.icf fully exploit known information training instances i.e. distribution keyword among categories. meanwhile tf.icf achieve consistently best performance experiments. section continue show superiority icf-based supervised term weighting scheme binary classification task compared existing methods i.e. tf.rf prob-based tf.logor tf.gr tf.ig tf.icf tf.idf. supervised term weighting schemes adopt ―local policy‖ construct training test sets supervised term weighting methods suited binary classification tasks. experiment chosen category tagged positive category rest categories training corpus combined together negative category. table report overall macro-f micro-f nine term weighting schemes skewed reuters- corpus respectively. shown table proposed icf-based method consistently outperforms eight methods three classifiers terms macro-f improvement icf-based method significant. example tf.gr performance icf-based method improves using compared respectively; also observe icf-based always superior prob-based tf.idf terms macro-f. compare performance icf-based method state-of-the-art tf.rf. best macro-f icf-based supervised term weighting scheme reuters- corpus reaches icf-based little superior tf.rf icf-based method improves tf.rf. meanwhile observe tf.icf achieved third best macro-f score using tf.ig tf.gr using centroid classifier icf-based method achieves best macro-f i.e. improved tf.rf method. reason tf.rf performs lower minor categories. prob-based methods achieve second best macro-f score. rest methods worse icf-based method. terms micro-f similar results shown table icf-based supervised term weighting scheme performs better results classifiers except centroid classifier achieves best micro-f another method i.e. tf.icf. example using icf-based method performs best accuracy i.e. bigger tf.rf method. centroid classifier used tf.icf method achieves best performance. table shows overall micro-f performance nine term weighting schemes balanced newsgroup corpus little different reuters-. shown table icf-based method tf.icf tf.logor achieve best micro-f using centroid-based classifiers respectively. used best micro-f icf-based reaches improved tf.rf. must noted tf.icf tf.logor centroid classifier significantly outperform icf-based method classifier. reason could skewed data affects performance classifier. ratio positive category negative category defaulted parameters train classifier binary classification tasks. global optimization scheme lead misclassifications instances minority classes contrary centroid classifiers advantages dealing imbalanced text classification also noted property data corpus great impact term weighting schemes. instance tf.idf achieves second best performance newsgroup even better icf-based scheme centroid-based classifier used. similar explanation found macro-f micro-f corpus shown table respectively. observe icf-based method outperforms methods terms macro-f micro-f. example compared tf.rf icf-based method improves using micro-f; improvement employing centroid-based classifier. besides tf.rf，the performance icf-based method significantly better methods e.g. tf.idf tf.logor tf.ig etc. newsgroup data icf-based adopting svm; using prob-based tf.icf tf.idf icf-based prob-based centroid-based classifier employed tf.logor tf.idf tf.icf", "year": 2010}