{"title": "Large Scale Distributed Semi-Supervised Learning Using Streaming  Approximation", "tag": ["cs.LG", "cs.AI"], "abstract": "Traditional graph-based semi-supervised learning (SSL) approaches, even though widely applied, are not suited for massive data and large label scenarios since they scale linearly with the number of edges $|E|$ and distinct labels $m$. To deal with the large label size problem, recent works propose sketch-based methods to approximate the distribution on labels per node thereby achieving a space reduction from $O(m)$ to $O(\\log m)$, under certain conditions. In this paper, we present a novel streaming graph-based SSL approximation that captures the sparsity of the label distribution and ensures the algorithm propagates labels accurately, and further reduces the space complexity per node to $O(1)$. We also provide a distributed version of the algorithm that scales well to large data sizes. Experiments on real-world datasets demonstrate that the new method achieves better performance than existing state-of-the-art algorithms with significant reduction in memory footprint. We also study different graph construction mechanisms for natural language applications and propose a robust graph augmentation strategy trained using state-of-the-art unsupervised deep learning architectures that yields further significant quality gains.", "text": "large label size problem recent works propose sketch-based methods approximate label distribution node thereby achieving space reduction certain conditions. paper present novel streaming graphbased approximation eﬀectively captures sparsity label distribution reduces space complexity node also provide distributed version algorithm scales well large data sizes. experiments real-world datasets demonstrate method achieves better performance existing state-of-the-art algorithms signiﬁcant reduction memory footprint. finally propose robust graph augmentation strategy using unsupervised deep learning architectures yields signiﬁcant quality gains natural language applications. semi-supervised learning methods small amounts labeled data along large amounts unlabeled data train prediction systems. approaches gained widespread usage recent years rapidly supplanting supervised systems many scenarios owing abundant amounts unlabeled data available domains. annotating creating labeled training data many predictions tasks quite challenging often expensive labor-intensive process. hand unlabeled data readily available leveraged approaches improve performance supervised prediction systems. several surveys cover various methods literature majority algorithms computationally expensive; example transductive graph-based algorithms subclass techniques received attention recently scale much better large problems data sizes. methods exploit idea constructing smoothing graph data represented nodes edges link vertices related other. edge weights deﬁned using similarity function node pairs govern strongly labels nodes connected edge agree. graph-based methods based label propagation work using class label information associated labeled seed node propagating labels graph principled iterative manner. methods often converge quickly time space complexity scales linearly number edges include wide range tasks computer vision information retrieval social networks natural language processing example class instance acquisition relation prediction name several classiﬁcation knowledge expansion type problems involve large number labels realworld scenarios. instance entity-relation classiﬁcation widely used freebase taxonomy requires learning thousands labels grow orders extending open-domain edge weight matrix. every edge assigned weight among number nodes |vl| labeled |vu| unlabeled. diagonal matrix record seeds node seed. represents output label whose size large real world. matrix seeds label distribution assignment matrix nodes. general method graph-based semi-supervised learning algorithm learns propagating information graph node prior distribution labels. objective function models that label distribution close gold label assignment seeds; label distribution pair neighbors similar measured aﬃnity score edge weight matrix; label distribution close prior uniform distribution. setting hyperparameters discussed section optimization criterion inspired similar existing approaches adsorption uses slightly diﬀerent objective function notably matrices diﬀerent constructions. section also compare vanilla version baselines completeness. traction social media; scenarios involving complex overlapping classes ﬁne-grained classiﬁcation large scale natural language computer vision applications unfortunately existing graph-based methods cannot deal initialized sparse label distributions become dense later iterations propagate graph. talukdar cohen recently proposed method seeks overcome label scale problem using count-min sketch approximate labels scores node. reduces memory complexity also report improved running times using sketch-based approach. however realworld applications number actual labels associated node typically sparse even though erly leveraging sparsity scenarios yield huge beneﬁts terms eﬃciency scalability. sketching technique approximates label space succinctly utilize sparsity full beneﬁt learning. contributions paper propose graph propagation algorithm general purpose semisupervised learning applications areas. show algorithm eﬃciently even label size huge. core approximation eﬀectively captures sparsity label distribution ensures algorithm propagates labels accurately. reduces space complexity node practice) scales better previous methods. show eﬃciently parallelize algorithm proposing distributed version scales well large graph sizes. also propose eﬃcient linear-time graph construction strategy eﬀectively combine information multiple signals vary sparse dense representations. particular show graphs nodes represent textual information possible robustly learn latent semantic embeddings associated nodes using text state-of-the-art deep learning techniques. augmenting original graph embeddings followed graph yields significant improvements quality. demonstrate power method evaluating diﬀerent knowledge expansion tasks using existing benchmark datasets. results show that compared existing state-of-the-art systems tasks method performs better terms space complexity qualitative performance. seed/labeled nodes unlabeled nodes output label distribution ˆyv... ˆyvm every node minimizing overall objective function here ˆyvl represents weight label assigned node send previous label distribution ˆyi− receive message neighbor corresponding label weights process message m...m|n update current label distribution iteratively using equation iteration index uniform distribution label iterative procedure starts initialized seed label weight else uniform distribution aggregates label distribution iteration neighbors details deriving update many applications semi-supervised learning becomes challenging graphs become huge. scale really large data sizes propose distexpander distributed version algorithm directly suited towards parallelization across many machines. turn pregel open source version giraph underlying framework distributed algorithm. systems follow bulk synchronous parallel model computation proceeds rounds. every round every machine local processing sends arbitrary messages machines. semantically think communication graph ﬁxed round node performs local computation sends messages neighbors. speciﬁc systems like pregel giraph build infrastructure ensures overall system fault tolerant eﬃcient fast. programmer’s simply specify code vertex every round. previously works explored using mapreduce framework scale large graphs unlike methods pregelbased model eﬃcient better suited graph algorithms iterative optimization scheme algorithms. pregel keeps vertices edges machine performs computation uses network transfers messages. mapreduce however essentially functional expressing graph algorithm chained mapreduce requires passing entire state graph stage next—in general requiring much communication associated serialization overhead results signiﬁcant network cost detailed comparison). addition need coordinate steps chained mapreduce adds programming complexity avoided distexpander iterations rounds/steps. furthermore version pregel allows spilling disk instead storing entire computation state unlike algorithm describes details. talukdar cohen proposed deal issue large label spaces employing count-min sketch approximation store label distribution node. however argue necessary approximate whole label distribution node especially large label sets label distribution node typically sparse ranking ones useful. moreover count-min sketch even harmful ranking labels approximation. authors also mention related works attempt induce sparsity using regularization techniques diﬀerent purpose contrast work tackles exact problem scale graph-based large label settings. method presented attempt enforce sparsity instead focuses eﬃciently storing updating label distributions semi-supervised learning streaming approximation. addition also compare relevant graphexpander-s method propose streaming sparsity approximation algorithm semi-supervised learning achieves constant space complexity huge memory savings current state-of-the-art approach addition signiﬁcant runtime improvements exact version. method processes messages neighbors eﬃciently streaming fashion records sparse ranking labels node approximate estimate remaining. general idea similar ﬁnding frequent items data streams item label streams messages neighbors case. pregel-based approach provides natural framework implement idea processing message streams. replace update step algorithm version thereby allowing scale large label spaces data using framework. preliminary manku motwani presented algorithm computing frequency counts exceeding user-speciﬁed threshold data streams others applied algorithm handle large amounts data problems general idea data stream containing elements split multiple epochs elements epoch. thus epochs total epoch starting algorithm processes elements epoch sequentially maintains list tuples form item reported frequency maximum error frequency estimation. current epoch item comes increments frequency count item contained list tuples. ples form label index weighted probability value maximum error weighted probability estimation. current neighbor node receives label distribution ˆyutl edge weight wvut. algorithm things label currently tuple list increments probability value yutl. creates tuple adding wvut form here probability threshold value item frequency stream naturally probability weight. moreover epoch neighbor task weighted edge weight wvut unlike previous settings then receive message t-th neighbor ﬁlter labels whose maximum probability small. delete label memory-bounded update given streaming sparsity algorithm ensure weighted-probability labels retained receiving messages neighbors. however many cases want number retained labels retain top-k label based bounded probability. case node record average probability mass remaining labels. apply previous streaming sparsity algorithm. diﬀerence label exists current tuple list creates tuple form intuitively instead setting ﬁxed global threshold vary threshold based sparsity previous seen neighbors. epoch receiving messages ˆyut current neighbor scan current tuple list. tuple increments probability value adding label within top-k label list current t-th neighbor. then wvuiδui. finally receiving messages neighbors rank remaining tuples based value within tuple value represents maximum weighted-probability estimation. pick top-k labels record probabilities current node normalized true label weights estimate given streaming sparsity approximation version expander algorithm given iteration. total number label entries received neighbors aggregation entries retained past graph-based methods widely applied several problems. many scenarios nodes represent textual information could augmented semantic information real world. recently researchers explored strategies enhance input graphs using external sources knowledge base. however methods require access structured information knowledge base access search results corresponding large number targeted queries particular domain. unlike methods propose robust strategy graph augmentation follows two-step approach using large corpus text. first learn dense vector representation captures underlying semantics associated node. resort recent state-of-the-art deep learning algorithms eﬃciently learn word phrase semantic embeddings dense low-dimensional space large text corpus using unsupervised methods. follow recent work mikolov compute continuous vector representations words large datasets. method takes text corpus input learns vector representation every word vocabulary. continuous skip-gram model combined hierarchical softmax layer word sentence used input log-linear classiﬁer tries maximize classiﬁcation another word within sentence using current word. details deep learning architecture training procedure found moreover models eﬃciently parallelized scale huge datasets using distributed training framework obtain -dimensional vector representation trained billion tokens newswire text. settings nodes represent entity names train embedding model take account treating entity mentions special words applying procedure earlier produce embedding vectors entities. next node ww...wn query pre-trained vectors obtain corresponding embedding vemb words node text. proof ﬁrst part statement derived following similar analysis using label weights instead frequency. iteration algorithm ensures labels weights retained remaining ones estimate close exact label weight within additive factor. second part statement follows direclty fact node retains atmost labels every iteration. detailed proof included here. main ingredient graph-based approaches input graph itself. demonstrate choice graph construction mechanism important eﬀect quality output. depending edge link information well choice vertex representation multiple ways create input graph ssl— generic graphs represent observed neighborhood link information connecting vertices graphs constructed sparse feature representations vertex graphs constructed dense representations vertex i.e. dense feature characteristics node deﬁne neighborhood augmented graphs mixture above. figure shows illustration various graph types. focus since applicable natural language scenarios. sparse instance-feature graphs typically provided input tasks nlp. next propose method automatically construct graph text applications using semantic embeddings produce augmented graph captures sparse dense per-vertex characteristics. similarity computations infeasible practice. address challenge resort locality sensitive hashing random projection method used eﬃciently approximate nearest neighbor lookups data size dimensionality large. node embedding vectors vemb perform signiﬁcantly reduce unnecessary pairwise computations would yield similarity values. freebase-entity exact dataset setup used previous works dataset consists cell value nodes property nodes entities table properties freebase. edge indicates entity appears table cell. second dataset freebase-relation dataset comprises entity-relation-entity triples freebase consists relations triples. extract kinds nodes triples entity-pair nodes entity nodes former labeled relation type edge created nodes entity common. graph-based systems compare diﬀerent graph-based methods expander vanilla method version runs graph semantic augmentation expander-s streaming approximation algorithm introduced section baseline comparison consider state-of-art existing works mad-sketch talukdar pereira show outperforms traditional graph-based algorithms. madsketch approximates label distribution node using count-min sketch reduce space time complexity. ensure fair comparison obtained code directly authors exact code machine expander experiments reported here. obtained performance freebase-entity dataset reported parameters objective function parameters tried multiple settings mad-sketch algorithms replicated best reported performance metrics using values baseline results comparable system. evaluation precisionk mean reciprocal rank used evaluation metrics experiments higher better. measures accuracy ranking labels returned method. calcu test node lated rankv rank gold label among label distribution ˆyv. first quantitatively compare graph-based methods terms precisionk without considering space time complexity. table shows results seeds/label seeds/label freebase-entity dataset. results several ﬁndings expander-based methods outperform consistently terms precisonk. algorithm enhanced graph using semantic embeddings produces signiﬁcant gains original graph indicates densifying graph additional information provides useful technique improving scenarios. section compare mad-sketch expander-s algorithms vanilla versions. former uses count-min sketch approximate whole label distribution node latter uses streaming approximation capture sparsity label distribution. freebaseentity dataset methods seeds/label. freebase-relation dataset single machine sample smaller dataset dataset fb-r compare approximation methods mad-sketch expander-s picking seeds/label taking average rounds. test mad-sketch since setting madsketch runs out-of-memory using single machine. protocol buﬀers store data expander-s. space report memory taken whole process. expander-s described section node stores labels precisionk available refer tables show results freebase-entity smaller freebase-relation datasets respectively. make following observations mad-sketch obtain similar performance compared achieve ever sketch size small algorithm loses quality terms precisionk. applications involving large label sizes space limitations allocate limited memory size node still able retain accurate relevant labels within available memory. fb-r data observe mad-sketch executable mad-sketch yields comparing expander poor results. expander-s latter obtains similar performance terms precisionk achieves speedup space reduction. compared mad-sketch speedup steep mainly algorithm needs tuple list ﬁlter ones below threshold ensure retain good labels. however easily execute expanderfrequency thresholding streaming sparsity also compare streaming approximation algorithm simple frequency-based thresholding technique used often online sparse learning algorithms however becomes computanodes high degree prohibitive since requires aggregate label distributions neighbors pruning. cases still maintain constant space complexity per-node retaining top-k labels update step. table shows streaming sparsity approximation produces signiﬁcantly better quality results frequency thresholding addition computationally eﬃcient. streaming approximation update focus large data settings freebaserelation data subsequent experiments. following previous work identity node label. words label size potentest computation time scales number nodes ﬁxing label size. follow straightforward strategy randomly sample diﬀerent number edges original graph. graph size randomly sample nodes seeds node identities labels. vanilla expander expander-s dist-expander-s graph. last distributed version partitions graph runs machines. show running time diﬀerent node sizes figure expander runs out-of-memory node size goes running time slows signiﬁcantly graph size increases. expander-s handle data sets single machine running time better expander starts slow noticeably larger graphs nodes. dist-expander-s scales quite well node size yields -fold speedup compared expander-s node size figure illustrates memory usage scales label size distributed version. scenario entire freebase-relation dataset vary label size randomly choosing diﬀerent number seed nodes labels. overall figure iteration runtime massive graphs varying sizes distributed across machines. space cost consistently around streaming algorithm captures sparse constant space approximation label distribution out-of-memory even large label sizes. note distributed version consumes primarily redundant information recorded partitioning graph finally test distributed sparsity approximation algorithm scales massive graphs billions nodes edges. since freebase graph suﬃciently large setting construct graphs varying sizes diﬀerent dataset figure illustrates streaming distributed algorithm scales eﬃciently scenarios runs quite fast. computing iteration distexpander-s runs completion seconds node/.m edge graph roughly minutes much larger node/.b edge graph. existing graph-based algorithms usually require space node scale scenarios involving large label sizes massive graphs. propose novel streaming algorithm eﬀectively accurately captures sparsity label distribution. algorithm operates eﬃciently streaming fashion reduces space complexity node addition yielding high quality performance. moreover extend method distributed algorithm scales elegantly large data label sizes also show graph augmentation using unsupervised learning techniques provide robust strategy yield performance gains problems involving natural language.", "year": 2015}