{"title": "Intriguing properties of neural networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.  First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.  Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "text": "deep neural networks highly expressive models recently achieved state performance speech visual recognition tasks. expressiveness reason succeed also causes learn uninterpretable solutions could counter-intuitive properties. paper report properties. first distinction individual high level units random linear combinations high level units according various methods unit analysis. suggests space rather individual units contains semantic information high layers neural networks. second deep neural networks learn input-output mappings fairly discontinuous signiﬁcant extent. cause network misclassify image applying certain hardly perceptible perturbation found maximizing network’s prediction error. addition speciﬁc nature perturbations random artifact learning perturbation cause different network trained different subset dataset misclassify input. deep neural networks powerful learning models achieve excellent performance visual speech recognition problems neural networks achieve high performance express arbitrary computation consists modest number massively parallel nonlinear steps. resulting computation automatically discovered backpropagation supervised learning difﬁcult interpret counter-intuitive properties. paper discuss counter-intuitive properties deep neural networks. ﬁrst property concerned semantic meaning individual units. previous works analyzed semantic meaning various units ﬁnding inputs maximally activate given unit. inspection individual units makes implicit assumption units last feature layer form distinguished basis particularly useful extracting semantic information. instead show section random projections semantically indistinguishable coordinates puts question conjecture neural networks disentangle variation factors across coordinates. generally seems entire space activations rather individual units contains bulk semantic information. similar even stronger conclusion reached recently mikolov word representations various directions vector space representing words shown give rise surprisingly rich semantic encoding relations analogies. time second property concerned stability neural networks respect small perturbations inputs. consider state-of-the-art deep neural network generalizes well object recognition task. expect network robust small perturbations input small perturbation cannot change object category image. however applying imperceptible non-random perturbation test image possible arbitrarily change network’s prediction perturbations found optimizing input maximize prediction error. term perturbed examples adversarial examples. natural expect precise conﬁguration minimal necessary perturbations random artifact normal variability arises different runs backpropagation learning. found adversarial examples relatively robust shared neural networks varied number layers activations trained different subsets training data. neural generate adversarial examples examples still statistically hard another neural network even trained different hyperparameters surprisingly trained different examples. results suggest deep neural networks learned backpropagation nonintuitive characteristics intrinsic blind spots whose structure connected data distribution non-obvious way. traditional computer vision systems rely feature extraction often single feature easily interpretable e.g. histogram colors quantized local derivatives. allows inspect individual coordinates feature space link back meaningful variations input domain. similar reasoning used previous work attempted analyze neural networks applied computer vision problems. works interpret activation hidden unit meaningful feature. look input images maximize activation value single feature aforementioned technique formally stated visual inspection images satisfy held-out images data distribution network trained natural basis vector associated i-th hidden unit. experiments show random direction gives rise similarly interpretable semantic properties. formally images semantically related other many suggests natural basis better random basis inspecting properties puts question notion neural networks disentangle variation factors across coordinates. first evaluated claim using convolutional neural network trained mnist. used mnist test figure shows images maximize activations natural basis figure shows images maximize activation random directions. cases resulting images share many high-level similarities. next repeated experiment alexnet used validation figures compare natural basis random basis trained network. rows appear semantically meaningful single unit combination units. although analysis gives insight capacity generate invariance particular subset input distribution explain behavior rest domain. shall next section counterintuitive properties neighbourhood almost every point form data distribution. unit-level inspection methods relatively little utility beyond conﬁrming certain intuitions regarding complexity representations learned deep neural network global network level inspection methods useful context explaining classiﬁcation decisions made model used instance identify parts input correct classiﬁcation given visual input instance represents conditional distribution label given input argued deep stack non-linear layers input output unit neural network model encode non-local generalization prior input space. words assumed possible output unit assign nonsigniﬁcant probabilities regions input space contain training examples vicinity. regions represent instance objects different viewpoints relatively share nonetheless label statistical structure original inputs. implicit arguments local generalization—in proximity training examples—works expected. particular small enough radius vicinity given training input satisfying ||r|| assigned high probability correct class model. kind smoothness prior typically valid computer vision problems. general imperceptibly tiny perturbations given image normally change underlying class. main result deep neural networks smoothness assumption underlies many kernel methods hold. speciﬁcally show using simple optimization procedure able adversarial examples obtained imperceptibly small perturbations correctly classiﬁed input image longer classiﬁed correctly. sense describe traverse manifold represented network efﬁcient ﬁnding adversarial examples input space. adversarial examples represent low-probability pockets manifold hard efﬁciently simply randomly sampling input around given example. already variety recent state computer vision models employ input deformations training increasing robustness convergence speed models deformations however statistically inefﬁcient given example highly correlated drawn distribution throughout entire training model. propose scheme make process adaptive exploits model deﬁciencies modeling local space around training data. make connection hard-negative mining explicitly close spirit hard-negative mining computer vision consists identifying training examples given probabilities model high probability instead training distribution changed emphasize hard negatives round model training performed. shall described optimization problem proposed work also used constructive similar hard-negative mining principle. formal description denote classiﬁer mapping image pixel value vectors discrete label set. also assume associated continuous loss function denoted lossf given image target label solve following box-constrained optimization problem minimizer might unique denote arbitrarily chosen minimizer informally closest image classiﬁed obviously task non-trivial general exact computation hard problem approximate using box-constrained l-bfgs. concretely approximation performing line-search minimum minimizer following problem satisﬁes observations suggest adversarial examples somewhat universal results overﬁtting particular model speciﬁc selection training set. also suggest back-feeding adversarial examples training might improve generalization resulting models. preliminary experiments yielded positive evidence mnist support hypothesis well successfully trained layer non-convolutional neural network test error keeping pool adversarial examples random subset continuously replaced newly generated adversarial examples mixed figure adversarial examples generated alexnet correctly predicted sample difference correct image image predicted incorrectly magniﬁed adversarial example. images right column predicted ostrich struthio camelus. average distortion based examples plase refer http//goo.gl/huagpb full resolution images. examples strictly randomly chosen. postselection involved. figure adversarial examples quocnet binary classiﬁer trained last layer features without ﬁne-tuning. randomly chosen examples left recognized correctly cars images middle recognized. rightmost column magniﬁed absolute value difference images. original training time. used weight decay dropout network. comparison network size gets errors regularized weight decay alone improved around using carefully applied dropout. subtle essential detail improvements generating adversarial examples layer outputs used train layers above. network trained alternating fashion maintaining updating pool adversarial examples layer separately addition original training set. according initial observations adversarial examples higher layers seemed signiﬁcantly useful input lower layers. future work plan compare effects systematic manner. space considerations present results representative subset mnist experiments performed. results presented consistent larger variety non-convolutional models. mnist results convolutional models ﬁrst qualitative experiments alexnet gives reason believe convolutional networks behave similarly well. models trained l-bfgs convergence. ﬁrst three models linear classiﬁers work pixel level various weight decay parameters examples quadratic weight decay connection weights added total loss number units layer. three models simple linear classiﬁer without hidden units them trained extremely high order test whether still possible generate adversarial examples extreme setting well.two models simple sigmoidal neural network hidden layers classiﬁer. last model consists single layer sparse autoencoder sigmoid activations nodes softmax classiﬁer. network trained high quality ﬁrst layer ﬁlters layer ﬁne-tuned. last column measures minimum average pixel level distortion necessary reach accuracy original distorted training set. distortion measure figure adversarial examples randomly chosen subset mnist compared randomly distorted examples. columns correspond original images even columns correspond distorted counterparts. adversarial examples generated speciﬁc model accuracy respective model. note randomly distorted examples hardly readable still classiﬁed correctly half cases adversarial examples never classiﬁed correctly. table cross-model generalization adversarial examples. columns tables show error induced distorted examples given model. last column shows average distortion wrt. original training set. ﬁrst experiment generated adversarial instances given network examples network measure proportion misclassiﬁed instances. last column shows average minimum distortion necessary reach accuracy whole training set. experimental results presented table columns table show error distorted training sets. last rows given reference showing error induced distorting given amounts gaussian noise. note even noise stddev greater stddev adversarial noise models. figure shows visualization generated adversarial instances networks used experiment general conclusion adversarial examples tend stay hard even models trained different hyperparameters. although autoencoder based version seems resilient adversarial examples fully immune either. still experiment leaves open question dependence training set. hardness generated examples rely solely particular choice training sample effect generalize even models trained completely different training sets? table models trained study cross-training-set generalization generated adversarial examples. errors presented table correpond original not-distorted data provide baseline. table cross-training-set generalization error rate adversarial examples generated different models. error induced random distortion examples displayed last row. study cross-training-set generalization partitioned mnist training images parts size trained three non-convolutional networks sigmoid activations them fc-- fc-- fc-- reason trained networks study cumulative effect changing hypermarameters training sets time. models fc-- share hyperparameters networks fc-- different number hidden units. experiment distorting elements test rather training set. table summarizes basic facts models. generate adversarial examples error rates minimum distortion test feed examples models. error model displayed corresponding column upper part table last experiment magnify effect distortion using examples rather magniﬁes distortion average stddev distorted examples back models error rates displayed lower part table intriguing conclusion adversarial examples remain hard models trained even disjoint training although effectiveness decreases considerably. previous section showed examples deep networks resulting purely supervised training unstable respect peculiar form small perturbations. independently generalisation properties across networks training sets adversarial examples show exist small additive perturbations input produce large perturbations output last layer. section describes simple procedure measure control additive stability network measuring spectrum rectiﬁed layer. \u0001−γr corresponds common operating regimes. results conservative measure unstability network obtained simply computing operator norm fully connected convolutional layer. fully connected case trivial since norm directly given largest singular value fully connected matrix. describe convolutional case. denotes generic -tensor implementing convolutional layer input features output features support spatial stride denotes c-th input feature image spatial kernel corresponding input feature output feature applying parseval’s formula obtain operator norm given table shows upper lipschitz bounds computed imagenet deep convolutional network using shows instabilities appear soon ﬁrst convolutional layer. results consistent exsitence blind spots constructed previous section don’t attempt explain examples generalize across different hyperparameters training sets. emphasize compute upper bounds large bounds automatically translate existence adversarial examples; however small bounds guarantee examples appear. suggests simple regularization parameters consisting penalizing upper lipschitz bound might help improve generalisation error networks. demonstrated deep neural networks counter-intuitive properties respect semantic meaning individual units respect discontinuities. existence adversarial negatives appears contradiction network’s ability achieve high generalization performance. indeed network generalize well confused adversarial negatives indistinguishable regular examples? possible explanation adversarial negatives extremely probability thus never observed test dense found near every virtually every test case. however don’t deep understanding often adversarial negatives appears thus issue addressed future research.", "year": 2013}