{"title": "Meta-Learning by Adjusting Priors Based on Extended PAC-Bayes Theory", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "In meta-learning an agent extracts knowledge from observed tasks, aiming to facilitate learning of novel future tasks. Under the assumption that future tasks are 'related' to previous tasks, representations should be learned in a way which captures the common structure across learned tasks, while allowing the learner sufficient flexibility to adapt to novel aspects of new tasks. We present a framework for meta-learning that is based on generalization error bounds, allowing us to extend various PAC-Bayes bounds to meta-learning. Learning takes place through the construction of a distribution over hypotheses based on the observed tasks, and its utilization for learning a new task. Thus, prior knowledge is incorporated through setting an experience-dependent prior for novel tasks. We develop a gradient-based algorithm which minimizes an objective function derived from the bounds and demonstrate its effectiveness numerically with deep neural networks. In addition to establishing the improved performance available through meta-learning, we demonstrate the intuitive way by which prior information is manifested at different levels of the network.", "text": "meta-learning agent extracts knowledge observed tasks aiming facilitate learning novel future tasks. assumption future tasks ‘related’ previous tasks representations learned captures common structure across learned tasks allowing learner sufﬁcient ﬂexibility adapt novel aspects tasks. present framework meta-learning based generalization error bounds allowing extend various pac-bayes bounds meta-learning. learning takes place construction distribution hypotheses based observed tasks utilization learning task. thus prior knowledge incorporated setting experience-dependent prior novel tasks. develop gradient-based algorithm minimizes objective function derived bounds demonstrate effectiveness numerically deep neural networks. addition establishing improved performance available meta-learning demonstrate intuitive prior information manifested different levels network. learning examples process inferring general rule ﬁnite examples. well known statistics learning cannot take place without prior assumptions. recent work deep neural networks achieved signiﬁcant success using prior knowledge implementation structural constraints e.g. convolutions weight sharing however often relevant prior information given task clear need building viterbi faculty electrical engineering technion israel institute technology haifa israel. correspondence amit <ronamitcampus.technion.ac.il> meir <rmeiree.technion.ac.il>. learning previous interactions world. learning previous experience take several forms continual learning learning agent trained sequence tasks aiming solve current task maintaining good performance previous tasks. multi-task learning learning agent learns solve several observed tasks exploiting shared structure. domain adaptation learning agent solves ‘target’ learning task using ‘source’ tasks work within framework meta-learning learning-to-learn lifelong learning inductive transfer ‘meta-learner’ extracts knowledge several observed tasks facilitate learning tasks ‘base-learner’ setup meta-learner must generalize ﬁnite observed tasks. performance evaluated learning related tasks motivational example consider case meta-learner observes many image classiﬁcation tasks natural images uses learn task. meta-learner might learn prior ﬁxes lower layers network extract generic image features allows variation higher layers adapt classes. thus tasks learned using fewer examples learning scratch generally scenarios might instead beneﬁt sharing parts network framework prior automatically inferred observed tasks rather manually inserted algorithm designer. notion ‘task-environment’ formulated baxter analogy standard single-task learning data sampled unknown distribution baxter suggested setting tasks sampled unknown task distribution knowledge acquired previous tasks used order improve performance novel task. baxter’s work provided interesting mathematically precise perspective meta-learning also provided generalization bounds unknown probability distribution namely notation denote distribution full sample. supervised learning samples input/output pairs usual learning goal based function so-called hypothesis space minimizes expected loss function loss function bounded distribution unknown learning consists selecting appropriate based sample classiﬁcation space classiﬁers mapping input space ﬁnite classes. noted introduction inductive bias required effective learning. standard approach learning described previous paragraph usually selects single classiﬁer pac-bayes framework ﬁrst formulated mcallester considers construction complete probability distribution selection single hypothesis based distribution. since distribution depends data referred posterior distribution denoted note term ‘posterior’ bayesian connotation framework necessarily bayesian posterior need related prior likelihood function standard bayesian analysis. pac-bayes framework widely studied recent years given rise signiﬁcant ﬂexibility learning importantly best generalization bounds available audibert mcallester lever framework recently extended meta-learning setting pentina lampert extended applied neural networks present contribution. following notation introduced deﬁne generalization error empirical used singletask setting. since distribution unknown cannot directly computed. pac-bayesian setting learner outputs distribution entire hypothesis space goal provide posterior distribution denotes distributions expected generalization error empirical error given setting averaging posterior distribution namely respectively. average describes gibbs prediction procedure ﬁrst draw hypothesis apply sample paper work within framework formulated baxter following setup pentina lampert provide generalization error bounds within pac-bayes framework. bounds used develop practical learning algorithm applied neural networks demonstrating utility approach. main contributions work following. improved tighter bound theoretical framework pentina lampert derived using technique extend different single-task pac-bayes bounds meta-learning setup. principled meta-learning method implementation using probabilistic feedforward neural networks. empirical demonstration performance enhancement compared naive approaches well recent methods ﬁeld. related work many recent developments meta-learning andrychowicz finn based generalization error bounds focus present work. elegant extension generalization error bounds meta-learning provided pentina lampert mentioned work however provide practical algorithm applicable deep neural networks. recently dziugaite developed single-task algorithm based pac-bayes bounds demonstrated yield good performance simple classiﬁcation tasks deep networks. recent theoretical approaches meta multitask learning ruvolo eaton maurer alquier provide increasingly general bounds directly practical learning algorithms deep neural networks. section introduce pac-bayesian bound single-task setting. bound also serve meta-learning setting next sections. pac-bayesian bounds based specifying ‘prior’ reference distribution must depend observed data distribution hypotheses provided output learning process called posterior classical pacbayes theorem single-task learning formulated mcallester theorem prior distribution following inequality holds uniformly posteriors distributions probability least theorem interpreted stating high probability expected error upper bounded empirical error plus complexity term. since high probability bound holds uniformly holds also data dependent choosing minimizes bound obtain learning algorithm generalization guarantees. note pac-bayes bounds express trade-off ﬁtting data complexity/regularization term encourages selecting ‘simple’ hypothesis namely similar prior. speciﬁc choice affects bound’s tightness express prior knowledge problem. generally want prior close posteriors achieve training error. section introduce meta-learning setting. setting meta-learning agent observes several ‘training’ tasks task environment. meta-learner must extract common knowledge tasks used learning tasks environment. literature setting often called learning-to-learn lifelong-learning metaas noted above terms ‘prior’ ‘posterior’ might misleading since bayesian inference setting however pac-bayes bayesian analysis interesting practical connections next sections learning bias learning formulate problem provide generalization bound later lead practical algorithm. work extends pentina lampert establishes potentially tighter bound. furthermore demonstrate apply result practically deep neural networks using stochastic learning. meta-learning problem formulation follows pentina lampert assume tasks share sample space hypothesis space loss function learning tasks differ unknown sample distribution associated task meta-learning agent observes training sets corresponding different tasks. number samples task denoted observed dataset assumed generated unknown sample distribution baxter assume sample distributions generated i.i.d. unknown tasks distribution goal meta-learner extract knowledge observed tasks used prior knowledge learning tasks prior knowledge comes form distribution hypotheses learning task base learner uses observed task’s data prior output posterior distribution assume tasks learned learning process. namely given speciﬁc output hence base learner mapping quality prior measured expected loss using learn tasks deﬁned since want prove pac-bayes style bound metalearning assume meta-learner select single prior instead infers ‘hyper’ distribution prior distributions since inferred observing tasks called hyper-posterior distribution serves prior task i.e. learning task learner draws prior uses learning. emphasize hyper-posterior learned using observed tasks goal learning independent task environment. ideally performance hyper-posterior measured expected generalization loss learning tasks using priors generated quantity detasks. term converges zero limit large number samples task second environment-complexity term. term converges zero inﬁnite number tasks observed task environment pentina lampert proof based main steps. second step similarly pentina lampert bounds generalization error task-environment level average generalization error observed tasks plus environment-complexity term. ﬁrst step differs pentina lampert instead using single joint bound average generalization error singletask pac-bayes theorem bound generalization error task separately union bound argument. bound takes account speciﬁc number samples observed task therefore bound better adjusted observed data set. proof technique utilize different single-task bounds steps. section mcallester’s bound tighter lemma used pentina lampert therefore complexity terms form pentina lampert means bound tighter section demonstrate technique other possibly tighter single-task bounds. section empirically evaluate different bounds meta-learning objectives show improved tightness critical performance. single-task case bound theorem evaluated training data serve minimization objective principled meta-learning algorithm. since bound holds uniformly ensured hold also minimizer objective section derive practical learning procedure applied large family differentiable models including deep neural networks. section choose speciﬁc form hyper-posterior distribution enables practical implementation. given parametric family priors space hyper-posteriors consists distributions limit search family gaussian distributions deﬁned prove pac-bayes style upper bound transfer error minimized single-task pac-bayes setting selects prior seeing data updates posterior observing training data. present meta-learning setup following pentina lampert selects initial hyper-prior distribution essentially distribution prior distributions following observation data tasks updates hyperposterior distribution simple example assume initial prior gaussian distribution neural network weights characterized mean covariance. hyper distribution would correspond case distribution mean covariance theorem base learner predeﬁned hyper-prior distribution. following inequality holds uniformly hyper-posterior distributions probability least notice transfer error bounded empirical multi-task error plus complexity terms. ﬁrst average task-complexity terms observed probability taken sampling section make meta-learning optimization problem explicit deﬁning model posterior prior distributions. first deﬁne hypothesis class family functions parameterized weight vector prior distributions present algorithm differentiable model neural network architectures. fact stochastic since setting weights random optimizing posterior distribution. techniques presented next mostly based blundell next deﬁne posteriors prior factorized gaussian distributions theorem allows choose procedure base learner. procedure minimizes following advantages minimizes bound generalization error observed task uses prior knowledge gained prior tighter bound better learning objective. shown next formulating single task learning optimization problem enables joint learning shared prior task posteriors. formulate single-task learning optimization problem choose parametric form posterior task base-learning algorithm formulated argminφi abuse notation denoting term evaluated posterior parameters expectation approximated averaging small number monte-carlo samples reasonable accuracy. ﬁxed sampled gradient easily computable backpropagation. summary meta-learning adjusting priors algorithm composed phases ﬁrst phase several observed training tasks used learn prior. second phase previously learned prior used learning task note ﬁrst phase used independently multi-task learning method. algorithms described pseudo-code supplementary material section demonstrate performance transfer method image classiﬁcation tasks solved deep neural networks. image classiﬁcation data samples consist image label hytheoretical framework deﬁned bounded loss experiments unbounded loss function learning objective. still theoretical guarantees variation loss clipped furthermore practice loss function almost always smaller one. conduct experiments different task environments based augmentations mnist dataset ﬁrst environment termed permuted labels task created random permutation labels. second environment termed permuted pixels task created permutation image pixels. pixel permutations created limited number location swaps ensure tasks stay reasonably related. experiments meta-training composed tasks environment training examples. following meta-training phase learned prior used learn meta-test task fewer training samples network architecture used permutedlabels experiment small convolutionallayers linear hidden layer linear output layer underlying optimization method stochastic gradient descent iteration algorithm takes parameter step direction estimated negative gradient. well known lower variance facilitates convergence speed. recall single-task bound composed empirical error term complexity term complexity term simple function easily differentiated analytically. however evaluating gradient empirical error term challenging. ew∼qφi term poses major challenges. data could large making expensive cycle samples. term might highly non-linear rendering expectation intractable. still unbiased variance estimate gradient. first instead using data gradient estimation randomly sampled mini-batch next require estimate gradient form common problem w∼qφ machine learning. ‘re-parametrization trick’ efﬁcient variance method re-parametrization trick easily applicable setup since using gaussian distributions. trick based describing gaussian distribution ﬁrst drawing applying deterministic function element-wise multiplication). therefore fact ‘local re-parameterization trick’ sample different data point batch reduces variance estimate. make computation efﬁcient neural-networks random number generation performed w.r.t activations instead weights details.). frozen. permuted pixels freeze layers except input layer. refer method ‘oracle’ since transfer technique tailored task-environment methods applied identically environment finally compare methods transfer knowledge training tasks mlap-m objective based theorem meta-learning bound obtained using theorem mlap-s objective based meta-learning bound derived seeger’s single-task bound mlap-pl method main theorem pentina lampert objective algorithm instead theorem mlapvb method learning objective derived hierarchal bayesian framework using variational bayes tools averaged training tasks learned standard obtain weights vector learned prior isotropic gaussian unit variances mean vector average prior used meta-testing mlap-s. maml model-agnostic-meta-learning algorithm finn maml base learner takes gradient steps initial point adapt task. meta-learner optimizes based losses observed tasks base-learning. tested several hyper-parameters report best results permuted labels experiment best results obtained oracle method. recall oracle method unfair advantage hand-engineered transfer technique based knowledge problem. contrast methods must automatically learn task environment observing several tasks. mlap-m mlap-s variants mlap algorithm improves considerably learning scratch naive warm-start transfer. even improve oracle method permuted pixels experiment. result mlap-vb close mlap-m mlap-s variants. however mlap-pl variant performed much worse demonstrates importance using tight generalization bound developed work learning objective. tested generalization performance learning ‘meta-test’ task using mlap algorithm transfer prior several ‘meta-training’ tasks different taskenvironments namely permuted labels environment permuted pixels environment pixel swaps. figure plot average test error learning task based number training-tasks different environments. results clearly show tasks used learn prior better performance task. example permuted labels case prior learned based tasks leads negative transfer worse results standard learning scratch achieves error. however observing tasks transfered prior facilitates learning lower generalization error. permuted pixels experiment standard learning scratch achieves test error. number training tasks needed positive transfer depends number pixels swapped. higher number swaps means larger variation task environment training-tasks needed learn beneﬁcial prior. next compare average generalization performance learning task using following methods. baseline measure performance learning without transfer meta-training tasks. scratch-d deterministic learning scratch. scratch-s stochastic learning scratch methods transfer knowledge meta-train tasks. warm-start standard learning initial weights taken standard learning single task meta-train set. oracle previous method layers frozen depending experiment. permuted labels experiment layers besides output results mlap algorithm slightly better maml. note maml meta-learning infers initial point base-learning. thus trade-off choosing number adaptation steps. taking many gradient steps exploits larger number samples effect initial weights diminishes. also taking large number steps computationally infeasible meta-training. therefore maml especially suited few-shot learning case experiment. method infer prior serves initial point regularizer weights allowing variation others depending amount data. recent work grant showed maml interpreted approximate empirical bayes procedure. interesting perspective differs present contribution based generalization bounds within non-bayesian setting. analysis learned prior qualitative examination learned prior afﬁrms indeed adjusted task environment. permuted-labels experiment learned prior assigns variance lower layers high variance output layer expected permuted-pixels experiment opposite phenomenon occurs. mapping ﬁnal hidden layer output becomes ﬁxed mapping input ﬁnal hidden layer ﬂexibility change light task data. presented framework representational metalearning motivated extended pac-bayes generalization bounds implemented adjustment learned prior based tasks encountered far. framework bears conceptual similarity empirical bayes method bayesian implemented level tasks rather samples combining ﬂexibility approach rich representational structure deep neural networks learning gradient based methods leads efﬁcient procedure meta-learning motivated theoretically demonstrated empirically. experimental results preliminary believe work attests utility using rigorous performance bounds derive learning algorithms demonstrates tighter bounds indeed lead improved performance. several open issues consider. first current version learns solve available tasks parallel useful procedure sequential nature. easily incorporated framework updating prior following novel task. second method requires training stochastic models challenging high-variance gradients. would like develop methods within framework stable convergence easier apply larger scale problems. third much current effort applying meta-learning ideas reinforcement learning example presents heuristically motivated framework conceptually similar ours. interesting challenge would extend techniques derive meta-learning algorithms reinforcement learning based performance bounds. thank asaf cassel tennenholtz baruch epstein daniel soudry elad hoffer zahavy helpful discussions work. gratefully acknowledge support nvidia corporation donation titan used research. andrychowicz marcin denil misha gomez sergio hoffman matthew pfau david schaul freitas nando. learning learn gradient descent advances neural information gradient descent. processing systems ben-david shai blitzer john crammer koby kulesza alex pereira fernando vaughan jennifer wortman. theory learning different domains. machine learning grant erin finn chelsea levine sergey darrell trevor grifﬁths thomas. recasting gradient-based meta-learning hierarchical bayes. arxiv preprint arxiv. kirkpatrick james pascanu razvan rabinowitz neil veness joel desjardins guillaume rusu andrei milan kieran quan john ramalho tiago grabskabarwinska agnieszka overcoming catastrophic forgetting neural networks. proceedings national academy sciences blundell charles cornebise julien kavukcuoglu koray wierstra daan. weight uncertainty neural network. international conference machine learning dziugaite gintare karolina daniel computing nonvacuous generalization bounds deep neural networks many parameters training arxiv preprint arxiv. data. germain pascal bach francis lacoste alexandre lacoste-julien simon. pac-bayesian theory meets bayesian inference. advances neural information processing systems rezende danilo jimenez mohamed shakir wierstra daan. stochastic backpropagation approximate inference deep generative models. international conference machine learning seldin yevgeny laviolette franc¸ois cesa-bianchi nicolo shawe-taylor john auer peter. pac-bayesian inequalities martingales. ieee transactions information theory whye bapst victor czarnecki wojciech marian quan john kirkpatrick james hadsell raia heess nicolas pascanu razvan. distral robust multitask reinforcement learning. arxiv preprint arxiv. section prove theorem proof based steps mcallaster’s classical pac-bayes bound. ﬁrst step bound error caused observing ﬁnite number samples observed tasks. second step bound generalization error observing limited number tasks environment. start restating classical pac-bayes bound using general notations. theorem sample space distribution hypothesis space functions deﬁne ‘loss function’ sequence independent random variables distributed according prior distribution following bound holds uniformly ‘posterior’ distributions first step theorem bound generalization error observed tasks learning done algorithm uses prior samples output distribution hypotheses. index observed task. theorem following substitutions. samples distribution deﬁne ‘tuple hypothesis’f ‘loss function’ regular loss uses element tuple deﬁne ‘prior hypothesis’ distribution ﬁrst sample sample according theorem ‘posterior hypothesis’ distribution particular bound hold following family distributions ﬁrst sample sample kl-divergence term second step next wish bound environment-level generalization theorem again following substitutions. i.i.d. samples distributed according task-distribution ‘hypotheses’ ‘loss function’ bound hold uniformly distributions following holds many pac-bayesian bounds single-task learning appeared literature. section demonstrate proof technique used different single-task bound derive possibly tighter meta-learning bound. section show variational inference method used hierarchical bayesian framework lead learning objective similar obtained using pac-bayesian analysis. material present completeness. bayesian framework assume probabilistic model unknown variables known prior distribution. given observed data infer posterior distribution variables using bayes rule. however obtaining posterior often intractable. variational methods solve problem ﬁnding approximate posterior. case observe data sets tasks composed samples zmi}. common hierarchal bayesian methods assume hierarchical model shared random variable task-speciﬁc random variables known prior distribution given pairs mutually independent. independent given recent works presented possibly tighter pac-bayesian bounds taking account empirical variance specializing bound deep neural networks however leave incorporation bounds future work. resulting learning objective similar meta-learning generalization bound develop work indeed experimental results similar however algorithm derived bound formulated within bayesian framework. sample random mini-batch data approximate empirical loss using averaging monte-carlo draws. evaluate gradient w.r.t using backprop. take optimization step. network architecture used permuted-labels experiment small convolutional-layers ﬁlters kernels hidden linear layer units linear output layer. convolutional layer followed pooling operation kernel size dropout performed output layer. networks activation function. phases mlap algorithm epochs batches samples task. take monte-carlo sample stochastic network output step. optimizer used adam learning rate means weights initialized randomly standard xavier method log-var maml implementation details report best results obtained combinations following representative hyper-parameters gradient steps meta-training gradient steps meta-testing iterations best results maml obtained gradient steps meta-training meta-testing. illustrate setup visually consider simple example estimation problem. task goal estimate mean data generating distribution. setup samples vectors hypothesis class vectors loss function euclidean distance artiﬁcially algorithm complexity terms according theorem seen figure learned prior single-task posteriors understood intuitively. first posteriors located close ground truth means task relatively small uncertainty covariance. second learned prior located middle posteriors covariance larger ﬁrst dimension. intuitively reasonable since prior learned tasks likely values around dimension values around dimension larger variance. thus similar tasks learned using prior fewer samples. figure example orange dots samples task respectively green purple dots means posteriors task respectively. mean prior blue dot. ellipse around distribution’s mean represents covariance matrix.", "year": 2017}