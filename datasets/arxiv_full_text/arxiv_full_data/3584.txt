{"title": "Worst-case Optimal Submodular Extensions for Marginal Estimation", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Submodular extensions of an energy function can be used to efficiently compute approximate marginals via variational inference. The accuracy of the marginals depends crucially on the quality of the submodular extension. To identify the best possible extension, we show an equivalence between the submodular extensions of the energy and the objective functions of linear programming (LP) relaxations for the corresponding MAP estimation problem. This allows us to (i) establish the worst-case optimality of the submodular extension for Potts model used in the literature; (ii) identify the worst-case optimal submodular extension for the more general class of metric labeling; and (iii) efficiently compute the marginals for the widely used dense CRF model with the help of a recently proposed Gaussian filtering method. Using synthetic and real data, we show that our approach provides comparable upper bounds on the log-partition function to those obtained using tree-reweighted message passing (TRW) in cases where the latter is computationally feasible. Importantly, unlike TRW, our approach provides the first practical algorithm to compute an upper bound on the dense CRF model.", "text": "submodular extensions energy function used eﬃciently compute approximate marginals variational inference. accuracy marginals depends crucially quality submodular extension. identify best possible extension show equivalence submodular extensions energy objective functions linear programming relaxations corresponding estimation problem. allows establish worst-case optimality submodular extension potts model used literature; identify worst-case optimal submodular extension general class metric labeling; eﬃciently compute marginals widely used dense model help recently proposed gaussian ﬁltering method. using synthetic real data show approach provides comparable upper bounds log-partition function obtained using tree-reweighted message passing cases latter computationally feasible. importantly unlike approach provides ﬁrst practical algorithm compute upper bound dense model. introduction desirable optimization properties submodular functions widely exploited design approximate estimation algorithms discrete conditional random ﬁelds submodularity also recently used design elegant variational inference algorithm compute marginals discrete minimising upper-bound log-partition function. initial work energy restricted submodular. later work algorithm extended handle general potts energy functions. idea deﬁne large ground subsets represent valid labelings sublabelings even incorrect labelings given large ground possible deﬁne submodular function whose value equal energy subsets specify valid labeling model. refer function submodular extension energy. given energy function exists large number possible submodular extensions. accuracy variational inference algorithm depends crucially choice submodular extension. previous work largely ignored question identifying best extension. indeed diﬃculty identifying submodular extensions general energy functions could major reason experiments restricted special case models speciﬁed potts energy functions. work establish hitherto unknown connection submodular extension potts model proposed zhang objective function accurate linear programming relaxation corresponding estimation problem connection three important practical consequences. first establishes accuracy submodular extension potts model ugc-hardness worst-case optimality relaxation. second provides accurate submodular extension hierarchical potts model relaxation corresponding estimation problem proposed kleinberg tardos since metric accurately approximated mixture hierarchical potts models result also provides computationally feasible algorithm estimating marginals metric labeling. third establishes property implies solved computing value lovasz extension using equation property lovasz extension submodular function convex piecewise linear function. property holds since pointwise maximum linear functions according equation energy functions deﬁned graph random variables related edges wish assign every variable labels quality labeling given energy function deﬁned unary pairwise potentials respectively. computer vision often think arranged grid. sparse deﬁned connected -connected neighbourhood relationships. dense hand every variable connected every variable. energy function also deﬁnes probability distribution follows known partition function. inference types inference problems crfs marginal inference want compute marginal probabilities every inference want labeling minimum energy minx∈ln equivalently inference ﬁnds mode review variational inference using equivalence subgradient relaxation conditional gradient problem minimising upper bound log-partition. allows employ widely used dense since subgradient relaxation eﬃciently computed using recently proposed modiﬁed gaussian ﬁltering algorithm consequence provide ﬁrst eﬃcient algorithm compute upper bound log-partition function dense crfs. provides complementary information popular mean-ﬁeld inference algorithm dense crfs computes lower bound log-partition show quality solution comparable tree reweighted message passing case sparse crfs. unlike approach computationally infeasible dense crfs thereby limiting practice. using dense models perform stereo matching standard data sets obtain better results complete code available https//github.com/pankajpansari/densecrf. preliminaries introduce notation deﬁnitions make remainder paper. submodular functions given ground denote power set. function submodular subsets denotes modular function considered vector. lovasz extension given function value lovasz extension deﬁned follows order components decreasing order corresponding permutation indices. then conditional gradient algorithm conditional gradient algorithm good candidate solving problem reasons. first problem convex. second solving computationally tractable conditional gradient found eﬃciently. algorithm starts initial solution iteration compute conditional gradient minimises linear approximation g+∇gt objective function. finally updated either ﬁxed step size schedule line algorithm line search min≤γ≤ gsk). worst-case optimal submodular extensions diﬀerent choices extensions change domain problem leading diﬀerent upper bounds log-partition function. come extension yields tightest bound? paper focus submodular extension families instance energy function belonging given class gives corresponding submodular extension extension family fopt worst-case optimal. implies exist another submodular extension family gives tighter upper bound problem fopt instances energy function formally note problem diﬀerent taking given energy model obtaining submodular extension optimal model. also seek closedform analytical expression sake clarity figure illustration -of-l encoding used variables labels. blue labeling corresponding valid. yellow labeling corresponding invalid since assigned multiple labels assigned none. submodular extensions submodular extension deﬁned using ground subsets correspond valid labelings. extension need encoding scheme gives sets corresponding valid labelings. example encoding scheme -of-l encoding illustrated ﬁgure variable take possible labels. scheme represent possible assignments val}. assigned label select element vai. extending variables ground becomes a=va. valid assignment assigns variable exactly label denote valid assignments a=ma using ground deﬁne submodular function equals sets corresponding valid labelings encoding call function submodular extension valid labelings. upperbound partition function distribution exp) factorises fully modular. since free parameter obtain good approximate marginals distribution minimising upper-bound. taking logs equivalently write optimisation tightest relaxation describing encoding submodular extension brieﬂy outline relaxation corresponding estimation problem. deﬁne indicator variables equal otherwise. following relaxation tightest known potts model worst-case assuming unique games conjecture true vector variables component corresponding encoding choose -of-l encoding potts model described section encoding scheme potts model above factorised problem rewritten marginal estimation temperature introduce temperature parameter problem divides equivalently divides belonging also since multiply objective leaving problem unchanged. without changing solution transform problem follows worst-case optimal submodular extension connect marginal estimation problem relaxations using following proposition. proposition using -of-l encoding scheme limit problem potts model becomes figure illustration worst-case optimal submodular extension potts model chain graph variables take labels. ﬁgure shows compute extension values analysis follows represent meaning clear context. classes energy functions consider paper potts hierarchical potts families. using relaxations introduce temperature parameter using decrease resulting distribution starts peak sharply around mode. marginal estimation becomes inference since resulting distribution mass mode everywhere else. given solution compute marginals iverson bracket. motivated connection introduce temperature parameter problem transform relaxation limit hope tightest relaxations problems known literature worstcase optimal submodular extensions. answer question aﬃrmative. speciﬁcally following sections show select encoding submodular extension convert problem tightest known relaxations potts hierarchical potts models. importantly prove worst-case optimality extensions thus obtained. figure hierarchical potts model instance illustrating notations meta-labels labels labels level. edge-length decreases level. also distance labels subtree hierarchical potts model tree comprising descendants node given subtree denotes length tree-edge leading upward root denotes leaves call leaves tree labels nodes tree expect root meta-labels. figure illustrates notations context hierarchical potts model. tightest relaxation indicator variables employed relaxation yai. following relaxation tightest known hierarchical potts model worst-case assuming unique games conjecture true figure st-graph specifying worst-case optimal submodular extension potts model variables labels connected other. node variable label elements ground set. nodes labeled ‘variable-label’ hence node represents element solid blue arcs model unary potentials dotted arcs represent pairwise potentials. dotted weight remark appendix). note problem becomes objective function relaxation limit seek obtain worst-case optimal submodular extension making objective since problems equivalent gives worst-case optimal extension problem well. question becomes recover worstcase optimal submodular extension using following propositions answers question. proposition worst-case optimal submodular extension potts model given otts hierarchical potts potts model imposes penalty unequal assignment labels neighbouring variables regardless label dissimilarity. natural approach vary penalty based diﬀerent labels are. hierarchical potts model permits specifying distance labels using tree following properties problem equivalent relaxation corresponding estimtation problem hence becomes objective function relaxation limit limit question becomes recover worst-case optimal fhier. since ﬁnite metric space probabilistically approximated mixture tree metric worst-case optimal submodular extension metric energies obtained using fhier. note fhier reduces otts potts model. considering potts model star-shaped tree edge weights fast conditional gradient term known label compatibility function labels potts model hierarchical potts models examples term mixture gaussian kernels called pixel compatibility function. terms features describe random variable practice similar details relaxation found appendix. encoding variable possible assignment labels meta-labels vam} total number nodes tree except root. ground consistent labeling variable assigns label meta-labels path root label. represent consistent assignments pal} collection elements label metalabels path root label valid labelings assigns variable exactly consistent label. constraint formally written a=ma exactly element elements worst-case optimal submodular extension following proposition connects marginal estimation problem relaxations proposition limit problem hierarchical potts energies becomes coordinates values associated pixel features. algorithm assumes conditional gradient step computed eﬃciently. certainly case dense crfs since computing involves function evaluations submodular extension number variables number labels. evaluation complexity using eﬃcient gaussian ﬁltering algorithm however computation would still clearly impractical computer-vision applications however using equivalence relaxed objectives lovasz extension submodular extensions proposition able compute time. speciﬁcally algorithm ajanthan provides eﬃcient ﬁltering procedure compute subgradient relaxation objective proposition computing subgradient equivalent computing conditional gradient submodular function otts. similar observation made case hierarchical potts model. hence ﬁrst practical algorithm compute upper bound log-partition function dense potts metric energies. experiments using synthetic data show upper-bound compares favorably potts hierarchical potts models. comparison restrict sparse crfs code available scale well dense crfs. also perform stereo matching using dense models compare results mean-ﬁeld-based approach experiments .ghz machine ram. section refer algorithm submod mean ﬁeld data generate lattices size lattice point represents variable taking labels. pairwise relations sparse crfs deﬁned -connected neighbourhoods. unary potentials uniformly sampled range consider potts model hierarchical potts models pairwise distance labels given trees ﬁgure pairwise weights varied range compare results worst-case optimal submodular extension figure hierarchical potts models deﬁning pairwise distance among labels used upper-bound comparison trw. blue nodes meta-labels yellow nodes labels. edges particular level edge weights. sequence weights root level leaf level tree tree yellow node shown clump together leaf nodes tree respectively. figure st-graph specifying alternate submodular extension potts model variables labels connected other. convention used ﬁgure dotted weight alternate extension also used derive extension hierarchical potts model. figure upper-bound comparison using synthetic data. plot shows ratio /|trw bound| averaged unary instances function pairwise weights using worst-case optimal alternate extension potts hierarchical potts models. observe worst-case optimal extension results tighter bounds compared respective alternate extensions also worst-case optimal extension bounds similar range bounds. figure best viewed color. method implementation mean-ﬁeld algorithm dense crfs baseline. algorithm make modiﬁed gaussian ﬁltering implementation dense crfs compute conditional gradient step. step size iteration selected line search. algorithm till iterations since visual quality solution show much improvement beyond point. mean-ﬁeld convergence threshold change kl-divergence. results figure shows example solutions obtained picking label maximum marginal probability variable mean-ﬁeld algorithm. also report time energy values solution methods. though performing estimation energy values give quantitative indication quality solutions. full image pairs average ratio energies solutions method compared mean-ﬁeld avearge time ratio observe algorithm results natural looking stereo matching results lower energy values images. however mean-ﬁeld runs faster method instance. discussion established relation submodular extension potts model relaxation estimation using lovasz extension. allowed identify worst-case optimal submodular extension potts well general metric labeling problems. worth noting might still possible obtain improved submodular extension given problem instance. design computationally feasible algorithm task interesting direction future research. current work focused pairwise graphical models readily applied high-order potentials considering corresponding relaxation objective lovasz extension submodular extension. identiﬁcation extensions popular high-order potentials potts model robust version could improve accuracy important computer vision applications semantic segmentation. method algorithm standard schedule obtain step size iteration algorithm till convergence iterations suﬃce this. experiments repeated randomly generated unaries model weight. used matlab toolbox baseline code optimise tree distributions. varied edge-appearance probability trees range found give tightest upper bound. results plot ratio normalised diﬀerence upper bound values method function pairwise weights. ratios averaged instances unaries. figure shows plots potts hierarchical potts models worst-case optimal alternate extension. optimal extension results tighter upper-bounds alternate extension models. reason this observe representation submodular function using ﬁgure necessitates non-negative. implies values larger worstcase optimal extension ﬁgure compared alternate extension. hence minimisation problem objective function cases domain equation larger optimal extension thereby resulting better minima. figure also indicates algorithm optimal extension provides similar range upper bound thereby providing empirical justiﬁcation method. note upper bound tighter method. makes standard relaxation involves marginal variables nodes well edges. hand method makes relaxation proposed kleinberg tardos involves marginal variables nodes. standard relaxation tighter kleinberg-tardos relaxation hence results better approximation. however scale well neighborhood size thereby prohibiting dense crfs. stereo matching using dense crfs data demonstrate beneﬁt algorithm stereo matching images extracted middlebury stereo matching dataset dense models potts compatibility term gaussian pairwise potentials. unary terms obtained using absolute diﬀerence matching function figure stereo matching using dense crfs potts compatibility gaussian pairwise potentials. compare solution mean-ﬁeld algorithm koltun krahenbuhl observe method gives better-looking solutions lower energy value cost higher computational time. wainwright jaakkola willsky class upper bounds partition function. ieee transactions information theory. zhang djolonga krause higherorder inference multi-class log-supermodular models. iccv.", "year": 2018}