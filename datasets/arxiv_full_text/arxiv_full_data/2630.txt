{"title": "Boosted Generative Models", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We propose a novel approach for using unsupervised boosting to create an ensemble of generative models, where models are trained in sequence to correct earlier mistakes. Our meta-algorithmic framework can leverage any existing base learner that permits likelihood evaluation, including recent deep expressive models. Further, our approach allows the ensemble to include discriminative models trained to distinguish real data from model-generated data. We show theoretical conditions under which incorporating a new model in the ensemble will improve the fit and empirically demonstrate the effectiveness of our black-box boosting algorithms on density estimation, classification, and sample generation on benchmark datasets for a wide range of generative models.", "text": "reweighted version original dataset. practice algorithms based boosting perform extremely well machine learning competitions show similar procedure applied generative models. given initial generative model provides imperfect data distribution construct second model correct error repeat recursively. second model also generative trained reweighted version original training set. meta-algorithm general construct ensembles existing generative model permits likelihood evaluation fully-observed belief networks sum-product networks variational autoencoders. interestingly method also leverage powerful discriminative models. speciﬁcally train binary classiﬁer distinguish true data samples fake ones generated current model provide principled include discriminator ensemble. prior attempt boosting density estimation proposed sum-of-experts formulation approach similar supervised boosting every round boosting derive reweighted additive estimate boosted model density. contrast proposed framework uses multiplicative boosting multiplies ensemble model densities interpreted product-of-experts formulation. provide holistic theoretical algorithmic framework multiplicative boosting contrasting competing additive approaches. unlike prior cases product-of-experts formulations approach black-box empirically test proposed algorithms several generative models simple ones mixture models expressive parameteric models sum-product networks variational autoencoders. propose novel approach using unsupervised boosting create ensemble generative models models trained sequence correct earlier mistakes. metaalgorithmic framework leverage existing base learner permits likelihood evaluation including recent deep expressive models. further approach allows ensemble include discriminative models trained distinguish real data model-generated data. show theoretical conditions incorporating model ensemble improve empirically demonstrate effectiveness black-box boosting algorithms density estimation classiﬁcation sample generation benchmark datasets wide range generative models. variety deep generative models shown promising results tasks spanning computer vision speech recognition natural language processing imitation learning parametric models differ ability perform various forms tractable inference learning algorithms objectives. despite signiﬁcant progress existing generative models cannot complex distributions sufﬁciently high degree accuracy limiting applicability leaving room improvement. paper propose technique ensembling generative models improve overall performance. meta-algorithm inspired boosting technique used supervised learning combine weak classiﬁers individually might perform well given classiﬁcation task powerful ensemble. boosting algorithm attempt learn classiﬁer correct mistakes made reweighting original dataset repeat procedure recursively. conditions weak classiﬁers’ effectiveness procedure drive error zero boosting also thought feature learning algorithm round feature learned training classiﬁer copyright association advancement artiﬁcial intelligence rights reserved. sufﬁcient necessary conditions require expected log-likelihood likelihood respectively current intermediate model better-or-equal combined previous model true distribution compared using density ratios. next consider alternative formulation multiplicative boosting improving model arbitrary data distribution. unnormalized estimate round base model learned using mle. conditions intermediate models reducing kl-divergence every round stated below. dklqt−) dklqt) theorem denote reduction kl-divergence round multiplicative boosting. following conditions hold eqt− sufﬁcient contrast additive boosting conditions compare expectations true distribution expectations model distribution previous round qt−. equality conditions holds corresponds trivial case current intermediate model ignored valid non-degenerate version sufﬁcient inequality guarantees progress towards true data distribution. note intermediate models increase overall capacity ensemble every round. shall demonstrate later models using multiplicative boosting outperform additive counterparts empirically suggesting conditions theorem easier fulﬁll practice. supervised boosting provides algorithmic formalization hypothesis sequence weak learners create single strong learner here propose framework extends boosting unsupervised settings learning generative models. ease presentation distributions respect arbitrary unless otherwise speciﬁed. upper-case symbols denote probability distributions assume admit absolutely continuous densities reference measure analysis naturally extends discrete distributions skip brevity. formally consider following maximum likelihood estimation setting. given data points rd}m sampled i.i.d. unknown distribution provide model class parameterizing distributions represented generative model minimize kullback-liebler divergence respect true distribution practice observe samples hence maximize log-likelihood observed data selecting model class maximum likelihood learning nontrivial; w.r.t. small class whereas large class poses risk overﬁtting absence sufﬁcient data even underﬁtting difﬁculty optimizing non-convex objectives frequently arise latent variable models neural networks etc. boosting intuition greedily increase model capacity learning sequence weak intermediate models ht}t correct mistakes made previous models ensemble. here predeﬁned model class defer algorithms pertaining learning intermediate models next section ﬁrst discuss mechanisms deriving ﬁnal estimate individual density estimates round {ht}t additive boosting additive boosting ﬁnal density estimate arithmetic average intermediate models denote weights assigned intermediate models. weights re-normalized every round gives valid probability density estimate. starting base model express density estimate round boosting recursively necessary condition good intermediate model assigns better-or-equal log-likelihood true distribution opposed model distribution qt−. condition suggests learning algorithms intermediate models discuss next. section design analyze meta-algorithms multiplicative boosting generative models. given base model permits likelihood evaluation provide mechanism boosting model using ensemble generative and/or discriminative models. generative boosting supervised boosting algorithms adaboost typically involve reweighting procedure training weak learners similarly train ensemble generative models unsupervised boosting every subsequent model performs w.r.t reweighted data distribution reweighting coefﬁcient round note coefﬁcients general different model weights appear proposition maximize objective optimally equality holding objective hard optimize practice target distribution becomes easier approximate reduce reweighting coefﬁcient. extreme case reweighted data distribution simply uniform. free lunch however since results slower reduction kl-divergence leading computational-statistical trade-off. practice observe samples true data distribution hence approximate based empirical data distribution deﬁned uniform dataset every subsequent round genbgm learns intermediate model maximizes log-likelihood data sampled reweighted data distribution. discriminative boosting base generative model boosted using discriminative approach well. here intermediate model speciﬁed density ratio obtained binary classiﬁer. consider following setup observe equal number samples drawn i.i.d. true data distribution model distribution previous round deﬁnition convex lower semicontinuous function satisfying f-divergence notable examples include kullback-liebler divergence hellinger distance jenson-shannon divergence among many others. binary classiﬁer discriminative boosting maximizes variational lower bound f-divergence round hence solution used estimate density ratios. density ratios naturally multiplicative boosting framework provide justiﬁcation objectives form learning intermediate models formalized proposition below. proposition given f-divergence denote optimal solution round boosting. then model density boosting round matches true density proof. appendix pseudocode corresponding meta-algorithm discbgm given algorithm every round train binary classiﬁer optimize objective chosen f-divergence. special case negative cross-entropy loss commonly used binary classiﬁcation also lower bound f-divergence. algorithm applicable f-divergence focus crossentropy henceforth streamline discussion. corollary consider cross-entropy objective maximized binary classiﬁer practice classiﬁer limited capacity trained ﬁnite dataset generally bayes optimal. corollary however suggests good classiﬁer provide ‘direction improvement’ similar spirit gradient boosting supervised learning additionally intermediate model distribution obtained using corollary satisﬁes conditions theorem guaranteed improve weights interpreted conﬁdence classiﬁcation estimates akin step size used gradient descent. practice heuristically assign weights intermediate models greedy optimum value weights every round critical point example extreme case uninformative i.e. bayes optimal hybrid boosting intermediate models need exclusively generators discriminators; design boosting ensemble combination generators discriminators. intermediate model chosen generator learn generative model using appropriately reweighting data points. discriminator used implicitly specify intermediate model binary classiﬁcation problem. regularization practice want boosted generative models generalize data outside training regularization bgms imposed primarily ways. first every intermediate model independently regularized incorporating explicit terms learning objective early stopping based validation error heuristics dropout etc. moreover restricting number rounds boosting another effective mechanism regularizing bgms. fewer rounds boosting required intermediate models sufﬁciently expressive. experiments designed demonstrate superiority proposed boosting meta-algorithms wide variety generative models tasks. reference implementation boosting meta-algorithms available https//github.com/ermongroup/bgm. additional implementation details experiments given appendix multiplicative additive boosting common pitfall learning parameteric generative models model misspeciﬁcation respect true underlying data distribution. quantitative qualitative understanding behavior additive multiplicative boosting begin considering synthetic setting density estimation mixture gaussians. density estimation synthetic dataset. true data distribution equi-weighted mixture four gaussians centered symmetrically around origin identity covariance matrix. contours underlying density shown figure observe training samples drawn independently data distribution task learn distribution. test contains samples distribution. repeat process times statistical signiﬁcance. base model mixture gaussians data; contours example instance shown figure compare multiplicative additive boosting rounds. additive boosting extend algorithm proposed rosset segal setting unity line search genbgm intermediate figure multiplicative boosting algorithms genbgm discbgm negative cross-entropy hellinger distance outperform additive boosting correcting model misspeciﬁcation. numbers parenthesis indicate boosting round models mixtures gaussians well. classiﬁers discbgm multi-layer perceptrons hidden layers units relu activations trained maximize f-divergences corresponding negative cross-entropy hellinger distance using adam optimizer test negative log-likelihood estimates listed table qualitatively contour plots estimated densities every boosting round sample instance shown figure multiplicative boosting algorithms outperform additive boosting correcting model misspeciﬁcation. genbgm initially leans towards maximizing coverage whereas versions discbgm relatively conservative assigning high densities data points away modes. heuristic model weighting strategies. multiplicative boosting algorithms require hyperparameters number rounds boosting weights assigned intermediate models. practical setting hyperparameters speciﬁc dataset task consideration based cross-validation. automatically setting model weights important direction future work propose heuristic weighting strategies. speciﬁcally unity heuristic assigns weight every model ensemble uniform heuristic assigns weight every model decay heuristic assigns weight model ensemble. figure observe performance algorithms sensitive weighting strategies. particular discbgm produces worse estimates increases uniform strategy. performance genbgm also degrades slightly increasing unity strategy. notably decay strategy achieves stable performance algorithms. intuitively heuristic follows rationale reducing step size gradient based stochastic optimization algorithms expect strategy work better even settings. however strategy could potentially result slower convergence opposed unity strategy. density estimation benchmark datasets. evaluate performance additive multiplicative boosting density estimation real-world benchmark datasets consider generative model families mixture bernoullis sum-product networks results multiplicative boosting sum-product networks competitive state-of-the-art goal experiments perform robust comparison boosting algorithms well demonstrate applicability various model families. table experimental results density estimation. negative log-likelihoods reported nats. lower better best performing models bold. overall multiplicative boosting outperforms additive boosting baseline models speciﬁed mixture bernoullis product networks table experimental results classiﬁcation. prediction accuracy predicting variable given rest. higher better best performing models bold. multiplicative boosting outperforms additive boosting baseline models speciﬁed mixture bernoullis product networks since discbgm requires samples model density every round ensure computational fairness samples obtained efﬁciently base model sidestepping running expensive markov chains. model weights chosen based cross-validation. results density estimation reported table since multiplicative boosting estimates unnormalized importance sampling estimate partition function. base model model underperforms often worse even baseline model best performing validated non-zero model weights. genbgm consistently outperforms improves baseline model cases discbgm performs best convincingly outperforms baseline genbgm datasets. results spns boosted models outperform baseline. genbgm edges models whereas discbgm models outperform models datasets. results demonstrate usefulness boosted expressive model families especially discbgm approach performs best genbgm preferable add. applications generative models classiﬁcation. here evaluate performance boosting algorithms classiﬁcation. since datasets explicit labels choose dimensions label letting denote remaining dimensions obtain prediction efﬁcient compute even unnormalized models. repeat procedure variables predicting variable time using values assigned remaining variables. results reported table base model observe approach could often worse base model whereas genbgm performs slightly better baseline discbgm approach consistently performs well outperformed genbgm datasets mob. spns used instead genbgm improve upon baseline model discbgm best performing model dataset. sample generation. compare boosting algorithms based ability generate image samples binarized mnist dataset handwritten digits variational autoencoders base model sufﬁciently expressive generate impressive examples design experiment evaluate model complexity approximated number learnable parameters. ancestral samples obtained baseline model shown figure evidence lower bound proxy approximately evaluating marginal log-likelihood learning. conventional approach improving performance latent variable model increase representational capacity adding hidden layers increasing number hidden units existing layers lead marginal improvement sample quality seen figure figure contrast boosting makes steady improvements sample quality. start much fewer parameters generate samples using hybrid boosting gendiscbgm sequence vae→cnn→vae discriminator used convolutional neural network trained maximize negative cross-entropy. generate samples using independent figure boosted model demonstrates ensembles weak learners generate sharper samples compared naively increasing model capacity note show samples binarized digits mean values pixels. hidden layer architecture given parenthesis. work revisited boosting class metaalgorithms developed response seminal question weak learners create single strong learner? boosting offered interesting theoretical insights fundamental limits supervised learning development algorithms work well practice work provides foundational framework unsupervised boosting connections prior work discussed below. sum-of-experts. rosset segal proposed algorithm density estimation using bayesian networks similar gradient boosting. models normalized easy sample generally outperformed multiplicative formulations correcting model misspeciﬁcation show work. similar additive approaches used improving approximate posteriors speciﬁc algorithms variational inference generative adversarial networks survey variations additive ensembling unsupervised settings refer survey bourel ghattas product-of-experts. multiplicative boosting formulation interpreted product-of-experts approach initially proposed feature learning energy based models boltzmann machines. example hidden units restricted boltzmann machine interpreted weak learners performing mle. number weak learners ﬁxed efﬁciently updated parallel risk learning redundant features weak learners also added incrementally based learner’s ability distinguish observed data model-generated data generalized latter boost arbitrary probabilistic models; algorithm special case discbgm discriminator boosted classiﬁer. discbgm additionally accounts imperfections learning classiﬁers ﬂexible model weights. further include classiﬁer trained maximize f-divergence. related techniques noise-contrastive estimation ratio matching score matching methods cast minimization bregman divergences akin discbgm unit model weights non-parametric algorithm similar genbgm proposed marzio taylor ensemble weighted kernel density estimates learned approximate data distribution. contrast framework allows parametric non-parametric learners uses different scheme reweighting data points proposed work. unsupervised-as-supervised learning. density ratios learned binary classiﬁer estimation ﬁrst proposed friedman hastie tibshirani subsequently applied elsewhere notably parameter estimation using noise-contrastive estimation sample generation generative adversarial networks gans consist discriminator distinguishing real data model generated data similar discbgm suitable f-divergence differ learning objective generator generator performs adversarial minimization objective discriminator maximizes whereas discbgm uses likelihood estimate base generator density ratios derived discriminator estimate model density ensemble. limitations future work. multiplicative boosting framework model density needs speciﬁed normalization constant given round boosting. additionally many applications generative modeling feature learning classiﬁcation sidestep computing partition function needed estimated using techniques annealed importance sampling similarly markov chain monte carlo methods used generate samples. lack implicit normalization however limiting applications requiring fast log-likelihood evaluation sampling. order sidestep issue promising direction future work consider boosting normalizing models models specify invertible multiplicative transformation distribution another using change-of-variables formula resulting distribution self-normalized efﬁcient ancestral sampling possible. genbgm algorithm adapted normalizing models whereby every transformation interpreted weak learner. parameters every transformation trained greedily suitable reweighting resulting self-normalized boosted generative model. presented general-purpose framework boosting generative models explicit factorization model likelihood product simpler intermediate model densities. intermediate models learned greedily using discriminative generative approaches gradually increasing overall model’s capacity. demonstrated effectiveness models baseline models additive boosting tasks density estimation classiﬁcation sample generation. extensions semi-supervised learning structured prediction exciting directions future work. thankful neal jean daniel levy russell stewart helpful critique. research supported microsoft research fellowship machine learning ﬁrst author grants future life institute grant intel.", "year": 2017}