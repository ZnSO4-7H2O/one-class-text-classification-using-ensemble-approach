{"title": "A Non-Parametric Bayesian Method for Inferring Hidden Causes", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We present a non-parametric Bayesian approach to structure learning with hidden causes. Previous Bayesian treatments of this problem define a prior over the number of hidden causes and use algorithms such as reversible jump Markov chain Monte Carlo to move between solutions. In contrast, we assume that the number of hidden causes is unbounded, but only a finite number influence observable variables. This makes it possible to use a Gibbs sampler to approximate the distribution over causal structures. We evaluate the performance of both approaches in discovering hidden causes in simulated data, and use our non-parametric approach to discover hidden causes in a real medical dataset.", "text": "present non-parametric bayesian approach structure learning hidden causes. previous bayesian treatments problem deﬁne prior number hidden causes algorithms reversible jump markov chain monte carlo move solutions. contrast assume number hidden causes unbounded ﬁnite number inﬂuence observable variables. makes possible gibbs sampler approximate distribution causal structures. evaluate performance approaches discovering hidden causes simulated data non-parametric approach discover hidden causes real medical dataset. variety methods bayesian statistics applied problem learning dependencies among observed variables however many settings dependencies interest exist among observed variables produced hidden causes. example medicine symptoms patients explained result diseases themselves directly observable assumption embodied graphical models medical diagnosis qmr-dt consider bayesian methods used infer existence hidden causes inﬂuence observed variables. medical example would mean discovering diseases symptoms patients. learning structure graphical models containing hidden causes presents signiﬁcant challenge since number hidden causes unknown potentially unbounded. researchers explored several approaches problem. approach uses statistical criteria identify hidden causes might present algorithms effective reﬂect developing bayesian approach solving problem. closely related work deﬁnes prior number hidden causes uses reversible jump markov chain monte carlo algorithms move structures diﬀerent numbers hidden causes methods satisfy desire bayesian solution designing wellmixing rjmcmc algorithms diﬃcult. previous bayesian approaches inferring hidden causal structure assume number hidden causes ﬁnite. many cases accurate assume instead number hidden causes inﬁnite. rather seeking determine number hidden causes instead seek count ﬁnite subset hidden causes manifest particular ﬁnite dataset. perspective dimensionality models common nonparametric bayesian statistics. example dirichlet process mixture models assume data come potentially inﬁnite number clusters ﬁnite subset observed however non-parametric bayesian methods previously applied problem learning causal structure data. paper develop non-parametric bayesian approach structure learning unbounded number hidden causes. speciﬁcally deﬁne prior causal structures using indian buﬀet process distribution inﬁnite binary matrices. using properties derive gibbs sampling algorithm used sample posterior distribution causal structures. compare approach standard rjmcmc methods infer hidden causes behind symptoms stroke patients. figure hypothetical bayesian network connecting hidden causes observed variables consider case number hidden causes unbounded. state hidden causes observed variables dependencies summarized using binary matrices respectively. assume observed instances binary variables. using denote value observed variable trial summarize observations binary matrix standard structure learning task would learn bayesian network representation dependencies among variables dependencies expressed using adjacency matrix directed graph nodes correspond variables edge exists node node otherwise. extend problem include hidden causes assume binary variables never observed. using denote value hidden cause trial summarize state hidden causes binary matrix edge exists node node otherwise represent dependencies hidden causes observable variables binary matrix full adjacency matrix bayesian network deﬁned variables hidden causes trial values observed variables deﬁne generative model specifying distribution compute posterior distribution given using bayes’ rule column ziyt zikykt. baseline probability probability hidden causes eﬀective. model makes sense applications many causes elicit eﬀect likelihood observing eﬀect increased number hidden causes active. medical diagnosis application consider later paper description well. focus paper learning bayesian methods learning easily combined methods described infer whole. problem then reduces learning structure bipartite graph product ranges values values model makes general assumption baseline prevalence hidden causes roughly same. assumption appropriate applications relaxed necessary. specifying distribution goal generate matrices allow multiple hidden causes aﬀect observed variable. characteristic desirable many settings exempliﬁed case reversible jump mcmc variant metropolishastings algorithm allows moves models diﬀerent dimensionality central idea augment sampler ﬁnite model dimension-shifting move. case standard birth/death proposal would following form pick single hidden cause check number incident edges remove cause decrement cause links generate values correspond according prior. letting denote values move accepted probability proposed value current value probability proposing given making dependency ﬁnite model explicit deﬁning prior probability factorize known probabilities using proposal hidden cause added probability k+/k. empty column added corresponding generated sampling according eqn. probability proposing conﬁguration thus yktpt index column. return previous conﬁguration delete hidden cause value proposed probability choosing number rows identical proposed row. consequently ratio proposal probabilities ratio probabilities resulting conﬁgurations needs take account diﬀerence probability probability yktpt diﬀerent probabilities without column diﬀerent probabilities gives ratio medical diagnosis multiple diseases cause symptom. simple process generating would assume hidden cause associated parameter sample values bernoulli distribution ranging make assumption generated beta distribution integrate probability non-parametric bayesian statistics common deﬁne models unbounded dimensionality taking inﬁnite limit models ﬁnite dimensionality spirit consider happens model deﬁned distribution remains well-deﬁned values need concerned rows correspond columns thus need consider happens eqn. attend columns deﬁne scheme ordering columns obtain distribution distribution shown result indian buﬀet process deﬁned terms sequence customers entering restaurant choosing inﬁnite array dishes ﬁrst customer tries ﬁrst poisson dishes remaining customers enter pick previously sampled dishes probability m−ik m−ik number customers already chosen dish. trying shared dishes customer also tries next poisson dishes. distribution results process exchangeable probability binary matrix unaﬀected order customers. m−ik essentially ﬁnite model given eqn. m−ik case m−ik requires careful treatment. non-parametric approach matrix inﬁnitely many columns practice non-zero columns matrix held memory still need sample columns. number columns contain complete speciﬁcation algorithm need scheme sampling gibbs sampling drawing component matrices distributions z−ik values except y−kt values matrix except ykt. binary probabilities computed enumeration. generative model bayes’ rule ·ykt last step makes binomial theorem. gives need compute conditional distribution deﬁned eqn. theory sampling distribution would require evaluating possible values longer require dimensionjumping moves simply gibbs sampler infer diﬀerence gibbs component rjmcmc algorithm outlined scheme sampling zik. exploiting exchangeability ibp. since ordering customers results distribution assume customer last enter restaurant. accordingly sample dish previously tasted m−ik customers probability m−ik number dishes m−ik thus need consider cases sampling gibbs sampler case m−ik case m−ik figure learning number hidden causes using rjmcmc gibbs sampling. line show mean standard deviation expected value dimensionality model taken iterations sampling datasets. dimensionality initialized result poor mixing proposals hidden causes often accepted values associated causes typically inconsistent structure consequently causes obtain links observable nodes. causes thus quickly deleted. cause might generated appropriate values given sufﬁciently many sampling iterations short like used here slow mixing results strong inﬂuence initialization. second evaluation compared ability algorithms recover speciﬁc structures. manually speciﬁed four matrices generated datasets using procedure outlined rjmcmc gibbs initialized empty matrix used measures evaluate performance. first in-degree error deﬁne diﬀerence true in-degree expected in-degree observed nodes computed samples. computed taking absolute diﬀerence diag in-degree observable nodes diag) expected in-degree computed samples. second structure error deﬁne absolute diﬀerence upper triangular portion evaluated rjmcmc algorithm ﬁnite model gibbs sampler inﬁnite model tasks using simulated data. first examined ability algorithms recover true number hidden causes used generate dataset. data generated ﬁxing number observations varying number hidden causes value diﬀerent datasets generated using rejection sampling draw matrix appropriate dimensionality drawing according eqn. drawing according eqn. rjmcmc gibbs initialized either empty matrix random matrices iterations dataset. model parameters ﬁxed results shown fig. gibbs sampler slightly over-estimates number hidden causes generally produces results close true dimensionality regardless initialization. contrast rjmcmc appears aﬀected initialization. particular systematically under-estimates true figure recovering causal structure rjmcmc gibbs sampling. left right columns results degree bipartite graph disconnected graph undercomplete random graph fewer causes observations overcomplete random graph causes observations shows true structures second shows mean runtime wall clock seconds function number iterations sampling. third fourth rows show in-degree error structure error algorithm deﬁned main text axis number iterations. error bars symmetric indicate standard deviation datasets. results shown fig. gibbs sampler consistently recovers structure close truth surprisingly iterations. reﬂects tendency move quickly good solution minimally explore space around solution. variance results grow slightly iterations reﬂecting greater exploration space structures. contrast rjmcmc performs poorly largest number iterations. reﬂection fact mixes slowly taking long time increase dimension model. noted poor performance overcomplete graph much problem algorithms indication unavoidable problem identiﬁability overcomplete models. instance particular graph information short prior used distinguish causal nodes hypothetical single combined node. used subset mount sinai stroke data bank illustrate approach inferring hidden causes real data. data bank consists stroke signs exhibited patients admitted acute stroke unit mount sinai hospital together lesion localization evaluations made neurologist special stroke expertise language preceding sections signs observed variables localizations hidden causes. data bank consisted signs localizations. signs left-right variables graded degrees severity. patient signs binarized steps. first graded signs like decreased level consciousness comprehension deﬁcit severity assigned level indicated indication made. second sided signs like visual ﬁeld deﬁcit abnormal deep tendon reﬂex created variables left right assigned varifigure trace plots histograms gibbs sampler applied signs exhibited stroke patients. left column shows current value sampler progressed obtained examining current sample. right column shows histograms variables computed samples. encouraging sign note data bank contained bilateral stroke suﬀerers recovered graph reﬂects correctly separating signs caused infarcts hemisphere. paper developed demonstrated nonparametric bayesian technique simultaneously inferring existence connectivity hidden causes. algorithm correctly recovers number hidden causes inﬂuence observed variables used obtain reasonably good estimates causal structure underlying domain. approach provides promising foundation development bayesian models used learn structure relating observed variables hidden causes inﬂuence variables. particular algorithm easily integrated existing mcmc methods bayesian structure learning results suggest number future directions. first focus case shared hidden causes variations algorithm describe could applied case hyperparameters vary across causes extending applicability model. second slow mixing exhibited rjmcmc requires able corresponding side sign observed. resulting sign variables expressed least patient. mean number signs patient mean number stroke localizations every localization found least patient although localizations found patient. although ground truth localizations known inferred localizations causal relationships signs directly signs exhibited patients using gibbs sampler. addition placed beta prior gamma prior sampled well using metropolis updates gibbs hyperparameters interpretations measures incidence rate localizations rate spontaneous sign expression measure reliably localization gives rise sign together measure number hidden localizations trace plots hyperparameters iterations gibbs sampling appear fig. interpretation results considered caveat datapoints data bank data stroke-speciﬁc. posterior distribution favored values suggesting prevalence signs absence particular stroke localization patients low. values reasonably high reﬂects fact localizations responsible producing particular sign produce sign high probability. posterior distributions favored values slightly higher lower ground truth respectively parameters expected coupled since inﬂuence overall prevalence signs. under-estimate unexpected paucity data many localizations. fact nine localizations exhibited least patients providing closer correspondence values favored sampler. causal structure highest posterior probability samples shown fig. attempt interpret hidden causes examining signs connected. showed clusters signs corresponding hidden causes found algorithm clinical neurologist familiar domain concluded localizations somewhat general inappropriately confounded. observations came caveat loss degree information abridgement typical clinical sign localization domain made precise localizations diﬃcult. agraphsideright agraphsideleft twopointsideright twopointsideleft viblosssideright viblosssideleft poslosssideright poslosssideleft tempsideright tempsideleft touchsideright touchsideleft ppsideright ppsideleft dsssideright dsssideleft sensedef gaittype babssideright babssideleft abndtrssideright abndtrssideleft decramsideright decramsideleft ataxiatype weaknesstyperight weaknesstypeleft dysarthriaseverity tongweakside gagseverity swallowseverity facialsidetyperight facialsidetypeleft facenumbsideright facenumbsideleft ptosisside abneomtypehgazeright abneomtypehgazeleft prdside abnpupilssidetype nystagmustype poorokndirectionright poorokndirectionleft vfdeficitsidetyperight vfdeficitsidetypeleft cogabn anomiaseverity repetitionseverity nonfluencyseverity compdefseverity denial hemineglectsideleft hemineglectsideright dyspraxia disorienteddegree declocdegree figure causal structure highest posterior probability. grouping signs highlighted. solid black grouping poor optokinetic nystagmus lack facial control weakness decreased rapid alternating movements abnormal deep tendon reﬂexes babinski sign double simultaneous stimulation neglect left side consistent right frontal/parietal infarct. dashed black grouping comprehension deﬁcit non-ﬂuency repetition anomia visual ﬁeld deﬁcit facial weakness general weakness latter three right side generally consistent part left temporal infarct. investigation. birth/death proposals kind used common possible develop faster-mixing proposal drawing inspiration gibbs sampler non-parametric model adding deleting nodes single link attached. non-parametric bayesian methods explored paper make possible learn bayesian networks inﬁnitely many nodes. might seem intractable ﬁrst glance assuming number nodes unbounded actually removes formal problems involved inferring hidden causes leads simple algorithm broad applicability. work supported nih-ninds part nsf/nih collaborative research computational neuroscience program. thank leigh hochberg graciously sharing expertise michael black suggestions stanley tuhrim sinai hospital data permissions.", "year": 2012}