{"title": "AIDE: An algorithm for measuring the accuracy of probabilistic inference  algorithms", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Approximate probabilistic inference algorithms are central to many fields. Examples include sequential Monte Carlo inference in robotics, variational inference in machine learning, and Markov chain Monte Carlo inference in statistics. A key problem faced by practitioners is measuring the accuracy of an approximate inference algorithm on a specific data set. This paper introduces the auxiliary inference divergence estimator (AIDE), an algorithm for measuring the accuracy of approximate inference algorithms. AIDE is based on the observation that inference algorithms can be treated as probabilistic models and the random variables used within the inference algorithm can be viewed as auxiliary variables. This view leads to a new estimator for the symmetric KL divergence between the approximating distributions of two inference algorithms. The paper illustrates application of AIDE to algorithms for inference in regression, hidden Markov, and Dirichlet process mixture models. The experiments show that AIDE captures the qualitative behavior of a broad class of inference algorithms and can detect failure modes of inference algorithms that are missed by standard heuristics.", "text": "approximate probabilistic inference algorithms central many ﬁelds. examples include sequential monte carlo inference robotics variational inference machine learning markov chain monte carlo inference statistics. problem faced practitioners measuring accuracy approximate inference algorithm speciﬁc data set. paper introduces auxiliary inference divergence estimator algorithm measuring accuracy approximate inference algorithms. aide based observation inference algorithms treated probabilistic models random variables used within inference algorithm viewed auxiliary variables. view leads estimator symmetric divergence approximating distributions inference algorithms. paper illustrates application aide algorithms inference regression hidden markov dirichlet process mixture models. experiments show aide captures qualitative behavior broad class inference algorithms detect failure modes inference algorithms missed standard heuristics. approximate probabilistic inference algorithms central diverse disciplines including statistics robotics machine learning artiﬁcial intelligence. popular approaches approximate inference include sequential monte carlo variational inference markov chain monte carlo. problem faced practitioners measuring accuracy approximate inference algorithm speciﬁc data set. accuracy inﬂuenced complex interactions speciﬁc data question model family algorithm tuning parameters number iterations associated proposal distributions and/or approximating variational family. unfortunately practitioners assessing accuracy inference rely heuristics either brittle specialized type algorithm both. example marginal likelihood estimates used assess accuracy sequential monte carlo variational inference estimates fail signiﬁcantly penalize algorithm missing posterior mode. expectations probe functions assess full approximating distribution require design speciﬁc model. paper introduces algorithm estimating symmetrized divergence output distributions broad class exact approximate inference algorithms. idea inference algorithms treated probabilistic models random variables used within inference algorithm viewed latent variables. show sequential monte carlo markov chain monte carlo rejection sampling variational inference represented common mathematical formalism based concepts generative inference models meta-inference algorithms. using framework introduce auxiliary inference divergence estimator estimates symmetrized divergence output distributions inference algorithms endowed meta-inference algorithm. also figure using aide estimate accuracy target inference algorithm relative goldstandard inference algorithm. aide monte carlo estimator symmetrized kullback-leibler divergence output distributions inference algorithms. aide uses metainference inference internal random choices made inference algorithm. figure aide applies variational mcmc algorithms. left aide estimates converge zero expected. right aide estimates variational inference converge nonzero asymptote depends variational family. middle symmetrized divergence posterior converges zero aide over-estimates divergence expectation. although increasing number meta-inference runs reduces bias aide aide practical measuring accuracy inaccurate meta-inference show conditional update andrieu reverse markov chain grosse special cases ‘generalized conditional update’ canonical meta-inference algorithm smc. aide practical tool measuring accuracy variational inference algorithms relative gold-standard inference algorithms. note paper provide practical solution mcmc convergence diagnosis problem. although principle aide applied mcmc practice require accurate meta-inference algorithms mcmc developed. marginal likelihood ‘evidence’. sampling-based approximate inference strategies including markov chain monte carlo sequential monte carlo annealed importance sampling importance sampling resampling generate samples latent variables approximately distributed according sampling-based inference algorithm often motivated theoretical guarantees exact convergence posterior limit inﬁnite computation however well sampling distribution approximates posterior distribution ﬁnite computation typically difﬁcult analyze theoretically estimate empirically conﬁdence. variational inference explicitly minimizes approximation error approximating distribution parameters variational family. error usually quantiﬁed using kullback-leibler divergence approximation posterior denoted unlike sampling-based approaches variational inference generally give exact results inﬁnite computation variational family include posterior. minimizing divergence performed maximizing ‘evidence lower bound’ since usually unknown actual error variational approximation also unknown. section deﬁnes mathematical formalism analyzing inference algorithms; shows represent mcmc rejection sampling variational inference formalism; introduces auxiliary inference divergence estimator algorithm estimating symmetrized divergence inference algorithms. deﬁne inference algorithm procedure produces single approximate posterior sample. repeated runs algorithm give independent samples. inference algorithm ‘output density’ represents probability algorithm returns given sample given algorithm. note depends observations deﬁne inference problem suppress notation. inference algorithm accurate denote sample produced running algorithm naive simple monte carlo estimator divergence output distributions inference algorithms requires output densities algorithms. however typically intractable compute output densities sampling-based inference algorithms like mcmc would require marginalizing possible values random variables drawn algorithm could possibly take. similar difﬁculty arises computing marginal likelihood generative probabilistic model suggests treat inference algorithm probabilistic model estimate output density using ideas marginal likelihood estimation estimates monte carlo estimator divergence. begin making analogy inference algorithm probabilistic model explicit deﬁnition generative inference model tuple joint density deﬁned generative inference model models inference algomodel element represents complete assignment internal random variables within inference algorithm called ‘trace’. ability simulate required ability compute density not. simulation denoted obtained running inference algorithm recording resulting trace output generative inference model understood generative probabilistic model latent variables observations. note different generative inference models different representations internal random variables inference algorithm. practice constructing generative inference model inference algorithm amounts deﬁning internal random variables. marginal likelihood estimation generative inference model ‘meta-inference’ algorithm deﬁnition given generative inference model meta-inference algorithm tuple density traces inference algorithm indexed outputs inference algorithm following require ability sample given value ability evaluate given call procedure sampling ‘meta-inference sampler’. require ability evaluate density meta-inference algorithm considered accurate given conceptually meta-inference sampler tries answer question ‘how could inference algorithm produced output note tractable evaluate marginal likelihood generative inference model normalizing constant necessary represent internal random variables inference algorithm generative inference model deﬁne trace empty token case meta-inference algorithm show construct generative inference models corresponding meta-inference algorithms mcmc rejection sampling variational inference. metainference algorithms mcmc derived special cases generic meta-inference algorithm. sequential monte carlo. consider general class samplers introduced moral used approximate inference sequential state space nonsequential models. brieﬂy summarize slightly restricted variant algorithm here refer reader supplement moral full details. algorithm propagates weighted particles steps using proposal kernels multinomial resampling based weight functions deﬁned terms ‘backwards kernels’ denote value unnormalized weight normalized weight particle time respectively. deﬁne output sample single draw particle approximation ﬁnal time step obtained sampling particle index categorical setting generative inference model uses traces form contains values particles time steps contains index parent particle particle time step algorithm deﬁnes canonical meta-inference sampler generative inference model takes input latent sample generates trace output. meta-inference sampler ﬁrst generates ancestral trajectory particles terminates output sample sampling sequentially backward kernels starting next runs conditional update conditioned ancestral trajectory. choice annealed importance sampling. single particle used forward kernel satisﬁes detailed balance intermediate density algorithm simpliﬁes annealed importance sampling canonical meta-inference inference consists running forward kernels reverse order reverse annealing algorithm grosse canonical meta-inference algorithm accurate markov chain kept close equilibrium times. achieved intermediate densities form sufﬁciently ﬁne-grained sequence. supplement analysis. markov chain monte carlo. deﬁne mcmc algorithm producing single output sample iterate markov chain produced predetermined number burnsteps passed. also assume mcmc transition operator satisﬁes detailed balance respect posterior then formally special case ais. however unless markov chain initialized near posterior chain equilibrium burn-in period meta-inference algorithm inaccurate. importance sampling resampling. importance sampling resampling seen special case number steps trace output particle index given output algorithm particles sample canonical meta-inference sampler simply samples uniform sets rejection sampling. model rejection sampler posterior distribution assume tractable evaluate unnormalized posterior density deﬁne described section meta-inference deﬁne necessary represent internal random variables rejection sampler. variational inference. suppose variational approximation computed optimization variational parameters assume possible sample variational approximation evaluate normalized density. then note formulation also applies amortized variational inference algorithms reuse parameters inference across different observation contexts consider probabilistic model observations inference algorithms approximate inference algorithms considered ‘gold-standard’ generative inference model meta-inference algorithm second algorithm considered ‘target’ algorithm generative inference model meta-inference algorithm section shows estimate upper bound symmetrized divergence take monte carlo approach. simple monte carlo applied equation requires evaluated would prevent estimator used either inference algorithm sampling-based. algorithm gives auxiliary inference divergence estimator target inference model meta-inference algorithm number runs gold-standard algorithm number runs meta-inference sampler gold-standard number runs target algorithm number runs meta-inference sampler target generic aide algorithm deﬁned terms abstract generative inference models meta-inference algorithms. concreteness supplement contains aide algorithm specialized case gold-standard target variational approximation. theorem estimate produced aide upper bound symmetrized divergence expectation expectation nonincreasing aide parameters supplement proof. brieﬂy aide estimates upper bound symmetrized divergence expectation uses unbiased estimates unbiased aide over-estimates estimates true symmetrized divergence note expression involves divergences meta-inference sampling densities posteriors respective generative inference models qt). therefore approximation error meta-inference determines bias aide. meta-inference algorithms exact aide unbiased. increased bias decreases generative inference model algorithms trace algorithm contribute divergence term bias equation analysis aide equivalent grosse target algorithm gold-standard inference algorithm rejection sampler. diagnosing convergence approximate inference long-standing problem. existing work either tailored speciﬁc inference algorithms designed detect lack exact convergence both. estimators non-asymptotic approximation error general approximate inference figure aide detects inference algorithm misses posterior mode. left bimodal posterior density kernel estimates output densities importance sampling resampling using proposals. ‘broad’ proposal covers modes ‘offset’ proposal misses mode. middle aide detects missing mode offset-proposal sir. right marginal likelihood estimates suggest offset-proposal nearly converged. algorithms received less attention. gorham mackey propose approach applies arbitrary sampling algorithms relies special properties posterior density log-concavity. approach rely special properties posterior distribution. work closely related bounding divergences reverse annealing also estimates upper bounds symmetric divergence output distribution sampling algorithm posterior distribution. aide differs bread ways first whereas bread handles single-particle samplers annealed importance sampling aide handles substantially broader family inference algorithms including samplers resampling rejuvenation steps variational inference rejection samplers. second bread estimates divergences target algorithm’s sampling distribution posterior distribution exact posterior samples necessary bread’s theoretical properties readily available observations deﬁne inference problem simulated generative model. instead aide estimates divergences exact approximate gold-standard sampler real inference problems. unlike bread aide used evaluate inference generative undirected models. aide estimates error sampling-based inference using mathematical framework roots variational inference. several recent works treated sampling-based inference algorithms variational approximations. monte carlo objective formalism maddison closely related formalism generative inference models meta-inference algorithms— indeed generative inference model meta-inference algorithm give deﬁned eux∼q/ξ)] denotes observed data. independent concurrent work naesseth maddison treat variational approximation using constructions similar ours. earlier work salimans recognized mcmc samplers treated variational approximations. however works concerned optimization variational objective functions instead estimation divergences involve generating trace sampler output. used bayesian linear regression inference problem exact posterior sampling tractable characterize bias aide applied three different types target inference algorithms sequential monte carlo metropolis-hastings variational inference. goldstandard algorithm used posterior sampler tractable output density introduce bias aide aide’s bias could completely attributed approximation error meta-inference target algorithm. figure shows results. bias aide acceptable aide unbiased variational inference better meta-inference algorithms mcmc needed make aide practical estimating accuracy applied aide measure approximation error algorithms posterior inference hidden markov model exact posterior inference tractable dynamic programming used opportunity compare aide estimates obtained using exact posterior gold-standard aide estimates obtained using ‘best-in-class’ algorithm gold-standard. figure shows results indicate aide estimates using approximate gold-standard algorithm nearly identical aide estimates obtained exact posterior gold-standard. figure comparing exact posterior gold-standard ‘best-in-class’ approximate algorithm gold-standard measuring accuracy target inference algorithms aide. consider inference exact posterior sampling tractable using dynamic programming. left ground truth latent states posterior marginals marginals output gold-standard three target algorithms particular observation sequence. right aide estimates using exact gold-standard using gold-standard nearly identical. estimated divergence bounds decrease number particles target sampler increases. optimal proposal outperforms prior proposal. increasing tightens estimated divergence bounds. used figure contrasting aide heuristic convergence diagnostic evaluating accuracy approximate inference dirichlet process mixture model heuristic compares expected number clusters target algorithm expectation gold-standard algorithm white circles identify single-particle likelihood-weighting samples prior. aide clearly indicates single-particle likelihood-weighting inaccurate heuristic suggests accurate. probe functions like expected number clusters error prone measures convergence track convergence along speciﬁc projection distribution. contrast aide estimates joint divergence. shaded areas plots show standard error. amount target inference computation used techniques although aide performs gold-standard meta-inference target inference run. feature aide applies different types inference algorithms. compared aide existing techniques evaluating accuracy inference algorithms share feature comparing marginal likelihood estimates made target algorithm estimates made gold-standard algorithm comparing expectation probe function approximating distribution expectation gold-standard distribution figure shows comparison aide inference problem posterior bimodal. figure shows comparison aide ‘number clusters’ probe function dirichlet process mixture model inference problem synthetic data set. also used aide evaluate accuracy several algorithms dpmm inference real data galaxy velocities relative gold-standard. experiment described supplement space constraints. aide makes practical estimate bounds error broad class approximate inference algorithms including sequential monte carlo annealed importance sampling sampling importance resampling variational inference. aide’s reliance gold-standard inference algorithm raises questions merit discussion already acceptable gold-standard would want evaluate inference algorithms? gold-standard algorithms long mcmc runs runs hundreds thousands particles runs annealing schedule often slow production. aide make possible gold-standard algorithms ofﬂine design evaluation phase quantitatively answer questions like particles rejuvenation steps samples away with? fast variational approximation good enough?. aide thus help practitioners conﬁdently apply monte carlo techniques challenging performance constrained applications probabilistic robotics web-scale machine learning. future work think valuable build probabilistic models aide estimates conditioned features data learn ofﬂine problem instances easy hard different inference algorithms. help practitioners bridge ofﬂine evaluation production rigorously. ensure gold-standard accurate enough comparison meaningful? intrinsically hard problem—we sure near-exact posterior inference really feasible interesting classes models. practice think gold-standard inference algorithms calibrated based subjective assumptions heuristic testing—much like models tested. example users could initially build conﬁdence gold-standard algorithm estimating symmetric divergence posterior simulated data sets aide trusted gold-standard focused evaluation target algorithms real data sets interest. think subjectivity gold-standard assumption unique limitation aide. limitation aide bias depends accuracy meta-inference i.e. inference auxiliary random variables used inference algorithm. currently lack accurate meta-inference algorithm mcmc samplers employ annealing therefore aide suitable general mcmc convergence diagnostic. research meta-inference algorithms mcmc comparisons standard convergence diagnostics needed. areas future work include understanding accuracy meta-inference depends parameters inference algorithm generally makes inference algorithm amenable efﬁcient meta-inference. note aide rely asymptotic exactness inference algorithm evaluated. interesting area future work using aide study non-asymptotic error scalable asymptotically biased sampling algorithms also seems fruitful connect aide results theoretical computer science including computability complexity probabilistic inference. possible study computational tractability approximate inference empirically using aide estimates well theoretically using careful treatment variance estimates. also seems promising ideas aide develop monte carlo program analyses samplers written probabilistic programming languages. research supported darpa iarpa ofﬁce naval research army research ofﬁce gifts analog devices google. research conducted government support awarded force ofﬁce scientiﬁc research national defense science engineering graduate fellowship references mary kathryn cowles bradley carlin. markov chain monte carlo convergence diagnostics comparative review. journal american statistical association nicholas metropolis arianna rosenbluth marshall rosenbluth augusta teller edward teller. equation state calculations fast computing machines. journal chemical physics noah goodman vikash mansinghka daniel keith bonawitz joshua tenenbaum. church language generative models non-parametric memoization approximate inference. uncertainty artiﬁcial intelligence chris maddison dieterich lawson george tucker nicolas heess mohammad norouzi andriy mnih arnaud doucet whye teh. filtering variational objectives. arxiv preprint arxiv. salimans diederik kingma welling. markov chain monte carlo variational inference bridging gap. proceedings international conference machine learning pages yener ulker bilge günsel taylan cemgil. sequential monte carlo samplers dirichlet process mixtures. proceedings thirteenth international conference artiﬁcial intelligence statistics pages michael drinkwater quentin parker dominique proust eric slezak hernán quintana. large scale distribution galaxies shapley supercluster. publications astronomical society australia cameron freer vikash mansinghka daniel roy. probabilistic programs probably computationally tractable? nips workshop advanced monte carlo methods applications sampler template based reproduced algorithm algorithm evolves particles approximate sequence target distributions using combination proposal kernels weighting resampling steps. ﬁnal target distribution sequence typically posterior version algorithm resamples ﬁnal weighted particle approximation returns particle output sample speciﬁcally algorithm uses sequence unnormalized target densities deﬁned spaces algorithm also makes initialization kernel deﬁned proposal kernels deﬁned indexed backward kernels deﬁned indexed simplicity analysis assume resampling occurs every step sequence. weight functions used algorithm note algorithm sample backward kernels serve deﬁne extended justify sampler sequential importance sampler detailed balance transition operator algorithm reduces ais. particle ﬁlter without rejuvenation also special case algorithm variety variants also seen special cases more generally proposal kernel needs stationary distribution pt−. backward kernel ‘reversal’ deﬁned proposal kernel satisﬁes detailed balance reversal therefore sampling backward kernel identical sampling forward kernel. canonical meta-inference sampler takes input latent sample returns trace algorithm containing particles time steps parent indices ﬁnal output particle index density outputs meta-inference sampler given make aide concrete reader provide aide algorithm specialized measure symmetrized divergence variational approximation annealed importance sampler variational inference meta-inference sampler necessary evaluate variational approximation density discussed main text. meta-inference sampler consists running chain reverse starting latent sample. trace generated vector intermediate states reverse ais. algorithm gives concrete instantiation aide simpliﬁed case gold-standard algorithm sampler target algorithm evaluated variational approximation. simplify algorithm ﬁxing gold-standard. algorithm must support primitives ais.forward runs forward returns resulting output sample resulting marginal likelihood proof. consider general case inference algorithms generative inference models meta-inference algorithms normalizing constants respectively. example ‘target’ inference algorithm ‘gold standard’ inference algorithm. note analysis aide symmetric first deﬁne following quantity relating obtain equation used fact log) invariant permutation arguments uma. substituting expression given equation expression given equation equation have next show nondecreasing first show introduce notation uk−k+ma denote subvector length obtained removing element vector uma−ma+ma uma−. note that generic algorithm used single particle algorithm becomes markov chain samples transition kernels canonical metainference algorithm also becomes markov chain samples transition kernels reverse order. analysis assume satisﬁes detailed balance respect intermediate distribution then incremental weight simpliﬁes deﬁned initialization distribution similar result obtained direction divergence. intermediate distributions sufﬁciently ﬁne-grained empirically divergence converges zero however standard markov chain monte carlo practice without annealing intermediate distributions case approximation error meta-inference divergence initializing distribution posterior generally large. better meta-inference algorithms rely assumption chain near equilibrium times needed order aide practical tool measuring accuracy standard non-annealed markov chain monte carlo. obtained data galaxy velocities based redshift randomly subsampled forty galaxies analysis. histogram data shown figure consider task inference collapsed normal-inverse-gamma dpmm. used particles optimal proposal cluster assignments metropolis-hastings rejuvenation kernels hyperparameters gibbs kernels cluster assignments gold-standard inference algorithm. using gold-standard evaluated accuracy inference prior proposal without rejuvenation kernels using aide using alternative diagnostic based comparing average number clusters sampling distribution relative average number gold-standard sampling distribution. results shown figure figure figure shows histogram velocities galaxies model data using dirichlet process mixture evaluate accuracy inference algorithms relative gold-standard using aide using heuristic diagnostic based measuring average number clusters approximating distribution gold-standard distribution. shows results aide. shows result heuristic diagnostic. techniques indicate rejuvenation kernels important fast convergence. unlike heuristic diagnostic aide require custom design probe function model. envision aide used concert heuristic diagnostics like experience aide provides conservative quantiﬁcation accuracy heuristic diagnostics. experiment performed subsampled data points data", "year": 2017}