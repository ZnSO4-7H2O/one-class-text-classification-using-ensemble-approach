{"title": "Exploring Latent Semantic Factors to Find Useful Product Reviews", "tag": ["cs.AI", "cs.CL", "cs.IR", "cs.SI", "stat.ML"], "abstract": "Online reviews provided by consumers are a valuable asset for e-Commerce platforms, influencing potential consumers in making purchasing decisions. However, these reviews are of varying quality, with the useful ones buried deep within a heap of non-informative reviews. In this work, we attempt to automatically identify review quality in terms of its helpfulness to the end consumers. In contrast to previous works in this domain exploiting a variety of syntactic and community-level features, we delve deep into the semantics of reviews as to what makes them useful, providing interpretable explanation for the same. We identify a set of consistency and semantic factors, all from the text, ratings, and timestamps of user-generated reviews, making our approach generalizable across all communities and domains. We explore review semantics in terms of several latent factors like the expertise of its author, his judgment about the fine-grained facets of the underlying product, and his writing style. These are cast into a Hidden Markov Model -- Latent Dirichlet Allocation (HMM-LDA) based model to jointly infer: (i) reviewer expertise, (ii) item facets, and (iii) review helpfulness. Large-scale experiments on five real-world datasets from Amazon show significant improvement over state-of-the-art baselines in predicting and ranking useful reviews.", "text": "abstract online reviews provided consumers valuable asset e-commerce platforms inﬂuencing potential consumers making purchasing decisions. however reviews varying quality useful ones buried deep within heap non-informative reviews. work attempt automatically identify review quality terms helpfulness consumers. contrast previous works domain exploiting variety syntactic community-level features delve deep semantics reviews makes useful providing interpretable explanation same. identify consistency semantic factors text ratings timestamps user-generated reviews making approach generalizable across communities domains. explore review semantics terms several latent factors like expertise author judgment ﬁne-grained facets underlying product writing style. cast hidden markov model latent dirichlet allocation based model jointly infer reviewer expertise item facets review helpfulness. large-scale experiments real-world datasets amazon show signiﬁcant improvement state-of-the-art baselines predicting ranking useful reviews. motivation rapid growth e-commerce product reviews become crucial component business. consumers cannot test functionality product prior purchase reviews help make informed decision product not. survey conducted nielsen corporations online consumers indicated would electronics without consulting online reviews ﬁrst increasing dependency user-generated reviews crucial understand quality widely vary excellent-detailed opinion superﬁcial criticizing praising spams worst case. without indication review quality overwhelming consumers browse multitude reviews. order help consumers ﬁnding useful reviews e-commerce platforms nowadays allow users vote whether product review helpful not. instance amazon product review accompanied information like users found review helpful. helpfulness score considered proxy review quality usefulness customers. work automatically helpfulness score review based certain consistency semantic aspects review like whether review written expert important facets product outlined review experts given product timeliness review etc. automatically mined latent factors review texts. predicting review helpfulness spams prior works predicting review usefulness mostly operate shallow syntactic textual features like bag-of-words part-of-speech tags tf-idf statistics works related works ﬁnding review spams classify extremely opinionated reviews helpful. similarly works exploiting rating activity features like frequency user posts average ratings users items consider extreme ratings deviations indicative unhelpful reviews. recent works incorporate additional information like community-speciﬁc characteristics explicit user network item-speciﬁc meta-data like explicit item facets product brands apart requirement large number meta-features restrict generalizability many models arbitrary domain shallow features analyze review about therefore cannot explain helpful given product. works identify expertise review’s author important feature. however absence suitable modeling techniques consider prior reputation features like user activity rating deviation proxy user expertise. latent factors review analysis prior approaches analyzing review texts learn latent topics latent aspects ratings user-user interactions author writing style also used however prior approaches factor temporal dynamics user expertise. modeling expertise model capturing user expertise draws motivation signiﬁcant diﬀerences. ignores content reviews focuses rating behavior modeling expertise evolution addressed proposes generalized version expertise evolution model however much complex computationally expensive. therefore work simpler version addressing computational deﬁciencies tractable inference problem setting. work aims overcome limitations prior works exploring semantics consistency review predict helpfulness given product. ﬁrst step towards understanding semantics review uncover facet descriptions target product outlined review. treat facets latent latent dirichlet allocation discover topic clusters. second step expertise users wrote review description diﬀerent facets product. work model expertise latent variable evolves time using hidden markov model make distributional hypotheses like expert users agree important facets product description facets inﬂuences helpfulness review. also derive several consistency features given interpretable explanation derive interesting insights latent word clusters used experts instance reviews describing underlying theme storytelling movies books style music hygiene food considered helpful respective domains. summary make following novel contributions model propose approach leverage semantics consistency reviews predict helpfulness. propose hidden markov model latent dirichlet allocation based model jointly learns product facets user expertise writing style observed words reviews explicit timepoints. algorithm introduce eﬀective learning algorithm based iterative stochastic optimization process reduces mean squared error predicted helpfulness scores ground scores well maximizes log-likelihood data. experiments perform large-scale experiments real-world datasets diﬀerent domains amazon together comprising million reviews million users million items demonstrate substantial improvement state-of-the baselines prediction ranking tasks. section outline components model analyze semantics consistency reviews. item facets essential understand diﬀerent facets item review. instance camera review focus facets like resolution zoom price size movie review focus narration cinematography acting direction etc. however facets equally important item. example review downrating camera late delivery seller helpful consumers opposed downrating grainy resolution shaky zoom. therefore helpful review focus important facets item. model facets latent variables item’s latent facet distribution review indicative detailed diverse review review writing style words used describe facets play crucial role making review useful consumers. important aspect expert writing style precise domain-speciﬁc vocabulary describe facet details rather using generic words. instance contrast expert camera review focus screen ‘grainy’. ‘precision matte’ surface helps increase owned. bought last xmas deal thrown away replaced decent camera. learn language model latent facets user expertise helps distinguish writing style experienced user amateur one. reviewer expertise prior works used proxy features like user activity reputation harness users’ expertise hypothesis expert reviews positively correlated review helpfulness. explicitly model user expertise drawing motivation recent works substantial modiﬁcations tractable inference expertise static evolves time. user amateur time entering community become expert now. model expertise latent variable evolves time exploiting hypothesis users similar levels expertise similar rating behavior facet preferences writing style. distributional hypotheses approach makes following hypotheses capture helpfulness reviews similar facet distribution items likely equally helpful. users similar facet preferences expertise likely equally helpful. traditional collaborative ﬁltering approaches recommender systems similar itemitem user-user similarities respectively. consistency users items gain reputation overnight. therefore prior reputation users items good indicators associated reviews’ helpfulness. following features guide model learning latent distributions based review helpfulness. prior user reputation average helpfulness votes received user’s past reviews users. prior item prominence average helpfulness votes received item’s past reviews users also indicative prominence item. user rating deviation absolute deviation user’s rating item average rating assigned user items. captures mean user rating behavior therefore scenarios user dis-satisﬁed item. item rating deviation absolute deviation user’s rating item average rating received item users. captures scenario user unnecessarily criticizes praises item. global rating deviation absolute deviation user’s rating item average rating items users community capturing deviation user behavior general community behavior. timeliness early-bird bias prior work shows positive inﬂuence review’s publication date perceived helpfulness. early timely reviews useful item launched consumers make informed decision. also early reviews exposed consumers longer period time allowing garner votes time compared recent reviews. timestamp ﬁrst review given item considered reference timepoint therefore timeliness review item time computed exp−. incorporating consistency factors user writing review time item ...w|nd|} corresponding review text sequence words rating review associated helpfulness average helpfulness score user reviews written average helpfulness score reviews item average rating assigned user items average rating assigned item users average global rating items users. consistency features include prior item user reputation deviation features burst. where regularization parameters penalize complex models. several ways estimate parameters like alternate least squares gradient-descent newton based approaches. incorporating latent facets principles latent dirichlet allocation learn latent facets associated item. review item assumed multinomial distribution facets symmetric dirichlet prior facet multinomial distribution words drawn vocabulary symmetric dirichlet prior exact inference possible intractable coupling popular ways approximate inference mcmc techniques like collapsed gibbs sampling variational inference. language model writing style also diﬀerent users diﬀerent levels expertise. therefore parametrize distributions user expertise similar prior work major modiﬁcations consider tensor dimension tensor dimension e×z×w denotes preference facet users expertise level φezw denotes probability word used describe facet users expertise changes users evolve time. however transition smooth. users cannot abruptly jump expertise level without passing expertise levels therefore timepoint assume user expertise level monotonically non-decreasing). progression depends writing style facet preferences user evolving respect expert users community; well rate activity user hyper-parameter controlling rate progression. activity rate user deﬁned davg denote number posts written average number posts written user community respectively. users start level expertise enter community; enter already expert. algorithm ﬁgures inference process. assume users start expertise level parameter initialization. diﬀerence prior works modeling expertise generative process user expertise motivated following diﬀerences prior works learn user-speciﬁc preferences personalized recommendation. however assume users level expertise similar facet preferences. therefore facet distribution conditioned user expertise user explicitly unlike prior works. helps reduce dimensionality exploit correspondence parameters consistency latent factor models together tractable inference. prior work incorporates supervision predicting ratings indirectly optimizing dirichlet hyper-parameters multinomial facet distribution cannot guarantee increase data log-likelihood iterations. contrast exploit learn expertise-facet distribution directly review helpfulness scores minimizing mean squared error inference. also tricky parameters distribution unconstrained optimization guaranteed simplex certain transformations discussed inference. therefore parameters strongly coupled model reducing mean squared error also leading near smooth increase data log-likelihood iterations review since review associated unique timestamp unique user expertise value review refers expertise user time writing following markovian assumption user’s expertise level transitions follow distribution i.e. expertise level time depends expertise level writing previous review time td−. expertise level user review known facet preferences given thereafter facet word drawn multinomial expertise level user facets interest known generate language model individual words review user draws word multinomial distribution φedzdw symmetric dirichlet prior refer figure generative process. collapsed gibbs sampling standard estimate conditional distribution latent facets computed current assignment hidden variables integrating following equation indicates summation counts possible similar process collapsed gibbs sampling also sample expertise levels keeping facet assignments ﬁxed. denote number transitions expertise level users community markovian constraint {ei− subscript denotes value variable excluding data position. note transition function similar prior works hidden markov model latent dirichlet allocation based models case known could directly plugged values equation learn model parameters however dimensions corresponding facets user expertise latent need inferred text. parameter corresponding learned equation depicts importance facet users expertise level predicting review helpfulness. want exploit observation infer latent dimensions text. cannot directly replace following reason. traditional parametrization multinomial distribution mean parameters. unconstrained optimization take parameters feasible i.e. simplex. hence easier work natural parameters instead. consider unconstrained factor models probability user reaching expertise level document whereas second third factor models probability facets {zj} chosen expertise level probability observing words {wj} facets {zj} expertise level respectively. following generates equation coupled equations iii) generates using equation learned regression using equations minimize mean squared error predicting review helpfulness. overall processing scheme exploiting results discussions overall inference iterative stochastic optimization process consisting following steps sort reviews timestamps estimate using equation gibbs sampling. process consider facet assignments earlier iteration ﬁxed. estimate using equation regression regression fast scalable support vector regression implementation liblinear uses trust region newton method learning parameters setup perform experiments data amazon diﬀerent domains movies music food books electronics. statistics dataset given table total million reviews million users million items domains combined. robust dataset learning. since food dataset less number reviews lowered threshold ﬁve. test used recent reviews user withheld test data received atleast votes data used models comparison. group long-tail users less reviews training data background model treated single user avoid modeling sparse observations. ignore user. test phase long-tail user take parameters background model. number facets number expertise levels datasets. tasks evaluation measures models following tasks prediction objective predict helpfulness score review number users voted review helpful number users. evaluation measures report mean squared error predicted scores ground helpfulness scores squared correlation coeﬃcient gives indication goodness model i.e. well regression function approximates real data points indicating perfect ranking suitable evaluation compare ranking reviews diﬀerent models sorted helpfulness scores reviews rank list helpful ones compute rank correlation gold/reference rank list using following measures spearman correlation assesses well relationship variables described using monotonic function kendall-tau correlation measures number concordant discordant pairs whether ranks elements agree based scores total number combinations possible. quantitative comparison baselines consider following baselines compare work p.o’mahony several rating based features proxy reviewer reputation sentiment; review length letter cases content; review count statistics social features. syntactic features sentiment review length reviewer rating statistics predict quality review. structural lexical syntactic semantic meta-data related features rank reviews based helpfulness. predict helpfulness reviews imdb based reviewer expertise syntactic features timeliness review. authors reviewer preferences explicit facets proxy expertise part-ofspeech tags words syntactic features review publication dates timeliness reviews. baseline closest work attempt model similar factors. however model reviewer expertise explicitly facets latent therefore relying additional item meta-data proxies user authority. baselines features works supported dataset instance could social network explicit product meta-data absent dataset fair comparison. table shows comparison mean squared error squared correlation coeﬃcient review helpfulness predictions generated model four baselines. model consistently outperforms baselines reducing mse. table shows comparison spearman kendall-tau correlation rank list helpful user reviews generated models gold rank list. competitive baseline model high overlap consistency features model baseline performance improvement attributed incorporation latent factors model. perform paired sample t-tests performance improvement baselines best domains movies music books large number reviews relatively worse domains food electronics data sparsity user maturity could captured well. log-likelihood data convergence inference model quite involved coupling several variables alternate stochastic optimization process. figure shows increase data log-likelihood model per-iteration diﬀerent domains. observe model stable achieves near smooth increase data loglikelihood per-iteration. also converges quite fast dataset. electronics convergence quite rapid data quite sparse model suﬃcient evidence categorizing users diﬀerent expertise levels; behavior reﬂected experiments involving electronics dataset. language model facet preference divergence figures show heatmaps kullbackleibler divergence facet preferences language models users diﬀerent expertise levels computed model conditioned review helpfulmain observation divergence higher larger diﬀerence expertise levels users. conﬁrms hypothesis expert users distinctive writing style facet preferences diﬀerent amateurs captured joint interactions review helpfulness reviewer expertise facet preferences writing style. also note increase divergence increase expertise levels smooth food electronics sparsity per-user data. table prediction task performance comparison model versus baselines. improvements baselines statistically signiﬁcant p-value using paired sample t-test. table ranking task correlation comparison ranking reviews gold rank list model versus baselines. improvements baselines statistically signiﬁcant p-value using paired sample t-test. music album lyrics recommend soundtrack touch songwriting features rare musical ears lyrical enjoy absolutely musically individual bland soothing released inspiration share mainstream deeper ﬂawless wonderfully eclectic heavily critics presence books serious complex claims content illustrations picture genre beautifully literary witty critics complicated argument premise scholarship talented divine twists exceptional obsession commentary landscape exposes inﬂuenced accomplished oriented movies scene recommend screenplay business depth justice humanity packaging perfection ﬂicks sequels propaganda anamorphic cliche&acute pretentious goofy ancient marvelous perspective outrageous intensity mildly immensely bland subplots anticipation electronics adapter wireless computer sounds camera range drives mounted photos shots packaging antenna ease careful broken cards distortion stick media application worthless clarity technical memory steady dock items cord systems amps skin watt food expensive machine months clean chips texture spicy odor inside processed robust packs weather sticking alot press poured swallow reasonably portions beware fragrance basket volume sweetness terribly caused scratching serves sensation sipping smelled music will good favorite cool great genius earlier notes attention place putting superb style room beauty realize brought passionate diﬀerence fresh save musical grooves consists tapes depressing interview short rock appeared learn brothers books will book time religious liberal material interest utterly moves movie consistent false committed question turn coverage decade novel understood worst leader history kind energy dropped current doubt books building travel sudden fails movies movie hour dont close previous features type months meaning wait boring absolutely truth generation going ﬁghting runs fantastic kids quiet kill lost angles previews crafted teens help believes brilliance touches hardcore continue album electronics order attach replaced write impressed install learn tool oﬀered details turns snap price digital well buds problems photos hear shoot surprisingly continue house card sports writing include adequate nice programming protected mistake food night going haven sour avoid sugar coﬀee store bodied graham variety salsa reasons favorite delicate purpose brands worst litter funny partially sesame handle excited close awful happily fully eﬀects virgin salt returned powdery meals great interpretable explanation salient words used experts helpful reviews table shows snapshot latent word clusters used experts amateurs helpful reviews otherwise generated model. observe helpful reviews pertaining music talk essence style; books describe theme writing style; movies write screenplay storytelling; food reviews mostly concerned hygiene allergens; electronics discuss speciﬁc product features. note prior works used explicit product features able automatically discover latent features reviews. least helpful reviews mostly describe generic concepts praise criticize item without going depth facets generally quite superﬁcial nature. proposed approach predict useful product reviews exploiting joint interaction user expertise writing style timeliness review consistency using hidden markov model latent dirichlet allocation. unlike prior works exploiting variety syntactic domain-speciﬁc features model uses information user reviewing item explicit timepoint task making approach generalizable across communities domains. additionally provide interpretable explanation review helpful terms salient words latent word clusters used experts describe important facets item review. experiments real-world datasets amazon like books movies music food electronics demonstrate eﬀectiveness approach state-of-the-art baselines.", "year": 2017}