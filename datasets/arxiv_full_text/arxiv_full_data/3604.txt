{"title": "DCFNet: Deep Neural Network with Decomposed Convolutional Filters", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Filters in a Convolutional Neural Network (CNN) contain model parameters learned from enormous amounts of data. In this paper, we suggest to decompose convolutional filters in CNN as a truncated expansion with pre-fixed bases, namely the Decomposed Convolutional Filters network (DCFNet), where the expansion coefficients remain learned from data. Such a structure not only reduces the number of trainable parameters and computation, but also imposes filter regularity by bases truncation. Through extensive experiments, we consistently observe that DCFNet maintains accuracy for image classification tasks with a significant reduction of model parameters, particularly with Fourier-Bessel (FB) bases, and even with random bases. Theoretically, we analyze the representation stability of DCFNet with respect to input variations, and prove representation stability under generic assumptions on the expansion coefficients. The analysis is consistent with the empirical observations.", "text": "filters convolutional neural network contain model parameters learned enormous amounts data. paper suggest decompose convolutional ﬁlters truncated expansion pre-ﬁxed bases namely decomposed convolutional filters network expansion coeﬃcients remain learned data. structure reduces number trainable parameters computation also imposes ﬁlter regularity bases truncation. extensive experiments consistently observe dcfnet maintains accuracy image classiﬁcation tasks signiﬁcant reduction model parameters particularly fourier-bessel bases even random bases. theoretically analyze representation stability dcfnet respect input variations prove representation stability generic assumptions expansion coeﬃcients. analysis consistent empirical observations. convolutional neural network become successful computational models machine learning artiﬁcial intelligence. remarkable progress achieved design successful network structures vgg-net resnet densenet less attention paid design ﬁlter structures cnns. filters namely weights convolutional layers important ingredients model ﬁlters contain actual model parameters learned enormous amounts data. filters cnns typically randomly initialized updated using variants extensions gradient descent result trained ﬁlters speciﬁc structures often leads signiﬁcant redundancy learned model filters improved properties direct impact accuracy eﬃciency theoretical analysis ﬁlters also central importance mathematical understanding deep networks. pre-ﬁxed bases spatial domain namely decomposed convolutional filters network expansion coeﬃcients remain learned data. representing ﬁlters terms functional bases come prior data task knowledge rather pixel values number trainable parameters reduced expansion coeﬃcients; furthermore regularity conditions imposed ﬁlters truncated expansion. image classiﬁcation tasks empirically observe dcfnet able maintain accuracy signiﬁcant reduction number parameters. observation holds even random bases used. particular adopt dcfnet leading fourier-bessel bases correspond low-frequency components input. experimentally observe superior performance dcfnet bases image classiﬁcation denoising tasks. dcf-fb network reduces response high-frequency components input least stable image variations deformation often aﬀect recognition suppressed. intuition supported mathematical analysis representation ﬁrstly develop general result representation stability input image undergoes deformation proper boundedness conditions convolutional ﬁlters imposing structure show long trainable expansion coeﬃcients layer dcf-fb network satisfy boundedness condition l-th-layer output stable respect input deformation diﬀerence bounded magnitude distortion apart bases dcfnet structure studied paper compatible general choices bases standard fourier bases wavelet bases random bases bases. numerically test several options section stability analysis dcf-fb networks extended bases choices well based upon general theory developed representation using similar techniques. work related recent results topics usage bases deep networks model reduction well stability analysis deep representation. review connections section finally though current paper focuses supervised networks classiﬁcation recognition applications image data introduced layers generic concept potentially used reconstruction generative models well. discuss possible extensions last section. deep network bases representation stability. usage bases deep networks previously studied including wavelet bases bases learned dictionary atoms etc. wavelets powerful tool signal processing shown optimal basis data representation generic settings pioneering mathematical model scattering transform used pre-ﬁxed weights network wavelet ﬁlters showed representation produced scattering network stable respect certain variations input. extension scattering transform studied includes larger class bases used network. apart wavelet deep network bases studied making connection dictionary learning studied deep networks form cascade convolutional sparse coding layers theoretically analyzed representation. deep networks random weights studied proved representation stability. dcfnet studied paper incorporates structured pre-ﬁxed bases combined adapted expansion coeﬃcients learned data supervised demonstrates comparable even improved classiﬁcation accuracy image datasets. meanwhile theoretical representation stability inherited thanks ﬁlter regularity imposed non-adapted bases. network redundancy. various approaches studied suppress redundancy weights trained cnns including model compression sparse connections. model compression network pruning studied combined quantization huﬀman encoding used hash functions reduce model size without sacriﬁcing generalization performance. low-rank compression ﬁlters studied explored sparse connections cnns recently studied theoretical side showed sparsely-connected network achieve certain asymptotic statistical optimality. proposed dcfnet relates model redundancy compression regularity conditions imposed ﬁlters. dcf-fb network redundancy reduction achieved suppressing network response high-frequency components inputs. figure multi-scale convolutional ﬁlters fourier-bessel bases various scales gabor ﬁlters directions size approximation leading bases reduction rate truncation incurs almost change ﬁlters. leading bases shown middle panel. images rescaled illustration purpose. ﬁlters biases parameters cnn. practice discretized cartesian grid continuous convolution approximated discrete analogue. throughout paper continuous spatial variable locally supported e.g. simplicity. importantly ﬁlters image patches. cnns typically represent store ﬁlters vectors size local patches equivalent expanding ﬁlters delta bases. delta bases optimal representing smooth functions. example regular functions fast decaying coeﬃcients fourier bases natural images sparse representation wavelet bases. layers represent convolutional ﬁlters truncated expansion basis functions non-adapted training process adaption comes combination bases. speciﬁcally suppose convolutional ﬁlters certain layer proper rescaling spatial variable supported unit disk given bases {ψk}k space ﬁlters represented bases numerically test diﬀerent choices section including data-adapted bases random bases. experiments consistently show convolutional layers drastically decomposed compressed almost reduction classiﬁcation accuracy sometimes even using random bases gives strong performance. particular motivated classical results harmonic analysis bases dcfnet regularity ﬁlters imposed though constraining magnitude coeﬃcients {λ)k}k example gabor ﬁlters approximated using leading bases plotted right figure experiments dcfnet bases shows superior performance image classiﬁcation denoising tasks compared original bases tested theoretically section analyzes representation stability dcfnet respect input variations provides theoretical explanation advantage bases. output activation spatial size original convolutional layer needs ﬂops take ﬂops summation contract layer takes channels take extra ﬂops many convolutions step ﬂops step). thus leading computation changes multiplied giving reduction rate number ﬁrst study stability features respect input variations particularly nuisance deformations aﬀect object recognized. stability conditions translated speciﬁc bounded conditions expansion coeﬃcients layer analyzed fourier-bessel bases. analysis extend bases using similar techniques. proposition mapping l-th convolutional layer denoted nonexpansive i.e. arbitrary output l-th layer zero input. result switch operator l-layer mapping idea control residual switching layer following lemma proved appendix lemma remark possible derive technical bound terms constants without assuming using technique. however later analysis dcfnet conditions implied single condition pursue technical result here. figure example convolutional ﬁlters network outputs second layer conv- trained mnist corresponding dcfnet using bases ﬁlters dcfnet visibly smoother network outputs. classiﬁcation accuracy networks same table domain e.g. rotation. turns multi-scale architecture induces decrease quantity proportional inverse domain diameter compensate increase |τ|∞ scale grows long rescaled ﬁlters properly bounded integral. downsampling support l-th layer ﬁlters enlarges increases. accordance pooling assume supported vanishing boundary disk radius image supported domain l-layer size patches smallest scale. {ψk}k bases supported dcfnet compatible general choices bases focus bases section example. bases indexed angular radial frequencies respectively. supported unit disk polar coordinates bessel function ﬁrst kind integers mqdu πδmmδqq. furthermore bases eigenfunctions dirichlet laplacian i.e. µkψk thus bases ordered increases leading illustrated fig. principle frequency truncated according nyquist sampling rate. truncation turned often used setting signiﬁcant bases truncation dcfnet. unit disk. negative dirichlet laplacian unit disk. result proposition using bases |v||∇w absolute constant. speciﬁcally notice boundedness implies decay |ak| least fast grows increases justiﬁes truncation expansion leading bases correspond low-frequency modes. proof. l.h.s. bounded second term less cl|∇τ|∞x theorem bound ﬁrst term apply proposition notice −j∇w akψk lemma thus −jlal. gives c−jl|τ|∞x proposition section experimentally demonstrate convolutional ﬁlters decomposed truncated expansion pre-ﬁxed bases expansion coeﬃcients remain learned data. though number trainable parameters signiﬁcantly reduced accuracy tasks image classiﬁcation face veriﬁcation still maintained. empirical observations hold data-independent fourier-bessel random bases data-dependent bases. clxlxmxm stands convolutional layer patch size input channel mplxl stands max-pooling. corresponding dcfnets lxlxmxm conv layer expended bases trainable coeﬃcients implemented conv layer. table classiﬁcation accuracy using dcfnets various image benchmarks diﬀerent number bases stand fourier-bessel bases random bases respectively. pca-s pca-f stand bases computed network pre-trained small subset training images full training respectively. param. number parameters convolutional layers mflops number ﬂops convolutional layers object classiﬁcation experiments evaluate dcfnet three types predeﬁned bases fourier-bessel bases random bases generated gaussian vectors bases principal components convolutional ﬁlters pretrained corresponding model three network architectures used classiﬁcation conv- conv- shown table vgg- generate corresponding dcfnet structure conv layer expended pre-deﬁned bases obtained trainable expansion coeﬃcients number basis used evaluate multiple diﬀerent levels parameter reduction. order compatible existing deep learning frameworks pre-ﬁxed bases currently implemented regular convolutional layers zero learning rate. bases used dcfnets outperform corresponding models still signiﬁcantly less parameters. bases correspond low-frequency components inputs dcffb network responds less high-frequency nuance details often irrelevant classiﬁcation tasks. surprisingly observe random bases also report acceptable performance. random bases data independent. comparison purposes also evaluate dcfnets data dependent bases principal components corresponding convolutional ﬁlters pre-trained models. model pre-trained training data bases shows comparable performance bases. however quality bases degenerates subset training used. svhn training images gray-scale version labels train three networks simply reconstruct input image figure shows three trained networks behave reconstructing examples svhn testing images. without noise added input images figure three networks report decent reconstruction dcf-rb shows inferior dcf-fb. psnr values indicate often produces precise reconstructions; however missing high-frequency components dcf-fb reconstructions mostly nuance details. noise added ﬁgures dcf-fb produces signiﬁcantly superior reconstruction dcf-rb tenth parameter number cnn. empirical observations clearly indicate fourier-bessel bases correspond low-frequency components inputs enable ignore high-frequency nuance details often less stable input variations mostly irrelevant tasks classiﬁcation. empirical observation provides good intuitions behind superior classiﬁcation performance also consistent theoretical analysis representation stability section present evaluation dcfnet face veriﬁcation tasks using very deep network architectures comprise long sequence convolutional layers. order train complex networks adopt large scale vgg-face dataset contains face images people. shown table adopt vgg-very-deep- architecture detailed modifying layer change output features dimension network comprises weight layers except last fully-connected layer utilize structure challenging benchmark. note model outperforms vgg-face model improvement mostly smaller output dimension adopted models share similar architecture trained face dataset. paper shows cnns convolutional ﬁlters decomposed structure namely dcfnet ﬁlters represented truncated expansion pre-ﬁxed bases expansion coeﬃcients remain trained supervised way. experimentally observe various object recognition datasets classiﬁcation accuracy maintained signiﬁcant reduction number parameters. performance fourier-bessel bases constantly superior experiments. truncation bases dcfnet viewed regularization ﬁlters result dcf-fb less susceptible high-frequency components input least stable expected input variations often aﬀect recognition suppressed. interpretation supported image denoising experiments dcf-fb performs preferably original dcfnets bases noisy inputs. also provide stability analysis dcfnet representation proved proper boundedness conditions expansion coeﬃcients perturbation deep features respect input variations bounded. beyond classiﬁcation. meanwhile test resilience adversarial noise extend study representation stability. finally structures imposed across channels used jointly imposes spatial structures ﬁlters.", "year": 2018}