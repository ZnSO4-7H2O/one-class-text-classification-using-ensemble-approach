{"title": "Incremental Learning of Event Definitions with Inductive Logic  Programming", "tag": ["cs.LG", "cs.AI"], "abstract": "Event recognition systems rely on properly engineered knowledge bases of event definitions to infer occurrences of events in time. The manual development of such knowledge is a tedious and error-prone task, thus event-based applications may benefit from automated knowledge construction techniques, such as Inductive Logic Programming (ILP), which combines machine learning with the declarative and formal semantics of First-Order Logic. However, learning temporal logical formalisms, which are typically utilized by logic-based Event Recognition systems is a challenging task, which most ILP systems cannot fully undertake. In addition, event-based data is usually massive and collected at different times and under various circumstances. Ideally, systems that learn from temporal data should be able to operate in an incremental mode, that is, revise prior constructed knowledge in the face of new evidence. Most ILP systems are batch learners, in the sense that in order to account for new evidence they have no alternative but to forget past knowledge and learn from scratch. Given the increased inherent complexity of ILP and the volumes of real-life temporal data, this results to algorithms that scale poorly. In this work we present an incremental method for learning and revising event-based knowledge, in the form of Event Calculus programs. The proposed algorithm relies on abductive-inductive learning and comprises a scalable clause refinement methodology, based on a compressive summarization of clause coverage in a stream of examples. We present an empirical evaluation of our approach on real and synthetic data from activity recognition and city transport applications.", "text": "abstract event recognition systems rely properly engineered knowledge bases event deﬁnitions infer occurrences events time. manual development knowledge tedious error-prone task thus event-based applications beneﬁt automated knowledge construction techniques inductive logic programming combines machine learning declarative formal semantics first-order logic. however learning temporal logical formalisms typically utilized logic-based event recognition systems challenging task systems cannot fully undertake. addition event-based data usually massive collected diﬀerent times various circumstances. ideally systems learn temporal data able operate incremental mode revise prior constructed knowledge face evidence. systems batch learners sense order account evidence alternative forget past knowledge learn scratch. given increased inherent complexity volumes real-life temporal data results algorithms scale poorly. work present incremental method learning revising event-based knowledge form event calculus programs. proposed algorithm relies abductive-inductive learning comprises scalable clause reﬁnement methodology based compressive summarization clause coverage stream examples. present empirical evaluation approach real synthetic data activity recognition city transport applications. growing amounts temporal data collected execution various tasks within organizations hard utilize without assistance automated processes. event recognition refers automatic detection event occurrences within system. sequence low-level events event recognition system recognizes high-level events interest events satisfy pattern. event recognition systems logic-based representation event deﬁnitions event calculus attracting signiﬁcant attention event processing community number reasons including expressiveness understandability formalized knowledge declarative formal semantics ability handle rich background knowledge. using logic programs particular extra advantage close connection logic programming machine learning ﬁeld inductive logic programming however applications impose challenges make systems inappropriate. several logical formalisms incorporate time change employ nonmonotonic operators means representing commonsense phenomena normal logic programs negation failure particular prominent non-monotonic formalism. learners cannot handle lack robust semantics another problem often arises dealing events need infer implicit missing knowledge instance indirect eﬀects events possible causes observed events. ability reason missing indirectly observable knowledge called non-observational predicate learning task systems diﬃculty handle especially combined background knowledge address problem combination abductive logic programming abduction logic programming usually given non-monotonic semantics addition nature appropriate framework reasoning incomplete knowledge. although long history literature recently combination brought systems xhail aspal used induction event-based knowledge. three systems which best knowledge learners address aforementioned learnability issues batch learners sense training data must place prior initiation learning process. always suitable event-oriented learning tasks data often collected diﬀerent times various circumstances arrives streams. order account training examples batch learner alternative re-learn hypothesis scratch. cost poor scalability learning large growing data. particularly true case temporal data usually come large volumes. consider instance data span large period time sensor data transmitted high frequency. alternative approach learning incrementally processing training instances become available altering previously inferred knowledge observations instead discarding starting scratch. process also known theory revision exploits previous computations speed-up learning since revising hypothesis generally considered eﬃcient learning scratch numerous theory revision systems proposed literature review however applicability non-monotonic domains limited issue addressed recent approaches theory revision non-monotonic non-monotonic learner used extract prescriptions turn interpreted syntactic transformations theory hand. however scaling large volumes today’s datasets handling streaming data remains open issue development scalable algorithms theory revision identiﬁed important research direction historical data grow time becomes progressively harder revise knowledge accounts evidence past experience. direction towards scaling theory revision systems development techniques reducing need reconsulting whole history accumulated experience updating existing knowledge. direction take work. build ideas nonmonotonic xhail basis scalable incremental learner induction event deﬁnitions form event calculus theories. xhail used induction action theories moreover used theory revision incremental setting revising hypotheses respect recent user-deﬁned subset perceived experience. contrast learner present performs revisions account examples seen far. describe compressive memory structure incorporated learning process reduces need reconsulting past experience response revision. using structure propose method which given stream examples theory accounts training instance requires pass examples order revise initial theory accounts past evidence.we evaluate empirically approach real synthetic data activity recognition application transport management application. results indicate approach signiﬁcantly eﬃcient xhail without compromising predictive accuracy scales adequately large data volumes. rest paper structured follows. section provides brief overview abductive inductive logic programming. section present event calculus dialect employ describe domain activity recognition running example show event deﬁnitions learnt using xhail. section present proposed method prove correctness present details abductive-inductive mechanism. section discuss theoretical practical implications approach. section present experimental evaluation ﬁnally sections discuss related work draw main conclusions. assume ﬁrst-order language front literals denotes negation failure call logic program horn naf-free normal otherwise. details basic terminology conventions logic programming used work appendix deﬁne entailment relation normal logic programs terms stable model semantics particular credulous version program entails program denoted least stable model stable model following prolog’s convention throughout paper predicates ground terms logical formulae start lower case letter variable terms start capital letter. inductive logic programming subﬁeld machine learning based logic programming. given positive negative examples represented logical facts algorithm derives non-ground rules discriminate positive negative examples potentially taking account background knowledge. deﬁnition provides formal account. logic program ground literals called positive negative examples clauses called language bias. normal logic program called inductive hypothesis task covers examples language bias mentioned deﬁnition reduces complexity task imposing syntactic restrictions hypotheses learnt. commonly used language bias also employed work mode declarations mode declaration template literal placed either head body hypothesis clause contains special placemarkers variables ground terms. mode declarations deﬁnes head body mode declarations replacing variable placemarkers actual variable symbols ground placemarkers ground terms. formal deﬁnitions mode declarations mode language provided appendix deﬁnition clause θ-subsumes clause denoted exists substitution headθ head bodyθ respectively. program θ-subsumes program clause exists clause cover positive examples inconsistent covers negative examples. inductive hypothesis task hypothesis complete consistent called correct. incomplete hypothesis made complete generalization syntactic transformations increase example coverage include addition clauses removal literals existing clauses. similarly inconsistent hypothesis made consistent specialization process aims restrict example coverage include removal clauses hypothesis addition literals existing clauses hypothesis. theory revision process acting upon hypothesis means syntactic transformations order change answer hypothesis examples accounts for. theory revision core incremental systems. incremental setting examples provided time. learner induces hypothesis scratch ﬁrst available examples treats hypothesis revisable background theory order account examples. event calculus temporal logic reasoning events eﬀects. formalism successfully used numerous event recognition applications ontology event calculus comprises time points i.e. integers real numbers; ﬂuents i.e. properties certain values time; events i.e. occurrences time aﬀect ﬂuents alter value. domain-independent axioms formalism incorporate common sense inertia according ﬂuents persist time unless aﬀected event. call event calculus dialect used work simpliﬁed discrete event calculus name building blocks sdec domain-independent axioms presented table ﬁrst axiom table states ﬂuent holds time initiated previous time point second axiom states continues hold unless terminated. initiatedat/ terminatedat/ deﬁned application-speciﬁc manner. examples presented shortly. throughout paper task activity recognition deﬁned caviar project running example. caviar dataset consists videos public space actors walk around meet other browse information displays ﬁght videos manually annotated caviar team provide ground truth types activity. ﬁrst type corresponds low-level events knowledge person’s activities certain time point second type corresponds high-level events activities involve person instance people moving together ﬁghting meeting recognize high-level events means combinations low-level events additional domain knowledge person’s position direction certain time point. low-level events represented sdec streams ground happensat/ atoms high-level events domain knowledge represented ground holdsat/ atoms. streams low-level events together domain-speciﬁc knowledge henceforth constitute narrative terminology knowledge high-level events annotation. table presents annotated stream low-level events. instance person inactive time coordinates direction annotation time point informs moving together. fluents express high-level events input information coordinates person. discriminate inertial statically deﬁned ﬂuents. former inferred event calculus axioms latter provided input. given domain description language sdec machine learning addressed work automatically derive domain-speciﬁc axioms axioms specify occurrence low-level events aﬀects truth values ﬂuents represent high-level events initiating terminating them. thus wish learn initiatedat/ terminatedat/ deﬁnitions positive negative examples narrative annotation. henceforth term example encompass anything known true speciﬁc time point. assume closed world thus anything explicitly given considered false example’s time point also serve reference. instance three diﬀerent examples presented table according annotation example either positive negative w.r.t. particular high-level event. instance table negative example moving high-level event positive example. learning event deﬁnitions form domain-speciﬁc event calculus axioms poses several challenges. note ﬁrst learning problem presented section requires non-observational predicate learning meaning instances target predicates provided supervision. using abduction obtain missing instances solution. abduction form logical inference seeks extract explanations make observations true. abductive logic programming observations represented queries derives explanations observations form ground facts make queries succeed. deﬁnition provides formal account. deﬁnition task triplet normal logic program predicates called abducibles ground queries called goals. ground atoms called abductive explanation using missing supervision learning problem section obtained abducing ground initiatedat/ terminatedat/ atoms explanations conjunction holdsat/ literals annotation principle several explanations possible given observations. avoid redundant explanations reasoners typically biased towards minimal explanations. instance atom initiatedat minimal abductive explanation holdsat/ literals table several systems proposed combine abductive reasoning. systems abduction obtain missing knowledge necessary explain provided examples employ standard techniques construct hypotheses. however systems cannot used learning event calculus programs. abductive-inductive systems restricted horn logic imparo others handle negation abduction limited. instance inthelex uses abduction generate facts might missing description example otherwise restricted opl. progol aleph alecto support form abductive reasoning lack full power alp. result cannot reason abductively negated atoms responsible shortcomings traditional approaches w.r.t. normal logic programs. first explained standard cover approach systems rely essentially unsound presence meaning return hypotheses cover examples. non-monotonicity newly inferred clauses invalidated past examples. time learner detect that cover approach designed operate monotonicity horn logic past examples retracted memory covered clause. second shortcoming concerns theory revision related standard θ-subsumption-based heuristics used horn logic known inapplicable general case normal logic programs systems construct clauses either bottom-up top-down manner i.e. searching general speciﬁc hypotheses respectively space ordered θ-subsumption. acceptable strategy guide search horn logic case moving subsumption lattice i.e. speciﬁc general increases example coverage moving down general speciﬁc restricts example coverage. always hold normal logic programs generalizing single clause hypothesis result less examples covered hypothesis. result revising hypothesis clause-by-clause manner using subsumption guide search cannot used full clausal logic. illustrate case simple example. clause states ﬁghting persons initiated exhibits abrupt behavior inactive distance less pixel positions video frame. clause states ﬁghting terminated people walks. clause specialization dictates ﬁghting persons terminated walks away. consider hypotheses observe sdec incomplete hypothesis cover positive example holdsat indeed means clause ﬂuent ﬁghting terminated time thus hold time hand hypothesis sdec cover positive example time clause terminate ﬁghting ﬂuent time thus hypothesis though speciﬁc covers examples. recently number hybrid ilp-alp systems proposed able overcome aforementioned shortcomings. xhail system basis approach learning event deﬁnitions streams event-based knowledge. next give detailed account xhail. examples narrative annotation mode declarations axioms sdec background knowledge. mode declarations specify atoms allowed heads clauses literals allowed bodies clauses input modeh/ modeb/ predicates respectively. variable ground placemarkers indicated terms form +type type respectively. variables mode declarations shown table either type representing person type time. ground term allowed generated literals type dist representing euclidean distance persons. annotation says ﬁghting persons holds time hold times hence terminated time respectively ﬁghting persons holds time hold times hence initiated time xhail obtains explanations holdsat/ literals annotation abductively using modeh atoms mode declarations abducible predicates. ﬁrst phase derives ground atoms presented phase table second phase xhail forms kernel presented phase table generating clause abduced atom using atom head body literals kernel multi-clause version bottom clause concept widely used inverse entailment systems like progol aleph. systems construct hypotheses clause time using positive example seed most-speciﬁc bottom clause generated inverse entailment good terms heuristic function hypothesis clause constructed search space clauses subsume bottom clause. contrast kernel generated positive examples once xhail performs search space theories subsume order arrive good hypothesis. necessary diﬃculties mentioned section related non-monotonicity typical systems learn clause time. another important diﬀerence kernel bottom clause latter constructed seed example must provided supervision former also utilize atoms derived abductively background knowledge allowing successfully address non-opl problems mentioned section notes input mode declarations. term kernel clause corresponds variable indicated mode declarations replaced actual variable term corresponds ground term retained intact. example table variabilized kernel presented phase variable placemarkers mode declarations indicate input variables meaning corresponding variable either appear head clause output variable preceding rations table variable appears body clause third phase xhail functionality concerns actual search hypothesis. contrary inverse entailment systems like progol aleph rely heuristic search xhail performs complete search space theories subsume order ensure soundness generated hypothesis. search biased minimality i.e. preference towards hypotheses fewer literals. hypothesis thus constructed dropping many literals clauses possible correctly accounting examples. subject syntactic transformation clauses involves predicates try/ use/ head atom clause contribute towards coverage example try) atoms must succeed. means rules added atom achieved ways either assuming satisfying abducing use. hypothesis clause constructed head atom i-th clause abduced j-th body literal abduced atom. clauses literals discarded. bias towards hypotheses fewer literals realized means abducing minimal use/ atoms. example presented next task phase table minimal abductive explanation task. correspond head atoms clauses correspond respectively third second body literal. output hypothesis table constructed literals literals clauses discarded. xhail provides appropriate framework learning event definitions form event calculus programs. however serious obstacle prevents xhail widely applicable machine learning system event recognition scalability. xhail scales poorly partly increased computational complexity adbuction lies core functionality partly combinatorial complexity learning whole theories result intractable search space. follows xhail machinery develop incremental algorithm scales large volumes sequential data typical event-based applications. begin presentation approach call iled deﬁning incremental setting assume elaborating main challenges stem setting. present basic ideas allow address challenges proceed detailed description method. main challenge adopting full memory approach scale growing size experience. line requirement incremental learning incorporation experience memory learning computationally eﬃcient theory revision must eﬃcient ﬁtting incoming observations stream processing literature number passes stream data often used measure eﬃciency algorithms spirit main contribution iled addition scaling xhail adopts single-pass theory revision strategy strategy requires single-pass revision strategy trivial. instance addition clause response examples implies must checked throughout case covers negative examples specialized turn aﬀect initial coverage specialization results rejection positive examples extra clauses must generated added order retrieve lost positives clauses checked correctness process continues hypothesis found accounts examples since experience grow time extent impossible maintain working memory follow external memory approach implies learner access past experience whole independent sets training data form sliding windows. sliding windows suﬃciently large capture temporal dependencies data imposed sdec axioms make truth value ﬂuent iled’s high-level strategy presented algorithm time iled presented hypothesis accounts historical memory example window hypothesis hand covers window returned otherwise iled starts process revising revision operators retract knowledge deletion clauses antecedents excluded exponential cost backtracking historical memory supported revision operators thus given running hypothesis window goal algorithm retain preservable clauses intact reﬁne revisable clauses necessary generate clauses account examples incoming window deﬁnition provides formal account preservable revisable clauses. revisions implemented revise function figure illustrates function simple example. clauses generated generalizing kernel incoming window shown figure terminatedat/ clause generated window moreover facilitate reﬁnement existing clauses clause running hypothesis associated memory examples covers throughout form bottom program call support set. support constructed gradually example windows arrive. serves reﬁnement search space shown figure single clause running hypothesis reﬁned w.r.t. incoming window specializations. specialization results adding initial clause antecedent support clauses presented figure revised hypothesis constructed reﬁned clauses ones along preserved clauses historical memory reconsulted clauses generated kernel window clauses checked example window reﬁned necessary. generated window consistent throughout line features iled contribute towards scalability first re-processing past experience necessary case clauses generated redundant case revision consists reﬁnements existing clauses. second shown iteration lines algorithm re-processing past experience requires single pass historical memory meaning suﬃces re-see past window exactly ensure output revised hypothesis complete consistent w.r.t. entire historical memory. properties iled support present detail. intuition behind support stems xhail methodology. given examples xhail learns hypothesis generalizing kernel examples. large process possible solution partition smaller example sets learn hypothesis accounts whole gradually revising initial hypothesis acquired process progressive revisions compressive memory small kernel sets used surrogate fact able reason whole kernel role support set. means memory clause reﬁnement concerned iled able repair problems locally i.e. single example window without aﬀecting coverage parts historical memory clause reﬁnement previously checked preservable. detail given hypothesis clause window must reﬁned denoting part know preservable iled reﬁnes reﬁnement covers positive examples covers making task checking response reﬁnement redundant. order formally deﬁne properties proposed memory structure notions depth-bound mode language most-speciﬁc clause. intuitively given mode declarations non-negative integer clause .supp deﬁne space specializations bound covers maximal clause length. words every .supp covers least example cove property deﬁnition ensures space contains reﬁnements however ensure support indeed used reﬁnement search space must ensure .supp always contain reﬁnement i.e. preservable program w.r.t. given window replace case latter revisable w.r.t. proposition shows indeed case. proposition incremental learning setting i.e. sdec example window. assume also exists hypothesis sdec clause revisable w.r.t. window .supp contains reﬁnement preservable w.r.t. proof assume towards contradiction reﬁnement contained .supp revisable w.r.t. follows .supp revisable w.r.t. i.e. either covers negative examples disproves positive examples example .supp fails satisfy assume simplicity single clause .supp responsible that. deﬁnition covers least positive example furthermore most-speciﬁc clause within property. i.e. exists hypothesis sdec contradicts assumption. hence .supp preservable w.r.t. thus contains reﬁnement preservable w.r.t. construction support presented algorithm process starts added running hypothesis continues long example windows arrive. happens clause reﬁned retained support updated accordingly. details algorithm presented example also demonstrates iled processes incoming examples revises hypotheses. example consider annotated examples running hypothesis related ﬁghting high-level event activity recognition application shown table assume iled starts empty hypothesis empty historical memory ﬁrst input example window. currently empty hypothesis cover provided examples since ﬁghting between persons initiated time thus holds time hence iled starts process generating initial hypothesis. case empty hypothesis iled reduces xhail operates kernel only trying induce minimal program accounts examples variabilized kernel case single-clause program presented table generated corresponding ground clause. generalizing kernel yields minimal hypothesis covers hypothesis window arrives next positive examples initiation ﬁghting. running hypothesis revisable window since clause covers negative example time means initiating ﬂuent ﬁghting time address issue iled searches .supp serves reﬁnement search space reﬁnement rejects negative examreﬁnement since cover negative example subsumes .supp. iled however biased towards minimal theories terms overall number literals would prefer compressed reﬁnement shown table also rejects negative example subsumes .supp. clause replaces initial clause running hypothesis. hypothesis becomes complete consistent throughout note hypothesis reﬁned local reasoning only i.e. reasoning within window support avoiding costly look-back historical memory. support clause initialized selecting subset support parent clause θ-subsumed shown example support clause compressed enumeration examples covers throughout historical memory. compressed expected encode many examples single variabilized clause. contrast ground version support would plain enumeration examples since general case would require ground clause example. main advantage lifted character support plain enumeration examples requires much less memory encode necessary information important feature large-scale applications. moreover given training examples typically characterized heavy repetition abstracting away redundant parts search space results memory structure expected grow size slowly allowing fast search scales large amount historical data. algorithm presents details revise function algorithm input consists sdec background knowledge running hypothesis example window variabilized kernel subject generalizationtansformation reﬁnementtransformation respectively presented table former transformation discussed section turns kernel defeasible program allowing construct clauses kernel select order cover examples. reﬁnementtransformation aims reﬁnement clauses using support sets. involves fresh predicates exception/ use/. program used background theory along sdec. minimal use/ use/ atoms abduced solution abductive task line algorithm abduced use/ atoms used construct newclauses discussed section clauses account examples cannot covered existing clauses abduced use/ atoms indicate clauses must reﬁned. atoms reﬁnement generated incorrect clause di.supp use. atoms indicate specialization generated adding body literals reﬁnement .supp generated selecting specialization clause support clause .supp. example table presents process iled’s reﬁnement. annotation lacks positive examples running hypothesis consists single clause support clauses. clause inconsistent since entails negative holdsat program results applying reﬁnementtransformation support clause presented table along minimal abductive explanation examples terms use/ atoms. atoms correspond respectively second third body literals ﬁrst support clause added body clause resulting ﬁrst specialization presented table third abduced atom corresponds second body literal second support clause results second specialization table together specializations form reﬁnement clause subsumes .supp. minimal abductive solutions imply running hypothesis minimally revised. revisions minimal w.r.t. length clauses revised hypothesis minimal w.r.t. number clauses since reﬁnement strategy described result reﬁnements include redundant clauses selecting randomly specialization support clause generate reﬁnement clause sub-optimal since exist reﬁnements fewer clauses also subsume whole support example demonstrates. avoid unnecessary increase hypothesis size generation reﬁnements followed reduction step reducereﬁned function works follows. reﬁned clause ﬁrst generates possible reﬁnements .supp. realized abductive reﬁnement technique described above. diﬀerence abductive solver instructed abductive explanations terms use/ atoms instead one. reﬁnements generated reducereﬁned searches revised case clause replaced reﬁnement .supp. property support deﬁnition covers positive examples covers hence hypothesis holds sdec furthermore sdec hence sdec soundness follows. case constructed single step i.e. reasoning within without re-seeing windows clause results generalization kernel response clause addition window must checked must reﬁned necessary shown line algorithm etested denote fragment tested point time. initially i.e. generated holds etested window tested clause remain intact reﬁned reﬁnements reﬁned. assume ﬁrst window clause must reﬁned. point etested holds preservable etested since reﬁned. clause replaced reﬁnement .supp. preservable etested since reﬁnement preservable clause furthermore covers positive examples covers means properties support complete consistent w.r.t. set. hence hypothesis etested argument shows above) resulting hypothesis remains complete consistent w.r.t. etested hence windows tested i.e. etested resulting hypothesis complete consistent w.r.t. furthermore window re-seen exactly once thus computed single pass non-monotonic xhail particular important properties means extend traditional systems. brieﬂy discussed section properties related challenging issues occur learning normal logic programs non-monotonic addresses robust elegant way. next discuss properties preserved iled sacriﬁced trade-oﬀ eﬃciency brieﬂy indicating directions improvement future work. like xhail iled aims soundness hypotheses cover given examples. xhail ensures soundness generalizing examples contrast iled preserves memory past experience newly acquired knowledge must account. soundness imposes restrictions tasks iled applied. particular assume supervision correct domain stationary sense knowledge already induced remains valid w.r.t. future instances retracting clauses literals hypothesis hand never necessary order account incoming example windows. iled terminates case computations result dead-end returning solution. results treating cases concept drift noise. possible relax requirement soundness implementation best-ﬁts training instances. handling noise concept drift promising extensions iled. xhail state-of-the-art system among inverse entailment-based peer algorithms terms completeness. hypotheses computable xhail form superset computable prominent inverse entailment systems like progol aleph although iled preserves xhail’s soundness preserve completeness properties fact iled operates incrementally gain eﬃciency. thus cases hypothesis discovered xhail missed iled. example consider cases target hypothesis captures long-term temporal relations data instance following clause cases parts data connected long-range temporal relation given diﬀerent windows iled correlate parts order discover temporal relation. however always achieve xhail’s functionality increasing appropriately iled’s window size. additional trade-oﬀ eﬃciency iled’s revisions fully evaluated historical memory. example clause generated kernel incoming window selected randomly among possible choices equally good locally i.e. window quality substantially diﬀer globally. instance selecting particular clause order cover example result large number reﬁnements unnecessarily lengthy hypothesis compared obtained selecting diﬀerent initial clause. hand fully evaluating iled large part theorem proving eﬀort involved clause reﬁnement reduces computing subsumption clauses hard task. moreover historical memory grows time support sets clauses running hypothesis increasing cost computing subsumption. however principle largest part search space redundant support focuses interesting parts would expect support grow size makes subsumption kernel clauses restricted size incoming sliding windows. smaller windows result smaller clauses making computation subsumption relations tractable. addition number optimization techniques developed years several generic subsumption engines proposed able eﬃciently compute subsumption relations clauses comprising thousands literals hundreds distinct variables. basic idea behind iled compress examples bottom clause-like structures order facilitate clause reﬁnement learning hypothesis incrementally. idea behind support generic enough applied inverse entailment system uses bottom clauses guide search order provide support eﬃcient clause reﬁnement. case support modiﬁed accordingly comply search method adopted system. instance work presented here support works xhail’s search procedure minimality-driven full search space theories subsume kernel designed address nonmonotonicity normal logic programs. diﬀerent settings developed. example requirement soundness abandoned eﬀort address noise heuristic search strategy could adopted like example progol’s section present experimental results real-world applications activity recognition using real data benchmark caviar video surveillance dataset well large volumes synthetic caviar data; city transport management using data pronto project. part experimental evaluation aims compare iled xhail. achieve implement xhail original implementation publicly available recently experiments conducted linux machine ram. algorithms implemented python using clingo answer solver main reasoning component mongodb nosql database historical memory examples. code datasets used experiments downloaded http//cer.iit.demokritos.gr/ iled/experiments. activity recognition goal learn deﬁnitions high-level events ﬁghting moving meeting streams low-level events like walking standing active abrupt well spatio-temporal knowledge. benchmark caviar dataset experimentation. details caviar dataset information activity recognition applications found consider instance following deﬁnition ﬁghting high-level event clause dictates period time persons assumed ﬁghting initiated time persons active inactive distance smaller pixel positions. clause states ﬁghting initiated people moves abruptly inactive persons suﬃciently close. clauses state ﬁghting terminated people walks runs away other. caviar contains noisy data mainly human errors annotation thus experiments manually selected noise-free subset caviar. resulting dataset consists examples concerning high-level events moving meeting ﬁghting. data selected diﬀerent parts caviar dataset combined continuous annotated stream narrative atoms time ranging addition real data generated synthetic data basis manually-developed caviar event deﬁnitions described particular streams low-level events concerning four diﬀerent persons created randomly classiﬁed using rules ﬁnal dataset obtained generating negative supervision closed world assumption appropriately pairing supervision narrative. generated data consists approximately examples amounts data. synthetic data much complex real caviar data. main reasons first synthetic data includes signiﬁcantly initiations terminations high-level event thus much larger learning eﬀort required explain second synthetic dataset high-level event initiated terminated time point. results kernel sets clauses hard generalize simultaneously. purpose experiment assess whether iled eﬃciently generate hypotheses comparable size predictive quality xhail. compared systems real synthetic data using -fold cross validation replacement. real data randomly selected examples total used training remaining retained testing. training data presented iled example windows sizes data presented batch xhail. synthetic data examples randomly sampled dataset training remaining data retained testing. similar real data experiments iled operated windows sizes examples xhail single batch. table presents experimental results. training times signiﬁcantly higher xhail increased complexity generalizing kernel sets account whole presented examples once. kernel sets consisted average -literal clauses case real data -literal clauses case synthetic data. contrast iled deal much smaller kernel sets. complexity abductive search aﬀects iled well size input windows grows. iled handles learning task relatively well examples presented windows examples training time increases almost times window size doubled. concerning size produced hypothesis results show case real caviar data hypotheses constructed iled comparable size hypothesis constructed xhail. case synthetic data hypotheses returned xhail iled signiﬁcantly complex. note iled hypothesis size decreases window size increases. reﬂected number revisions iled performs signiﬁcantly smaller input comes larger batches examples. principle richer input better hypothesis initially acquired consequently less need revisions response training instances. tradeoﬀ window size number revisions. small number revisions complex data greater total cost terms training time compared greater number revisions simpler data example case window size real caviar data iled performs revisions average requires signiﬁcantly time case window size performs revisions average. hand training times windows size slightly better obtained examples presented smaller windows size case unit cost performing revisions w.r.t single window comparable windows size thus overall cost terms training time determined total number revisions greater case window size fig. average times needed iled revise initial hypothesis face evidence presented windows size examples. initial hypothesis obtained training varying size subsequently served historical memory. concerning predictive quality results indicate iled’s precision recall scores comparable xhail. larger input windows precision recall almost xhail. iled produces better hypotheses larger input windows. precision recall smaller case synthetic data systems testing case much larger complex case real data. purpose experiment assess scalability iled. experimental setting follows sets examples varying sizes randomly sampled synthetic dataset. example used training order acquire initial hypothesis using iled. window satisfy hypothesis hand randomly selected presented iled subsequently revised initial hypothesis order account historical memory evidence. historical memories ranging examples training window size selected whole dataset. process repeated times diﬀerent combination historical memory window size. figure presents average revision times. revision times window sizes examples close therefore omitted avoid clutter. results indicate revision time grows polynomially size historical memory. goal inform decision-making transport oﬃcials recognising high-level events related punctuality public transport vehicle passenger/driver comfort safety. high-level events requested public transport control centre helsinki finland order support resource management. low-level events provided sensors installed buses trams reporting changes position acceleration/deceleration in-vehicle temperature noise level passenger density. time project available datasets included subset anticipated low-level event types low-level event detection components functional. needs project therefore synthetic dataset generated. synthetic pronto data proven considerably challenging event recognition real data therefore chose former evaluating iled only. deﬁnitions signiﬁcantly complex learn compared non-hierarchical ones. initiations terminations events lower levels hierarchy appear bodies event deﬁnitions higher levels hierarchy hence target deﬁnitions must learnt simultaneously. show experiments striking eﬀect required learning eﬀort. solution simplifying learning task utilize knowledge domain learn event deﬁnitions separately acquired theories lower levels event hierarchy non-revisable background knowledge learning event deﬁnitions higher levels. fragment event hierarchy clauses state period time vehicle said non-punctual initiated enters stop later leaves stop earlier scheduled time. clauses state period vehicle said non-punctual terminated vehicle arrives stop earlier than scheduled time. deﬁnition non-punctual vehicle uses low-level events stopenter stopleave. clauses deﬁne driving quality. essentially driving quality said driving style unsafe vehicle non-punctual. driving quality deﬁned terms high-level events therefore bodies clauses deﬁning driving quality include initiatedat/ terminatedat/ literals. experiment tried learn simultaneously deﬁnitions target concepts total nine interrelated high-level events seven level- level- level-. according employed language bias high-level event must learnt time present body another high-level event form holdsat/ initiatedat/ terminatedat/ predicate. total number low-level events involved used tenfold cross validation replacement small amounts data complexity learning task. cross validation randomly sampled examples dataset used training retained testing. example size selected experimentation order xhail able perform acceptable time frame. sample consisted approximately atoms examples given iled windows granularity xhail batch. table presents average training times hypothesis size number revisions precision recall. iled took average hours complete learning task windows examples xhail required hours average learn hypotheses batches examples. compared activity recognition learning setting requires larger kernel structures hard reason with. average kernel generated batch examples consisted approximately clauses literals each. like activity recognition experiments precision recall scores iled comparable xhail latter slightly better. unlike activity recognition experiments precision recall large diversity diﬀerent runs. complexity dataset constructed hypotheses large diversity depending random samples used training. example high-level event deﬁnitions unnecessarily lengthy diﬃcult understood human expert. hand level- deﬁnitions could runs experiment learnt correctly even limited amount data. deﬁnitions fairly simple consisting initiation termination rule body literal case. experiment demonstrates several limitations learning large complex applications. complexity domain increases intensity learning task turn makes training times forbidding even small amount data examples forces process small sets examples time complex domains like results over-ﬁtted theories rapid increase hypothesis size. eﬀort improve experimental results utilized domain knowledge event hierarchy attempted learn high-level events diﬀerent levels separately. learn complete deﬁnition entire dataset high-level event utilizing background knowledge learning process higher-level event. facilitate learning task further also used expert knowledge relation speciﬁc low-level high-level events excluding language bias mode declarations irrelevant high-level event learnt time. experimental setting therefore follows starting level- target events processed whole dataset windows examples iled. high-level event learnt independently others. complete deﬁnitions level- high-level events constructed added background knowledge. proceeded learning deﬁnition single level- event. finally successfully constructing level- deﬁnition performed learning top-level hierarchy using previously constructed level- level- event deﬁnitions background knowledge. attempt comparison xhail since amounts data latter able operate entire dataset. table presents results. level- events scores presented minimummaximum pairs. instance training times level- events windows examples ranges minutes. levels deﬁnition each therefore table presents respective scores run. training times hypothesis sizes overall numbers revisions comparable levels event hierarchy. level- event deﬁnitions easiest acquire training times ranging approximately minutes. expected since clauses level- deﬁnitions signiﬁcantly simpler level- level- ones. level- event deﬁnition hardest construct training times ranging minutes signiﬁcant number revisions required window granularities. deﬁnition highlevel event relatively complex contrast simpler level- deﬁnition training times comparable ones level- events. largest parts training times dedicated checking already correct deﬁnition part dataset processed yet. target events iled converged complete deﬁnition relatively quickly i.e. approximately minutes initiation learning process. point extra time spent testing hypothesis incoming data. window granularity slightly aﬀects produced hypothesis target highlevel events. indeed deﬁnitions constructed windows examples slightly larger ones constructed larger window sizes examples. notably deﬁnitions constructed windows granularity found concise meaningful close actual hand-crafted rules utilized pronto. thorough review drawbacks state-of-the-art systems respect non-monotonic domains well deﬁciencies existing approaches learning event calculus programs found main obstacle common many learners combine form abduction like progol alecto hail imparo cannot perform abduction negation thus essentially limited observational predicate learning. top-down non-monotonic learner able solve class problems xhail. obtains theory appropriately mapping problem hand corresponding instance solutions latter translated solutions initial problem. recently main ideas behind employed aspal system inductive learner relies answer programming unifying abductive-inductive framework. aspal obtains theory skeleton rules forming possible clause structures formed mode declarations. structure complemented properly formed abducible predicates. abductive reasoning proper meta-level representation original problem returns abducibles which construction allow hypothesize variables constants skeleton rules linked together. thus abduced atoms prescriptions variable constant terms original skeleton rules handled order obtain hypothesis. aspal induce possible hypotheses w.r.t certain problem well optimal ones computing minimal sets abducibles. recently applied meta-interpretive learning learning framework goal obtain hypotheses presence meta-interpreter. latter higher-order program hypothesizing predicates even rules domain. given background knowledge sets positive negative examples uses abduction w.r.t. meta-interpreter construct ﬁrst-order hypotheses. realized prolog answer programming implemented metagol system application examples involve learning deﬁnite clause grammars discovering relations entities learning simple robot action strategies elegant framework able address diﬃcult problems under-explored traditional like handling predicate invention learning mutually recursive programs. however number important drawbacks. first expressivity signiﬁcantly limited currently restricted dyadic datalog i.e. datalog arity predicate two. noted constructing meta-interpreters richer fragments ﬁrst-order logic straight-forward task requires careful mathematical analysis. second given increased computational complexity higher-order reasoning scaling large volumes data potential bottleneck mil. non-monotonic setting traditional approaches cover examples sequentially cannot ensure soundness completeness deal issue non-monotonic learners like xhail aspal generalize available examples disadvantage approach however poor scalability. recent advancement addresses issue scalability non-monotonic presented approach combines top-down meta-level learning aspal theory revision non-monotonic address grounding bottleneck aspal’s functionality. theory derived aspal starting point search based combinations available mode declarations grows exponentially length clauses. thus obtaining ground program theory often expensive cause learning task become intractable raspal system proposed addresses issue imposing bounds length theory. partial hypotheses speciﬁed clause length iteratively obtained reﬁnement loop. iteration loop partial hypothesis obtained previous reﬁnement step reﬁned using theory revision described process continues complete consistent hypothesis obtained. authors show approach results shorter ground programs derives complete consistent hypothesis derivable input data. important diﬀerence raspal approach former addresses scalability related application domains require complex language bias approach scales potentially simpler massive volumes sequential data typical temporal applications. aspal raspal top-down learners. work presented here xhail bottom-up non-monotonic learning system natural choice basis approach since intended provide clause reﬁnement search bias means most-speciﬁc clauses work theory revision system forte enhanced porting progol’s bottom construction routine functionality towards eﬃcient reﬁnement operator. resulting system forte works follows clause must reﬁned forte uses mode declarations inverse entailment search background knowledge construct bottom clause positive example covered searches antecedents within bottom clause. case iled constrained search space results eﬃcient clause reﬁnement process. however forte learns horn theories support non-observational predicate learning thus cannot used revision event calculus programs. addition cannot operate empty hypothesis another important diﬀerence forte iled former handles potential incompleteness result specialization clause. particular clause specialized forte checks whole database examples. positive examples become unprovable specialization forte picks diﬀerent positive example covered initial inconsistent clause constructs bottom clause searches specialization clause process continues original coverage example database restored. contrast means support specializations performed iled preserve prior coverage historical memory thus saving inference eﬀort signiﬁcantly. mentioned renewed interest scaling theory revision systems applications last years availability large-scale domain knowledge various scientiﬁc disciplines temporal stream data exception need scalable theory revision techniques event-based domains. however theory revision systems systems described limit applicability horn theories. well-known theory revision system inthelex fully incremental system learns/revises datalog theories used study several aspects incremental learning. particular order eﬀects simple learning tasks discussed concept drift authors present approach towards scaling inthelex. contrast systems keep examples main memory follows external memory implementation approach adopted iled. moreover work authors associate clauses theory hand examples cover relational schema. thus clause reﬁned examples previously covered clause checked. similarly clause generalized negative examples checked again. scalable version inthelex presented maintains alternative versions hypothesis step allowing backtrack previous states. addition keeps memory several statistics related examples system already seen number reﬁnements example caused reﬁnement history clause etc. hand inthelex limitations make inappropriate inducing/revising event calculus programs event recognition applications. first restriction input language datalog limits applicability richer relational event domains. instance complex relations entities cannot easily expressed inthelex. second background knowledge limited excluding instance auxiliary clauses used spatio-temporal reasoning learning time. third although inthelex uses abduction completion imperfect input data relies observational predicate learning meaning able reason predicates directly observable examples. therefore cannot used learning event deﬁnitions. presented incremental system iled machine learning knowledge bases event recognition form event calculus theories. iled combines techniques non-monotonic particular xhail algorithm theory revision. acquires initial hypothesis ﬁrst available piece data revises hypothesis data arrive. revisions account accumulated experience. main contribution iled scales-up xhail large volumes sequential data time-like structure typical event-based applications. means compressive memory structure supports clause reﬁnement iled scalable single-pass revision strategy thanks cost theory revision grows tractable function perceived experience. work iled evaluated activity recognition application transport management application. results indicate iled signiﬁcantly eﬃcient xhail without compromising quality generated hypothesis terms predictive accuracy hypothesis size. moreover iled scales adequately large data volumes xhail cannot handle. future work concerns mechanisms handling noise concept drift.", "year": 2014}