{"title": "Multi-Task Video Captioning with Video and Entailment Generation", "tag": ["cs.CL", "cs.AI", "cs.CV"], "abstract": "Video captioning, the task of describing the content of a video, has seen some promising improvements in recent years with sequence-to-sequence models, but accurately learning the temporal and logical dynamics involved in the task still remains a challenge, especially given the lack of sufficient annotated data. We improve video captioning by sharing knowledge with two related directed-generation tasks: a temporally-directed unsupervised video prediction task to learn richer context-aware video encoder representations, and a logically-directed language entailment generation task to learn better video-entailed caption decoder representations. For this, we present a many-to-many multi-task learning model that shares parameters across the encoders and decoders of the three tasks. We achieve significant improvements and the new state-of-the-art on several standard video captioning datasets using diverse automatic and human evaluations. We also show mutual multi-task improvements on the entailment generation task.", "text": "also step forward static image captioning addition modeling spatial visual features model also needs learn temporal across-frame action dynamics logical storyline language dynamics. previous work video captioning shown recurrent neural networks good choice modeling temporal information video. sequence-to-sequence model used ‘translate’ video caption. venugopalan showed linguistic improvements fusing decoder external language models. furthermore attention mechanism video frames caption words captures temporal matching relations better recently hierarchical two-level rnns proposed allow longer inputs model full paragraph caption dynamics long video clips despite recent improvements video captioning models still suffer lack sufﬁcient temporal logical supervision able correctly capture action sequence storydynamic language videos esp. case short clips. hence would beneﬁt incorporating complementary directed knowledge visual textual. address jointly training task video captioning related directed-generation tasks temporallyvideo captioning task describing content video seen promising improvements recent years sequence-to-sequence models accurately learning temporal logical dynamics involved task still remains challenge especially given lack sufﬁcient annotated data. improve video captioning sharing knowledge related directed-generation tasks temporally-directed unsupervised video prediction task learn richer context-aware video encoder representations logically-directed language entailment generation task learn better video-entailed caption decoder representations. this present many-to-many multi-task learning model shares parameters across encoders decoders three tasks. achieve signiﬁcant improvements state-ofthe-art several standard video captioning datasets using diverse automatic human evaluations. also show mutual multi-task improvements entailment generation task. video captioning task automatically generating natural language description content video shown fig. various applications assistance visually impaired person improving quality online video search retrieval. task gained recent momentum natural language processing computer vision communities esp. advent powerful image processing features well sequence-to-sequence lstm models. directed unsupervised video prediction task logically-directed language entailment generation task. model many-to-many multi-task learning based sequence-to-sequence models allow sharing parameters among encoders decoders across three different tasks additional shareable attention mechanisms. i.e. video-to-video generation shares encoder video captioning task’s encoder helps learn richer video representations predict temporal context action sequence. entailment generation task i.e. premise-to-entailment generation shares decoder video captioning decoder helps learn better video-entailed caption representations since caption essentially entailment video i.e. describes subsets objects events logically implied full video content. overall many-tomany multi-task model combines three tasks. three novel multi-task models show statistically signiﬁcant improvements state-ofthe-art achieve best-reported results multiple datasets based several automatic human evaluations. also demonstrate video captioning turn gives mutual improvements multi-reference entailment generation task. early video captioning work used two-stage pipeline ﬁrst extract subject verb object triple generate sentence based venugopalan mean-pooled static frame-level visual features video input language decoder. harness important frame sequence temporal ordering venugopalan proposed sequence-to-sequence model video encoder language decoder rnns. recently venugopalan explored linguistic improvements caption decoder fusing external language models. moreover attention alignment mechanism added encoder decoder learn temporal relations video frames caption words contrast static visual features also considered temporal video features d-cnn model pretrained action recognition task. explore long range temporal relations proposed two-level hierarchical encoder limits length input information allows temporal transitions segments. hierarchical generates sentences ﬁrst level second level captures inter-sentence dependencies paragraph. proposed simultaneously learn word probabilities visual-semantic joint embedding space enforces relationship semantics entire sentence visual content. despite useful recent improvements video captioning still suffers limited supervision generalization capabilities esp. given complex action-based temporal story-based logical dynamics need captured short video clips. work addresses issue bringing complementary temporal logical knowledge video prediction textual entailment generation tasks training together many-to-many multi-task learning. learning paradigm improve supervision generalization performance task jointly training related tasks recently luong combined multi-task learning sequence-to-sequence models sharing parameters across tasks’ encoders decoders. showed improvements machine translation using parsing image captioning. additionally incorporate attention mechanism many-to-many multi-task learning approach improve multimodal temporal-logical video captioning task sharing video encoder encoder video-to-video prediction task sharing caption decoder decoder linguistic premise-to-entailment generation task. image representation learning successful supervision large object-labeled datasets. however similar amounts supervision lacking video representation learning. srivastava address proposmodel ﬁnal state encoder input initial state decoder shown fig. based long short term memory units good memorizing long sequences forget-style gates video captioning input encoder video frame features length caption word sequence length generated decoding phase. distribution output sequence w.r.t. input sequence attention model architecture similar bahdanau bidirectional lstmrnn encoder unidirectional lstmrnn decoder fig. time step decoder lstm hidden state nonlinear recurrent function previous decoder hidden state previous time-step’s generated word context vector unsupervised video representation learning sequence-to-sequence models reconstruct input video sequence predict future sequence. model video generation attention-enhanced encoder-decoder harness improve video captioning. recognizing textual entailment classify whether relationship between premise hypothesis sentence entailment contradiction independence helpful several downstream tasks. recent stanford natural language inference corpus bowman allowed training end-to-end neural networks outperform earlier feature-based models however directly generating entailed hypothesis sentences given premise sentence would even beneﬁcial retrieving reranking sentence pairs downstream generation tasks come source sentence pairs. recently kolesnyk tried sequenceto-sequence model original snli dataset single-reference setting hence restricts automatic evaluation. modify snli corpus multi-reference setting present novel multi-task training setup related video captioning task showing mutual improvements tasks. ﬁrst discuss simple encoder-decoder model baseline reference video captioning. next improve attention mechanism. finally present similar models unsupervised video prediction entailment generation tasks combine video captioning many-to-many multi-task approach. figure many-to-many multi-task learning model share encoders decoders video captioning unsupervised video prediction entailment generation tasks. helps video encoder learn rich temporal representations aware action-based context also robust missing frames varying frame lengths motion speeds. optimization function deﬁned given sentence task entailment generation generate sentence logical deduction implication premise. entailment generation model uses bidirectional lstm-rnn encoder lstm-rnn decoder attention mechanism premise sequence words hypothesis distribution entailed hypothesis w.r.t. premise learned parameters. attention-based sequence-to-sequence model enhanced baseline video captioning. next discuss similar models tasks unsupervised video prediction entailment generation ﬁnally share multi-task learning. model unsupervised video representation predicting sequence future video frames given current frame sequence. similar sec. bidirectional lstm-rnn encoder lstm-rnn decoder used along atframe level features video tention. length divided sets given current frames model predict rest frames {fk+ fn}. motivation multi-task learning helps sharing information different tasks across domains. primary improve video captioning model visual content translates textual form directed generation way. hence presents interesting opportunity share temporally logically directed knowledge visual linguistic generation tasks. fig. shows overall many-to-many multi-task model jointly learning video captioning unsupervised video prediction textual entailment generation. here video captioning task shares video encoder encoder video prediction task learn context-aware temporally-directed visual representations decoder video captioning task shared decoder textual entailment generation task thus helping generate captions ‘entailed’ i.e. logically implied follow video content one-to-many many-to-one settings also allow attention parameters shared separated. overall many-to-many setting thus improves visual language representations video captioning model. train multi-task model alternately optimizing task mini-batches based mixing ratio. number mini-batches optimized alternately three tasks video captioning unsupervised video future frames prediction entailment generation resp. mixing ratio deﬁned empirically logical entailment helped captioning simple fusion language modeling caption also ‘entailed’ video logically-directed sense hence entailment generation task matches video captioning task better language modeling. moreover multi-task setup suitable directed information entailment details sec. tains youtube videos wild several different reference captions video also msr-vtt diverse video clips video clipsentence pairs around captions video; m-vad movie-based video clips captions video making evaluation metrics infeasible. standard splits three datasets. details datasets provided supplementary. video prediction dataset unsupervised video representation learning task ucf- action videos dataset contains video clips action categories suits video captioning task well also contains short video clips single action actions. standard splits details supplementary. entailment generation dataset entailment generation encoder-decoder model stanford natural language inference corpus contains human-annotated english sentence pairs classiﬁcation labels entailment contradiction neutral. total sentence pairs correspond true entailment pairs subset multi-task video captioning model. improving video captioning training/validation/test splits provided bowman training validation testing pairs however entailment generation multitask results modify splits create multi-reference setup afford evaluation automatic metrics. given premise usually multiple entailed hypotheses original snli corpus single-reference this different entailed hypotheses premise land different splits dataset many cases. therefore regroup premiseentailment pairs modify split follows among premise-entailment pairs subset snli corpus unique premises; hypothesis make training rest hypothesis randomly shufﬂe divide equally test validation sets sets approximately distribution number reference hypotheses premise. validation test sets hence contain premises multiple entailed hypotheses ground truth references thus allowing automatic metric evaluation differing generations still positive scores matching multiple references. also creates challenging dataset entailment generation because zero premise overlap training val/test sets. make split details publicly available. pre-trained visual frame features three video captioning ucf- datasets sampling rate bring uniformity temporal representation actions across videos. sampled frames converted features using several stateof-the-art pre-trained models imagenet vggnet googlenet inception-v details feature dimensions layer positions supplementary. video captioning well entailment generation results four diverse automatic evaluation metrics popular image/video captioning language generation general meteor bleu- cider-d rouge-l particularly meteor cider-d justiﬁed better generation tasks cider-d uses consensus among number references meteor uses soft matching based stemming paraphrasing wordnet synonyms. standard evaluation code microsoft coco server obtain results also compare results previous papers. tune hyperparameters splits lstm-rnn hidden state size learning rate weight initializations mini-batch mixing ratios following settings models unroll video encoder/decoder rnns time steps language encoder/decoder rnns time steps. -dimension hidden state size -dim vectors embed visual features word vectors. adam optimizer apply dropout subsections supp full details. table presents primary results youtubetext dataset reporting several previous works baselines attention model ablations three multi-task models using four automated evaluation metrics. subsection below reported important training details inline refer supplementary full details baseline performance ﬁrst present baseline model choices table baselines represent standard sequence-tosequence model three different visual feature types well attention mechanisms. baseline model trained three random seed initializations average reported ﬁnal baseline model instead uses ensemble standard denoising method performs inference randomly initialized models i.e. time step decoder generate word based avg. likelihood probabilities models. moreover beam search size baseline models. overall ﬁnal baseline model inceptionv features attention -ensemble performs table primary video captioning results youtubetext showing previous works several strong baselines three multi-task models. here short vggnet googlenet inception-v alexnet visual features; ensemble. multi-task models applied best video captioning baseline ensemble. multi-task models statistically signiﬁcant baseline multi-task video prediction here video captioning unsupervised video prediction tasks share encoder lstm-rnn weights image embeddings one-to-many multi-task setting. important hyperparameters tuned ratio encoder decoder frames video prediction ucf- mini-batch mixing ratio captioning video prediction tasks table shows statistically signiﬁcant improvement metrics comparison best baseline model well w.r.t. previous works demonstrating effectiveness multi-task learning video captioning video prediction even unsupervised signals. multi-task entailment generation here video captioning entailment generation tasks share language decoder lstm-rnn weights word embeddings many-to-one multi-task setting. observe mixing ratio alternating minibatches works well here. again table shows statistically signiﬁcant improvements metrics comparison best baseline model multi-task setting. note initial experiments entailment generation model helped video captioning task signiﬁcantly alternative approach simply improving ﬂuency adding external language model decoder caption also ‘entailed’ video logically-directed sense hence matches captioning task better table multi-task video entailment generation combining one-tomany many-to-one multi-task learning models full model -task many-to-many model video encoder language decoder video captioning model shared unsupervised video prediction entailment generation models respectively. mixing ratio alternate mini-batches statistical signiﬁcance four metrics. found setting unshared attention parameters work best likely video captioning video prediction prefer different alignment distributions. video captioning unsupervised video prediction entailment generation resp. works well. table shows many-to-many multi-task model outperforms strongest baseline well previous state-of-theart results large absolute margins metrics. also achieves signiﬁcant improvements metrics one-to-many many-toone models. overall achieve best results date youtubetext metrics. table also train evaluate ﬁnal many-to-many multi-task model video captioning datasets first evaluate msr-vtt dataset since recent dataset list previous works’ results reported msr-vtt dataset paper itself. improve signiﬁcantly. moreover maintain leaderboard dataset also report systems based ranking method multi-task model achieves rank leaderboard. table evaluate model challenging movie-based m-vad dataset achieve improvements previous work hyperparameter details presented supplementary. table presents entailment generation results baseline multi-task model uses video captioning baseline. mixing ratio alternate mini-batches entailment generation video captioning works well. multitask model achieves stat. signiﬁcant improvements baseline metrics thus demonstrating video captioning entailment generation mutually help other. addition automated evaluation metrics present pilot-scale human evaluations youtubetext entailment generation results. case compare strongest baseline ﬁnal multi-task model evaluate random sample generated captions test across three human evaluators. remove model identity anonymize models human evaluators choose better model based relevance coherence shown table table following previous work meteor because m-vad single reference caption video. note many-to-one model prefers different mixing ratio learning rate many-to-one model improving video captioning hyperparameters depend primary task improved also discussed previous work figure examples generated video captions youtubetext dataset complex examples multi-task model performs better baseline; ambiguous examples multi-task model still correctly predicts possible categories complex examples models perform poorly. multi-task models always better strongest baseline video captioning entailment generation relevance coherence similar improvements automatic metrics fig. shows video captioning generation results youtubetext dataset ﬁnal m-to-m multi-task model compared strongest attention-based baseline model three categories videos complex examples multi-task model performs better stilts playing tuba money boardwalk child dressed spiderman ringing doorbell several young people table playing poker woman dress children blue silver monster truck making huge jump crushed cars baseline; ambiguous examples multi-task model still correctly predicts possible categories complex examples models perform poorly. overall multi-task model generates captions better temporal action prediction logical entailment w.r.t. ground truth captions. supplementary also provides ablation examples improvements -to-m video prediction based multi-task model alone well m-to- entailment based multi-task model alone analyzing cases baseline better ﬁnal m-to-m multi-task model often scenarios multitask model’s caption also correct baseline caption speciﬁc e.g. holding shooting gun. finally table presents output examples entailment generation multi-task model showing model accurately learns produce logically implied subsets premise. presented multimodal multi-task learning approach improve video captioning incorporating temporally logically directed knowledge video prediction entailment generation tasks. achieve best reported results three datasets based multiple automatic human evaluations. also show mutual multi-task improvements entailment generation task. future work applying entailment-based multi-task paradigm thank anonymous reviewers helpful comments. work partially supported google faculty research award faculty award bloomberg data science research grant nvidia awards. youtubetext msvd microsoft research video description corpus youtubetext used primary video captioning experiments. youtube videos wild many diverse captions multiple languages video. caption annotations videos collected using amazon mechanical turk experiments english captions. average video captions overdataset unique video-caption pairs. average clip duration roughly seconds. used standard split stated venugopalan i.e. videos training videos validation testing. msr-vtt msr-vtt recent collection video clips hours duration annotated workers. video clip-sentence pairs covering diverse content commercial video search engine. average clip annotated natural language captions. used standard split provided i.e. video clips training validation testing. m-vad m-vad movie description dataset video clips collected movies average clip duration seconds. alignment descriptions video clips done automatic procedure using descriptive video service provided movies. video clip description sentences making evaluation metrics infeasible. again unsupervised video representation learning task ucf- action videos dataset contains video clips action categories average clip length seconds each. dataset suits video captioning task well contain short video clips single action actions hence using future frame prediction ucf- helps learn robust context-aware video representations short clip video captioning task. standard split videos training datasets unsupervised video prediction dataset sampling rate bring uniformity temporal representation actions across videos. sampled frames converted features using several state-of-theart pre-trained models imagenet vggnet googlenet inception-v vggnet layer features dimension googlenet inception-v layer fully connected layer dimensions respectively. follow standard preprocessing convert natural language descriptions lower case tokenize sentences remove punctuations. experiments tune model hyperparameters validation corresponding dataset. consider following short hyperparameters ranges tune lightly lstm-rnn hidden state size learning rate range uniform intervals log-scale; weight initializations range mixing ratios range uniform intervals log-scale. following settings models unroll video encoder/decoder lstm-rnns time steps language encoder/decoder lstm-rnns time steps. -dimension lstm-rnn hidden state size. -dimension vectors embed frame level visual features word vectors. embedding weights learned training. adam optimizer default coefﬁcients batch size apply dropout probability vertical connections lstm reduce overﬁtting. model video captioning unsupervised video prediction tasks share encoder lstm-rnn weights image embeddings one-to-many multi-task setting. learning rate initialize learnable weights uniform distribution range important hyperparameters tuned ratio encoder decoder frames video prediction ucf- mini-batch mixing ratio captioning video prediction tasks many-to-many three-task model video encoder shared video captioning unsupervised video prediction tasks language decoder shared video captioning entailment generation tasks. learning rate trainable weights initialized uniform distribution range found mixing ratio alternative mini-batches video captioning unsupervised video prediction entailment generation works best. also evaluate many-to-many multi-task model video captioning datasets. msr-vtt train model using learning rate trainable weights initialized uniform distribution range found mixing ratio alternative mini-batches video captioning unsupervised video prediction entailment generation works best. m-vad dataset dimension hidden vectors lstms reduce overﬁtting. initialize lstm weights uniform distribution range weights uniform distribution range learning rate found mixing ratio alternative mini-batches video captioning unsupervised video prediction entailment generation works best. model video captioning entailment generation tasks share language decoder lstm-rnn weights word embeddings many-to-one multi-task setting. learning rate trainable weights initialized uniform distribution range observe mixing ratio alternating minibatches works well here. here video captioning turn help improve entailment generation results. hyperparameters baseline multi-task model learning rate trainable weights initialized uniform distribution range found mixing ratio alternate mini-batches training entailment generation video captioning perform best. ground-truth video captions drinks glass water drinks something scores playing basketball young dribbles throws basketball person cuts piece blue paper woman cutting paper square scissor cutting meat chopping chicken woman slicing onions woman chopping onion train going track near shore high speed train running track ground-truth video captions walking ground sneaking grass woman applying liner woman applies makeup brows baby tiger playing tiger playing woman driving motorcycle gave woman ride motorcycle walking treadmill exercising baby puppy playing sofa puppy running around sofa", "year": 2017}