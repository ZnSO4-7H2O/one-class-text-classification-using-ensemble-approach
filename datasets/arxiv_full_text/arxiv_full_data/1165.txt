{"title": "Deep Convolutional Inverse Graphics Network", "tag": ["cs.CV", "cs.GR", "cs.LG", "cs.NE"], "abstract": "This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that learns an interpretable representation of images. This representation is disentangled with respect to transformations such as out-of-plane rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative results of the model's efficacy at learning a 3D rendering engine.", "text": "paper presents deep convolution inverse graphics network model learns interpretable representation images. representation disentangled respect transformations out-of-plane rotations lighting variations. dc-ign model composed multiple layers convolution de-convolution operators trained using stochastic gradient variational bayes algorithm propose training procedure encourage neurons graphics code layer represent speciﬁc transformation given single input image model generate images object variations pose lighting. present qualitative quantitative results model’s efﬁcacy learning rendering engine. deep learning remarkable breakthroughs automatically learning hierarchical representations images. models convolutional neural networks restricted boltzmann machine-based generative models auto-encoders successfully applied produce multiple layers increasingly abstract visual representations. however relatively little work characterizing optimal representation data. cohen considered problem proposing theoretical framework learn irreducible representations invariances equivariances coming best representation given task open question. various work done theory practice representation learning work consistent desiderata representations emerged invariance meaningfulness representations abstraction disentanglement. particular bengio propose disentangled representation changes encoded data sparse real-world transformations; changes latents time able represent sequences likely happen real world. vision inverse graphics model suggests representation images provides features. computer graphics consists function compact descriptions scenes images graphics code typically disentangled allow rendering scenes ﬁne-grained control transformations object location pose lighting texture shape. encoding designed easily interpretably represent sequences real data common transformations compactly represented software code; criterion almost identical bengio graphics codes conveniently align properties ideal representation. figure model architecture deep convolutional inverse graphics network encoder decoder. follow variational autoencoder architecture variations. encoder consists several layers convolutions followed max-pooling decoder several layers unpooling followed convolution. during training data passed encoder produce posterior approximation consists scene latent variables pose light texture shape. order learn parameters dc-ign gradients back-propagated using stochastic gradient descent using following variational object function −log) kl||p every force dc-ign learn disentangled representation showing mini-batches inactive active transformations test data passed encoder latents images re-rendered different viewpoints lighting conditions shape variations manipulating appropriate graphics code group would manipulate off-the-shelf graphics engine. recent work inverse graphics follows general strategy deﬁning probabilistic deterministic model latent parameters using inference optimization algorithm appropriate latent parameters given observations. recently tieleman moved beyond two-stage pipeline using generic encoder network domainspeciﬁc decoder network approximate rendering function. however none approaches shown automatically produce semantically-interpretable graphics code learn rendering engine reproduce images. paper present approach learning interpretable graphics codes complex transformations out-of-plane rotations lighting variations. given images hybrid encoder-decoder model learn representation disentangled respect various transformations object out-of-plane rotations lighting variations. achieve this employ deep directed graphical model many layers convolution de-convolution operators trained using stochastic gradient variational bayes algorithm propose training procedure encourage group neurons graphics code layer distinctly represent speciﬁc transformation. learn disentangled representation train using data mini-batch active inactive transformations provide target values supervised learning; objective function remains reconstruction quality. example nodding face would elevation transformation active shape texture afﬁne transformations would inactive. exploit type training data force chosen neurons graphics code layer speciﬁcally represent active transformations thereby automatically creating disentangled representation. given single face image model regenerate input image different pose lighting. present qualitative quantitative results model’s efﬁcacy learning rendering engine. mentioned before number generative models proposed literature obtain abstract visual representations. unlike rbm-based models approach trained using back-propagation objective function consisting data reconstruction variational bound. relatively recently kingma proposed sgvb algorithm learn generative models continuous latent variables. work feed-forward neural network used approximate posterior distribution decoder network serves enable stochastic reconstruction observations. order handle ﬁne-grained geometry faces work relatively large scale images approach extends applies sgvb algorithm jointly train utilize many layers convolution de-convolution operators encoder decoder network respectively. decoder network function transform compact graphics code image. propose using unpooling followed convolution handle massive increase dimensionality manageable number parameters. recently proposed using cnns generate images given object-speciﬁc parameters supervised setting. approach requires ground-truth labels graphics code layer cannot directly applied image interpretation tasks. work similar ranzato whose work amongst ﬁrst generic encoder-decoder architecture feature learning. however comparison proposal model trained layer-wise intermediate representations disentangled like graphics code approach variational autoencoder loss approximate posterior distribution. work also similar spirit comparison model assume lambertian reﬂectance model implicitly constructs representations. another piece related work desjardins used spike slab prior factorize representations generative deep network. comparison existing approaches important note encoder network produces interpretable disentangled representations necessary learn meaningful graphics engine. number inverse-graphics inspired methods recently proposed literature however methods rely hand-crafted rendering engines. exception work hinton tieleman transforming autoencoders domain-speciﬁc decoder reconstruct input images. work similar spirit works differences uses generic convolutional architecture encoder decoder networks enable efﬁcient learning large datasets image sizes; handle single static frames opposed pair images required generative. shown figure basic structure deep convolutional inverse graphics network consists parts encoder network captures distribution graphics codes given data decoder network learns conditional distribution produce approximation given disentangled representation containing factored latent variables pose light shape. important learning meaningful approximation graphics engine helps tease apart generalization capability model respect different types transformations. denote encoder output dc-ign encoder. encoder output used parametrize variational approximation chosen multivariate normal distribution. reasons using parametrization gradients samples respect parameters easily obtained using reparametrization trick proposed various statistical shape models trained scanner data faces multivariate normal latent distribution given model parameters connect distribution parameters latents expressed main goal work learn representation data consists disentangled semantically interpretable latent variables. would like small subset latent variables change sequences inputs corresponding real-world events. natural choice target representation information scenes already designed graphics engines. deconstruct face image splitting variables pose light shape trivially represent transformations variables used graphics applications. figure depicts representation attempt learn. achieve goal perform training procedure directly targets deﬁnition disentanglement. organize data mini-batches corresponding changes single scene variable transformations might occur real world. term extrinsic variables represented components encoding. also generate mini-batches three extrinsic scene variables held ﬁxed properties face change. batches consist many different faces viewing conditions pose. intrinsic properties model describe identity shape expression etc. represented remainder latent variables minibatches varying intrinsic properties interspersed stochastically varying extrinsic properties. select random latent variable ztrain wish correspond {azimuth select random mini-batch variable changes. show network example minibatch capture latent representation calculate average representation vectors entire batch. putting encoder’s output decoder replace values ztrain calculate reconstruction error backpropagate sgvb decoder. replace gradients latents ztrain difference continue backpropagation encoder using modiﬁed gradient. since intrinsic representation much higher-dimensional extrinsic ones requires training. accordingly select type batch ratio azimuthelevationlightingintrinsic; arrived ratio extensive testing works well datasets. figure training minibatch azimuth angle face changes. forward step output component encoder altered sample batch. reﬂects fact generating variables image correspond desired values latents unchanged throughout batch. holding outputs constant throughout batch single neuron forced explain variance within batch i.e. full range changes image caused changing backward step neuron receives gradient signal attempted reconstruction receive signal nudges closer respective averages batch. complete training process batch another batch selected random; likewise contains variations intrinsic; neurons correspond selected latent clamped; training proceeds. figure manipulating pose variables qualitative results showing generalization capability learned dc-ign decoder rerender single input image different pose directions. change latent zelevation smoothly leaving latents unchanged. change latent zazimuth smoothly leaving latents unchanged. training procedure works train encoder decoder represent certain properties data speciﬁc neuron. clamping output neurons force decoder recreate variation batch using changes neuron’s value. clamping gradients train encoder information variations batch output neuron. training method leads networks whose latent variables strong equivariance corresponding generating parameters shown figure allows value true generating parameter trivially extracted encoder. figure manipulating light variables qualitative results showing generalization capability learnt dc-ign decoder render original static image different light directions. latent neuron zlight changed random values latents clamped. entangled versus disentangled representations. original reconstruction transformed using normally-trained network. bottom transformation using dc-ign. training transformation time encouraging certain neurons contain speciﬁc information; equivariance. also wish explicitly discourage information; want invariant transformations. since mini-batches training data consist transformation batch goal corresponds output neurons encoder give output every image batch. encourage property dc-ign train neurons correspond inactive transformations error gradient equal difference mean. simplest think gradient acting subvectors zinactive encoder input batch. zinactive’s pointing close-together identical point high-dimensional space; invariance training signal push closer together. don’t care are; network represent face shown batch however likes. care network always represents still face matter it’s facing. regularizing force needs scaled much smaller true training signal otherwise overwhelm reconstruction goal. empirically factor works well. trained model batches faces generated face model obtained paysan batch consists faces random variations face identity variables pose lighting. used rmsprop learning algorithm training meta learning rate equal momentum decay weight decay ensure techniques work types data also trained networks perform reconstruction images widely varied chairs many perspectives derived pascal visual object classes dataset extracted aubry task tests ability dc-ign learn rendering function dataset high variation elements set; chairs vary ofﬁce chairs wicker modern designs viewpoints span degrees elevations. networks trained methods parameters ones above. figure generalization decoder render images novel viewpoints lighting conditions generated several datasets varying light azimuth elevation tested invariance properties dc-ign’s representation show quantitative performance three network conﬁgurations described section dc-ign encoder networks reasonably predicts transformations static test images. interestingly seen encoder network seems learnt switch node separately process azimuth left right proﬁle side face. decoder network learns approximate rendering engine shown figures given static test image encoder network produces latents depicting scene variables light pose shape etc. similar off-the-shelf rendering engine independently control generate images decoder. example shown figure given original test image vary lighting image keeping latents constant varying zlight. perhaps surprising fully-trained decoder network able function rendering engine. also quantitatively illustrate network’s ability represent pose light smooth linear manifold shown figure directly demonstrates training algorithm’s ability disentangle complex transformations. plots inferred ground-truth transformation values plotted random subset test set. interestingly shown figure encoder network’s representation azimuth discontinuity explore much difference dc-ign training procedure makes compare novelview reconstruction performance networks entangled representations versus disentangled representations baseline network entangled representations identical every dc-ign trained sgvb without using training procedures propose paper. figure feed network single input image attempt decoder re-render image different azimuth angles. this ﬁrst must ﬁgure latent entangled representation closely corresponds azimuth. rather simply. first encode images azimuth-varied batch using baseline’s encoder. calculate variance latents batch. latent largest variance closely associated azimuth face call zazimuth. found latent zazimuth varied models render novel view face given single image face. figure shows explicit disentanglement critical novel-view reconstruction. performed similar experiments chairs dataset described above. dataset contains still images rendered models different chairs model skinned photographic texture real chair. models rendered different poses; elevations images taken degrees around model. used approximately chairs training remaining test set; such networks never seen chairs test angle tests explore networks ability generalize arbitrary chairs. resized images pixels made grayscale match face dataset. figure manipulating rotation generated encoding input image encoder changing value single latent putting modiﬁed encoding decoder. network never seen chairs orientation. positive examples. note dc-ign making conjecture components chair cannot see; particular guesses chair arms can’t doesn’t. examples network extrapolates viewpoints less accurately. trained networks azimuth chair disentangled variable represented single node variation images undifferentiated represented dc-ign network succeeded achieving mean-squared error reconstruction test set. image grayscale values range pixels. figure included examples networks ability re-render previously-unseen chairs different angles given single image. chairs able render fairly smooth transitions showing chair many intermediate poses others seems capture sort keyframes representation distinct outputs angles. interestingly task rotating chair seen angle requires speculation unseen components; chair might arms not; curved seat one; etc. shown possible train deep convolutional inverse graphics network interpretable graphics code layer representation static images. utilizing deep convolution de-convolution architecture within variational autoencoder formulation model trained end-to-end using back-propagation stochastic variational objective function proposed training procedure force network learn disentangled interpretable representations. using face analysis working example demonstrated invariant equivariant characteristics learned representations. scale approach handle complex scenes likely important experiment deeper architectures order handle large number object categories within single network architecture. also appealing design spatio-temporal based convolutional architecture utilize motion order handle complicated object transformations. furthermore current formulation sgvb restricted continuous latent variables. however real-world visual scenes contain unknown number objects move frame. therefore might necessary extend formulation handle discrete distributions extend model recurrent setting. decoder network model also replaced domain-speciﬁc decoder ﬁne-grained model-based inference. hope work motivates research automatically learning interpretable representations using variants model. thank thomas vetter giving access basel face model. kulkarni graciously supported leventhal fellowship. would like thank ilker yildrim kleiman-weiner karthik rajagopal geoffrey hinton helpful feedback discussions.", "year": 2015}