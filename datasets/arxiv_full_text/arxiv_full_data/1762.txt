{"title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER.", "text": "state-of-the-art sequence labeling systems traditionally require large amounts taskspeciﬁc knowledge form handcrafted features data pre-processing. paper introduce novel neutral network architecture beneﬁts wordcharacter-level representations automatically using combination bidirectional lstm crf. system truly end-to-end requiring feature engineering data preprocessing thus making applicable wide range sequence labeling tasks. evaluate system data sets sequence labeling tasks penn treebank corpus part-of-speech tagging conll corpus named entity recognition obtain state-of-the-art performance datasets accuracy tagging ner. linguistic sequence labeling part-ofspeech tagging named entity recognition ﬁrst stages deep language understanding importance well recognized natural language processing community. natural language processing systems like syntactic parsing entity coreference resolution becoming sophisticated part utilizing output information tagging systems. traditional high performance sequence labeling models linear statistical models including hidden markov models conditional random fields rely heavily hand-crafted features taskspeciﬁc resources. example english taggers beneﬁt carefully designed word spelling features; orthographic features external resources gazetteers widely used ner. however task-speciﬁc knowledge costly develop making sequence labeling models difﬁcult adapt tasks domains. past years non-linear neural networks input distributed word representations also known word embeddings broadly applied problems great success. collobert proposed simple effective feed-forward neutral network independently classiﬁes labels word using contexts within window ﬁxed size. recently recurrent neural networks together variants long-short term memory gated recurrent unit shown great success modeling sequential data. several rnn-based neural network models proposed solve sequence labeling tasks like speech recognition tagging achieving competitive performance traditional models. however even systems utilized distributed representations inputs used augment rather replace hand-crafted features performance drops rapidly models solely depend neural embeddings. paper propose neural network architecture sequence labeling. truly endto-end model requiring task-speciﬁc resources feature engineering data pre-processing beyond pre-trained word embeddings unlabeled corpora. thus model easily applied wide range sequence labeling tasks different languages domains. ﬁrst convolutional neural networks encode character-level information word character-level representation. combine characterword-level representations feed bi-directional lstm model context information word. blstm sequential jointly decode labels whole sentence. evaluate model linguistic sequence labeling tasks tagging penn treebank english data conll shared task end-to-end model outperforms previous stateof-the-art systems obtaining accuracy tagging ner. contributions work proposing novel neural network architecture linguistic sequence labeling. giving empirical evaluations model benchmark data sets classic tasks. achieving state-of-the-art performance truly end-to-end system. previous studies shown effective approach extract morphological information characters words encode neural representations. figure shows extract character-level representation given word. similar chiu nichols except character embeddings inputs without character type features. dropout layer applied character embeddings input cnn. figure convolution neural network extracting character-level representations words. dashed arrows indicate dropout layer applied before character embeddings input cnn. bi-directional lstm lstm unit recurrent neural networks powerful family connectionist models capture time dynamics cycles graph. though theory rnns capable capturing long-distance dependencies practice fail gradient vanishing/exploding problems lstms variants rnns designed cope gradient vanishing problems. basically lstm unit composed three multiplicative gates control proportions information forget pass next time step. figure gives basic structure lstm unit. vector word. {y··· represents generic sequence labels denotes possible label sequences probabilistic model sequence deﬁnes family conditional probability possible label sequences given following form character-level representation computed figure character embeddings inputs. character-level representation vector concatenated word embedding vector feed blstm network. finally output vectors blstm layer jointly decode best label sequence. shown figure dropout layers applied input output vectors blstm. experimental results show using dropout signiﬁcantly element-wise sigmoid function element-wise product. time input vector hidden state vector storing useful information time denote weight matrices different gates input weight matrices hidden state denote bias vectors. noted include peephole connections lstm formulation. blstm many sequence labeling tasks beneﬁcial access past future contexts. however lstm’s hidden state takes information past knowing nothing future. elegant solution whose effectiveness proven previous work bi-directional lstm basic idea present sequence forwards backwards separate hidden states capture past future information respectively. hidden states concatenated form ﬁnal output. sequence labeling tasks beneﬁcial consider correlations labels neighborhoods jointly decode best chain labels given input sentence. example tagging adjective likely followed noun verb standard annotation i-org cannot follow i-per. therefore model label sequence jointly using conditional random ﬁeld instead decoding label independently. also experiments sets published embeddings namely senna dimensional embeddings trained wikipedia reuters rcv- corpus google’s wordvec -dimensional embeddings trained billion words google news test effectiveness pretrained word embeddings experimented randomly initialized embeddings dimensions embeddings unidim dimension embeddings performance different word embeddings discussed section character embeddings. character embeddings initialized uniform samples weight matrices bias vectors. matrix parameters randomly initialized uniform number rows columns structure bias vectors initialized zero except bias forget gate lstm initialized parameter optimization performed minibatch stochastic gradient descent batch size momentum choose initial learning rate learning rate updated epoch training decay rate number epoch completed. reduce effects gradient exploding gradient clipping explored sophisticated optimization algorithms adadelta adam rmsprop none meaningfully improve upon momentum gradient clipping preliminary experiments. early stopping. early stopping based performance validation sets. best parameters appear around epochs according experiments. figure main architecture neural network. character representation word computed figure character representation vector concatenated word embedding feeding blstm network. dashed arrows indicate dropout layers applied input output vectors blstm. section provide details training neural network. implement neural network using theano library computations single model geforce titan gpu. using settings discussed section model training requires hours tagging hours ner. hyper-parameter window size number ﬁlters state size initial state peepholes dropout rate batch size initial learning rate decay rate gradient clipping fine tuning. embeddings ﬁne-tune initial embeddings modifying during gradient updates neural network model back-propagating gradients. effectiveness method previously explored sequential structured prediction problems dropout training. mitigate overﬁtting apply dropout method regularize model. shown figure apply dropout character embeddings inputting input output vectors blstm. dropout rate dropout layers experiments. obtain signiﬁcant improvements model performance using dropout tuning hyper-parameters table summarizes chosen hyper-parameters experiments. tune hyper-parameters development sets random search. time constrains infeasible random search across full hyper-parameter space. thus tasks tagging share many hyper-parameters possible. note ﬁnal hyper-parameters tasks almost same except initial learning rate. state size lstm tuning parameter signiﬁcantly impact performance model. ﬁlters window length tagging. english tagging wall street journal portion penn treebank contains different tags. order compare previous work adopt standard splits section training data section development data section test data ner. perform experiments english data conll shared task data contains four different types named entities person location organization misc. bioes tagging scheme instead standard previous studies reported meaningful improvement scheme main results ﬁrst experiments dissect effectiveness component neural network architecture ablation studies. compare performance three baseline systems brnn bi-direction rnn; blstm bidirection lstm blstm-cnns combination blstm model characterlevel information. models using stanford’s glove dimensional word embeddings hyper-parameters shown table according results shown table blstm obtains better performance brnn evaluation metrics tasks. blstm-cnn models signiﬁcantly outperform blstm model showing characterlevel representations important linguistic sequence labeling tasks. consistent results reported previous work finally adding layer joint decoding achieve signiﬁcant improvements blstmcnn models tagging metrics. demonstrates jointly decoding label sequences signiﬁcantly beneﬁt ﬁnal performance neural network models. comparison previous work tagging table illustrates results model tagging together seven previous topperformance systems comparison. model signiﬁcantly outperform senna feed-forward neural network model using capitalization discrete sufﬁx features data pre-processing. moreover model achieves improvements accuracy charwnn neural network model based senna also uses cnns model characterlevel representations. demonstrates effectiveness blstm modeling sequential data comparing traditional statistical models system achieves state-of-the-art accuracy obtaining improvement previously best reported results søgaard noted huang also evaluated blstm-crf model tagging corpus. used different splitting training/dev/test data sets. thus results directly comparable ours. table shows scores previous models test data conll- shared task. purpose comparison list results together ours. similar observations tagging model achieves signiﬁcant improvements senna three neural models namely lstm-crf proposed huang lstm-cnns proposed chiu nichols lstmcrf lample huang utilized discrete spelling context features chiu nichols used charactertype capitalization lexicon features three model used task-speciﬁc data preprocessing model require carefully designed features data pre-processing. point result reported chiu nichols incomparable ours ﬁnal model trained combination training development data sets. knowledge previous best score reported conll data joint entity linking model model used many hand-crafted features including stemming spelling features chunks tags wordnet clusters brown clusters well external knowledge bases freebase wikipedia. end-to-end model slightly improves model yielding state-of-the-art performance. mentioned section order test importance pretrained word embeddings performed experiments different sets publicly published word embeddings well random sampling method initialize model. table gives performance three different word embeddings well randomly sampled one. according results table models using pretrained word embeddings obtain signiﬁcant improvement opposed ones using random embeddings. comparing tasks relies different pretrained embeddings stanford’s glove dimensional embeddings achieve best results tasks better accuracy better score senna dimensional one. different results reported chiu nichols senna achieved slightly better performance embeddings. google’s wordvec dimensional embeddings obtain similar performance senna tagging still slightly behind glove. performance wordvec behind glove senna. possible reason wordvec good embeddings vocabulary mismatch wordvec embeddings trained casesensitive manner excluding many common symbols punctuations digits. since data pre-processing deal common symbols rare words might issue using wordvec. table compares results without dropout layers data set. hyperparameters remain table observe essential improvement tasks. demonstrates effectiveness dropout reducing overﬁtting. error analysis better understand behavior model perform error analysis out-of-vocabulary words speciﬁcally partition data four subsets in-vocabulary words out-of-training-vocabulary words out-of-embedding-vocabulary words out-of-both-vocabulary words word considered appears training embedding vocabulary oobv neither. ootv words ones appear training embedding vocabulary ooev ones appear embedding vocabulary training set. entity considered oobv exists lease word training least word embedding vocabulary three subsets done similar manner. table informs statistics partition corpus. embedding used stanford’s glove dimension section table illustrates performance model different subsets words together baseline lstm-cnn model comparison. largest improvements appear oobv subsets corpora. demonstrates adding joint decoding model powerful words training embedding sets. recent years several different neural network architectures proposed successfully applied linguistic sequence labeling tagging chunking ner. among neural architectures three approaches similar model blstm-crf model proposed huang lstmhuang used blstm word-level representations jointly label decoding similar model. main differences model ours. first employ cnns model character-level information. second combined neural network model handcrafted features improve performance making model end-to-end system. chiu nichols proposed hybrid blstm cnns model characterword-level representations similar ﬁrst layers model. evaluated model achieved competitive performance. model mainly differ model using joint decoding. moreover model truly end-to-end either utilizes external knowledge character-type capitalization lexicon features data preprocessing speciﬁcally recently lample proposed blstmcrf model utilized blstm model characterword-level information data pre-processing chiu nichols instead model character-level information achieving better performance without using data preprocessing. several neural networks previously proposed sequence labeling. labeau proposed rnn-cnns model german tagging. model similar lstm-cnns model chiu nichols difference using vanila instead lstm. another neural architecture employing model character-level information charwnn architecture inspired feed-forward network charwnn obtained near state-of-the-art accuracy english tagging similar model also applied spanish portuguese ling yang also used bsltm compose character embeddings word’s representation similar lample peng dredze improved chinese social media word segmentation. paper proposed neural network architecture sequence labeling. truly end-toend model relying task-speciﬁc resources feature engineering data pre-processing. achieved state-of-the-art performance linguistic sequence labeling tasks comparing previously state-of-the-art systems. several potential directions future work. first model improved exploring multi-task learning approaches combine useful correlated information. example jointly train neural network model tags improve intermediate representations learned network. another interesting direction apply model data domains social media since model require domaintaskspeciﬁc knowledge might effortless apply domains. research supported part darpa grant fa--- funded deft program. opinions ﬁndings conclusions recommendations expressed material authors necessarily reﬂect views darpa.", "year": 2016}