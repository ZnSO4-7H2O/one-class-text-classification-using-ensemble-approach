{"title": "Model-Based Hierarchical Clustering", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We present an approach to model-based hierarchical clustering by formulating an objective function based on a Bayesian analysis. This model organizes the data into a cluster hierarchy while specifying a complex feature-set partitioning that is a key component of our model. Features can have either a unique distribution in every cluster or a common distribution over some (or even all) of the clusters. The cluster subsets over which these features have such a common distribution correspond to the nodes (clusters) of the tree representing the hierarchy. We apply this general model to the problem of document clustering for which we use a multinomial likelihood function and Dirichlet priors. Our algorithm consists of a two-stage process wherein we first perform a flat clustering followed by a modified hierarchical agglomerative merging process that includes determining the features that will have common distributions over the merged clusters. The regularization induced by using the marginal likelihood automatically determines the optimal model structure including number of clusters, the depth of the tree and the subset of features to be modeled as having a common distribution at each node. We present experimental results on both synthetic data and a real document collection.", "text": "present erarchical jective ysis. model organizes cluster feature-set nent model. features unique distribution distribution clusters. features tion correspond tree representing general ment clustering mial likelihood algorithm cess wherein followed erative mining features distributions merged clusters. induced regularization likelihood timal model clusters features modeled distribution experimental results real document model-based generative agglomerative algorithms resulting algorithms special scribed algorithm developed. ences version likelihood concentrating hierarchical ture include terize ministic latent base hierarchy additional ables used define intermediate additional model conditional ularization separate model differs first intermediate part model selection). features number adjustable dramatically reduce rameters model especially high-dimensional achieved bayesian fashion. entire validation algorithm cient statistics merging less expensive. nizes data elements nodes different mined modeling children features across clusters tures clusters clusters. features \"useful\". model lends taxonomy like automatic lection documents. noise useful features tions. rich area several hierarchical concept case certain common distribution clusters. candi足 dates common distributions. feature hierarchy scendants associated node ancestor parent define mhac perform increase equation formed first finding feature noise features modeled consideration. using single consideration potentially algorithm increase using mhac algorithm below. perform scribed here. algorithm finds maximum-marg inal足 equation likelihood models subsection algorithm distributional-clustering search space. bayesian feature-partition marginal-likel allows termined associated clustering subsets optimal model tures global noise features common distribution hierarchy also includes distributions find associated partitions archical denote mhac. mhac algorithm feature features. feature space obtain search able features. resort ture time stops feature increase hood. accomplish features imum likelihood worthwhile algorithm starts mhac algorithm. data point present value obtained result computationally algorithm lower parts dendrogram created every merge operation thus provide change mlh. problem posse generated ters using minimum spanning postprocessed tant clusters algorithm. also applied real world data consisting uments. important automatic large collections tion used first topics trec collection. topics leaf nodes taxonomy domains level taxonomy. data sets used mhac algorithm find best estimate plication structures numbers clusters numbers gorithm generated optimum clustering lihood. algorithm bottom. results experiments shown tables tables tion number features noise features intermediate nodes number merges mhac algorithm uncovered. rithm synthetic clusters ranging number clusters different highest structures clusters. tual information clusters found algorithm. table leaf com足 puted following leaf clusters gorithm nodes. noting mhac performs mhac increases find table mhac algorithm seen tables em+mhac algo足 rithm able find structure first data opposed tantly structures partly associated useful features forced features difficult nodes resort tions. pursued however. level indicating data leaf nodes noise features important structure unlike case first structure mhac-fs algorithm structure rithm performed wrong merges. drastic without version without clusters hood. results discussed also conducted given randomly original users required hierarchy automaticly) documents trec hierarchy). number misclassifications taken classify took minutes hierarchy paper introduced ranging exploited common distribution different distribution several automatic function orous bayesian form bayesian archy certain novel algorithm objective first stage flat partitions modified used model intermediate common across applied real world data-set.", "year": 2013}