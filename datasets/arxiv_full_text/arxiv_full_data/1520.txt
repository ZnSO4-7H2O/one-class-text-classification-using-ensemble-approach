{"title": "Video Captioning via Hierarchical Reinforcement Learning", "tag": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "Video captioning is the task of automatically generating a textual description of the actions in a video. Although previous work (e.g. sequence-to-sequence model) has shown promising results in abstracting a coarse description of a short video, it is still very challenging to caption a video containing multiple fine-grained actions with a detailed description. This paper aims to address the challenge by proposing a novel hierarchical reinforcement learning framework for video captioning, where a high-level Manager module learns to design sub-goals and a low-level Worker module recognizes the primitive actions to fulfill the sub-goal. With this compositional framework to reinforce video captioning at different levels, our approach significantly outperforms all the baseline methods on a newly introduced large-scale dataset for fine-grained video captioning. Furthermore, our non-ensemble model has already achieved the state-of-the-art results on the widely-used MSR-VTT dataset.", "text": "figure video captioning examples. example msr-vtt dataset summarized three single captions. bottom example charades dataset consists several dependent human activities described multiple long sentences complex structure. descriptive sentence paragraph generation tends grasp detailed actions generates multiple sentences descriptions. however even paragraph generation paragraph often split multiple singlesentence generation scenarios associated ground truth temporal video intervals. works employed action detection technique predict temporal intervals shown signiﬁcant improvement resulting captioning results. many practical cases human activities complex described short simple sentences temporal intervals hard predicted ahead time without good understanding linguistic context. instance bottom example figure human actions total laptop happening simultaneously followed stand shoulder walk room order. ﬁne-grained caption requires subtle expressive mechanism capture temporal dynamics video captioning task automatically generating textual description actions video. although previous work shown promising results abstracting coarse description short video still challenging caption video containing multiple ﬁne-grained actions detailed description. paper aims address challenge proposing novel hierarchical reinforcement learning framework video captioning highlevel manager module learns design sub-goals low-level worker module recognizes primitive actions fulﬁll sub-goal. compositional framework reinforce video captioning different levels approach signiﬁcantly outperforms baseline methods newly introduced large-scale dataset ﬁne-grained video captioning. furthermore non-ensemble model already achieved state-of-the-art results widelyused msr-vtt dataset. people watching brief video describing happened easy task. machines extracting meaning video pixels generating natural-sounding description challenging problem. however wide range applications intelligent video surveillance assistance visuallyimpaired people video captioning drawn increasing attention computer vision community recently. different image captioning aims describing static scene video captioning challenging sense series coherent scenes need understood order jointly generate multiple description segments current video captioning tasks mainly divided families single-sentence generation paragraph generation single-sentence generation tends abstract whole video simple high-level order tackle issue propose divide conquer solution ﬁrst divides long caption many small text segments employs sequence model conquer segment. instead forcing sequence model generate whole sequence shot propose guide model generate sentences segment segment. higher-level sequence model designing context segment low-level sequence model follows guidance generate segment word word. paper propose novel hierarchical reinforcement learning framework realize two-level mechanism. textual video context viewed reinforcement learning environment. framework fully-differentiable deep neural network consists higher-level sequence model manager sets goals lower temporal resolution lowerlevel sequence model worker selects primitive actions every time step following goals manager internal critic determines whether goal accomplished not. speciﬁcally exploiting context environment ﬁnished goals manager emits goal segment worker receives goal guidance generate segment producing words sequentially. besides internal critic employed evaluate whether current textual segment accomplished. furthermore equip manager worker attention module video features introduce hierarchical attention internally manager focus wider range temporal dynamics worker’s attention narrowed local dynamics conditioned goals. meanwhile since vocabulary usually contains thousands words resulting large action space hard explore. using hierarchical reinforcement learning goal manager largely restrict exploration space worker improve word prediction accuracy. best knowledge ﬁrst work strives develop hierarchical reinforcement learning approach reinforce video captioning different levels. main contributions four-fold related work sequence-to-sequence model video captioning ﬁrst generalized lstm video captioning proposed sequence-to-sequence model since then numerous improvements introduced attention hierarchical recurrent neural network features joint embedding space language fusion multi-task learning etc. maximum-likelihood algorithm maximizes probability current groundtruth output given previous ground-truth output previous ground-truth general unknown test time. inconsistency issue known exposure bias largely hindered system performance. here utilize reinforcement learning algorithm resolve problem. reinforcement learning image/video captioning ranzato proposed address inconsistency issue directly optimizing non-differentiable metric scores using reinforce algorithm problem persisted expected gradient computed using policy gradient typically exhibited high variance often unstable without proper context-dependent normalization. naturally variance could reduced adding baseline even actor-critic method trained additional critic estimate value generated word pasunuru bansal applied policy gradient baseline video captioning presented textual entailment loss adjust cider reward. unfortunately previous work image/video captioning fail grasp high-level semantic video. model aims address issue hierarchical architecture. hierarchical reinforcement learning recent work revealed effectiveness hierarchical reinforcement learning frameworks atari games peng built composite dialogue policy using hierarchical q-learning fulﬁll complex dialogue tasks like traveling plans typical setting highlevel agent operated lower temporal resolution charades captions obtained preprocessing charades dataset processed charades captions dataset downloaded here https//www.dropbox.com/s/imvxonaizcbhh/ charadescaptions.zip?dl= sub-goal low-level agent selected primitive actions following sub-goal high-level agent. proposed framework video captioning aligned studies difference typical setting instead internal critic provide intrinsic reward encourage low-level agent accomplish sub-goal focus exploiting extrinsic rewards different time spans. proposed framework follows general encoder-decoder framework encoding stage video frame features {vi} ﬁrst extracted pretrained convolutional neural network model indexes frames temporal order. frame features passed low-level bi-lstm encoder high-level lstm encoder successively obtain low-level encoder output {hew high-level encoder output {hem decoding stage agent plays role decoder outputs language description aa...at length generated caption vocabulary set. agent composed three components lowlevel worker high-level manager internal critic. manager operates lower temporal resolution emits goal needed worker accomplish worker generates word time step following goal proposed manager. words manager asks worker generate semantic segment worker generates corresponding words next time steps order fulﬁll job. internal critic determines worker accomplished goal sends binary segment signal manager help update goals. whole pipeline terminates sentence token reached. attention module mentioned above cnn-rnn encoder receives video inputs generate sequence vectors {hew directly take inputs worker manager. instead adopt attention mechanism better capture temporal dynamics form context vector manager worker. model manager worker equipped attention module. internal critic order evaluate whether worker accomplished goal employ internal critic evaluate worker’s progress. internal critic uses structure takes sequence input discriminate whether reached. denote signal internal critic denote hidden state time step formally describe probability follows action taken worker denotes parameters feed-forward neural network. order train parameters linear layer recurrent network propose maximize likelihood given ground truth signal manager worker shown figure cont−] input manager catenation representing parameters manager worker policy stochastic policy denoted represents parameters worker. reason worker policy stochastic action selecting word vocabulary manager generated goal latent cannot directly supervised. thus deterministic manager policy warm start manager worker simultaneously viewing composite agent. section ﬁrst derive mathematical reinforce learning methods policies separately introduce training algorithm proposed method also discuss reward deﬁnitions imitation learning policy consider standard reinforcement learning setup. step worker select action conditioned manager. environment responds state scalar reward process continues <eos> token generated. objective worker agent maximize discounted return γkrt+k. thus loss function worker worker policy training target manager policy worker policy oracle behavior policy. speciﬁcally manager outputs goal step worker runs steps generate expected segment atat+...at+c− following goal since worker ﬁxed oracle behavior policy need consider training manager. environment responds state st+c scalar reward thus objective becomes minimizing negative discounted return formula applying chain rule loss function respect manager’s parameters manager updated −egt∇gt π∇θmµθm]. gradients approximated single sampled segment adopting policy gradient worker policy −r∇gt π∇θmµθm estimated baseline function case baseline estimated linear regressor worker’s hidden state input. back propagation gradient passing worker lstm baseline estimator. means reward sample word greater baseline gradient negative thus model encourages distribution increasing probability word otherwise discourages distribution accordingly. framework effectively learn goal generated manager guides worker achieve latent objective. difﬁculty training manager directly interact environment action takes produce latent vector continuous high-dimensional space indirectly inﬂuences environment directing worker’s behavior. therefore especially interested coming solutions encourage manager towards effective caption generation. algorithm training algorithm require training pairs <video caption> randomly initialize model parameters load pretrained model internal critic iteration=m major challenge reinforcement learning agent good convergence property agent must start good policy beginning stage. model apply cross-entropy loss optimization warm start worker manager simultaneously manager completely treated latent parameters. parameters whole model ground-truth word sequence cross-entropy loss deﬁned experimental results datasets msr-vtt msr-vtt dataset general video captioning derived wide variety video categories contains video clips video contains human annotated reference captions collected amazon mechanical turk charades captions charades large-scale dataset composed videos daily indoors activities collected amt. different users presented sentence script included objects actions ﬁxed vocabulary users recorded video following recent work image captioning shown cider reward performs best among traditional evaluation metrics image/video captioning gain improvement metrics. model also cider score compute reward. instead directly using ﬁnal cider score whole generated caption reward word adopt delta cider score immediate reward. cider cider sent previous generated caption. discounted return worker illustrate learning methods train manager worker. alg. present overview training algorithm video captioning. iteration train manager policy worker policy alternately basically training worker assume manager well-posed disable goal exploration update worker policy according equation training manager treat worker oracle behavior policy generate caption greedy decoding update manager policy following equation script using provided objects actions. original dataset contains temporal annotations action classes labels object classes textual descriptions videos. charades dataset used action recognition segmentation note collected textual descriptions detailed depict ﬁne-grained human activities happening long videos. thus preprocessed charades dataset combining textual descriptions sentence scripts veriﬁed built large-scale dataset detailed video captioning charades captions consists videos training validation testing. video clip annotated multiple captions. captions detailed longer msr-vtt suitable ﬁne-grained video captioning. caption segmentation order train internal critic works oracle determine goal accomplished preprocessed ground truth captions training sets datasets breaking caption multiple semantic chunks. segmented captions mainly based noun phrase verb phrase labels provided constituency parsing results nltk constituency parsing). instance caption person tidies area done eating segmented three sub-phrases person tidies area done eating labels respectively. however need train internal critic chunks labels used. evaluation metrics adopted four diverse automatic evaluation metrics bleu meteor rouge-l cider-d. used standard evaluation code mscoco server obtain results. example sentence script video person taking picture light sitting chair. textual description video person bedroom appears phone take picture light ﬁxture ceiling. latter usually detailed. extract frame features without ﬁne-tuning. frame features projected -dim. lowlevel encoder bi-lstm hidden size high-level encoder lstm hidden size worker network consisted worker lstm hidden size attention module similar proposed bahdanau word embedding size projection module produced probabilities tokens vocabulary. manager network composed manager lstm hidden size attention module linear layer projected output lstm latent goal space. internal critic also network contained built-in word embedding linear layer sigmoid function. hidden size word embedding size msr-vtt charades captions. training details hyperparameters tuned validation including dimension sizes sec. moreover adopted dropout value regularization. gradients clipped range initialized parameters uniform distribution range msr-vtt dataset used ﬁxed step size encoder lstms maximum length captions. charades captions dataset respectively. train cross-entropy models adadelta optimizer used batch size learning rate initially reduced factor current cider score didn’t surpass previous best epochs. schedule sampling employed train models. training models used pretrained models warm start continued training learning rate discounted factors manager worker test time used beam search size results analysis comparison state arts msr-vtt table compared single-sentence captioning results the-state-of-the-art methods msr-vtt dataset. listed results mean-pooling soft-attention reported previous work also compared top- results msr-vtt challenge including navigator aalto videolab implemented baseline methods attentionbased sequence-to-sequence model trained crossentropy loss model trained policy gradient cider score reward shown table xe-baseline achieved comparable results state-ofthe-art results reinforcement learning rlresult analysis charades captions since papers reporting results charades captions mainly compared model implementation xe-baseline rl-baseline. meanwhile explored dimension latent goal vector observed table models outperformed baseline methods brought signiﬁcant improvements different evaluation metrics. note model achieved bigger improvement baseline methods charades captions dataset msr-vtt. given fact average cation length charades captions much longer mst-vtt difference improvement gaps demonstrated model gain better improveamong models hrl- achieved best almost metrics even though hrl- obtained better results bleu cider results metrics worse hrl- thus comparing results different models could conclude hrl- hrl- hrl-. result accorded speculation higher dimension guarantee better performance conversely exploration space grows exponentially dimension increases making learning even harder. latent vector small dimension like able represent semantically meaningful goal well. qualitative comparison baseline methods figure illustrated examples charades captions test set. according captions generated different models obvious generated results model matched ground truth captions better baseline methods. moreover segment-bysegment generation manner model able output sequence semantically meaningful phases tails video content generate ﬁne-grained descriptions. example model provided event scene example baseline methods failed depict event happening. example illustrated correctness accuracy results. instance example result method described video correctly. ground truth caption group racing around track result group people running track. xe-baseline rl-baseline captioned mistake video group people playing game playing football game respectively. apparently compared results baseline methods results accurate descriptive general. learning curve intuitive view models drew learning curves cider scores validation note rl-baseline model ﬁrst warmed cross-entropy loss improved using reinforce algorithm. particularly after trained xe-baseline model switched policy gradient continued training rl-baseline model models resumed training shorter warmstart period. shown figure models converged faster achieved better peak points baseline methods. hrl- reached highest point. paper introduce hierarchical reinforcement learning framework video captioning aims improving ﬁne-grained generation video descriptions rich activities. two-level agents closely interact exhibiting structural semantic coordinations complex generation task. ﬁrst evaluate approach popular msr-vtt dataset demonstrate effectiveness approach. next introduced largescale dataset ﬁne-grained video captioning show state-of-the-art performance model. future plan explore attention space incorporate spatial attention form spatiotemporal attention model boost agent. besides experimented frame-level features pre-trained model. believe results method improved employing different types features features optical ﬂows etc. meanwhile investigate proposed framework similar sequence generation tasks video/document summarization.", "year": 2017}