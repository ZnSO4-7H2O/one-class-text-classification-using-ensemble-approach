{"title": "A Flexible Iterative Framework for Consensus Clustering", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "A novel framework for consensus clustering is presented which has the ability to determine both the number of clusters and a final solution using multiple algorithms. A consensus similarity matrix is formed from an ensemble using multiple algorithms and several values for k. A variety of dimension reduction techniques and clustering algorithms are considered for analysis. For noisy or high-dimensional data, an iterative technique is presented to refine this consensus matrix in way that encourages algorithms to agree upon a common solution. We utilize the theory of nearly uncoupled Markov chains to determine the number, k , of clusters in a dataset by considering a random walk on the graph defined by the consensus matrix. The eigenvalues of the associated transition probability matrix are used to determine the number of clusters. This method succeeds at determining the number of clusters in many datasets where previous methods fail. On every considered dataset, our consensus method provides a final result with accuracy well above the average of the individual algorithms.", "text": "novel framework consensus clustering presented ability determine number clusters ﬁnal solution using multiple algorithms. consensus similarity matrix formed ensemble using multiple algorithms several values variety dimension reduction techniques clustering algorithms considered analysis. noisy high-dimensional data iterative technique presented reﬁne consensus matrix encourages algorithms agree upon common solution. utilize theory nearly uncoupled markov chains determine number clusters dataset considering random walk graph deﬁned consensus matrix. eigenvalues associated transition probability matrix used determine number clusters. method succeeds determining number clusters many datasets previous methods fail. every considered dataset consensus method provides ﬁnal result accuracy well average individual algorithms. cluster analysis important tool used hundreds data mining applications like image segmentation text-mining genomics biological taxonomy. clustering allows users explore patterns structure data withprior knowledge training information. hundreds algorithms exist task single algorithm guaranteed work best given class real-world data. inconsistency performance cluster analysis unique clustering algorithms themselves. fact dimension reduction techniques expected algorithms revealing cluster tendencies data also tend compete unpredictably diﬃcult know beforehand low-dimensional approximation might provide best separation clusters. many tools ways make informed decision tool high-dimensional cluster analysis doomed become science analysts blindly reach tool hope best. cluster analysis ﬁrst type data mining encounter problem. data scientists quick develop ensemble techniques escape unreliability individual algorithms tasks like prediction classiﬁcation. ensemble methods become integral part many areas data mining cluster analysis methods largely ignored. additional problem stems fact vast majority algorithms require user specify number clusters algorithm create. applied setting unlikely user know information hand. fact number distinct groups data question analyst attempting answer. determining number clusters data long considered diﬃcult aspects cluster analysis. fact boils basics cluster? deﬁne count separate clusters? approach provides original answer question group points considered cluster variety algorithms agree considered cluster. majority algorithms less agree break dataset clusters cannot agree partition data clusters determine data clusters. essence framework suggested herein. purpose address problems determining number clusters determining ﬁnal solution multiple algorithms. propose consensus method number algorithms form voting ensemble proceeding several rounds elections majority rule determined. allows user implement many tools once increasing conﬁdence ﬁnal solution. recent years consensus idea promoted many researchers main challenge ensemble methods using multiple algorithms generally identiﬁed wide variety results produced diﬀerent algorithms diﬀerent cluster criteria inherent algorithm. thus direct combination results ensemble often generate meaningful result often consensus problem formulated optimization minimizes relative distance problem optimal clustering clusterings ensemble. metric many ways deﬁne distance clusterings example could take minimum number elements need deleted partitions become identical using denote measure problem known median partition problem literature dates back work régnier mirkin alternatively authors relative validity metric like normalized mutual information place distance function attempt maximize objective function median partition problem shown krivanek moravek also wakabayashi np-complete many heuristics since proposed approximate solutions believe methods bound suﬀer clustering ensemble given equal importance. suppose perfect clusterings terribly inaccurate clustering. methods would take account fact majority algorithms share agreement perfect clustering instead shift optimal clustering away perfection towards inaccuracy. thus feel optimization leads middle-of-the-road solution compromise algorithms rather solution agreement consensus. method clustering algorithms voting ensemble continually move series elections desired level consensus reached. additionally introduce parameter intolerance allows user impose level agreement must reached algorithms order accept cluster relationship objects. begin introduce notation. since consensus methods combine multiple solutions multiple algorithms start cluster ensemble. cluster ensemble clusterings data objects clustering ensemble kj-way partition data composed individual clusters consensus matrix figure interesting ensemble used create clusterings various values reasonable number clusters colored circles figure clusterings ensemble depict clusters. however resulting consensus matrix clearly block-diagonal diagonal blocks isolated phenomenon fact something expect consensus matrices labor following reasonable assumptions further clear subcluster\" structure meaning original clusters break meaningful components diﬀerent algorithms break clusters apart diﬀerent ways. similarity matrix consensus matrix oﬀers beneﬁts overs traditional approaches like gaussian cosine similarity matrices. problem traditional methods curse dimensionality high dimensional spaces distance similarity metrics tend lose meaning. range values pairwise distances tightens dimensionality space grows little done address fact. figure show distribution similarity values million entries consensus matrix compared cosine similarity matrix. consensus approach allows user witness high levels similarity high-dimension data whereas cosine similarities much smaller range. dataset formally introduced section medlarscranﬁeld-cisi document collection contrast typical among high-dimensional datasets. additional beneﬁt entries consensus matrix depth. this mean result summing entries adjacency matrices output individual clustering algorithms information available meaning similarity value. cosine angle data vectors tell something correlation knowing instance objects clustered together algorithms algorithms never provides depth insight previously considered. information explicitly analysis beneﬁcial practical research. greatest beneﬁt using consensus matrix clustering provides superior information clustering within data. demonstrated time literature pile evidence statement experiments paper. consensus approach outlined herein based work consensus matrix treated similarity matrix used input clustering algorithm reach ﬁnal solution. authors suggest using many runs k-means algorithm initialized randomly build consensus matrix using spectral clustering method normalized determine ﬁnal solution. approach build consensus matrix using many runs k-means algorithm cluster consensus matrix ﬁnal k-means consensus matrix formed k-means used input stochastic clustering algorithm methods provide better results individual algorithms still rely single algorithm make consensus matrix ﬁnal decision cluster membership. method uses variety algorithms rather k-means create initial cluster ensemble. addition algorithm paired diﬀerent dimension reductions often unclear dimension reduction gives best conﬁguration data; lower dimensional representation potential separate diﬀerent sets clusters data. essentially gather initial round votes whether pair objects belong cluster. votes collected consensus matrix deﬁned deﬁnition experiments contained herein number diﬀerent clustering algorithms. details popular algorithms omitted space considerations refer reader following resources algorithm ensemble assumed reasonable making good choices cluster membership time. inevitable clustering algorithms make mistakes particularly noisy data assumed rarely majority algorithms make mistake. account error introduce intolerance parameter entries consensus matrix zero. words minimum proportion algorithms must agree upon single cluster relationship order keep votes consensus matrix. initial consensus matrix formed input clustering algorithms again. essentially start debate algorithms asking collective votes ensemble determine second solution. solutions collected consensus matrix process repeats simple majority algorithms agree upon solution. majority algorithms chosen common solution algorithms reached consensus call resulting solution ﬁnal consensus solution. process illustrated chart figure illustrate eﬀectiveness procedure consider combination text datasets used frequently information retrieval literature. simplicity assume number clusters known priori. section information extracted data. combined medlars-cranﬁeld-cisi collection consists nearly scientiﬁc abstracts diﬀerent disciplines. disciplines form natural clusters data document data high-dimensional features result clustering algorithms tend slowly data. thus reduce dimensions data using preferred algorithms realize fact singular value decomposition data z-score normalization. however practice decompositions generally provide quite diﬀerent results particularly high-dimensional data. dimension reduction technique number features reduced creating total input data sets. input dataset diﬀerent clustering methods used cluster data accuracy results ranges reasonable question might this choose solution lowest k-means objective value? biggest problem boils curse dimensionality distance measures used compute metrics lose meaning high-dimensional space comparison could make clusterings would full dimensional data surprisingly objective function values minimum accuracy solution approximately equal maximum accuracy solution internal metrics like popular silhouette coeﬃcient suﬀer problem. must careful attempting compare high-dimensional clusterings metrics. suggestion instead compile clusterings table consensus matrix cluster consensus matrix multiple algorithms repeat process majority algorithms agree upon solution. done without dimension reduction consensus matrix. simplicity we’ll proceed without reducing dimensions consensus matrix include additional clustering algorithm nmfcluster well suited analysis low-dimensional representations table table provides accuracy clustering algorithms consensus matrices iteration. boxes drawn around values indicate common solution chosen algorithms. ﬁnal consensus solution found third iteration algorithms agreeing upon single solution. accuracy ﬁnal consensus solution much greater average initial clustering results table result typical across datasets considered. section example given alluded methodology determining number clusters. approach task using perron-cluster methodology consensus similarity matrix. perroncluster analysis involves examination eigenvalues nearly uncoupled nearly completely reducible markov chain. consider transition probability matrix random walker graph deﬁned consensus matrix diagonal matrix containing sums diag. according assumptions section exists simultaneous permutation rows columns consensus matrix result block-diagonally dominant. essentially mean perturbation block-diagonal matrix oﬀ-diagonal blocks much smaller magnitude diagonal blocks. fact entries oﬀ-diagonal blocks small enough diagonal blocks nearly stochastic i.e. biie transition probability matrix taking form describes nearly uncoupled nearly completely reducible markov chain. degree matrix considered nearly uncoupled dependent one’s criteria measuring level coupling aggregates markov chain deviation complete reducibility deﬁned follows deﬁnition irreducible stochastic matrix k-cluster partition important point parameter parameter measures level coupling clusters graph cannot computed without prior knowledge clusters graph. parameters merely tools perturbation analysis used present following important fact regarding spectrum blockdiagonally dominant stochastic matrices remaining eigenvalues bounded away order recover number blocks simply examine eigenvalues stochastic matrix count number eigenvalues perron cluster separated remaining eigenvalues perron largest diﬀerence consecutive eigenvalues λk+. size determined level uncoupling graph larger gaps indicating nearly uncoupled structures build consensus similarity matrix algorithms cluster data varying number clusters. algorithm ensemble clusters data. choice values user suggest choosing values might overestimate number clusters remain less construct consensus similarity matrix resulting clusterings examine eigenvalues transition probability matrix count number eigenvalues near locating largest eigenvalues. sometimes helpful phase algorithm consider intolerance parameter construction consensus matrix allowed algorithms partition data fewer clusters actually exist data. return medlars-cranﬁeld-cisi collection discussed section example traditional plots generally fail provide clear picture many clusters optimal discussing results method ﬁrst look eigenvalues transition probability matrix would result using cosine measure similarity largest eigenvalues matrix displayed figure plot shows eigenvalue perron cluster therefore methods discussed information gathered number clusters data. look eigenvalues transition probability matrix associated consensus similarity matrix. consensus matrix built ensemble various algorithms paired diﬀerent dimension reductions diﬀerent levels dimension reduction. authors’ preferred dimension reduction techniques used reduce dimensions data dimensions creating total data inputs clustering algorithm. three diﬀerent clustering methods used cluster data input pddp spherical k-means initialized randomly spherical k-means initialized centroids clusters found pddp. counting every combination dimension reduction clustering procedure ensemble algorithms work. algorithms clusters determined resulting clusterings collected consensus matrix show figure side-by-side images showing eigenvalues transition probability matrix associated consensus similarity matrix without intolerance parameter particularly text high-dimensional data intolerance parameter removing extraneous connections consensus graph encourages nearly uncoupled structure clustering results uncoupling eﬀect even conservative values clearly identiﬁed widened eigenvalue graphs. figure dataset largest eigenvalues found using consensus similarity matrices without intolerance parameter ensemble algorithms clustering data clusters seen medlars-cranﬁeld-cisi collection consensus matrix provide better clustering information data. therefore seems reasonable iterating consensus process using multiple values reﬁne consensus matrix minimizes eliminates elements outside diagonal blocks revealing identiﬁable perron cluster. often case. iterating clustering process uncoupling eﬀect consensus matrix figure show matrix heat-map consensus matrix formed clustering documents clusters diﬀerent algorithms diﬀerent dimension reductions. pixels indicate high levels similarity yellow pixels represent lower levels similarity. considerable amount noise outside diagonal blocks. consensus matrix clustered algorithms dimension reductions heat resulting consensus matrix shown figure easy reﬁnement clusterings reduction noise outside diagonal blocks. diﬀerence also clearly shown eigenvalue plots displayed figure high-dimensional noise-ridden data suggest iterated procedure determining perron spurious cluster relationships often couple clusters together. section ﬂexibility approach demonstrated using comprehensive example another benchmark dataset. iterative consensus clustering framework summarized algorithm newsgroups dataset subset infamous \"twenty newsgroups\" text corpus become common benchmark cluster analysis. twenty newsgroups corpus consists approximately news documents partitioned somewhat evenly across diﬀerent topics. collection documents attributed lang although never mentioned explicitly original paper publicly available create collection documents topics randomly selected resulting term-document matrix terms documents. initial consensus matrix formed using k-means algorithm performed data dimension reductions. determine appropriate rank dimension reduction followed convention observing screeplot data matrix. screeplot shown figure indicates decision reduce dimensions data performed reduced data phase process proceeds extremely fast. result total clusterings contributed initial consensus matrix. intolerance parameter used initial matrix. next step analysis examine eigenvalues transition probability matrix associated initial consensus matrix. figure dataset eigenvalues associated consensus similarity matrices adjusted intolerance adjusted iteration using clusters. eigenvalues perron cluster correctly identify number clusters. purposes comparison present figure eigenvalues transition probability matrix associated cosine similarity matrix commonly used cluster document datasets spectral algorithms. information regarding number clusters revealed perron cluster contains single eigenvalue. comparing consensus matrix input tables demonstrate superiority consensus matrices traditional data inputs. accuracies certain algorithms increase much consensus matrix used input compared data. average accuracies algorithms also increases dramatically. interesting fact point example algorithms perform quite poorly dimension reduction. even though results contained consensus matrices nature process able weed poor results. authors experimentally discovered time process sensitive small number poor results contained cluster ensemble. comes clustering ensemble results essentially voted out. second step process number clusters determined eigenvalue analysis iterate consensus procedure using determined number clusters attempt witness agreement algorithms. combining clustering results algorithm consensus another consensus matrix second round voting. matrix consensus chosen eigenvalue larger although using consensus matrix provides similar result. accuracies resulting solutions given table boxed values indicate common solution among algorithms. call ﬁnal consensus solution. began analysis collection documents algorithms dimension reduction clustering. traditional tools determining number clusters successful. analyst somehow determined number clusters attempting cluster data given tools chance ﬁnding solution accuracy ranging analyst chosen best dimension reduction algorithm particular dataset accuracy his/her solution internal cluster validation metrics like k-means objective function would much help choosing solutions measures difﬁcult compare high-dimensional data. however using tools disposal iterative consensus clustering framework found clustering algorithms worked diﬀerences constructively ﬁnally settling solution highest level accuracy achieved algorithms independently. herein presented ﬂexible framework combining results multiple clustering algorithms and/or multiple data inputs. framework provide user average clustering solution also discovered consensus matrices built using multiple algorithms multiple values number clusters often allow users estimate appropriate number clusters data determining maximum number clusters algorithms likely agree common solution. provided several examples show approach succeeds determining number clusters datasets methods fail. initial consensus matrix provide information reﬁned intolerance parameter iteration clearer picture many clusters algorithms might able agree upon. consensus matrix idea practice using multiple algorithms dimension reductions together create matrix previously explored varying number clusters purposes approximating approach building consensus matrix novel improves clustering results nearly every clustering algorithm datasets considered. consensus matrix several advantages traditional similarity matrices discussed section framework encourages clustering algorithms agree common solution help escape unreliability individual algorithms. previous consensus methods aimed average cluster solutions another ﬁrst emphasize agreement clustering algorithms. seeing results individual algorithms ensemble clear average solution could poor indeed. rather deciding clustering equally valid simply number times cluster relationship made points algorithms decide whether considerable enough draw points together whether might reasonable dissolve connection favor others. framework iteratively encourages algorithms agree upon common solution value similarity metric reﬂects level algorithmic agreement step. thus iteration cluster relationships upon algorithms agree abandoned favor relationships higher levels agreement.", "year": 2014}