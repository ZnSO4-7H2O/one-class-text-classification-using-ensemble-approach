{"title": "A novel sparsity and clustering regularization", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "We propose a novel SPARsity and Clustering (SPARC) regularizer, which is a modified version of the previous octagonal shrinkage and clustering algorithm for regression (OSCAR), where, the proposed regularizer consists of a $K$-sparse constraint and a pair-wise $\\ell_{\\infty}$ norm restricted on the $K$ largest components in magnitude. The proposed regularizer is able to separably enforce $K$-sparsity and encourage the non-zeros to be equal in magnitude. Moreover, it can accurately group the features without shrinking their magnitude. In fact, SPARC is closely related to OSCAR, so that the proximity operator of the former can be efficiently computed based on that of the latter, allowing using proximal splitting algorithms to solve problems with SPARC regularization. Experiments on synthetic data and with benchmark breast cancer data show that SPARC is a competitive group-sparsity inducing regularizer for regression and classification.", "text": "vector zeros proxf oscar obtained using algorithm proposed therefore solve proximal splitting algorithms fista twist sparsa algorithm adopted experiments. sparsa fast proximal spltting algorithm based step-length selection method barzilai borwein application sparc leads following algorithm propose novel sparsity clustering regularizer modiﬁed version previous octagonal shrinkage clustering algorithm regression where proposed regularizer consists k-sparse constraint pair-wise norm restricted largest components magnitude. proposed regularizer able separably enforce k-sparsity encourage non-zeros equal magnitude. moreover accurately group features without shrinking magnitude. fact sparc closely related oscar proximity operator former efﬁciently computed based latter allowing using proximal splitting algorithms solve problems sparc regularization. experiments synthetic data benchmark breast cancer data show sparc competitive group-sparsity inducing regularizer regression classiﬁcation. recent years much attention paid sparsity also structured/group sparsity. several group-sparsity-inducing regularizers proposed including group lasso fused lasso elastic octagonal shrinkage clustering algorithm regression several others listed space limitations however glasso require prior knowledge structure groups strong requirement many applications flasso depends given order variables; classes approaches thus better suited signal processing applications variable selection grouping machine learning problems regression classiﬁcation contrast oscar proposed regression problems rely ordering variables knowledge group structure. oscar regularizer outperform feature grouping) deﬁned non-negative parameters norm pairwise penalty simultaneously encourage components sparse equal magnitude respectively. however happen components small magnitude shrunk zero norm also penalized pairwise term prevent accurate grouping; moreover components large magnitude simply grouped pairwise norm also shrunk norm paper overcome drawbacks propose sparsity-and-clustering regularizer cardinality support solution restricted pairwise penalty applied non-zero elements also show compute proximity operator sparc regularizer allows using proximal splitting algorithms problems regularizer. correlated responses. data randomly chosen training cross validation testing respectively. results averaged repetitions show table observe sparc competitive group-sparsity-inducing regularizer classiﬁcation terms able select features lower degrees freedom lasso oscar. proposed sparsity clustering regularizer regression classiﬁcation. shown proposed sparc able separably enforce k-sparsity encourage non-zeros equal magnitude thud accurately grouping features without parameter shrinkage outperforming lasso elastic octagonal shrinkage clustering algorithm regression future work involve considering faster algorithms solve problems sparc regularization.", "year": 2013}