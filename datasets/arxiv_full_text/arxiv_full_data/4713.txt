{"title": "Quantum Annealing for Clustering", "tag": ["cs.AI", "cs.LG"], "abstract": "This paper studies quantum annealing (QA) for clustering, which can be seen as an extension of simulated annealing (SA). We derive a QA algorithm for clustering and propose an annealing schedule, which is crucial in practice. Experiments show the proposed QA algorithm finds better clustering assignments than SA. Furthermore, QA is as easy as SA to implement.", "text": "paper studies quantum annealing clustering seen extension simulated annealing derive algorithm clustering propose annealing schedule crucial practice. experiments show proposed algorithm ﬁnds better clustering assignments furthermore easy implement. clustering popular methods data mining. typically clustering problems formulated optimization problems solved algorithms example algorithm convex relaxation. however clustering typically np-hard. simulated annealing promising candidate. geman geman proved able global optimum slow cooling schedule temperature although schedule practice slow clustering large amount data well known still ﬁnds reasonably good solution even faster schedule geman geman proposed. statistical mechanics quantum annealing proposed novel alternative adds another dimension annealing fig.. thus seen extension succeeded speciﬁc problems e.g. ising model statistical mechanics still unclear works better general. actually think intuitively helps clustering apply clustering procedure derive algorithm. derived algorithm depends deﬁnition quantum eﬀect propose quantum eﬀect leads search strategy clustering. contribution algorithm propose markov chain monte carlo sampler call qa-st sampler. explain later naive sampler intractable even mcmc. thus approximate suzuki-trotter expansion derive tractable sampler qa-st sampler. qa-st looks like parallel interaction beginning annealing process qa-st almost sas. hence qa-st ﬁnds optima independently. annealing process continues interaction fig. becomes stronger move states closer. qa-st picks state lowest energy states ﬁnal solution. qa-st proposed quantum eﬀect works well clustering. fig. example data points grouped four clusters. locally optimal globally optimal. suppose equal fig. correspond fig.. although local optima interaction fig. allows search better clustering assignment quantum eﬀect deﬁnes distance metric clustering assignments. case proposed locates thus interaction gives good chance makes closer proposed algorithm actually ﬁnds fig. example. however similar situation often occurs clustering. clustering algorithms cases give almost figure illustrative explanation left ﬁgure shows independent right algorithm derived suzuki-trotter expansion. denotes clustering assignment. globally optimal solutions like majority data points well-clustered not. thus better clustering assignment constructed picking well-clustered data points many sub-optimal clustering assignments. note assignment constructed located sub-optimal ones proposed quantum eﬀect qa-st better assignment sub-optimal ones. brieﬂy review simulated annealing particularly clustering. stochastic optimization algorithm. objective function given energy function better solution lower energy. step searches next random solution near current one. next solution chosen probability depends temperature energy function value next solution. almost ranfirst introduce notation used paper. assume data points assigned clusters. assignment data point denoted binary indicator vector example equal denote i-th data point assigned ﬁrst second cluster respectively. assignment data points also denoted indicator vector whose length number available assignments constructed {˜σi}n kronecker product special case tensor product matrices. matrices example). element others zero. example ˜σ⊗˜σ ﬁrst data point assigned ﬁrst cluster second data point assigned second cluster also matrix denote assignment data example indicates t-th assignment available assignments e−βhc matrix exponential since diagonal e−βhc also diagonal exp)). hence e−βhc exp)) equal practice mcmc methods sample need calculate easy evaluate e−βhc equal exp). samples mcmc methods exploited sampling. however quantum models cannot apply mcmc methods directly intractable evaluate e−βhσ unlike e−βhc exp). e−βh diagonal whereas e−βhc diagonal. thus exploit trotter product formula approximate symmetric matrices state normalization factor deﬁned probabilistic models energy function deﬁned pprob-model pprob-model given probabilistic model data. note pprob-model. loss-functionbased models searches argminσ loss energy function deﬁned loss. many cases calculation intractable. thus markov chain monte carlo utilized sample state current state. paper focus gibbs sampler mcmc methods. step gibbs sampler draws assignment i-th data point from goal section derive sampling algorithm based quantum annealing clustering. goal contribution three folds well-formed quantum eﬀect section appropriate similarity measure clustering section annealing schedule section goal make eﬃcient sampling algorithm. similar fashion construct gibbs sampler pqa-st whose computational complexity however sampler easily stuck local optima example pqa-st draw fig.. fig. pqa-st better state pqa-st i.e. pqa-st since sampler pqa-st changes label data point time sampler cannot sample eﬃciently. statistical mechanics cluster label permutation sampler applied cases fig.. label permutation sampler change cluster assignments draws cluster label permutation e.g. words sampler exchanges rows matrix deﬁned case fig. equal four choices label permutation. computational complexity sampler because normalization factor requires summation choices. sampler tractable statistical mechanics relatively small however intractable machine learning large. derivation called suzukitrotter expansion. show details derivation appendix means sampling approximated sampling pqa-st. shows pqa-st similar parallel {psa}m quantum interaction note i.e. interaction disappears pqa-st becomes independent sas. takes completely diﬀerent. thus call similarity. even ﬁnite show approximation becomes exact enough annealing iterations passed annealing schedule proposed section similarity depends quantum eﬀect diﬀerent results diﬀerent similarity. example derive algorithm quantum eﬀect gives similarity going back fig. notice case pqa-st independent interaction canceled pqa-st unlikely search hand pqa-st likely search interaction allows construct gibbs sampler based pqa-st similar fashion although sampler tractable statistical mechanics intractable machine learning. give solution using another representation diﬀerent develop sampler need label permutation. however computational complexity step much expensive thus sampler less eﬃcient proposed sampler even though sampler need solve label permutation. experiments observe qa-st works well suboptimal assignments {σj}m convergence. shows qa-st searches better assignment suboptimal {σj}m hand current {σj}m away global optimum even sub-optima qa-st necessarily work well. comparing terms sampled i.e. interaction σj+. hand become close regardless energy discussion beginning larger large enough collect suboptimal assignments become larger point make {σj}m closer. best path would like fig.. fig. stronger beginning allow qa-st search good assignments strong quantum interaction always smaller hence qa-st never makes {σj}m words qa-st search closer. middle assignment {σj}m speciﬁcally following annealing schedule step algorithm note much diﬀerence diﬃculty qa-st choose annealing schedules. general choose schedule schedule given. shown next section qa-st works well thus diﬃculty choosing annealing schedules qa-st reduced choosing schedule show experimental results fig.. three rows fig. vary schedule ﬁxed schedule qa-st work better best energy schedule lets path fig.. bottom ﬁgure compare qa-st slower schedule experiment shows whether qast still works better slower schedule improves apply qa-st models mixture gaussians conjugate normalinverse-wishart prior latent dirichlet allocation models parameters marginalized data. thus qa-st search maximum posteriori assignment applied mnist data applied nips corpus reuters. mnist randomly choose data points apply reduce dimensionality nips corpus documents randomly choose words vocabulary. also randomly choose documents words vocabulary reuters. mnist nips corpus reuters respectively. schedule qa-st particular qa-st qa-st. diﬀerence qa-st keeps qa-st similar terms β-annealing fair comparison data vary ﬁxed path becomes fig.. data qa-st. consume time qa-st. thus compare qa-st terms iteration fig.. consequently mnist nips reuters respectively. fig. plot qa-st happen iteration qa-st fig. left column middle column show minimum mean energy {σj}m since optimization problem interested minimum energy left column. data qa-st achieved better results right column fig. shows mean purity expect larger resulted higher bottom fig. shows result nips slower schedule schedule third fig.. although found better results third fig. qa-st still worked better experimental results also consistent claim matsuda works better diﬃcult problems. worked better mog. right column fig. shows converged smaller values mog. means local optima mog. many techniques accelerate sampling studied. techniques applied proposed algorithm. example split-merge sampler permutation augmented sampler global move escape local minima. techniques available proposed algorithm well. also apply exchange monte carlo method. applied quantum annealing clustering. best knowledge ﬁrst study clustering. proposed quantum eﬀect clustering derived qa-based sampling algorithm. also proposed good annealing schedule crucial applications. computational complexity larger single simulated annealing however empirically shown ﬁnds better clustering assignment best multiple-run randomly restarted consumes time words better many times. actually typical many times sa’s fast cooling schedule temperature necessarily global optimum. thus strongly believe novel alternative optimizing clustering. addition easy implement proposed algorithm similar tally show qa’s performance problem like paper. however worth trying develop qabased algorithms diﬀerent models e.g. bayesian networks diﬀerent quantum eﬀect proposed algorithm looks like genetic algorithms terms running multiple instances. studying relationship also interesting future work. work partially supported research priority areas physics quantum phases superclean materials mext also next generation super computer project nanoscience program mext. special thanks taiji suzuki tprimal members sato lab.", "year": 2014}