{"title": "Reinforcement Learning of Speech Recognition System Based on Policy  Gradient and Hypothesis Selection", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Speech recognition systems have achieved high recognition performance for several tasks. However, the performance of such systems is dependent on the tremendously costly development work of preparing vast amounts of task-matched transcribed speech data for supervised training. The key problem here is the cost of transcribing speech data. The cost is repeatedly required to support new languages and new tasks. Assuming broad network services for transcribing speech data for many users, a system would become more self-sufficient and more useful if it possessed the ability to learn from very light feedback from the users without annoying them. In this paper, we propose a general reinforcement learning framework for speech recognition systems based on the policy gradient method. As a particular instance of the framework, we also propose a hypothesis selection-based reinforcement learning method. The proposed framework provides a new view for several existing training and adaptation methods. The experimental results show that the proposed method improves the recognition performance compared to unsupervised adaptation.", "text": "speech recognition systems achieved high recognition performance several tasks. however performance systems dependent tremendously costly development work preparing vast amounts task-matched transcribed speech data supervised training. problem cost transcribing speech data. cost repeatedly required support languages tasks. assuming broad network services transcribing speech data many users system would become self-sufﬁcient useful possessed ability learn light feedback users without annoying them. paper propose general reinforcement learning framework speech recognition systems based policy gradient method. particular instance framework also propose hypothesis selection-based reinforcement learning method. proposed framework provides view several existing training adaptation methods. experimental results show proposed method improves recognition performance compared unsupervised adaptation. today’s speech recognition systems heavily rely supervised training using large amounts task-matched training data achieve high speech recognition performance. prepare labeled speech data large transcription cost required. particularly problem resource-limited languages. however even resourcerich languages signiﬁcant factor limits application area speech recognition additional transcription cost required support tasks different initial training condition. considering network applications automatic speech recognition many users strategy improve system performance without incurring development cost utilize feedback users providing recognition results them. ogata developed service called podcastle uses speech recognizer automatically transcribe speech contents podcasts users read search system includes user interface allows users correct recognition errors word word. gathering corrected transcriptions speech recognition system re-estimated improved using supervised model training adaptation methods system motivation users errors automatic transcriptions contribute sharing contents like. however considerable amount effort required produce correct transcription user contribution would limited contents enthusiastic listeners. users asked recognition quality rather corrections errors transcriptions system could utilize scalar feedback update model reinforcement learning would greatly reduce effort required users. reducing effort users larger applications would become possible. reinforcement learning based common sense idea action followed improvement state affairs tendency produce action strengthened major formalizations reinforcement learning value-based methods including q-learning approaches policy-based methods including policy gradient methods paper ﬁrst formulate general reinforcement learning framework speech recognition systems based policy gradient method. then propose reinforcement learning method following framework feedback based hypothesis selection users. remainder paper organized follows. ﬁrst brieﬂy review application reinforcement learning speech information processing section policy gradient method section explain proposed method section implementation experiments section experimental setup described section results shown section finally conclusions presented section many studies apply reinforcement learning speech dialogue systems improve dialogue control source enhancement koizumi proposed q-learningbased method dnn-based system method speech enhancement performance improved based feedback human evaluators perceptual quality enhanced speech. however studies apply reinforcement learning speech recognition systems limited noted studies speech recognition nisida proposed method tunes update coefﬁcient adaptation gmm-hmm method used conﬁdence measure obtained result viterbi decoding utterance reward. therefore human interaction. small used speech segments high conﬁdence large used segments conﬁdence. molina proposed two-pass decoding method also based conﬁdence measure idea reinforce phone models second pass high conﬁdence value whereas weakened conﬁdence. algorithm choice phone models decoding process regarded action reinforcement learning broad sense. conﬁdence measure estimated ﬁrst pass used second pass adding value acoustic likelihood. algorithm decoding process acoustic model updated. methods based intuitive ideas modify model update decoding process based conﬁdence measure. however connections major formalizations reinforcement learning methods explained. sense two-pass unsupervised adaptation algorithms reject conﬁdence hypotheses also seen type reinforcement learning. general setup policy gradient method-based reinforcement learning system actions policy function takes state returns probability distribution action take. policy function parameterized parameters action sampled executed. according action system gets scalar reward tion performed applying gradient ascent method. however points that reward evaluated given choice action exist analytical functional form reward enumerating possible actions tractable. therefore need scheme evaluate gradient follows parallel derivation process natural evolution strategy using log-trick assume situation speech recognition system used serve vast number general users internet. users input speech data want transcribe. data would include recordings school lectures invited talks presentations meetings. interactive applications voice input email also target. users want reasonably good transcript quickly easily time correct recognition errors word word. user interface equipped mechanism allows users provide scalar evaluation score recognition result user feedback. several design choices types scores expect users provide intentionally unintentionally assume given utterance basis. function takes feature sequence utterance input returns probability distribution word sequence recognition hypothesis action. particular recognition system based acoustic model language model probability distribution given equation assume want update acoustic model dnn-hmm want update parameters better predict posterior probability states gradient equation becomes independent language model. moreover decomposed time frame becomes acoustic feature vector time frame state aligned frame. equation indicates update formula reinforcement learning dnn-hmm using policy gradient method simply reward weighted version normal cross-entropy based back-propagation. update formula satisﬁes criterion reinforce algorithm form shown equation conﬁdence measure reward round binary value clearly state conventional unsupervised adaptation hypothesis rejection mechanism mentioned section example policy gradient-based reinforcement learning hypothesis obtained sampling. utilize human feedback direct measure recognition performance word accuracy. however asking general users evaluate word accuracy would realistic. even users technical background speech recognition time consuming calculate. avoid problem propose hypothesis selection-based reinforcement learning method prepare recognition systems. system subject reinforcement learning used rival. input utterance recognition hypothesis sampled systems presented user. then user selects better hypothesis among them. case selection feedback system feedback hypothesis ﬁrst system selected feedback otherwise. based binary reward update using weighted gradient deﬁned equation standard evaluation including hours speech data used evaluate updated models using data set. table summarizes data sets. feedback users simulated evaluating word error rates hypotheses system using reference labels performing hypothesis selection based true wer. simulate selection errors caused users experiments introduced random swapping selected unselected hypotheses performed. input acoustic features dimensional fmllr features. computed using lattices lattices made forced aligning true labels training decoding speech data large batches evaluation set. size input layer dnns hidden layers sigmoid activation function. units hidden layer units output softmax layer. dnn-hmm baseline system trained pretraining ﬁne-tuning using -hour labeled training data. large batch based reinforcement learning initial learning rates batches stages respectively. -hour labeled training data always used mixing unlabeled large batches. learning rate controls training data large batches based cross-validation using labeled trained data held set. learning rate halved improvement cross-entropy cross-validation fell below epoch. upper limit number iterations epoch figure shows outline reinforcement learning process. unlabeled large batch decoded using initial baseline dnn-hmm model. candidate hypotheses best results n-best list candidate hypotheses either ﬁfth results list. n-best list created first used viterbi decoding normal speech recognition systems best hypothesis rather sampling hypothesis posterior distribution. second instead preparing separate rival system used n-th best hypothesis system rival hypothesis constant. refer best hypothesis candidate hypothesis rival hypothesis candidate hypothesis since hypotheses come model used symmetric manner gradient update shown equation corresponds collecting feedbacks actions time. example assuming compute gradient using candidate hypothesis selected weight candidate hypothesis weight otherwise. third parameter update reinforcement learning performed based large batches rather utterance utterance update. mainly purpose quick implementation. rigorous implementation sampling unnormalized posterior beam sampling could used another strategy preparing rival system would system randomly selected previous stage update alphago rewriting equation becomes equation form seen hypothesis selection method similar discriminative training tries increase difference likelihood selected hypothesis hypothesis however selected hypothesis reference usually contains errors within formulation expected reward. performed experiments using data corpus spontaneous japanese based recipe kaldi speech recognition toolkit experiments made subsets original training data. ﬁrst subset contained hours data used labeled training train initial baseline system. subset hours total divided four subsets contained hours data. four subsets used unlabeled large batches reinforcement learning assuming corresponding transcripts given system. additionally figure shows simulated results relation selection error rate users wers selected hypotheses. selection error rate equal lower expect lower selected hypotheses candidate hypotheses. based analysis next investigated performance reinforcement learning errors hypotheses selection. figure shows wers. stage figure conﬁrmed reinforcement learning still outperformed unsupervised adaptation. stage slight increase observed unsupervised adaptation reinforcement learning reason before. finally evaluated performance reinforcement learning th-best results used candidate hypotheses instead th-best results. figure shows wers selection errors. reinforcement learning improvement became small reinforcement learning still gave better results unsupervised adaptation. paper proposed policy gradient-based reinforcement learning framework speech recognition systems also proposed hypothesis selecting-based reinforcement learning method particular instance framework. experiments shown proposed method reduces compared unsupervised adaptation. tendencies simulated noise hypothesis selection introduced improvement became slightly smaller. number stages increased tendency increase unsupervised adaptation reinforcement learning several cases. future work includes addressing problem overtraining adjusting strategy learning rate number iterations stage improving performance investigating effective ways update model. decoded lattice. ﬁrst updated model made using large batch used recognize large batch based recognition results model updated making next model process repeated large batches. comparison purposes unsupervised adaptation performed model updated using candidate hypothesis withhypothesis selection. figure shows successively updated models based unsupervised adaptation reinforcement learning using large batches sequentially. stage initial baseline model used decode large batch hypothesis selection model update wers ﬁgure based -best result. therefore differences wers arise stage ﬁgure initial model indicates wers large batches using baseline initial model. unsupervised adaptation gave better results non-updated initial model. reinforcement learning th-best results used candidate hypotheses. using reinforcement learning larger improvement unsupervised adaptation obtained coefﬁcient chosen choosing greater means hypotheses used. lowest obtained larger second hypothesis affected gradient much greatly increased. stage slightly increased except including unsupervised adaptation. partly learning rate reducing strategy optimal partly fourth batch simply contained relatively difﬁcult utterances recognize seen using initial model also higher compared large batches. evaluate updated models using data figure shows wers common evaluation set. initial baseline model unsupervised adaptation gave absolute improvement stage. consistent improvement observed reinforcement learning gave lowest stage. leggetter woodland maximum likelihood linear regression speaker adaptation continuous density hidden markov models computer speech language vol. j.-l. gauvain c.-h. maximum posteriori estimation multivariate gaussian mixture observations markov chains ieee transactions speech audio processing vol. seide deng gong adaptation context-dependent deep neural networks automatic speech recognition proc. ieee workshop spoken language technology volodymyr mnih koray kavukcuoglu david silver alex graves ioannis antonoglou daan wierstra martin riedmiller playing atari deep reinforcement learning nips deep learning workshop. richard sutton david mcallester satinder singh yishay mansour policy gradient methods reinforcement learning function approximation proceedings international conference neural information processing systems nips’ volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu asynchronous methods deep reinforcement learning proceedings international conference machine learning maria florina balcan kilian weinberger eds. vol. proceedings machine learning research pmlr. vandyke gasic mrksic young learning real users rating dialogue success neural networks reinforcement learning spoken dialogue systems. proc. interspeech wang multi-agent reinforcement learning algorithm disambiguation spoken dialogue system proceedings international conference technologies applications artiﬁcial intelligence. taai ieee computer society. koizumi niwa hioka kobayashi haneda dnn-based source enhancement self-optimized reinforcement learning using sound quality measurements proc. icassp march molina yoma huenupan garreton wuth maximum entropy-based reinforcement learning using conﬁdence measure speech recognition telephone speech ieee transactions audio speech language processing vol. july hansen m¨uller koumoutsakos reducing time complexity derandomized evolution strategy covariance matrix adaptation evolutionary computation vol. jurgen gael yunus saatci whye zoubin ghahramani beam sampling inﬁnite hidden markov model proceedings international conference machine learning. icml acm. silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot dieleman grewe nham kalchbrenner sutskever lillicrap leach kavukcuoglu graepel hassabis mastering game deep neural networks tree search nature vol. jan. povey ghoshal boulianne burget glembek goel hannemann motlıcek qian schwarz silovskı stemmer veselı kaldi speech recognition toolkit proc. asru", "year": 2017}