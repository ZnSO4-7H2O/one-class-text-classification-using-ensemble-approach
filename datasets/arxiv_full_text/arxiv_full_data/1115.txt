{"title": "Variational Walkback: Learning a Transition Operator as a Stochastic  Recurrent Net", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We propose a novel method to directly learn a stochastic transition operator whose repeated application provides generated samples. Traditional undirected graphical models approach this problem indirectly by learning a Markov chain model whose stationary distribution obeys detailed balance with respect to a parameterized energy function. The energy function is then modified so the model and data distributions match, with no guarantee on the number of steps required for the Markov chain to converge. Moreover, the detailed balance condition is highly restrictive: energy based models corresponding to neural networks must have symmetric weights, unlike biological neural circuits. In contrast, we develop a method for directly learning arbitrarily parameterized transition operators capable of expressing non-equilibrium stationary distributions that violate detailed balance, thereby enabling us to learn more biologically plausible asymmetric neural networks and more general non-energy based dynamical systems. The proposed training objective, which we derive via principled variational methods, encourages the transition operator to \"walk back\" in multi-step trajectories that start at data-points, as quickly as possible back to the original data points. We present a series of experimental results illustrating the soundness of the proposed approach, Variational Walkback (VW), on the MNIST, CIFAR-10, SVHN and CelebA datasets, demonstrating superior samples compared to earlier attempts to learn a transition operator. We also show that although each rapid training trajectory is limited to a finite but variable number of steps, our transition operator continues to generate good samples well past the length of such trajectories, thereby demonstrating the match of its non-equilibrium stationary distribution to the data distribution. Source Code: http://github.com/anirudh9119/walkback_nips17", "text": "propose novel method directly learn stochastic transition operator whose repeated application provides generated samples. traditional undirected graphical models approach problem indirectly learning markov chain model whose stationary distribution obeys detailed balance respect parameterized energy function. energy function modiﬁed model data distributions match guarantee number steps required markov chain converge. moreover detailed balance condition highly restrictive energy based models corresponding neural networks must symmetric weights unlike biological neural circuits. contrast develop method directly learning arbitrarily parameterized transition operators capable expressing nonequilibrium stationary distributions violate detailed balance thereby enabling learn biologically plausible asymmetric neural networks general non-energy based dynamical systems. proposed training objective derive principled variational methods encourages transition operator \"walk back\" multi-step trajectories start datapoints quickly possible back original data points. present series experimental results illustrating soundness proposed approach variational walkback mnist cifar- svhn celeba datasets demonstrating superior samples compared earlier attempts learn transition operator. also show although rapid training trajectory limited ﬁnite variable number steps transition operator continues generate good samples well past length trajectories thereby demonstrating match non-equilibrium stationary distribution data distribution. source code http//github.com/anirudh/walkback_nips fundamental goal unsupervised learning involves training generative models understand sensory data employ understanding generate sample data make inferences. machine learning vast majority probabilistic generative models learn complex probability distributions data fall classes directed graphical models corresponding ﬁnite time feedforward generative process like variational auto-encoder energy function based undirected graphical models corresponding sampling stochastic process whose equilibrium stationary distribution obeys detailed balance respect energy function detailed balance condition highly restrictive example energy-based undirected models corresponding neural networks require symmetric weight matrices speciﬁc computations match well biological neurons analog hardware could compute. contrast biological neural circuits capable powerful generative dynamics enabling model world imagine futures. cortical computation highly recurrent therefore generative dynamics cannot simply purely feed-forward ﬁnite time generative process directed model. moreover recurrent connectivity biological circuits symmetric generative dynamics cannot correspond sampling energy-based undirected model. thus asymmetric biological neural circuits brain instantiate type stochastic dynamics arising repeated application transition operator∗ whose stationary distribution neural activity patterns non-equilibrium distribution obey detailed balance respect energy function. despite fundamental properties brain dynamics machine learning approaches training generative models currently lack effective methods model complex data distributions repeated application transition operator indirectly speciﬁed energy function rather directly parameterized ways inconsistent existence energy function. indeed lack methods constitutes glaring pantheon machine learning methods training probabilistic generative models. fundamental goal paper provide step ﬁlling proposing novel method learn directly parameterized transition operators thereby providing empirical method control stationary distributions non-equilibrium stochastic processes obey detailed balance match distributions data. basic idea underlying training approach start training example iteratively apply transition operator gradually increasing amount noise injected heating process yields trajectory starts data manifold walks away data heating mismatch model data distribution. similarly update denoising autoencoder modify parameters transition operator make reverse heated trajectory likely reverse cooling schedule. encourages transition operator generate stochastic trajectories evolve towards data distribution learning walk back heated trajectories starting data points. walkback idea introduced generative stochastic networks denoising autoencoders heuristic without temperature annealing. here derive speciﬁc objective function learning parameters principled variational lower bound hence call training method variational walkback despite fact training procedure involves walking back trajectories last ﬁnite variable number time-steps empirically yields transition operator continues generate sensible samples many time-steps used train demonstrating ﬁnite time training procedure sculpt non-equilibrium stationary distribution transition operator match data distribution. show emerges naturally variational derivation need annealing arising objective making variational bound tight possible. describe experimental results illustrating soundness proposed approach mnist cifar- svhn celeba datasets. intriguingly ﬁnite time training process involves modiﬁcations variational methods training directed graphical models potentially asymptotically inﬁnite generative sampling process corresponds non-equilibrium generalizations energy based undirected models. thus goes beyond disparate model classes undirected directed graphical models simultaneously incorporating good ideas each. variational walkback training process goal learn stochastic transition operator repeated application yields samples data manifold. reﬂects underlying temperature modify training process. transition operator speciﬁed parameters must learned data. steps chosen generate sample generative process temperature step joint probability transitions. destructive forward process starts datapoint gradually heats applications qtt. larger temperatures right correspond ﬂatter distribution whole destructive forward process maps data distribution gaussian creation process operates reverse. strategy similar introduced alain bengio particular imagine destructive process qtt+ starts data point evolves stochastically obtain trajectory data distribution. note chains share parameters transition operator start different priors ﬁrst step data distribution factorized prior training procedure trains transition operator make reverse transitions destructive process likely. reason index time destructive process operates forward time reverse generative process operates backwards time data distribution occurring particular need train transition operator reverse time -step step making unnecessary solve deep credit assignment problem performing backpropagation time across multiple walk-back steps. overall destructive process generates trajectories walk away data manifold transition operator learns walkback trajectories sculpt stationary distribution match data distribution. choose parameters transition operator joint whole sequence differing initial distributions trajectory. also choose increase temperature time destructive process following temperature schedule thus forward destructive process corresponds heating protocol. training procedure similar spirit dae’s major difference destructive process works corresponds addition random noise knows nothing current generative process training. understand tying together destruction creation good idea consider special case corresponds stochastic process whose stationary distribution obeys detailed balance respect energy function undirected graphical model. learning model involves fundamental goals model must place probability mass data located remove probability mass elsewhere. probability modes data known spurious modes fundamental goal learning hunt spurious modes remove them. making destructive process identical transition operator learned motivated notion destructive process efﬁciently explore spurious modes current transition operator. walkback training destroy modes. contrast dae’s net’s since destructive process corresponds addition unstructured noise knows nothing generative process clear agnostic destructive process efﬁciently seek spurious modes reverse generative process. chose annealing schedule empirically minimize training time. generative process starts sampling state broad gaussian whose variance initially equal total data variance sample ptmax tmax high enough temperature resultant injected noise move state across whole domain data. injected noise used simulate effects ﬁnite temperature variance linearly proportional temperature. thus equivalent noise injected transition operator choose tmax achieve goal ﬁrst sample able move across entire range data distribution. successively cool temperature sample previous states according reduced factor step followed steps temperature cooling protocol requires number steps order tmax steps. choose random distribution. thus training procedure trains rapidly transition simple gaussian distribution data distribution ﬁnite variable number steps. ideally training procedure indirectly create transition operator whose repeated iteration samples data distribution relatively rapid mixing time. interestingly intuitive learning algorithm recurrent dynamical system formalized algorithm derived principled manner variational methods usually applied directed graphical models next. algorithm variationalwalkback train generative model associated transition operator temperature parameterized transition operator injects noise variance step noise level temperature require transition operator sample compute gradient similarly algorithm approximately maximize log-likelihood -step procedure. parameters generative model parameters approximate inference procedure seeing next example ﬁrst step update towards maximizing variational bound example stochastic gradient descent step. second step update setting objective reduce term decomposition. sec. regarding conditions tightness bound perfect yielding possibly biased gradient force constraint continue iterating procedure training examples obtain unbiased monte-carlo estimator follows single trajectory respect sampled data distribution single sequence |s). making reverse heated trajectories sampled heating process except explicitly different parameters variants however p-parameters implicitly forming using likelihood gradient iteratively form approximate posterior. tightness variational lower bound |s)] seen controlled temperature probability observing transition stationary state time-reversal transition operator makes reverse transition equally likely state pairs therefore obeys pairs states well known valid stochastic transition operator stationary distribution furthermore process obeys detailed balance invariant time-reversal better understand divergence temperature relation replace cooling process occurs backwards time time-reversal unfolding forward time expense introducing ratios stationary probabilities. also exploit fact transition operator. substitutions ﬁrst term simply divergence distribution heated trajectories time reversal cooled trajectories. since heating cooling processes tied divergence time-reversal invariance requirement vanishing divergence equivalent transition operator obeying detailed balance temperatures. similar shape product probability ratios computed annealed importance sampling reverse annealed importance sampling manifest that slow incremental annealing schedules comparing probabilities state slightly different distributions ratios close example many steps slow annealing generative process approximately reaches stationary distribution slow annealing corresponds quasistatic limit statistical physics work required perform transformation equal free energy difference states. faster must perform excess work beyond free energy difference excess work dissipated heat surrounding environment. writing distributions terms energies free energies e−e/tt e−−fk e−−f] second term divergence closely related average heat dissipation ﬁnite time heating process intriguing connection size variational lower bound excess heat dissipation ﬁnite time heating process opens door exploiting wealth work statistical physics ﬁnding optimal thermodynamic paths minimize heat dissipation provide ideas improve variational inference. summary tightness variational bound achieved transition operator approximately obeys detailed balance temperature annealing done slowly many steps. intriguingly magnitude looseness bound related physical quantities degree irreversiblity transition operator measured divergence time reversal excess physical work equivalently excess heat dissipated performing heating trajectory. check post-hoc potential looseness variational lower bound measure degree irreversibility estimating divergence dkl|s)πt obeys detailed balance therefore time-reversal invariant. quantity estimated long sequence sampled repeatedly applying transition operator draw quantity strongly positive forward transitions likely reverse transitions process time-reversal invariant. estimated divergence normalized corresponding entropy relative value estimating likelihood importance sampling derive importance sampling estimate negative log-likelihood following procedure. training example sample large number destructive paths following formulation estimate log-likelihood approach allows considerable freedom choosing transition operators obviating need specifying indirectly energy function. consider bernoulli isotropic gaussian transition operators binary real-valued data respectively. form stochastic state update imitates discretized version langevin differential equation. bernoulli transition operator computes element-wise probability sigmoid∗st−+α∗fρ gaussian operator computes conditional mean standard deviation log). functions arbitrary parametrized functions neural temperature time step natural question ﬁnite time training process learn transition operator whose stationary distribution matches data distribution repeated sampling beyond training time continues yield data samples. partially address this prove following theorem proposition enough capacity training data training time slow enough annealing small departure reversibility match convergence training transition operator data generating distribution stationary distribution. proof found appendix essential intuition ﬁnite time generative process converges data distribution multiple different walkback time-steps remains data distribution future time cannot always guarantee preconditions theorem experimentally essential outcome holds practice. related work variety learning algorithms cast framework fig. example directed graphical models like vaes dbns helmholtz machines general corresponds recognition model transforming data latent space corresponds generative model goes latent visible data ﬁnite number steps. none directed models designed learn transition operators iterated inﬁnitum moreover learning models involves complex deep credit assignment problem limiting number unobserved latent layers used generate data. similar issues limited trainable depth ﬁnite time feedforward generative process apply generative adversarial networks also eschew goal speciﬁcally assigning probabilities data points. method circumvents deep credit assignment problem providing training targets time-step; essence past time-step heated trajectory constitutes training target future output generative operator thereby obviating need backpropagation across multiple steps. similarly unlike generative stochastic networks draw also require training iterative operators backpropagating across multiple computational steps. similar spirit approaches retains crucial differences. first frameworks corresponds simple destruction process unstructured gaussian noise injected data. agnostic destruction process knowledge underlying generative process learned therefore cannot expected efﬁciently explore spurious modes regions space unoccupied data assigns high probability. advantage using high-temperature version model part destructive process better random noise injection ﬁnding spurious modes. second crucial difference ties weights transition operator across time-steps thereby enabling learn bona transition operator iterated well beyond training time unlike daes net. there’s also another related recent approach learning transition operator denoising cost developed parallel called infusion training tries reconstruct target data chain instead previous step destructive chain. experiments evaluated four datasets mnist cifar svhn celeba mnist svhn cifar datasets used except uniform noise added mnist cifar theis aligned cropped version celeba scaled pixels pixels center-cropped pixels used adam optimizer theano framework details appendix code training generation http//github.com/anirudh/walkback_nips. image generation. figure show samples datasets. mnist real-valued views data modeled. image inpainting. clamped bottom part celeba test images model. figure shows generated conditional samples. main advance involves using variational inference learn recurrent transition operators rapidly approach data distribution iterated much longer training time still remaining data manifold. innovations enabling achieve involved tying weights across time tying destruction generation process together efﬁciently destroy spurious modes using past destructive process train future creation process thereby circumventing issues deep credit assignment introducing aggressive temperature annealing schedule rapidly approach data distribution introducing variable trajectory lengths training encourage generator stay data manifold times longer training sequence length. indeed often difﬁcult sample recurrent neural networks many time steps duration training sequences especially non-symmetric networks could exhibit chaotic activity. transition operators learned stably sampled exceedingly long times; example experiments trained model celeba steps test time sampled time-steps. overall method learning transition operator outperforms previous attempts learning transition operators using local learning rule. overall introduced approach learning non-energy-based transition operators inherits advantages several previous generative models including training objective requires rapidly generating data ﬁnite number steps re-using parameters step directly parametrizing generator using model quickly spurious modes also anchor algorithm variational bound show analysis suggests transition operator destruction inference process creation generation process cooling schedule generation reverse heating schedule inference. connected variational physical notions like reversibility heat dissipation. novel bridge variational inference concepts like excess heat dissipation non-equilbrium statistical physics could potentially open door improving variational inference exploiting wealth work statistical physics. example physical methods ﬁnding optimal thermodynamic paths minimize heat dissipation could potentially exploited tighten lowerbounds variational inference. moreover motivated relation variational reversibility veriﬁed empirically model converges towards approximately reversible chain making variational bound tighter. obey detailed balance yielding access larger potentially powerful space models. particular enables relax weight symmetry constraint undirected graphical models corresponding neural networks yielding brain like iterative computation characteristic asymmetric biological neural circuits. approach thus avoids biologically implausible requirement weight transport arises consequence imposing weight symmetry hard constraint. hard constraint removed although training procedure converge towards symmetry. approach towards symmetry consistent empirical observations theoretical analysis auto-encoders symmetric weights associated minimizing reconstruction error. connection neurobiology dreams learning rule underlying applied asymmetric stochastic neural network yields speculative intriguing connection neurobiology dreams. discussed bengio spike-timing dependent plasticity plasticity rule found brain corresponds increasing probability conﬁgurations towards network intrinsically likes reverse-stdp corresponds forgetting unlearning states towards network goes update applied neural network resultant learning rule indeed strengthen synapses presynaptic neuron active postsynaptic neuron generative cooling process weakens synapses postsynaptic neuron active presynaptic neuron heated destructive process suggested neurobiological function sleep involves re-organizing memories particular unlearning spurious modes reverse-stdp heating destructive process sleep states brain hunting destroying spurious modes. contrast cooling generative dynamics awake states stdp reinforces neural trajectories moving towards observed sensory data. mapping relative incoherence dreams compared reality qualitatively consistent heated destructive dynamics compared cooled transition operator place awake states. future work many questions remain open terms analyzing extending particular interest incorporation latent layers. state step would include visible latent components. essentially procedure except chain initialization sample posterior distribution given another interesting direction replace log-likelihood objective step gan-like objective thereby avoiding need inject noise independently pixels transition step allowing latent variable sampling inject required high-level decisions associated transition. based earlier results sampling latent space rather pixel space allow better generative models even better mixing modes overall work takes step ﬁlling relatively open niche machine learning literature directly training non-energy-based iterative stochastic operators hope many possible extensions approach could lead rich class powerful brain-like machine learning models. acknowledgments authors would like thank benjamin scellier poole cooijmans philemon brakel gaétan marceau caron alex lamb helpful feedback discussions well nserc cifar google samsung nuance canada research chairs funding compute canada computing resources. s.g. would like thank simons mcknight james mcdonnell burroughs wellcome foundations ofﬁce naval research support. would also like thank geoff hinton analogy used work discussing contrastive divergence authors would also like express debt gratitude towards contributed theano years making great tool. bengio thibodeau-laufer alain yosinski deep generative stochastic networks trainable backprop. proceedings international conference international conference machine learning volume icml’ pages ii––ii–. jmlr.org. goodfellow pouget-abadie mirza warde-farley ozair courville bengio generative adversarial nets. advances neural information processing systems pages netzer wang coates bissacco reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning volume page vincent larochelle bengio manzagol p.-a. extracting composing robust features denoising autoencoders. proceedings international conference machine learning pages acm. vincent larochelle lajoie bengio manzagol p.-a. stacked denoising autoencoders learning useful representations deep network local denoising criterion. machine learning res. proposition enough capacity training data training time slow enough annealing small departure reversibility match convergence training transition operator data generating distribution stationary distribution. proof. conditions data distribution. means data distribution running annealed trajectory steps integer last steps temperature since last steps temperature apply transition operator. consider consecutive sampling steps among last steps. samples coming distribution means temperature transition operator leaves data distribution unchanged. implies data distribution eigenvector linear operator associated temperature transition operator data generating distribution stationary distribution temperature transition operator. image inpainting samples celeba dataset shown sub-ﬁgure shows masked image face bottom sub-ﬁgure shows inpainted image. images drawn test set. samples celeba cifar svhn shown figure inpainting celeba images. images left ground truth images corrupted bottom half goal bottom half face image given observed half image images right show inpainted lower halves images. fig. shows model chains repeated application transition operator temperature empirically prove conjecture mentioned paper ﬁnite time generative process converges data distribution multiple different walkback time-steps remains data distribution future time section provide details architecture used dataset. details hyper parameter architecture used dataset also found tables complete speciﬁcations available experiment scripts http//github.com/ anirudh/walkback_nips. lower bound comparisons network trained mnist composed fully connected layers units using batch-normalization network different ﬁnal layers number units corresponding image size corresponding mean variance pixel. softplus output variance. don’t share batch-normalization parameters across different time steps. real-values mnist dataset samples used encoder-decoder architecture convolutional layers. encoder consists convolutional layers kernel length stride followed decoder strided convolutions. addition used fully connected feedforward layers connect encoder decoder. applied batch normalization convolutional layers applied layer normalization feedforward layers. network separate output layers corresponding mean gaussian sample corresponding variance added gaussian noise. adam learning rate optimize network. details hyper parameter architecture also available table similar encoder-decoder architecture stated above. convolutional layers encoder well decoder. also apply batch normalization convolutional layers well layer normalization feedforward layers. details hyper parameter architecture also available table table hyperparameters mnist experiments layer encoder-decoder adam optimizer learning rate model mean variance pixel. reconstruction error per-step loss function. improvements using layernorm bottleneck compared batchnorm. using dropout also helps results reported paper without dropout. table hyperparameters celeba experiments layer encoder-decoder adam optimizer learning rate model mean variance pixel. reconstruction error per-step loss function. table hyperparameters cifar experiments layer encoder-decoder adam optimizer learning rate model mean variance pixel. reconstruction error per-step loss function. variational walkback algorithm three unique hyperparameters. specify number walkback steps used training number extra walkback steps used sampling also temperature increase step. conservative setting would allow model slowly increase temperature training. however would require large number steps model walk noise would signiﬁcantly slow training process also means would require large number steps used sampling. exist dynamic approach setting number walkback steps temperature schedule. work hyperparameters heuristically. found heating temperature schedule step produced good results initial temperature. sampling found good results using exactly reversed schedule step index total number cooling steps. mnist cifar svhn celelba training steps sampling steps. also found could achieve better quality results allow model extra steps temperature sampling. finally model able achieve similar results compared model. considering model uses steps mnist uses steps mnist. figure samples svhn dataset using gaussian noise transition operator. model trained using steps walk away samples generated using annealing steps. observed empirically variational lower bound necessarily correspond sample quality. among trained models higher value lower bound clear indication visually better looking samples. mnist samples shown example phenomenon. model better lower bound could give better reconstructions producing better generated samples. resonates ﬁnding measured degree estimating divergence dkl|s)πt obeys detailed balance therefore time-reversal invariant computing monte-carlo estimator long sequence sampled repeatedly applying transition operator draw i.e. taking samples burn-in period sense magnitude reversibility measure corresponds estimated divergence estimate corresponding entropy normalizing denominator telling much depart reversibility nats relative number nats entropy. justify this consider minimal code length required code figure proposed modeling framework trained swiss roll data. algorithm trained swiss roll annealing steps using annealing schedule increasing temperator time. shown every sample suppose evaluate samples using instead code them. code length fractional increase code length wrong distribution report here forward transition probability backward transition probability. compute quantity took best model mnist time steps constant temperature. learned generative chain time steps getting computing generative chain divided per-step average. runs compute ∗log generative chain. estimate entropy unit time chain. repeated multiple times average many runs reduce variance estimator. figure sample chain starting pure noise. model trained using steps walk away samples generated using steps annealing. ﬁgure shows every sample chain column. image experiments observed different batchnorm papemeters different steps actually improves result considerably. different batchnorm parameters also necessery making work mixture gaussian. authors able make work without different parameters. possible could optimizer know different step giving temperature information optimizer too. figure sample chain. coloumn corresponds sampling chain. shown every sample. applied transition operator time-steps temperature demonstrate even long chain transition operator continues generate good samples. figure sample chain. column corresponds sampling chain. shown every sample. applied transition operator time-steps temperature demonstrate even long chain transition operator continues generate good samples.operation convolution convolution convolution table hyperparameters svhn experiments layer encoder-decoder adam optimizer learning rate model mean variance pixel. reconstruction error per-step loss function. figure sample chain. column corresponds sampling chain. shown every sample. applied transition operator time-steps temperature demonstrate even long chain transition operator continues generate good samples. figure sample chain. column corresponds sampling chain. shown every sample. applied transition operator time-steps temperature demonstrate even long chain transition operator continues generate good samples. figure sample chain. column corresponds sampling chain. shown every sample. applied transition operator time-steps temperature demonstrate even long chain transition operator continues generate good samples. figure samples models higher lower bound whose samples shown figure generated samples clearly good suggesting either bound sometimes tight enough log-likelihood always clear indicator sample quality.", "year": 2017}