{"title": "Triplet Probabilistic Embedding for Face Verification and Clustering", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Despite significant progress made over the past twenty five years, unconstrained face verification remains a challenging problem. This paper proposes an approach that couples a deep CNN-based approach with a low-dimensional discriminative embedding learned using triplet probability constraints to solve the unconstrained face verification problem. Aside from yielding performance improvements, this embedding provides significant advantages in terms of memory and for post-processing operations like subject specific clustering. Experiments on the challenging IJB-A dataset show that the proposed algorithm performs comparably or better than the state of the art methods in verification and identification metrics, while requiring much less training data and training time. The superior performance of the proposed method on the CFP dataset shows that the representation learned by our deep CNN is robust to extreme pose variation. Furthermore, we demonstrate the robustness of the deep features to challenges including age, pose, blur and clutter by performing simple clustering experiments on both IJB-A and LFW datasets.", "text": "tigate much performance improved provided domain speciﬁc data. speciﬁcally contributions follows propose deep network architecture training formulate triplet probability embedding learning method improve performance deep features face veriﬁcation subject clustering. training publicly available face dataset train deep architecture. image pre-processed aligned canonical view passing deep network whose features used represent image. case ijb-a dataset data divided splits split containing training test set. hence improve performance learn proposed triplet probability embedding using training provided split features extracted dcnn model. deployment phase given face template extract deep features using model implementing automatic pre-processing steps face detection ﬁducial extraction. deep features projected onto low-dimensional space using embedding matrix learned training -dimensional feature ﬁnal representation given face template. paper organized follows section places work among recently proposed approaches face veriﬁcation. section details network architecture training scheme. triplet probabilistic embedding learning method described section followed results ijb-a datasets brief discussion section section demonstrate ability proposed method cluster media collection ijb-a datasets. related work past years numerous works using deep features tasks related face veriﬁcation. deepface approach uses carefully crafted alignment procedure preprocess face images feeds despite signiﬁcant progress made past twenty years unconstrained face veriﬁcation remains challenging problem. paper proposes approach couples deep cnn-based approach low-dimensional discriminative embedding step learned using triplet probability constraints address unconstrained face veriﬁcation problem. aside yielding performance improvements embedding provides signiﬁcant advantages terms memory post-processing operations like subject speciﬁc clustering. experiments challenging ijb-a dataset show proposed algorithm performs close state methods veriﬁcation identiﬁcation metrics requiring much less training data training/test time. superior performance proposed method dataset shows representation learned deep robust large pose variation. furthermore demonstrate robustness deep features challenges including pose blur clutter performing simple clustering experiments ijb-a datasets. introduction recently advent curated face datasets like labeled faces wild advances learning algorithms like deep neural nets hope unconstrained face veriﬁcation problem solved. face veriﬁcation algorithm compares given templates typically seen training. research face veriﬁcation progressed well past years resulting saturation performance dataset problem unconstrained face veriﬁcation remains challenge. evident performance traditional algorithms publicly available ijb-a dataset released recently. moreover despite superb performance cnn-based approaches compared traditional methods drawback methods long training time needed. work present deep architecture ensures faster training invesdeep network trained using large training set. recently facenet uses large private dataset train several deep network models using triplet distance loss function. training time network order weeks. since release ijb-a dataset several works published veriﬁcation results dataset. previous approaches presented train deep networks using casia-webface dataset vgg-face dataset respectively requiring substantial training time. paper proposes network architecture training scheme needs shorter training time small query time. idea learning compact discriminative representation around decades. weinberger used semi deﬁnite programming -based formulation learn metric satisfying pairwise triplet distance constraints large margin framework. recently idea successfully applied face veriﬁcation integrating loss function within deep network architecture joint bayesian metric learning also another popular metric used face veriﬁcation methods either require large dataset convergence learn metric directly therefore amenable subsequent operations like discriminative clustering hashing. classic methods like t-sne t-ste crowd kernel learning perform extremely well used visualize cluster given data collection. either operate data matrix directly distance matrix generated data generating large pairwise triplet constraints. methods perform well given data points generalize out-of-sample data. current work generalize formulations traditional classiﬁcation setting domain speciﬁc training testing data provided. formulate optimization problem based triplet probabilities performs dimensionality reduction aside improving discriminative ability test data. embedding scheme described work general framework applied setting labeled training data available. section details architecture training algorithm deep network used work. architecture consists convolutional layers varying kernel sizes. initial layers larger size rapidly subsampling image reducing parameters subsequent layers consist small ﬁlter sizes proved useful face recognition tasks furthermore parametric rectiﬁer linear units instead relus since allow negative value output three convolutional layers initialized weights alexnet model trained imagenet challenge dataset. several recent works empirically shown transfer knowledge across different networks albeit different objective improves performance signiﬁcantly reduces need train large number iterations. compared methods either learn deep models scratch ﬁnetune last layer fully pretrained models. former results large training time latter generalize well task hand hence resulting optimal performance. current work even though pretrained model initialize proposed deep network ﬁrst three convolutional layers since retain generic information subsequent layers learn representations speciﬁc task hand. thus learn task speciﬁc information convolutional layers consisting kernels size layers conv-conv downsample input thereby learning complex higher dimensional representations. hybrid architecture proves extremely effective representation outperforms deep models ijb-a dataset addition achieve performance training proposed deep network using relatively smaller casia-webface dataset. architecture network shown table layers conv-conv fully connected layers fc-fc initialized scratch using random gaussian distributions. prelu activation functions added layer. since network used feature extractor last layer removed deployment thus reducing number parameters inputs network images. network deployed features extracted layer resulting dimensionality network trained using learning discriminative embedding section describe algorithm learning low-dimensional embedding resulting projections discriminative. aside improved performance embedding provides signiﬁcant advantages terms memory enables post-processing operations like visualization clustering. consider triplet class belongs different class. consider function parameterized matrix rn×n measures similarity vectors ideally triplets exist training would like following constraint satisﬁed speciﬁc form similarity function given case deep features normalized unit length. learn embedding given triplets solve following optimization features. initialized ﬁrst principal components training data. iteration random anchor random positive data point chosen. choose negative perform hard negative mining choose data point least likelihood among randomly chosen negative instances iteration. since compute embedding matrix optimizing triplet probabilities call method triplet probability embedding technique closest presented section used recent works computes embedding based satisfying hinge loss constraint interpreted maximizing likelihood minimizing negative log-likelihood triplet practice problem solved large-margin framework using stochastic gradient descent triplets sampled online. gradient update given estimate iteration updated estimate triplet sampled current iteration learning rate. choosing dimension achieve dimensionality reduction addition improved performance. work based cross validation dimensionality deep acts margin parameter loss function. consistent terminology used paper call triplet distance embedding appreciate difference approaches figure shows case gradient update method occurs. value appropriately chosen triplet considered good even positive negative close another. proposed formulation cases referred figure update gradient contribution gradient modulated probability violate constraint modulation factor speciﬁed term gradient update implying likelihood sampled triplet satisfying high gradient update given lower weight vice-versa. thus method margin parameter automatically based likelihood. figure sample comparison pairs dataset used evaluating face veriﬁcation approaches handle pose variation. hence consists images frontal view images extreme proﬁle. data organized splits containing equal number frontal-frontal frontal-proﬁle comparisons. sample comparison pairs dataset shown figure pre-processing training phase given input image hyperface method face detection ﬁducial point extraction. hyperface detector automatically extracts many faces given image. ijb-a dataset since images contain face bounding boxes provided along dataset select person interest list automatic detections. select detection maximum area overlap manually provided bounding box. ijb-a dataset images hyperface detector canﬁnd relevant face. missed cases crop face using bounding information provided dataset pass hyperface extract ﬁducials. ﬁducial points align detected image canonical view using similarity transform. dataset since keypoints cannot computed proﬁle faces three keypoints side face aligning them. parameters training times training proposed deep architecture done using momentum learning rate decreased uniformly factor every iterations. weight decay layers. training batch size training time deep network hours single nvidia titanx gpu. ijb-a dataset training data provided split obtain triplet embedding takes mins split. additional splitwise processing done proposed approach. deployment average enrollment time image pre-processing including alignment feature extraction figure performance improvement ijb-a split plot. values speciﬁed brackets. compare relative performances features projection plot traditional curve far) split ijb-a verify protocol three methods figure equal error rate metric speciﬁed method. performance improvement signiﬁcant especially regions far= observed similar behaviour splits ijb-a dataset. experimental setup results section evaluate proposed method challenging datasets iarpa janus benchmark-a dataset contains subjects total images faces ijb-a dataset contain extreme poses illuminations challenging sample images ijb-a dataset shown figure additional challenge ijb-a veriﬁcation protocol template comparisons include image image image comparisons. work given test template ijb-a data perform kinds pooling produce ﬁnal representation average pooling deep features images and/or frames present template combined taking componentwise average produce feature vector. thus feature equally contributes ﬁnal representation. media pooling deep features combined keeping mind media source come from. metadata provided ijb-a gives media item template. thus ﬁnal feature vector ﬁrst take intra-media average combine taking intermedia average. thus feature’s contribution ﬁnal representation weighted based source. table identiﬁcation veriﬁcation results ijb-a dataset. identiﬁcation scores reported tpir values indicated points. results averages splits standard deviation given brackets methods reported them. implies result reported method. best results given bold. given image pre-process described section deep features computed average image ﬂip. given deep features compare compute cosine similarity score. speciﬁcally ijb-a dataset given template containing multiple faces ﬂatten template features average pooling media pooling obtain vector representation. split learn projection using provided training data. given templates comparison compute cosine similarity score using projected -dimensional representations. matrix. report types results ijb-a dataset veriﬁcation identiﬁcation. veriﬁcation protocol report false non-match rate values several false match rates identiﬁcation results report open closed metrics. open metrics true positive identiﬁcation rate quantiﬁes fraction subjects classiﬁed correctly among ones exist probe gallery. closed metrics report numbers different values false positive identiﬁcation rates ranks. details evaluation metrics ijb-a protocol found dataset following protocol report area curve equal error rate values averages across splits addition classiﬁcation accuracy. obtain accuracy split threshold similarity scores threshold value provides highest classiﬁcation accuracy training data split. table presents results proposed methods compared existing results ijb-a veriﬁcation identiﬁcation protocol. compared methods described below government-of-the-shelf baseline parkhi train deep network vgg-face dataset contains images subjects. neural aggregation network trained large amount videos celeb- dataset starting googlenet architecture. deep based approach includes combination in-plane aligned images rendered images augment performance. rendered images also generated test time template comparison. noted many test images ijb-a dataset contain extreme poses harsh illumination conditions signiﬁcant blur. crosswhite template adaptation tune evaluating clustering results metrics deﬁned summarized below pairwise precision fraction pairs samples within cluster among possible pairs class total number cluster pairs. pairwise recall fraction pairs samples within class among possible pairs placed cluster total number same-class pairs. simplest found demonstrate effectiveness deep features proposed method standard matlab implementation agglomerative clustering algorithm average linkage metric. cosine similarity basic clustering metric. simple clustering algorithm used computational complexity current form scale large datasets millions images. currently working efﬁcient scalable version algorithm. clustering lfwimages dataset pre-processed described section image deep features extracted using proposed architecture averaged normalized unit norm. clustering algorithm entire data single shot. clustering algorithm takes input cut-off parameter acts distance threshold experiments vary cut-off parameter small range evaluate resulting clustering using f-score. pick result yields best f-score. table shows result approach compares recently released clustering approach based approximate rankorder clustering noted that case clustering result chosen varying number clusters picking best f-score. approach vary cut-off threshold property deep features hence intuitive parameter tune. table aside better performance total cluster estimate closer ground truth value compared methods proposed method trains single model casia-webface dataset consists images requires much shorter training time fast query time shown table features media pooling perform better compared methods across veriﬁcation identiﬁcation protocols ijb-a dataset exception template adaptation method crosswhite discussed below. method provides signiﬁcant improvement identiﬁcation veriﬁcation tasks shown table method crosswhite uses vgg-face network descriptors features. concept template adaptation improve performance follows pooling multiple faces given template train linear features template positive ﬁxed negatives extracted training data ijb-a splits. let’s denote pooled template feature classiﬁer pair then query time comparing templates similarity score computed even using carefully engineered fast linear classiﬁer training algorithm procedure increases time pooling procedure. query time template comparison also higher high dimensionality input features. contrast proposed approach requires matrix multiplication vector product comparison. using simple neural network architecture relatively smaller training dataset fast embedding method realized faster efﬁcient end-to-end system. improve performance further currently incorporating video data approach. dataset achieve state-of-art frontal-frontal frontal-proﬁle comparisons latter large margin. speciﬁcally frontal-proﬁle case manage reduce error rate noted fair comparison used features without performing tpe. shows features learn effective even extreme pose variations. figure sample clusters output clustering approach discussed section data split ijb-a dataset. shows robustness pose blur; bottom contains clusters robust clustering ijb-aijb-a dataset processed described section section cluster query templates provided split verify protocol. report results experiments features projected features projection matrix learned proposed method cut-off threshold required clustering algorithm learned automatically based training data i.e. choose threshold gives maximum fscore training data. scores reported table average values splits. expected method improves clustering performance features. subject estimate number clusters produced direct result clustering algorithm. pruned estimate obtained ignoring clusters fewer images. complete evaluation performance varying threshold values plot precision-recall curve ijb-a clustering experiment figure observed curve clustering ijb-a data using embedded features exhibits better performance operating points. transparent evaluation reporting f-score since latter effectively paper proposed deep cnn-based approach coupled low-dimensional discriminative embedding learned using triplet probability constraints large margin fashion. proposed pipeline enables faster training time improves face veriﬁcation performance especially fmrs. demonstrated effectiveness proposed method challenging datasets ijb-a achieved performance close state using deep model compact trained using moderately sized dataset. demonstrated robustness features using simple clustering algorithm ijb-a datasets. future work plan videos directly training also embed approach training deep network. intend scale clustering algorithm handle large scale scenarios large impostor sets order millions. acknowledgement research based upon work supported ofﬁce director national intelligence intelligence advanced research projects activity iarpa contract views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied odni iarpa u.s. government. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon. klare taborsky blanton cheney allen grother burge jain pushing frontiers unconstrained face detection recognition iarpa janus benchmark algorithms vol. ranjan patel chellappa hyperface deep multi-task learning framework face detection landmark localization pose estimation gender recognition arxiv ppreprint arxiv.", "year": 2016}