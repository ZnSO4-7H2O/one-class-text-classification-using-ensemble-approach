{"title": "Simple And Efficient Architecture Search for Convolutional Neural  Networks", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Neural networks have recently had a lot of success for many tasks. However, neural network architectures that perform well are still typically designed manually by experts in a cumbersome trial-and-error process. We propose a new method to automatically search for well-performing CNN architectures based on a simple hill climbing procedure whose operators apply network morphisms, followed by short optimization runs by cosine annealing. Surprisingly, this simple method yields competitive results, despite only requiring resources in the same order of magnitude as training a single network. E.g., on CIFAR-10, our method designs and trains networks with an error rate below 6% in only 12 hours on a single GPU; training for one day reduces this error further, to almost 5%.", "text": "neural networks recently success many tasks. however neural network architectures perform well still typically designed manually experts cumbersome trial-and-error process. propose method automatically search well-performing architectures based simple hill climbing procedure whose operators apply network morphisms followed short optimization runs cosine annealing. surprisingly simple method yields competitive results despite requiring resources order magnitude training single network. e.g. cifar- method designs trains networks error rate hours single gpu; training reduces error further almost neural networks rapidly gained popularity last years success variety tasks image recognition speech recognition machine translation cases neural networks still designed hand exhausting time-consuming process. additionally vast amount possible conﬁgurations requires expert knowledge restrict search. therefore natural goal design optimization algorithms automate neural architecture search. however classic optimization algorithms apply problem since architecture search space discrete conditional thus methods rely e.g. differentiability independent parameters applicable. growing interest using evolutionary algorithms reinforcement learning automatically designing architectures. unfortunately proposed methods either costly yield non-competitive performance. work dramatically reduce computational costs still achieving competitive performance. speciﬁcally contributions follows propose baseline method randomly constructs networks trains sgdr demonstrate simple baseline achieves test error cifar- already rivals several existing methods neural archictecture search. simplicity hope baseline provides valuable starting point development sophisticated methods future. propose neural architecture search hillclimbing simple iterative approach that step applies alternative network morphisms current network trains resulting child networks short optimization runs cosine annealing moves promising child network. nash ﬁnds trains competitive architectures computational cost order magnitude training single network; e.g. cifar- nash ﬁnds trains cnns error rate roughly hours single gpu. error reduced almost models different stages algorithm combined achieve error within days single gpu. cifar- achieve error close days. ﬁrst discuss related work section then formalize concept network morphisms section propose architecture search methods based section evaluate methods section conclude section hyperparameter optimization. neural networks known quite sensitive setting various hyperparameters learning rates regularization constants. exists long line research automated methods setting hyperparameters including e.g. random search bayesian optimization bandit-based approaches evolutionary strategies automated architecture search. recent years research focus shifted optimizing hyperparameters optimizing architectures. architectural choices treated categorical hyperparameters optimized standard hyperparameter optimization methods current focus development special techniques architectural optimization. popular approach train reinforcement learning agent objective designing well-performing convolutional neural networks baker train agent sequentially choose type layers parameters. zoph recurrent neural network controller sequentially generate string representing network architecture. approaches train generated networks scratch evaluate performance validation represents costly step. follow-up work agent learned build cells used building blocks neural network ﬁxed global structure. unfortunately training agent objective designing architecture extremely expensive baker zoph required fully trained networks requiring hundreds thousands days. overcome drawback proposed apply concept network transformations/morphisms within work basic idea transformation generate pre-trained architectures avoid large cost training networks scratch. compared work approach much simpler times faster obtaining better performance. real suganuma utilized evolutionary algorithms iteratively generate powerful networks small network. operations like inserting layer modifying parameters layer adding skip connections serve mutations framework evolution. whereas real also used enormous computational resources suganuma restricted relatively small networks handling population networks. contrast previous methods network capacity increases time saxena verbeek start training large network prune end. recently brock used hypernetworks generate weights randomly sampled network architecture goal eliminating costly process training vast amount networks. network morphism/ transformation. network transformations ﬁrst introduced chen context transfer learning. authors described function preserving operation make network deeper wider goal speeding training exploring network architectures. proposed additional operations e.g. handling non-idempotent activation functions altering kernel size introduced term network morphism. mentioned above used network morphisms architecture search though employ netdeeper netwider operators chen well altering kernel size i.e. limit search space simple architectures without e.g. skip connections. following give examples network morphisms standard operations building neural networks expressed network morphism. this network morphism type replace equation obviously holds morphism used fully-connected convolutional layer layers simply linear mappings. chen dubbed morphism netdeepernet. alternatively replacement could also choose ﬁxed non-learnable. case network morphism equation holds −cb. batch normalization layer written form represent batch statistics learnable scaling shifting. network morphism type assume function replace arbitrary function ˜hw˜h. parameters again equation trivially satisﬁed setting think modiﬁcations expressed morphism. firstly layer widened think layer widened. example simply double width. secondly skip-connections concatenation used huang formulated network morphism. sequence layers could choose realize skip layer subsequent network morphism type iii. deﬁnition every idempotent function special case operator deal non-linear non-idempotent activation functions. another example would insertion additive skip connection proposed simplify training could choose realize skip note every combinations network morphisms yields morphism. could example insert block conv-batchnorm-relu subsequent relu layer using equations proposed algorithm simple hill climbing strategy start small pretrained network. then apply network morphisms initial network generate larger ones perform better trained further. child networks seen neighbors initial parent network space network architectures. network morphism equation child networks start performance parent. essence network morphisms thus seen initialize child networks perform well avoiding expensive step training scratch thereby reducing cost evaluation. various child networks trained brief period time exploit additional capacity obtained network morphism search move best resulting child network. constitutes step proposed algorithm neural architecture search hill-climbing nash execute step several times performance validation saturates; note greedy process principle stuck poorly-performing region never escape evidence experiments. figure visualizes step nash approach algorithm provides full details algorithm. implementation function applyn orph applies network morphisms sampled uniformly random following three make network deeper i.e. conv-batchnorm-relu block described section position block well kernel size uniformly sampled. number channels chosen equal number channels closest preceding convolution. make network wider i.e. increase number channels using network morphism type conv layer widened well widening factor sampled uniformly random. model model start with nsteps number hill climbining steps nneigh number neighbours number net. morph. applied epochneigh number epochs training every neighbour epochf inal number epochs ﬁnal training initial λstart annealed λend sgdr training modelbest model start hill climbing nsteps note current best model also considered child i.e. algorithm forced select model rather also keep improves upon important method child networks need trained epochs hence optimization algorithm good anytime performance required. therefore chose cosine annealing strategy loshchilov hutter whereas learning rate implicitly restarted training line always starts learning rate λstart annealed λend epochneigh epochs. learning rate scheduler ﬁnal training presented method simple hill-climbing method note also interpreted simple evolutionary algorithm population size nneigh cross-over network morphisms mutations selection mechanism considers best-performing population member parent next generation. interpretation also suggests several promising possibilities extending simple method. i.e. epochneigh small since networks need trained. fact total number epochs training algorithm computed epochtotal epochneighnneighnsteps epochf inal. later experiments chose epochneigh modifying training networks harms eventual performance. finally compare proposed method automated architecture algorithms well hand crafted architectures. standard data augmentation scheme cifar datasets used loshchilov hutter following experiments. training split training validation purpose architecture search. eventually performance evaluated test set. experiments nvidia titan gpus code implemented keras tensorflow backend. comparing method others baseline experiments whether considerations previous chapter coincide empirical data. first investigate simple hill climbing strategy able distinguish models high performance. this nneigh i.e. model selction simply construct random networks train them. experiments nneigh compare results. parameters experiment namely nsteps epochneigh epochf inal choose λstart λend done loshchilov hutter model simple conv conv-maxpool-conv-maxpool-conv-fc-softmax pretrained epochs achieving validation accuracy figure appendix. algorithm able identify better networks would expect better results setting nneigh second experiment turn cosine annealing restarts hill climbing stage i.e. training line algorithm done constant learning rate namely starting learning rate sgdr λstart note still cosine decay ﬁnal training. results experiments summarized table hill climbing strategy actually able identify better performing models. notice hill climbing prefers larger models also sgdr plays important role. resulting models chosen algorithm training done constant learning rate perform similarly models without model selection strategy indicates performance epochs validation trained without sgdr correlates less ﬁnal performance test case training sgdr. constant learning rate epochs spent sufﬁcient improve performance model. figure shows progress running algorithm examples trained trained without sgdr. experiment investigate whether weight inheritance network morphisms used algorithm harms ﬁnal performance ﬁnal model. weight inheritance seen strong prior weights could suspect larger model able overcome possibly poor prior. additionally interested measuring overhead architecture search process compared times generating training model time needed training ﬁnal model scratch. retraining scratch figure best model found algorithm tracked time. without using sgdr training within hill climbing final training plotted. vertical lines highlight times network morphisms applied done number epochs total number epochs spent train model returned algorithm list ﬁndings table major difference observed model cases difference sometimes favor algorithm sometimes favor retraining scratch. experiments indicates algorithm harm ﬁnal performance model. regarding runtime overhead ﬁrst search architecture roughly factor think advantage method shows architecture search done order magnitude single training. compare algorithm popular wide residual networks state model gastaldi well automated architecture search methods. beside results nsteps previous section also tried nsteps generate larger models. improving results take snapshots best models every iteration running algorithm following idea huang using sgdr training. however different huang immediately fully trained models free snapshots trained validation rather training set. hence spent additional resources train snapshots sets. afterwards ensemble model build combining snapshot models uniform weights. table results cifar-. methods stated resources parameters errors averaged runs. resources spent denotes training costs case handcrafted models. table results cifar-. methods stated resources parameters errors averaged runs. resources spent denotes training costs case handcrafted models. error proposed method able generate competitive network architectures hours. spending another hours outperforms automated architecture search methods although require time gpus. reach performance handcrafted architectures well ones found zoph brock however note zoph spent resources did. unsurprisingly ensemble models perform better. simple cheap improve results everyone consider number parameters relevant. repeat previous experiment cifar-; hyperparameters changed. results listed table unfortunately automated architecture methods consider cifar. method real single gpu. snapshot ensemble performs similar brock ensemble model build runs compete hand crafted performance shake-shake network reached. proposed nash simple fast method automated architecture search based hill climbing strategy network morphisms training sgdr. experiments cifar- cifar- showed method yields competitive results requiring considerably less computational resources alternative approaches. algorithm easily extendable e.g. network morphisms evolutionary approaches generating models methods cheap performance evaluation hypernetworks better resource handling strategies sense hope approach serve basis development sophisticated methods yield improvements performance. references mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg man´e rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda vi´egas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous systems https//www.tensorflow.org/. software available tensorﬂow.org. geoffrey hinton deng dong george dahl abdel rahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara sainath brian kingsbury. deep neural networks acoustic modeling speech recognition. ieee signal processing magazine klein falkner springenberg hutter. learning curve prediction bayesian neural networks. international conference learning representations conference track april alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. pereira burges bottou weinberger advances neural information processing systems curran associates inc.", "year": 2017}