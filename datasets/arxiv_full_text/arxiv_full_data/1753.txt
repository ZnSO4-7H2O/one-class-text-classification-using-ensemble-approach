{"title": "Multi-task Sequence to Sequence Learning", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.", "text": "sequence sequence learning recently emerged paradigm supervised learning. date applications focused task much work explored framework multiple tasks. paper examines three multi-task learning settings sequence sequence models oneto-many setting encoder shared several tasks machine translation syntactic parsing many-to-one setting useful decoder shared case translation image caption generation many-to-many setting multiple encoders decoders shared case unsupervised objectives translation. results show training small amount parsing image caption data improve translation quality english german bleu points strong single-task baselines benchmarks. furthermore established state-of-the-art result constituent parsing lastly reveal interesting properties unsupervised learning objectives autoencoder skip-thought context autoencoder helps less terms perplexities bleu scores compared skip-thought. multi-task learning important machine learning paradigm aims improving generalization performance task using related tasks. framework widely studied thrun caruana evgeniou pontil ando zhang argyriou kumar among many others. context deep neural networks applied successfully various problems ranging language vision speech recently sequence sequence learning proposed kalchbrenner blunsom sutskever emerges effective paradigm dealing variable-length inputs outputs. seqseq learning core uses recurrent neural networks variable-length input sequences variable-length output sequences. relatively seqseq approach achieved state-of-the-art results original application machine translation also image caption generation constituency parsing despite popularity multi-task learning sequence sequence learning little work combining seqseq learning. best knowledge recent publication dong applies seqseq models machine translation goal translate language multiple languages. work propose three approaches complement another one-to-many approach tasks encoder common translation parsing; applies multi-target translation setting well many-to-one approach useful multisource translation tasks decoder easily shared translation image captioning lastly many-to-many approach share multiple encoders decoders study effect unsupervised learning translation. show syntactic parsing image caption generation improves translation quality english german bleu points strong single-task baselines benchmarks. furthermore established state-of-the-art result constituent parsing also explore unsupervised learning objectives sequence autoencoders skip-thought vectors reveal interesting properties setting autoencoder helps less terms perplexities bleu scores compared skip-thought. sequence sequence learning aims directly model conditional probability mapping input sequence output sequence accomplishes goal encoder-decoder framework proposed sutskever illustrated figure encoder computes representation input sequence. based input representation decoder generates output sequence unit time hence decomposes conditional probability natural model sequential data recurrent neural network used recent seqseq work. work however differ terms architecture unidirectional bidirectional deep multi-layer rnns; type long-short term memory gated recurrent unit another important difference seqseq work lies constitutes input representation early seqseq work uses last encoder state initialize decoder sets recently bahdanau proposes attention mechanism provide seqseq models random access memory handle long input sequences. accomplished setting encoder hidden states already computed. decoder side time step attention mechanism decide much information retrieve memory learning focus i.e. computing alignment weights input positions. recent work found crucial empower seqseq models attention mechanism. generalize work dong multi-task sequence-to-sequence learning setting includes tasks machine translation constituency parsing image caption generation. depending tasks involved propose categorize multi-task seqseq learning three general settings. addition discuss unsupervised learning tasks considered well learning process. scheme involves encoder multiple decoders tasks encoder shared illustrated figure input task sequence english words. separate decoder used generate sequence output units either sequence tags figure one-to-many setting encoder multiple decoders. scheme useful either multi-target translation dong different tasks. here english german imply sequences words respective languages. values give proportions parameter updates allocated different tasks. constituency parsing used sequence german words machine translation sequence english words autoencoders related sequence english words skip-thought objective scheme opposite one-to-many setting. illustrated figure consists multiple encoders decoder. useful tasks decoder shared example tasks include machine translation image caption generation addition machine translation perspective setting beneﬁt large amount monolingual data target side standard practice machine translation system also explored neural gulcehre lastly name describes category general consisting multiple encoders multiple decoders. explore scheme translation setting involves sharing multiple encoders multiple decoders. addition machine translation task include unsupervised objectives source target languages illustrated figure ﬁrst unsupervised learning task involves learning autoencoders monolingual corpora recently applied sequence sequence learning however work authors experiment pretraining ﬁnetuning joint training viewed form multi-task learning such interested knowing whether trend extends settings. additionally investigate skip-thought vectors context framework. skip-thought vectors trained training sequence sequence models pairs consecutive sentences makes skip-thought objective natural seqseq learning candidate. minor technical difﬁculty skip-thought objective training data must figure many-to-many setting multiple encoders multiple decoders. consider scheme limited context machine translation utilize large monolingual corpora source target languages. here consider single translation task unsupervised autoencoder tasks. consist ordered sentences e.g. paragraphs. unfortunately many applications include machine translation sentence-level data sentences unordered. address that split sentence halves; half predict half. dong adopted alternating training approach optimize task ﬁxed number parameter updates switching next task setting tasks diverse contain different amounts training data. result allocate different numbers parameter updates task expressed mixing ratio values parameter update consists training data task only. switching tasks select randomly task probability convention ﬁrst task reference task number training parameter updates task prespeciﬁed typical task trained parameter updates. convention makes easier fairly compare reference task single-task setting also trained exactly parameter updates. evaluate multi-task learning setup wide variety sequence-to-sequence tasks constituency parsing image caption generation machine translation number unsupervised learning summarized table experiments centered around translation task determine whether tasks improve translation vice versa. wmt’ data english⇆german translation problem. following luong frequent words language training corpus. vocabularies shared tasks except parsing target language vocabulary tags. newstest validation select hyperparameters e.g. mixing coefﬁcients. testing comparable existing results ﬁltered newstest english→german translation task newstest german→english task. summary table english→german translation german→english translation english unsupervised german unsupervised penn tree bank parsing high-conﬁdence corpus parsing image captioning table data training details information different datasets used work. task display following statistics number training examples sizes vocabulary number training epochs details frequent halve learning rates unsupervised tasks english german monolingual corpora wmt’. since experiments unsupervised tasks always coupled translation tasks validation test sets accompanied translation tasks. parsing tasks however evaluated validation test sets data. note also parse trees linearized following vinyals lastly image caption generation dataset image caption pairs provided vinyals experiments following sutskever luong train deep lstm models follows lstm layers -dimensional cells embeddings parameters uniformly initialized mini-batch size dropout applied probability vertical connections ﬁxed learning rate input sequences reversed lastly simple ﬁnetuning schedule epochs halve learning rate every epochs. values referred ﬁnetune start ﬁnetune cycle table together number training epochs task. described section multi-task experiment need choose task reference task choice reference task helps specify number training epochs ﬁnetune start/cycle values also training reference task alone fair comparison. make sure ﬁndings reliable experimental conﬁguration twice report average performance format mean explore several multi-task learning scenarios combining large task with small task penn tree bank parsing medium-sized task image caption generation another large task parsing high-conﬁdence corpus lastly unsupervised tasks autoencoders skip-thought vectors. terms evaluation metrics report validation test perplexities tasks. additionally also compute test bleu scores translation task. training sizes reported unsupervised tasks original wmt’ monolingual corpora randomly sample from. reduced sizes faster training time already three times larger parallel data. consider using monolingual data future work. setting want understand small task parsing help improve performance large task translation. since parsing task maps sequence english words sequence parsing tags encoder shared english→german translation task. result one-to-many scenario surprise results table suggest adding small number parsing minibatches improve translation quality substantially. concretely best multi-task model yields gain bleu points single-task baseline. worth pointing shown table single-task baseline strong even better equivalent non-attention model reported larger mixing coefﬁcients however overﬁt small corpus; hence achieve smaller gains translation quality. parsing vinyals shown attention crucial achieve good parsing performance training small corpus high attention-free systems setup nevertheless parsing results table indicate also beneﬁcial parsing yielding improvement points baseline. would interesting study useful presence attention mechanism leave future work. table english→german wmt’ translation penn tree bank parsing results shown perplexities bleu scores parsing various systems. muli-task models reference tasks italic mixing ratio parentheses. results averaged runs format mean best results highlighted boldface. investigate whether pattern carries medium task image caption generation. since image caption generation task maps images sequence english words decoder shared german→english translation task. hence setting falls many-to-one setting results table show trend observed before training another task small fraction time model improves performance main task. speciﬁcally parameter updates image caption generation updates translation obtain gain bleu scores strong single-task baseline. baseline almost bleu point better equivalent non-attention model reported luong while perplexities correlate well bleu scores shown observe empirically section parsing perplexities reliable less hence omit parsing perplexities table clarity. parsing test perplexities last four rows table valid perplexities similar. table german→english wmt’ translation captioning results shown perplexities bleu scores various tasks similar format table reference tasks italic mixing ratios parentheses. average results runs mean format. data. instead using small penn tree bank corpus consider large parsing resource high-conﬁdence corpus provided vinyals highlighted table trend consistent; helps boost translation quality bleu points. table english→german wmt’ translation shown perplexities bleu scores various translation models. multi-task systems combine translation parsing highconﬁdence corpus together. mixing ratios parentheses average results runs mean format. best results bolded. second experiments shifts attention parsing reference task. show table results combine parsing either english autoencoder task english→german translation task. models compared best attention-based systems including state-of-the-art result discussing multi-task results note interesting observations. first small parsing perplexities close achieved large training data. second baseline system obtain competitive score rivaling vinyals systems. rather surprising since models attention mechanism. closer look models reveal seems architectural difference vinyals -layer lstm cells -dimensional embeddings; whereas models -layer lstm cells -dimensional embeddings. supports ﬁndings larger networks matter sequence models. multi-task results autoencoder seem help parsing translation does. mixing ratio obtain non-negligible boost baseline multi-task system best single system reported furthermore ensembling different multi-task models able establish state-of-the-art result english constituent parsing score. training solely small penn tree bank corpus reduce perplexity evidenced poor parsing results table time parsing perplexities much smaller achieved translation task. parsing tags target vocabulary compared words translation case. note theoretical lower bound. parsing parsing autoencoder parsing autoencoder parsing autoencoder parsing translation parsing translation parsing translation parsing translation ensemble multi-task systems table large-corpus parsing results shown perplexities scores various parsing models. mixing ratios parentheses average results runs mean format. show individual perplexities runs small differences among them. vinyals parsing results lstm+a represents single lstm attention whereas lstm+a+e indicates ensemble systems. important results bolded. main focus section determine whether unsupervised learning help improve translation. speciﬁcally follow many-to-many approach described section couple german→english translation task unsupervised learning tasks monolingual corpora language. results tables show similar trend before small amount tasks case autoencoder objective mixing coefﬁcient improves translation quality bleu scores. however train autoencoder task i.e. larger mixing ratios translation performance gets worse. table german→english wmt’ translation unsupervised learning results shown perplexities translation unsupervised learning tasks. experiment autoencoders skip-thought vectors unsupervised objectives. numbers mean format average results runs; others only. skip-thought objectives hand behave differently. merely look perplexity metric results encouraging skip-thought data perform better consistently across translation unsupervised tasks. however computing bleu scores translation quality degrades increase mixing coefﬁcients. anticipate fact skip-thought objective changes nature translation task using half sentence predict half. problem autoencoder objectives however since think autoencoding sentence translating language. believe ﬁndings pose interesting challenges quest towards better unsupervised objectives satisfy following criteria desirable objective compatible supervised task focus e.g. autoencoders viewed special case translation unsupervised data intrinsic extrinsic metrics improved; skip-thought objectives satisfy criterion terms intrinsic metric extrinsic one. paper showed multi-task learning improve performance attention-free sequence sequence model found surprising training syntactic parsing image caption data improved translation performance given datasets orders magnitude smaller typical translation datasets. furthermore established state-of-the-art result constituent parsing ensemble multi-task models. also show unsupervised learning objectives autoencoder skip-thought behave differently context involving translation. hope interesting ﬁndings motivate future work utilizing unsupervised data sequence sequence learning. criticism work sequence sequence models employ attention mechanism leave exploration attention future work. bojar ondˇrej chatterjee rajen federmann christian haddow barry huck matthias hokamp chris koehn philipp logacheva varvara monz christof negri matteo post matt scarton carolina specia lucia turchi marco. findings workshop statistical machine translation. kyunghyun merrienboer bart gulcehre caglar bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoder-decoder statistical machine translation. emnlp gulcehre caglar firat orhan kelvin kyunghyun barrault loic huei-chi bougares fethi schwenk holger bengio yoshua. using monolingual corpora neural machine translation. arxiv preprint arxiv. heigold georg vanhoucke vincent senior alan nguyen patrick ranzato marc’aurelio devin matthieu dean jeffrey. multilingual acoustic models using distributed deep neural networks. icassp xiaodong jianfeng xiaodong deng kevin wang ye-yi. representation learning using multi-task deep neural networks semantic classiﬁcation information retrieval. naacl pham bluche th´eodore kermorvant christopher louradour j´erˆome. dropout improves recurrent neural networks handwriting recognition. frontiers handwriting recognition international conference ieee kelvin jimmy kiros ryan kyunghyun courville aaron salakhutdinov ruslan zemel richard bengio yoshua. show attend tell neural image caption generation visual attention. icml", "year": 2015}