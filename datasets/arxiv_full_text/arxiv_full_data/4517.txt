{"title": "On Kernelization of Supervised Mahalanobis Distance Learners", "tag": ["cs.LG", "cs.AI"], "abstract": "This paper focuses on the problem of kernelizing an existing supervised Mahalanobis distance learner. The following features are included in the paper. Firstly, three popular learners, namely, \"neighborhood component analysis\", \"large margin nearest neighbors\" and \"discriminant neighborhood embedding\", which do not have kernel versions are kernelized in order to improve their classification performances. Secondly, an alternative kernelization framework called \"KPCA trick\" is presented. Implementing a learner in the new framework gains several advantages over the standard framework, e.g. no mathematical formulas and no reprogramming are required for a kernel implementation, the framework avoids troublesome problems such as singularity, etc. Thirdly, while the truths of representer theorems are just assumptions in previous papers related to ours, here, representer theorems are formally proven. The proofs validate both the kernel trick and the KPCA trick in the context of Mahalanobis distance learning. Fourthly, unlike previous works which always apply brute force methods to select a kernel, we investigate two approaches which can be efficiently adopted to construct an appropriate kernel for a given dataset. Finally, numerical results on various real-world datasets are presented.", "text": "ratthachat chatpatanasiri ratthachat.cstudent.chula.ac.th teesid korsrilabutr gtkrcp.eng.chula.ac.th pasakorn tangchanachaianan pasakorn.tstudent.chula.ac.th boonserm kijsirikul boonserm.kchula.ac.th department computer engineering chulalongkorn university pathumwan bangkok thailand. paper focuses problem kernelizing existing supervised mahalanobis distance learner. following features included paper. firstly three popular learners namely neighborhood component analysis large margin nearest neighbors discriminant neighborhood embedding kernel versions kernelized order improve classiﬁcation performances. secondly alternative kernelization framework called kpca trick presented. implementing learner framework gains several advantages standard framework e.g. mathematical formulas reprogramming required kernel implementation framework avoids troublesome problems singularity etc. thirdly truths representer theorems assumptions previous papers related ours here representer theorems formally proven. proofs validate kernel trick kpca trick context mahalanobis distance learning. fourthly unlike previous works always apply brute force methods select kernel investigate approaches eﬃciently adopted construct appropriate kernel given dataset. finally numerical results various real-world datasets presented. recently many mahalanobis distance learners invented recently proposed learners carefully designed handle class problems data class form multi-modality classical learners principal component analysis fisher discriminant analysis cannot handle. therefore promisingly learners usually outperform classical learners experiments reported recent papers. nevertheless since learning mahalanobis distance equivalent learning linear inability learn non-linear transformation important limitation mahalanobis distance learners. research mahalanobis distance learning recently begun several issues left open eﬃcient learners non-linear extensions kernel trick standard non-linearization method fully automatic sense mathematical formulas derived programming codes implemented; convenient non-experts existing algorithms assume truth representer theorem however knowledge formal proof theorem context mahalanobis distance learning problem select eﬃcient ponent analysis large margin nearest neighbors discriminant neighborhood embedding kernelized order improve classiﬁcation performances respect algorithm. framework. contrast kernel trick kpca trick require users derive mathematical formulas. also whenever implementation original learner available users required re-implement kernel version original learner. moreover framework avoids problems singularity eigen-decomposition provides convenient speed learner. theorems justify kernel-trick kpca-trick frameworks. moreover theorems validate kernelized algorithms learning mahalanobis distance separable hilbert space also cover kernelized algorithms performing dimensionality reduction. alignment method proposed previous works whether appropriate kernelized mahalanobis distance learner not. secondly investigate simple method constructs unweighted combination base kernels. theoretical result provided support simple approach. kernel constructions based approaches require much shorter running time comparing standard cross validation approach. kernelizing mahalanobis distance learners provide explanation conduct extensive experiments real-world datasets prove usefulness kernelization. yi}n denote training labeled examples inputs corresponding class labels cp}. mahalanobis distance represented symmetric positive semi-deﬁnite matrix space matrices. given points matrix mahalanobis distance respect points deﬁned ||xi xj||m note standard setting purpose dimensionality reduction learn low-rank projection restricting learning best linear used compute distance points transformed space ||a∗xi a∗xj||. following subsections three popular algorithms whose objective functions mainly designed classiﬁcation presented. despite efﬁciency popularity three algorithms kernel versions thus paper primarily interested kernelizing three algorithms order improve classiﬁcation performances. original goal optimize leave-one-out performance training data. however actual classiﬁcation error non-smooth function matrix goldberger propose minimize stochastic variant score deﬁned follows lmnn output mahalanobis distance optimized goal point k-nearest neighbors always belong class examples diﬀerent classes separated large margin. point deﬁne target neighbors inputs label closest indicate whether input target neighbor input convenience deﬁne indicate whether labels match. variation lmnn called large margin component analysis proposes optimize instead however lmca preserve desirable properties convexity lmnn therefore algorithm kernel lmca presented diﬀerent kernel lmnn presented paper. main idea quite similar lmnn. seeks linear transformation neighborhood points class squeezed diﬀerent classes separated much possible. however care notion margin; case lmnn want every point stay points classes want average distance neighborhood points diﬀerent classes large. another diﬀerence lmnn learn full mahalanobis distance i.e. general weighted linear projection learn unweighted linear projection. similar lmnn deﬁne sets target neighbors point based euclidean distance input space. eigi nearest neighbors label eige nearest neighbors diﬀerent labels deﬁne follows symmetric matrix elements diagonal matrix matrix input points well-known result spectral graph theory symmetric necessarily psd. solve problem eigen-decomposition constraint added following optimization problem section focus kernelization frameworks going non-linearize three algorithms presented previous section. first standard kernel trick framework presented. next kpca trick framework alternative kerneltrick framework presented. kernelization framework conveniently done application kernel principal component analysis finally representer theorems proven validate applications kernelization frameworks context mahalanobis distance learning. note previous works validity applications kernel trick proven. ﬁnishing writing current paper knew name ﬁrst appeared paper chapelle sch¨olkopf ﬁrst applied method invariant support vector machines; moreover appeared kpca trick known researchers without knowing fact reinvented framework coincidentally called kpca trick ourselves. nevertheless shown section that context mahalanobis distance learning kpca trick non-trivially many advantages kernel trick; believe consequence consequence previous works. also mathematical tools provided previous works enough prove validity kpca trick context thus validation proof kpca trick needed emphasize above although kernel trick framework applied non-linearize three learners introduced section often happens ﬁnding much troublesome ﬁnding input space even optimization problems look similar section develop kpca trick framework much conveniently applied kernelize three learners. denote subsection central idea kpca trick represent ﬁnite-dimensional space without loss information. within framework coordinate example computed explicitly example coordinate used input existing mahalanobis distance learner. result using kpca trick place kernel trick need derive mathematical formulas need implement algorithms. note although unable numerically represent inner-product conveniently computed kpca likewise test point mapped consequently mapped data {ϕi} ﬁnite-dimensional kpca-trick algorithm consisting three simple steps shown figure lmnn learners including settings whose kernel versions previously unknown kernelized simple algorithm. therefore much convenient kernelize learner applying kpca-trick framework rather applying kernel-trick framework. algorithm denote mahalanobis distance learner maha performs optimization process shown outputs best mahalanobis distance easily achieved straightforwardly extending proof established representer theorem context mahalanobis distance learning however proofs provided previous works cannot directly extended. note that cases considered previous works learned hyperplane linear functional outputting -dimensional value. case shown learned linear which general outputs countably inﬁnite dimensional vector. hence prove validity kpca trick case need mathematical tools handle countably inﬁnite dimensionality. give versions representer theorems prove validity kpca trick current context. representer theorems fact that given objective function optimal value based input {φi} equal optimal value based input {ϕi}. hence representation safely applied. separate problem mahalanobis distance learning diﬀerent cases. ﬁrst theorem covers mahalanobis distance learners second theorem covers dimensionality reduction algorithms knowledge mathematical tools provided work sch¨olkopf smola enough prove theorem proof presented nonstraightforward extension sch¨olkopf smola’s work. also note theorem well theorem shown below general discuss above. justify kernel trick kpca trick proof. bounded linear functional eiiy bounded every since cauchy-schwarz inequality |haφ eiiy| ||aφ||||ei|| ||a||||φ||. riesz representation theorem eiiy written τiix unique since {ei}i∈n since every span conclude ˜ϕi. easily check haφi aφji ˜ϕj. hence whenever given construct results objective function value. reversing proof easy converse also true thus theorem proven positive integer thus assumption theorem well separable hilbert space valid. also theorems require objective function learning algorithm must depend {haφi aφji}n equivalently {hφi φji}n running time learner strongly depends dimensionality input data. recommended weinberger helpful ﬁrst apply dimensionality reduction algorithm performing learning process learning process tremendously speed retaining only says largestvariance principal components input data. kpca trick framework illustrated proof result similar omit details here. fact prove validation kdne need strong representer theorem apply theorem framework simply view ||τi|| ||a||hs ||hs square function regularizer becomes hilbert-schmidt norm operator. ﬁnite-dimensional norm reduced frobenius norm here allow norm bounded linear operator take value kernel trick result states optimal {τi} must represented {φui}. therefore using existing objective functions class learners namely regularized mahalanobis distance learners regularized knca regularized klmnn regularized kdne framework extended problem semi-supervised setproblem selecting eﬃcient kernel function central kernel machines. previous works mahalanobis distance learners exhaustive methods cross validation select kernel function. section investigate possibility automatically construct kernel appropriate mahalanobis distance learner. ﬁrst part section consider popular method called kernel alignment able learn training kernel theoretical result provided support simple approach. kernel constructions based approaches require much shorter running time comparing standard cross validation approach. here kernel alignment formulation belongs class quadratic programs solved eﬃciently formulations proposed lanckriet belong class semideﬁnite programs quadratically constrained quadratic programs respectively using standard trick absolute-valued objective function solved linear programming. note optimization algorithm minimizing upper bound desired objective function similar popular support vector machines hinge loss minimized instead loss. ment method mahalanobis distance learner optimal transformation returned. objective function depends inner product haφi aφji since proposition ha∗φk ha∗bφk′ thus applying training {φk′ learner tries minimize learner return linear objective value less equal notice invertible value fact optimal. consequently following claim stated there need apply methods usually learner discovers local solution hence learners based give solution. knca belongs case. non-existence unique global solution. optimization problems many diﬀerent linear maps optimal values hence guarantee learners based give solution. size constraints. size constraint used kdne arguments used previous subsection cannot applied i.e. given preprocesses. example learning takes place klmnn kdne algorithms specify target neighbors point case using kpca trick speciﬁcation based euclidean distance respect selected kernel case euclidean distance respect aligned kernel zero coeﬃcients. proposition assume often alignment algorithm return deﬁne above. following line proof proposition cases alignment method gives section advantages kpca trick kernel trick demonstrated. that conduct extensive experiments illustrate performance kernelized algorithms especially applying kernel construction methods described previous section. dimensional space formula cannot always implemented practice. order implement kernel-trick version knca users need prove following proposition stated original work goldberger step size. kernel-trick formulas knca thus ﬁnally achieved. however emphasize process proving proposition trivial tedious diﬃcult non-experts well practitioners focus tasks applications rather theories. moreover since formula ca/∂a signiﬁcantly diﬀerent ca/∂a users required re-implement knca convenient. contrast note diﬃculties disappeared kpca trick algorithm consisting three simple steps shown fig. applied instead kernel trick. another advantage using kpca trick knca. nature gradient optimizer takes large amount time knca converge local solution thus method speeding algorithms needed. recommended weinberger helpful ﬁrst apply performing learning process learning process tremendously speed retaining only says largest-variance principal components input data. kpca trick framework extra work required speed-up task kpca already applied ﬁrst place. similar knca online-available code lmnn employs gradient based optimization thus gradient formulas feature space derived implementation done order apply kernel trick. hand applying kpca trick original lmnn code immediately used. note kernel-trick formula kdne involves generalized eigenvalue problem instead plain eigenvalue problem involved dne. consequence face singularity full-rank constraint cannot satisﬁed. using problem i.e. elementary linear algebra shown full-rank {φi} linearly independent condition highly improbable. sugiyama yang yang yang suggest methods cope singularity problem context fisher discriminant analysis applicable kdne. sugiyama recommends constraint instead original constraint; however appropriate value tuned cross validation time-consuming. alternatively yang yang yang propose complicated methods directly minimizing objective function null space constraint matrix singularity problem explicitly avoided. note kpca-trick implementation kdne singularity problem plain eigenvalue problem solved. moreover klmnn applying kpca trick instead kernel trick kdne avoid tedious task modifying original code appropriately specify feature space. figure synthetic examples lmnn cannot learn eﬃcient mahalanobis distances knn. note example data class simple non-linear -dimensional subspace contrast kernel versions three algorithms learn eﬃcient distances i.e. non-linear subspaces discovered kernelized algorithms. table average accuracy standard deviation kernel versions. bottom win/draw/lose statistics kernelized algorithm comparing original version drawn. page lmnn paper weinberger gave comment klmnn lmnn already yields highly nonlinear decision boundaries original input space however obvious kernelizing algorithm lead signiﬁcant improvement’. here giving experimental results explain kernelizing algorithm lead signiﬁcant improvements. main intuition behind kernelization mahalanobis distance learners classiﬁcation algorithm lies fact non-linear boundaries produced usually helpful problems multi-modalities; however non-linear boundaries sometimes helpful data class stay low-dimensional non-linear manifold shown figure section conduct experiments lmnn kernel versions nine real-world datasets show really case kernelized algorithms usually outperform original versions real-world datasets performance linearly combined kernels achieved methods presented section comparable kernels exhaustively selected kernel alignment method requires much shorter running time. table average accuracy standard deviation kernel versions. name aligned kdne unweighted kdne balance breast cancer glass ionosphere iris musk pima satellite yeast win/draw/lose measure generalization performance algorithm nine realworld datasets obtained repository balance breast cancer glass ionosphere iris musk pima satellite yeast. following previous works randomly divide dataset training testing sets. repeating process times training testing sets dataset. generalization performance algorithm measured average test accuracy testing sets dataset. number training data except glass iris examples datasets contain total examples respectively. three methods consider scaled base kernels dimensionality input data. twenty based kernels speciﬁed following values considered kernelized algorithms implemented kpca trick illustrated figure noted figure ﬁgure illustrates performance unweighted kdne diﬀerent number base kernels. observed ﬁgure generalization performance unweighted kdne eventually stable base kernels. informative thus used specify target neighbors point. therefore cases klmnn kdne apply unweighted kernel employ euclidean distance respect input space specify target neighbors. slightly modify original codes lmnn fulﬁll desired speciﬁcation. experimental results shown tables results clear kernelized algorithms usually improve performance original algorithms. kernelized algorithms applying cross validation obtain best performance. outperform original methods datasets. kernel versions three original algorithms also satisﬁable performance. kernelized algorithms applying kernel alignment outperform original algorithms datasets obtain equal performance datasets. datasets original algorithms outperform kernel algorithms applying kernel alignment. similarly kernelized algorithms applying unweighted kernel outperform original algorithms datasets obtain equal performance datasets. datasets original algorithms outperform kernel algorithms applying unweighted kernel. note although cross validation method usually gives best performance kernel construction methods provide comparable results much shorter running time. dataset run-time overhead kernelized algorithms applying cross validation several hours run-time overheads kernelized algorithms applying aligned kernels unweighted kernel minutes seconds respectively dataset. therefore time-limited circumstance attractive apply aligned kernel unweighted kernel. note kernel alignment method appropriate multi-modal dataset several clusters data points class since function align attain maximum value points class collapsed single point. reason explains cross validated kernels give better results results aligned kernels experiments. developing kernel alignment algorithm suitable multi-modality currently open problem. comparing generalization performance induced aligned kernels unweighted kernel algorithms applying aligned kernels perform slightly better algorithms applying unweighted kernel. little overhead satisﬁable performance however unweighted kernel still attractive algorithms like required speciﬁcation target neighbors wij. since euclidean distance respect unweighted kernel usually appropriate specifying kpca-trick application algorithms like lmnn still require re-programming. noted previous section aligned kernels usually base kernels contrast unweighted kernel uses base kernels hence described section feature space corresponding unweighted kernel usually contains feature space corresponding aligned kernels. therefore informally feature space induced unweighted kernel larger ones induced aligned kernels. indeed occur. compactness show results unweighted kdne. experiments shown ﬁgure base kernels adding following order observed ﬁgure generalization performance unweighted kdne eventually stable base kernels. also observed base kernels enough obtain stable performance. interesting investigate overﬁtting behavior learner applying methods bias-variance analysis investigate whether appropriate apply adaptive resampling combining method improve classiﬁcation performance supervised mahalanobis distance learner. presented general frameworks kernerlize mahalanobis distance learners. three recent algorithms kernelized examples. although focused supervised settings frameworks clearly applicable learners settings well e.g. semi-supervised learner. representer theorems justify framework previous works formally proven. theorems also applied mahalanobis distance learners unsupervised semi-supervised settings. moreover present methods eﬃciently used constructing good kernel function training data. although concentrated mahalanobis distance learners kernel construction methods indeed applied kernel classiﬁers. numerical results various real-world datasets showed consistent improvements kernelized learners original versions. work supported thailand research fund. thank wicharn lewkeeratiyutkul taught theory hilbert space. also thank prasertsak pungprasertying preparing datasets experiments.", "year": 2008}