{"title": "ConvNets and ImageNet Beyond Accuracy: Explanations, Bias Detection,  Adversarial Examples and Model Criticism", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.CY", "stat.ML"], "abstract": "ConvNets and Imagenet have driven the recent success of deep learning for image classification. However, the marked slowdown in performance improvement, the recent studies on the lack of robustness of neural networks to adversarial examples and their tendency to exhibit undesirable biases (e.g racial biases) questioned the reliability and the sustained development of these methods. This work investigates these questions from the perspective of the end-user by using human subject studies and explanations. We experimentally demonstrate that the accuracy and robustness of ConvNets measured on Imagenet are underestimated. We show that explanations can mitigate the impact of misclassified adversarial examples from the perspective of the end-user and we introduce a novel tool for uncovering the undesirable biases learned by a model. These contributions also show that explanations are a promising tool for improving our understanding of ConvNets' predictions and for designing more reliable models", "text": "genet features. besides convolutional architectures initially developed image classiﬁcation residual networks routinely used machine translation speech recognition since top- error state models imagenet reduced recently evolution best performance seems plateau despite efforts designing novel architectures introducing data augmentation schemes optimization algorithms concomitantly several studies demonstrated lack robustness deep neural networks adversarial examples raised questions tendency exhibit biases adversarial examples synthetic images designed indistinguishable natural ones human capable fooling best image classiﬁcation systems. undesirable biases patterns behaviors learned data often highly inﬂuential decision model convnets imagenet driven recent success deep learning image classiﬁcation. however marked slowdown performance improvement recent studies lack robustness neural networks adversarial examples tendency exhibit undesirable biases questioned reliability sustained development methods. work investigates questions perspective end-user using human subject studies explanations. experimentally demonstrate accuracy robustness convnets measured imagenet underestimated. show explanations mitigate impact misclassiﬁed adversarial examples perspective end-user introduce novel tool uncovering undesirable biases learned model. contributions also show explanations promising tool improving understanding convnets’ predictions designing reliable models. convolutional neural networks imagenet instrumental recent breakthroughs image classiﬁcation. imagenet provided convnets data needed demonstrate superiority compared previously used handcrafted features fisher vectors success triggered renewed interest convolutional approaches. novel architectures resnets densenets introduced improve state performance imagenet. impact virtuous circle permeated aspects computer vision deep learning large. indeed feature extractors pre-trained imagenet ubiquitous. example state image segmentation pose estimation models heavily rely pre-trained imaaligned values society model operates. examples biases include racial gender biases accuracy leading factor broad adoption deep learning across industries sustained improvement together desirable properties robustness adversarial examples immunity biases critical maintaining trust technology. therefore essential improve understanding questions context imagenet especially perspective end-user. show human studies explanations valuable tools perform task. human studies yield judgment quality model’s predictions perspective end-user addition traditional ground truth used evaluate model. also feature-based example-based explanations. hand explaining prediction black classiﬁer subset features input yield valuable insights workings models underline essential features decision. hand examplebased explanations provide increased interpretability model yielding examples representative distribution given category captured model. particular form example-based explanation called model criticism combines prototypes criticisms proven capture better complex distributions. therefore facilitates human understanding. main ﬁndings summarized below accuracy convolutional networks evaluated imagenet vastly underestimated. example top- error resnet- trained imagenet evaluated standard validation instead errors validated human subjects. similarly top- error instead observation holds across models. robustness convnets adversarial examples also underestimated. addition providing explanations helps mitigate misclassiﬁcation adversarial examples perspective end-user. model criticism good tool detecting biases predictions models. further adversarial examples effectively used model criticism. similar observations ﬁrst point existed prior work however scale conclusions implications study different. indeed consider top- error measure interest imagenet related work adversarial achieve high accuracy previously unseen examples vulnerable small adversarial perturbations inputs perturbed inputs called adversarial examples recently aroused keen interest community several studies subsequently analyzed phenomenon various approaches proposed improve robustness neural networks closely related work different proposals aiming generating better adversarial examples given input example adversarial example perturbed version original pattern small enough undistinguishable human causes network predict incorrect target. given network p-norm adversarial example formally deﬁned represents strength adversary. assuming loss function differentiable propose take ﬁrst order taylor expansion compute solving following simpler problem sign corresponds fast gradient sign method instead obtain often normalized. optionally perform iterations steps using smaller step-size. strategy several variants rest paper refer method iterative fast gradient method measure robustness given model perform model criticism. model criticism. example-based explanations well-known tool realm cognitive science facilitating human understanding extensively used case-based reasoning improve interpretability models cases consists helping human understand complex distribution presenting prototypical examples. however distribution model seeking explanation complex prototypes enough. recently proposed addition prototypes data points sampled regions input space well captured model prototypes. examples called criticism known improve human’s mental model distribution. introduced mmd-critic approach inspired bayesian model criticism select prototypes critics among given examples. mmd-critic uses maximum mean discrepancy large-scale submodular optimization given exi= amples given rkhs kernel function prototypes mmd-critic selected minimizing maximum mean discrepancy formally written given prototypes criticisms similarly selected using maximize deviation prototypes. objective function case regularized promote diversity among criticisms. greedy algorithm used select prototypes criticisms since corresponding optimization problems submodular certain conditions work mmdcritic baseline example-based explanations. feature-based explanation. machine learning especially deep neural networks core technological advances across various ﬁelds. however models still widely considered black boxes leading users mistrust predictions even underlying models. order promote adoption algorithms foster positive technological impact recent studies focusing understanding model’s behavior human point view particular propose explain predictions classiﬁer approximating locally interpretable model role provide qualitative understanding input classiﬁer’s output given class. case input image vector denoting presence absence super-pixels partition image explain classiﬁer’s decision. best explanation minimizes local weighted loss vicinity regularized complexity explanation. authors restrict linear model whz. deﬁne vicinity samples using exponential kernel dataset perturbed samples obtained randomly activating deactivating super-pixels note denotes one-hot encoding superpixels whereas actual image formed superpixels. finally interpretability representation controlled authors solve optimization problem ﬁrst selecting features lasso learning weights least squares. case dnns explanation generated algorithm called lime allows user highlight super-pixels positive weights towards speciﬁc class follows keep constant. following refer image prompted solely super-pixels image attention. conduct study misclassiﬁcations various pre-trained architectures human subjects amazon mechanical turk center-cropped images size standard validation imagenet. setting holds experiments. every architecture consider examples misclassiﬁed model. examples presented different turkers together class predicted model. turker asked following question class relevant image. possible answers former means turker agrees prediction network latter means example misclassiﬁed. figure shows breakdown misclassiﬁed images resnet- according number positive answers receive turkers. ﬁrst results show resnet- misclassiﬁed examples turkers agree prediction model. consider prediction model given image correct least four turkers agree rectiﬁed top- error models drastically reduced. table shows original top- errors together rectiﬁed versions. resnet- densenet rectiﬁed top- error respectively similarly top- error resnet- single crop present misclassiﬁed images turkers together predictions model rectiﬁed top- error drops instead submitting turker misclassiﬁed images present pictures ground truth top- predictions rectiﬁed top- error drops therefore observation top- line conclusions regarding top-. top- important measure experiment suggests imagenet solved accuracy concerned. therefore explaining marked slowdown performance improvement observed recently imagenet. difference ground truth labeling images predictions models validated turkers traced back collection protocol imagenet. indeed create dataset deng ﬁrst queried several image search engines obtain good candidate images. turkers subsequently cleaned collected images validating contains objects given synset. labeling procedure ignores intrinsic multilabel nature images neither take account important factors composionality. indeed natural label wing also relevant picture displaying plane labelled airliner. figure shows examples misclassiﬁcations restnet- densenet-. cases predicted class present image. study robustness adversarial examples used center-cropped images imagenet validation set. next consider subset classes generate adversarial example legitimate image using ifgsm attack pre-trained resnet-. used step size maximum number iterations attack deteriorated accuracy network validation note deﬁnition need generate adversarial samples correctly predicted test images. consider settings experiment. ﬁrst conﬁguration present turkers whole adversarial image together prediction network. turker predicted label relevant given picture. again possible answers image shown different turkers. second conﬁguration show turker interpretation image generated lime instead whole adversarial image using important features. rest experimental setup identical previous conﬁguration showed whole images turkers. turker participated second experiment interpretation images participating ﬁrst experiment whole images displayed answers could biased. avoid issue perform studies three days intervals. similarly previous experiments report rectiﬁed top- error adversarial examples considering prediction model correct least turkers agree table shows standard rectiﬁed top- errors adversarial examples. observations made. first robustness models measured top- error adversarial samples generated validation imagenet also underestimated. indeed whole images displayed them turkers agree predictions networks adversarial examples. suggests often predicted figure test samples misclassiﬁed resnet- densenet- predicted class indicated ground truth black parenthesis. examples gathered four positive answers amt. note adversarial noise added images. inspected adversarial images turkers agree model’s prediction explanation shown mostly disagree whole image shown. figure displays examples images. cases even though predicted label seem correct looking whole image explanation following effects. either reveals relevant object supporting prediction creates ambiguous context renders predicted label plausible cases providing explanations user mitigates impact misclassiﬁcations adversarial perturbations. example-based explanation methods model criticism summarizing statistics learned model using carefully selected subset examples. mmdcritic proposes effective selection procedure combining prototypes criticisms improving interpretability. though applicable pre-trained neural network using hidden representation examples indirectly exploits discriminative nature classiﬁer. figure positive answers adversarial samples. images either displayed whole attention every image prompted different subjects. positive answer means subject agrees predicted adversarial class. label easily identiﬁable picture human. figure shows example explanations predictions legitimate adversarial versions respectively. adversarial perturbation exploits ambiguity image shifting attention model towards regions supporting adversarial prediction table results prototype criticism study. report average success rate task seven conditions. adversarial selection procedures outperforms mmdcritic well baselines used study. distribution achieve higher scores assignment task prototypes used prototypes combined criticisms second criticisms additionally prototypes always helps better grasp class distribution. qualitatively display figures prototypes criticisms class banana generated using respectively adversarial method test samples demonstrate superiority adversarial method. uncovering biases model criticism uncover undesirable biases learned model adversarial example approach model criticism since worked better mmd-critic previous experiments. consider class basketball select inspect reduced subset prototypes criticisms category basketball. percentage basketball training images least white person appears similarly percentage images least black person appears relative balance contrasts statistics captured model. indeed hand found prototypes contain least black person prototypes contain white person more. hand criticisms images contain least white person include black person more. suggests model learned biased representation class basketball images containing black persons prototypical. validate hypothesis sample pairs similar pictures internet. pairs sampled primary apparent difference images skin color persons. images model gathered predictions. figure shows results experiments. images containing black person classiﬁed basketball similar photos persons figure left adversarial image true class jeep predicted ambulance network. center explanation clean image prediction right explanation adversarial image prediction work argue adversarial examples accurate alternative mmd-critic model criticism. particular offer natural selecting prototypes criticism based number steps fgsm necessary change decision classiﬁer given example. indeed given class ﬁxed number maximum steps ifgsm examples still correctly classiﬁed steps considered useful prototypes since representative classiﬁer learned data class. contrast examples whose decision change steps fgsm likely valid criticisms quite model. conducted human study evaluate hypothesis. round present turker classes represented images each well target sample randomly drawn classes. measure well subject assign target sample class belongs. assignment task requires class well-explained images. challenge therefore select well candidate images. adversarial method selects prototypes examples misclassiﬁed ifgsm steps prototypes examples misclassiﬁed step. addition mmd-critic compare adversarial approach simple baseline using probabilities selecting prototypes criticisms also compare baseline randomly sampling examples given class. method experiment showing prototypes showing three prototypes three criticisms instead. properly exploit results discarded answers based time spent turkers question consider answers given range seconds minutes. resulted valid answers report results table observations arise results. first prototypes criticisms sampled using adversarial method represent better class figure adversarial samples displayed whole explanation adversarial class indicated true class black parenthesis. also displayed percentage positive answers displayed image cases positive answer means subject agrees predicted adversarial class. different skin color labeled differently. figure provides additional supporting evidence hypothesis. shows progressive feature based explanation path uncovering super-pixels example order importance. importance super-pixels depict jersey skin color player. reasons model learns biases unclear. hypothesis despite balanced distribution races pictures labeled basketball blacks persons represented class comparison classes. similar phenomenon previously noted context textual data defer remark focused racial biases human easily spot them. fact found similar biases pictures displaying asians dressed often classiﬁed ping-pong ball. however hypothesize biases model numerous diverse. example also found model often predicts class traffic-light images blue street lamps depicted figure case model criticism proven effective uncovering undesirable hidden biases learned model. human studies explanations proven performance sota models imagenet underestimated. leaves little room improvement calls large-scale benchmarks involving example multi-label annotations. also improved understanding adversarial examples perspective end-user positioned model criticism valuable tool uncovering undesirable biases. result open exciting perspective designing explanations automating bias detection vision models. study suggests research topics necessary sustain machine learning general purpose technology achieve breakthroughs image classiﬁcation.", "year": 2017}