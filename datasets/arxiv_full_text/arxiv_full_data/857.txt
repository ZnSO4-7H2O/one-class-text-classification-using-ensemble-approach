{"title": "Learning deep dynamical models from image pixels", "tag": ["stat.ML", "cs.LG", "cs.NE", "cs.SY"], "abstract": "Modeling dynamical systems is important in many disciplines, e.g., control, robotics, or neurotechnology. Commonly the state of these systems is not directly observed, but only available through noisy and potentially high-dimensional observations. In these cases, system identification, i.e., finding the measurement mapping and the transition mapping (system dynamics) in latent space can be challenging. For linear system dynamics and measurement mappings efficient solutions for system identification are available. However, in practical applications, the linearity assumptions does not hold, requiring non-linear system identification techniques. If additionally the observations are high-dimensional (e.g., images), non-linear system identification is inherently hard. To address the problem of non-linear system identification from high-dimensional observations, we combine recent advances in deep learning and system identification. In particular, we jointly learn a low-dimensional embedding of the observation by means of deep auto-encoders and a predictive transition model in this low-dimensional space. We demonstrate that our model enables learning good predictive models of dynamical systems from pixel information only.", "text": "department electrical engineering link¨oping university sweden department information technology uppsala university sweden department computing imperial college london united kingdom modeling dynamical systems important many disciplines e.g. control robotics neurotechnology. commonly state systems directly observed available noisy potentially high-dimensional observations. cases system identiﬁcation i.e. ﬁnding measurement mapping transition mapping latent space challenging. linear system dynamics measurement mappings eﬃcient solutions system identiﬁcation available. however practical applications linearity assumptions hold requiring non-linear system identiﬁcation techniques. additionally observations high-dimensional non-linear system identiﬁcation inherently hard. address problem non-linear system identiﬁcation high-dimensional observations combine recent advances deep learning system identiﬁcation. particular jointly learn low-dimensional embedding observation means deep auto-encoders predictive transition model low-dimensional space. demonstrate model enables learning good predictive models dynamical systems pixel information only. data. dynamical models describing data desired forecasting controller design play important role e.g. autonomous systems machine translation robotics surveillance applications. challenge system identiﬁcation i.e. ﬁnding mathematical model dynamical system based information provided measurements underlying system. context state-space models includes ﬁnding functional relationships states diﬀerent time steps states corresponding measurements linear case problem well studied many standard techniques exist e.g. subspace methods expectation maximization prediction-error methods however realistic practical scenarios require non-linear system identiﬁcation techniques. learning non-linear dynamical models inherently diﬃcult problem active areas system identiﬁcation last decades recent years sequential monte carlo methods received attention identifying non-linear state-space models also recent surveys methods based powerful also computationally expensive. learning non-linear dynamical models high-dimensional sensor data even challenging. first ﬁnding functional relationships high dimensions hard second amount data required good function approximator enormous. fortunately high-dimensional data often possesses intrinsic lower dimensionality. exploit property system identiﬁcation ﬁnding low-dimensional representation highdimensional data learning predictive models low-dimensional space. purpose need automated procedure ﬁnding compact lowdimensional representations/features. state learning parsimonious representations high-dimensional data currently deﬁned deep learning architectures deep neural networks stacked/deep auto-encoders convolutional neural networks successfully applied image text speech audio data commercial products e.g. google amazon facebook. typically feature learning methods applied static data sets e.g. image classiﬁcation. auto-encoder gives explicit expressions generative mappings encoder mapping high-dimensional data features decoder mapping features high-dimensional reconstructions. machine learning literature exists vast number well studied nonlinear dimensionality reduction methods gaussian process latent variable model kernel laplacian eigenmaps locally linear embedding however provide mappings paper combine feature learning dynamical systems modeling obtain good predictive models high-dimensional time series. particular deep auto-encoder neural networks automatically ﬁnding compact low-dimensional representation image. low-dimensional feature space neural network modeling nonlinear system dynamics. embedding predictive model feature space learned jointly. simpliﬁed illustration approach shown feature space prediction model maps feature forward time subsequently decoder used generate predicted image next time step. framework needs access encoder decoder motivates auto-encoder dimensionality reduction technique. consequently contributions paper model learning low-dimensional dynamical representation high-dimensional data used long-term predictions; experimental evidence demonstrating jointly learning parameters latent embedding predictive model latent space increase performance compared separate training. low-dimensional latent variable exists compactly represents relevant properties since consider dynamical systems low-dimensional representation image insuﬃcient capture important dynamic information velocities. therefore introduce additional latent variable state. case state contains features multiple time steps measurement described low-dimensional feature representation features turn modeled low-dimensional state-space model state contains full information state system time instant also fig. sequences independent random variables model parameters. identify parameters dynamical systems prediction-error method applied extensively within system identiﬁcation community during last decades. based minimizing error sequence measurements general diﬃcult derive predictor model based nonlinear state-space model closed form expression prediction available special cases however approximating optimal solution predictor model features stated form includes past features control inputs nonlinear function corresponding model parameters. note predictor model longer explicit notion state model introduced indeed ﬂexible work restricted ﬂexibility somewhat working figure combination deep learning architectures feature learning predictor models feature space. camera observes robot approaching object. good low-dimensional feature representation image important learning predictive model camera sensor available. figure general graphical model high-dimensional data data point lowdimensional representation modeled using state-space model hidden state control input approximate predictor model predicted feature zt|t− function past high-dimensional data yt−n encoder predicted feature zt|t− mapped predicted ever since interested good predictive performance high-dimensional data rather features transform predictions back high-dimensional space obtain prediction deep auto-encoder neural network parameterize feature mapping inverse. consists deep encoder network deep decoder network layer encoder neural network summarize model contains following free parameters parameters encoder parameters decoder parameters predictor model train model employ cost functions prediction errors normally features used inference dynamical models ﬁrst extracted data pre-processing step second step predictor model estimated based features. setting would correspond sequentially training model using cost functions ﬁrst learn compact feature representation minimizing reconstruction error computes squashing function free parameters. control input ﬁrst layer image i.e. last layer low-dimensional feature representation image parameters neural network layers. decoder consists number layers reverse order fig. considered approximate inverse enrealize autoencoder suits problem hand well since provides explicit expression mapping features data well approximate inverse convenient form predictions many nonlinear dimensionality reduction methods gaussian process latent variable model kernel laplacian eigenmaps locally linear embedding provide explicit expression mappings main auto-encoder strong similarities principal component analysis precisely linear activation function consider single layer auto-encoder identical exploited relationship initialize parameters auto-encoder. auto-encoder network unfolded pair layers encoder decoder combined corresponding solution computed pairs. starting high-dimensional data layer using principal components pair layers input next pair layers recursively compute good initialization parameters auto-encoder network. similar pre-training routines found restricted boltzmann machine used instead pca. report results identiﬁcation nonlinear dynamics pendulum moving horizontal plane torque control input object moving d-plane velocity serves control input. examples learn dynamics solely based pixel information. pixel component measurement assumes continuous grayvalue torque pendulum control input. speed training image input reduced prior model learning using pca. dimensional inputs four layers used encoder well decoder dimension ----. hence features dimension order dynamics chosen capture velocity information. predictor model used two-layer neural network architecture. predictive performance exemplary image sequences validation data system identiﬁcation models illustrated fig. control inputs assumed known. rows show ground truth images center rows show predictions based model using joint training bottom rows show corresponding predictions model auto-encoder predictive model trained sequentially according model based jointly training parameters obtain good predictive performance one-step ahead prediction multiple-step ahead prediction. contrast predictive performance learning features dynamics separately worse predictive performance model trained jointly optimizing parameters. although auto-encoder perfect already one-step ahead prediction similar ground-truth image. also seen table reconstruction error equally good models prediction error manage better value using joint training using separate training. closer look model based separate training since auto-encoder performs well learned transition model reason predictive performance. believe auto-encoder found good feature representation reconstruction representation ideal learning transition model. fig. displays decoded images corresponding latent representations using joint separate training respectively. joint training feature values line circular shape enabling low-dimensional dynamical description whereas separate training ﬁnds feature values even placed sequentially low-dimensional representation. separate training extracts low-dimensional representations without context i.e. knowledge figure exp. typical image sequences corresponding prediction results computed according rows show nine consecutive ground truth image frames time instant second third rows display corresponding long-term ahead predictions based measured images time joint separate training model parameters. clearly joint learning outperforms separate learning terms predictive performance prediction horizons greater even using last available image frame prediction obtain better model learns parameter sequentially fact dynamical model often predicts frames correspond real pendulum fig. leading poor furthermore joint training gives better predictions naive prediction. predictive performance slightly degrades prediction horizon increases expected. finally also compare subspace identiﬁcation method restricted linear models. restriction capture non-linear embedded features hence predictive performance sub-optimal. particular data data points clearly reside one-dimensional manifold encoded pendulum angle. however one-dimensional feature space would insuﬃcient since one-dimensional manifold cyclic fig. compare also period angle. therefore used twodimensional latent space. further along manifold latent space training data reside decoder produces reasonable outputs. inspected zooming smaller region displayed fig. decoded high-dimensional image displayed. feature values corresponding training validation data displayed. feature spaces found joint separate parameter learning. zoomed version green rectangle presented fig. system identiﬁcation point view prediction error method minimize one-step ahead prediction error fairly standard. however future control reinforcement learning setting primarily interested good prediction performance longer horizon order planning. thus also investigated whether additionally include multi-step ahead prediction error cost models achieved similar performance signiﬁcantly better prediction error could observed either one-step ahead predictions longer prediction horizons. instead computing prediction errors image space compute errors feature space avoid decoding step back high-dimensional space according however require extra penalty term order avoid trivial solutions everything zero eventually resulting complicated less intuitive cost function. previous example evaluate performance terms long-term predictions. performance joint training illustrated fig. validation data set. model predicts future frames tile high accuracy. fig. feature representation data displayed. features reside two-dimensional manifold encoding twodimensional position moving tile. four corners manifold represent four corners tile position within image frame. structure induced dynamical description. corresponding feature representation case sepapossible directions future work include robustify learning using denoising autoencoders deal noisy real-world data apply convolutional neural networks often suitable images; exploiting learned model learning controller purely based pixel information; sequential monte carlo methods investigated systematic treatments nonlinear probabilistic models required reinforcement learning setting. setting make decisions based predictions optimal control model-based reinforcement learning probabilistic model often needed robust decision making need account models errors extension present model probabilistic setting non-trivial since random variables transformed neural networks exact probability density functions intractable compute. sampling-based approaches determinresentation suitable modeling lowdimensional dynamical behavior pre-training initialization described section not. pre-training yields feature values useful ones modeling dynamics joint training might good model. network structure chosen actual training starts. especially dimension latent state order dynamics chosen user requires prior knowledge system identiﬁed. examples chose latent dimensionality based insights true dynamics problem. general model selection procedure preferable good network structure good latent dimensionality. presented approach non-linear system identiﬁcation high-dimensional time series data. model combines techniques system identiﬁcation machine learning community. particular used deep auto-encoder ﬁnding low-dimensional features high-dimensional data nonlinear autoregressive exogenous model used describe low-dimensional dynamics. framework applied pendulum moving horizontal plane. proposed model exhibits good predictive performance major improvement identiﬁed training autoencoder dynamical model jointly instead work supported swedish foundation strategic research project cooperative localization swedish research council under project probabilistic modeling dynamical systems supported imperial college junior research fellowship. second experiment paper results joint learning prediction reconstruction error reported. joint learning brings structure feature values present autoencoder learn separately fig. ghahramani. learning dynamic bayesian networks. giles gori editors adaptive processing sequences data structures volume lecture notes computer science pages springer kantas doucet singh maciejowski. overview sequential monte carlo methods parameter estimation general state-space models. proceedings ifac symposium system identiﬁcation pages saint-malo france july schneider. exploiting model uncertainty estimates safe dynamic control learning. advances neural information processing systems morgan kaufman publishers", "year": 2014}