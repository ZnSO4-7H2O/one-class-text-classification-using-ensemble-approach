{"title": "Small-sample Brain Mapping: Sparse Recovery on Spatially Correlated  Designs with Randomization and Clustering", "tag": ["cs.LG", "cs.CV", "stat.AP", "stat.ML"], "abstract": "Functional neuroimaging can measure the brain?s response to an external stimulus. It is used to perform brain mapping: identifying from these observations the brain regions involved. This problem can be cast into a linear supervised learning task where the neuroimaging data are used as predictors for the stimulus. Brain mapping is then seen as a support recovery problem. On functional MRI (fMRI) data, this problem is particularly challenging as i) the number of samples is small due to limited acquisition time and ii) the variables are strongly correlated. We propose to overcome these difficulties using sparse regression models over new variables obtained by clustering of the original variables. The use of randomization techniques, e.g. bootstrap samples, and clustering of the variables improves the recovery properties of sparse methods. We demonstrate the benefit of our approach on an extensive simulation study as well as two fMRI datasets.", "text": "functional neuroimaging measure brain’s response external stimulus. used perform brain mapping identifying observations brain regions involved. problem cast linear supervised learning task neuroimaging data used predictors stimulus. brain mapping seen support recovery problem. functional data problem particularly challenging number samples small limited acquisition time variables strongly correlated. propose overcome diﬃculties using sparse regression models variables obtained clustering original variables. randomization techniques e.g. bootstrap samples clustering variables improves recovery properties sparse methods. demonstrate beneﬁt approach extensive simulation study well fmri datasets. functional brain imaging instance using functional nowadays central human neuroscience research corresponding medical applications. learning statistical links observed brain images corresponding subject’s behavior formulated machine learning problem indeed prediction neuroimaging data lead impressive results brain reading e.g. guessing image subject looking however main goal functional neuroimaging study often prediction brain mapping identifying brain regions involved cognitive processing external stimuli. fmri data hand brain activity amplitude measured grid voxels. classically modeled linear eﬀect driven cognitive task performed subjects. detecting regions brain active speciﬁc task formalized identifying non-zero coeﬃcients linear model predicting external stimuli neuroimaging data. statistical standpoint estimation challenging dimensionality problem behavior must linked brain activity measured voxels often hundred observations. reason problem most-often tackled mass-univariate approach model ﬁtted separately voxel. detection active voxels suﬀers multiple comparison problem; detection power decreases linearly number variables tested. brain regions active given task linear model sparse. great potential interest sparse recovery techniques recover active voxels suﬀering loss detection power sub-linear number voxels. however brain mapping many experimental ﬁelds design matrix imposed problem strong correlations across regressors univariate approaches often eﬀective multivariate approaches main contribution paper propose eﬃcient sparse recovery procedure well-suited speciﬁcities brain mapping situations spatiallyclustered weights high-dimensional correlated designs. unlike previous work fmri focus recovery predictive power. also provide detailed empirical study sparse estimation case large spatial conﬁgurations weight correlated designs. organization paper following section review results related small sample sparse estimation section expose contributed method section report experimental results spatiallycorrelated synthetic data brain images. brain images composed voxels rn×p corresponding behavioral variable related related linear model measurement noise coeﬃcients sparsity pattern supp s.t. application hand quired identify coeﬃcients corresponding estimation problem solved using tractable algorithms -penalized square loss regression –the lasso greedy approaches types conditions govern success recovery k-sparse vector noisy measurements i.e. model consistency estimator subsets columns design matrix larger well conditioned instance implied restricted isometry property particular signal subspace design matrix suﬃciently well conditioned. regressors signal subspace correlated regressors noise subspace formalized tropp’s exact recovery condition -penalized regression irrepresentable condition mutual incoherence importantly lasso estimator yields non-zero coeﬃcients nmin sparse methods often fail dramatically recover sparsity pattern close ground truth. common cause failure sparse recovery multicolinearity presence strong correlations design practice several columns strongly correlated sparse estimators often select arbitrarily others leading instance high rate false negatives support estimation correspond signal subspace. extreme case exactly collinear regressors lasso non-strictly convex optimization admit unique solution. reason elastic adds strongly convex term lasso form penalty. term eﬀect grouping together correlated regressors. opens door sparse recovery relaxed conditions particular illconditioned signal-subspace design matrices addition sample complexity elastic scales similarly lasso former select coeﬃcients. another approach ensuring strong convexity presence correlated variables based mixed norms imposing penalty sub-groups features known priori covary group structure unknown grave proposed trace-lasso adapting penalization design matrix. limitation approach small-sample limit design matrix deﬁne small number strictly convex directions. image data correlation across neighboring voxels modeled using overlapping groups another challenge high-dimensional sparsity recovery noisy observations diﬃculty control false positives i.e. inclusion estimated support variables present true support lasso theoretical bounds false detections conditional choosing right value parameter controlling amount regularization. optimum depends noise level structure generally considered challenging smallsample settings. reasons bach meinshausen b¨uhlmann introduce resampled estimators based lasso. well known mass-univariate analysis sampling posterior yields control probability false selection weaker constraints model addition diﬀerent variables strongly correlated group selected across resampling estimates result schemes theory recover correlated variables variables observations. another lasso variant randomized lasso introduces random perturbation design rescaling regressors. achieves sparse recovery rip-like condition bounds sparse eigenvalues design without need irrepresentable-like condition imposing strong separation signal noise regressors. provided chosen large enough randomized lasso yields good control inclusion noise variables even small number observations. theoretical results sparse recovery established square loss techniques carry losses instance logistic loss strongly convex square loss well suited classiﬁcation problems rather regression. bach extends non-asymptotic least-square results logistic regression approximating weighted least square show similar conditions design apply sparse recovery penalization. small sample learning rates -penalized logistic regression established earlier based rotational invariance ball. general argument gives simple necessary condition sub-linear sample complexity empirical risk minimizers corresponding optimization problem rotational invariant. addition rotational asymmetries learning problem correspond good representation data. indeed adapting representation signal decomposition sparse performance sparse methods. approach sparse recovery spatially correlated images summarized clustering highly-correlated variables sparse regression randomizing clustering sparse estimation. work based randomized lasso ﬁrst brieﬂy present. randomized lasso randomized lasso consists randomly perturbing design matrix taking fraction training samples randomly scaling variable case voxel. repeating later procedure counting equal probability. procedure subsampling data random perturbation column. stability score voxel percentage repetitions voxel non-zero weight estimated sparse regression i.e. randomized lasso fully address problem large groups correlated variables. indeed selection score undoubtedly tend zero size group increase confounded selection scores uninformative variables chance. adding randomized clustering improve stability estimation propose work clusters correlated variables. motivation clustering variables replacing groups variables mean value reduce number variables improve properties resulting design xred less variables less correlation them. variables voxels deﬁned dimensional grid correlation voxels strongly local constrain clustering procedure cluster neighboring voxels. following michel spatially-constrained ward hierarchical clustering cluster voxels spatially connected regions signal averaged. merging singleton clusters ward criteria chooses samples largest crosscovariance. beneﬁt algorithm thus bottom-up approach captures well local correlations spatial clusters. addition gives multi-scale representation tree nested regions. clustering algorithm randomization loop randomized lasso sub-sampling observations rescaling variables. hierarchical tree estimated time random fraction data tree diﬀerent every randomization. note similar randomization trees resampling performed random forests algorithm motivated sub-sample card randomize feature scaling ˜x·w bern cluster features clusters mean ˜xred ward ˜xred estimate label initial features estimated coeﬃcient corresponding cluster ward high-variance estimators based greedy tree construction. similarly consensus clustering resampling applied construction clusters account sensibility algorithms often based greedy approaches non-convex problems. variable clustering sparse estimator ﬁtted qdimensional dataset. variable marked selected repetition belongs cluster non-zero weight. estimated supp write supp. constrain clustering spatial grid number agglomeration grows linearly cost lasso estimate data generation compare diﬀerent sparse estimators simulated data ground truth known. simulate data according simple model reﬂecting elements brain imaging data local correlations design matrix i.e. observed brain images spatially clustered non-zero features. generate i.i.d gaussian design spatially smooth gaussian ﬁlter standard deviation weight vectors recover non-zero coeﬃcients. split spatial clusters size evenly-separated spatial grid. non-zero coeﬃcients uniformly sampled βmin βmin. finally target variable generated according linear model additive gaussian i.i.d noise. amplitude noise ensure explains variance true model. parameters procedure smoothing cluster size control point synthetic data violate conditions recovery. suﬃcient smoothing render arbitrarily conditioned groups regressors located spatial neighborhood thus violating sparse eigenvalue properties rip. particular clusters large design matrix restricted signal subspace conditioned whereas small non-zero coeﬃcients well separated well conditioned. finally smoothing increases coupling signal noise subspaces although coupling less important large clusters. success metrics compare ability recover true support diﬀerent estimators. estimator yields score variable. higher score likely variable support. precision-recall curves quantify ability estimator recover non-zero coefﬁcients threshold scores varied. summarize precision-recall area curve chance perfect recovery exists threshold active non-active features perfectly separated. practice consider near-perfect recovery usable. match context real data ground truth unknown note report performance automatically selected parameters best possible performance unlike e.g. grave results approaches greedy algorithm orthogonal matching pursuit iterative approach adaptive lasso report results achieve useful recovery. failure understood number samples nmin smoothed design fragility non-convex estimators working correlated designs. addition studied screening based f-tests elastic randomized lasso contribution randomfigure recovery power simulated data function cluster size smoothing. recovery power measured area precision-recall curve contours diﬀerent methods. bottom best performing method. ized ward lasso. time regularization parameters using -fold cross-validation maximize explained variance left data. elastic also cross-validation amount regularization randomized ward lasso number clusters. setting penalties minimizing test error gives guaranties estimation error terms recovery alternative methods like information-based criterion hold theory small-sample settings. simulations cross-validation superior results seen ﬁgure smoothing methods fail give satisfactory recovery although large clusters partially recovered randomized ward lasso large cluster sizes adding small amount correlation design improves recovery. indeed added correlation consistent sparsity pattern recover. elastic randomized lasso recover even small clusters. small sample size method performs well case small clusters without smoothing case small clusters large smoothing corresponds ill-posed deconvolution problem. large cluster large smoothing case univariate approach outperforms others. situation sub-matrices design matrix ill-conditioned sparse methods succeed eﬀective number degrees freedom estimated strongly reduced inter-feature correlations univariate approach perform many independent hypothesis tests thus number false positive reduced. large region parameter space explored clusters composed features smoothing standard deviation order pixel proposed method randomized ward lasso outperforms methods. note small sample case parameter selection particularly diﬃcult method. interestingly that settings explore randomized lasso often outperformed elastic suggesting elastic net’s relaxation conditions sparse eigenvalues better suited problem randomized lasso’s relaxation irrepresentable condition. finally conﬁrmed empirically that randomized ward lasso randomizing clusters iteration important fully-random clusters outperform using non-random clusters randomized approach performs best assess performance approach real fmri data investigated datasets. ﬁrst gambling task subject asked accept reject gambles offered chance gaining losing money. gamble amount used target regression setting. second dataset haxby visual object recognition task. object category used target classiﬁcation setting. setting sparse logistic regression model instead lasso. refer haxby detailed description experimental protocol. data publicly available http//openfmri.org. regression standard preprocessing regression dataset consists subjects fmri observations subject. gain condition used observations contain repetitions experiment levels gain. fmri volumes downsamclassiﬁcation object recognition experiment subjects asked recognize diﬀerent types objects focus binary classiﬁcation task consists predicting whether subject viewing object data consist sessions volumes category. considering sessions training data consist volumes containing voxels. problem intra-subject sessions used learning prediction performed left sessions subject. real data ground truth known test qualitative observations training predictive model voxels highest scores rationale voxels highest f-scores contain false positives voxels highest stability scores predictive model trained ﬁrst give signiﬁcantly worse performance model trained second one. validate hypothesis train logistic regression model best voxels scored methods diﬀerent object contrasts numbers training sessions. number selected voxels regularization parameter optimized cross-validation. training sessions sessions unseen feature-selection method. completeness also benchmark linear well logistic regressions trained whole brain. experiments performed scikit-learn using liblinear logistic regression libsvm svm. results figure shows maps values scores obtained standard stability selection scores obtained using approach. scores thresholded visualization purposes meaning zeros obtained actually zeros. datasets observe despite fairly noisy f-tests highlight well localized regions brain. similar regions clearly outlined approach whereas standard stability selection procedure yields spatially scattered selection scores. also elastic-net regression settings cross-validation lead setting compromise penalty favor result maps figure score maps estimated diﬀerent methods fmri datasets. left column regression settings. right column classiﬁcation settings using sessions training data. ﬁgures suggests thresholding scores probably leads false detections thus estimating estimation problems tackled safely considered well beyond reach conventional sparse estimators terms number samples. prediction results reported fig. first observe linear -logistic regression yield similar results expected similarity logistic loss hinge loss. second learning problem displays many non-informative features using penalization actually helps prediction. finally observe using best voxels scored randomized ward logistic leads signiﬁcantly better performance using voxels highest f-score. suggests actually achieves better identiﬁcation voxels involved cognitive task. tially correlated designs well number samples. propose clustering variables well randomization techniques order position problem settings good recovery achievable i.e. correlation variables support limited number variables compared number samples. informed strong spatial correlations voxels fmri data constrain spatially clusters. constraint injects additional prior estimation facilitate recovery spatially contiguous supports. formulation enables computationally-eﬃcient sparse linear regression models yields overall algorithmic complexity linear number features. results simulations highlight settings approach outperforms techniques like elastic univariate feature screening. results publicly available fmri datasets demonstrate approach able zero non-informative regions outlining relevant clusters voxels. although true support unknown real data prediction scores obtained best voxels outperforms alternative supervised methods suggesting approach better terms support identiﬁcation also signiﬁcantly improve prediction data. monti tamayo mesirov golub consensus clustering resampling-based method class discovery visualization gene expression microarray data. machine learning figure average classiﬁcation scores obtained using full brain linear -logistic regressions logistic regression deﬁned signiﬁcant voxels given f-test randomized ward logistic. average computed binary classiﬁcation tasks p-values computed wilcoxon -sided paired test randomized ward logistic prediction scores larger ones obtained methods. randomized ward logistic signiﬁcantly outperforms methods. tackled recovery contiguous clusters image measurements local correlations. indeed clustering variables exploit fact observed correlation shares common structure sparsity pattern recover. work include investigating clustering algorithms comparing computationally costly convex estimators using overlapping group penalties.", "year": 2012}