{"title": "A Convex Formulation for Learning Task Relationships in Multi-Task  Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Multi-task learning is a learning paradigm which seeks to improve the generalization performance of a learning task with the help of some other related tasks. In this paper, we propose a regularization formulation for learning the relationships between tasks in multi-task learning. This formulation can be viewed as a novel generalization of the regularization framework for single-task learning. Besides modeling positive task correlation, our method, called multi-task relationship learning (MTRL), can also describe negative task correlation and identify outlier tasks based on the same underlying principle. Under this regularization framework, the objective function of MTRL is convex. For efficiency, we use an alternating method to learn the optimal model parameters for each task as well as the relationships between tasks. We study MTRL in the symmetric multi-task learning setting and then generalize it to the asymmetric setting as well. We also study the relationships between MTRL and some existing multi-task learning methods. Experiments conducted on a toy problem as well as several benchmark data sets demonstrate the effectiveness of MTRL.", "text": "multi-task learning learning paradigm seeks improve generalization performance learning task help related tasks. paper propose regularization formulation learning relationships tasks multi-task learning. formulation viewed novel generalization regularization framework single-task learning. besides modeling positive task correlation method called multi-task relationship learning also describe negative task correlation identify outlier tasks based underlying principle. under regularization framework objective function mtrl convex. efﬁciency alternating method learn optimal model parameters task well relationships tasks. study mtrl symmetric multi-task learning setting generalize asymmetric setting well. also study relationships between mtrl existing multi-task learning methods. experiments conducted problem well several benchmark data sets demonstrate effectiveness mtrl. multi-task learning learning paradigm seeks improve generalization performance learning task help related tasks. learning paradigm inspired human learning activities people often apply knowledge gained previous learning tasks help learn task. example baby ﬁrst learns recognize human faces later uses knowledge help learn recognize objects. multi-task learning formulated different settings symmetric asymmetric symmetric multi-task learning seeks improve performance tasks simultaneously objective asymmetric multi-task learning improve performance target task using information source tasks typically source tasks learned using symmetric multi-task learning method. sense asymmetric multi-task learning related transfer learning major difference source tasks still learned simultaneously asymmetric multi-task learning learned independently transfer learning. major advances made multi-task learning past decade although preliminary ideas actually date back much earlier work psychology cognitive science. multi-layered feedforward neural networks provide earliest models multi-task learning. neural network hidden layer represents common features data points tasks unit output layer usually corresponds output task. similar neural networks multi-task feature learning also learns common features tasks regularization framework. unlike methods regularized multi-task support vector machine enforces parameters tasks close other. another widely studied approach multi-task learning task clustering approach main idea group tasks several clusters learn similar data features model parameters tasks within cluster. advantage approach robustness outlier tasks reside separate clusters affect tasks. moreover bayesian models proposed multi-task learning methods focus symmetric multi-task learning also exist previous works study asymmetric multi-task learning since multi-task learning seeks improve performance task help related tasks central issue characterize relationships tasks accurately. given training data multiple tasks important aspects distinguish different methods characterizing task relationships. ﬁrst aspect task relationships represented method. generally speaking three types pairwise task relationships positive task correlation negative task correlation task unrelatedness positive task correlation useful characterize task relationship since similar tasks likely similar model parameters. negative task correlation since model parameters tasks negative correlation likely dissimilar knowing tasks negatively correlated help reduce search space model parameters. task unrelatedness identifying outlier tasks prevent impairing performance tasks since outlier tasks unrelated tasks. second aspect obtain relationships either model assumption automatically learned data. obviously learning task relationships data automatically favorable option model assumption adopted incorrect even worse easy verify correctness assumption data. multi-layered feedforward neural networks multi-task feature learning assume tasks share representation without actually learning task relationships data automatically. moreover consider negative task correlation robust outlier tasks. regularization methods assume task relationships given utilize prior knowledge learn model parameters. however utilize positive task correlation task unrelatedness negative task correlation. task clustering methods viewed learn task relationships data. similar tasks grouped task cluster outlier tasks grouped separately making methods robust outlier tasks. however local methods sense similar tasks within task cluster interact help other thus ignoring negative task correlation exist tasks residing different clusters. hand powerful multi-task learning method based gaussian process provides global approach model learn task relationships form task covariance matrix. task covariance matrix model three types task relationships positive task correlation negative task correlation task unrelatedness. however although method provides powerful model task relationships learning task covariance matrix gives rise non-convex optimization problem sensitive parameter initialization. number tasks large authors proposed low-rank approximation weaken expressive power task covariance matrix. moreover since method based scaling large data sets poses serious computational challenge. goal paper inherit advantages overcoming disadvantages. speciﬁcally propose method called multi-task relationship learning also models relationships tasks nonparametric manner task covariance matrix. based regularization framework obtain convex objective function allows learn model parameters task relationships simultaneously. mtrl viewed generalization regularization framework single-task learning multitask setting. efﬁciency alternating optimization method subproblem convex problem. study mtrl symmetric multi-task learning setting generalize asymmetric setting well. also study relationships mtrl existing multi-task learning methods showing methods viewed special cases mtrl. multi-task relationship learning suppose given learning tasks \u0007\u0001\u0001\u0007\u0001 task training consists data points corre\u0001\u0002 regression problem sponding output binary classiﬁcation problem. linear function deﬁned identity matrix. ﬁrst term prior penalize complexity column separately second term model structure since matrix variable natural matrix-variate distribution model matrix-variate normal distribution speciﬁcally called matrix fractional function example proved convex function respect positive semideﬁnite matrix since also convex respect summation operation preserve convexity according analysis page convex respect objective function constraints problem convex respect variables hence problem jointly convex. even though optimization problem convex respect jointly easy optimize objective function respect variables simultaneously. propose alternating method solve problem efﬁciently. speciﬁcally ﬁrst optimize objective function respect ﬁxed optimize respect ﬁxed. procedure repeated convergence. follows present subproblems separately. ó\u0001\u0000\u0001. covariance matrix models relationships features column covariance matrix models relationships different m\u0001’s. words models relationships tasks. task given positive scalar model degenerate probabilistic model regularized least-squares regression least-squares probabilistic model viewed generalization probabilistic model single-task learning. however unlike single-task learning cannot given priori multi-task learning applications seek estimate data automatically. follows posterior distribution proportional product prior likelihood function given denotes data matrix data points tasks taking negative logarithm combining eqs. obtain maximum posteriori estimation maximum likelihood estimation minimizing denotes trace square matrix denotes determinant square matrix. simplicity discussion assume \u0001\u0001\u0002࢘\u0001 effect last term problem penalize complexity however later ﬁrst three terms problem jointly convex respect variables last term concave since \u0006\u0006ࢯ\u0003ࢯ convex function respect according moreover kernel extension idea even inﬁnite feature mapping making problem difﬁcult optimize. replace last term problem constraint restrict complexity problem reformulated regularization parameters means matrix positive semideﬁnite. ﬁrst constraint holds fact deﬁned task covariance matrix. ﬁrst term measures empirical loss training data second term penalizes complexity third term measures relationships tasks based total number data points tasks large computational cost required solve linear system directly high. situation another optimization method solve easy show dual form problem formulated note similar dual form least-squares except difference constraint least-squares constraints constraint corresponding task. algorithm similar least-squares optimizing w.r.t. ﬁxed ﬁxed optimization problem ﬁnding becomes ﬁrst equality holds last constraint problem last inequality holds cauchy-schwarz inequality frobenius norm. moreover attains minimum value constant analytical solution method described learn multiple tasks simultaneously setting symmetric multi-task learning. asymmetric multi-task learning task arrives could data task training train model scratch tasks using method. facilitate kernel extension given later general nonlinear case reformulate optimization problem dual form ﬁrst expressing problem constrained optimization problem linear multi-task kernel kernel matrix deﬁned data points tasks using linear multi\u0001 zero task kernel matrix vector diagonal matrix whose diagonal element equal corresponding data point zero vector elements whose indices equal problem. real applications number tasks usually large standard solver solve problem moreover reformulate problem second-order cone programming problem efﬁcient large. present procedure appendix. optimization problem kernel extension essentially problem difference data point reproducing kernel hilbert space denotes feature map. corresponding kernel function satisﬁes also alternating method solve optimization problem. ﬁrst step alternating method nonlinear multi-task kernel rest linear case. second step change needed calculation since similar representer theorem single-task learn\u0001 \u0003a\u0001a\u0001 task arrives optimization problem still relevant requiring small changes calculation similar calculation details omitted here. however undesirable incorporate tasks high computational cost incurred. introduce algorithm asymmetric multi-task learning efﬁcient. notational simplicity denote denote task training task covariances existing tasks represented vector \u0002\u0001\u0002\u0001 task variance deﬁned thus augmented task covariance matrix tasks denotes -norm vector problem problem. assume positive deﬁnite. constraint holds according schur complement thus constraint equivalent problem becomes convex problem thus also alternating method solve using alternating method optimize respect similar single-task learning. optimizing respect optimization problem formulated different choices formulation method assumes tasks similar parameter vector task similar average parameter vector. corresponding formulation given laplacian matrix deﬁned fully connected graph edge weights equal corresponds special case mtrl obviously limitation method positive task correlation modeled. methods assume task cluster structure task similarity tasks given. formulated denotes similarity tasks laplacian matrix deﬁned graph based \u0007\u0001\u0001\u0001\u0007. again corresponds special case mtrl note method requires also model positive task correlation task unrelatedness. negative task correlation modeled well problem become non-convex making difﬁcult solve. moreover many realworld applications prior knowledge available. authors assume existence task network neighbors task network encoded index pairs similar. formulated deﬁne similarity matrix whose elements equal otherwise. simpliﬁed laplacian matrix similar thus also corresponds special case mtrl similar difﬁculty method prior knowledge form task network available many applications. aspect understand difference between method multi-task feature learning. multitask feature learning learn covariance structure model parameters parameters different tasks independent given covariance structure. however task relationship clear method know task helpful. formulation relationships tasks described explicitly task covariance matrix another advantage formulation kernel extension natural single-task learning. multi-task feature learning however gram-schmidt orthogonalization kernel matrix needed hence incur additional computational cost. moreover matrix-variate distributions also used matrix-variate distribution resulting optimization problem similar problem difference third term objective function. using matrix-variate distribution third term becomes \u0006\u0006ࢯ\u0001 making optimization problem non-convex. nevertheless still alternating method local optimum. applications exist prior knowledge relationships tasks e.g. tasks similar tasks tasks task cluster etc. easy incorporate prior knowledge introducing additional constraints problem example tasks similar tasks corresponding constraint represented \u0003\u0001\u0001; know tasks cluster enforce covariances tasks large covariances tasks close existing multi-task learning methods also model relationships tasks regularization framework. methods assume task relationships given priori utilize prior knowledge learn model parameters. hand method learns task cluster structure data. section discuss relationships study problem learning inverse dynamics -dof sarcos anthropomorphic robot arm. observation sarcos data consists input features corresponding seven joint positions seven joint velocities seven joint accelerations well seven joint torques seven degrees freedom thus input dimensions seven tasks. randomly select data points task form training data points task test set. performance measure used normalized mean squared error mean squared error divided variance ground truth. single-task learning method kernel ridge regression. kernel used kernel. five-fold cross validation used determine values kernel parameter regularization parameters perform random splits data report mean standard derivation trials. results summarized table mean task correlation matrix trials recorded table results performance mtrl better mtfl mtgp. table tasks positively correlated negatively correlated uncorrelated table mean task correlation matrix learned sarcos data different tasks. moreover plot figure change value objective function problem objective function value decreases rapidly levels showing fast convergence algorithm takes iterations. centering matrix cluster assignment matrix mtrl reduce method. however local method model positive task correlation within cluster canmodel negative task correlation. moreover original optimization problem convex relaxation fact non-convex optimal solution convex relaxation guaranteed optimal solution original non-convex problem. another difﬁculty method lies determining number task clusters. compared existing methods mtrl appealing learn three types task relationships nonparametric way. makes easy identify tasks useful multi-task learning exploited. section study mtrl empirically data sets compare single-task learning method multi-task feature learning method multi-task method also learn global task relationships. ﬁrst generate data conduct proof concept experiment experiments real data sets. data generated follows. regression functions corresponding three regression tasks deﬁned task randomly sample points uniformly function output corrupted gaussian noise process zero mean variance equal data points plotted figure color corresponding task. coefﬁcients regression functions expect correlation ﬁrst tasks approach pairs tasks approach apply mtrl linear kernel learning procedure converges estimated regression functions three tasks \u0002''\"\u0001 \"\u0002'\" \u0002'&\"&. based task covariance matrix learned obtain following task correlation matrix table comparison different methods sarcos data. column represents task. ﬁrst method records mean nmse trials second records standard derivation. tasks matches intuition. moreover interesting patterns exist mean task correlation matrices different training sizes. example correlation third fourth tasks always largest training size varies; correlation ﬁrst second tasks larger ﬁrst third tasks also ﬁrst fourth tasks. school data widely used studying multi-task regression. consists examination scores students secondary schools london years thus totally tasks. input consists year examination four school-speciﬁc three student-speciﬁc attributes. replace categorical attribute binary variable possible attribute value result preprocessing total input attributes. experimental settings i.e. random splits data generate training test sets examples school belong training test set. performance measure measure percentage explained variance deﬁned percentage minus nmse. ﬁve-fold cross validation determine values kernel parameter regularization parameters since experimental setting same compare result results reported results summarized table performance mtrl better mtfl slightly better mtgp. next study multi-domain sentiment classiﬁcation application multi-task classiﬁcation problem. goal classify reviews products classes positive negative reviews. data four different products amazon.com books dvds electronics kitchen appliances. task positive negative data points corresponding positive negative reviews respectively. data point feature dimensions. effect varying training size randomly select data task form training rest test set. performance measure used classiﬁcation error. single-task learning method. kernel used linear kernel widely used text applications high feature dimensionality. five-fold cross validation used determine values regularization parameters perform random splits data report mean standard derivation trials. results summarized left column table table performance mtrl better mtfl mtgp every task different training sizes. moreover mean task correlation matrices trials different training sizes recorded right column table table ﬁrst task ‘books’ correlated second task ‘dvds’ tasks; third fourth tasks achieve largest correlation among pairs tasks. ﬁndings table easily interpreted follows ‘books’ ‘dvds’ mainly entertainment; almost elements ‘kitchen appliances’ belong ‘electronics’. knowledge found method relationships table comparison different methods multi-domain sentiment data different training sizes. three tables left column record classiﬁcation errors different methods respectively data used training. column table represents task. method ﬁrst records mean classiﬁcation error trials second records standard derivation. three tables right column record mean task correlation matrices learned different tasks different training sizes task books; task dvds; task electronics; task kitchen appliances. paper presented regularization formulation learning relationships tasks multitask learning. method model global task relationships learning problem formulated directly convex optimization problem. study proposed method symmetric asymmetric multitask learning settings. multi-task learning applications exist additional sources data unlabeled data. future research consider incorporating additional data sources regularization formulation similar manifold regularization boost learning performance multi-task learning setting.", "year": 2012}