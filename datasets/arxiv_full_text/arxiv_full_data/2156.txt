{"title": "Learning Scalable Deep Kernels with Recurrent Structure", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Many applications in speech, robotics, finance, and biology deal with sequential data, where ordering matters and recurrent structures are common. However, this structure cannot be easily captured by standard kernel functions. To model such structure, we propose expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent networks, while retaining the non-parametric probabilistic advantages of Gaussian processes. We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic gradient procedure and exploit the structure of these kernels for scalable training and prediction. This approach provides a practical representation for Bayesian LSTMs. We demonstrate state-of-the-art performance on several benchmarks, and thoroughly investigate a consequential autonomous driving application, where the predictive uncertainties provided by GP-LSTM are uniquely valuable.", "text": "many applications speech robotics ﬁnance biology deal sequential data ordering matters recurrent structures common. however structure cannot easily captured standard kernel functions. model structure propose expressive closed-form kernel functions gaussian processes. resulting model gplstm fully encapsulates inductive biases long short-term memory recurrent networks retaining non-parametric probabilistic advantages gaussian processes. learn properties proposed kernels optimizing gaussian process marginal likelihood using provably convergent semi-stochastic gradient procedure exploit structure kernels scalable training prediction. approach provides practical representation bayesian lstms. demonstrate state-of-the-art performance several benchmarks thoroughly investigate consequential autonomous driving application predictive uncertainties provided gp-lstm uniquely valuable. exists vast array machine learning applications underlying datasets sequential. applications range entirety robotics speech audio video processing. neural network based approaches dealt issue representation learning sequential data important question modeling propagating uncertainty across time rarely addressed models. robotics application self-driving however desirable essential complete predictive densities variables interest. trying stay lane keep safe following distance vehicle front knowing uncertainty associated lanes lead vehicles important point estimates. maruan al-shedivat andrew gordon wilson yunus saatchi zhiting eric xing. license cc-by https//creativecommons.org/licenses/by/./. attribution requirements provided http//jmlr.org/papers/v/-.html. recurrent models long short-term memory recently emerged leading approach modeling sequential structure. lstm eﬃcient gradient-based method training recurrent networks. lstms memory cell inside hidden unit special gating mechanism stabilizes back-propagated errors improving learning process model. lstm provides state-of-the-art results speech text data quantifying uncertainty extracting full predictive distributions deep models still area active research paper quantify predictive uncertainty deep models following bayesian nonparametric approach. particular propose kernel functions fully encapsulate structural properties lstms gaussian processes. resulting model enables gaussian processes achieve state-of-the-art performance sequential regression tasks also allowing principled representation uncertainty non-parametric ﬂexibility. further develop provably convergent semi-stochastic optimization algorithm allows mini-batch updates recurrent kernels. empirically demonstrate semi-stochastic approach signiﬁcantly improves upon standard non-stochastic ﬁrst-order methods runtime quality converged solution. additional scalability exploit algebraic structure kernels decomposing relevant covariance matrices kronecker products circulant matrices training time test predictions model interpreted gaussian process recurrent kernel also deep recurrent network probabilistic outputs inﬁnitely many hidden units utility function robust overﬁtting. throughout paper assume basic familiarity gaussian processes provide brief introduction background section; comprehensive reference e.g. rasmussen williams following sections formalize problem learning sequential data provide background recurrent networks lstm present extensive empirical evaluation model. speciﬁcally apply model number tasks including system identiﬁcation energy forecasting self-driving applications. quantitatively model assessed data ranging size hundreds points almost million various signal-to-noise ratios demonstrating state-of-the-art performance linear scaling approach. qualitatively model tested consequential self-driving applications lane estimation lead vehicle position prediction. indeed main focus paper achieving stateof-the-art performance consequential applications involving sequential data following straightforward scalable approaches building highly ﬂexible gaussian process. release code library http//github.com/alshedivat/keras-gp. library implements ideas paper well deep kernel learning gaussian process layer added arbitrary deep architectures deep learning frameworks following keras speciﬁcation. tutorials resources found https//people.orie.cornell.edu/andrew/code. consider problem learning regression function maps sequences real-valued target vectors. formally {xi}n xli] corresponding length arbitrary domain. collection corresponding real-valued target vectors. {yi}n assuming recent steps sequence predictive targets goal learn function family based available data. working example consider problem estimating position lead vehicle next time step lidar gyroscopic measurements self-driving available number previous steps. task classical instance sequenceto-reals regression temporal sequence measurements regressed future position estimates. notation sequences inputs vectors measurements indexed time would growing lengths. typically input sequences considered ﬁnite-time horizon assumed predictive future targets interest. targets two-dimensional vectors encode positions lead vehicle ego-centric coordinate system self-driving car. considering whole sequences observations input features necessary capturing long-term temporal correlations virtually blows dimensionality problem. assume measurement p-dimensional i.e. consider previous steps distinct features regression problem become -dimensional. therefore avoid overﬁtting able extract meaningful signal ﬁnite amount data crucial exploit sequential nature observations. input observation time corresponding latent representation target vector. functions specify model transitions emissions respectively additive noises. arbitrary typically time-invariant. strong realistic assumption incorporated structure recurrent mapping signiﬁcantly reduces complexity family functions regularizes problem helps avoid severe overﬁtting. recurrent models account various patterns sequences memorizing internal representations dynamics adjusting recurrent neural networks model recurrent processes using linear parametric maps followed nonlinear activations weight matrices learned ﬁxed element-wise functions. importantly contrary standard hidden markov models state time distributed eﬀectively represented training nontrivial so-called vanishing gradient problem error back-propagated time steps diminishes exponentially makes learning long-term relationships nearly impossible. lstm. overcome vanishing gradients hochreiter schmidhuber proposed long short-term memory mechanism places memory cell hidden unit uses diﬀerentiable gating variables. update rules hidden representation time following form tanh element-wise sigmoid hyperbolic tangent functions respectively) illustrated above correspond input forget output gates respectively. variables take values combined internal states inputs multiplicative fashion play role soft gating. gating mechanism improves errors time also allows network decide whether keep erase overwrite certain memorized information based forward inputs backward errors. mechanism adds stability network’s memory. gaussian processes. gaussian process bayesian nonparametric model generalizes gaussian distributions functions. random function drawn mean function covariance kernel vector inputs corresponding vector function values gaussian here kx∗x kxx∗ kx∗x∗ matrices consist covariance function evaluated corresponding points mean function evaluated data optimizing evidence—the marginal probability data given model—with respect kernel hyperparameters. evidence form shorthand implicitly depends kernel hyperparameters. objective function consists model complexity penalty term results automatic occam’s razor realizable functions optimizing evidence respect kernel hyperparameters eﬀectively learn structure space functional relationships inputs targets. details gaussian processes relevant literature refer interested readers classical book rasmussen williams turning back problem learning sequential data seems natural apply powerful machinery modeling complicated relationships. however limited learning pairwise correlations inputs unable account long-term dependencies often dismissing complex temporal structures. combining recurrent models potential addresses issue. problem learning sequential data especially temporal sequences well known control dynamical systems literature. stochastic temporal processes usually described either generative autoregressive models statespace models former approach includes nonlinear auto-regressive models exogenous inputs constructed using e.g. neural networks gaussian processes latter approach additionally introduces unobservable variables state constructs autoregressive dynamics latent space. construction allows represent propagate uncertainty time explicitly modeling signal noise. generative ssms also used conjunction discriminative models fisher kernel modeling time series equivalent using linear-gaussian autoregressive models learning inference eﬃcient models designed capture long-term dependencies correlations beyond pairwise. wang introduced gp-based state-space models transition and/or observation functions. models appear general ﬂexible account uncertainty state dynamics though require complicated approximate training inference hard scale perhaps recent relevant work approach recurrent gaussian processes extends gp-ssm framework regression sequences using recurrent architecture gp-based activation functions. structure model mimics standard every parametric layer substituted gaussian process. procedure allows propagate uncertainty throughout network additional cost. inference intractable eﬃcient training requires sophisticated approximation procedure so-called recurrent variational bayes. addition authors turn rnn-based approximation variational mean functions battle growth number variational parameters size data. technically promising seems problematic application perspective especially implementation scalability aspects. model several distinctions prior work aiming regress sequences reals. firstly goals keep model simple possible able represent quantify predictive uncertainty. maintain analytical objective function refrain complicated diﬃcult-to-diagnose inference schemes. simplicity achieved giving idea propagating signal chain connected recurrent fashion. instead propose directly learn kernels recurrent structure joint optimization simple functional composition standard recurrent model described detail following section. similar approaches recently explored proved fruitful non-recurrent deep networks remark combinations nonlinear functions also considered past slightly diﬀerent setting warped regression targets additionally uncertainty recurrent parts model represented dropout computationally cheap turns equivalent approximate bayesian inference deep gaussian process particular intermediate kernels finally also view model standalone ﬂexible gaussian process leverages learning techniques scale massive datasets figure graphical representation recurrent model maps input sequence target value one-step-ahead prediction manner. shaded variables observable diamond variables denote deterministic dependence inputs. graphical model gplstm time training time points testing time point latent representations mapped outputs gaussian ﬁeld globally correlates predictions. dashed variables represent data instances unused given time step. graphical representation kernel structured parametric gaussian processes diﬀerent kernel functions correspond diﬀerent structured probabilistic models. example special cases matérn class kernels induce models markovian structure construct deep kernels recurrent structure transform original input space lstm network build kernel directly transformed space shown figure particular arbitrary deterministic transformation input sequences latent space next real-valued kernel deﬁned decomposition kernel transformation deﬁned trivial show valid kernel deﬁned addition represented neural network resulting model viewed network additional gp-layer negative marginal likelihood objective function used instead standard mean squared error input embedding well-known gaussian process literature recently wilson successfully scaled approach demonstrated strong results regression classiﬁcation tasks kernels based feedforward convolutional architectures. paper apply technique learn kernels recurrent structure transforming input sequences recurrent neural network acts particular lstm architecture used embed steps input time series single vector hidden space embedding common last hidden vector produced recurrent network. note however variations embedding fairly straightforward. generally recurrent transformation random would enable direct modeling uncertainty within recurrent dynamics would also require inference study limit consideration random recurrent maps induced dropout. unfortunately objective substituted nlml longer factorizes data. prevents using well-established stochastic optimization techniques training recurrent model. case feedforward convolutional networks wilson proposed pre-train input transformations ﬁne-tune jointly optimizing marginal likelihood respect hyperparameters network weights using full-batch algorithms. transformation recurrent stochastic updates play role. therefore propose semi-stochastic block-gradient optimization procedure allows mini-batching weight updates fully joint training model scratch. gram kernel matrix computed {φ}n implicitly depends base kernel hyperparameters parameters recurrent neural transformation denoted referred transformation hyperparameters. goal optimize respect derivative nlml objective respect standard takes following form corresponds latent representation i-th data instance. derivatives computed model trained ﬁrst-order quasi-newton optimization routine. however application stochastic gradient method—the facto standard optimization routine deep recurrent networks—is straightforward neither objective derivatives factorize data kernel matrix inverses hence convergence guaranteed. ﬁxed. subsequently turns weighted independent functions data point. observation suggests that given ﬁxed kernel matrix could compute stochastic update mini-batch training points using corresponding sub-matrix hence propose optimize recurrent kernels semi-stochastic fashion alternating updating kernel hyperparameters full data ﬁrst updating weights recurrent network using stochastic steps. procedure given algorithm semi-stochastic alternating gradient descent special case block-stochastic gradient iteration latter splits variables arbitrary blocks applies gauss–seidel type stochastic gradient updates them procedure alternates applying deterministic updates stochastic updates corresponding algorithm form provably convergent convex non-convex problems certain conditions. following theorem adapts results optimization scheme. applying alternating gradient case catch kernel matrix updated time changed i.e. every mini-batch iteration computationally updating strategy defeats purpose stochastic gradients entire data step. deal issue computational eﬃciency ideas asynchronous optimization. step-by-step derivations given appendix cannot represented sums independent functions data point. principle stochastic updates also possible. next choose practice keep kernel matrix ﬁxed performing stochastic updates. sensitivity kernel even small changes convergence fully stochastic scheme fragile. algorithm semi-stochastic asynchronous gradient descent. input data kernel recurrent transformation initialize compute initial repeat convergence output optimal asynchronous techniques. recent trends parallel distributed optimization applying updates asynchronous fashion strategies naturally require tolerance delays parameter updates case modify algorithm allow delayed kernel matrix updates. observation intuitive stochastic updates small enough change much mini-batches hence perform multiple stochastic steps re-computing kernel matrix still converge. example updated pass entire data ensure convergence algorithm important strike balance learning rate frequency kernel matrix updates. following theorem provides convergence results certain conditions. theorem semi-stochastic gradient descent τ-delayed kernel updates converges ﬁxed point learning rate decays however stochastic gradient methods proved attain better generalization often demonstrate superior performance deep recurrent architectures moreover stochastic methods ‘online’ i.e. update model parameters based subsets incoming data stream hence scale large datasets. experiments demonstrate recurrent kernels trained algorithm converge faster attain better performance trained full-batch techniques. stochastic variational inference. stochastic variational inference gaussian processes another viable approach enabling stochastic optimization recurrent kernels. method would optimize variational lower bound original objective factorizes data construction. recently wilson developed stochastic variational approach context deep kernel learning. note unlike previous existing work proposed approach require variational approximation marginal likelihood perform mini-batch training gaussian processes. kernel matrix computing determinant operations typically require computations training data points storage. approach scalability achieved semi-stochastic training structure-exploiting inference. particular asynchronous semi-stochastic gradient descent reduces total number passes data required model converge number calls linear system solver; exploiting structure kernels signiﬁcantly reduces time memory complexities linear algebraic operations. precisely replace instances covariance matrix sparse interpolation matrix covariance matrix evaluated latent inducing points decomposes kronecker product circulant matrices construction makes inference learning scale test predictions preserving model structure. sake completeness provide overview underlying algebraic machinery appendix high level sparse structured possible take extremely fast matrix vector multiplications approximate covariance matrix kxx. methods linear conjugate gradients mvms eﬃciently solve linear systems. scaled eigenvalue approaches also used eﬃciently compute determinant derivatives. kernel interpolation also enables fast predictions describe appendix. compare proposed gaussian processes recurrent kernels based lstm architectures number baselines datasets various complexity ranging size hundreds almost million time points. datasets thousand points massively scalable version demonstrate scalability inference learning. carry table statistics data used experiments. determined assuming certain degree smoothness signal ﬁtting kernel ridge regression kernel predict targets input time series regarding residuals noise. tasks average correlation inputs targets lower harder prediction problems. number experiments help gain empirical insights convergence properties proposed optimization procedure delayed kernel updates. additionally analyze regularization properties gp-rnn/lstm compare techniques dropout. finally apply model problem lane estimation lead vehicle position prediction critical autonomous driving applications. system identiﬁcation. ﬁrst experiments used publicly available nonlinear system identiﬁcation datasets actuator drives datasets dimensional input output time series. actuator size valve opening input resulting change pressure output. drives system motors drive pulley using ﬂexible belt; input voltages applied motors output speed belt. smart grid data. considered problem forecasting smart grid consisted tasks ﬁrst task predict power load historical temperature data. data input time series coming hourly measurements temperature zones output time series represented cumulative hourly power load u.s. utility. second task predict power generated wind farms wind forecasts. data consisted diﬀerent hourly forecasts wind hourly values generated power wind farm. wind forecast http//www.iau.dtu.dk/nnbook/systems.html http//www.it.uu.se/research/publications/reports/-/nonlineardata.zip. smart grid data taken global energy forecasting kaggle competitions organized figure left visualization gef-power time series zones cumulative load time resolution day. cumulative power load generally negatively correlated temperature measurements zones. right visualization gef-wind time series time resolution day. -element vector corresponded zonal component meridional component wind speed wind angle. experiments concatenated diﬀerent -element forecasts resulted -dimensional input time series. self-driving dataset. main target applications proposed model prediction autonomous driving. considered large dataset coming sensors self-driving recorded trips discretization data featured sets ecef locations ecef velocities measurements ﬁber-optic gyro compass lidar time series variety sensors. additionally locations left right lanes extracted video stream time step well position lead vehicle lidar measurements. considered data ﬁrst trip training second trip validation testing. visualization routes second discretization coordinates given figure consider four tasks ﬁrst proof-of-concept type variety ﬁnal fundamental good performance self-driving released part public creative commons attribution license http//archive.org/details/comma-dataset. self-driving http//www.bloomberg.com/features/-george-hotz-self-driving-car/. figure table left train test routes self-driving coordinates origin starting location. arrows point direction motion; color encodes speed. insets zoom selected regions routes. best viewed color. right summary data collected train test routes. models metrics. used number classical baselines narx gp-narx models classical lstm architectures. kernels models gp-narx/rnn/lstm used base kernel function structured corresponding baselines. primary metric used root mean squared error held additionally negative marginal likelihood training gp-based models. train models perform one-step-ahead prediction autoregressive setting targets future time steps predicted input target values ﬁxed number past time steps. system identiﬁcation task additionally consider non-autoregressive scenario performing prediction free simulation mode included recurrent gaussian processes comparison. case none future targets available models re-use past predictions produce future forecasts). note implementation. recurrent parts model implemented using keras library. extended keras gaussian process layer developed backed engine based gpml library. approach allows take full advantage functionality available keras gpml e.g. automatic diﬀerentiation recurrent part model. code available http//github.com/alshedivat/kgp/. figure charts left convergence optimization terms rmse test nlml train. inset zooms region plot right beneath using scale vertical axis. full mini denote full-batch mini-batch optimization procedures respectively refer models respective number units hidden layer. charts right test rmse given architecture trained speciﬁed method and/or learning rate. section discusses quantitative qualitative experimental results. brieﬂy introduce model architectures training schemes used experiments. provide comprehensive summary details appendix address question whether stochastic optimization recurrent kernels necessary assess behavior proposed optimization scheme delayed kernel updates conducted number experiments actuator dataset first constructed gp-lstm models recurrent hidden layer hidden units trained full-batch iterative procedure semi-stochastic optimizer delayed kernel updates convergence results given ﬁrst charts. terms error held nlml training models trained mini-batches converged faster demonstrated better ﬁnal performance. next compared optimization schemes gp-lstm architecture diﬀerent sizes hidden layer ranging clear third chart that even though full-batch approach seemed better optimum number hidden units small stochastic approach clearly superior larger hidden layers. finally compared behavior algorithm diﬀerent number mini-batches used epoch diﬀerent learning rates. results give last chart. expected balance number mini-batches learning rate number mini-batches large learning rate high enough optimization converge; time appropriate combination learning rate mini-batch size leads better generalization default batch approach wilson table average performance models terms rmse system identiﬁcation tasks. averages computed runs; standard deviation given parenthesis. results model reported mattos available free simulation. experiments main goal provide comparison three diﬀerent modes one-step-ahead prediction referred regression autoregression free simulation compare performance models rgp—a classical every parametric layer substituted gaussian process actuator drives datasets. diﬀerence prediction modes consists whether information past targets used. regression scenario inputs targets separate time series model learns input values number past time points target value future point time. autoregression additionally uses true past target values inputs; free simulation mode model learns past inputs past predictions future target. experiments autoregression free simulation modes used short time lags suggested mattos regression mode since model build recurrent relationships based information targets generally requires larger time lags capture state dynamics. hence increased time regression mode. details given appendix present results table note gp-based architectures consistently yielded improved predictive performance compared vanilla deep learning counterparts datasets mode. given small size datasets attribute behavior better regularization properties negative marginal likelihood loss function. also found gp-based models initialized weights pre-trained neural networks tended overﬁt give overly conﬁdent predictions tasks. best performance achieved models trained random initialization free simulation mode performs best compared models. result expected—rgp particularly designed represent propagate uncertainty recurrent process. suitability prediction mode depends task hand. many applications future targets become readily available time passes autoregression mode preferable. particularly consider autoregressive prediction experiments. smart grid prediction tasks used lstm gp-lstm models hour time lags predicting target values hour ahead. lstm gp-lstm trained layers hidden units. best models selected training data used validation. autonomous driving prediction tasks used architectures time steps models regularized dropout self-driving datasets used scalable version gaussian process given scale data challenge nonlinear optimization recurrent models initialized recurrent parts gp-rnn gp-lstm pre-trained weights corresponding neural networks. fine-tuning models performed algorithm quantitative results provided table demonstrate recurrent kernels attain state-of-the-art performance. additionally investigated convergence regularization properties lstm gp-lstm models gef-power dataset. ﬁrst charts figure demonstrate gp-based models less prone overﬁtting even data enough. third panel shows architectures particular number hidden units layer attain best performance power prediction task. additional advantage gp-layers standard recurrent networks best architecture could identiﬁed based negative likelihood model shown last chart. figure qualitative comparison lstm gp-lstm predictions self-driving tasks. predictive uncertainty gp-lstm model showed contour plots error-bars; latter denote standard deviation predictive distributions. finally figure qualitatively demonstrates diﬀerence predictions given lstm gp-lstm point-wise lane estimation front vehicle tracking tasks. note gp-lstm provides robust also estimates uncertainty predictions. information used downstream prediction-based decision making e.g. whether self-driving slow switch cautious driving style uncertainty high. figure left right rmse number training points; rmse number model parameters layer; nlml number model parameters layer gp-based models. metrics averages runs diﬀerent random initializations computed held-out set. figure charts demonstrate scalability learning inference msgp lstm-based recurrent kernel. legends points denote number inducing points used. legends percentages denote percentage training dataset used learning model. following wilson performed generic scalability analysis msgplstm model sensors data. lstm architecture described previous section transforming multi-dimensional sequences inputs two-dimensional representation. trained model epochs training inducing points dimension measured average training time epoch average prediction time testing point. measured time total time spent lstm optimization msgp computations. results presented figure training time epoch grows linearly number training examples depends linearly number inducing points thus given ﬁxed number inducing points dimension time complexity msgp-lstm learning inference procedures linear number training examples. prediction time testing data point virtually constant depend neither number training points number inducing points proposed method learning kernels recurrent long short-term memory structure sequences. gaussian processes kernels termed gp-lstm structure learning biases lstms retaining probabilistic bayesian nonparametric representation. gp-lstm outperforms range alternatives several sequence-toreals regression tasks. gp-lstm also works data high signal-to-noise ratios scaled large datasets straightforward practical generally applicable model speciﬁcation. moreover semi-stochastic scheme proposed paper provably convergent eﬃcient practical settings conjunction structure exploiting algebra. short gp-lstm provides natural mechanism bayesian lstms quantifying predictive uncertainty harmonizing standard deep learning toolbox. predictive uncertainty high value robotics applications autonomous driving could also applied areas ﬁnancial modeling computational biology. several exciting directions future research. gp-lstm quantiﬁes predictive uncertainty model propagation uncertainty inputs recurrent structure. treating free simulation structured prediction problem using online corrective algorithms e.g. dagger likely improve performance gp-lstm free prediction mode. approach would require explicitly modeling propagating uncertainty recurrence would maintain high computational eﬃciency method. alternatively would exciting probabilistic treatment parameters gp-lstm kernel including lstm weights. extension could combined stochastic variational inference enable classiﬁcation non-gaussian likelihoods wilson also open doors stochastic gradient hamiltonian monte carlo eﬃcient inference kernel parameters. indeed sg-hmc recently used eﬃcient inference network parameters bayesian bayesian approach marginalizing weights gp-lstm kernel would also provide principled probabilistic mechanism learning model hyperparameters. could relax several additional assumptions. modeled output dimension independent shared recurrent transformation. capture correlations output dimensions would promising move multi-task formulation. future could also learn time horizon recurrent transformation could lead major additional performance gains. finally semi-stochastic learning procedure naturally complements research asynchronous optimization combination stochastic variational inference semi-stochastic approach could used parallel kernel learning side-stepping independence assumptions prior work. envision eﬀorts gaussian processes harmonize current progress bayesian deep learning. massively scalable gaussian processes signiﬁcant extension kernel interpolation framework originally proposed wilson nickisch core idea framework improve scalability inducing point methods placing virtual points regular grid exploiting resulting kronecker toeplitz structures relevant covariance matrices local cubic interpolation back kernel evaluated original points. combination techniques brings complexity training test prediction. below overview methodology. remark major diﬀerence philosophy msgp many classical inducing point methods points selected ﬁxed rather optimized over. allows signiﬁcantly virtual points typically results better approximation true kernel. structured kernel interpolation given inducing points cross-covariance matrix training inputs inducing points approximated ˜kxu using matrix interpolation weights allows approximate arbitrary inputs ˜kxu given kernel function inducing points structured kernel interpolation procedure gives rise following approximate kernel wilson nickisch note standard inducing point approaches subset regression fully independent training conditional reinterpreted perspective. importantly eﬃciency ski-based msgp methods comes from ﬁrst clever choice inducing points allows exploit algebraic structure second using sparse local interpolation matrices. practice local cubic interpolation used eigendecompose toeplitz matrix approximated circulant matrix eigendecomposes simply applying discrete fourier transform ﬁrst column. therefore approximate eigendecomposition computed fast fourier transform requires time. structure exploiting inference perform inference need solve kernel learning requires evaluating det. ﬁrst task accomplished using iterative scheme—linear conjugate gradients—which depends matrix vector multiplications second done exploiting kronecker toeplitz structure computing approximate eigendecomposition described above. ˜kxx ˜kx∗x w∗kuu n∗×m sparse interpolation matrices respectively. since precomputed training time test time multiply latter matrix results costs operations leading operations test point. similarly approximate predictive variance also estimated operations note fast prediction methodology readily applied trained gaussian process model agnostic inference learning performed. derivative ∂k/∂θ also standard depends form particular chosen kernel function however computing part ∂l/∂w subtle hence elaborate derivations below. consider ij-th entry kernel matrix kij. think matrix-valued function data vectors d-dimensional transformed space denote notice ∂kij/∂h derivative scalar w.r.t. matrix hence matrix; ∂h/∂wl derivative matrix w.r.t. scalar taken element-wise also gives matrix. also notice function depends i-th j-th elements kernel computed. means ∂kij/∂h non-zero i-th j-th column allows re-write follows practice deriving computationally eﬃcient analytical form ∂k/∂h might complicated kernels especially grid-based approximations kernel enabled. cases simply ﬁnite diﬀerence approximation derivative. remark following section numerical errors result approximation aﬀect convergence algorithm. convergence results semi-stochastic alternating gradient schemes without delayed kernel matrix updates based notable diﬀerences original setting considered paper every iteration sampled underlying distribution. case goal minimize negative marginal likelihood particular given dataset. equivalent original formulation expectation taken w.r.t. empirical distribution corresponds given dataset. optimization procedure access single random point generated data distribution step. algorithm requires access entire training data time kernel matrix computed. given sample propose loop number coordinate blocks apply gauss–seidel type gradient updates block. semi-stochastic scheme parameter blocks updated deterministically entire dataset updated stochastic gradient samples empirical distribution. here consider gradients estimates scaled number full data points mini-batch size respectively. constant scaling introduced sake cleaner proofs. lemma assumption result stronger condition original assumption given semi-stochastic nature algorithm simpliﬁes analysis though critical. assumptions straightforwardly adapted original paper. negative marginal likelihood gaussian process structured kernel nonconvex function arguments. therefore show algorithm converges stationary point i.e. point gradient objective zero. proof proof adaptation given following three adjustments blocks coordinates updates deterministic zeros variance terms stochastic gradients unbiased. show given bounded delay kernel matrix updates algorithm still applying convergent. analysis based computing change argument theorem diﬀerence need take account perturbations kernel matrix introduced delays hence impose certain assumptions spectrum. note assumptions relevant practice assumptions also controlled choosing class kernel functions used model. example smallest eigenvalue kernel matrix controlled smoothing properties kernel consider particular stochastic step algorithm time given mini-batch assuming kernel last updated steps ago. stochastic gradient take following form remark even though provided bounds crude pessimistic estimates perturbed kernel matrix spectrum still balance delay learning rate given expression constant. lane sequence prediction lane represented cubic polynomial coeﬃcients coeﬃcients axes carcentric frame]. instead predicting coeﬃcients discretized lane curves using points table summary feedforward recurrent neural architectures corresponding hyperparameters used experiments. gp-based models used architectures non-gp counterparts. activations given hidden units; vanilla neural nets used linear output activations.", "year": 2016}