{"title": "Tensorial Recurrent Neural Networks for Longitudinal Data Analysis", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Traditional Recurrent Neural Networks assume vectorized data as inputs. However many data from modern science and technology come in certain structures such as tensorial time series data. To apply the recurrent neural networks for this type of data, a vectorisation process is necessary, while such a vectorisation leads to the loss of the precise information of the spatial or longitudinal dimensions. In addition, such a vectorized data is not an optimum solution for learning the representation of the longitudinal data. In this paper, we propose a new variant of tensorial neural networks which directly take tensorial time series data as inputs. We call this new variant as Tensorial Recurrent Neural Network (TRNN). The proposed TRNN is based on tensor Tucker decomposition.", "text": "recent developments recurrent neural networks generally focused lstm model. example lstm combined convolutional neural networks sequence representation learning however traditional lstm model deal vectorised data leads loss spatial information multidimensional time series. intention paper propose fully tensorial connected neural networks tensorial longitudinal data. recent paper also considered tensorial structure classical recurrent neural networks however main purpose reduce number networks parameters vectorial data high-dimension rest paper organized follows. section introduces basic recurrent neural network types tensorial i.e. tensorial lstm tensorial section derive backpropagation algorithms proposed tlstm rgru. section experimental results presented evaluate performance proposed models. finally conclusions future works summarized section abstract—traditional recurrent neural networks assume vectorized data inputs. however many data modern science technology come certain structures tensorial time series data. apply recurrent neural networks type data vectorisation process necessary vectorisation leads loss precise information spatial longitudinal dimensions. addition vectorized data optimum solution learning representation longitudinal data. paper propose variant tensorial neural networks directly take tensorial time series data inputs. call variant tensorial recurrent neural network proposed trnn based tensor tucker decomposition. recent years interests time series sequential effects among data constantly growing academic ﬁeld industry. interests development technology social science including limited multimedia social network economic political network especially international relationship study. time series data acquired many discipline large volume terms time also ever complicated structures multihigh-dimension. rise massive multi-dimensional data demands machine learning systems learn complex models millions billions parameters types data structures promise adequate capacity digest massive datasets offer powerful predictive analytics thereupon. tensorial data come special spatial structure highly desired maintain structure information learning process. seen recent development deep learning architecture multi-dimensional tensor data extending conventional neural networks structured data matrix neural network independent works tensorial neural networks graph data even neural networks manifold-valued data recurrent neural networks commonly applied tool longitudinal data analysis constantly investigated last couple decades many successful applications language processing speech recognition human action recognition etc. many different architectures basic mingyuan junbin discipline business analytics university sydney business school university sydney australia. e-mail mbaiuni.sydney.edu.au; junbin.gaosydney.edu.au loss function single data series. training data presented single time series apply ltsm series collect outputs time point. simple loss time deﬁned building block overall loss funcnetworks input ...xt}. loss function usual squared loss function regression cross-entropy loss classiﬁcation. function multiple data series length/duration. time recurrent network structure certain duration. case assume training data consist number training series loss function panel data many application case particularly panel data duration period series different thus recurrent network different loops. suppose data operator denotes hadamard product i.e. entry-wise product well matrices relevant order applied hidden tensorial variables input tensorial variables terms tensorial mode product tensorial biases. note actual output lstm. fact jointly regulated make potential output hidden variable depending type response data apply extra layer neural network convert tensor hidden shape/structure sake notation simplicity gated recurrent unit introduced slightly dramatic variation lstm. similar proposed tensorial forget input gates combined single update gate. tgru simpler aforementioned tlstm models. consider loss functions deﬁned first calculated note according given layer cost function response case information otherwise hence calculated accordingly. major difference proposed tensorial vectorial linear mapping replaced tensor multiple linear mapping i.e. tucker multiplication denote tucker mapping first derive algorithm lstm step. computation deﬁned equations shown fig. along time forwarded next lstm step hidden tensorial carried onto next step also output extra layer match response hence algorithm lstm unit could pieces information next output loss lstm unit denoted thus combined derivative information denoted backpropagated paper introduced recurrent neural networks high-order tensor data. special recurrent structures i.e. tlstm tgru proposed detailed algorithm derivation. simple experiments demonstrated performance recurrent neural networks. experiments shall conducted demonstrate efﬁciency accuracy existing neural networks. also intend explore applications video data analysis. ionescu vantzos sminchisescu matrix backpropagation deep networks structured layers proceedings ieee international conference computer vision zhang tensorial neural networks application longitudinal network data analysis submitted international conference neural information processing j.-t. chien y.-t. tensor-factorized neural networks ieee transactions neural networks learning systems vol. available http//ieeexplore.ieee.org/document// collected integrated crisis warning system weekly dataset applied study mltr relationship countries four types actions material cooperation material conﬂict verbal cooperation verbal conﬂict mid-. thus particular time point data tensor dimensions input r××. explore different types patterns often seen relational data social networks organise explanatory tensors following different ways. done construct target tensor time lagged xt−. total construct overall dataset size tensors. take period time series sections data training deﬁned following cases defferrard vandergheynst bresson structured sequence modeling graph convolutional recurrent networks proceedings international conference learning representation graves rahman mohamed hinton speech recognition deep recurrent neural networks proceedings ieee international conference acoustics speech signal processing merri¨enboer g¨ulc¸ehre bahdanau bougares schwenk bengio learning phrase representations statistical machine translation using encoder–decoder proceedings natural language processing doha qatar association computational linguistics oct. available http//www.aclweb.org/anthology/d-", "year": 2017}