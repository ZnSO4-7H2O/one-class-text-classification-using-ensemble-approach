{"title": "Analogical Dissimilarity: Definition, Algorithms and Two Experiments in  Machine Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "This paper defines the notion of analogical dissimilarity between four objects, with a special focus on objects structured as sequences. Firstly, it studies the case where the four objects have a null analogical dissimilarity, i.e. are in analogical proportion. Secondly, when one of these objects is unknown, it gives algorithms to compute it. Thirdly, it tackles the problem of defining analogical dissimilarity, which is a measure of how far four objects are from being in analogical proportion. In particular, when objects are sequences, it gives a definition and an algorithm based on an optimal alignment of the four sequences. It gives also learning algorithms, i.e. methods to find the triple of objects in a learning sample which has the least analogical dissimilarity with a given object. Two practical experiments are described: the first is a classification problem on benchmarks of binary and nominal data, the second shows how the generation of sequences by solving analogical equations enables a handwritten character recognition system to rapidly be adapted to a new writer.", "text": "paper deﬁnes notion analogical dissimilarity four objects special focus objects structured sequences. firstly studies case four objects null analogical dissimilarity i.e. analogical proportion. secondly objects unknown gives algorithms compute thirdly tackles problem deﬁning analogical dissimilarity measure four objects analogical proportion. particular objects sequences gives deﬁnition algorithm based optimal alignment four sequences. gives also learning algorithms i.e. methods triple objects learning sample least analogical dissimilarity given object. practical experiments described ﬁrst classiﬁcation problem benchmarks binary nominal data second shows generation sequences solving analogical equations enables handwritten character recognition system rapidly adapted writer. analogy reasoning studied throughout history philosophy widely used artiﬁcial intelligence linguistics. focus paper restricted concept analogy called ‘analogical proportion’. analogical proportion four elements universe usually expressed follows depending elements analogical proportions different meanings. example natural language analogical proportions could crow raven merlin peregrine vinegar wine sloe cherry. based semantics words. contrast formal universe sequences analogical proportions abcd abbd morphological. examples show intrinsic ambiguity good reasons deﬁning analogical proportion. could well accept ggtt vinegar wine vulgar wul. obviously ambiguities inherent semantic analogies since related meaning words hence seems important ﬁrst step focus formal morphological properties. moreover solving analogies sequences four elements unknown analogical proportion turns equation. instance sequences letters analogical proportion wolf leaf wolves corresponds equation wolf leaf wolves resolving equation consists computing sequences satisfy analogy. sequence leaves exact semantic morphological solution. shall that however straightforward design algorithm able solve kind equation particular looking approximate solution necessary. solving analogical equations sequences useful linguistic analysis tasks applied mainly lexical analysis tasks. example yvon presents analogical approach grapheme-to-phoneme conversion text-to-speech synthesis purposes. generally resolution analogical equations also seen basic component learning analogy systems part lazy learning techniques ﬁnite training examples description example label ﬁnite set. given description pattern would like assign label based knowledge problem inductive learning classiﬁcation rule examples consists ﬁnding value point nearest neighbor method popular lazy learning technique simply ﬁnds description minimizes distance hypothesizes label label moving step further learning analogical proportions consists searching triple predicts label solution equation triple found voting procedure used. learning technique based resolution analogical equations. pirrelli yvon discuss relevance learning procedure various linguistic analysis tasks. important notice different domains example simple case learning classiﬁcation rule sequence whereas class label. next step learning analogical proportions given triple holds almost true closeness measure deﬁned triple closest term analogical proportion. study article quantify measure order provide ﬂexible method learning analogy. paper related several domains artiﬁcial intelligence. obviously ﬁrst reasoning analogy. much work done subject cognitive science point view computational models reasoning analogy example classical paper book recent survey usually works notion transfer within scope article. means knowledge solving problem domain transported another domain. since work four objects space implicitly ignore notion transfer different domains. technically speaking restriction allows axiom called ‘exchange means’ deﬁne analogical proportion however share works following idea similar relation couples structured objects even objects apparently quite different. interested giving formal algorithmic deﬁnition relation. work also aims deﬁne supervised machine learning process spirit lazy learning. seek extract model learning data merely conclude class generally supervision object inspecting learning data. usually lazy learning like k-nearest neighbors technique makes unstructured objects vectors. since distance measures also deﬁned strings trees even graphs technique also used structured objects framework structural pattern recognition extend search nearest neighbor learning best triple requires deﬁning analogical proportion structured objects like sequences also give deﬁnition -tuple objects analogy learning analogy sequences already studied restricted manner linguistic data reasoning learning analogy proven useful tasks like grapheme phoneme conversion morphology translation. sequences letters and/or phonemes natural application work also interested type data structured sequences trees prosodic representations speech synthesis biochemical sequences online handwriting recognition etc. analogical proportions four structured objects universe mainly strings studied mathematical algorithmic approach like ours mitchell hofstadter dastani schmid best knowledge proposition original give formal deﬁnition analogical dissimilarity four objects particular sequences produce algorithms enable efﬁcient concept machine learning practical problems. already discussed compute exact analogical proportions sequences paper yvon given preliminary attempt compute analogical dissimilarity sequences paper delhay miclet excerpts present article presented conferences connect another ﬁeld a.i. quote aamodt plaza term ‘analogy’ case-based reasoning ’analogy-based reasoning term sometimes used synonym case-based reasoning describe typical case-based approach. however also often used characterize methods solve problems based past cases different domain typical case-based methods focus indexing matching strategies single-domain cases.’ according authors word ‘analogy’ broader meaning typical deals single domain problems analogical proportions also sense study could seen particular case applied paper supervised learning classiﬁcation rules. paper organized sections. introduction present section general principles govern deﬁnition analogical proportion four objects deﬁne analogical equation set. apply deﬁnitions finally section deﬁnes analogical proportion four sequences alphabet analogy deﬁned using optimal alignment method four sequences. sections introduces concept analogical dissimilarity four objects measuring much objects analogy. particular must equivalent four objects analogy analogical dissimilarity null. extend sequences. section gives algorithms sequana computes value four sequences solvana solves analogical equations generalized manner produce approximate solutions section begins explore concept analogical dissimilarity supervised machine learning. give algorithm fast search k-best analogical -tuples learning set. section presents applications concepts algorithms real problems. ﬁrstly apply fadana objects described binary nominal features. experiments conducted classical benchmarks favorably compared standard classiﬁcation techniques. secondly make solvana produce examples handwritten recognition system. allows training classiﬁer small number learning patterns. section give formal deﬁnition analogical proportion four objects explain solve analogical equation. instanciations general deﬁnitions given objects either ﬁnite sets vectors real numbers sequences ﬁnite alphabets. meaning analogical proportion four objects depends nature ‘as’ relations deﬁned. however general properties required according usual meaning word ’analogy’ philosophy linguistics. according lepage three basic axioms given analogical proportion write ‘the analogical proportion holds true’ simply reads every -tuple analogical proportion following equivalences must hold true already know previous sections that depending nature objects deﬁnition analogy analogical equation either solution unique solution several solutions. study sequel solve analogical equations different sets. ‘as’ relation equality sets lepage given deﬁnition analogical proportion sets coherent axioms. useful section objects described sets binary features. cases among eight exists. derives deﬁning binary features equivalent deﬁning ﬁnite set. theorem imposes conditions resolution analogical equations ﬁnite sets results fact binary analogical equations solution. solving analogical equation vectors unknown derives directly deﬁnition analogy vector spaces four vectors must form parallelogram. always solution given equation sequence ﬁnite series symbols ﬁnite alphabet sequences denoted denotes concatenation also denote length write x|x| denote empty word null length σ⋆\\{ǫ}. deﬁnition alignment sequences lengths word alphabet whose ﬁrst projection semantically equivalent whose second projection semantically equivalent informally alignment represents one-to-one letter matching sequences letters inserted. matching permitted. alignment presented array rows word completed resulting words length. note lepage stroppa yvon already proposed deﬁnition analogical proportion sequences applications linguistic data. basically difference accept trivial analogies alphabet section interested deﬁning could relaxed analogy linguistic expression would almost remain coherent previous deﬁnitions measure term ‘almost’ positive real value equal analogy stands true increasing four objects less likely analogy. also want value call ‘analogical dissimilarity’ good properties respect analogy. want symmetrical stay unchanged permute mean terms analogy ﬁnally respect triangle inequality. requirements allow section generalize classical fast nearest neighbor search algorithm exhibit algorithmic learning process principle extract learning -tuple objects least combined another unknown object. lazy learning technique therefore generalization nearest neighbor method. words four binary values minimal number bits switched order produce analogical proportion. seen extension edit distance four dimensions supports coherence analogy. holds true every -tuple elements every feature property true. demonstration done examining possible cases impossible binary features precisely also equal four values take. analogical dissimilarity four vectors must reﬂect constructing parallelogram. four vectors analogical proportion opposite sides −→uv −→wx equivalently chosen following deﬁnition deﬁnition analogical dissimilarity four vectors deﬁned norm corresponding distance given real positive value also equal solution analogical equation property deﬁnition analogical dissimilarity guarantees following properties hold true coherence analogy symmetry ‘as’ exchange medians triangle inequality asymmetry to’. present following deﬁnition algorithms. firstly extend notion analogical dissimilarity sequences. ﬁrst algorithm called sequana computes analogical dissimilarity four sequences second called solvana given analogical equation sequences produces directed acyclic graph solutions. solution gives sentences least analogical dissimilarity associated three known sentences equation. algorithms quite general since make particular assumption alphabet sequences. alphabet simply augmented produce alignments described section analogical dissimilarity must that every constraint required. structure functional similarity like protein modelization pattern identiﬁcation structure prediction methods using simultaneous alignment like iterative alignment like muscle best. however alignment methods heuristic algorithms overcome problem time space complexity introduced ﬁrst length sequences second number sequences align. generation problem neither sequence length around characters number sequences align always four analogy need heuristic alignment speed algorithm. techniques used bio-informatics compute automatically substitution matrix could helpful interesting handwritten characters recognition. introducing penalties like protein sequences also interesting idea explore. correctness correctness algorithm demonstrated recurrence since uses dynamic programming principles. requires analogical dissimilarity properties called coherence analogy symmetry ‘as’ exchange medians. triangle inequality property necessary. considered analogical equation either exact solutions solution. latter case concept analogical dissimilarity useful deﬁne approximate solution. words best approximate solutions objects closest analogical proportion obviously deﬁnition generalizes solution analogical equation given section since deﬁned good properties several alphabets sequences alphabets compute approximate solution analogical equations domains. turn algorithm ﬁnds best approximate solutions equation objects sequences alphabet deﬁned. also make comments extend capacity k-best solutions. alignment four sequences different lengths realized inserting letters four sequences length. done consider column alignment analogical dissimilarity augmented alphabet. step save cell cost also letter found analogical resolution along optimal progression. completed backward propagation gives optimal generated sequences optimal analogical dissimilarity strucured dag. computational complexity algorithm card maximum length sequences ﬁrst three features indicates letter last indicate case letter example analogical equation. exact solution best approximate solutions example figure displays results produced solvana example. assume exists analogy deﬁned analogical dissimilarity following properties coherence analogy symmetry ‘as’ triangle inequality exchange medians asymmetry to’. obvious solution examine triples brute force method requires calls procedure computing analogical dissimilarity four objects according properties analogical dissimilarity number actually divided change theoretical practical complexity search. situation similar search nearest neighbor machine learning naive algorithm requires distance computations. many proposals made decrease complexity chosen focus extension aesa algorithm based property triangle inequality distances since deﬁned concept analogical dissimilarity similar property natural explore extend algorithm. section describes fast algorithm given objects cardinalty object three objects analogical dissimilarity minimal. based aesa technique extended analogical dissimilarity. thanks properties analogical dissimilarity seen distance couples consequently basically work couples objects. equivalently paragraph terms distance couples dissimilarity four elements describe part done line compute analogical dissimilarity every four objects data base. step complexity time space size come back point section progress aesa-like laesa-like technique reduce computational complexity. basic operation compose couple objects adding object goal couple objects lowest distance change xi+. looping times aesa-like select eliminate technique insures ﬁnally triple lowest analogical dissimilarity associated further devised ameliorated version fadana algorithm preliminary computation storage limited certain number couples objects. principle similar laesa base prototypes couples selected among possibilities greedy process ﬁrst chosen random second possible ﬁrst distance couples objects according deﬁnition analogical dissimilarity conducted experiments measure efﬁciency fadana. tested algorithm four databases repository noting time percentage computed in-line different numbers base prototypes compared made naive method number base prototypes expressed percentage learning set. obviously learning contains elements number possible -tuples built point explains percentage base prototypes compared size learning rise number in-line computations mean test set. purpose ﬁrst experiment measure beneﬁt analogical dissimilarity applied basic problem classiﬁcation compared standard classiﬁers k-nearest neighbors neural networks decision trees. benchmarking interested classifying sequences merely investigate basic concept analogical dissimilarity bring learning classiﬁcation rule symbolic objects. objects deﬁned binary attributes. object learning problem class object using learning this deﬁne learning rule based concept analogical dissimilarity depending integer could called least dissimilar -tuple rule. basic principle following among -tuples consider subset produce least analogical dissimilarity associated part them analogical equation exact solution ﬁnite classes. keep -tuples choose class takes majority among values class table example classiﬁcation analogical dissimilarity. analogical proportions whose analogical resolution classes solution taken account. short example labelled objects object classiﬁed. according analogical proportion axioms card)/) non-equivalent analogical equations among equations formed three objects table shows ﬁrst lines sorting regard arbitrarily analogical dissimilarity. following table gives classiﬁcation object according basic idea weighting attributes importance classiﬁcation importance given discriminative. idea selecting enhancing interesting attributes classical machine learning quite framework analogy. paper turney discrimination done keeping frequent patterns words. therefore greater importance given attributes actually discriminant. however analogical classiﬁcation system several ways class unknown element. take preceding class problem example focus point. notice ways decide class class therefore take account equation used class. deﬁne weights attribute depending number classes. sets stored call analogical weighting matrix. deﬁnition analogical weighting matrix three dimensional array. ﬁrst dimension attributes second class ﬁrst element analogical proportion third class last element analogical proportion. analogical proportion weighting matrix matrix number attributes number classes. given attribute rank element wkij matrix indicates weight must given encountered analogical proportion classes whose ﬁrst element computed solution. goal three dimensional analogical weighting matrix using learning set. estimate wkij frequency attribute analogical proportion ﬁrst element class solves class attribute number objects class secondly compute wkij estimating probability correct analogical proportion attribute ﬁrst element class solves class following table show possible ways analogical proportion binary attribute value attribute class given -tuples produce solution class every -tuple among consider class ﬁrst element class solution. compute analogical dissimilarity -tuple weighted monk problems monk problem noise added. spect heart data balance-scale hayes roth database multiclass database. breast-w mushroom data sets contain missing values. kr-vs-kp kasparov karpov order measure efﬁciency applied standard classiﬁers databases also applied point contribution weighting matrix give parameters used comparison method table worked weka package choosing different classiﬁcation rules data. well binary data like part decision table. others like multilayer perceptron adapted numerical noisy data. results given table arbitrarily taken rules. value sensitive case nominal binary data small databases ones used experiments however possible using validation set. draw following conclusions study ﬁrstly according good classiﬁcation rate databases handles missing values well. secondly seems belong best classiﬁers databases conﬁrms deals well multiclass problems. thirdly shown good classiﬁcation rate problem handles well noisy data. finally results database exactly weighted decision rule fact computed null value. data bases weighting quite effective. unfortunatly last database show poor recognition rate databases means analogy classiﬁcation problems. number pattern recognition systems acquisition labeled data expensive user unfriendly process. example buying smartphone equipped handwritten recognition system customer likely write dozens examples every letter digit order provide system consequent learning sample. however efﬁcient statistical classiﬁcation system retrained personal writing style patterns many examples possible least sufﬁcient number well chosen examples. overcome paradox hence make possible learning classiﬁer examples straightforward idea generate examples randomly adding noise elements small learning sample. recent book bishop gives theoretical coverage procedure rather draws pragmatic conclusion addition random noise inputs shown improve generalization appropriate circumstances’. acter). ofﬂine character recognition several image distortions however used slanting shrinking erosion dilatation. online character recognition several online distortions used speed variation angular variation therefore interested quick tuning handwritten character recognition user consider small examples character required user. learn writer-dependent system synthetic data keep handwriting style original data. second experiment interested handwritten characters captured online. represented sequence letters alphabet freeman symbols code augmented symbols anchorage points. anchorage points come analysis stable handwriting properties deﬁned pen-up/down y-extrema angular points in-loop y-extrema. learning contains examples letter generate synthetic examples analogical proportion described section hence generating artiﬁcial examples letter analogical proportion using three instances augment learning different examples shown following pictures. stratiﬁed cross validation. experiments composed phases three writerdependent recognition systems learned radial basis function network k-nearest neighbor one-against-all support vector machine firstly compute reference recognition rates without data generation recognition rate achievable original characters without character generation gives idea achievable recognition rates original data. practically speaking context learning phase user input characters class. secondly artiﬁcial character generation strategies tested. given writer characters class randomly chosen. synthetic characters class generated make synthetic learning database. experiment done times cross validation split writer mean standard deviation performance rates computed. finally means measurements computed give writer dependent mean recognition rate associated standard deviation. study three different strategies generation synthetic learning databases. strategy ’image distortions’ chooses randomly generation among several image distortions. strategy ’online image distortions’ chooses randomly distortion among image distortions online distortions. ’analogy distortions’ strategy generates two-thirds base previous strategy remaining third generation. results figure compares recognition rates achieved three generation strategies three classiﬁers. firstly note global behavior three classiﬁers. thus following conclusions depend classiﬁer type. secondly three generation strategies complementary using ’online image distortions’ better ’image distortions’ alone ’analogy distortions’ better using distortions. furthermore using four original character complete generation strategy better achieved using original characters. thus conclude using generation strategies learns classiﬁer original data efﬁciently using original data long input phase need three times fewer original data achieve recognition rate. comparing ‘image distortions’ ‘online distortions’ ‘analogy’ alone shows ‘analogy’ less efﬁcient ad-hoc methods. nevertheless generating sequences approximate analogical proportion meaningful somewhat independant classical distorsions. words analogy character shapes used ‘natural intelligence’ somehow captured deﬁnition algorithms. know difference average three methods signiﬁcant. performed methods validation evaluate difference stategies. ﬁrst method parametric t-test second method non-parametric sign test methods comparaison ﬁrst second strategy second third strategy number original characters. t-test compares value difference generation methods regarding variation differences. assumption errors normal distribution errors independent. mean difference large comparing standard deviation strategies statistically different. case probability results random artefact less sign test non-parametric comparison method. beneﬁt avoid assumptions normal distribution observations errors. test replaces difference sign difference. occurrences compared value hypothesis thus strategy frequently better expected mean strategy signiﬁcantly better. case probability hypothesis true less hence difference signiﬁcantly better. figure writer-dependent recognition rates depending number used original characters compared reference rates using characters class rbfn classiﬁers. article investigated formal notion analogy four objects universe. given deﬁnitions analogy formulas algorithms solving analogical equations particular sets. given special focus objects structured sequences original deﬁnition analogy based optimal alignments. also introduced coherent manner notion analogical dissimilarity quantiﬁes four objects analogy. notion useful lazy supervised learning shown time consuming brute force algorithm could ameliorated generalizing fast nearest neighbor search algorithm given preliminary experiments. however much left done want especially explore following questions sort data particularly suited lazy learning analogy? know bibliography linguistic data successfully processed learning analogy techniques ﬁelds grapheme phoneme transcription morphology translation. currently working experiments phoneme grapheme transcription useful special cases speech recognition also interested sequential real data biosequences analogical reasoning technique presently already used. selection data supervision equally important since search less dissemblant analogic triple labelling process based concept analogy. sort structured data processed? sequences naturally extended ordered trees several generalizations alignments already deﬁned. could useful example extending nearest neighbor technique learning prosodic trees speech synthesis could also imagine sequences models like hidden markov models could combined analogical construction. sort algorithms devised large amount data processed techniques? given ﬁrst answer fadana algorithm believe quality results still increased. experiments remain done type algorithm. notice also properties analogical dissimilarity used far. believe algorithm precomputing storage devised currently working conclusion conﬁdent fact notion analogical dissimilarity lazy learning technique associated extended real data structures data larger problems.", "year": 2014}