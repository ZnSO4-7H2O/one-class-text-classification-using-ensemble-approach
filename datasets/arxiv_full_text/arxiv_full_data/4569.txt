{"title": "Datum-Wise Classification: A Sequential Approach to Sparsity", "tag": ["cs.AI", "cs.LG"], "abstract": "We propose a novel classification technique whose aim is to select an appropriate representation for each datapoint, in contrast to the usual approach of selecting a representation encompassing the whole dataset. This datum-wise representation is found by using a sparsity inducing empirical risk, which is a relaxation of the standard L 0 regularized risk. The classification problem is modeled as a sequential decision process that sequentially chooses, for each datapoint, which features to use before classifying. Datum-Wise Classification extends naturally to multi-class tasks, and we describe a specific case where our inference has equivalent complexity to a traditional linear classifier, while still using a variable number of features. We compare our classifier to classical L 1 regularized linear models (L 1-SVM and LARS) on a set of common binary and multi-class datasets and show that for an equal average number of features used we can get improved performance using our method.", "text": "abstract. propose novel classiﬁcation technique whose select appropriate representation datapoint contrast usual approach selecting representation encompassing whole dataset. datum-wise representation found using sparsity inducing empirical risk relaxation standard regularized risk. classiﬁcation problem modeled sequential decision process sequentially chooses datapoint features classifying. datum-wise classiﬁcation extends naturally multi-class tasks describe speciﬁc case inference equivalent complexity traditional linear classiﬁer still using variable number features. compare classiﬁer classical regularized linear models common binary multi-class datasets show equal average number features used improved performance using method. feature selection main contemporary problems machine learning approached many directions. modern approach feature selection linear models consists minimizing regularized empirical risk. particular risk encourages model good balance classiﬁcation error high sparsity regularized problem combinatorial many approaches lasso address combinatorial problem using practical norms approaches developed main goals mind restricting number features improving classiﬁcation speed limiting used features useful prevent overﬁtting. classical approaches sparsity ﬁnding sparse representation features space global entire dataset. propose approach sparsity goal limit number features datapoint thus datum-wise sparse classiﬁcation means approach allows choice features used classiﬁcation vary relative datapoint; data points easy classify inferred without looking many features diﬃcult datapoints classiﬁed using features. underlying motivation that classical approaches balance accuracy sparsity dataset level approach optimizes balance individual datum level thus resulting equivalent accuracy higher overall sparsity. kind sparsity interesting several reasons first simpler explanations always preferred occam’s razor. second knowledge extraction process datum-wise sparsity able provide unique information underlying structure data space. typically dataset organized onto diﬀerent subspaces datum-wise sparsity principle allows model automatically choose classify using features another subspace. dwsc considers feature selection classiﬁcation single sequential decision process. classiﬁer iteratively chooses features classifying particular datum. sequential decision process datum-wise sparsity obtained introducing penalizing reward agent chooses incorporate additional feature decision process. model learned using algorithm inspired reinforcement learning contributions paper threefold propose approach classiﬁcation seen sequential process choose features depending input inferred upon. approach results model obtains good performance terms classiﬁcation maximizing datum-wise sparsity i.e. mean number features used classifying whole dataset. also naturally handles multi-class classiﬁcation problems solving using features possible classes combined. perform series experiments diﬀerent corpora compare model obtained lars l-regularized thus providing qualitative study behaviour algorithm. paper organized follow first deﬁne notion datum-wise sparse classiﬁers explain interest models section describe sequential approach classiﬁcation detail learning algorithm complexity algorithm section describe approach extended multi-class classiﬁcation section detail experiments datasets also give qualitative analysis behaviour model section related work given section empirical risk minimization problem consider prior assumption constraint concerning form solution result overﬁtting models. moreover facing large number features obtained solutions usually need perform computations features classifying input thus negatively impacting model’s classiﬁcation speed. propose diﬀerent risk minimization problem penalization term encourages obtained classiﬁer classify using average features possible. comparison classical regularized approaches goal constraint number features used dataset level approach performs sparsity datum level allowing classiﬁer diﬀerent features classifying diﬀerent inputs. results datum-wise sparse classiﬁer that possible uses features classifying easy inputs features classifying diﬃcult ambiguous ones. predicted output n-dimensional vector implies feature taken consideration computing label datum convention denote predicted label corresponding z-vector thus feature used classifying category deﬁnition data-wise classiﬁers main advantages first next section explain features constraints features used classiﬁcation. allows encourage datum-wise sparsity deﬁne below. second main focus article analysis gives qualitative explanation classiﬁcation decision made study section note deﬁne datum-wise classiﬁcation extension usual deﬁnition classiﬁer. classifying number elements equal general case minimization risk results classiﬁer average selects features classifying diﬀerent features w.r.t input classiﬁed. consider crux dwsc model classiﬁer takes datum consideration diﬀerently inference process. fig. sequential process problem features possible categories left gray circle initial state particular input small circles correspond terminal states classiﬁcation decision made. example classiﬁcation made sequentially choosing acquire feature feature classify category bold arrows correspond trajectory made current policy. right value diﬀerent states illustrated. value arrows corresponds immediate reward received agent assuming belongs beginning information attribute/feature values. then step choose acquire particular feature classify classifying category ends episode sequential process. classiﬁcation process deterministic process deﬁned feature selection actions that corresponds choosing feature action corresponds vector element equal i.e. note possible feature selection actions state denoted equal subset currently unselected features i.e. s.t. classiﬁcation actions correspond assignpolicy decides action take applying scoring function every action possible state greedily taking highest scoring action. scoring function reﬂects overall quality taking action started state followed policy parameterization steps. taking rewards gives total reward state episode. since policy deterministic refer parameterized policy using simply note optimal parameterization obtained learning parameterization maximizes expected reward state-action pairs process. practice initial state process input corresponds empty vector feature selected. policy sequentially picks features pertinent classiﬁcation task chooses classify enough features considered. explained section ultimate goal parameterization minimizes datum-wise empirical loss deﬁned equation training process described maximization reward function. therefore show maximizing reward function equivalent minimizing datum-wise empirical loss. equivalence risk minimization reward maximization shows optimal classiﬁer corresponds optimal policy deﬁned previously. equivalence allows classical resolution algorithms order best classiﬁer. detail learning procedure section inﬁnite number possible inputs number states also inﬁnite. moreover reward function known values training cannot computed input. reasons possible compute score function state-action pairs tabular manner function approximated. scoring function underlies policy approximated inﬁniteness state-action pairs represented feature space. note featurized representation stateaction pair. many deﬁnitions used feature representation propose simple projection restrict representation although non-linear models neural networks used chosen restrict linear model able properly compare performance state-of-the-art linear sparse models. able diﬀerentiate attribute known attribute simply equal must keep information present intermediate representation corresponds concatenation simply need keep information present manner allows action easily distinguished linear classiﬁer. block-vector trick consists projecting higher dimensional space position inside global vector dependent action learning goal learning phase optimal policy parameterization maximizes expected reward thus minimizing datum-wise regularized loss deﬁned explained section cannot exhaustively explore state space training therefore monte-carlo approach sample example states learning space. approximate policy iteration algorithm rollouts sampling state-action pairs according previous policy consists iteratively learning better policy bellman equation. rollouts algorithm composed three main steps iteratively repeated state sampled state policy used compute expected reward choosing possible action state. feature vector state-action pair sampled corresponding expected reward denoted parameters policy computed using classical linear regression states corresponding expected rewards obtained previously. generalizing capacity classiﬁer gives estimated score state-action pairs even never visited them. section explain process which step either choose feature classify current datum. process core dwsc suﬀer overﬁtting number features larger number training examples. case dwsc would tend learn select speciﬁc features training example. classical regularization models datum-wise classiﬁer must features classifying data thus overly speciﬁc features chosen usually appear training examples. propose simple variant general model allows avoid overﬁtting. still allow dwsc choose many features classifying input constrain choose features order inputs. that constrain score feature selection actions depend vector state example eﬀect constraint presented fig. constraint handled following manner fig. diﬀerence base unconstrained model constrained model described section ﬁgure shows diﬀerent inputs features selected classiﬁers classiﬁcation. constrained model chooses features order inputs. although constraint forces dwsc choose features order still automatically learn best order choose features stop adding features classify. however avoid choosing diﬀerent features sets classifying diﬀerent inputs thus avoid overﬁtting problem. complexity analysis learning complexity explained section learning method based reinforcement learning rollouts. approach expensive term computations needs iteration algorithm simulate trajectories decision process learn scoring function based trajectories. without giving details computation states used rollouts number features number possible categories. implies learning method quadratic w.r.t. number features; proposed approach able deal problems thousands possible features. breaking complexity active research perspective leads. features chosen system classifying. fact shape function presented section linear nature score actions eﬃciently incrementally computed step process adding contribution newly added feature. complexity thus reduced moreover constrained model results ordering features lower complexity case model choose diﬀerent remaining features choice classify next feature w.r.t. learned order. learning complexity model higher baseline global linear methods inference speed close unconstrained model equivalent constrained one. practice baseline methods choose subset variables couple seconds couple minutes whereas method takes dozen minutes hour depending number features categories. practice inference indeed speed opinion important factor. experiments experiments diﬀerent datasets obtained libsvm website. datasets correspond binary classiﬁcation task four multi-class problem. datasets described table dataset randomly sampled diﬀerent training sets taking examples training examples remaining examples kept testing. performed experiments three diﬀerent models l-svm used fig. accuracy w.r.t. sparsity. plots left side x-axis corresponds sparsity right side corresponds high sparsity. performances models usually decreasing sparsity increases except case overﬁtting. baseline linear model regularization. lars used obtain optimal solution lasso problem values regularization coeﬃcient once. datum-wise sequential model tested versions presented above dwsm-un original unconstrained model dwsm-con constrained model preventing overﬁtting. splits given dataset obtain averaged ﬁgures. sparsity measured proportion features used l-svm lars binary classiﬁcation mean proportion features used classify testing examples dwsm. multi-class problems lars/svm model table table contains accuracy model binary classiﬁcation problems depending three levels sparsity using diﬀerent training sizes. accuracy linearly interpolated curves like ones given figure sequential experiments number rollout states number policy iterations ﬁxed note experiments rollout states and/or iterations give similar results. experiments made using alpha mixture policy ensure stability learning process. tested diﬀerent models diﬀerent values controls sparsity. note even value contrary baseline models dwsm model features classiﬁcation. results corpus training size computed sparsity/accuracy curves showing performance diﬀerent models w.r.t. sparsity solution. representative curves given figure summarize performances datasets give accuracy diﬀerent models three levels sparsity tables lack space tables present lars’ performance equivalent performances l-svm. note order obtain accuracy given level sparsity computed linear interpolation diﬀerent curves obtained corpus training size. linear interpolation allows compare baseline sparsity methods choose ﬁxed number features average number features chosen dwsc. compares average amount information considered classiﬁer. believe approach still provides good appreciation algorithm’s capacities. table shows that sparsity level dwsm-un dwsm-con models outperform baseline l-svm classiﬁer. particularly true datasets results ambiguous three others datasets breast ionosphere sonar. sparsity similar results obtained. depending corpus training size diﬀerent conﬁgurations observed. datasets easily classiﬁed using features australian example. case approach gives similar results comparison methods datasets method clearly outperforms baseline methods splice dataset model better best using less features average. fact sequential process solves diﬀerent classiﬁcation problem appropriate particular datasets particularly distribution data split amongst distinct subspaces. case model able choose appropriate features input. using small training sets datasets sonar ionosphere overﬁtting observed dwsm-con seems better choice unconstrained version thus version algorithm well-suited number learning examples small. concerning multi-class problems similar eﬀects observed model seems particularly interesting number categories high segment vowel. fact average sparsity optimized sequential model multi-class problem l-svm lars need learn model category perform separate sparsity optimizations class. feature. example dwsm-con uses feature classifying test examples dwsm-un uses feature classifying examples. right mean proportion features used classifying. example dwsm-con classiﬁes examples using exactly features dwsm-un classiﬁes examples using exactly features. figure gives qualitative results. first left histogram features used decisions. illustrates ability model detect important features must used decision. note many features also used l-svm lars models. sparsity gain comparison baseline model obtained features used decisions. right histogram dwsm model mainly classiﬁes using features showing model able adapt behaviour diﬃculty classifying particular input. conﬁrmed green violet histograms show incorrect decisions classiﬁer almost always acquires features classifying. diﬃcult inputs seem identiﬁed features suﬃcient good understanding. behaviour opens appealing research directions concerning acquisition creation features related work feature selection comes three main ﬂavors wrapper ﬁlter embedded approaches. wrapper approaches involve searching feature space optimal subset features maximize classiﬁer performance. feature selection step wraps around classiﬁer using classiﬁer black-box evaluator selected feature subset. searching entire feature space quickly intractable therefore various approaches proposed restrict search advantage wrapper approaches feature subset decision take consideration feature inter-dependencies avoid redundant features however problem remains exponential size search space. filter approaches rank features scoring function independent eﬀect associated classiﬁer. since choice features inﬂuenced classiﬁer performance ﬁlter approaches rely purely adequacy scoring functions. filtering methods susceptible discriminating redundant features missing feature interdependencies filter approaches however easier compute statistically stable relative changes dataset. embedded approaches include feature selection part learning machine. include algorithms solving lasso problem linear models involving regularizer based sparsity inducing norm p∈-norms group lasso ...). kernel machines provide mixture feature selection construction part classiﬁcation problem. decision trees also considered embedded approaches although also similar ﬁlter approaches heuristic scores tree construction. main critique embedded approaches two-fold susceptible include redundant features techniques described easily applied multi-class problems.in brief ﬁltering embedded approaches drawbacks terms ability select best subset features whereas wrapper methods main drawback intractability searching entire feature space. furthermore existing methods perform feature selection based whole training features used represent data. sequential decision problem deﬁnes feature selection classiﬁcation tasks. sense approach resembles embedded approach. practice however ﬁnal classiﬁer single datapoint remains separate entity sort black-box classifying machine upon performance evaluated. additionally learning algorithm free navigate entire combinatorial feature space. sense approach resembles wrapper method. work using similar formalisms diﬀerent goals lacking experimental results. sequential decision approaches used cost-sensitive classiﬁcation similar models also applications reinforcement learning optimize anytime classiﬁcation previously looked using reinforcement learning ﬁnding stopping point feature quantity text classiﬁcation finally sense dwsc similarity decision trees datapoint labeled following diﬀerent path feature space. however underlying mechanism quite diﬀerent term inference procedure learning criterion. work using generating decision trees approach still tied decision tree construction heuristics product remains decision tree. conclusion article introduced concept datum-wise classiﬁcation learn classiﬁer sparse representation data adaptive datum classiﬁed. took approach sparsity considers combinatorial space features proposed sequential algorithm inspired reinforcement learning solve problem. showed ﬁnding optimal policy reinforcement learning problem equivalent minimizing regularized loss classiﬁcation problem. additionally showed model works naturally multi-class problems easily extended avoid overﬁtting datasets number features larger number examples. experimental results datasets showed approach indeed able increase sparsity maintaining equivalent classiﬁcation accuracy.", "year": 2011}