{"title": "A Gauss-Newton Method for Markov Decision Processes", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Approximate Newton methods are a standard optimization tool which aim to maintain the benefits of Newton's method, such as a fast rate of convergence, whilst alleviating its drawbacks, such as computationally expensive calculation or estimation of the inverse Hessian. In this work we investigate approximate Newton methods for policy optimization in Markov Decision Processes (MDPs). We first analyse the structure of the Hessian of the objective function for MDPs. We show that, like the gradient, the Hessian exhibits useful structure in the context of MDPs and we use this analysis to motivate two Gauss-Newton Methods for MDPs. Like the Gauss-Newton method for non-linear least squares, these methods involve approximating the Hessian by ignoring certain terms in the Hessian which are difficult to estimate. The approximate Hessians possess desirable properties, such as negative definiteness, and we demonstrate several important performance guarantees including guaranteed ascent directions, invariance to affine transformation of the parameter space, and convergence guarantees. We finally provide a unifying perspective of key policy search algorithms, demonstrating that our second Gauss-Newton algorithm is closely related to both the EM-algorithm and natural gradient ascent applied to MDPs, but performs significantly better in practice on a range of challenging domains.", "text": "approximate newton methods standard optimization tool maintain beneﬁts newton’s method fast rate convergence whilst alleviating drawbacks computationally expensive calculation estimation inverse hessian. work investigate approximate newton methods policy optimization markov decision processes ﬁrst analyse structure hessian objective function mdps. show that like gradient hessian exhibits useful structure context mdps analysis motivate gauss-newton methods mdps. like gauss-newton method non-linear least squares methods involve approximating hessian ignoring certain terms hessian diﬃcult estimate. approximate hessians possess desirable properties negative deﬁniteness demonstrate several important performance guarantees including guaranteed ascent directions invariance aﬃne transformation parameter space convergence guarantees. ﬁnally provide unifying perspective policy search algorithms demonstrating second gauss-newton algorithm closely related em-algorithm natural gradient ascent applied mdps performs signiﬁcantly better practice range challenging domains. gauss-newton methods estimation preconditioners gauss-newton update direction performance guarantees analysis ascent directions aﬃne invariance convergence analysis aﬃne invariance experiment cart-pole swing-up benchmark experiment non-linear navigation experiment -link rigid manipulator experiments experiment using line search experiment using fixed step size tetris experiment robot experiment proofs theorems proof theorem proof theorem deﬁniteness results proof theorem proof theorem proof theorem proofs theorems proof theorem proof theorem update direction recurrent state search direction evaluation second gauss-newton method inversion preconditioning matrices hessian-free conjugate gradient method fast gauss-newton ascent markov decision processes standard model optimal control fully observable environment strong empirical results obtained numerous challenging real-world optimal control problems using framework. includes problems non-linear control robotic applications biological movement systems traﬃc management helicopter ﬂight control elevator scheduling numerous games including chess backgammon atari video games well-known global optimum obtained methods based dynamic programming value iteration policy iteration however techniques known suﬀer curse dimensionality makes infeasible real-world problems interest. result research reinforcement learning control theory literature focused obtaining approximate locally optimal solutions. exists broad spectrum techniques including approximate dynamic programming methods tree search methods local trajectory-optimization techniques diﬀerential dynamic programming ilqg policy search methods focus paper policy search methods family algorithms proven extremely popular recent years numerous desirable properties make attractive practice. policy search algorithms typically specialized applications techniques numerical optimization such controller deﬁned terms diﬀerentiable representation local information objective function gradient used update controller smooth non-greedy manner. updates performed incremental manner algorithm converges local optimum objective function. several beneﬁts approach smooth updates control parameters endow algorithms general convergence guarantees; performance improved iteration algorithms good anytime performance properties; necessary approximate value function typically diﬃcult function approximate instead necessary approximate low-dimensional projection value function observation emergence called actor-critic methods policy search methods easily extendable models optimal control partially observable environment ﬁnite state controllers gradient objective function. steepest gradient ascent enjoyed success suﬀers serious issue hinder performance. speciﬁcally steepest ascent direction invariant rescaling components parameter space gradient often poorly-scaled i.e. variation objective function diﬀers dramatically along diﬀerent components gradient leads poor rate convergence. also makes construction good step size sequence diﬃcult problem important issue stochastic methods. poor scaling well-known problem steepest gradient ascent alternative numerical optimization techniques considered policy search literature. approaches proven particularly popular expectation maximization natural gradient ascent successfully applied various challenging mdps kober peters toussaint kakade bagnell schneider respectively). avenue research received less attention application newton’s method markov decision processes. although baxter bartlett provide extension gpomdp algorithm give empirical results either baxter bartlett accompanying paper empirical comparisons since limited amount research using second order information contained hessian parameter update. best knowledge attempts made schraudolph on-line estimate hessian-vector product used adapt step size sequence online manner; bayesian policy gradient methods extended newton method. several reasons lack interest. firstly many problems construction inversion hessian computationally expensive feasible. additionally objective function typically concave hessian isn’t guaranteed negative-deﬁnite. result search direction newton method ascent direction hence parameter update could actually lower objective. additionally variance sample-based estimators hessian larger estimators gradient. important point variance gradient estimates problematic issue various methods baselines exist reduce variance. many problems particular markov decision processes general longstanding issues plague newton method. various methods developed optimization literature alleviate issues whilst also maintaining desirable properties newton method. instance quasi-newton methods designed eﬃciently mimic newton method using evaluations gradient obtained previous iterations algorithm. methods computational costs super-linear rate convergence proven extremely eﬀective practice. nocedal wright introduction quasi-newton methods. alternatively well-known gauss-newton method popular approach aims eﬃciently mimic newton method. gauss-newton method particular non-linear least squares objective functions hessian particular structure. structure exist certain terms hessian used useful proxy hessian itself resulting algorithm various desirable properties. instance preconditioning matrix used gauss-newton method guaranteed positive-deﬁnite non-linear least squares objective guaranteed decrease suﬃciently small step size. straightforward application quasi-newton methods typically possible mdps paper consider whether analogue gauss-newton method exists beneﬁts methods applied mdps. speciﬁc contributions follows policy hessian theorem analyse behaiviour individual terms hessian provide insight constructing eﬃcient approximate newton methods policy optimization. particular show certain terms negligible near local optima. policy optimization mdps retain certain terms hessian decomposition preconditioner gradient-based policy search algorithm. ﬁrst method discards terms negligible near local optima diﬃcult approximate. second method discards additional term cannot guarantee negative-deﬁnite. provide analysis gauss-newton methods give several important performance guarantees second gauss-newton method demonstrate pre-conditioning matrix negative-deﬁnite controller log-concave control parameters guaranteeing search direction ascent direction. provide convergence analysis demonstrating linear convergence local optima terms step size update. practical beneﬁt analysis step size incremental update chosen independently unknown quantities retaining guarantee convergence. preconditioner particular form enables assent direction computed particularly eﬃciently hessian-free conjugate gradient method large parameter spaces. quasi-newton methods ensure increase objective function necessary satisfy secant condition condition satisﬁed objective concave/convex strong wolfe conditions line search. reason stochastic applications quasi-newton methods restricted convex/concave objective functions particular relate search direction second gauss-newton algorithm expectation maximization also discuss relationship natural gradient algorithm. section introduce markov decision processes along standard terminology relating models required throughout paper. section introduce policy search methods detail several algorithms literature. markov decision process agent controller interacts environment course planning horizon. point planning horizon agent selects action receives scalar reward. amount reward received depends selected action state environment. action performed system transitions next point planning horizon state environment determined action agent selected current state environment. optimality agent’s behaviour measured terms total reward agent expect receive course planning horizon optimal control obtained quantity maximized. conditional distributions state space }∈s×a reward function assumed bounded nonnegative. given planning horizon time-point planning horizon notation denote random variable state action time-point respectively. state initial time-point determined initial state distribution given time-point given state environment agent selects action according policy dynamics process selecting actions transitioning objective policy maximizes given function expected reward course planning horizon. paper usually consider inﬁnite horizon discounted reward framework objective function takes shall consider forms policy search algorithm paper gradient-based optimization methods methods based iteratively optimizing lower-bound objective function. gradient-based methods update policy parameters take form update increase total expected reward. provided preconditioning matrix always negative-deﬁnite step size sequence appropriately selected iteratively updating policy parameters according policy parameters converge local optimum generic gradient-based policy search algorithm given algorithm gradient-based methods vary form preconditioning matrix used parameter update. choice preconditioning matrix determines various aspects resulting algorithm computational complexity rate algorithm converges local optimum invariance properties paoptimal policy iteratively optimizing lower bound objective function. em-algorithm doesn’t update form given shall section algorithm closely related update. review speciﬁc policy search methods. enumeration sets feasible. alternatively continuous domains presence non-linearities transition dynamics makes calculation occupancy marginals intractable problem. various techniques proposed literature estimate gradient including method ﬁnite-diﬀerences simultaneous perturbation methods likelihood-ratio methods likelihood-ratio methods originated statistics literature later applied mdps prominent method estimating gradient. numerous methods literature including monte-carlo methods actor-critic methods steepest gradient ascent known perform poorly objective functions poorly-scaled changes parameters produce much larger variations function changes parameters. case steepest gradient ascent zig-zags along ridges objective parameter space extremely diﬃcult gauge appropriate scale steps sizes poorly-scaled problems robustness optimization algorithms poor scaling signiﬁcant practical importance reinforcement learning since line search procedures suitable step size often impractical. natural gradient ascent techniques originated neural network blind source separation literature introduced policy search literature kakade address issue poor scaling natural gradient methods take perspective parameter space viewed manifold structure distance points manifold captures discrepancy models induced diﬀerent parameter vectors. natural gradient ascent viewed imposing local norm parameter space second order approximation kl-divergence induced policy distributions. trajectory distribution satisﬁes fisher regularity conditions additionally using compatible function approximator within actor-critic framework optimal critic parameters coincide natural gradient. furthermore natural gradient ascent shown perform well diﬃcult environments including tetris several challenging robotics problems however theoretically rate convergence natural gradient ascent steepest gradient ascent i.e. linear although noted substantially faster practice. alternative optimization procedure focus much research planning reinforcement learning communities em-algorithm em-algorithm powerful optimization technique popular statistics machine learning community successfully applied large number problems. barber general overview applications algorithm machine learning literature. among strengths algorithm guarantee increasing objective function iteration often simple update equations generalization highly intractable models variational bayes approximations given advantages em-algorithm natural extend algorithm framework. several derivations em-algorithm mdps exist reference state lower-bound upon algorithm based following theorem. theorem suppose given markov decision process objective markovian trajectory distribution given distribution space trajectories following bound holds noted section newton method suﬀers issues often make application mdps unattractive practice. result comparatively little research newton method policy search literature. however newton method signiﬁcant attractive properties aﬃne invariance policy parametrization quadratic rate convergence. interest therefore consider whether construct eﬃcient gauss-newton type method mdps positive aspects newton method maintained negative aspects alleviated. section provide analysis hessian mdp. analysis used section propose gauss-newton type methods mdps. section provide novel representation hessian section detail deﬁniteness properties certain terms hessian section analyse behaviour individual terms hessian vicinity local optimum. remark relatively simple estimate manner estimating policy gradient. term diﬃcult estimate since contains terms involving unknown gradient removing dependence present analysis terms policy hessian simplifying expansion demonstrating conditions certain terms disappear. analysis used motivate gauss-newton methods section non-negative entire state-action space seen positive-deﬁnite similarly shown certain common policy parametrizations negative-deﬁnite entire parameter space. summarized theorem expansion gives terms positive-deﬁnite term negative-deﬁnite term remainder term shall show section becomes negligible around local optimum given suﬃciently rich policy parametrization. contrast state-action value function advantage function takes positive negative values state-action space. result notion value consistent policy class. property policy class captures idea policy class rich enough changing parameter maximally improve value state worsen value another state. i.e. policy class value consistent trade-oﬀs improving value diﬀerent states. example. illustrate concept value consistent policy parametrization consider simple maze navigation mdps value consistent policy parametrization policy parametrization value consistent. mdps displayed figure walls maze solid lines dotted lines indicate state boundaries passable. agent starts equal probability states marked ‘s’. agent receives positive reward reaching goal state marked reset start states. state-action pairs return reward zero. four possible actions state optimal policy move probability direction indicated state boundaries. perceptual aliasing occurs mdps policy parametrization states aliased hallway problem states aliased mccallum’s grid. hallway problem aliased states optimal action value states increase/decrease unison. hence seen policy parametrization value consistent hallway problem. mccallum’s grid however optimal action states move upwards state move downwards. example increasing probability moving downwards state also increase probability moving downwards states point therefore increasing probability moving downwards state decrease value states thus policy parametrization value consistent mccallum’s grid. show tabular policies i.e. policies that state conditional distribution parametrized separate parameter vector value consistent regardless given markov decision process. figure hallway problem. feature states feature optimal policy identical states. mccallum’s grid. feature states feature optimal policy diﬀers among states. example returning mdps given figure empirically policy approaches local optimum terms spectral norm relation distance local optimum. correspondence hallway problem section propose several gauss-newton type methods mdps motivated analysis section algorithms outlined section performance analysis provided section figure graphical illustration logarithm spectral norm terms hallway problem mccallum’s grid given policy parametrization plot displays components hessian policy converges local optimum. expected hallway problem dominates. example magnitude roughly hundred times greater conversely mccallum’s grid larger magnitude example. ﬁrst gauss-newton method propose drops hessian terms diﬃcult estimate expected negligible vicinity local optima. speciﬁcally shown section policy parametrization value consistent given function. similarly policy parametrization suﬃciently rich although necessarily value consistent expected negligible vicinity local optimum. cases deﬁned theorem shall second gauss-newton method important performance guarantees including guaranteed ascent direction; linear convergence local optimum step size depend upon unknown quantities; invariance aﬃne transformations parameter space; eﬃcient estimation procedures preconditioning matrix. also show section second gauss-newton method closely related natural gradient algorithms. call methods diagonal ﬁrst second gauss-newton methods respectively. diagonalization amounts performing approximate newton methods parameter independently simultaneously. possible extend typical techniques used estimate policy gradient estimate preconditioner gauss-newton method including either hessian logpolicy outer product derivative log-policy respective diagonal terms. example section appendix detail extension recurrent state formulation gradient evaluation average reward framework second gauss-newton method. extension tetris experiment consider section given sampled state-action pairs complexity extension provide details situations inversion preconditioning matrices performed eﬃciently section appendix. finally second gauss-newton method ascent direction estimated particularly eﬃciently general objective concave means hessian negative-deﬁnite entire parameter space. cases newton method actually lower objective undesirable aspect newton method. consider ascent directions gauss-newton methods particular demonstrate proposed second gauss-newton method guarantees ascent direction typical settings. ﬁrst gauss-newton method necessarily result increase objective function. however standard correction techniques could consider ensure increase objective function obtained adding ridge term preconditioning matrix. survey correction techniques found boyd vandenberghe log-concave respect policy parameters policy constant curvature respect action space. follows cases increase objective function obtained using second gauss-newton method suﬃciently small step-size. additionally diagonal terms negative-deﬁnite matrix negather log-concave blockwise log-concave. firstly consider gibb’s policy feature vector. policy widely used discrete systems log-concave seen fact undesirable aspect steepest gradient ascent performance dependent choice basis used represent parameter space. important desirable property newton method invariant non-singular aﬃne transformations newton steps. method said scale invariant invariant non-singular rescalings parameter space. non-singular diagonal matrix. proposed approximate newton methods various invariance properties properties summarized following theorem. theorem ﬁrst second gauss-newton methods invariant aﬃne transformations parameter space. diagonal versions algorithms invariant rescalings parameter space. provide local convergence analysis gauss-newton framework. shall focus full gauss-newton methods analysis diagonal gauss-newton method following similarly. additionally shall focus case constant a)−∇u αh−∇u given matrix denote spectral radius maxi |λi| {λi}n eigenvalues throughout section shall denote ∇w|w=w∗g. theorem suppose ∇w|w=w∗u invertible fr´echet diﬀerentiable takes form seen section second gauss-newton method close relationship em-algorithm. reason postpone additional discussion rate convergence second gauss-newton method then. section detail relationship second gauss-newton method existing policy search methods; section detail relationship natural gradient ascent section detail relationship em-algorithm. comparing form fisher information matrix given additional weighting integrand state-action value function. hence incorporates information reward structure natural gradient method allows less movement directions high norm which seen form directions induce large changes policy parts state-action space likely visited current policy parameters. movement allowed directions either induce small change policy induce large changes policy parts state-action space unlikely visited current policy parameters. similar manner second gauss-newton method obtained term additionally weighted state-action value function thus directions high norm policy rapidly changing state-action pairs likely visited current policy also high value. thus second gauss-newton method updates parameters carefully behaviour high value states aﬀected. conversely directions induce change state-action pairs value norm larger increments made directions. ﬁrst variable alternative terms em-algorithm generalized emalgorithm equivalent steepest gradient ascent. given em-algorithm typically used overcome negative aspects steepest gradient ascent undesirable alternative. possible optimum numerically also undesirable results double-loop algorithm could computationally expensive. finally result provides insight behaviour em-algorithm terms direction parameter update maximization performed explicitly. theorem suppose given markov decision process objective markovian trajectory distribution consider parameter update expectation maximization iteration algorithm i.e. em-algorithm second gauss-newton method considering constant step size one. formalize intuition provide convergence properties em-algorithm applied markov decision processes following theorem. knowledge ﬁrst formal derivation convergence properties application em-algorithm. ﬁrst experiment give empirical illustration full gauss-newton methods invariant aﬃne transformations parameter space. additionally illustrate diagonal gauss-newton methods invariant rescalings dimensions parameter space. consider simple state example kakade example problem policy parameters possible plot trace policy training. policy trained using steepest gradient ascent full gauss-newton methods diagonal gauss-newton methods. train policy original linearly transformed parameter space. policy traces various algorithms given figure expected steepest gradient ascent aﬀected forms transformation diagonal gauss-newton methods invariant diagonal rescalings parameter space full gauss-newton methods invariant forms transformation. also implemented gauss-newton methods standard simulated cart-pole benchmark problem. problem involves pole attached pivot cart applying force cart pole must swung vertical position balanced. problem under-actuated sense insuﬃcient power available drive pole directly vertical position hence problem captures notion trading immediate reward long term gain. episodic experiment used actor-critic architecture using compatible features q-function. used simulator lagoudakis parr except allow continuous actions choose continuous reward signal. state space dimensional representing angle angular velocity pole. action space representing horizontal figure results scale invariance experiment aﬃne invariance experiment. plots show trace policy parameter space course training. plots give trace policy trained original parameter space trained transformed parameter space comparison policy traces transformed parameter space mapped back original space. plots show trace policy policy trained steepest gradient ascent ﬁrst gauss-newton method second gauss-newton method ﬁrst diagonal gauss-newton method second diagonal gauss-newton method policy updated every trajectories i.e. iteration corresponds episodes experience. these trajectories used estimate policy gradient preconditioning matrix remaining trajectories used learn using steepest descent using gradient warm start maximum iterations rather direct inversion. found stable experiment inversion preconditioning matrices methods since fisher information matrix hessians poorly conditioned example policy trajectories supported entirely region space features never active neither gradient hessian fisher information matrix components corresponding feature dimensions. i.e. experiments times best step size unbiased estimate performance step size report. policy update estimated cumulative reward policy return found decreased returned previous parameter point. simple heuristic prevents variance gradient estimates causing policy degradation instability. figure shows cumulative reward iteration methods along standard error. cumulative reward near optimal policy pole quickly swung balanced entire episode. cumulative reward indicates pole swung balanced either optimally quickly controller unable balance pole entire remainder episode. gauss-newton methods signiﬁcantly outperform competitor methods terms speed good policies learned average value policy convergence. furthermore predicted theory step-size gauss-newton methods found perform well; i.e. good performance could obtained without step-size tuning. next domain consider synthetic two-dimensional non-linear considered vlassis state-space problem two-dimensional agent’s position agent’s velocity. control onedimensional dynamics system given follows zero-mean gaussian random variable standard deviation agent starts state addition gaussian noise standard deviation objective agent reach target state starget policy vlassis given control parameters objective function non-trivial experiment initial control parameters sampled region algorithms trajectories sampled training iteration used estimate search direction. consider ﬁnite planning horizon experiment repeated times results experiment given figure gives mean standard error results. step size sequences steepest gradient ascent natural gradient ascent gauss-newton method tuned performance results shown obtained best step size sequence algorithm. -link rigid robot manipulator standard continuous model consisting eﬀector connected -linked rigid body graphical depiction -link rigid manipulator given figure typical continuous control problem systems apply appropriate torque forces joints manipulator move eﬀector desired position. state system given inertia matrix denotes coriolis centripetal forces gravitational force. system highly nonlinear possible deﬁne appropriate control function results linear dynamics diﬀerent stateaction space. technique known feedback linearisation figure results cart-pole experiment. results non-linear navigation task results steepest gradient ascent expectation maximization natural gradient ascent gauss-newton method case -link rigid manipulator recasts torque action space acceleration action space. means state system given control ordinarily problems reward would function generalized co-ordinates eﬀector results non-trivial reward function accounted modelling reward function terms mixture gaussians simplicity consider simpler directly. experiments problem reward function section consider -link rigid manipulator. certain forms policy parametrization possible perform exact evaluation search direction systems. such systems allow direct comparison search direction various policy search algorithms suﬃciently diﬃcult optimization problems provide challenging platform methods. experiments consider policy form furmston experiments maximal value objective function varied dramatically depending random initialization system. account variation maximal value objective function results experiment normalized maximal value achieved algorithms experiment result displayed percentage reward received comparison best results among algorithms considered experiment. ﬁrst experiment compare search direction steepest gradient ascent natural gradient ascent expectation maximization second gauss-newton method. algorithms required speciﬁcation step size minfunc optimization library perform line search. also minfunc library provide stopping criterion algorithms. found line search algorithm step size initialization signiﬁcant eﬀect performance algorithms. therefore tried various combinations settings algorithm selected gave best performance. tried bracketing line search algorithms with step size halving; quadratic/cubic interpolation function values; cubic interpolation function gradient values; step size doubling bisection; cubic interpolation/extrapolation function gradient values. tried following step size initializations quadratic initialization using previous function value function value gradient; twice previous step size. handle situations initial policy parametrization ‘ﬂat’ area parameter space optima function point toleration minfunc zero algorithms. repeated experiment times results shown figure second gauss-newton method signiﬁcantly outperforms comparison algorithms. step direction expectation maximization similar search direction second gauss-newton method problem. fact given log-policy quadratic mean parameters figure normalized total expected reward plotted training time -link rigid manipulator. results line search experiment plot showing results steepest gradient ascent expectation maximization second gauss-newton method natural gradient ascent results ﬁxed step size experiment plot showing results steepest gradient ascent expectation maximization second gauss-newton method natural gradient ascent mean parameters. diﬀerence performance gauss-newton method expectation maximization largely explained tuning step size gaussnewton method compared constant step size expectation maximization. observe eﬀect poor scaling performance various algorithms observe number iterations algorithm requires. counts given table steepest gradient ascent required iterations either natural gradient ascent gauss-newton method require roughly amount iterations. validates natural gradient ascent gauss-newton method robust poor scaling steepest gradient ascent. line search performed previous experiment expensive perform practice particularly stochastic environments many function evaluations required obtain accurate function estimates. obtain gauge diﬃculty selecting step size sequence various policy search methods consider -link manipulator consider ﬁxed step size throughout training. diﬃcult problem algorithms steepest gradient ascent parameter space non-trivial number dimensions objective poorly-scaled. steepest gradient ascent natural gradient ascent considered following ﬁxed step sizes unable obtain reasonable results steepest gradient ascent ﬁxed step sizes reason results omitted. natural gradient ascent found best step size considered. gauss-newton method considered following ﬁxed step sizes found ﬁxed step size gave consistently good results without overstepping parameter space. smaller step sizes obtained better results expectation maximization less ﬁxed step size larger step sizes often found superior results would sometimes overstep parameter space. reasons used ﬁxed step size ﬁnal experiment. repeated experiment times results experiment plotted figure results show even though step size tuning crude still possible obtain strong results comparison expectation maximization doesn’t require selection step size sequence. experiment gauss-newton method took around seconds obtain performance seconds training expectation maximization. furthermore expectation maximization able obtain performance gauss-newton method natural gradient ascent able obtain around performance. reason natural gradient ascent performed poorly problem initial control parameters typically plateau region parameter space objective close zero. plateau region regular basis given amount training time would require overly large step size. however high reward part parameter space found that using natural gradient ascent large step sizes would result overshooting parameter space poor performance. step size able locate areas high reward subset problems considered experiment suﬀering overshooting much larger step sizes. experiment highlights robustness gauss-newton method poor scaling well relative ease selecting good step size sequence. figure results tetris experiment results steepest gradient ascent natural gradient ascent diagonal gauss-newton method gauss-newton method results robot experiment results second gauss-newton method em-algorithm grid empty beginning game. stage game four block piece called tetrzoid appears board begins fall board. whilst tetrzoid moving player allowed rotate tetrzoid move left right. tetrzoid stops moving reaches either bottom board previously positioned tetrzoid. manner board begins tetrzoid pieces. seven diﬀerent variations tetrzoid shown figure horizontal line board completely ﬁlled tetrzoids line removed board player receives score one. game terminates player able fully place tetrzoid board insuﬃcient space remaining board. example conﬁguration board game tetris given figure details game tetris found fahey applications tetris reinforcement learning literature consider simpliﬁed version game current tetrzoid remains board player decides upon desired rotation column position tetrzoid. table iteration counts -link manipulator experiment steepest gradient ascent natural gradient ascent gauss-newton method using minfunc optimization library. model policy using gibb’s distribution consider feature vector following features heights column diﬀerence heights adjacent columns maximum height number ‘holes’. features used bertsekas ioﬀe kakade policy possible obtain explicit maximum straightforward application em-algorithm possible problem. therefore compare diagonal full gauss-newton methods steepest natural gradient ascent. procedure evaluate search direction algorithms experiment. irrespective policy game tetris guaranteed terminate ﬁnite number turns therefore model game absorbing state mdp. reward time-point equal number lines deleted. recurrent state approach estimate gradient using empty board recurrent state. analogous versions recurrent state approach natural gradient ascent diagonal gauss-newton method full gauss-newton method. kakade sample trajectories obtained gradient evaluation estimate fisher information matrix. training iteration approximation search direction obtained sampling games using current policy sample games. given current approximate search direction following basic line search method obtain step size every step size given ﬁnite step sizes sample number games return step size maximal score games. practice order reduce susceptibility random noise used simulator seed possible step size set. line search procedure sampled games possible step sizes. step sizes diﬀerent training algorithms experiment. reduce amount noise results simulator seeds search direction evaluation algorithms considered experiment. particular generate experiment niterations number training iterations experiment. matrix simulator seeds diﬀerent training algorithms element column corresponding simulator seed training iteration experiment. similar manner simulator seeds line search procedure diﬀerent training algorithms. finally make line search consistent among diﬀerent training algorithms normalize search direction resulting unit vector line search procedure. actually approximation doesn’t take account state given conﬁguration board current piece particular ‘recurrent state’ ignores current piece. empirically found approximation gave better results presumably reduced variance estimands reason believe unfairly biasing comparison various parametric policy search methods. repetitions experiment consisting training iterations mean standard error results given figure seen full gauss-newton method outperforms methods performance diagonal gauss-newton method comparable natural gradient ascent. obtained roughly training iterations. approximate dynamic programming based method previously applied tetris domain bertsekas ioﬀe features used score roughly completed lines obtained around training iterations solution deteriorated. recently modiﬁed policy iteration approach able obtain signiﬁcantly better performance game tetris completing approximately ﬁnal experiment consider robotic application. simulation environment provides physically realistic engine barrett wamtm robot arm. consider ball-in-a-cup domain challenging motor skill problem based traditional children’s game. domain small attached eﬀector robot arm. ball attached piece string. beginning task robot stationary ball hanging stationary position. task robot learn appropriate joint movements ﬁrst swing ball catch ball ball downward trajectory. domain episodic episode seconds length. state domain given angles velocities seven joints robot along cartesian coordinates ball. action given joint accelerations robot motor primitive framework domain applying separate motor primitive dimension action space. motor primitive consists parametrized curve models desired action sequence course episode. given collection motor primitives control engine within simulator tries follow desired action sequence closely possible whilst also satisfying constraints system physical constraints torques safely applied without damaging robot arm. kober peters dynamic motor primitives using shape parameters individual motor primitives. robot joints motor primitive parameters total. optimize parameters motor primitives considering induced motor primitive framework. action space corresponds space possible ﬁrst gauss-newton method coincides newton method mdp. policy block-wise log-concave jointly log-concave result construct block diagonal forms preconditioning matrices ﬁrst second gauss-newton methods separate block additionally since planning horizon length possible calculate fisher information exactly domain. steepest gradient ascent natural gradient ascent considered several diﬀerent step size sequences. sequence considered constant step size throughout sequences diﬀered size step size. considered step sizes length gauss-newton methods considered ﬁxed step size throughout training kober peters initial value trajectory robot mimics given human demonstration. diagonal elements precision matrix initialized training iteration sampled actions policy used episodes generated samples estimate search direction. deal number samples used samples last training iterations calculating search direction taking ‘eﬀective’ sample size finally used reward/ﬁtness shaping approach wierstra algorithms considered using shaping function wierstra experiment performed updates policy parameters. repeated experiment times results given figure unable successfully learn catch ball using either steepest gradient ascent natural gradient ascent ﬁrst gauss-newton method. reason results algorithms omitted. seen second gauss-newton method signiﬁcantly outperforms em-algorithm domain. runs experiment second gauss-newton method successfully able learn catch ball times. em-algorithm successfully learnt task times. log-policy quadratic ﬁxed step size used second gauss-newton method follows update approximate newton methods quasi-newton methods gauss-newton method standard optimization techniques. methods maintain beneﬁts newton’s method whilst alleviating shortcomings. paper considered approximate newton methods context policy optimization mdps. ﬁrst contribution paper provide novel analysis hessian objective function policy optimization. included providing novel form hessian well detailing positive/negative deﬁniteness properties certain terms hessian. furthermore shown policy parametrization suﬃciently rich remaining terms hessian vanish vicinity local optimum. motivated analysis introduced gauss-newton methods mdps. like gauss-newton method non-linear least squares methods involve approximating hessian ignoring certain terms hessian diﬃcult estimate. approximate hessians possess desirable properties negative deﬁniteness demonstrate several important performance guarantees including guaranteed ascent directions invariance aﬃne transformation parameter space convergence guarantees. also demonstrated second gauss-newton algorithm closely related emalgorithm natural gradient ascent applied mdps providing novel insights algorithms. compared proposed gauss-newton methods techniques policy search literature range challenging domains including tetris robotic application. found second gauss-newton method performed signiﬁcantly better methods domains considered. would like thank peter dayan david silver nicolas heess david barber helpful discussions work. would also like thank gerhard neumann christian daniel assistance robot experiment. work supported european community seventh framework programme grant agreement epsrc grant agreement ep/m/ theorem proof. theorem follows immediately lemma taking expectation w.r.t. start state distribution using deﬁnition discounted trajectory distribution. lemma suppose given markov decision process objective markovian trajectory distribution policy parametrization constant curvature respect action space given concatenation parameter vectors diﬀerent order show tabular policies value consistent start relating gradient gradient gradient taken respect policy parameters state policy parameters remaining states held ﬁxed. theorem proof. order obtain contradiction suppose stationary point means exists ∇w|w=w∗v suppose ∇w|w=w∗v ∇w|w=w∗v policy parametrization value used case consistent follows that methods. given iterative optimization method deﬁned mapping local convergence point determined spectral radius jacobian ∇w|w=w∗g. formalized welllemma suppose mapping ﬁxed-point furthermore fr´echet diﬀerentiable spectral radius satisﬁes point attraction furthermore convergence towards linear rate given λmin λmax respectively denoting minimal maximal eigenvalues a)−h. hence provided written terms spectral radius a)−h)). theorem follows convergence case follows manner. orem shown provided that −h)). negative-deﬁnite eigenvalues positive. furthermore lemma positive-deﬁnite follows eigenvalues range means suﬃcient ensure denote algorithm detail straightforward extension algorithm estimation approximate hessian framework. analogous algorithm estimation diagonal matrix follows similarly. algorithm make eligibility trace gradient approximate hessian denote respectively. estimates gradient approximate hessian denoted respectively. performed iteration large parameter systems becomes prohibitively costly. consider inversion preconditioning matrix proposed gaussnewton methods. algorithm recurrent state sampling algorithm estimate search direction second gauss-newton method. algorithm applicable markov decision processes inﬁnite planning horizon average rewards. firstly diagonal forms gauss-newton methods preconditioning matrix diagonal inversion matrix trivial scales linearly number parameters. general preconditioning matrix full gauss-newton methods form sparsity computational savings possible inverting preconditioning matrix. however source sparsity allows behaviour agent modeled ﬁnite state controller three functions optimized initial belief distribution belief transition dynamics policy. case dynamics system given observation ﬁnite observation space belief state ﬁnite belief space initial belief given initial belief distribution parameters optimized system seen system block-diagonal exhibit general matrix inversion required full gauss-newton methods scales cubically number policy parameters prohibitively expensive large parameter systems. possible however approximate search direction second gaussnewton method computational cost linear number policy parameters. focus form gauss-newton method remainder section. computational savings achieved application conjugate-gradient algorithm along hessian-free approximation matrix-vector product occurs within conjugate-gradient algorithm. conjugate-gradient algorithm iterative algorithm solving linear systems. algorithm maintains estimate solution linear system course algorithm. denote estimate iteration ﬁrst approximation propose given approximation search direction gauss-newton method. selected ascent direction property guaranteed when instance iteration conjugate-gradient algorithm scales quadratically used place gauss-newton method computational complexity scale therefore computational computational bottleneck iteration conjugate-gradient algorithm matrix-vector product scales quadratically size linear system. case gauss-newton method matrix-vector product iteration conjugate gradient algorithm takes form conjugate direction found conjugate-gradient algorithm. matrix-vector product equivalently viewed weighted summation matrix-vector products which state-action pair matrix-vector product πpk. perspective allows vector products thus itself. particular introducing scalar matrix-vector product iteration conjugate-gradients computational complexity linear dimension parameter space. using iterations conjugate-gradients approximate search direction results computational cost conjugate-gradient algorithm approximately solve linear system ﬁnite-diﬀerence approximation conjugate-gradient gauss-newton method. summary algorithm given algorithm conjugate-gradient algorithm ﬁnite-diﬀerence approximation based upon methods used hessian-free algorithms numerical optimization literature. case markov decision processes conjugate-gradient algorithm would used within hessian-free algorithm solve linear system given similarities conjugate-gradient gauss-newton method hessianfree methods worth noting important diﬀerences algorithms. firstly hessian necessarily negative-deﬁnite necessarily case conjugate-gradient algorithm able solve linear system longer initialization conjugate-gradient algorithm additionally comparing ﬁnite-diﬀerence approximation ﬁnite-diﬀerence approximation seen rightmost term discounted occupancy distribution state-action value function depend current policy parameters calculated exactly large-scale mdps interest instead must estimated. standard application hessian-free method therefore would necessary re-estimate quantities iteration conjugate-gradient algorithm. contrast conjugate-gradient gauss-newton method estimate state-action value function discounted occupancy marginals used iterations conjugate-gradient algorithm. policy gradient algorithms estimating terms typically forms expensive part overall algorithm means iteration conjugate-gradient gauss-newton method computationally eﬃcient hessian-free methods. also means appear approximation cost gradient evaluations typically cheaper practice. furthermore means additional level approximation hessian-free methods present conjugate-gradient gauss-newton method. additionally considering fisher information matrix takes form approach presented section could also applied natural gradient framework.", "year": 2015}