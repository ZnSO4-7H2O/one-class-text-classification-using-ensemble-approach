{"title": "Deep Unsupervised Clustering with Gaussian Mixture Variational  Autoencoders", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.", "text": "dilokthanakul∗ pedro mediano marta garnelo matthew hugh salimbeni arulkumaran murray shanahan department computing department bioengineering imperial college london london ∗n.dilokthanakulimperial.ac.uk study variant variational autoencoder model gaussian mixture prior distribution goal performing unsupervised clustering deep generative models. observe known problem over-regularisation shown arise regular vaes also manifests model leads cluster degeneracy. show heuristic called minimum information constraint shown mitigate effect vaes also applied improve unsupervised clustering performance model. furthermore analyse effect heuristic provide intuition various processes help visualizations. finally demonstrate performance model synthetic data mnist svhn showing obtained clusters distinct interpretable result achieving competitive performance unsupervised clustering state-of-the-art results. unsupervised clustering remains fundamental challenge machine learning research. longestablished methods k-means gaussian mixture models still core numerous applications similarity measures limited local relations data space thus unable capture hidden hierarchical dependencies latent spaces. alternatively deep generative models encode rich latent structures. often applied directly unsupervised clustering problems used dimensionality reduction classical clustering techniques applied resulting low-dimensional space unsatisfactory approach assumptions underlying dimensionality reduction techniques generally independent assumptions clustering techniques. deep generative models estimate density observed data assumptions latent structure i.e. hidden causes. allow reason data complex ways models trained purely supervised learning. however inference models complicated latent structures difﬁcult. recent breakthroughs approximate inference provided tools constructing tractable inference algorithms. result combining differentiable models variational inference possible scale inference datasets sizes would possible earlier inference methods popular algorithm framework variational autoencoder paper propose algorithm perform unsupervised clustering within framework. postulate generative models tuned unsupervised clustering making assumption observed data generated multimodal prior distribution correspondingly construct inference model directly optimised using reparameterization trick. also show problem over-regularisation vaes severely effect performance clustering mitigated minimum information constraint introduced kingma unsupervised clustering considered subset problem disentangling latent variables aims structure latent space unsupervised manner. recent efforts moved towards training models disentangled latent variables corresponding different factors variation data. inspired learning pressure ventral visual stream higgins able extract disentangled features images adding regularisation coefﬁcient lower bound vae. vaes also effort going obtaining disentangled features generative adversarial networks recently achieved infogans structured latent variables included part noise vector mutual information latent variables generator distribution maximised mini-max game networks. similarly tagger combines iterative amortized grouping ladder networks aims perceptually group objects images iteratively denoising inputs assigning parts reconstruction different groups. johnson introduced combine amortized inference stochastic variational inference algorithm called structured vaes. structured vaes capable training deep models prior distribution. introduced multimodal prior optimize variational approximation standard variational objective showing performance video prediction task. work closely related stacked generative semi-supervised model kingma main differences fact prior distribution neural network transformation continuous discrete variables gaussian categorical priors respectively. prior model hand neural network transformation gaussian variables parametrise means variances mixture gaussians categorical variables mixture components. crucially kingma apply model semi-supervised classiﬁcation tasks whereas focus unsupervised clustering. therefore inference algorithm speciﬁc latter. compare results several orthogonal state-of-the-art techniques unsupervised clustering deep generative models deep embedded clustering adversarial autoencoders categorial gans vaes result combining variational bayesian methods ﬂexibility scalability provided neural networks using variational inference possible turn intractable inference problems optimisation problems thus expand available tools inference include optimisation techniques well. despite this limitation classical variational inference need likelihood prior conjugate order problems tractably optimised turn limit applicability algorithms. variational autoencoders introduce neural networks output conditional posterior thus allow variational inference objective tractably optimised stochastic gradient descent standard backpropagation. technique known reparametrisation trick proposed enable backpropagation continuous stochastic variables. normal circumstances backpropagation stochastic variables would possible without monte carlo methods bypassed constructing latent variables combination deterministic function separate source noise. refer reader kingma welling details. regular vaes prior latent variables commonly isotropic gaussian. choice prior causes dimension multivariate gaussian pushed towards learning separate continuous factor variation data result learned representations structured disentangled. allows interpretable latent variables gaussian prior limited learnt representation unimodal allow complex representations. result numerous extensions developed complicated latent representations learned specifying increasingly complex priors paper choose mixture gaussians prior intuitive extension unimodal gaussian prior. assume observed data generated mixture gaussians inferring class data point equivalent inferring mode latent distribution data point generated from. gives possibility segregate latent space distinct classes inference model non-trivial. well known reparametrisation trick generally used vaes cannot directly applied discrete variables. several possibilities estimating gradient discrete variables proposed graves also suggested algorithm backpropagation gmms. instead show adjusting architecture standard estimator variational lower bound gaussian mixture variational autoencoder optimised standard backpropagation reparametrisation trick thus keeping inference model simple. predeﬁned number components mixture µµµzk given neural networks parameters respectively. observed sample generated neural network observation model parametrised continuous latent variable xxx. furthermore distribution xxx|www gaussian mixture means variances speciﬁed another neural network model parametrised input www. speciﬁcally neural network parameterised outputs means µµµzk given input. one-hot vector sampled mixing probability variances chooses component gaussian mixture. parameter make uniformly distributed. generative variational views model depicted fig. assume mean-ﬁeld variational family proxy posterior factorises indexes data points. simplify notation drop consider data point time. parametrise variational factor recognition networks output parameters variational distributions specify form gaussian posteriors. derived z-posterior conditional prior term reconstruction term estimated drawing monte carlo samples gradient backpropagated standard reparameterisation trick w-prior term calculated analytically. importantly constructing model conditional prior term estimated using eqn. without need sample discrete distribution since computed forward pass expectation calculated straightforward manner backpropagated usual. expectation estimated monte carlo samples gradients backpropagated reparameterisation trick. method calculating expectation similar marginalisation approach kingma subtle difference. kingma need multiple forward passes obtain component z-posterior. method requires wider output layers neural network parameterised need forward pass. methods scale linearly number clusters. unusual term elbo z-prior term. z-posterior calculates clustering assignment probability directly value asking cluster positions generated therefore z-prior term reduce divergence z-posterior uniform prior concurrently manipulating position clusters encoded point intuitively would merge clusters maximising overlap them moving means closer together. term similar klregularisation terms tension reconstruction term expected over-powered amount training data increases. possible overpowering effect regularisation term training described numerous times literature result strong inﬂuence prior obtained latent representations often overly simpliﬁed poorly represent underlying structure data. main approaches overcome effect solution anneal term training allowing reconstruction term train autoencoder network slowly incorporating regularization term main approach involves modifying objective function setting cut-off value removes effect term certain threshold show experimental section below problem over-regularisation also prevalent assignment gmvae clusters manifests large degenerate clusters. show second approach suggested kingma indeed alleviate merging phenomenon ﬁnding solutions over-regularization problem remains challenging open problem. main objective experiments evaluate accuracy proposed model also understand optimisation dynamics involved construction meaningful differentiated latent representations data. section divided three parts ﬁrst study inference process low-dimensional synthetic dataset focus particular over-regularisation problem affects clustering performance gmvae alleviate problem; evaluate model mnist unsupervised clustering task; ﬁnally show generated images model conditioned different values latent variables illustrate gmvae learn disentangled interpretable latent representations. mnist standard handwritten digits dataset composed grayscale images consisting training samples testing samples svhn collection images house numbers cropped version standard extra training sets adding total approximately images. quantify clustering performance plotting magnitude z-prior term described eqn. training. quantity thought measure much different clusters overlap. since goal achieve meaningful clustering latent space would expect quantity model learns separate clusters. empirically however found case. latent representations model converges merges classes large cluster instead representing information different clusters seen figs. result data point equally likely belong clusters rendering latent representations completely uninformative respect class structure. argue phenomenon interpreted result over-regularisation z-prior term. given quantity driven optimisation term lower bound reaches maximum possible value zero opposed decreasing training ensure encoding information classes. suspect prior strong inﬂuence initial training phase drives model parameters poor local optimum hard driven reconstruction term later observation conceptually similar over-regularisation problem encountered regular vaes thus hypothesize applying similar heuristics help alleviate problem. show fig. using previously mentioned modiﬁcation lower-bound proposed kingma avoid over-regularisation caused z-prior. achieved maintaining cost z-prior constant value exceeds threshold. formally modiﬁed z-prior term written modiﬁcation suppresses initial effect z-prior merge clusters thus allowing spread cost z-prior cost high enough. point effect signiﬁcantly reduced mostly limited merging individual clusters overlapping sufﬁciently. seen clearly figs. former shows clusters z-prior cost taken consideration clusters able spread out. z-prior activated clusters close together merged seen fig. finally order illustrate beneﬁts using neural networks transformation distributions compare density observed model regular data space. illustrated ﬁgures gmvae allows much richer thus accurate representations regular gmms therefore successful modelling nongaussian data. figure visualisation synthetic dataset data distributed modes dimensional data space. gmvae learns density model model data using mixture non-gaussian distributions data space. cannot represent data well restrictive gaussian assumption. gmvae however suffers over-regularisation result poor minima looking latent space. using modiﬁcation elbo allows clusters spread out. model converges z-prior term activated regularises clusters ﬁnal stage merging excessive clusters. figure plot z-prior term without information constraint gmvae suffers overregularisation converges poor optimum merges clusters together avoid cost. reaching threshold value gradient z-prior term turned avoid clusters pulled together time threshold value reached clusters sufﬁciently separated. point activated gradient z-prior term merges overlapping clusters together. even activating gradient value z-prior continues decrease over-powered terms lead meaningful clusters better optimum. assess model’s ability represent discrete information present data image clustering task. train gmvae mnist training dataset evaluate clustering performance test dataset. compare cluster assignments given gmvae true image labels follow evaluation protocol makhzani summarise clarity. method element test highest probability belonging cluster assign label test samples belonging repeated clusters assigned labels compared true labels obtain unsupervised classiﬁcation error rate. observe cluster degeneracy problem training gmvae synthetic dataset problem arise mnist dataset. thus optimise gmvae using elbo directly without need modiﬁcations. summary results obtained mnist benchmark gmvae well recent methods shown table achieve classiﬁcation scores competitive state-of-the-art techniques except adversarial autoencoders suspect reason again related terms vae’s objective. indicated hoffman difference adversarial autoencoders objective replacement term elbo adversarial loss allows latent space manipulated carefully details network architecture used experiments found appendix empirically observe increasing number monte carlo samples number clusters makes gmvae robust initialisation stable shown fig. fewer samples clusters used gmvae occasionally converge faster poor local minima missing modes data distribution. initial submission published blog post analysis gaussian mixture vaes. addition providing insightful comparisons aforementioned algorithm implements version achieves competitive clustering scores using comparably simple network architecture. crucially shows model discrete latent variables trained without labels. reason problem severe gmvae might possibly restrictive assumptions generative process helps optimisation argued blog. figure clustering accuracy different numbers clusters monte carlo samples epochs gmvae converges solution. increasing number clusters improves quality solution considerably. argued gmvae picks natural clusters dataset clusters share structure actual classes images. train gmvae mnist show learnt components distribution latent space actually represent meaningful properties data. first note sources stochasticity play sampling gmvae namely fig. explore latter option setting sampling multiple times resulting gaussian mixture. fig. corresponds samples different component gaussian mixture clearly seen samples component consistently result images class digit. conﬁrms learned latent representation contains well differentiated clusters exactly digit. additionally fig. explore sensitivity generated image gaussian mixture components smoothly varying sampling component. reliably controls class generated image sets style digit. finally fig. show images sampled gmvae trained svhn showing gmvae clusters visually similar images together. figure generated mnist samples contains randomly generated samples different gaussian components gaussian mixture. gmvae learns meaningful generative model discrete latent variables correspond directly digit values unsupervised manner. samples generated traversing around space position correspond speciﬁc style digit. figure generated svhn samples corresponds samples generated randomly different gaussian components. gmvae groups together images visually similar. formulate variational bayes optimisation objective. discuss problem overregularisation vaes. context model show problem manifests form cluster degeneracy. crucially show speciﬁc manifestation problem solved standard heuristics. evaluate model unsupervised clustering tasks using popular datasets achieving competitive results compared current state art. finally show sampling generative model learned clusters latent representation correspond meaningful features visible data. images generated cluster latent space share relevant high-level features trained entirely unsupervised manner. worth noting gmvaes stacked allowing prior gaussian mixture distribution well. deep gmvae could scale much better number clusters given would combinatorial regards number layers number clusters layer. such future research deep gmvaes hierarchical clustering possibility crucial also address enduring optimisation challenges associated vaes order would like acknowledge nvidia corporation donation geforce titan used experiments. would like thank jason rolfe reviewers useful comments. importantly would also like acknowledge variational family used throughout version paper suggested anonymous reviewer. chen diederik kingma salimans duan prafulla dhariwal john schulman ilya sutskever pieter abbeel. variational lossy autoencoder. arxiv preprint arxiv. eslami nicolas heess theophane weber yuval tassa koray kavukcuoglu geoffrey hinton. attend infer repeat fast scene understanding generative models. arxiv preprint arxiv. goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems klaus greff antti rasmus mathias berglund tele hotloo j¨urgen schmidhuber harri valpola. tagger deep unsupervised perceptual grouping. arxiv preprint arxiv. karol gregor danihelka alex graves danilo rezende daan wierstra. draw recurrent neural network image generation. proceedings international conference machine learning matthew hoffman matthew johnson. elbo surgery another carve variational evidence lower bound. workshop advances approximate bayesian inference nips matthew johnson david duvenaud alexander wiltschko sandeep datta ryan adams. composing graphical models neural networks structured representations fast inference. arxiv preprint arxiv. diederik kingma shakir mohamed danilo jimenez rezende welling. semi-supervised learning deep generative models. advances neural information processing systems casper kaae sønderby tapani raiko lars maaløe søren kaae sønderby winther. train deep variational autoencoders probabilistic ladder networks. arxiv preprint arxiv. table neural network architecture models hidden layers shared except output layer neural network split output streams dimension dimension exponentiate variance components keep value positive. asterisk indicates batch normalization relu nonlinearity. convolutional layers numbers parentheses indicate stride-padding. table neural network architecture models network outputs gaussian parameters synthetic dataset bernoulli parameters mnist svhn logistic function keep value bernoulli parameters asterisk indicates batch normalization relu nonlinearity. convolutional layers numbers parentheses indicate stride-padding.", "year": 2016}