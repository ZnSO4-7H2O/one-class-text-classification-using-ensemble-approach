{"title": "Incremental Model-based Learners With Formal Learning-Time Guarantees", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Model-based learning algorithms have been shown to use experience efficiently when learning to solve Markov Decision Processes (MDPs) with finite state and action spaces. However, their high computational cost due to repeatedly solving an internal model inhibits their use in large-scale problems. We propose a method based on real-time dynamic programming (RTDP) to speed up two model-based algorithms, RMAX and MBIE (model-based interval estimation), resulting in computationally much faster algorithms with little loss compared to existing bounds. Specifically, our two new learning algorithms, RTDP-RMAX and RTDP-IE, have considerably smaller computational demands than RMAX and MBIE. We develop a general theoretical framework that allows us to prove that both are efficient learners in a PAC (probably approximately correct) sense. We also present an experimental evaluation of these new algorithms that helps quantify the tradeoff between computational and experience demands.", "text": "model-based learning algorithms shown experience eﬃciently learning solve markov decision processes ﬁnite state action spaces. however high computational cost repeatedly solving internal model inhibits large-scale problems. propose method based real-time dynamic programming speed model-based algorithms rmax mbie resulting computationally much faster algorithms little loss compared existing bounds. speciﬁcally learning algorithms rtdp-rmax rtdp-ie considerably smaller computational demands rmax mbie. develop general theoretical framework allows prove eﬃcient learners sense. also present experimental evaluation algorithms helps quantify tradeoﬀ computational experience demands. paper deals important problem learning markov decision process ﬁnite number states actions. problem fundamental reinforcement learning requires learning agent handle exploration/exploitation dilemma conjunction temporal credit assignment problem. exploration/exploitation dilemma mean conﬂict involved choosing behavior intended gain information meant maximize reward utilizing current knowledge sequential decision making problem involving temporal credit assignment agent’s actions eﬀect environment determine actions rewards available future. agents learn model unknown environment called model-based learners. build previous work area introduce modelbased learning algorithms rtdp-rmax rtdpie. algorithms distinguish previous model-based algorithms avoid completely solving model. modiﬁcation alleviates great computational burden resulting algorithms improved per-action computational complexities. main theoretical contribution showing algorithms still possess polynomial bounds amount mistakes make learning. section introduces markov decision process notation used throughout paper; sutton barto introduction. ﬁnte tuple ﬁnite state space ﬁnite action space transition function reward function discount factor summed sequence rewards. also denote number states number actions respectively. state action agent receives random reward expectation transported state probability policy strategy choosing actions. deterministic policies dealt paper. stationary policy produces action based current state. assume rewards between policy denote discounted inﬁnite-horizon value function state positive denote -step value function teger γj−rj] policy speciﬁcally γj−rj] reward sequence generated following policy state expectations taken possible inﬁnite paths agent might follow. optimal policy denoted value functions note policy cannot value greater setting assume learner receives input. reinforcement-learning problem deﬁned follows. agent always occupies single state learning algorithm told state must compute action agent receives reward transported another state according reward transition functions section procedure repeats forever. ﬁrst state occupied agent chosen arbitrarily. deﬁne timestep single interaction environment described above. evaluating algorithms three essential traits consider. space complexity computational complexity sample complexity. space complexity measures amount memory required implement algorithm computational complexity measures amount operations needed execute algorithm step experience. sample complexity measures number timesteps algorithm behave near optimally words amount experience takes learn behave well. algorithm pac-mdp sample complexity bounded polynomial environment size approximation parameters high probability. algorithms including rmax mbie typically build internal model environment solved near-optimal policy. section present algorithms based well-known model-based algorithms rmax mbie. algorithms like based idea using internal model guide behavior. however main goal avoid computational burden repeatedly solving model preserving beneﬁts fast learning keeping model. main idea summed applying real-time dynamic programming algorithm agent’s internal model. approach similar adaptive-rtdp algorithm barto except sophisticated exploration techniques employed. ﬁrst algorithm called rtdp-rmax uses model similar rmax algorithm. treats exploration problem loosely identiﬁed approach taken na¨ıve algorithm k-armed bandit problem essentially state-actions must tried certain ﬁxed number times statistics gathered incorporated agent’s model. happens state-action considered maximally rewarding model. second algorithm called rtdp-ie uses model similar mbie algorithm action-elimination algorithm treats exploration problem similarly interval estimation algorithm k-armed bandit problem intuition algorithm action chosen maximizes upper tail conﬁdence interval computed action values maintained algorithm. compared na¨ıve algorithm strategy exhibit focused behavior faster learning experience incorporated much faster agent’s model. algorithms consider maintain action-value estimates state-action pair time denote algorithm’s current action-value estimate denote maxa∈a learner always acts greedily respect estimates meaning state reached argmaxa∈a next action chosen. additionally algorithm makes optimistic initialization main diﬀerence algorithms action values updated timestep. algorithms number times experienced including timestep addition standard inputs rtdp-rmax algorithm requires additional positive integer parameter later analysis section provide formal procedure choosing consider free parameter controls purpose concrete deﬁnition deﬁne rmax algorithm maintains model rtdp-rmax always chooses actions according optimal policy model. similarly deﬁne mbie algorithm maintains model rtdp-ie acts according optimal policy model. speciﬁcally rmax mbie action values equal agent’s model time deﬁnition diﬀers slightly original development rmax brafman tennenholtz signiﬁcantly mbie presented strehl littman however diﬀerences important terms analysis learning bounds apply versions. also likely diﬀerent experimentally. however completely fair experiments used form mbie algorithm appears strehl littman adaptive-rtdp algorithm simply rtdprmax algorithm rtdp algorithm characterized identical adaptive-rtdp algorithm except true used model rather empirical model. algorithms introduce similar. provide incentive explore unknown parts environment increasing action-value estimate used algorithm state-action pairs tried often past. incentive encourages exploration agent chooses actions maximum current action value updates action values state respect action values reachable next-states chain updates allows exploration bonuses propagate action value another hence encourages directed exploration. summarize rtdp-rmax algorithm chooses step either update single state-action pair not. last state occupied agent last action chosen agent experienced least times action-value estimate state-action pair updated. update standard full bellman backup value iteration empirical transition probabilities empirical reward functions used approach diﬀers rmax step value iteration taken state instead running completion. empirical reward transition functions computed using history agent time update occurs. like rtdp-rmax update standard full bellman backup empirical transition probabilities reward functions used plus exploration bonus proportional decreases state-action pair tried least times. method works well drawback ignoring statistics collected ﬁrst tries state-action pair. would expect intelligent algorithm take advantage experience quickly. second algorithm rtdp-ie accomplishes objective providing bonus state-action pair decreases number experiences state-action pair. update allows experience useful immediately still recognizes state-action pairs little experience need continue explored. intuitions precisely intuitions used rmax algorithm mbie algorithm important diﬀerence rmax mbie work completely solving internal model computation incurs worst-case per-step computational cost highly detrimental domains large number states actions. hand rtdp-rmax rtdp-ie require single bellman-style backup resulting perstep computational complexity great interest tremendous reduction computational eﬀort aﬀects performance algorithms. question diﬃcult answer completely. provide theoretical analysis empirical evaluation sheds light issue. first show possible prove bounds number sub-optimal choices made rtdp-rmax rtdp-ie algorithms larger constant times best bounds known rmax mbie logarithmic factors ignored. second evaluate performance four algorithms experimentally diﬀerent mdps. empirical results indicate performance algorithms terms learning comparable old. also expected diﬀerent algorithms exhibit diﬀerent tradeoﬀs computational sample complexity. experiments show sometimes case beneﬁt using rtdp-ie rtdp-rmax small. logarithmic dependence number actions achieved using priority queue access update action values. performance bounds consider. then develop general framework proving performance bounds algorithms. framework allows avoid repetition analysis algorithms. also provides methods applied algorithms discovered future. finally apply techniques rtdp-rmax rtdp-ie show eﬃcient learners mdps. formalize notion eﬃcient learning allow learning algorithm receive additional inputs positive real numbers. ﬁrst parameter controls quality behavior require algorithm second parameter must less measure conﬁdence parameters decrease greater exploration necessary expected algorithms. much discussion community deﬁnes eﬃcient learning algorithm deﬁne sample complexity. ﬁxed kakade deﬁnes sample complexity exploration algorithm number timesteps non-stationary policy time \u0001-optimal current state time ∗−\u0001). believe deﬁnition captures essence measuring learning. algorithm said eﬃcient pac-mdp algorithm per-step computational complexity sample complexity less polynomial relevant quantities probability least simply pac-mdp relax deﬁnition computational complexity requirement. terminology borrowed valiant classic paper dealing classiﬁcation. deﬁnition penalizes learner executing non-\u0001-optimal policy rather nonoptimal policy. keep mind that ﬁnite amount experience algorithm identify optimal policy complete conﬁdence. addition noise algorithm misled underlying dynamics system. thus failure probability allowed. kakade full motivation performance measure. analysis rmax kakade mbie completely deﬁned note learning algorithms consider take input. denote version algorithm parameterized proof proposition follows structure work kakade generalizes several steps. proposition greedy learning algorithm every timestep exists state-action pairs. assume unless timestep update action value occurs event happens. known state-action current greedy policy states argmaxa suppose inputs probability least following conditions hold states actions timesteps total number updates action-value estimates plus number times escape event occur bounded then executed follow \u0001-optimal policy current state proof sketch suppose executed history agent timestep state reached. denote current policy agent. state policy denote event that executing policy state timesteps following events occur algorithm performs successful update state-action pair state-action pair experienced theory focused algorithms maintain table action values stateaction pair time also assume algorithm always chooses actions greedily respect action values. constraint really restriction since could deﬁne algorithm’s action values action chooses actions. however general framework understood developed easily assumptions. deﬁnition suppose algorithm maintains value denoted state-action pair denote estimate immediately action agent. greedy algorithm action argmaxa∈a state reached agent. following deﬁnition useful analysis. deﬁnition given action values state-action pair state-action pairs deﬁne known state-action follows. additional state added state space actions agent returned probability reward taking action known state-action generalization standard notions known state kearns singh kakade whose dynamics equal true dynamics subset state-action pairs state-action pairs value taking state-action pairs equal current action-value estimates intuitively view state-action pairs agent suﬃciently accurate estimates dynamics. expect modiﬁcations typically little eﬀect behavior performance algorithm since bound number times model change learning simplify analysis. condition update aﬀect updates would resulted minimal change action-value estimates. suﬃciently large reﬁnements model also minor eﬀect. note ˆmkt well-deﬁned known-state respect agent’s model time viewed approximation mkt. novel deﬁnition extends standard definition associates state-action pairs tried times allow incremental updates propagate value information gradually. following condition needed proof rtdp-rmax pac-mdp. provide suﬃcient condition guarantee holds. words ﬁrst part condition says value greedy policy empirical known stateaction \u0001-close value true known state-action second part says optimal value function last ﬁnal model learned rtdp-rmax would correct transitions rewards used state-actions tried least times. ﬁrst step follows fact following results behavior identical following long action-value updates performed state-action pairs experienced. second step follows deﬁnition above. third ﬁnal steps follow preconditions respectively proposition. hold probability least suppose then agent’s policy timestep \u0001-optimal otherwise application hoeﬀding bound union bound precondition proposition latter case cannot occur timesteps probability least modiﬁcation original algorithms follows. allow update equation take place action value results decrease least words following equation must satisﬁed update occur otherwise change made addition empirical transitions rewards respectively computed using ﬁrst experiences additional experiences discarded aﬀect model. second modiﬁcation eﬀect single empirical reward transition function learned agent; continue adjust model time. hence simpler notation also denote empirical mdp. many samples empirical model close enough true dynamics argument boils showing θ)accurate rewards transitions lead \u0001-optimal policies θ/α) samples needed ensure α-accurate rewards transitions high probability. additional factors application union bound result hold state-action pairs. open question whether current bounds rmax improved terms dependence thesis kakade provides sufﬁcient condition parameter must satisfy pac-mdp proof rmax through. condition assumption stronger similar spirit condition version rtdp-ie analyze update performed speciﬁed equation would result decrease least addition empirical transitions rewards respectively computed using ﬁrst experiences furthermore rithm note update equation identical update used value iteration well known chapter bertsekas tsitsiklis given optimistic initialization sequence updates cannot drive action values optimal q∗-values assumption optimal q∗-values less optimal q∗-values true minus yields result. proof apply proposition first note deﬁnition algorithm performs updates event occurs current state-action pair tried least times. hence suﬃcient bound number times occurs. state-action pair tried times every additional update decreases action value least since action value initialized state-action pair updated times total sa/) timesteps occur. proposition optimism precondition satisﬁed. finally claim that assumption always holds. verify claim note solution following equations vector solution similar equations except additional positive reward terms bounded follows combining fact assumption yields vt−v thus letting satisfy vt−v desired proposition ignoring factors analysis leads total sample complexity bound better quantify relationship them performed sets experiments versions four algorithms ﬁrst version severely restricted model size speciﬁcally model limited size rmax rtdprmax size mbie rtdp-ie state-action pair. second version allowed model grow size numbers chosen somewhat arbitrarily small big. experimented range various parameter settings optimized parameter setting gathered reward fewest timesteps. experiment recorded number timesteps bellman backups required agent achieve ﬁxed equally spaced levels cumulative reward. implemented various optimizations algorithm avoid unnecessary bellman backup computations. experiments designed supplement theoretical analysis section meant thorough evaluation algorithms. allowing model size change independently algorithm’s internal parameters required slight change algorithm cases. example rmax agent’s model change state-action pair already known. case agent must solve model time changes. thus allowing model grow increase computational complexity also generally decrease sample complexity. reachable every state random hamiltonian circuits constructed action probability assigned corresponding transitions. then state-action pair transition function completed assigning random probabilities randomly selected next states. therefore state-action pair next-states. mean reward randomly selected mean proportional index experiment repeated times results averaged figure provides plot runs algorithms whose models restricted samples since curves rmax mbie algorithms alike omitted mbie graph contains data point every units cumulative reward graph found mean rewards chosen uniformly random optimal policy almost always picks action maximizes immediate reward interesting sense sequential decision making. mbie/rmax able obtain cumulative reward timestep incremental algorithms bottom graph shows achievement came signiﬁcantly larger computational cost. second experiments consisted similar k-armed bandit problem noise arms modeled transition function speciﬁcally states start state actions. taking action state results transition state probability transition back state probability state action agent transitioned state choosing action state results reward dynamics created better choose action lowest payoﬀ probability recap state action behaves like pulling one-armed bandit. pays agent transitioned away state another state agent free choose action obtain non-zero reward. experiment repeated times results averaged. shown provably eﬃcient model-based reinforcement learning achieved without computational burden completely solving internal model step. developed algorithms rtdp-rmax rtdp-ie analyzed sample complexity general mdps.", "year": 2012}