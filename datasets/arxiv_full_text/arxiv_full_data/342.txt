{"title": "Interpretable Explanations of Black Boxes by Meaningful Perturbation", "tag": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "abstract": "As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks \"look\" in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.", "text": "answered providing interpretable rules describe input-output relationship captured example rule could rotation invariant sense whenever images related rotation. paper make several contributions. first propose general framework explanations metapredictors extending work. second identify several pitfalls designing automatic explanation systems. show particular neural network artifacts major attractor explanations. artifacts informative since explain part network behavior characterizing properties network requires careful calibration generality interpretability explanations. third reinterpret network saliency framework. show provides natural generalization gradient-based saliency technique integrating information several rounds backpropagation order learn explanation. also compare technique methods terms meaning obtained results. work builds gradient-based method backpropagates gradient class label image layer. backpropagation methods include deconvnet guided backprop builds deconvnet gradient method produce sharper visualizations. machine learning algorithms increasingly applied high impact high risk tasks medical diagnosis autonomous driving critical researchers explain algorithms arrived predictions. recent years number image saliency methods developed summarize highly complex neural networks look image evidence predictions. however techniques limited heuristic nature architectural constraints. paper make main contributions first propose general framework learning different kinds explanations black algorithm. second specialise framework part image responsible classiﬁer decision. unlike previous works method model-agnostic testable grounded explicit interpretable image perturbations. given powerful often opaque nature modern black predictors deep neural networks considerable interest explaining understanding predictors a-posteriori learned. remains largely open problem. reason lack formal understanding means explain classiﬁer. existing approaches etc. often produce intuitive visualizations; however since visualizations primarily heuristic meaning remains unclear. paper revisit concept explanation formal level goal developing principles methods explain black function e.g. neural network object classiﬁer. since function learned automatically data would like understand learned answering what question means determining properties map. question investigates internal mechanisms allow achieve properties. focus mainly what question argue explanation rule predicts response black certain inputs. example explain behavior robin classiﬁer rule subset robin images. since imperfect rule applies approximately. measure faithfulness explanation expected prediction error indicator function event note implicitly requires distribution possible images note also simply expected prediction error classiﬁer. unless know trained robin classiﬁer insightful interpretable since explanations also make relative statements black outcomes. example black could rotation invariant ∼rot ∼rot means related rotation. like before measure faithfulness explanation rule interpretable relation ∼rot learning explanations. signiﬁcant advantage formulating explanations meta predictors faithfulness measured prediction accuracy. furthermore machine learning algorithms used discover explanations automatically ﬁnding explanatory rules apply certain classiﬁer large pool possible rules particular ﬁnding accurate explanation similar traditional learning problem formulated computationally regularized empirical risk minimization here regularizer goals allow explanation generalize beyond samples considered optimization pick explanation simple thus hopefully interpretable. relaxed generalization grad-cam visualize linear combination late layer’s activations class-speciﬁc weights layer-wise relevance propagation excitation backprop backpropagate class-speciﬁc error signal though network multiplying convolutional layer’s activations. exception gradient method techniques introduce different backpropagation heuristics results aesthetically pleasing heuristic notions image saliency. also model-agnostic limited neural networks many requiring architectural modiﬁcations and/or access intermediate layers techniques examine relationship inputs outputs editing input image observing effect output. include greedily graying segments image misclassiﬁed visualizing classiﬁcation score drop image occluded ﬁxed regions however techniques limited approximate nature; introduce differentiable method allows effect joint inclusion/exclusion different image regions considered. research also builds work idea explanations predictors inspired work generalize types explanations classiﬁcation invariance. local intepretable model-agnostic explanation framework relevant local explanation paradigm saliency method function’s output respect inputs neighborhood around input generated perturbing image. however method takes much longer converge produces coarse heatmap deﬁned ﬁxed super-pixels. similar paradigm aims learn image perturbation mask minimizes class score feedback networks learn gating masks every relu network maximize class score. however masks plainly interpretable directly edit image relu gates directly used visual explanation; furthermore method requires architectural modiﬁcation yield different results different networks model-agnostic. explaining black boxes meta-learning black input space output space typically obtained opaque learning process. make discussion concrete consider input color images discrete domain. output boolean telling whether image contains object certain type maximally informative explanations. simplicity interpretability often sufﬁcient good explanations must paired informativeness. consider following variant rule means rotation invariance condition probability independently sampling rotated zero that without conditioning would true probability figure comparison saliency methods. left right original image ground truth bounding learned mask subtracted gradient-based saliency guided backprop contrastive excitation backprop grad-cam occlusion figure gradient saliency maps bounding highlight object meant recognized image. note strong response apparently non-relevant image regions. related rotation angle explanations larger angles imply ones smaller ones trivially satisﬁed. regularizer used select maximal angle thus explanation informative possible. formulation provides interpretation saliency maps visualize gradient indication salient image regions. argue large values gradient identify pixels strongly affect network output. however issue interpretation breaks linear classiﬁer independent image hence cannot interpreted saliency. reason failure studies variation arbitrary displacements linear classiﬁer change regardless starting point non-linear black neural network problem reduced eliminated explain saliency rather diffuse strong responses even obvious information found image argue meaning explanations depends large part meaning varying input black box. example explanations sec. based letting vary image category rotation. saliency interested ﬁnding image regions impact output. thus natural consider perturbations obtained deleting subregions model deletion multiplying point-wise mask naively strict invariance implies invariance arbitrary rotations small rotations compose larger ones. however formulation still used describe rotation insensitivity ∼θ’s meaning changed indicate rotation w.r.t. canonical upright direction certain object classes etc. amounts studying function taylor expansion m)/dm|m= linear classiﬁer results saliency large pixels large simultaneously. reﬁne idea non-linear classiﬁers next section. order deﬁne explanatory rule black must start specifying variations input used study saliency identify regions image used black produce output value observing value changes obtained deleting different regions example denotes robin image expect well unless choice deletes robin image. given perturbation local explanation expect explanation characterize relationship conceptually simple several problems idea. ﬁrst specify means delete information. discussed detail sec. generally interested simulating naturalistic plausible imaging effect leading meaningful perturbations hence explanations. since access image generation process consider three obvious proxies replacing region constant value injecting noise blurring image formally mask associating pixel scalar value perturbation operator deﬁned figure left right image correctly classiﬁed large conﬁdence googlenet perturbed image recognized correctly anymore; deletion mask learned artifacts. mask learned minimizing predicted classes jointly applying constant random noise blur perturbations. note mask learns highly structured swirls along bottom minimizing-top mask learned applying constant perturbation. notice mask learns introduce sharp unnatural artifacts instead deleting pole although existence characterization artifacts interesting problem wish characterize behavior black boxes normal operating conditions. unfortunately illustrated objectives strongly attracted artifacts naively learn subtly-structured deletion masks trigger them. particularly true noise constant perturbations easily blur create artifacts using sharp color contrasts suggests approaches avoid artifacts generating explanations. ﬁrst powerful explanations should like predictor generalize much possible. deletion game means relying details singly-learned mask hence reformulate problem apply mask stochastically small random jitter. second argue masks co-adapted network artifacts representative natural perturbations. noted before meaning explanation depends meaning changes applied input obtain mask representative natural perturbations encourage simple regular structure canco-adapted artifacts. regularizing total-variation norm upsampling resolution version. given image goal summarize compactly effect deleting image regions order explain behavior black box. approach problem deletion regions maximally informative. order simplify discussion rest paper consider black boxes generate vector scores different hypotheses content image then consider deletion game goal smallest deletion mask causes score drop signiﬁcantly target class. finding formulated following learning problem also play symmetric preservation game goal smallest subset image must retained preserve score argminm fc). main difference deletion game removes enough evidence prevent network recognizing object image whereas preservation game ﬁnds minimal subset sufﬁcient evidence. iterated gradients. optimization problems solved using local search means gradient descent methods. manner method extracts information black computing gradient similar approach however differs extracts information progressively several gradient evaluations accumulating increasingly information time. dealing artifacts committing ﬁnding single representative perturbation approach incurs risk triggering artifacts black box. neural networks particular known affected surprising artifacts works demonstrate possible particular inputs drive neural network generate nonsensical unexpected outputs. entirely surprising since neural networks trained discriminatively natural image statistics. artifacts look unnatural nevertheless form subset images sampled negligible probability network operated normally. heatmaps similar masks method introduces quantitative criterion veriﬁable nature allows compare differing proposed saliency explanations demonstrate learned masks better metric. show applying bounded perturbation informed learned mask signiﬁcantly suppresses truck softmax score whereas boxed perturbation truck’s back bumper highlighted contrastive excitation backprop actually increases score principled interpretability method also allows identify instances algorithm learned wrong association. case chocolate sauce surprising spoon highlighted learned mask might expect sauce-ﬁlled salient. however manually perturbing image reveals indeed spoon suppressive jar. explanation imagenet chocolate sauce images contain spoons jars appears true upon examining images. generally method allows diagnose highly-predictive non-intuitive possibly misleading correlations identiﬁed machine learning algorithms data. test learned masks generalizable robust artifacts simplify masks blurring slicing binary masks thresholding smoothed masks tends cover salient part identiﬁed learned mask). simpliﬁed masks edit imagenet images constant noise blur perturbations. using googlenet compute normalized softmax probabilities fact simpliﬁed masks quickly suppress scores increases three perturbations gives conﬁdence learned masks identifying right regions perturb generalizable extracted masks perturbations trained experiments assess ability method correctly identify minimal region suppresses object. given output saliency normalize intensities range threshold tightest bounding around resulting heatmap. blur image compute normalized target softmax probability figure interrogating suppressive effects. left right original image learned mask overlaid; boxed perturbation chosen interest another boxed perturbation based learned mask implementation details. unless otherwise speciﬁed visualizations shown generated using adam minimize googlenet’s softmax probability target class using blur perturbation following parameters learning rate iterations upsampling mask factor blurring upsampled mask gσm= jittering mask drawing integer discrete uniform distribution initialize mask smallest centered circular mask suppresses score original image compared fully perturbed image i.e. fully blurred image. advantage proposed framework generated visualizations clearly interpretable. example deletion game produces minimal mask prevents network recognizing object. compared techniques method pinpoint reason certain object recognized without highlighting non-essential evidence. noted player visualizations also emphasize neighboring speakers similarly cliff street sign sunglasses sometimes shows part object essential face pekenese upper half truck spoon chocolate sauce plate found minimally sufﬁcient parts. contrastive figure left right original image learned mask simpliﬁed masks sec. swift softmax score suppression observed using three perturbations simpliﬁed binary masks derived learned masks thereby showing generality masks. bounding boxes normalized scores given amount score suppression smallest bounding achieves amount suppression. figure shows that average method yields smallest minimal bounding boxes considering suppressive effects results show method ﬁnds small salient area strongly impacts network. adversarial examples often generated using complementary optimization procedure method learns imperceptible pattern noise causes image misclassiﬁed added using reimplementation highly effective one-step iterative method generate adversarial examples method yielded visually distinct abnormal masks compared produced natural images train alexnet classiﬁer distinguish clean adversarial images using given heatmap visualization respect predicted class clean adversarial images method greatly outperforms methods achieves discriminating accuracy lastly learned masks applied back corresponding adversarial images minimize adversarial label often allow original predicted label clean image rise back predicted class. method recovers original label predicted clean image time ground truth label moreover time original predicted label recovered top- predicted labels mask+adversarial setting. knowledge ﬁrst work able recover originally predicted labels without modiﬁcation training set-up and/or network architecture. qualitatively examining learned masks different animal images noticed faces appeared salient appendages like feet. produce dense heatmaps test hypothesis. annotated subset imagenet dataset identiﬁes keypoint locations non-occluded eyes feet vertebrate animals select images classes least images contain least foot annotation resulting images animal classes every keypoint calculate average heatmap intensity window around saliency methods often assessed terms weaklysupervised localization pointing game tests discriminative heatmap method calculating precision heatmap’s maximum point lies instance given object class harder datasets like coco deletion game meant discover minimal salient part and/or spurious correlation expect particularly competitive localization pointing tested completeness. localization similar predict bounding dominant object table optimal thresholds error rates weak localization task imagenet validation using saliency heatmaps generate bounding boxes. †feedback error rate taken others code recalcucam) taken lated errors originally reported rates. ‡minimized predicted classes’ softmax scores used grad guid cexc g-cam occ§ v-occ† mask‡ diff table pointing game precision coco subset §occluded circles softened gσm= used perturb blur †occluded variable-sized blur circles; suppressive occlusions smallest radius chosen center used point. ‡used min. top- hyper-parameters effective using smallest highly suppressive mask sufﬁcient blur occlusion outperforms methods except contrast excitation backprop variable variable occlusion outperforms except contrast excitation backprop occlusion methods suggesting perturbation choice blur principle identifying smallest highly suppressive mask sound even implementation struggles task propose comprehensive formal framework learning explanations meta-predictors. also present novel image saliency paradigm learns algorithm looks discovering parts image affect output score perturbed. unlike many saliency techniques method explicitly edits image making interpretable testable. demonstrate numerous applications method contribute insights fragility neural networks susceptibility artifacts. figure difference learned masks clean adversarial images classiﬁcation accuracy without bilinear upsampling). discriminating clean adversarial images using heatmap visualizations imagenet validation images employ three simple thresholding methods ﬁtting bounding boxes. first value thresholding normalize heatmaps range threshold value second energy thresholding threshold heatmaps percentage energy salient subset covered finally mean thresholding threshold heatmap mean intensity heatmap thresholding method search optimal value heldout set. localization error calculated threshold table conﬁrms method performs reasonably shows three thresholding techniques affect method differently. non-contrastive excitation backprop performs best using energy mean thresholding; however method performs best value thresholding competitive using methods beats gradient guided backprop using energy thresholding; beats contrastive excitation backprop using mean thresholding out-performs gradcam occlusion thresholding methods. pointing table shows method outperforms center baseline gradient guided backprop methods beats grad-cam difﬁcult images total area target category less image least different object classes). noticed qualitatively method produce salient heatmaps objects small. regularization yield wellformed masks easily visible objects. test variants occlusion blur variable occlusion interrogate blur perturbation smoothed masks szegedy sermanet reed anguelov erhan vanhoucke rabinovich. going deeper convolutions. proceedings ieee conference computer vision pattern recognition pages zhou khosla lapedriza oliva torralba. learning deep features discriminative localization. proceedings ieee conference computer vision pattern recognition pages", "year": 2017}