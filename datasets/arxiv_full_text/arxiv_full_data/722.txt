{"title": "Echo State Queueing Network: a new reservoir computing learning tool", "tag": ["cs.NE", "cs.AI", "cs.LG", "92B20, 90B22, 90B20, 37M10", "I.2; D.4.8; F.1.1"], "abstract": "In the last decade, a new computational paradigm was introduced in the field of Machine Learning, under the name of Reservoir Computing (RC). RC models are neural networks which a recurrent part (the reservoir) that does not participate in the learning process, and the rest of the system where no recurrence (no neural circuit) occurs. This approach has grown rapidly due to its success in solving learning tasks and other computational applications. Some success was also observed with another recently proposed neural network designed using Queueing Theory, the Random Neural Network (RandNN). Both approaches have good properties and identified drawbacks. In this paper, we propose a new RC model called Echo State Queueing Network (ESQN), where we use ideas coming from RandNNs for the design of the reservoir. ESQNs consist in ESNs where the reservoir has a new dynamics inspired by recurrent RandNNs. The paper positions ESQNs in the global Machine Learning area, and provides examples of their use and performances. We show on largely used benchmarks that ESQNs are very accurate tools, and we illustrate how they compare with standard ESNs.", "text": "supervised learning problems gradient descent algorithm described quasi-newton methods proposed additionally function approximation properties model studied structure model leads efﬁcient numerical evaluation procedures good performance learning algorithms easy hardware implementations. consequently since introduction model applied variety scientiﬁc ﬁelds. nevertheless randnns model suffers limitations. related feedforward topology original acronym refer model rnn. work avoid conﬂict notation randnn random neural networks rnns machine learning literature recurrent neural networks. concerning models recurrences topologies recognized powerful tools number tasks machine learning however main limitation comes difﬁculty implementing efﬁcient training algorithms. main drawbacks related learning algorithms following convergence always guaranteed many algorithmic parameters involved sometimes long training times required reasons learning using recurrent neural networks principally feasible relatively small networks. recently paradigm called reservoir computing developed overcome main drawbacks learning algorithms applied networks cyclic topologies. years main models proposed echo state networks liquid state machines models describe possibility using recurrent neural networks without adapting weight connections involved recurrences. network outputs generated using simple learning methods classiﬁcation regression models. approach successfully applied many machine learning tasks achieving goods results specially temporal learning tasks abstract—in last decade computational paradigm introduced ﬁeld machine learning name reservoir computing models neural networks recurrent part participate learning process rest system recurrence occurs. approach grown rapidly success solving learning tasks computational applications. success also observed another recently proposed neural network designed using queueing theory random neural network approaches good properties identiﬁed drawbacks. paper propose model called echo state queueing network ideas coming randnns design reservoir. esqns consist esns reservoir dynamics inspired recurrent randnns. paper positions esqns global machine learning area provides examples performances. show largely used benchmarks esqns accurate tools illustrate compare standard esns. artiﬁcial neural networks class computational models proven powerful statistical learning tools solve complicated engineering tasks well many theoretical issues. several types anns designed originating ﬁeld machine learning others coming biophysics neuroscience. random neural network proposed gelenbe mathematical object inspired biological neuronal behavior merges features spiking neural networks queueing networks. literature actually different interpretations exactly mathematical model proposed. type spiking neuron associated network called randnns. type queue networks queues respectively called g-queues g-networks. randnn connectionist model spikes circulate among interconnected neurons. discrete state-space used represent internal state neuron. ﬁring times spikes modeled poisson processes. potential neuron represented positive integer increases spike arrives decreases neuron ﬁres. order randnns last usual independence assumptions considered poisson routing processes model assumed. call state network time observe continuous time markov process state space assume irreducible ergodic. interested network’s behavior steady-state assume equilibrium probability neuron excited parameter called activity rate neuron since process ergodic neron gelenbe shows equilibrium situation satisfy following non-linear system equations supplementary condition that neuron words assumption irreducibility system equations solution neuron solution unique markov process ergodic. moreover stationary distribution given product marginal probabilities neuron’s potential. details proofs learning tool used learn unknown function function’s variables external arrival rates input neuron function’s variables only). network’s output loads. learning parameters weights model. appropriate optimization method used weights arrival rate equals input data network output matches corresponding known output data values. model widely used ﬁelds combinatorial optimization machine learning problems communication networks computer systems recurrent neural networks large class computational models used several applications machine learning neurosciences. main characteristic type anns existence least feedback loop among connections circuit connections.the cyclic topology causes non-linear transformation models. section discusses contribution paper model similar using also ideas inspired queuing theory. finally present experimental results conclusions well discussion regarding future lines research. random neural network speciﬁc queuing network proposed merges concepts spiking neural networks queuing theory. depending context nodes seen queues spiking neurons. neurons receives spikes outside disjoint types called excitatory inhibitory associated neuron integer variable called neuron’s potential. time neuron receives excitatory spike potential increased one. neuron receives inhibitory spike potential strictly positive decreases one; equal remains value. neuron’s potential strictly positive neuron sends excitatory spikes outside. neuron’s potential strictly positive neuron excited active. numbering neurons arbitrary order let’s denote potential neuron time periods neuron active produces excitatory spikes rate words output process pulses coming active neuron poisson process. then spike produced neuron transferred environment probability synapse neuron excitatory spike produced switched neuron vu). literature related probability randnns probability pulse generated neuron goes neuron usually denoted p+/− different notation used standard anns literature direct connection often denoted reverse order. paper follow latter notation. routing procedure performed independently anything else happening network including previous future switches neuron one. observe neuron number neurons network. weight connection neurons deﬁned assume external arrival process positive spikes rates meaning spike considered type arrives given neuron coming network’s environment. order avoid trivial case input history stored internal states. hence recurrent neural networks powerful tool forecasting time series processing applications. also useful building associative memories data compression static pattern classiﬁcation however spite important abilities fact efﬁcient algorithms training neural networks without recurrences efﬁcient algorithms exist case recurrences present. since early reservoir computing gained prominence community. basic forms model described before esns lsms least three well-differenced structures identiﬁed input layer neurons receive information environment; reservoir liquid nonlinear expansion function implemented using recurrent neuronal network; readout usually linear function neural network without recurrences producing desired output. weight connections among neurons reservoir connections input reservoir neurons ﬁxed learning process weights input neurons readout units reservoir readout units object training process. reservoir recurrences circuits allows kind expansion input possibly history data larger space. point view reservoir idea similar expansion function used kernel methods example support vector machine projection enhance linear separability data hand readout layer built performant learning specially robust fast process. approach based empirical observation certain assumptions training linear readout often sufﬁcient achieve good performance many learning tasks instance model best known learning performance mackey–glass times series prediction task topology model consists input layer units sending pulses reservoir recurrent neural network units layer readout neurons adjustable connections reservoir layer. main difference lsms esns consists type nodes included reservoir. original model liquid built using model derived hodgkin-huxley’s work called leaky integrate fire neurons. standard model activation function units often tanh. basically threelayered hidden layer recurrences allowing connections input readout training data consists pairs inputoutput values unknown function weights matrices output computed wout; ensure good properties reservoir matrix usually scaled control spectral radius role spectral radius complex reservoir built spiking neurons paper propose reach objective simultaneously keeping good properties models previously described. purpose introduce echo state queuing network model reservoir dynamics based speciﬁc type queuing network behavior steady-state. architecture esqn consists input layer reservoir readout layer. input layer composed random neural units send spikes toward reservoir toward readout nodes. reservoir dynamics designed inspired equations recurrent randnns index input neurons reservoir neurons input offered network ﬁrst identify rates external positive spikes input traditionally done randnns standard randnn neuron’s loads computed solving expressions precisely input neurons behave m/m/ queues. load activity rate neuron stable case simply au/ru. reservoir units loads computed solving non-linear esqn model input neurons reservoir introduce concept state. state simply vector loads need network output corresponding input ﬁrst compute state using computed parametric function used dynamical prediction system. paper present simple case computing readout using linear regression. easy change another type function independent structure reservoir readout layers. thus network output computed using expression written follows temporal version time numerical experiences consider simulated time series data widely used literature real world data sets internet trafﬁc used research work forecasting techniques evaluate models’ accuracy normalized mean square error empirical mean notation data. positive negative weights esqn model initial reservoir state randomly initialized intervals respectively. usual training performance depend choice starting weights. take account experiment different random initial weights calculate average performance. preprocessing data step consisted rescaling data interval learning method used ofﬂine ridge regression algorithm contains regularization parameter fixed order nonlinear autoregressive moving average system. series generated following expression unif. generated training data samples validation samples. trafﬁc data internet service provider working european cities. original data bits collected every minutes. rescaled size training data size validation input neurons mapped last points past data values time conﬁguration suggested authors discuss different neural network topologies taking account seasonal traits data. trafﬁc data united kingdom education research networking association internet trafﬁc collected every day. network input time triple composed trafﬁc times studied small data training pairs validation samples. narma series data studied deep last data sets performance using arima holt-winters methods seen typical model consists reservoir following characteristics random topology large enough sparsely connected speciﬁc used sparsity spectral radius reservoir matrix. authors obtained best performance narma data problem spectral radius close performance improved using leakyintegrator neurons feedback connections initializing reservoir weights using another initializing criteria models units reservoir narma data units data sets. paper order compare performance esqn figure esqn model performance different reservoir sizes computed narma validation data set. reservoir weights randomly initialized. figure shows nmse average achieved runs different esqn initial weights. figure example esqn prediction instances validation european trafﬁc data. instances correspond time steps reservoir randomly initialized neurons. models standard versions them. table presents accuracy esqn models. last column give conﬁdence interval obtained independent runs. order narma ukerna data performance obtained esqn better case european data shows signiﬁcant better performance. observe cases accuracy obtained esqn good. also observe using years cumulated knowledge esns implementation comparing ﬁrst versions largely unexplored esqn model. happens model general larger reservoir enriches learning abilities model. sparsity density reservoir esqn model studied work. left future efforts. narma interesting time series data outputs depend input previous outputs. modeling problem difﬁcult solve non-linearity data necessity kind long memory. figure illustrates estimation esqn units reservoir randomly chosen figures show prediction values interval validation data. figure shows prediction instances beginning time validation set. main difﬁculty model ukerna data training small. spite this figure illustrates good performance esqn model. ﬁgure shows estimation last contribution presented type reservoir computing model call echo state queuing network combines ideas queueing neural networks. based computational models echo state network random neural network. methods successfully used forecasting machine learning problems. particularly esns applied many temporal learning tasks. model used predict three time series data widely used machine learning literature. cases tested performance results good. empirically investigated relation reservoir size esqn performance. found reservoir size signiﬁcant impact accuracy. another positive property esqns simplicity since reservoir units counter functions. last tool easy implement software hardware. still several aspects model studied future work. examples impact sparsity reservoir weights weight initialization methods used scaling reservoir weights utilization leaky integrators. gelenbe spiked random neural network nonlinearity learning approximation proc. fifth ieee international workshop cellular neural networks applications london england april georgiopoulos koc¸ak learning feedreview performance forward random neural network critical evaluation vol. available http//www.sciencedirect.com/science/article/pii/s luko˘sevi˘cius jaeger reservoir computing approaches recurrent neural network training computer science review available http//dx.doi.org/./j. cosrev.. maass natschl¨ager markram real-time computing without stable states framework neural computation based perturbations neural computation november rubino quantifying quality audio video transmissions internet psqa approach design operations communication networks review wired wireless modelling management challenges ser. edited barria. imperial college press cancela robledo rubino grasp algorithm based local search designing access network electronic notes discrete mathematics vol. available http//dx.doi.org/./j.endm... harnessing nonlinearity predicting chaotic systems saving energy wireless communication science vol. available http//www.sciencemag.org/content///.abstract paugam-moisy bohte handbook natural computing. springer-verlag sep. computing spiking neuron networks. available http//liris.cnrs.fr/publis/?id= cortez rocha sousa multiscale internet trafﬁc forecasting using neural networks time series methods expert systems available http//dx.doi.org/./j. -...x time basterrech fyfe rubino self-organizing maps scale-invariant maps echo state networks intelligent systems design applications international conference nov.", "year": 2012}