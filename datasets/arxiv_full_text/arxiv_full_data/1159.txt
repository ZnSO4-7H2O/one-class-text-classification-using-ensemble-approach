{"title": "Diving deeper into mentee networks", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Modern computer vision is all about the possession of powerful image representations. Deeper and deeper convolutional neural networks have been built using larger and larger datasets and are made publicly available. A large swath of computer vision scientists use these pre-trained networks with varying degrees of successes in various tasks. Even though there is tremendous success in copying these networks, the representational space is not learnt from the target dataset in a traditional manner. One of the reasons for opting to use a pre-trained network over a network learnt from scratch is that small datasets provide less supervision and require meticulous regularization, smaller and careful tweaking of learning rates to even achieve stable learning without weight explosion. It is often the case that large deep networks are not portable, which necessitates the ability to learn mid-sized networks from scratch.  In this article, we dive deeper into training these mid-sized networks on small datasets from scratch by drawing additional supervision from a large pre-trained network. Such learning also provides better generalization accuracies than networks trained with common regularization techniques such as l2, l1 and dropouts. We show that features learnt thus, are more general than those learnt independently. We studied various characteristics of such networks and found some interesting behaviors.", "text": "might well true. even networks features unique datasets matter. ﬁnetuning already trained network works certain extent features learnt traditional manner target dataset simply copied. also guarantee features best representations target dataset although validity expecting representation might work well since learnt large enough dataset. computer vision scientists attempt train architecture scratch training even mid-sized deep network small dataset notoriously difﬁcult task. training deep network even mid-level depth require supervision order avoid weight explosion. imaging datasets image sizes memory insufﬁciency typical restricts mini-batches less using small mini-batches small datasets lead noisy untrustworthy gradients. leads weight explosions unless learning rates made sufﬁciently smaller. smaller learning rates learning slowed down. smaller mini-batches learning unstable. avoid problems using regmodern computer vision possession powerful image representations. deeper deeper convolutional neural networks built using larger larger datasets made publicly available. large swath computer vision scientists pre-trained networks varying degrees successes various tasks. even though tremendous success copying networks representational space learnt target dataset traditional manner. reasons opting pre-trained network network learnt scratch small datasets provide less supervision require meticulous regularization smaller careful tweaking learning rates even achieve stable learning without weight explosion. often case large deep networks portable necessitates ability learn mid-sized networks scratch. article dive deeper training midsized networks small datasets scratch drawing additional supervision large pre-trained network. learning also provides better generalization accuracies networks trained common regularization techniques dropouts. show features learnt thus general learnt independently. studied various characteristics networks found interesting behaviors. proliferation off-the-shelf downloadable networks vgg- overfeat r-cnn several others caffe model become common practice computer vision community simply ﬁne-tune networks task networks usually trained large dataset imagenet pascal proponents networks argue networks learnt image representations pertinent datasets deal natural images. assumption datasets natural images derived similar distribution ularization. regularizing penalize gradients trying make weights higher higher. batch normalization another technique quite commonly used keep weight explosion check even regularization techniques difﬁculty training deep network scratch leads computer vision scientists pre-trained networks. several reasons might favour smaller mid-sized network even though might better solution available using large pre-trained networks. large pre-trained networks computationally intensive often depth excess layers. computational requirement networks make easily portable. networks require state-ofthe-art gpus work even simple feed forward modes. impracticality using pre-trained networks smaller computational form factors necessitates need learn smaller network architectures. quandary smaller networks architectures cannot produce powerful enough representations. many methods recently proposed draw additional supervision large well-trained networks regularize network learning scratch works inspired dark knowledge approach techniques layer supervision softmax supervision technique learn deeper networks better. figure shows conceptualization idea. paper make shallower mentee network learn representation larger well-trained mentor network various depths network hierarchy. mentorship happens tagging loss mentee network dissimilarity loss layer want mentored. best knowledge hasn’t work regularized layer way. also hasn’t work trained mid-sized network larger deeper network scratch. study idiosyncratic properties novel conﬁgurations mentee networks. argue mentoring avoids weight explosion. even using smaller minibatches mentee networks ample supervision capable stable learning even high learning rates. show mentee networks produce better generalization performance independently learnt baseline network. also show mentee networks better transferable independently learnt baselines also good initializer. also show mentee networks learn good representations little data sometimes even without supervision dataset unsupervised fashion. hinton tried make networks portable learning softmax outputs larger well-trained network along label costs previously explored using logits caruana directly learning softmax layers forcing softmax layer smaller network mimic mapping larger network onto label space. tried learn better second third guesses. called dark knowledge knowledge learnt available larger network. attempting learn softmax layer able transfer distil knowledge networks. drawback work works long larger network already well-trained stable. relied upon network’s predictive softmax layer learnt perfectly target dataset propagate knowledge. also assumes relationships classes exploited. work cases true character recognition voice recognition doesn’t work object detection datasets relationship classes given terms appearance features. also distil softmax labels representational space itself. also requires smaller network capable training stable manner. dark knowledge extended upon several previous works extension work generalize article using layer-wise knowledge transfer layer middle network. used show thinner deeper network trained better regularization another method uses similar one-layer regularizer knowledge transfer mentored training also shown extremely useful training lstms rnns independent mentor supervision methods discussed essentially technique dark-knowledge method extended beyond softmax layer. methods ﬁxed one-layer regularizations although trivial generalize many layers. mentee networks typically much deeper complex mentors means build complex models study best knowledge builds less complex models capability larger models. also neither study studls network loss mini-batch weighs losses together enable balanced training weight probe softmax layers. annealing functions parametrized iteration progress. although methods literature constants found preferable retain throughout anneal linearly. discuss value need parameters detail further. since pre-trained stable second third terms equation penalties activations layers resembling activations probed layer respectively. losses deﬁned equation functions weights layers only. restrict weights within proximity region produces activations known mentor better activations. restricting behaviour acts guided regularization process allowing weights explore direction mentor thinks good direction still letting gradients explode vanish. last terms guided version noise decreases iteration. earlier stages training allows weights explore space also restricts weights exploding direction weights allowed explore controlled mentor. freedom explore tightens learning proceeds provided monotonically annealing function respect note even though calculate error gradients need forward propagation back propagate penalty weights even though using activations penalize weights indirectly. although mentee networks regularized dropouts batch normalizations recommended mentee networks imposes additional regularizations mirroring mentor networks better learning. ﬁrst generalize methods knowledge transfer follows consider large mentor network layers suppose represent neuron activations layer network consider smaller mentee network layers suppose already well-trained stable general enough dataset consider using learn classiﬁcation newer dataset less general much smaller determined priori. although constraint smaller less general dataset emphasizes core need mentored learning useful. deﬁne probe error measure error modelled rmse error follows minimum number neurons neurons convolutional consider element-wise errors ﬁlters. adding cost label cost network back propagation learn discriminative enough representation samples labels also layers pre-determined layers representation closer produced implementations loses literature tend learn regressor instead simply adding loss concluded experiments computational requirements regressors justify contributions. adding regressor would involve embedding activations mentor mentee onto common space minimizing distances embeddings. quite simply circumvent consider minimum number matching neurons. enables slimmer fatter mini-batch data sized mentee. suppose dataset suppose pre-determined probes tuples layers overall network cost label costs leading believe representations mentored indeed relevant long datasets share common characteristics. plethora conﬁgurations could tried many unique characteristics discovered. article limit enable stability learning focus help better generalizations. learning large networks prefer obedient networks obedient networks heavily regularized beginning leading careful initialization stabilization network learning labels takes over. call stabilization phase mentoring phase rest self-study phase. mentoring phase learning cases increasing slow steady. function aggressive climb annealing rates typical obedient mentee adamant mentee shown ﬁgure also typically later layers stubborn mentored earlier layers. although typically expected obedience enforced choosing higher values layers deeper network. demonstrate effectiveness learning ﬁrst train larger network dataset. using network mentor train mentee network dataset. unlike literature choose mentee networks generally much smaller mentor. show generalizes least well independent network exact architecture regularized mentor batch normalization norms dropouts. training mid-sized networks small datasets often difﬁcult. best knowledge provided best effort meticulously learning networks. learning independent network often spent additional effort adjusting learning rates opportune moments. show mentee networks outperforms independent networks even worst case performs well independent networks. generality learnt representations demonstrate network learns general representation gather pair datasets seemingly similar characteristics general larger other. train mentor general dataset ﬁrst tune less general dataset. train independent mentee nets less general dataset demonstrate worst mentee performs independent net. different combinations produces different characteristics mentee networks. equation seen learning three different learning rates simulate using three parameters idiosyncratic personalities mentee networks obedient network adamant network. obedient network network focuses learning representation label costs beginning stages good representation learnt focuses learning label space. tends towards over-regularized regularization relaxes epochs. adamant network network focuses almost immediately labels much learning representation focus positively towards learning label only. learning rates personalities shown ﬁgure independent network considered special case adamant network probe weights ignored extreme case obedient network perhaps gullible network learns embedding space mentor. gullible networks also good initialize network unsupervised mentoring fashion. consider dataset labels. neither mentor mentee could potentially learn discriminative features. using probes could build error function could make smaller mentee network still learn good representation dataset. information parent network learn good representation simply back propagating second term equation alone. gullible mentees come really handy dataset considerably less samples supervised with. unsupervised mentoring also aggressive initialize network often helpful learning large networks stable manner stable initialization. typically deeper goes difﬁcult becomes learn activations costs saturate quickly. softmax layer difﬁcult learn. surprise probe costs converge much sooner learn dimensional representation vgg- network using ambitiously less number layers. dataset pairs experiments explicit mentor network learnt. simply learnt probes without retraining vgg- network. attempting learn vgg-’s view caltech dataset probing representational frame vgg- network. used relatively obedient student shown ﬁgure case. implementation details independent networks regularized penalties weight seems give best results. networks also applied parametrized batch norm fully connected convolutional layers dropouts rate fully connected layers dropout bath norm together help avoiding over-ﬁtting. activation functions rectiﬁed linear units learning mentee network start learning rates high larger independent networks forced learning rate smaller experiments able high since batch sizes larger. training ever exploding gradients nans reduce learning rate times reset parameters back epoch continue training. train epochs reduce learning rate hundred times continue ﬁne-tuning early stopping. unless early stopped train epochs. initializations -mean gaussian distribution except biases initialized zeros. experiment set-up designed using theano programs written ourselves experiments mnist datasets conducted nvidia others nvidia tesla cudnn nvidia cuda minibatch sizes mnist cifar experiments mini-batch sizes caltech experiments images resized vgg- requirement. apart normalization meansubtraction pre-processing applied images. caltech experiments used adagrad polyak’s momentum experiments smaller networks used rmsprop nesterov’s accelerated gradient proceed tune classiﬁer layer mentee independent using general dataset since layers allowed change mentee additional supervision. tests quality features learnt networks general difﬁcult dataset. sake experiments consider pairing assume cifar- general cifar- caltech- general caltech. additionally conduct another experiment learn mentor network trained full mnist dataset mentee network supervision part dataset independent network also case learns redacted dataset. redact dataset samples class dataset essentially ambitious goal -shot learning scratch using deep network. also mentee network initialized unsupervised mentoring mentor network. acknowledge comparison unsupervised mentoring unfair because mentee initialized mentor information ﬁltered data unavailable independent network. latter results demonstrate unsupervised mentoring could learn effective feature space even without labels less samples. particular learning classiﬁcation caltech dataset learn representation popularly used vgg- network various levels network hierarchy vgg- network’s dimensional representation coveted iconic image features computer vision time writing article. vgg- network convolutional layers fully-connected layers last produces dimensions features upon many similar trend mnist experiments also. less data higher gain mentee networks. note even though mentee networks regularized care taken ensure exact number iterations exact learning rate. also found unsupervised mentoring always keeps learning high standard although discussed section additional supervision entire dataset unsupervised mentoring unfair. experiments caltech datasets mentee networks perform better vanilla network. mentee network also able perform signiﬁcantly better independent network classiﬁer/mlp sections allowed learn caltech dataset representation learnt caltech. proves generality feature space learnt. even obedient student able learn feature space vgg- network remarkable degree. ﬁrst convolutional layer able learn minimum rmse random. last layers able learn upto rmse random. figure shows ﬁlters learnt epoch gullible network obedient network adamant network. networks initialized random values inception. easily notice gullible network already sway towards vgg- ﬁlters. obedient mentee notice corner detector features already swaying towards mentee network complex features swaying much gullible network. surprise notice even adamant network corner detectors swaying towards vgg-. shows even weights ﬁrst layer features learning vgg-’s representation. noted learning weights directly learning activations produced vgg- network caltech dataset leads learn ﬁlters vgg-. implies corner features general among imagenet dataset vgg- trained caltech dataset explains learnt earlier others. large pre-trained networks continue remain popular ease copying network ﬁne-tuning last layers believe still need learning small mid-sized networks scratch. also recognize difﬁculty involved reliably training deep networks data samfigure vgg- ﬁrst layer ﬁlters ﬁlters probed using caltech gullible obedient adamant mentee epoch training. recommend viewing image computer monitor. achieve state-of-the-art accuracies datasets didn’t implement several techniques commonly applied boost network performances modern computer vision. purpose experiments unequivocally demonstrate among networks learn scratch mentored perform better learn general features not. results split across tables based network architectures. smaller experiments layer network shown ﬁgure larger ones ﬁgure symbol shows layers probed where. ﬁgure results clearly demonstrate strong performance mentee networks independent networks. cifar experiments under-weighted purposely didn’t want propagate error mentor network mentee network. results cifar scratch seem indicate networks reached best possible performance architecture. believe amount supervision already provided training images mentoring effective. already ample supervision menples. meet best worlds using mentored learning approach. study shallower mentee network able learn representation scratch regularized mentor network’s activations input samples. found mentoring provided much stabler training even higher learning rates. noted special cases networks recognize idiosyncratic personalities. extended able perform unsupervised initialization technique. showed compelling experiments strong performance generality mentor networks.", "year": 2016}