{"title": "Training deep neural networks with low precision multiplications", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications.", "text": "multipliers space power-hungry arithmetic operators digital implementation deep neural networks. train state-of-the-art neural networks three benchmark datasets mnist cifar- svhn. trained three distinct formats ﬂoating point ﬁxed point dynamic ﬁxed point. datasets formats assess impact precision multiplications ﬁnal error training. precision sufﬁcient running trained networks also training them. example possible train maxout networks bits multiplications. training deep neural networks often limited hardware. lots previous works address best exploitation general-purpose hardware typically clusters gpus faster implementations usually lead state results actually approaches always consist adapting algorithm best exploit state general-purpose hardware. nevertheless dedicated deep learning hardware appearing well. fpga asic implementations claim better power efﬁciency general-purpose hardware contrast general-purpose hardware dedicated hardware asic fpga enables build hardware algorithm. hardware mainly made memories arithmetic operators. multipliers space power-hungry arithmetic operators digital implementation deep neural networks. objective article assess possibility reduce precision multipliers deep learning maxout networks state-of-the-art neural networks train maxout networks slightly less capacity goodfellow three benchmark datasets mnist cifar- svhn three datasets three formats assess impact precision multiplications ﬁnal error training. precision multiplications sufﬁcient running trained networks also training made code available applying deep neural network mainly consists convolutions matrix multiplications. arithmetic operation dnns thus multiply-accumulate operation. artiﬁcial neurons basically multiplier-accumulators computing weighted sums inputs. cost ﬁxed point multiplier varies square precision small widths cost adders accumulators varies linear function precision result cost ﬁxed point multiplier-accumulator mainly depends precision multiplier shown table modern fpgas multiplications also implemented dedicated blocks/slices. block/slice implement single multiplier double multiplier triple multiplier. reducing precision thus lead gain number available multipliers inside modern fpga. floating point formats often used represent real values. consist sign exponent mantissa illustrated ﬁgure exponent gives ﬂoating point formats wide range mantissa gives good precision. compute value single ﬂoating point number using following formula table shows exponent mantissa widths associated ﬂoating point format. experiments single precision ﬂoating point format reference widely used format deep learning especially computation. show half precision ﬂoating point format little impact training neural networks. time writing article standard exists half precision ﬂoating point format. fixed point formats consist signed mantissa global scaling factor shared ﬁxed point variables. scaling factor seen position radix point. usually ﬁxed hence name ﬁxed point. reducing scaling factor reduces range augments precision format. scaling factor typically power computational efﬁciency result ﬁxed point format also seen ﬂoating point format unique shared ﬁxed exponent illustrated ﬁgure fixed point format commonly found embedded systems relies integer operations. hardware-wise cheaper ﬂoating point counterpart exponent shared ﬁxed. algorithm policy update scaling factor. require matrix scaling factor maximum overﬂow rate rmax. ensure updated scaling factor st+. overﬂow rate rmax else overﬂow rate rmax dynamic ﬁxed point format variant ﬁxed point format several scaling factors instead single global one. scaling factors ﬁxed. such seen compromise ﬂoating point format scalar variable owns scaling factor updated operations ﬁxed point format global scaling factor never updated. dynamic ﬁxed point grouped variables share scaling factor updated time time reﬂect statistics values group. practice associate layer’s weights bias weighted outputs respective gradients vectors matrices different scaling factor. scaling factors initialized global value. initial values also found training higher precision format. training update scaling factors given frequency following policy described algorithm higher precision parameters updates forward backward propagations respectively called fprop bprop. idea behind able accumulate small changes parameters hand sparing bits memory bandwidth fprop. done implicit averaging performed stochastic gradient descent training terms statistically independent dominant variations come random sample examples minibatch strong averaging effect takes place contribution relatively small hence demand sufﬁcient precision maxout network multi-layer neural network uses maxout units hidden layers. maxout unit outputs maximum products weight vectors input vector unit corresponds maxout unit ﬁlters forced combined dropout effective regularization method maxout networks achieved state-of-the-art results number benchmarks part fully connected feedforward deep nets part deep convolutional nets. dropout technique provides good approximation model averaging shared parameters across exponentially large number networks formed subsets units original noise-free deep network. train maxout networks slightly less capacity goodfellow three benchmark datasets mnist cifar- svhn. section hyperparameters section train maxout networks precision multiplications. table test error rates single half ﬂoating point formats ﬁxed dynamic ﬁxed point formats permutation invariant mnist mnist cifar- svhn datasets. prop. bitwidth parameters updates. single precision ﬂoating point line refers results experiments. serves baseline evaluate degradation brought lower precision. mnist dataset described table dataaugmentation unsupervised pre-training. simply minibatch stochastic gradient descent momentum. linearly decaying learning rate linearly saturating momentum. regularize model dropout constraint norm weight vector train different models mnist. ﬁrst permutation invariant model unaware structure data. consists fully connected maxout layers followed softmax layer. second model consists three convolutional maxout hidden layers followed densely connected softmax layer. procedure goodfellow except train model validation examples. consequence test error slightly larger reported goodfellow ﬁnal test error table comparative characteristics cifar- dataset given table preprocess data using global contrast normalization whitening. model consists three convolutional maxout layers fully connected maxout layer fully connected softmax layer. follow similar procedure mnist dataset. procedure goodfellow except reduced number hidden units train model validation examples. consequence test error slightly larger reported goodfellow ﬁnal test error table svhn dataset described table applied local contrast normalization preprocessing zeiler fergus model consists three convolutional maxout layers fully connected maxout layer fully connected softmax layer. otherwise followed approach mnist dataset. procedure goodfellow except reduced length training. consequence test error bigger reported goodfellow ﬁnal test error table figure final test error depending radix point position dataset ﬁnal test errors normalized divided dataset single ﬂoat test error. propagations parameter updates bit-widths bits figure final test error depending propagations bit-width format dataset ﬁnal test errors normalized means divided dataset single ﬂoat test error. formats parameter updates bit-width bits ﬁxed point format radix point ﬁfth bit. dynamic ﬁxed point format maximum overﬂow rate figure final test error depending parameter updates bit-width format dataset ﬁnal test errors normalized means divided dataset single ﬂoat test error. formats propagations bit-width bits ﬁxed point format radix point ﬁfth bit. dynamic ﬁxed point format maximum overﬂow rate figure final test error depending maximum overﬂow rate propagations bit-width. ﬁnal test errors normalized means divided dataset single ﬂoat test error. parameter updates bit-width bits half precision ﬂoating point format little impact test error rate shown table conjecture high-precision ﬁne-tuning could recover small degradation error rate. optimal radix point position ﬁxed point ﬁfth important illustrated figure corresponding range approximately corresponding scaling factor depends bit-width using. minimum bit-width propagations ﬁxed point bit-width test error rate rises sharply illustrated figure minimum bit-width parameter updates ﬁxed point bit-width test error rate rises sharply illustrated figure doubling number hidden units allow reduction bit-widths initial scaling factors training higher precision format. scaling factors found reinitialize model parameters. update scaling factors every examples. augmenting maximum overﬂow rate allows reduce propagations bit-width also signiﬁcantly augments ﬁnal test error rate illustrated figure consequence maximum overﬂow rate rest experiments. minimum bit-width propagations dynamic ﬁxed point bit-width test error rate rises sharply illustrated figure minimum bit-width parameter updates dynamic ﬁxed point bit-width test error rate rises sharply illustrated figure doubling number hidden units allow reduction bit-widths permutation invariant mnist. using bits propagations bits parameter updates little impact ﬁnal test error exception svhn dataset shown table signiﬁcantly better ﬁxed point format consistent predictions section vanhoucke bits linear quantization store activations weights. weights scaled taking maximum magnitude layer normalizing fall range. total memory footprint network reduced similar dynamic ﬁxed point format however vanhoucke apply already trained neural networks actually train them. precision multipliers sufﬁcient training deep neural networks. dynamic ﬁxed point seems well suited training deep neural networks. using higher precision parameters updates helps. recent work also trains neural networks precision. authors propose replace round-to-nearest stochastic rounding allows reduce numerical precision bits using ﬁxed point format. would interesting combine dynamic ﬁxed point stochastic rounding. thank developers theano python library allowed easily develop fast optimized code gpu. also thank developers pylearn python library built theano allowed easily interface datasets theano code. also grateful funding nserc canada research chairs compute canada cifar. references bastien lamblin pascanu bergstra goodfellow bergeron bouchard bengio theano features speed improvements. deep learning unsupervised feature learning nips workshop. chen wang chen temam diannao smallproceedings footprint high-throughput accelerator ubiquitous machine-learning. international conference architectural support programming languages operating systems pages acm. farabet martini corda akselrod culurciello lecun neuﬂow runtime reconﬁgurable dataﬂow processor vision. computer vision pattern recognition workshops ieee computer society conference pages ieee. goodfellow warde-farley lamblin dumoulin mirza pascanu bergstra bastien bengio pylearn machine learning research library. arxiv preprint arxiv.. holt baker back propagation simulations using limited precision calculations. neural networks ijcnn--seattle international joint conference volume pages ieee. jarrett kavukcuoglu ranzato lecun best multi-stage architecture object recognition? proc. international conference computer vision pages ieee. netzer wang coates bissacco reading digits natural images unsupervised feature learning. deep learning unsupervised feature learning workshop nips. pham p.-h. jelaca farabet martini lecun culurciello neuﬂow dataﬂow vision processing system-on-a-chip. circuits systems ieee international midwest symposium pages ieee. presley haggard ﬁxed point implementation backpropagation learning algorithm. southeastcon’. creative technology transfer-a global affair. proceedings ieee pages ieee.", "year": 2014}