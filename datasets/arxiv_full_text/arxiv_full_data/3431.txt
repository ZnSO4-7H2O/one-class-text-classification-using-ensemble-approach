{"title": "Count-ception: Counting by Fully Convolutional Redundant Counting", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Counting objects in digital images is a process that should be replaced by machines. This tedious task is time consuming and prone to errors due to fatigue of human annotators. The goal is to have a system that takes as input an image and returns a count of the objects inside and justification for the prediction in the form of object localization. We repose a problem, originally posed by Lempitsky and Zisserman, to instead predict a count map which contains redundant counts based on the receptive field of a smaller regression network. The regression network predicts a count of the objects that exist inside this frame. By processing the image in a fully convolutional way each pixel is going to be accounted for some number of times, the number of windows which include it, which is the size of each window, (i.e., 32x32 = 1024). To recover the true count we take the average over the redundant predictions. Our contribution is redundant counting instead of predicting a density map in order to average over errors. We also propose a novel deep neural network architecture adapted from the Inception family of networks called the Count-ception network. Together our approach results in a 20% relative improvement (2.9 to 2.3 MAE) over the state of the art method by Xie, Noble, and Zisserman in 2016.", "text": "counting objects digital images process replaced machines. tedious task time consuming prone errors fatigue human annotators. goal system takes input image returns count objects inside justiﬁcation prediction form object localization. repose problem originally posed lempitsky zisserman instead predict count contains redundant counts based receptive ﬁeld smaller regression network. regression network predicts count objects exist inside frame. processing image fully convolutional pixel going accounted number times number windows include size window recover true count take average redundant predictions. contribution redundant counting instead predicting density order average errors. also propose novel deep neural network architecture adapted inception family networks called count-ception network. together approach results relative improvement state method noble zisserman counting objects digital images process time consuming prone errors fatigue human annotators. goal research area system takes input image returns count objects inside justiﬁcation prediction form object localization. classical approach counting involves ﬁne-tuning edge detectors segment objects background counting one. large challenge dealing overlapping objects require methods watershed transformation approaches many hyperparameters speciﬁcally task complicated build. core modern approaches described lempitsky zisserman given labels point annotations object construct density image. here object predicted takes density density reveal total number objects image. method naturally accounts overlapping objects extend idea focus main areas figure given image regression network counts number objects receptive ﬁeld. predicted count corresponds receptive ﬁeld regression network. upper left pixel activation based pixel input image upper left corner. repose problem predicting density instead predict count contains redundant counts based receptive ﬁeld smaller regression network. regression network predicts count objects exist inside frame shown figure processing image fully convolutional pixel going accounted number times number windows include size window recover true count take average predictions. figure illustrates change kernel makes sense respect receptive ﬁeld network must make predictions. using gaussian density forces model predict speciﬁc values based cell center receptive ﬁeld. harder task predicting existence cell receptive ﬁeld. comparison types count maps shown figure perform prediction focus method using deep learning convolutional neural networks like arteta have. utilized networks similar fcn- form bottlenecks core network capture complex relationships different parts image. instead borders input figure comparing single count calculated single cell. line values network trained predict gaussian kernel used. green values square kernel used. square kernel size receptive ﬁeld. idea counting density began lempitsky zisserman used dense sift features image input linear regression predict density map. predict redundant counts instead density map. although summation output model taken causes method explicitly designed tolerate errors predictions made. however density objects count multiple times indirectly. needs properly predict density objects generated small gaussian mean point annotation. values need predict vary mean not. doesn’t take account receptive ﬁeld objects view network suppress prediction. many approaches introduced predict better density map. fiaschi used regression forest instead linear model make density prediction based bow-sift features. arteta proposed interactive counting algorithm would extend algorithm dynamically learn count various concepts image. introduced deep neural networks problem. method built network would convolve region density map. network trained fully convolutional similar method. however approaches focus predicting density differentiates work. arteta discuss approaches past density model. focus different work. tackle problem incorporating multiple point annotations noisy crowd sourced data. also utilize segmentation background ﬁlter erroneous predictions happen there. segui method takes entire image input output single count value using fully connected layers break spatial relationship. discover network learn count learn features identifying objects mnist digits. idea regression network learning count frame. expect produce errors perform task redundantly. presented interesting idea similar direction going goal predict proximity consists cone shaped distributions cell smooths cell prediction using surrounding detections. cone extended pixels point annotation average size cell. however approach line density count map. would like obtain count objects input image given training examples point annotations object. objects count often small overall image large. counting labor-intensive often labeled images practice. input image target image constructed image point notations stride length width length receptive ﬁeld receptive ﬁeld associated predicted counts number training validation images figure count-ception network architecture used regression network. intermediate tensor labeled ﬁlters points network size reduced. convolutions padded reduce size. batch normalization layers inserted convolution pictured here. motivation want merge idea networks count everything receptive ﬁeld segui density objects lempitsky zisserman using fully convolutional processing like arteta technique instead using takes entire image input produces single prediction number objects smaller network image produce intermediate count map. smaller network trained count number objects receptive ﬁeld. formally; process image network fully convolutional produce matrix represents counts objects speciﬁc receptive ﬁeld sub-network performs counting. high-level overview want count target objects image image multiple target objects labelled single point labels counting network reduces dimensions input must padded order deal objects appear border. objects border image receptive ﬁeld network column overlapping input image. pixel pixels border target image constructed point-annotated size input image object annotated single pixel. desirable labeling dots much easier drawing boundaries segmentation. fully convolutional networks receptive ﬁeld output fully convolutional network entire image pixels. yields fully convolutional network output image larger original input. pixel output represent count targets receptive ﬁeld. perform mapping propose count-ception architecture adapted inception family networks szegedy proposed model shown figure core model inception units used perform convolutions multiple layers without reducing size tensor. every convolution leaky relu activation applied notice improvement regression predictions leaky relu training output pushed zero recover predict correct count. figure pipeline given input image input image padded convolved calculate prediction count match target count map. count nonzero border input image. loss calculated prediction count target count order update weights counting network better match target count map. fully convolutional network processes image applying network small receptive ﬁeld entire image. effects reduce overﬁtting. first small fully convolutional network much fewer parameters network trained entire image. second splitting image fully convolutional network much training data parameters following discussions consider receptive ﬁeld simplicity order concrete examples. method used receptive ﬁeld size. overview process shown figure perform sampling locations using large ﬁlters greatly reduce size tensor. necessity allowing model train utilizing batch normalization layers every convolution. found penalty harsh network training. reached conclusion conﬁguration chose loss instead. also tried combine basic pixel-wise loss loss based overall prediction entire image. found caused over-ﬁtting provided assistance training. network would simply learn artifacts image order correctly predict overall counts. loss surrogate objective real count want. intentionally count cell multiple times order average possible errors. stride target counted pixel receptive ﬁeld. stride increases number redundant counts decreases. many beneﬁts using redundant counts. pixel label exactly center cell even outside cell network still learn average cell appear receptive ﬁeld. approach sacriﬁce ability localize cell exactly coordinates. viewing predicted count localize detection came speciﬁc coordinate. many applications accurate counting important exact localization. another issue approach correct overall count come correctly identifying cells could network adapting average prediction regression. common example training data contains many images without cells network predict order minimize loss. solution similar curriculum learning ﬁrst train balanced examples take well performing networks train sparse datasets. cells compare state ﬁrst standard benchmark dataset introduced lempitsky zisserman images resolution contain simulated bacterial cells ﬂuorescence-light microscopy created image contains cells overlap various focal distances simulating real life imaging microscope. cells also real dataset based dataset introduced kainz consists eleven resolution images bone marrow height healthy individuals. standard staining procedure used depicts blue nuclei various cell types present whereas cell constituents appear various shades pink red. modiﬁed dataset ways create dataset first images cropped order process images memory also smooth evaluation errors training better comparison. yields total images containing cells addition ground truth annotations updated visual inspection capture number unlabeled nuclei help domain experts. adipocyte cells ﬁnal dataset human subcutaneous adipose tissue dataset obtained genotype tissue expression consortium regions interest representing adipocyte cells sampled high resolution histology slides using sliding window images sampled representing suitable scale cells could counted using receptive ﬁeld. average cell count across images adipocytes vary size dramatically given first compare overall performance proposed model existing approaches table dataset. dataset follow evaluation protocol used lempitsky zisserman used future papers. evaluation protocol training validation testing subsets used. held-out testing size ﬁxed experiments training validation sizes varied simulate lower higher numbers labeled examples. algorithm trains training able early stop evaluating performance validation set. size training validation sets varied together simplicity. results algorithm using least random splits computed present mean standard deviation. testing size remains constant order provide constant evaluation. testing chosen remaining examples instead ﬁxed size smaller values would less impacted difﬁcult examples test examples sampled replacement. practitioner baseline comparison compare results cell proﬁler’s uses segmentation perform object identiﬁcation counting. representative cells typically counted biology laboratories. designed main different pipelines evaluated error splits randomly chosen images synthetic dataset splits images bone marrow dataset mimic experimental setup place since cell proﬁler training set. cells report performance using pipeline images using three slightly modiﬁed versions pipeline parameter adjusted account color differences seen images. among methods compare xie’s fcrn-a network xie’s method neural network based approaches. network sufﬁciently deeper xie’s fcrn-a network representational power together redundant counting able perform signiﬁcantly better. show performance model matches xie’s redundant counting disabled changing stride eliminate redundant counting. claim redundant counting signiﬁcant success method. increasing stride reduce double counting none. present reader table indicates stride meaning maximum amount redundant counting patch size optimal choice. increase stride equal patch size redundant counting occurring accuracy reduced. power algorithm redundant counting. however increasing redundant count complicated. receptive ﬁeld could increased parameters cause network overﬁt training data. explored receptive ﬁeld found perform better. another approach could dilated convolutions would equivalent scaling input image resolution. run-time algorithm trivial. explored models less parameters found could achieve performance. shorter models narrower models tended enough representational power count correctly. making network wider would cause model overﬁt. complexity inception modules signiﬁcant performance model. work rethink density method lempitsky zisserman instead predict counts redundant fashion order average errors reduce overﬁtting. redundant counting approach merges ideas segui networks count everything receptive ﬁeld ideas lempitsky zisserman using density objects together ideas arteta using fully convolutional processing. call approach count-ception approach utilizes counting network internally perform redundant counting. demonstrate approach outperforms existing approaches also perform well complicated cell structure even cell table comparison test mean absolute error counts image prior work. images dataset images randomly selected training validation ﬁxed size used testing set. least runs using different random splits different network initializations used calculate mean standard deviation. table comparison different strides order reduce redundant counting. results compared using mean absolute error output predictions. experiments examples. train test means stride value training testing. larger stride training means seeing less data. network trained seen times less data test case network trained evaluation test limited different strides less redundant predictions made. work partially funded grant u.s. national science foundation graduate research fellowship program institut valorisation donn´ees work utilized supercomputing facilities managed montreal institute learning algorithms nserc compute canada calcul queb´ec. also thank nvidia donating dgx- computer used work.", "year": 2017}