{"title": "Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in  the Blanks", "tag": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "abstract": "Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem, we develop a collaborative recurrent autoencoder (CRAE) which is a denoising recurrent autoencoder (DRAE) that models the generation of content sequences in the collaborative filtering (CF) setting. The model generalizes recent advances in recurrent deep learning from i.i.d. input to non-i.i.d. (CF-based) input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. To do this, we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting. The synergy between denoising and CF enables CRAE to make accurate recommendations while learning to fill in the blanks in sequences. Experiments on real-world datasets from different domains (CiteULike and Netflix) show that, by jointly modeling the order-aware generation of sequences for the content information and performing CF for the ratings, CRAE is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information.", "text": "hybrid methods utilize content rating information commonly used many recommender systems. however either handcrafted features bag-of-words representation surrogate content information neither effective natural enough. address problem develop collaborative recurrent autoencoder denoising recurrent autoencoder models generation content sequences collaborative ﬁltering setting. model generalizes recent advances recurrent deep learning i.i.d. input non-i.i.d. input provides denoising scheme along novel learnable pooling scheme recurrent autoencoder. this ﬁrst develop hierarchical bayesian model drae generalize setting. synergy denoising enables crae make accurate recommendations learning blanks sequences. experiments real-world datasets different domains show that jointly modeling order-aware generation sequences content information performing ratings crae able signiﬁcantly outperform state recommendation task based ratings sequence generation task based content information. high prevalence abundance internet services recommender systems becoming increasingly important attract users help users make effective information available. companies like netﬂix using recommender systems extensively target users promote products. existing methods recommender systems roughly categorized three classes content-based methods user proﬁles product descriptions only collaborative ﬁltering based methods ratings only hybrid methods make both. hybrid methods using types information best worlds result usually outperform content-based cf-based methods. among hybrid methods collaborative topic regression proposed integrate topic model probabilistic matrix factorization appealing method produces promising interpretable results. however uses bag-of-words representation ignores order words local context around word provide valuable information learning article representation word embeddings. deep learning models like convolutional neural networks layers sliding windows potential capturing order local context words. however kernel size ﬁxed training. achieve good enough performance sometimes ensemble multiple cnns different kernel sizes used. natural adaptive modeling text sequences would gated recurrent neural network models gated takes word time lets learned gates decide whether incorporate forget word. intuitively generalize gated rnns setting jointly model generation sequences relationship items users recommendation performance could signiﬁcantly boosted. nevertheless attempts made develop feedforward deep learning models alone recurrent ones. partially fact deep learning models like many machine learning models assume i.i.d. inputs. restricted boltzmann machines instead conventional matrix factorization formulation perform although methods involve deep learning actually belong cf-based methods incorporate content information like crucial accurate recommendation. uses low-rank last weight layer deep network reduce number parameters classiﬁcation instead recommendation tasks. also nice explorations music recommendation deep belief network directly used content-based recommendation. however models deterministic less robust since noise explicitly modeled. besides directly linked ratings making performance suffer greatly ratings sparse shown later experiments. recently collaborative deep learning proposed probabilistic model joint learning probabilistic stacked denoising autoencoder collaborative ﬁltering. however feedforward model uses bag-of-words input model order-aware generation sequences. consequently model would inferior recommendation performance capable generating sequences shown experiments. besides order-awareness another drawback lack robustness address problems propose hierarchical bayesian generative model called collaborative recurrent autoencoder jointly model order-aware generation sequences rating information setting. main contributions exploiting recurrent deep learning collaboratively crae able sophisticatedly model generation items extracting implicit relationship items design novel pooling scheme pooling variable-length sequences ﬁxed-length vectors also propose denoising scheme effectively avoid overﬁtting. besides recommendation crae also used generate sequences best knowledge crae ﬁrst model bridges especially respect hybrid methods recommender systems. besides bayesian nature also enables crae seamlessly incorporate auxiliary information boost performance. extensive experiments real-world datasets different domains show crae similar recommendation task considered paper takes implicit feedback training test data. items dataset. item corresponding sequence consisting words vector speciﬁes t-th word using -of-s representation i.e. vector length value element corresponding word elements. vocabulary size dataset. deﬁne i-by-j binary rating matrix denotes number users. example citeulike dataset user article personal library otherwise. given ratings corresponding sequences words problem predict ratings refers following sections concatenation -dimensional column vectors. input weights dimensionality -by-kw output state recurrent weights cell state gate units wildcard marks sentence. used place followed introduction concepts wildcard denoising beta-pooling model. that generative process crae provided show generalize hierarchical bayesian model i.i.d. setting setting. problem models like long short-term memory networks computation deterministic without taking noise account means robust especially insufﬁcient training data. address robustness problem propose type noisy gated rnn. gates latent variables designed incorporate noise making model robust. note unlike noise directly propagated back forth network without need using separate neural networks approximate distributions latent variables. much efﬁcient easier implement. provide generative process rrn. using index words sequence word embedding t-th word -by-s word embedding matrix -of-s representation mentioned above stands element-wise product operation vectors denotes sigmoid function cell state t-th word denote biases input weights recurrent weights respectively. forget gate units input gate units equation drawn gaussian distributions depending corresponding weights biases information processed sequence contained cell states output states column vectors length note seen generalized bayesian version lstm similar rrns concatenated form encoder-decoder architecture. since input output identical here unlike input source language output target language naive autoencoder suffer serious overﬁtting even taking noise account reversing sequence order improve recommendation performance). natural handling borrow ideas denoising autoencoder randomly dropping words encoder. unfortunately directly dropping words mislead learning transition words. example drop word ‘is’ sentence ‘this good idea’ encoder wrongly learn subsequence ‘this never appears grammatically correct sentence. propose another denoising scheme called wildcard denoising special word ‘wildcard’ added vocabulary randomly select words replace ‘wildcard’. encoder take ‘this wildcard good idea’ input successfully avoid learning wrong subsequences. call denoising recurrent autoencoder note word ‘wildcard’ also corresponding word embedding. intuitively wildcard denoising autoencoder learns blanks sentences automatically. denoising scheme much better naive one. example dataset citeulike wildcard denoising provide relative accuracy boost autoencoders would produce representation vector input word. order facilitate factorization rating matrix need pool sequence vectors single vector ﬁxed length encoded k-dimensional vector. natural weighted average vectors. unfortunately different sequences need weights different size. example pooling sequence vectors needs weight vector entries pooling sequence vectors needs entries. words need weight vector variable length pooling scheme. tackle problem propose beta distribution. vectors pooled single vector area range x-axis probability density function beta distribution beta pooling weight. resulting pooling weight vector becomes since total area always x-axis bounded beta distribution perfect type variable-length pooling hyperparameters equivalent average pooling. large enough peak slightly left means last time step encoder directly used pooling result. parameters beta-pooling able pool vectors ﬂexibly enough without risk overﬁtting data. perparameters conﬁdence parameter note goes inﬁnity gaussian distribution become dirac delta distribution centered mean. compression decompression like bottleneck bayesian rrns. purpose reduce overﬁtting provide necessary nonlinear transformation perform dimensionality reduction obtain compact ﬁnal representation graphical model example crae shown figure )}t) equation result beta-pooling hyperparameters denote cumulative distribution function beta distribution b))φt. please section supplementary materials details beta-pooling. generative process crae bayesian deep learning models perception component task-speciﬁc component. according crae model above parameters like treated random variables full bayesian treatment methods based variational approximation used. however extreme nonlinearity setting kind treatment non-trivial. besides primary baselines would fairer maximum posteriori estimates end-to-end joint learning maximization posterior probability equivalent maximizing joint log-likelihood {ui} {vj} {θj} {γj} given corresponds prior likelihood terms drae involving {θj} simplicity computational efﬁciency hyperparameters beta-pooling beta peaks slightly left leads tanh further approaches inﬁnity terms vanish become tanh )+b). figure shows graphical model degenerated crae approaches positive inﬁnity learning degenerated version crae equivalent jointly training wildcard denoising encoding coupled rating matrix. crae degenerate two-step model representation learned drae directly used contrary decoder essentially vanishes. extreme cases greatly degrade predictive performance shown experiments. robust nonlinearity distributions different nonlinear transformation performed adding noise precision equation case input nonlinear transformation distribution rather deterministic value making nonlinearity robust leading efﬁcient direct learning algorithms cdl. consider univariate gaussian distribution sigmoid function +exp expectation superscript dropped. overlines denote mean distribution hidden variable drawn. applying equation recursively compute similar approximation used tanh equation since tanh feedforward computation drae would seamlessly chained together leading efﬁcient learning algorithms layer-wise algorithms learning parameters learn block coordinate ascent used. given current compute tanh )}t) following update rules column vector containing ratings user given learned using back-propagation algorithm according equation generative process section alternating update gives local optimum learned predict ratings section report experiments real-world datasets different domains evaluate capabilities recommendation automatic generation missing sequences. datasets different real-world domains. citeulike users items netﬂix consists users movies ratings removing users less positive ratings ratings larger regarded positive ratings). please section supplementary materials details. recommendation recommendation task similar items associated user randomly selected form training rest used test set. evaluate models ratings different degrees density value repeat evaluation times different training sets report average performance. following recall performance measure since ratings form implicit feedback speciﬁcally zero entry fact user interested item user aware existence. thus precision suitable performance measure. sort predicted ratings candidate items recommend items target user. recallm user deﬁned figure performance comparison crae deepmusic svdfeature based recallm datasets citeulike netﬂix. varied ﬁrst ﬁgures. also another evaluation metric mean average precision experiments. exactly cutoff point user. sequence generation sequence generation task terms content information randomly select items include content training set. trained models used predict content sequences items. bleu score used evaluate quality generation. compute bleu score citeulike titles training sentences titles sentences abstracts articles used reference sentences. netﬂix ﬁrst sentences plots used training sentences. movie names sentences plots used reference sentences. higher bleu score indicates higher quality sequence generation. since cannot generate sequences directly nearest neighborhood based approach used resulting note task extremely difﬁcult sequences test unknown training testing phases. reason task impossible existing machine translation models like collective matrix factorization model incorporating different sources svdfeature svdfeature model feature-based collaborative ﬁltering. deepmusic deepmusic feedforward model music recommendation mentioned collaborative topic regression model performing topic modeling collaborative deep learning proposed probabilistic feedforward crae collaborative recurrent autoencoder proposed recurrent model. jointly experiments -fold cross validation optimal hyperparameters crae baselines. crae wildcard denoising rate section supplementary materials details. recommendation ﬁrst plots figure show recallm datasets varied outperforms baselines except cdl. note previously mentioned datasets deepmusic suffers badly overﬁtting rating matrix extremely sparse achieves comparable performance rating matrix dense strongest baseline consistently outperforms baselines. jointly learning order-aware generation content performing collaborative ﬁltering crae able outperform baselines margin citeulike netﬂix. note since standard deviation minimal included ﬁgures tables avoid clutter. last plots figure show recallm citeulike netﬂix varies shown plots performance deepmusic svdfeature similar setting. crae able outperform baselines large margin margin gets larger increase shown figure table also investigate effect beta-pooling drae temporal average pooling performs poorly information concentrates near bottleneck; right bottleneck contains information left. please section supplementary materials details. another evaluation metric table compares different models based map. compared crae provide relative boost citeulike netﬂix respectively. besides quantitative comparison qualitative comparison crae provided section supplementary materials. terms time cost needs epochs crae needs epochs optimal performance. sequence generation evaluate ability sequence generation compute bleu score sequences generated different models. mentioned section task impossible existing machine translation models like lack source sequences. table crae achieves bleu score citeulike netﬂix much higher pmf. incorporating content information learning user item latent vectors able outperform baselines crae boost bleu score sophisticatedly jointly modeling generation sequences ratings. note although able outperform baselines recommendation task performs poorly generating sequences demonstrates importance modeling sequence recurrently whole rather separate words. develop collaborative recurrent autoencoder sophisticatedly model generation item sequences extracting implicit relationship items design pooling scheme pooling variable-length sequences propose wildcard denoising scheme effectively avoid overﬁtting. best knowledge crae ﬁrst model bridge extensive experiments show crae signiﬁcantly outperform state-of-the-art methods recommendation sequence generation tasks. bayesian nature crae easily generalized seamlessly incorporate auxiliary information accuracy boost. moreover multiple bayesian recurrent layers stacked together increase representation power. besides making recommendations guessing sequences wildcard denoising recurrent autoencoder also potential solve challenging problems recovering blurred words ancient documents. )}t) result beta-pooling. cumulative mentioned paper distribution function beta distribution ta−b−dt incomplete beta function denominator gamma function also called regularized incomplete beta function. denote )φt. written evaluate gradient respect gradient-based methods learn them. illustrate clearly take positive inﬁnity learn optimal value maximize following joint log-likelihood note denotes cross-entropy loss generating words mult bg)). term corresponds prior weights biases. using property regularized incomplete beta function joint log-likelihood simpliﬁed order gain better insight crae train crae sparsest setting dataset citeulike recommend articles example users. corresponding articles target users training recommended articles shown table note sparsest setting recommendation task extremely challenging since single article user training set. crae successfully identiﬁed user researcher working information retrieval interest user modeling using user feedback. consequently crae achieves high precision focusing recommendations articles information retrieval user modeling relevance feedback. hand topics articles recommended span visual tracking bioinformatics programming language possible reason uses bag-of-words representation input consider word separately without taking account local context words. example looking cdl’s recommendations closely article article actually irrelevant training article ‘bayesian adaptive user proﬁling explicit implicit feedback’. probably recommends article word ‘proﬁles’ title overlaps article training set. thing happens article word ‘bayesian’. recurrent learning crae sequence modeled whole instead separate words. result local context word taken consideration crae recognize whole phrase ‘user proﬁling’ rather ‘user’ ‘proﬁling’ theme article. bayesian adaptive user proﬁling explicit implicit feedback user incorporating user search behavior relevance feedback query chains learning rank implicit feedback implicit feedback inferring user preference bibliography modeling user rating proﬁles collaborative ﬁltering improving retrieval performance relevance feedback language models relevance feedback context-sensitive information retrieval using implicit feedback implicit user modeling personalized search model-based feedback language modeling approach information retrieval user language model collaborative personalized search user implicit feedback inferring user preference bibliography seeing stars exploiting class relationships sentiment categorization respect rating scales knowledge-based approach interpreting genome-wide expression proﬁles tutorial particle ﬁlters online non-linear/non-gaussian bayesian tracking query chains learning rank implicit feedback mapreduce simpliﬁed data processing large clusters correlating user proﬁles multiple folksonomies evolving object-oriented designs refactorings trapping neutral sodium atoms radiation pressure scheme efﬁcient quantum computation linear optics taxonomy trust categorizing reputation systems user effects positive reputation systems trust recommender systems trust metrics recommender systems structure collaborative tagging systems effects energy policies industry expansion renewable energy limited reputation sharing systems survey wireless indoor positioning techniques systems design coordination distributed environments using virtual reality systems propagation trust distrust physiological measures presence stressful virtual environments user trust recommender systems position paper tagging taxonomy flickr article toread taxonomy workﬂow management systems grid computing usage patterns collaborative tagging systems semantic blogging decentralized knowledge management flickr recommendation based collective knowledge delivering real-world ubiquitous location systems shilling recommender systems proﬁt privacy risks recommender systems probabilistic reasoning intelligent systems networks plausible inference similar phenomenon found user article ‘taxonomy trust categorizing reputation systems’. cdl’s recommendations single word ‘systems’ crae identiﬁed article trust propagation words ‘trust’ ‘pp’. crae achieves precision cdl’s precision note hyperparameters here. generalized setting learned automatically. essentially motivation beta-pooling handle variable length different sequences using uniﬁed distribution. beta-pooling close average pooling larger weights left center following generative process output cell states word concatenated timesteps beta-pooled vector length vector encoded vector length used guide rating matrix. since information ﬂows ways rating matrix return provide useful information wildcard denoising recurrent autoencoder tries learn blanks. two-way interaction enables tasks beneﬁt results effective representation item. note compression layer beta-pooling share weights biases. hyperparameters beta-pooling ﬁxed beta peaks slightly left generation generative process equivalent directly setting tanh compressed representation compression layer. example beta peaks slightly left time step interacts rating matrix encoded connected item latent vector mentioned paper beta-pooling able pool sequence vectors single vector size. note vary different hyperparameters control behavior beta-pooling. beta-pooling equivalent temporal average pooling takes average vectors. extreme case pooling result equal vectors figure shows shape beta distribution different table shows corresponding recall different beta distributions citeulike. average pooling figure pooling inverted bell curve figure perform poorly. hand distributions figure yield highest accuracy means information concentrates near bottleneck drae. among them distributions figure outperform figure shows simply setting pooling result middle vector good enough aggregation vectors near middle would better choice. comparing distributions figure seen latter slightly outperforms former probably input words decoder part drae makes hidden cell states decoder part representative. similar phenomena happen figure note since crae joint model information ﬂows ways beta-pooling. example item representations used recommendation mostly come cell output states near bottleneck return rating information affects learning drae mainly cell output states near bottleneck. mentioned paper wildcard denoising scheme citeulike crae performs best wildcard denoising rate achieving recall number crae conventional denoising reference recall crae without denoising similar phenomena found netﬂix. hyperparameter settings vocabulary size wildcard included) citeulike netﬂix respectively. svdfeature optimal regularization hyperparameters used different learning rate svdfeature. deepmusic best performance achieved using convolutional layers. achieve good prediction performance similar hyperparameters mentioned denoising rate dropout rate using validation sets. sequence generation task postprocess generated sequences deleting consecutive repeated words often done rnn-based sentence generation models. figure shows recallm citeulike mentioned paper crae degenerates two-step model joint learning content sequences ratings. decoder side crae essentially vanish. apparently performance suffers extremes shows effectiveness joint learning full crae model. different nonlinear transformation performed adding noise precision case input nonlinear transformation distribution rather deterministic value making nonlinearity robust leading efﬁcient direct learning algorithms cdl. consider univariate gaussian distribution expectation superscript dropped clarity. overlines denote mean distribution hidden variable drawn. applying equation recursively compute similarly since tanh have could used approximate feedforward computation drae would seamlessly chained together leading efﬁcient learning algorithms layer-wise algorithms datasets different real-world domains citeulike netﬂix. ﬁrst dataset citeulike users items titles articles used content information model. second dataset netﬂix consists movie ratings users plots movies. removing users less positive ratings ratings larger regarded positive ratings) movies without plots users movies ratings ﬁnal dataset.", "year": 2016}