{"title": "Adversarial Examples for Semantic Image Segmentation", "tag": ["stat.ML", "cs.CR", "cs.CV", "cs.LG", "cs.NE"], "abstract": "Machine learning methods in general and Deep Neural Networks in particular have shown to be vulnerable to adversarial perturbations. So far this phenomenon has mainly been studied in the context of whole-image classification. In this contribution, we analyse how adversarial perturbations can affect the task of semantic segmentation. We show how existing adversarial attackers can be transferred to this task and that it is possible to create imperceptible adversarial perturbations that lead a deep network to misclassify almost all pixels of a chosen class while leaving network prediction nearly unchanged outside this class.", "text": "volker fischer mummadi chaithanya kumar hendrik metzen thomas brox bosch center artiﬁcial intelligence robert bosch gmbh university freiburg {volker.fischer janhendrik.metzen}de.bosch.com chaithugmail.com; broxcs.uni-freiburg.de machine learning methods general deep neural networks particular shown vulnerable adversarial perturbations. phenomenon mainly studied context whole-image classiﬁcation. contribution analyse adversarial perturbations affect task semantic segmentation. show existing adversarial attackers transferred task possible create imperceptible adversarial perturbations lead deep network misclassify almost pixels chosen class leaving network prediction nearly unchanged outside class. machine learning methods particular deep neural networks signiﬁcant performance increases numerous tasks several studies found vulnerable adversarial attackers attackers compute perturbed versions input data fool classiﬁer change predictions version perturbations stay almost imperceptible human eye. work adversarial examples focusses task image classiﬁcation. paper investigate effect adversarial attacks localization task particular semantic segmentation. work uses common deep neural network semantic segmentation evaluates extent task vulnerable adversarial examples. introduce adversarial examples could deﬁned semantic segmentation show existing methods create adversarial perturbations adapted semantic segmentation. basis cityscapes dataset easily possible deep network misclassify entire regions image replacing misclassiﬁed areas convenient substitution. finally introduce measure analyse effectiveness given attacker semantic segmentation network. generating adversarial examples denote function semantic segmentation deep neural network parameters input image size ground-truth label ytrue. semantic segmentation tasks assume ytrue pixel dimensions image times number classes. also denote jcls classiﬁcation loss assume differentiable respect since adversarial examples discovered szegedy several methods introduced generate adversarial perturbations scope work focus so-called least likely method presented kurakin despite name method restricted target least-likely class contrast approaches allows explicitly specify target class want classiﬁer predict. method easily transferred single pixel entire image. figure adversarial example task semantic segmentation original cityscapes image. adversarial example computed described noise restricted person pixels only. adversarial noise. adversarial noise restricted person pixels. adversarial target. network prediction adversarial example. network prediction adv. example restricted noise. network prediction original image. classiﬁcation difference classiﬁcation difference denotes maximum l∞-norm single step-size iteration. hence changing pixel maximally iteration. suggested kurakin number iterations adversarial perturbations evaluated different function clipε clips values intervall deﬁnition adversarial target semantic segmentation approach adversarial target covers entire image pixels speciﬁc class changed towards classes. look natural suggest choose target class ytarget replacing class nearest neighbor different pixels class original network prediction ytarget measure effect adversarial examples percentage pixels chosen class changed percentage background pixels preserved. want fool classiﬁer relative actual prediction ground-truth generate target label ytarget basis network’s actual prediction unaltered image. figure mean standard deviation cityscapes validation dataset percentages preserved background fooled person pixels different noise applied entire image restricted person pixels experiments fully convolutional network architecture introduced long model. trained version achieving intersection union network training evaluation well generation evaluation adversarial examples done downscaled cityscapes dataset task hiding pixels class chose hide person pixels cityscapes validation dataset. described above adversarial target labels ytarget created ﬁrst ﬁxating non-person pixels second replacing person pixels nearest-neighbor non-person class target label labels created network predictions cityscapes validation images used compute adversarial variations images described above. used different values investigating effect different magnitudes adversarial perturbations. fig. representative example shown possible fool deep network almost recognising person pixels predicting substituted background preserving background outside original persons nearly perfectly. also adversarial perturbation hard detect. finally restricted adversarial perturbations computed applied person class pixels network’s prediction original image. right column fig. shows example case. majority person pixels cloaked preserving background. indicates especially noise humans important hide them. fig. shows mean standard deviation percentages person pixels changed background pixels preserved different left plot fig. adversarial perturbations applied entire image right person class pixels. ﬁrst case sufﬁciently person pixels could hidden background pixels preserved. considering case adversarial noise restricted person pixels background preserved even smaller number cloaked person pixels decreases small recovers larger values adapted concept adversarial examples task semantic segmentation showed existing approaches generate adversarial examples classiﬁcation easily transferred task. showed exist imperceptible adversarial perturbations cloak almost pixels target class leaving classes across image nearly unchanged. many open topics remain usage performant networks comparison different network architectures sophisticated methods measure effectiveness adversarial examples semantic segmentation adversarial examples applied physical world? references marius cordts mohamed omran sebastian ramos timo rehfeld markus enzweiler rodrigo benenson franke stefan roth bernt schiele. cityscapes dataset semantic urban scene understanding. proc. ieee conference computer vision pattern recognition jonathan long evan shelhamer trevor darrell. fully convolutional networks semantic segmentation. ieee conference computer vision pattern recognition christian szegedy wojciech zaremba ilya sutskever joan bruna dumitru erhan goodfellow fergus. intriguing properties neural networks. international conference learning representations", "year": 2017}