{"title": "Learning Longer-term Dependencies in RNNs with Auxiliary Losses", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Despite recent advances in training recurrent neural networks (RNNs), capturing long-term dependencies in sequences remains a fundamental challenge. Most approaches use backpropagation through time (BPTT), which is difficult to scale to very long sequences. This paper proposes a simple method that improves the ability to capture long term dependencies in RNNs by adding an unsupervised auxiliary loss to the original objective. This auxiliary loss forces RNNs to either reconstruct previous events or predict next events in a sequence, making truncated backpropagation feasible for long sequences and also improving full BPTT. We evaluate our method on a variety of settings, including pixel-by-pixel image classification with sequence lengths up to 16\\,000, and a real document classification benchmark. Our results highlight good performance and resource efficiency of this approach over competitive baselines, including other recurrent models and a comparable sized Transformer. Further analyses reveal beneficial effects of the auxiliary loss on optimization and regularization, as well as extreme cases where there is little to no backpropagation.", "text": "several promising approaches proposed alleviate aforementioned problems. first instead using vanilla recurrent network long short-term memory designed improve gradient recurrent networks. addition also gradient clipping stabilize training lstm. finally reduce memory requirement either store hidden states periodically truncated bptt synthetic gradients convolutional neural networks also mitigate problem long-term dependencies since large kernel sizes deep networks resnets allow long-term dependencies learnt across distant parts image. however fundamentally different kind architecture tradeoffs. example entire input intermediate activations model must stored memory training. inference time typical cnns also need storage size input. transformer similar issue though somewhat magniﬁed since computation training inference requires randespite recent advances training recurrent neural networks capturing long-term dependencies sequences remains fundamental challenge. approaches backpropagation time difﬁcult scale long sequences. paper proposes simple method improves ability capture long term dependencies rnns adding unsupervised auxiliary loss original objective. auxiliary loss forces rnns either reconstruct previous events predict next events sequence making truncated backpropagation feasible long sequences also improving full bptt. evaluate method variety settings including pixel-by-pixel image classiﬁcation sequence lengths real document classiﬁcation benchmark. results highlight good performance resource efﬁciency approach competitive baselines including recurrent models comparable sized transformer. analyses reveal beneﬁcial effects auxiliary loss optimization regularization well extreme cases little backpropagation. many important applications artiﬁcial intelligence require understanding long term dependencies events sequence. example natural language processing sometimes necessary understand relationships distant events described book answer questions typically achieved gradient descent bptt recurrent networks. learning long term dependencies gradient descent however difﬁcult gradients computed bptt tend vanish explode training additionally bptt work needs store intermediate hidden states figure overview methods. random anchor point build auxiliary loss position. left predict random subsequence occurs inserted decoder network start reconstruction optionally fed. right predict subsequence stacking auxiliary rnns main one. gradients auxiliary loss truncated cases keep overall cost bptt constant. rnns therefore advantage assuming ﬁxed bptt length training requires storage. commonly case training language models dataset state never reset entire million token sequence. therefore theory learn relationships across extremely long distance. furthermore inference rnns also requires storage since rnns need ‘look back’. paper propose orthogonal technique further address weakness recurrent networks purely relying bptt. technique introduces unsupervised auxiliary loss main supervised loss reconstructs/predicts random segment sequence before/after anchor point. enables learning need bptt steps supervised loss. results show unsupervised auxiliary losses signiﬁcantly improve optimization generalization lstms. moreover using technique perform lengthy bptt training obtain good results. method therefore lends long sequences vanishing/exploding gradients well cost lengthy bptt become critical bottlenecks. experiments sequences elements processed lstms auxiliary losses train much faster less memory usage training lstms full backprop becomes difﬁcult. learning long term dependencies recurrent networks important problem machine learning many approaches proposed tackle challenge. well known approaches include recurrent long shortterm memory networks gated recur multiplicative units specialized kingma identity initialization connections highway connections orthogonal; henaff arjovsky dilated convolutions connections attention mechanisms recent approach skip input information certain steps training long recurrent networks memory-demanding many techniques also proposed tackle problem propose methods orthogonal approaches used combination improve rnns. work inspired recent approaches pretraining recurrent networks sequence autoencoders language models. work however focuses short sequences using pretraining improve generalization short recurrent networks. contrast work focuses longer sequences studies effects auxiliary losses learning long term dependencies. combining auxiliary losses truncated bptt also described context online learning main network learns predict concatenation next input token target vector distilled knowledge auxiliary network. auxiliary network predicts sequence tokens introducing auxiliary losses subsequences input inevitably introduces extra hyper-parameters. indicates frequently sample reconstruction segments others indicate long segment denoting former later {li}n sequences characters tokenlosst crossentropy loss ground truth one-hot vector predicted distribution vocabulary produced decoder network. types input treat token continuous multi-dimensional real vector perform distance minimization. tuning hyper-parameters known expensive especially training rnns long sequences. therefore sampled segments length sample frequency experiments. tuning hyper-parameters also explored cases sequence length relatively short. later experiments show tuned values generalize well much longer input sequences. evaluate effectiveness models consider wide variety datasets sequences varying lengths ﬁrst benchmark pixel-by-pixel image classiﬁcation task mnist pixels image recurrent model sequentially prediction made. dataset proposed become popular benchmark testing long term dependency learning. overview methods shown figure suppose goal recurrent network read sequence classify propose randomly sample multiple anchor positions insert unsupervised auxiliary loss them. reconstructing past events sample subsequence before anchor point insert ﬁrst token subsequence decoder network; decoder network predict rest subsequence. whole process illustrated figure -left. intuition events predicted close enough anchor point number bptt steps needed decoder reconstruct past events quite small. furthermore training anchor points serve temporary memory recurrent network remember past events sequence. choose enough anchor points memory built sequence reach sequence classiﬁer remembers enough sequence good classifying consequently classiﬁer needs backpropagation steps ﬁne-tune lstm’s weights since good embeddings input sequence learnt optimizing auxiliary objective. another auxiliary loss consideration analogous language modelling loss illustrated figure -right. case decoder network predict next token given current sequentially subsequence starting anchor point. type unsupervised auxiliary loss ﬁrst examined applied whole input sequence. experiments however interested scalable schemes learning long term dependencies therefore apply loss subsequence random anchor point. name former method r-lstm later p-lstm train phases. ﬁrst pure unsupervised pretraining auxiliary loss minimized. second phase semi-supervised learning performed minimize main objeccreates even complex dependencies across various time scales. test methods larger dataset include pixel-by-pixel cifar additionally perform control experiments several scales sequence lengths stanforddogs dataset contains large images categorized breeds. images scaled different sizes ﬂattened sequences pixels without permutation. setup results sequences lengths times longer previously used benchmark ﬂavor. lastly explore well truncated bptt auxiliary losses perform real language task previous variants already reported remarkable accuracy. task dbpedia character level classiﬁcation task chosen large-scale dataset well benchmarked follow procedure suggested zhang normalize dataset. single-layer lstm cells embedding size read input sequence. supervised loss ﬁnal state main lstm passed two-layer feedforward network hidden units making prediction. apply dropconnect probability second layer. auxiliary losses two-layer lstm bottom layer initialized current state main classiﬁcation lstm starts zero state. reconstructing image pixels two-layer applied auxiliary lstm timestep. batch size mizer unsupervised pretraining done epochs initial learning rate halved halfway pretraining. semi-supervised phase learning rate halved every epochs training reaches epochs. scheduled sampling auxiliary lstms annealed linearly zero training steps. scale methods various input lengths make sure backpropagation cost constant regardless input length. speciﬁcally gradients truncated time steps supervised auxiliary losses. auxiliary losses choose simplest setup sampling segment length training example. section explore different values complement results purely recurrent models section also compare models transformer transformer completely different paradigm processing sequences sidesteps difﬁculties bptt self-attention. advantage achieved cost working memory training inference compared rnns. even though main interest improve recurrent models include results study scalable self-attention mechanism tensortensor train transformer models off-the-shelf conﬁguration comparable number parameters rnns simple setting classiﬁcation adopted transformer output vectors average-pooled two-layer making predictions done rnns. ﬁrst explore sequences length longer mnist pmnist cifar. besides results previous works pixel mnist permuted mnist arjovsky evaluated fully trained lstm lstm trained steps bptt main baselines much disadvantage truncating classiﬁcation gradients might cause. stage also affordable include test accuracies truncated fully-trained r-lstm p-lstm complete result. figure -top report test accuracy fully backpropagated lstm baseline r-lstm p-lstm levels. since stanforddogs even challenging classiﬁcation problem compared cifar pursuing useful accuracy non-convolutional models main goal. instead examine relative robustness different methods input sequences longer. models evaluated epochs training additional epochs pretraining models auxiliary loss. using unsupervised auxiliary losses able obtain much better results compared methods. figure -top shows r-lstm p-lstm exhibit strongest resistance growing difﬁculty lstm trained full backpropagation slow improve produces better random predictions input sequence length reaches mark. mark memory constraint exceeded model. time virtually accuracy loss r-lstm going element long sequences. gradient truncation r-lstm p-lstm also offers much greater computational advantage sequence length gets arbitrarily large. figure -bottom illustrates time ﬁnish training step recurrent model. lstm takes seconds mark quickly stretches seconds mark. iary losses produce gradually larger improvements moving mnist pmnist cifar. pixel-by-pixel mnist truncated lstm baseline nearly untrainable accuracy. fact gradients back-propagated loss reach largely non-informative solid pixels near sequence. despite detrimental effect gradient truncation proposed unsupervised losses bring r-lstm p-lstm fully trained rnns like urnn lstm. permuted pmnist complex long-range dependencies test r-lstm p-lstm easily outperform fully trained lstm baselines well fully trained urnn using less half number gradients classiﬁcation loss. fully backpropagated gradients classiﬁcation loss obtain best accuracy across datasets recurrent models. notably harder benchmarks pmnsit cifar r-lstm outperforms fully-trained lstm large margin. experiments give hints r-lstm plstm scale better performance input sequences longer complex. next present trend elaborates input sequences extend order magnitude higher steps. presented earlier dataset stanforddogs resized levels sequence lengths test models levels. range training expensive terms time computational resources especially lstms parallelization time dimension possible. therefore restrict training session amount resource report infeasible whenever mini-batch training example longer memory. experiments explore well proposed recurrent models fare utilize selfattention mechanism. noted introduction models require random access entire sequence inference time quick become infeasible sequences longer mnist pmnist transformer outperforms best model shown table cifar however transformer performance drops signiﬁcantly worse recurrent models dataset. note proposed method orthogonal models process sequences. incorporating technique scalable transformer variant therefore likely result signiﬁcant improvements. current work however focuses improving recurrent networks therefore leaves option future exploration. explore well truncated bptt auxiliary losses sequences discrete data previous methods already reported remarkable accuracy. task dbpedia dataset chosen provides large carefully curated clean wikipedia texts duplication. experiments document dataset processed character level makes average sequence length training examples elements long. additionally evaluate results t-dmca though strictly speaking unfair comparison since t-dmca adds convolutions selfattention layer thus additional processing inference time. compared transformer t-dmca memory efﬁcient utilizes localattention memory-compressed attention. results indicate t-dmca performs better transformer mnist cifar pmnist spatial locality exploited convolution t-dmca achieves accuracy slightly worse transformer. similar previous section transfer hyper-parameter settings transformer much longer sequences created using stanforddogs dataset. shown figure transformer starts almost twice accuracy r-lstm p-lstm performance quickly degrades much higher rate input sequences longer. speciﬁcally transformer performs worse methods mark slightly better random prediction around mark. training using explore well auxiliary losses help limited backpropagation supervised gradients truncated time steps anchored subsequences sampled length similar perform joint-training since slightly degrades performance large dataset hyper-parameters reused. also test r-lstm -sample setting full bptt trained lstm baseline truncated lmlstm sa-lstm baselines seen table auxiliary losses truncated bptt signiﬁcantly outperform lstm baseline absolute accuracy. methods also better results truncated lm-lstm sa-lstm. conjecture comes combination randomness truncation training process. keep total cost backpropagation approximately previous experiments gradually increase shrink subsequence length proportionately. also unsupervised bptt truncation table report results obtained different sampling frequencies ranging samples. indeed observed accuracy gain test across almost frequencies sampling. interestingly peak samples sequence accuracy gain starts decaying point directions. words harmful sample little sample many cost little backpropagation. comparing extremes observe slightly better accuracy many small reconstructions reconstruction. sampling frequency single time step backpropagation obtain increase completely unsupervised training remarkable increase increase implies r-lstm great potential improve long sequences relatively supervised gradients long able afford tuning extra hyperparameters. explore potential stanforddogs retraining rlstm sampling frequency levels. shown figure -top best performing setting found cifar generalizes difﬁculty levels stanforddogs. namely r-lstm closes transformer shorter sequences stays position throughout outperforming recurrent models well transformer large margin starting mark furthermore independently sampling several segments equal length batch utilize data parallelism subsequently speed training process even more. illustrated figure -bottom singlebatch training time r-lstm consistently stays lower recurrent method. given clear trend demonstrated previous sections natural question much longer input grow r-lstm p-lstm becomes untrainable. simulate effect without growing sequence length indeﬁnitely instead keep input sequence length ﬁxed truncating backpropagation incrementally. perform experiments cifar start shrinking bptt length gradients classiﬁcation loss minimal impact main lstm. results figure shows r-lstm p-lstm afford reduction another bptt steps still able generalize better fully trained lstm. moreover applying gradients steps less total input steps r-lstm p-lstm’s accuracy still approximate fully trained counterpart. extreme point one-step backpropagation rlstm p-lstm perform commendably well. going step further prevent classiﬁcation gradients updating main lstm thereby training completely unsupervised. attempt address question human brain understand long sequences events even though bptt argued biologically implausible results figure indicate r-lstm p-lstm still classify unseen data far-from-random accuracy. adopt simplest setting reconstruction sample sequence. also tune hyper-parameter even better results. explore option improve zero step supervised bptt. signiﬁcant r-lstm/p-lstm fully-trained lstm almost benchmarks important understand why. particular whether regularization optimization advantage added truncated bptt auxiliary losses. point training identify optimization advantage training accuracy auxiliary losses much better baseline corresponding improvement test signiﬁcant. hand models generalize better harder insigniﬁcantly easier train improvement comes regularization. therefore conclude types pretraining bring optimization advantages early stages training. later minimizing semi-supervised loss creates regularization effect quickly takes end. section evaluate relative contribution different factors r-lstm’s performance. test factor turning retraining model scratch cifar using random seed. firstly reported table eliminating auxiliary loss leaving main lstm truncation bptt steps cause loss nearly test accuracy. auxiliary loss effect figure shows results turning parts original full setting. figure shows training/testing accuracy r-lstm p-lstm lstm training. r-lstm plstm training curves trace almost identically throughout r-lstm gives better result testing data. implies r-lstm regularizes better plstm. comparing lstm baseline r-lstm p-lstm start much higher training accuracy testing accuracy reveals signiﬁcant improvement unsupervised pretraining rlstm p-lstm’s optimization. gradually throughout training process optimization baseline becomes smaller corresponding difference test jointly training unsupervised supervised loss important corresponding loss reported. long joint training effect pretraining slightly less important applying scheduled sampling auxiliary lstms. randomness better. instead reconstructing immediate past allowing reconstruction segments randomly sampled distant past gives almost accuracy gain. allowing part sampled segment spread anchor point’s future also gives boost. improvements come embedding input pixels dimensionality lstm’s hidden size reversing order reconstruction stacking second layer lstm receives outputs anchor point. paper presented simple approach improve learning long-term dependencies rnns. auxiliary loss added main supervised loss offer main beneﬁts. first induces regularization effect allowing models generalize well long sequences length second provides computational advantages input sequence gets long needs backpropagate small number time steps obtain competitive performance. extreme cases little backpropagation models perform better random predictions. comprehensive benchmarks ranging pixelby-pixel image classiﬁcation character-level document classiﬁcation models demonstrated competitive performance strong recurrent baselines non-recurrent ones transformer. long sequences results superior despite using much fewer resources.", "year": 2018}