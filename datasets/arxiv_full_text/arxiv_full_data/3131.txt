{"title": "Fast, Robust and Non-convex Subspace Recovery", "tag": ["cs.LG", "cs.CV", "stat.AP", "stat.ML"], "abstract": "This work presents a fast and non-convex algorithm for robust subspace recovery. The data sets considered include inliers drawn around a low-dimensional subspace of a higher dimensional ambient space, and a possibly large portion of outliers that do not lie nearby this subspace. The proposed algorithm, which we refer to as Fast Median Subspace (FMS), is designed to robustly determine the underlying subspace of such data sets, while having lower computational complexity than existing methods. We prove convergence of the FMS iterates to a stationary point. Further, under a special model of data, FMS converges to a point which is near to the global minimum with overwhelming probability. Under this model, we show that the iteration complexity is globally bounded and locally $r$-linear. The latter theorem holds for any fixed fraction of outliers (less than 1) and any fixed positive distance between the limit point and the global minimum. Numerical experiments on synthetic and real data demonstrate its competitive speed and accuracy.", "text": "work presents fast non-convex algorithm robust subspace recovery. data sets considered include inliers drawn around low-dimensional subspace higher dimensional ambient space possibly large portion outliers nearby subspace. proposed algorithm refer fast median subspace designed robustly determine underlying subspace data sets lower computational complexity existing methods. prove convergence iterates stationary point. further special model data converges point near global minimum overwhelming probability. under model show iteration complexity globally bounded locally r-linear. latter theorem holds ﬁxed fraction outliers ﬁxed positive distance limit point global minimum. numerical experiments synthetic real data demonstrate competitive speed accuracy. modern data collected increasingly higher dimensions massive quantities. important method analyzing large high-dimensional data involves modeling low-dimensional subspace. projecting data subspace signiﬁcantly reduce dimension data capturing signiﬁcant information. classically problem principal component analysis ﬁnds subspace maximum variance. eﬃciently implemented moderate-size data using singular value decomposition data matrix. larger data recently developed random methods proved stable accurate fast despite impressive progress eﬀective algorithms underlying idea completely useless data corrupted. among many possible models corrupted data sets follow inliersoutliers corruption model. precisely assume data points sampled around d-dimensional subspace whereas rest sampled diﬀerent model. problem robust subspace recovery asks robustly estimate underlying low-dimensional subspace presence outliers. note problem distinct commonly referred robust recovering rank structure matrix sparse element-wise corruptions experience dictated robust algorithms tend perform poorly regime especially proportion outliers high. much recent work devoted developing numerically eﬃcient solutions problem. current batch formulations best comparable full complexity order number iterations till convergence number points ambient dimension. unaware suﬃciently accurate batch algorithms scale least like dimension approximated subspace. address void propose novel non-convex algorithm fast median subspace algorithm. computational cost order depends linearly empirically seems obtain smallest highest accuracy among algorithms comparable accuracy many cases computational cost iteration signiﬁcantly larger moderate high ambient dimensions). theoretical guarantees under model corrupted data empirical tests demonstrate merit algorithm. classic ubiquitous method data analysis since obtained data matrix enjoys wealth eﬃcient numerical methods. last decade various random methods proposed fast accurate computation singular vectors values example liberty demonstrated order randomized algorithm d-approximation pca; rokhlin combined random dimension reduction power method obtain algorithm complexity signiﬁcantly improved accuracy singular values decay suﬃciently fast. complexity state-of-the-art algorithms online best order however practice often large accuracy often competitive. ubiquitous subspace modeling without corruption still clear choice best algorithm. many strategies established last three decades recent developments mccoy tropp zhang lerman lerman zhang feng hardt moitra goes emphasis theoretical analysis algorithms quantifying largest percentage outliers studied algorithm suﬃciently accurate particular hardt moitra shown small expansion problem; also showed fraction achieved setting; though possible achieve better fraction special instances opposed algorithms lerman zhang lerman zhang hardt moitra algorithms accurate high percentage outliers. table summarizes theoretical bounds percentage inliers outliers required recovery. algorithms table asymptotically depend also depend variances inliers outliers. many successful algorithms involve minimizing energy robust outliers. example mccoy tropp zhang lerman lerman goes convex relaxations energy later formulated believe since targets original robust energy convex relaxation achieves higher accuracy possibly even faster convergence; however analysis diﬃcult non-convexity. tyler m-estimator minimizes possibly robust energy thus obtains competitive accuracy however cannot obtain suﬃciently competitive speed since requires full eigenvalue decomposition well initial dimensionality reduction onto subspace whose dimension order number points. many algorithms suﬃciently fast others also well justiﬁed theory. example hr-pca dhrpca quantify recovery expressed variance actual bounds seem weak. evident theorem gives asymptotic guarantees. consider case outliers drawn standard gaussian inliers drawn standard gaussian restricted subspace. shown lower bound amounts exact recovery. hand procedures hardt moitra involve robust energy minimization many diﬀerent subspaces success. suﬃciently fast unfamiliar truly competitive implementations them. online algorithms suﬀer problems online algorithms mentioned above. namely number iterations required quite large accuracy often competitive. important algorithm compare spherical spca involves performing data centered projected unit sphere. maronna determined spca method choice compared various algorithms further complexity running spca data faster multiplicative constant. tests indicate spca faster achieve competitive accuracy subspace recovery problems also worth noting couple recent works scale larger data previous algorithms. work adaptive compressive outlier sampling haupt viewed solution problem drastically reduced complexity depends many rows columns data selected. however eﬀective precisely identifying underlying subspace method stems fact builds outlier pursuit could compete accuracy algorithms many regimes test. another recent algorithm potential scale well work grassmann averages provided correct robust function µrob chosen. however grassmann averages lack sort theoretical justiﬁcation convergence robustness. similar algorithm explored wang proposed non-convex robust algorithm. although algorithm suited problem still relevant work fms. first borrowed wang argument proof convergence iterates stationary point second algorithm might viewed soft analog alternating least squares procedure finally many recent works analysis non-convex algorithms surprising eﬀectiveness problems structured data. examples include works zhang balzano jain dauphin keshavan boumal bandeira particular related work non-convex analysis related rank modeling jain hardt netrapalli jain netrapalli jain zhang balzano examples). analysis presents another example non-convex algorithm surprisingly accurate rank modeling despite potential issues non-convex optimization slow convergence convergence non-optimal point. also point reader work daubechies aims analyzing convergence irls method energy non-convex. although method diﬀerent problem strong similarity non-convex energies case case algorithm improves existing methods fast runtime state-of-the-art accuracy. however underlying minimization non-convex thus diﬃcult analyze. work contributes non-convex addition theory rely careful numerical experimentation believe results reported paper strongly indicate merit fms. algorithm displays competitive speed accuracy synthetic data sets. unlike algorithms also shows strong performance dimension reduction tool clustering data since scales well section presents algorithm. first presents basic notation used throughout paper. then describe underlying minimization problem robustness. next propose algorithm motivating heuristic way. finally summarizes complexity discusses choices parameters algorithm. seek approximating d-dimensional subspace short d-subspace. denote grassmannian manifold linear d-subspaces denote orthogonal projector onto view element rd×d. denote euclidean norm denote dist euclidean distance distance denote setting energy results natural robust extension since solution minimization thought geometric median subspace. even approximate minimization energy nontrivial since domain minimization aﬃne d-subspaces experience shows initial robust centering geometric median minimization successful. thus assume discussion data centered domain minimization important aware presence single outlier suﬃciently large magnitude minimizer fail approximate underlying subspace. case avoided normalizing centered data points according euclidean norms unit sphere surface rather natural robust minimization problem formally generalizes notion geometric median subspaces however minimization non-convex analyzed special assumptions data. particular lerman zhang showed spherically symmetric outliers spherically symmetric inliers within d-subspace asymptotic exact recovery possible even fraction outliers approaches similarly near recovery possible small amount noise. furthermore noiseless case outliers theory zhang lerman lerman imply theory directly extends abstract global minimizer regularized term follows appears later proof theorem solve weighted problem ﬁrst needs weight centered data points latter term apply compute lk+. ability directly apply equivalently scaled data matrix numerically attractive apply state-of-the-art suites rokhlin suﬃciently fast without sacriﬁcing accuracy). subspace span vectors. procedure iterated suﬃciently converges. formally call iterative procedure fast median subspace summarize algorithm words randomizedpca replaced exact convergence accuracy same randomizedpca results shorter runtime. also seems advantageous initialize result randomizedpca full data although initialization also done randomly. random initialization recommended though. example regime outliers another weaker subspace starting close outlier subspace result convergence local minimum rather global minimum. empirically initialization cases converges stronger subspace finally denote algorithm parameter fmsp remainder paper. iteration fmsp creates scaled data matrix centered data points takes operations although scaling done parallel. ﬁnds singular vectors scaled data matrix update subspace takes thus total complexity number iterations. empirically noticed treated relatively small constant. example special case theorem point. reduces iterates suﬃciently close limiting stationary point theorem storage fmsp algorithm involves data matrix weighted data matrix iteration. fmsp must also store bases subspaces although always diﬀerence cases choices performing better other. also appear advantage using currently theory seems support best rates recovery further theory seems also indicate smaller leads less robustness higher small numbers points choice small lead convergence non-optimal point still able converge believe optimized speciﬁc data given prior knowledge words chosen user designates training data truth known. complexity method possible eﬃciently array values thus speciﬁed training cross-validation used select proper value given type data although requires user prior knowledge data. choice noticed much diﬀerence diﬀerent values although certain cases necessary careful choice theorem rates asymptotic recovery fmsp algorithm special model data. theory seems point taking small possible take larger values further experiments smaller sample sizes indicate using parameter small lead convergence non-optimal point thus advocate choosing small possible care must taken ensure chosen small. finally select subspace dimension fmsp. picking correct subspace dimension well studied justiﬁed literature. heuristic strategies elbow method used guess best subspace dimension strategies usually require test range possible values hand domains prior knowledge example facial recognition type datasets images person’s face constant pose diﬀering illumination conditions approximately dimensional subspace practice advocate either using elbow method domain knowledge select best value since fmsp proposes iterative process rather important analyze formulate main convergence theorem convergence. establishes convergence fmsp stationary point next assumes data sampled certain distribution proves fmsp converges point near global minimum overwhelming probability. shown model overwhelming probability). proofs theorems left theorem sequence obtained applying fmsp without stopping data ﬁxed then converges stationary point accumulation points form continuum stationary points. proof theorem appears assumptions important discuss implications theorem. discuss possibility fmsp converges continuum stationary points. then discusses possibility convergence saddle point discusses convergence local minimum. theorem proves convergence fmsp iterates stationary point continuum stationary points. another think issue continuum stationary points also continuum ﬁxed points fmsp algorithm. desirable know algorithm converges single point versus continuum. however cannot rule continuum case also cannot construct example discrete data continuum stationary points rank data less subspace dimension rank data less subspaces containing data essentially equivalent respect data. conjecture actual convergence single stationary point data sets full rank. prove convergence stationary point able kind stationary point converge theory cannot rule saddle point unaware example saddle point also ﬁxed point fmsp. following example describes fmsp energy function line deﬁned saddle point fmsp energy since minimum along geodesic maximum along geodesic however ﬁxed point fmsp. suppose selected candidate subspace fmsp. then data points equidistant scaled amount. then done subspace scaled data lines along geodesic solutions thus selected probability line selected points longer equidistant points dominate next round pca. gives convergence either min. however shown overwhelming probability ﬁnite unlike potentially many local minima energy local minima also ﬁxed points fmsp algorithm. hence cannot guarantee fmsp converges optimal stationary point general since could converge local minima. example local minima discussed lerman zhang modiﬁed argument used show local minima still exist observe local minima generally occur points concentrate around lower dimensional subspaces. another simple example many local minima symmetric case suppose points symmetrically distributed even data consists points sin)t section show certain model probabilistic generation data fmsp algorithm nearly recovers underlying subspace overwhelming probability w.o.p. mean probability section proves global convergence special model data. simple version signiﬁcant subspace model outlined much notation concepts borrowed paper. general approximate recovery case however conjecture general theorem holds subspace dimension empirical performance algorithm data sets sampled distributions. signiﬁcant subspace within data set. construct mixture measure combining uniform distribution uniform distribution noisy case additive noise distribution supp sd−. also require moment smaller finally attach weights mixture assume data sampled independently identically points sampled represent pure outliers points sampled represent pure inliers. everything prove spherical model generalize spherically symmetric model outliers spherically symmetric symmetrically distributed less signiﬁcant subspaces inliers symmetrically distributed signiﬁcant subspace note practice normalizing latter distribution sphere yields mixture measure form proof theorem given theorem gives probabilistic near recovery result fmsp algorithm pca. note result given comparable asymptotic result vershynin albeit diﬀerent argument. result restricted though since result vershynin applies also estimates constants ideal however issue since interested contrasting dependence probabilities restriction although probability recovery depends fractions. bounds constants seen later respectively. worst case estimates given later proposition theorem shows beneﬁt using fmsp pca. following discussion follow default choice algorithm assume order machine precision. means need consider ﬁrst term within minimum function examining dependence probability bounds exponent formulation exponent fmsp exponent altogether means fmsp expected much precise recovery vastly smaller sample sizes. also advocate choosing running fmsp algorithm reason chosen small exponent leads much worse bound exponent another consequence theorem generally advocate larger values smaller values demonstrations phase transitions exhibited probability recovery figures again emphasize diﬀerence bounds probabilities values seems give robustness noise. theorem indicates best stability noise although estimates ideal. result stands contrast result shows higher robustness noise convex relaxation less robustness noise fmsp attributable non-convexity cannot make deﬁnitive statement fact. future plan follow coudron lerman establish stronger robustness noise fmsp least demonstrations phase transitions exhibited probability recovery noisy model figures another important setting signiﬁcant subspace model recover underlying subspace found hard prove anything general case hard characterize derivative general. however able prove near recovery case proven hard derive bounds closed form expressions constants probability bound present here. further similar stability result theorem holds theorem however display here. also important guarantees algorithms hardt moitra zhang lerman break setting. further although speciﬁc important example fmsp still able recover correct subspace presence another potential local minimum finally clear example cannot recover signiﬁcant subspace asymptotically fmsp can. algorithm. begin probabilistic global rate convergence bound fmsp algorithm models theorem theorem show near w.o.p. proof theorem given theorem suppose number iterations dist theorems combined give bound overall iteration complexity fmsp. general given choice number iterations required converge bounded section illustrate fmsp algorithm performs various synthetic real data sets matlab test environment interesting test fmsp value order compare various convex relaxations energy case. denotes case algorithm value certain cases noticed diﬀerence performance choosing lower values make clear case. places noted diﬀerence report fms. denote case parameter algorithms compare tyler m-estimator median k-ﬂats reaper r-pca robust online mirror descent also tried algorithms particular hr-pca dhr-pca outlier-pursuit competitive; thus report results. example hr-pca dhr-pca slower surprisingly worse many tests. even though r-md competitive either many cases important compare online algorithms. comparison r-pca also important since aims directly minimize addition compared principal component pursuit aims solve robust problem. code chosen comparison accelerated proximal gradient partial obtained http//perception.csl.illinois.edu/ matrix-rank/sample_code.html although similar results given codes emphasize robust methods designed regime sparse element-wise corruptions data matrix rather wholly corrupted data points consider paper. experiments demonstrate advantage scaling centered data point norm i.e. spherizing data point cases examine eﬀect spherizing data denote algorithm spherized data preﬁx example baseline many experiments compare spca performed using randomizedpca algorithm singular vectors spherized data. vein sometimes also compare sfmsp fmsp spherized data. spherizing seems reduce noise produce better subspace cases display results sfmsp spca case tyler m-estimator used regularization parameter median k-flats passes data many times single subspace dimension step size maximum number iterations reaper algorithm regularization parameter r-pca uses stopping parameter capped iterations. r-md passes iteration parameter data times uses step size series synthetic tests determine fmsp compares state-of-the-art algorithms. examples data sampled according variants needle-haystack model precisely inliers sampled gaussian distribution within random linear d-subspace outliers sampled gaussian distribution within ambient space. noise also added points. experiments fraction outliers restricted hardt moitra’s upper bound ﬁrst experiment demonstrates eﬀect percentage outliers total runtime error subspace recovery. denote orthogonal projector onto randomly selected subspace σout denote identity transformation experiment inliers drawn distribution outliers distribution scaling respectively ensures samples comparable magnitudes. error measured calculating distance found ground truth subspaces. ambient dimension ﬁxed subspace dimension total number points ﬁxed figure displays results recovery error total runtime versus percentage outliers data set. percentage value experiment repeated randomly generated data sets results averaged. note runtime r-md large displayed graph figure plots demonstrating accuracy total runtime subspace recovery algorithms versus percentage outliers data rsr). fms. obtain competitive time accuracy exception outliers. total runtime. fms. tyler m-estimator best accuracy data demonstrates problems high ends outlier percentages. robust methods fms. fastest interesting note ﬁgures fails lower percentages outliers; zhang lerman acknowledge recovery. authors advocate either initial dimensionality reduction addition synthetic outliers increase chances ﬁnding correct subspace. tests initial dimensionality reduction still competitive addition synthetic outliers results precise recovery second experiment displayed figure demonstrate total runtime superiority fmsp versus algorithms. figure runtime plotted function ambient dimension diﬀerent algorithms. figure corresponding errors runtimes given. total number points inliers outliers. subspace dimension ﬁxed variance model sampled ambient dimension varied method runtime seconds. plotted runtime averaged randomly generated data sets. robust online mirror descent shown figure large runtime required higher dimensions. data sets tested here reaper tyler m-estimator precisely found subspace ambient dimension. among these note best runtime higher ambient dimension lower complexity algorithms like reaper tyler scale nearly well. figure demonstration accuracy total runtime various subspace recovery algorithms. left ﬁgure displays total runtime varies versus ambient dimension. right ﬁgure shows corresponding recovery error ambient dimension. runtime experiments algorithm exceeded seconds. runtimes fms. superior existing methods. fms. tyler reaper achieve competitive accuracy data sets note algorithm especially slow data requiring large runtime even relatively dimensional data. runtimes calculated computer intel core cpu. remark fmsp faster tyler m-estimator also require initial dimensionality reduction required tyler m-estimator applied subspace recovery figure demonstrate accuracy achieved diﬀerent algorithms function evolving time. data points consisting inliers outliers ambient dimension subspace graph corresponds accuracy achieved given algorithm certain amount runtime passed. clearly fmsp fastest convergence existing algorithms achieves competitive accuracy matter seconds. shown large amount time required complete even iteration. figure demonstration error time evolves subspace recovery algorithms. marks appear iterations algorithms fms. demonstrate fastest convergence accurate subspace among existing methods. points average error randomly generated data sets. tyler m-estimator perfect performance across scale. fms. displayed identical performance fms. again fails outliers addition synthetic outliers leads better results ﬁgure takeaway tests fmsp algorithm oﬀers state-of-the-art accuracy synthetic data model complexity leads better runtimes high dimensions. models ﬁgures validate theorems states fmsp asymptotic recovery underlying subspace rate fmsp much better sample sizes varied. sample size data sets generated recovery error calculated distance found subspace underlying subspace plots value sample size percentage times recovery error less equal noiseless case displayed figures within ﬁgures fmsp shows clear advantage asymptotic rate recovery underlying subspace. even small numbers points fmsp approximate underlying subspace precision hand approximate subspace precision sample sizes large note fms. seem trouble around sample size indicates convergence non-optimal solution. earlier theory indicates possibility although mitigated larger choice precision lost larger values noisy case displayed figures within ﬁgures notice rate recovery change noiseless case. fmsp plots seems issues small numbers points. issues occur around slight issues noiseless case also. convergence fmsp nonoptimal solution theorem theorem again alleviated choosing larger values solutions precise. comparing versus larger appears figure percentage times η-accurate better solution given varying sample sizes. ratio inliers outliers data sets i.i.d. sampled added gaussian noise directional variance figure percentage times η-accurate better solution given varying sample sizes. ratio inliers outliers data sets i.i.d. sampled added gaussian noise directional variance figure percentage times η-accurate better solution given fms. varying sample sizes. ratio inliers outliers data sets i.i.d. sampled added gaussian noise directional variance real strengths reducing dimensionality comes denoising eﬀect. projection subspace long popular preprocessing step classiﬁcation clustering denoising eﬀect. cases robust algorithms seem demonstrate higher resistance noise data pca. ﬁrst experiments displayed show viability fmsp denoising. ﬁnish stylized experiment real data explicit outliers demonstrate accuracy fmsp demonstrates ability ﬁrst experiment demonstrating usefulness fmsp algorithm real data comes astronomy. goal experiment robustly locate eigenspectra large galaxy spectrum data. eigenspectra found used classiﬁcation galaxies within complete data since galaxy spectra decomposed projection onto span eigenspectra. budav´ari provide criteria determining makes resulting eigenspectra good. attribute good eigenspectra noisy themselves. would turn introduce noise decomposition individual galaxy spectra using eigenspectra turn data taken sixth data release sloan digital survey total spectra taken databases using code spectral reduction performed account resampling restframe shifting correction resulting data consisted data points dimension data follow example previous work done ﬁnding eigenspectra data ﬁrst centered subtracting mean spectra values. fms. randomizedpca tyler m-estimator applied data eigenspectra data set. additionally spherize data whether changes resulting eigenspectra methods shown either better standard methods take long feasibly large size data set. figure shows results running fms. sfms tyler m-estimator spca data. parts eigenspectra standard quite noisy especially third fourth ﬁfth eigenspectra. tyler m-estimator although converges iterations makes improvement eigenspectra found standard pca. robustness fms. algorithms allows eigenspectra noisy sacriﬁcing much speed. note spca also shows qualitatively good results comparable fmsp. however fmsp sfmsp spca qualitatively diﬀerent looking results suggests comprehensive testing seeing method produces best eigenspectra. fmsp also used preprocessing step clustering. previous section examined projection robust subspace denoising technique section demonstrate gains denoising gives preprocessing data fmsp k-means. assume given data desire partition clusters. decides fmsp reduce dimensionality data thought must given dimension subspace project literature suggests good rule thumb choosing subspace dimension without clear model generating data following experiments show results range possible values https//archive.ics.uci.edu/ml/datasets/daily+and+sports+activities human activity recognition using smartphones data https //archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+ smartphones daily sports activities data consists sensor data taken second period subject performs certain action. together diﬀerent actions would like cluster points according action. total data points dimension compare three techniques classifying activities k-means projection+k-means projection+k-means. mean since observed results similar fms.. projection methods dimensional subspace project data subspace running k-means. k-means built matlab method default parameters initializes using points data set. clustering accuracy measured number correct pairwise relations points total number pairwise relations. accuracy measure also known rand index results averaged runs. display resulting experiment figure clustering accuracy approximate conﬁdence intervals given three methods. experiment clear choice denoising technique data. second clustering data training human activity recognition using smartphones data consists points dimension point consists sensor outputs taken second window accelerometer gyroscope samsung galaxy diﬀerent activities performed subject would like classify data activity. results test data displayed figure clustering accuracy results daily sports activities data set. number clusters results averaged runs. cluster accuracy calculated correct number pairwise relations points total number pairwise relations. data sets feasible robust algorithms. large dimension number points runtimes would large memory issues trying matlab personal machine. future would ideal depth experiments determine algorithms perform dimensionality reduction tasks. however believe experiments demonstrate selling point fmsp. data able processed matlab personal machine matter minutes unaware robust methods able this. worth noting though matter seconds eﬃcient cases. ﬁrst case shows better performance fmsp second case shows example fmsp improve results degree. next experiment real data stylized example image processing. experiment shown ’faces crowd’ experiment outlined experiment motivated fact images individual’s face ﬁxed pose varying lighting conditions fall subspace dimension draw data cropped face images extended yale face database face images figure clustering accuracy results human activity recognition data set. number clusters results averaged runs. cluster accuracy calculated correct number pairwise relations points total number pairwise relations. data spherized algorithms tend better running non-spherized data. thus report results algorithms apply initial spherizing denote additional remark tyler m-estimator implicitly spherizes data. dimensional subspace data using spca sfms sfms. tyler m-estimator s-reaper. pictures downsampled tests show performance algorithms images space face inliers outlier point out-of-sample faces. better subspace distort original image faces evident robust algorithms s-reaper tyler versions fmsp appear work well data. however ﬁrst test image appears better fms.. another comparison performance algorithms given figure graph displays ordered distances out-of-sample faces robust subspaces ordered distances subspace. point algorithm corresponds closest distance robust subspace closest distance subspace. general closer faces robust subspace better. algorithms appear oﬀer robust approximations underlying figure faces crowd experiment pictures downsampled dimensional pictures. fig. fig. correspond experiment pictures downsampled fig. fig. correspond experiment dimensional pictures. show projections pictures onto subspaces found method. bottom show ordered distances subspace distance subspace. lower distances robust subspace signify greater degree accuracy locating -dimensional subspace outliers. robust subspace higher dimension. spca tyler m-estimator also struggle scheme. however sfms sfms. quite well ﬁnding face subspace. fact looking figure sfms sfms. outperform algorithms signiﬁcant degree. demonstrate ability fmsp scale truly large data sets follow example fmsp algorithm portion star wars episode movie. using fmsp minutes star wars episode dimensional robust subspace. point data image results data matrix size altogether took store matrix single precision memory. experiment nodes intel sandy bridge processors each. fmsp implemented python numpy randomized truncatedsvd sklearn. set-up took total hours complete. good choice error metric evaluate found subspace here note average peak signal noise ratio fmsp slightly better plain however emphasize point experiment demonstrate possible data sets size knowledge truly accurate algorithm able this. proof theorem given following sections. first prove monotonicity consequently convergence )k∈n. next prove iterates converge ﬁxed point. finally prove ﬁxed point necessarily stationary point. consider function show minimization simply least squares minimization solved data scaled max/ pδ)−. suppose want calculate subspace data {xi/ max/ next step show convergence iterates ﬁxed point continue useful remind general results algorithms point-to-set maps. discussion closely follows discussion given tropp wang luenberger take general space point ﬁxed point generalized ﬁxed point given point-to-set associated iterative algorithm generates sequence points algorithm said monotonic respect implies strictly monotonic equality zero taking corresponds case thus gives fpδ. hand dist−p function given satisﬁes words since must thus fpδ. apparent imply argminl∈g hpδ. solution minimization motivates additional stopping condition fmsp stop consecutive iterates belong equivalence class. practice checking condition needed therefore include algorithm point ﬁnite number iterations. since continuous function inﬁmal given argminl∈g closed therefore theorem sequence generated fmsp accumulation point strict ﬁnish proof theorem showing fmsp ﬁxed point stationary point cost function fpδ. since ﬁxed point know argminl∈g hpδ. arbitrary parametrize geodesic facts ﬁxed point derivative l∗;x exists give prove preliminary lemmas §... next prove proposition gives probabilistic estimates stationary points occur sampled then ﬁnish proof theorem §... next gives proof theorem finally gives bounds proved lemma below. notice global minimum contains basis vectors lemma stationary points proof ﬁrst show stationary points. cost constant respect application observation leads simpliﬁcation max) angle less thus integral inside right hand side strictly positive overall derivative negative. similar argument used show derivative positive direction again argument used proof ﬁrst part proof show derivative bounded away zero w.o.p. geodesic towards derivative along dependence principal angles subspaces close together angles small. means directional derivative made smaller fact geodesic shorter. issue suppose want eﬀect lengthened geodesic maximum principal angle subspace principal angles refer extended geodesic geodesic maintains property still geodesic dependence lessened derivatives point extended geodesic deﬁne function magnitude point using extended geodesic compactness nonzero lower words derivative bounded away zero show radius exists directional derivative bounded away zero. show this must look derivative expression given extended geodesic thus derivative continuous function dist subspace obtained then note principal angles identical further deﬁne orthonormal basis complementary orthogonal basis then since dist inequalities turn implies putting together bound thus argument used lemma discrete version positive w.o.p.further covering argument true lemma conclude initial fmsp iterate w.o.p. probability dominates probabilities lemma next fmsp iterate point lies w.o.p. probability dominates probabilities thus limiting probabilities recovery w.o.p.as stated section analyze stability global convergence result small noise added data set. consider general mixture measure given able convergence point deﬁned deﬁned give overwhelming probability recovery fmsp must examine constants kind gain fmsp gives pca. following analysis restrict subspaces readily apparent restrict allow favorable estimates restriction reasonable take initialization fmsp. theorem taking solution lies w.o.p. thus fmsp initialization starts w.o.p. focus probabilistic recovery within neighborhood ﬁxed compare global minimum global minimum proposition sampled independently identically mixweighted solution argminl∈g lies η-neighborhood w.o.p. −ce−cn constants argminl∈g omit proof proposition since essentially proof theorem reweighted measure. continue proposition derivative proposition derivative given subspace proof. examine derivative prove proposition. deﬁne vector vector derivative spanning geodesic simpliﬁes proof. point geodesic dist dist. note geodesics length less equal length greater equal proof either geodesics. ulpδ dist−p pδ}. subspaces write asymptotic majorizing function mixture measure since derivative negative stationary point along must closer apparent stationary point also global minimum function proposition global minimum arbitrarily close global minimum proposition closer proof. point consider asymptotic majorization function point along geodesic dist point always exists along geodesics previous proposition given data sampled i.i.d. mixture measure number proposition cover mm}. next fmsp iterate point ball closer w.o.p. cover ﬁnite sub-cover compactness thus ﬁxed points w.o.p. finally fact iterates closer fmsp must converge point fmsp algorithm strongly geodesically convex around global minimum w.o.p. condition another consequence theorem hpδα dist arcsin). then second derivative positive geodesic directions thus must condition order strong convexity suﬃcient condition arcsin/) distα since second term positive strong convexity case. hand dist suﬃcient condition αpδ/π true suﬃciently small finally second derivative continuous. ﬁnish proof proposition minimum second derivative across directions. directional derivative along geodesic positive w.o.p. data sets sampled i.i.d arcsin)). fact case dist/)) must sure proposition extended strong geodesic convexity limit point sequence. guaranteed arcsin/) limit point within notice modiﬁed version still positive w.o.p. implies strong geodesic convexity since majorizes true limit point fmsp algorithm. strong geodesic convexity w.o.p. previous argument exists geodesics strongly convex. dist. taylor’s theorem write proposed fmsp algorithm fast robust recovery lowdimensional subspace presence outliers. algorithm aims solve non-convex minimization studied before. recent successful methods minimize convex relaxations problem. main reason aimed solve non-convex problem ability obtaining truly fast algorithm rsr. indeed complexity fmsp algorithm order number required iterations empirically small. also prove globally bounded locally r-linear convergence special model data. side product minimizing non-convex problem minimizer seems robust outliers minimizers convex relaxations problem. furthermore even include non-convex energies yield faster convergence although theoretical results point problems empirically faster convergence figure similar result daubechies although obvious theory case. non-convexity minimization makes hard theoretically guarantee success fmsp. able verify convergence iterates stationary point. further special cases data sampled mixture measure fmsp algorithm converges global minimum w.o.p. interesting directions theory fmsp extended. first plan extend robustness noise result reaper setting. also empirically fmsp converges correct solution cases signiﬁcant subspace model suﬃciently large. hope extend theorems encompass cases finally figure shows global linear convergence theoretically justiﬁed. interesting notice synthetic real data reﬂect model never problems global convergence iterates view current theory strong experimental experience fmsp algorithm seems promising potential robustly reducing dimension clustering classiﬁcation tasks. denoising eﬀect dimensionality reduction fmsp seems potential better demonstrated figure standard technique dimensionality reduction fmsp much complexity thus easily tested anywhere used. make implementation available. work supported awards dms-- dms-- feinberg foundation visiting faculty program fellowship weizmann institute science. thank joshua vogelstein recommendation astronomy data; ching-wa creating astronomy dataset used paper; tam´as budav´ari david lawlor help processing interpreting astronomy data; teng zhang useful comments earlier versions manuscript helpful discussions; nati srebro encouraging write submit results.", "year": 2014}