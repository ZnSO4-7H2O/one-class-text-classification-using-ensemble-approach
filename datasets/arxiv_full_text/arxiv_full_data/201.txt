{"title": "Tensor Field Networks: Rotation- and Translation-Equivariant Neural  Networks for 3D Point Clouds", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "abstract": "We introduce tensor field networks, which are locally equivariant to 3D rotations, translations, and permutations of points at every layer. 3D rotation equivariance removes the need for data augmentation to identify features in arbitrary orientations. Our network uses filters built from spherical harmonics; due to the mathematical consequences of this filter choice, each layer accepts as input (and guarantees as output) scalars, vectors, and higher-order tensors, in the geometric sense of these terms. We demonstrate how tensor field networks learn to model simple physics (Newtonian gravitation and moment of inertia), classify simple 3D shapes (trained on one orientation and tested on shapes in arbitrary orientations), and, given a small organic molecule with an atom removed, replace the correct element at the correct location in space.", "text": "signiﬁcantly important without equivariant ﬁlters like design achieving angular resolution would require factor ﬁlters ﬁlters second rotationtranslation-equivariant network identify local features different orientations locations ﬁlters helpful interpretability. finally network naturally encodes geometric tensors mathematical objects transform predictably geometric transformations rotation. here rest paper word tensor refers geometric tensors generic multidimensional arrays. call tensor ﬁeld networks every layer network inputs outputs tensor objects scalars vectors higher-order tensors every geometric point network. tensor ﬁelds ubiquitous geometry physics chemistry. motivation design universal architecture deep learning atomic systems network design quite general. expect tensor ﬁeld networks useful many tasks involving geometry. example tensor ﬁeld networks could used process images rotationtranslationequivariant way. mention potential applications section introduce tensor ﬁeld networks locally equivariant rotations translations permutations points every layer. rotation equivariance removes need data augmentation identify features arbitrary orientations. network uses ﬁlters built spherical harmonics; mathematical consequences ﬁlter choice layer accepts input scalars vectors higher-order tensors geometric sense terms. demonstrate tensor ﬁeld networks learn model simple physics classify simple shapes given small organic molecule atom removed replace correct element correct location space. convolutional neural networks translation-equivariant means features identiﬁed anywhere given input. capability contributed signiﬁcantly widespread success. paper present family networks enjoy much richer equivariance symmetries euclidean space. includes rotation equivariance translation equivariance. *equal contribution stanford university stanford california berkeley berkeley california lawrence berkeley national laboratory berkeley california google mountain view california usa. correspondence nathaniel thomas <ncthomasstanford.edu> tess smidt <tsmidtberkeley.edu>. work builds upon harmonic networks uses discrete convolutions ﬁlters composed circular harmonics achieve rotation equivariance schnet presents rotation-invariant network using continuous convolutions. mathematics rotation equivariance much complicated rotations commute; rotation matrices general mathematics tensors rotations). cohen introduce spherical cnns equivariant models spherical spaces. network differs work equivariant symmetries euclidean space rotation translation directly operates general data types points. networks presented three papers emulated tensor ﬁeld network. many authors investigated problems rotation equivariance zhou gonzalez typically work looking rotations ﬁlter differ exactly rotations orientation information preserved higher network. authors dealt similar issues invariance equivariance particular input transformations. gcnns create invariance ﬁnite symmetry groups cohen spherical harmonics wigner d-matrices address spherical signals kondor tensor algebra create neural network layers extend message passing neural networks permutation group tensors geometric tensors. networks presented operate point clouds symmetric functions encode permutation invariance. employ randomized sampling algorithm include pooling network. networks include rotation equivariance. neural networks designed evaluated atomic systems using nuclei centered calculations. many capture geometry including pairwise distance atoms graph convolutional model faber al.). work notably draws schnet point convolutions radial functions self-interaction layers. models rely pairwise distances angles points weakness distinguish chiral inputs identical pairwise distances different shapes major approach modeling atomic systems voxelize space general subject signiﬁcant expense guarantees smooth transformation rotation edge effects voxelization step. invariance type equivariance identity concerned group symmetry operations includes isometries space permutations points. composing equivariant networks yields equivariant network therefore proving equivariance layer network sufﬁcient prove whole network equivariant. furthermore network equivariant respect transformations equivariant composition transformations implies demonstrating permutation translation rotation equivariance individually sufﬁcient prove equivariance network group containing combinations transformations. describe shortly translation permutation equivariance manifest core layers focus demonstrating rotation equivariance. point coordinates feature vectors combination another vector space written refers direct operation vectors space concatenated matrices acting space combined larger block diagonal matrix. describe conditions equivariance respect different input transformations. layers introduce section manifestly permutation-equivariant treat point clouds points never requiring imposed order like list. implementation points array index associated them index ever used symmetric way. ttra xa). condition analogous translation equivariance condition cnns. possible general translation equivariance condition operator right-hand side equation also acts upon representation subspace consider paper. nontrivial convention omitting layer output equations.) attaining local rotation equivariance restrict convolution ﬁlters particular form. features different types corresponding whether transform scalars vectors higher tensors. make analysis implementation easier decompose representations irreducible representations. irreducible representations dimensions chosen unitary. term rotation order refer expression. rotation orders correspond scalars vectors symmetric traceless matrices respectively. group elements represented called wigner d-matrices elements -dimensional complex matrices. could additionally equivariance respect group rotations mirror operations would augment convolution using method obtaining equivariance respect discrete symmetries described cohen welling discuss section level tensor ﬁeld network ﬁnite points vector representation associated point. essentially ﬁnite version tensor ﬁeld inspiration network design. point multiple instances lrotation-order representations corresponding different features point. refer different instances channels. implement object dictionary multidimensional arrays shapes corresponding figure example encode simple system notation. describe tensor ﬁeld network layers prove equivariant. prove layer equivariant prove rotation equivariance given rotation order. requires showing point cloud rotates input features transformed output features transform accordingly. order produce output feed downstream layers need combine layer input ﬁlters output also transforms appropriately filters inhabit representation inputs another outputs another. need clebsch-gordan coefﬁcients combine inputs ﬁlters yield desired output representation subscripts denote representations input ﬁlter output respectively. point convolution ﬁlter input yields outputs different rotation orders inclusive) though designing particular network choose calculate outputs. design rotation-equivariant point convolution want rotation-equivariant ﬁlters. spherical harmonics orthonormal basis functions sphere. functions equivariant property non-negative integers corresponding rotation order input ﬁlter respectively; learned functions; direction unit vector magnitude respectively. filters form inherit transformation property spherical harmonics rotations scalar choice ﬁlter restriction analogous circular harmonics worrall weight matrix scalar transform respect representation index d-matrices commute straightforwardly implying layer equivariant. equivariance straightforward general combine incoming channels rotation order next point convolution layer concatenate along channel axis apply self-interaction layer. concatenation operation equivariant concatenation affect representation index. section attempt demystify wigner d-matrices spherical harmonics clebsch-gordan coefﬁcients stating values real representation corresponding objects complicated. follow sch¨utt using point convolutions scale feature vectors elementwise using self-interaction layers together components feature vectors point. self-interaction layers analogous convolutions implicitly like points generated hyperparameters radial functions given supplementary material.) moment inertia task also designate different special point want calculate moment inertia tensor. networks simple learning newtonian gravity single convolution channel; learning moment inertia tensor single layer comprised convolutions channel each. figure diagram networks. elementwise summed losses difference acceleration vectors difference moment inertia tensor. excellent agreement newtonian gravity inverse square training steps moment inertia tensor radial functions steps. figure network diagram learning gravitational accelerations moment inertia tensor. input tensor indices input gravitational acceleration moment inertia tasks point mass scalars therefore representation index -dimensional. convolution ﬁlters indicated blocks marked clebschgordan tensors indicated numbers along arrows representation index indicate arrow blocks combining input ﬁlter generate output speciﬁc could obtained equivariant accelerations using pure network predict gravitational potential scalar ﬁeld taking derivatives respect point coordinates obtain forces accelerations however many important vector quantities magnetic ﬁelds electromagnetism cannot derived scalar ﬁelds. tasks used radial functions identical used sch¨utt used radial basis functions composed gaussians dense layers applied basis vector. number spacing standard deviation gaussians hyperparameters. used shifted softplus activation function radial functions nonlinearity layers. implemented models tensorflow code available https//github.com/ tensorfieldnetworks/tensorfieldnetworks network types demonstrate simplest non-trivial tensor ﬁeld networks train networks calculate acceleration vectors point masses newtonian gravity moment inertia tensor speciﬁc point collection point masses. tasks require single layer point convolutions demonstrate. furthermore check learned radial functions analytical solutions. moment inertia tensor used classical mechanics calculate angular momentum. objects generally different moments inertia depending axis rotate about captured moment inertia tensor figure radial function learned ﬁlter gravity dataset. minimum radial cutoff distance chosen randomly sampled points enough samples generated near minimum distance. shape classiﬁcation network type three dimensions unique shapes made adjacent cubes figure call shapes tetris represent using points center cube. demonstrate rotation equivariance network classifying tetris pieces following training input network dataset shapes single orientation outputs classiﬁcation shape seen. demonstrate equivariance network test network shapes dataset rotated translated randomly. network performs perfectly task. -layer network includes following every layer possible paths convolutions concatenation self-interaction layer rotation-equivariant nonlinearity. output network since shape classes invariant rotation hence scalars. classiﬁcation output network output points figure network diagrams shape classiﬁcation task showing information ﬂows tensors different order. clebschgordan tensors implied arrows indicating convolutions. numbers self-interactions indicate number channels. individual convolutions indicated arrows produce separate tensor concatenation performed convolutions. tensor ﬁeld network include paths invariant mirror symmetry. example network correctly classify tetris shapes except chiral shapes. smallest network classify chiral shapes must include path yields vector triple product ﬁlter vectors layer. combination picks minus sign mirror operations. missing point network type task randomly remove point shape network replace point. ﬁrst step toward general isometry-equivariant generative models point clouds. output array scalars special scalar vector point. vector indicates missing point relative starting point array scalars indicates point type. special scalar softmax used probability measuring conﬁdence point’s vote. aggregate votes location using figure hypothetical example input output missing point network. benzene molecule hydrogen removed relative output vectors produced network arrows shaded associated probabilities. network layers deep channels rotation order layer. input features network one-hot encoding atom types point. layer radial functions gaussians centers evenly spaced angstroms variance half spacing. covalent bonds angstroms length radial functions smaller diameter molecules dataset. train molecule subset molecules atoms. single epoch train molecule training randomly selected atom deleted test network random selection molecules atoms molecules atoms molecules atoms. epoch training molecule randomly remove atom. evaluation molecule make prediction every possible atom could removed. example molecule atoms generate predictions different atom missing. number predictions larger number molecules. deﬁne accurate placement missing atom position within angstroms removed atom position correct atom type according argmax returned atom type array. choose angstrom cutoff smaller smallest covalent bond hydrogen-hydrogen bond length angstroms. present accuracy predictions table train epochs show comparable prediction accuracy test sets atoms training atoms. include breakdown accuracy distance mean absolute error supplementary material. hope tensor ﬁeld networks applied wide range phenomena context atomic systems hope train networks predict properties large heterogeneous systems learn molecular dynamics calculate electron densities hypothesize stable structures. ultimately hope design useful materials drugs chemicals. general physics potential applications modeling complex ﬂuid ﬂows analyzing detector events particle physics experiments studying conﬁgurations stars galaxies. applications perception robotics computational geometry bioimaging. references abadi agarwal barham brevdo chen citro corrado davis dean devin ghemawat goodfellow harp irving isard jozefowicz kaiser kudlur levenberg man´e monga moore murray olah schuster shlens steiner sutskever talwar tucker vanhoucke vasudevan vi´egas vinyals warden wattenberg wicke zheng tensorflow largescale machine learning heterogeneous systems https//www.tensorflow.org/. software available tensorﬂow.org. boomsma frellsen spherical convolutions application molecular modelling. guyon luxburg bengio wallach fergus vishwanathan garnett advances neural information processing systems curran associates inc. http// papers.nips.cc/paper/-sphericalconvolutions-and-their-applicationin-molecular-modelling.pdf. faber hutchison huang gilmer schoenholz dahl vinyals kearnes riley lilienfeld prediction errors molecular machine learning models lower hybrid error. journal chemical theory computation. gilmer schoenholz riley vinyals dahl neural message passing quantum chemistry. precup doina whye proceedings international conference machine learning volume proceedings machine learning research international convention centre sydney australia pmlr. http//proceedings.mlr. press/v/gilmera.html. sch¨utt kindermans p.-j. sauceda chmiela tkatchenko m¨uller k.-r. schnet continuousﬁlter convolutional neural network modeling quantum interactions. arxiv e-prints june smith isayev roitberg ani- extensible neural network potential accuracy force ﬁeld computational cost. chem. sci. ./csca. http //dx.doi.org/./csca. torng altman deep convolutional neural networks amino acid environment similarity analysis. bioinformatics issn ./s---. https //doi.org/./s---. wallach dzamba heifets atomnet deep convolutional neural network bioactivity prediction structure-based drug discovery. corr abs/. http//arxiv.org/ abs/.. worrall garbin turmukhambetov brostow harmonic networks deep translation rotation equivariance. ieee conference computer vision pattern recognition july number points uniformly randomly selected masses scalar values randomly chosen uniform distribution coordinates points randomly generated uniform distribution inside cube sides length gravity moment inertia. gaussian basis functions whose centers evenly spaced gaussian variance half distance centers. batch size test simply randomly generated points distribution. figure radial function learned ﬁlters moment inertia dataset. ﬁlters learn analytical radial functions. collection randomly generated point sets mean minimum distance average minimum distance points point set. distances smaller mean minimum distance might seen network enough times correct radial ﬁlter. note diverges choose cutoff minimum distance distance easy generate sufﬁcient examples distance number points example. wanted properly sample closer distances would need change generate random points close points example. table give prediction accuracy distance missing point task broken atom type. molecules train test sets; however comparing results atom type relevant number compare number examples speciﬁc atom type removed. figure figure give accuracy distance missing point task function number training epochs", "year": 2018}