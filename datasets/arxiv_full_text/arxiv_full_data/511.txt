{"title": "Signer-independent Fingerspelling Recognition with Deep Neural Network  Adaptation", "tag": ["cs.CL", "cs.CV", "cs.NE"], "abstract": "We study the problem of recognition of fingerspelled letter sequences in American Sign Language in a signer-independent setting. Fingerspelled sequences are both challenging and important to recognize, as they are used for many content words such as proper nouns and technical terms. Previous work has shown that it is possible to achieve almost 90% accuracies on fingerspelling recognition in a signer-dependent setting. However, the more realistic signer-independent setting presents challenges due to significant variations among signers, coupled with the dearth of available training data. We investigate this problem with approaches inspired by automatic speech recognition. We start with the best-performing approaches from prior work, based on tandem models and segmental conditional random fields (SCRFs), with features based on deep neural network (DNN) classifiers of letters and phonological features. Using DNN adaptation, we find that it is possible to bridge a large part of the gap between signer-dependent and signer-independent performance. Using only about 115 transcribed words for adaptation from the target signer, we obtain letter accuracies of up to 82.7% with frame-level adaptation labels and 69.7% with only word labels.", "text": "study problem recognition ﬁngerspelled letter sequences american sign language signer-independent setting. fingerspelled sequences challenging important recognize used many content words proper nouns technical terms. previous work shown possible achieve almost accuracies ﬁngerspelling recognition signer-dependent setting. however realistic signer-independent setting presents challenges signiﬁcant variations among signers coupled dearth available training data. investigate problem approaches inspired automatic speech recognition. start best-performing approaches prior work based tandem models segmental conditional random ﬁelds features based deep neural network classiﬁers letters phonological features. using adaptation possible bridge large part signerdependent signer-independent performance. using transcribed words adaptation target signer obtain letter accuracies framelevel adaptation labels word labels. automatic sign language recognition nascent technology potential improve ability deaf hearing individuals communicate well deaf individuals’ ability take full advantage modern information technology. example online sign language video blogs news currently almost completely unindexed unsearchable include little accompanying annotation. research problem included speechinspired approaches computer vision-based techniques using either/both video depth sensor input focus recognition video applicability existing recordings. technology applied wild must overcome challenges posed visual nuisance parameters signer variation. annotated data sets problem scarce part need recruit signers skilled annotators. consider american sign language particular ﬁngerspelling component spelling word sequence handshapes hand trajectories corresponding individual letters. fig. gives example ﬁngerspelling sequences. fingerspelling accounts roughly typically used proper nouns borrowings english often important content words. aspects ﬁngerspelling characterized phonology handshape described terms phonological features. prior research ﬁngerspelling recognition focused constrained tasks single-letter handshape classiﬁcation word recognition known vocabulary unconstrained letter sequence recognition problem obtained average letter accuracies signer-dependent setting using either tandem hidden markov models segmental conditional random ﬁelds features neural network classiﬁers letters phonological features. work used largest video data aware containing unconstrained connected ﬁngerspelling consisting four signers signing word tokens total image frames. paper consider problem signer-independence unconstrained ﬁngerspelling sequence recognition context limited training data. prior work address signer adaptation large-vocabulary german sign language recognition knowledge paper ﬁrst address adaptation ﬁngerspelling. investigate approaches signer-independence including speed normalization neural network adaptation. adaptation techniques largely borrowed speech recognition research application quite different overall amount data much smaller types variation different. simple signer normalization ineffective adaptation effective. fig. images ground-truth segmentations ﬁngerspelled word ‘tulip’ produced signers. image frames sub-sampled rate signers show true relative speeds. asterisks indicate manually annotated peak frames letter. </s> denote non-signing intervals before/after signing. task convert video fig. sequence letters. segmentation letters unknown sequence prediction task analogous connected phone word recognition. start recognition approaches achieved best prior results task updates improved performance deeper neural networks. next brieﬂy describe recognizers neural network classiﬁers adaptation. recognizers ﬁrst recognizer tandem model based frame-level features neural network classiﬁers predicts frame’s letter label others predict handshape phonological features. classiﬁer outputs concatenated image features dimensionality reduction input hidden markov model recognizer gaussian mixture observation densities. second recognizer segmental model based scrfs conditional log-linear models feature functions based variablelength segments input frames allowing great ﬂexibility deﬁning feature functions. scrf rescore lattices produced baseline frame-based recognizer feature functions include language model features feature measures agreement baseline recognizer means letter/phonological feature neural network classiﬁer outputs segment peak detection features measure dynamics segment. finally also ﬁrst-pass decoding scrf tang independent frame-based recognizer. feature functions namely average outputs segment samples outputs within segment duration bias lexicalized. adaptation dnns ﬁrst trained signer-independent test signer using l-regularized cross-entropy loss. inputs image features concatenated multiframe window several fully connected layers followed softmax output layer. inspection data fig. reveals main sources signer variation speed hand appearance non-signing motion variation before/after signing. speed variation large factor fastest slowest signers. absence adaptation data consider simple speed normalization augment training data resampled image features original frame rate. access labeled data test signer sufﬁcient amount training full signerspeciﬁc dnns apply adaptation. number adaptation approaches developed ﬁrst consider simple approaches based linear input networks linear output networks shown fig. network parameters ﬁxed; limited weights input output layers learned. ﬁrst approach apply single afﬁne transformation wlin static features frame feed result trained signer-independent dnns. jointly learn wlin adapt last layer weights minimizing cross-entropy loss adaptation data warm-start softmax layer learned signerindependent weights. second approach uses input adaptation layer rather adapting softmax weights removes softmax output activation adds softmax output layer wlon test signer trained jointly cross-entropy loss. finally also consider adaptation ﬁne-tuning; updating weights adaptation data starting signer-independent weights. adaptation fig. left unadapted classiﬁer; middle adaptation linear input network output layer updating right adaptation linear input network linear output network video data comprising four signers ﬁngerspelling word tokens consisting repetitions -word list including common english words names foreign words. annotators marked peak articulation letter annotations converted ground-truth frame labeling assuming letter boundaries occur mid-way peaks. following hand portion image extracted hand detection segmentation using signer-speciﬁc gaussian color model followed suppression irrelevant pixels. extracted hand images resized histogram gradient features extracted using multiple spatial grids followed dimensionality reduction principal components analysis frame classiﬁcation initial unadapted signer-independent dnns trained test signer seven tasks input -dimensional features concatenated -frame window networks three hidden layers relus cross-entropy training done weight decay penalty stochastic gradient descent -sample minibatches epochs dropout rate hidden layer ﬁxed momentum initial learning rate halved held-out accuracy stops improving. hyperparameters tuned held-out data initial experiments reported interest space. pick best-performing epoch held-out data. next consider normalization adaptation different types amounts supervision. lin+up lin+lon adapt running minibatches samples ﬁxed momentum epochs initial learning rate fig. frame accuracies without normalization/adaptation. horizontal axis labels indicate amount adaptation data ground truth labels; forced alignment labels; ﬁne-tuning. ﬁne-tuning procedure signerindependent dnns. pick epoch highest accuracy adaptation data. resulting frame accuracies given fig. addition fig. includes result speed normalization case letter classiﬁcation. speed normalization provides consistent small improvements adaptation gives large improvements settings. lin+up slightly outperforms lin+lon ﬁne-tuning outperforms lin+up lin+lon. letter sequence recognition next section adapt ﬁne-tuning using test signer’s data. fig. analyzes dnns confusion matrices. main effects large number incorrect predictions non-signing classes observe effect phonological feature classiﬁers. previously mentioned fact non-linguistic gestures variable easy confuse signing given signer’s image frames. confusion matrices show that dnns adapted main type error corrected. fig. confusion matrices classiﬁers test signer test signer’s data used adaptation disjoint used compute confusion matrices. matrix cell empirical probability predicted class given ground-truth class diagonal zeroed clarity. connected letter recognition measure performance letter accuracy analogously word phone accuracy speech recognition. table shows letter accuracies obtained tandem rescoring scrf ﬁrst-pass scrf models adaptation ﬁne-tuning using different types adaptation data. models retrain models adapted dnns tune several hyperparameters test signer’s data. tuned models evaluated unseen test signer’s remaining data; ﬁnally repeat eight choices tuning test sets covering test signer’s data adaptation report mean letter accuracy test sets. shown table without adaptation tandem scrf models poorly achieving roughly letter accuracies rescoring scrf slightly outperforming others adaptation however performance jumps letter accuracy forced-alignment adaptation labels accuracy ground-truth adaptation labels. adapted models perform similarly interestingly ﬁrst-pass scrf slightly worse others adaptation better ground-truth adaptation. hypothesis ﬁrst-pass scrf dependent performance tandem model uses original image features rescoring scrf uses tandem model hypotheses scores. dnns adapted however ﬁrst-pass scrf outperforms models. study signer-independent adapted ﬁngerspelling recognition seen ﬁngerspelling great variability speed hand appearance appearance non-signing gestures. improved performance signers adaptation dnns tandem scrf recognizers. several adaptation approaches successful largest improvements coming simple ﬁne-tuning adaptation data. approach improves letter accuracies around weak word-level supervision groundtruth frame labels adaptation data. models perform similarly best adapted model ﬁrst-pass scrf. main improvements come resolving confusions actual letters non-signing class. future work continue improve models adaptation approaches well address types variability needed port models video data wild. roussos theodorakis pitsikalis maragos dynamic afﬁne-invariant shape-appearance handshape features classiﬁcation sign language videos journal machine learning research vol. abdel-hamid jiang fast speaker adaptation hybrid nn/hmm model speech recognition based discriminative learning speaker code proc. icassp swietojanski renals learning hidden unit contributions unsupervised speaker adaptation neural network acoustic models proc. srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overﬁtting journal machine learning research vol.", "year": 2016}