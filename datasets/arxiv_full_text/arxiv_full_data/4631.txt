{"title": "A Minimum Description Length Approach to Multitask Feature Selection", "tag": ["cs.LG", "cs.AI", "I.2.6; G.3; J.3"], "abstract": "Many regression problems involve not one but several response variables (y's). Often the responses are suspected to share a common underlying structure, in which case it may be advantageous to share information across them; this is known as multitask learning. As a special case, we can use multiple responses to better identify shared predictive features -- a project we might call multitask feature selection.  This thesis is organized as follows. Section 1 introduces feature selection for regression, focusing on ell_0 regularization methods and their interpretation within a Minimum Description Length (MDL) framework. Section 2 proposes a novel extension of MDL feature selection to the multitask setting. The approach, called the \"Multiple Inclusion Criterion\" (MIC), is designed to borrow information across regression tasks by more easily selecting features that are associated with multiple responses. We show in experiments on synthetic and real biological data sets that MIC can reduce prediction error in settings where features are at least partially shared across responses. Section 3 surveys hypothesis testing by regression with a single response, focusing on the parallel between the standard Bonferroni correction and an MDL approach. Mirroring the ideas in Section 2, Section 4 proposes a novel MIC approach to hypothesis testing with multiple responses and shows that on synthetic data with significant sharing of features across responses, MIC sometimes outperforms standard FDR-controlling methods in terms of finding true positives for a given level of false positives. Section 5 concludes.", "text": "central problems statistics machine learning regression given values input variables called features develop model output variable called response task. many settings potentially thousands possible features feature selection required reduce number predictors used model. feature selection interpreted broad ways. first viewed means reducing prediction error unseen test data improving model generalization. largely focus within machine-learning community primary goal train highly accurate system. second approach feature selection often interest scientists form hypothesis testing assuming true model generates data small number features determine features actually belong model. metrics interest precision recall test-set error. many regression problems involve several response variables. often responses suspected share common underlying structure case advantageous share information across responses; known multitask learning. special case multiple responses better identify shared predictive features—a project might call multitask feature selection. thesis organized follows. section introduces feature selection regression focusing regularization methods interpretation within minimum description length framework. section proposes novel extension feature selection multitask setting. approach called multiple inclusion criterion designed borrow information across regression tasks easily selecting features associated multiple responses. show experiments synthetic real biological data sets reduce prediction error settings features least partially shared across responses. section surveys hypothesis testing regression single response focusing parallel standard bonferroni correction approach. mirroring ideas section section proposes novel approach hypothesis testing multiple responses shows synthetic data signiﬁcant sharing features across responses outperforms standard fdr-controlling methods terms ﬁnding true positives given level false positives. section concludes. assume ﬁrst feature intercept whose value always given observations features responses write values vector values matrix assuming observations independent identically distributed rewritten many cases regression problems large numbers potential features. instance predicted credit-card bankruptcy using model potential features. bioinformatics applications common thousands tens thousands features type number genetic markers expression levels number gene transcripts. number observations contrast typically hundred best. esimate breaks since matrix invertible rank statistics machine-learning communities developed number approaches addressing problem. common regularized regression aims minimize directly penalized version residual squares number less much smaller original number norm oﬀers little incentive drive entries prior unlike regularization doesn’t square coeﬃcients hence entries tend sparse. regression seen form feature selection—i.e. choosing subset original features keep model sparsity helps avoid overﬁtting training set; result number training examples required successful learning regularization grows logarithmically number irrelevant features whereas number grows linearly regression sparse models also beneﬁt interpretable important scientists want know particular variables actually relevant given response. building regression models interpretation discussed sections regression achieve even sparsity approaches approaches number nonzero values hence regularization called norm subset selection choosing small number original features retain regression model. coeﬃcient model there’s incentive drive small value; counts cost adding ﬁrst place. norm number advantages including bounded worst-case risk respect norm better control measure called false discover rate explained fully section moreover note virtue approach focuses qualitative decision whether covariate relevant problem hand decision conceptually distinct parameter estimation. however virtue approach computational tractability. indeed exact regularization requires subset search proved np-hard practice therefore approximate greedy algorithm like forward stepwise selection necessary turns criteria derived information-theoretic principle called minimum description length diﬀerent model coding schemes forms focus thesis approach regression subject next subsection. method model selection treats best model maximally compresses digital representation observed data. imagine sender wants transmit data email receiver using bits possible case linear regression assume sender receiver know matrix sender wants convey values matrix would kraft inequality information theory implies probability distribution {pi} ﬁnite countable exists corresponding code codeword lengths moreover code lengths optimal sense minimizing expected code length respect {pi}. sender receiver agree model data e.g. probability distribution possible conﬁgurations residuals agree code residuals lengths this calls crude two-part version mdl. beginning rissanen introduced one-part version based concept called stochastic complexity. however divides description length terms model model complexity practice similar two-part overﬁtting comes fact thatσ assumes current feature actually comes model; prevent overﬁtting instead estimate based model without current feature βq−. coding model depends model residuals sender receiver choose coding scheme transmit relevant hence matrix contain mostly zeros. ﬁrst step coding second step encode numerical values coeﬃcients. rissanen suggested basic approach this create discrete grid possible parameter values code integers specify grid sender encode feature coeﬃcients receiver would least average values reconstructing thus sender transmits rounded coeﬃcient values. rounding adds residual coding cost sender specifying exact require instead simple number bits sender uses convey integer vary needs tell receiver many bits expect. number bits integer coded hence iteration logarithms. middle table shows example costs code. fact practice it’s unnecessary allow integer code extend arbitrarily large integers. we’re interested features near limit detectability expect z-scores roughly range since much higher true features would obvious wouldn’t require sensitive feature selection. could thus impose maximum possible z-score might ever want encode assume z-scores fall case constant reduced value large enough implementation avoid computing actual values z-scores instead assuming constant cost bits coeﬃcient. though theoretically tunable parameters sender receiver decided models number bits specify coeﬃcient much eﬀect signiﬁcance level hypothesis test found bits coeﬃcient work well practice. standard multivariate linear regression described section assumes many features single response. however number practical problems involve multiple features multiple responses. instance biologist transcript abundances thousands genes would like predict using genetic markers multiple diagnostic categories cancers whose presence absence would like predict using transcript abundances describe situation asked econometric variables predict stock prices diﬀerent industries. general noise responses correlated; instance responses consist temperature measurements various locations taken thermometer instrument drifted high location high other. second practical reason might make assumption comes stepwise regression choose model early iterations stepwise algorithm eﬀects features present model show part noise error term responses share feature model portion error term feature same. thus decide take rows nonzero covariance works case covariance matrix non-diagonal maximum-likelihood solution mean multivariate gaussian independent covariance however intuitively regression problems related ought able better borrowing strength across multiple responses. concept goes various names; machine-learning community it’s often known multitask learning transfer learning several familiar machine-learning algorithms naturally multitask learning; instance neural networks share hidden-layer weights multiple responses. addition number explicit multitask regression methods proposed curds whey takes advantage correlation among responses various dimensionalityreduction algorithms techniques even explicit feature selection. instance present bayesian markov chain monte carlo approaches large numbers features. present multiresponse sparse regression algorithm selects features stepwise manner choosing iteration feature correlated residuals responses previous model. explored algorithms detail comparing could fruitful direction future work. elaborate multitask-learning approaches used. ando zhang multiple prediction problems learn underlying shared structural parameter input space. presentation general give particular example linear model we’ll rewrite using notation thesis. given tasks features task authors assume existence matrix task superscript denotes column matrix weights lower dimensional features created product transformation common across tasks thus share information. authors develop algorithm referred andozhang thesis chooses ando zhang present algorithm tool semi-supervised learning given particular problem solved it’s possible generate auxiliary problems known data solve them shared structural parameter original problem. course approach works well start tasks problems solved right. multitask learning methods actually feature selection i.e. building models contain subset original features. andozhang regularization penalty leaves features methods bblasso presented below not. present algorithm learning functions input features across tasks. general approach considered thesis choosing merely subset original features note approach reduces feature selection identity function. regularizing norm features problem thus becomes norm favors shared coeﬃcients across responses features. this suggest case entries diﬀerent features single nonzero response coeﬃcient penalty since feature however single feature shares coeﬃcient across number magnitude nonzero coeﬃcients thus leads coeﬃcients however reality coeﬃcient values restricted meaning unhelpful coeﬃcients rather tend small values whose square negligible -norm penalty. practice then course note norm used could generalized norm corresponds sharing across responses since would reduce objective function independent -penalized regressions. opposite extreme corresponds maximal sharing; case maximum absolute coeﬃcient value across matters. number eﬃcient optimization approaches proposed e.g. relatedwork sections survey. particular propose fast approximate algorithm using regularization path method implicitly evaluates values following section section considers ways might approach regression multiple responses perspective information theory. strategy course would apply penalized-likelihood criterion response isolation; we’ll call method dominant penalty value prescribed risk inﬂation criterion equation however following intuition bblasso multitask algorithms suspect features predictive task likely predictive other related tasks. example responses level cholesterol level cholesterol expect lifestyle variables correlated tend correlated other. following multitask coding scheme called multiple inclusion criterion designed take advantage situations making easier models features shared across responses. na¨ıve approach would coeﬃcients linear order specify index desired coeﬃcient using bits. better. expect nearly responses correlated predictive features could give responses nonzero coeﬃcients simply specify feature we’re talking using bits section we’ll call fully dependent many cases however assumption feature correlated almost responses unrealistic. ﬂexible coding scheme would allow specify subset responses want give nonzero coeﬃcients. instance suppose we’re considering feature number responses think responses nonzero coeﬃcients current feature. bits specify feature once list particular responses nonzero coeﬃcients feature thereby avoiding four diﬀerent penalties specify coeﬃcient isolation. standard code subset size ﬁrst specify many elements subset contains subscript stands full covariance instead prevent overﬁtting singleusing diagonal matrix that’s except oﬀ-diagonal entries case observed informally performed comparably however appear performance advantage continued faster computationally. section discussed three information-theoretic approaches multitask regression full partial mic. general negative log-likelihood portion diﬀer full partial nondiagonal covariance estimate like operating response time implicitly uses however since also full partial real diﬀerences come coding penalties. compared table various values number responses current feature consideration. course full allowed take actually nonzero coeﬃcients three rows table. however extra coeﬃcients correspond non-predictive features extra reduction residual-coding cost full enjoys methods likely small. numbers beside coding-cost formulas illustrate case features responses. expected coding scheme cheapest case designed; however note methods never excessively expensive unlike mentioned section searching possible combinations zero nonzero coeﬃcients model minimizes description length computationally intractable. therefore implement using forward stepwise search procedure detailed algorithm beginning null model containing intercept feature evaluate feature addition model would reduce description length computed using algorithm could also assume uniform distribution spend bits code index. however practice smaller also experimented regularized full covariance-matrix estimators proposed respectively. never systematic comparisons informal assessment methods generally improve performance relative current approach sometimes reduced table costs bits three schemes code appearance feature response models. general assume note examples values appear brackets; smallest costs appears bold. procedure requires evaluating quality features times number features eventually added model. partial time evaluate feature determine best subset responses might associated feature. algorithm notes also done using stepwise-style search start responses best response re-evaluate remaining responses next-best response unlike ordinary stepwise algorithm however don’t terminate search current number responses model description length fails best seen yet. we’re interested borrowing strength across responses need avoid overlooking cases correlation feature single response insuﬃciently strong increase monotonically even adding feature intermediate number response models doesn’t look promising adding might. thus perform full search eventually responses model. result partial requires total evaluations description length. however optimizations reduce computational cost partial practice. quickly ﬁlter irrelevant features iteration evaluating feature decrease negative log-likelihood would result simply adding responses without subset search. keep features according criterion proceed full search subsets. long bigger than makes essentially impact quality results. reduces number model evaluations often short-circuit search response subsets noting model nonzero coeﬃcients always lower negative log-likelihood fewer nonzero coeﬃcients. allows lower bound description length current feature number nonzero responses might choose need check values smaller best description length candidate feature’s best response subset seen far. practice evaluating usually enough; i.e. typically need responses stepwise manner stopping cost although attempt possible formulate using regularization path homotopy algorithm sort become popular performing regularization without need cross-validation possible would signiﬁcantly faster stepwise search. evaluate several synthetic real data sets multiple responses associated features. focus parameter regimes designed namely relatively small number features expected predictive. describe details data subsection below. comparing existing multitask methods used matlab transfer learning toolkit provides implementations seven algorithms literature. unfortunately apply data sets since often required meta-features expected features frequency counts unsupervised learners. methods apply andozhang bblasso described section included several free parameters including regularization coeﬃcients task optimization method tried values settling ﬁnding exact value generally important. performed informal cross-validation values found perhaps surprisingly consistently gave best results. used value throughout experiments below. bblasso implementation written guillaume obozinski based paper published earlier cited above; however algorithm essentially same. parameter used default package setting. andozhang bblasso classiﬁcation methods order compare them turn classiﬁcation method well. would update reﬂect logistic model; however resulting stepwise search re-evaluates quality response iteration necessary because take covariance matrix nondiagonal values residuals response aﬀect likelihood residuals responses. take diagonal diagonal don’t need re-evaluate residual likelihoods iteration cost evaluations description length. responses values we’ve chosen features enter models ﬁnal round logistic regression response separately chosen features slightly better classiﬁer. created synthetic data according three separate scenarios we’ll call partial full independent. scenario generated matrix continuous responses features responses observations. then produce binary responses response values greater equal average value columns rest yielding roughly split normality data. entry xsim i.i.d. nonzero entry βsim i.i.d. entry \u0001sim i.i.d. covariance among \u0001sim entries diﬀerent responses. response beneﬁcial features i.e. column βsim nonzero entries. scenarios diﬀered according distribution beneﬁcial features βsim. partial scenario ﬁrst feature shared across responses second shared across ﬁrst responses third across ﬁrst responses fourth across ﬁrst responses. response four features responses didn’t ﬁrst four features features randomly distributed among remaining features full scenario response shared exactly features none features part model. independent scenario response four random features among figure illustrates feature distributions showing ﬁrst rows random βsim matrices. table shows performance methods random instances data sets. test-set errors true model obtained logistic regression response separately using model containing exactly features βsim nonzero coeﬃcients. addition test-set error show precision recall metrics. coeﬃcientlevel precision recall refer individual coeﬃcients coeﬃcients data-generating βsim nonzero ﬁnal show nonzero vice versa? feature-level precision recall look question entire features βsim nonzero coeﬃcients corresponding nonzero coeﬃcients baseline accuracy corresponding guessing majority category roughly andozhang’s values fact explicitly select features. competes full even full regime. though results shown observed informally andozhang bblasso tended larger diﬀerences training testing error implying less likely overﬁt. resistance overﬁtting general property approaches requirement completely ﬁrst real data comes consists real-valued growth measurements strains yeast drug conditions. order make computations faster hierarchically clustered conditions groups using single-link clustering correlation similarity measure. taking average values cluster produced real-valued responses binarized categories values least average response values average features consisted markers transcript levels rich media total features. yeast growth section table shows test errors -fold data set. though diﬀerence isn’t statistically signiﬁcant partial appears outperform bblasso test error. event partial produces much sparser model seen numbers nonzero coeﬃcients features partial includes nonzero presumably assumptions complete sharing sharing features across responses rarely hold real-world data. like partial andozhang well data set; however algorithm scales poorly large numbers responses took days run. table accuracy number coeﬃcients features selected folds yeast growth yeast markers breast cancer data sets. standard errors folds; i.e. represent majority label column represents classiﬁer guesses common labels seen training set. andozhang’s values fact explicitly select features. yeast growth data described includes features markers transcripts yeast strains. consider variables prediction right without reference growth responses all. fact regression transcripts markers commonly done; it’s known expression quantitative trait loci mapping since marker variables already binary decided problem around usual eqtl setup using transcripts features predicted subset markers results shown yeast markers section table unlike case yeast growth data apparently less sharing feature information across markers appears outperform partial test error. andozhang data set. breast cancer data represents combination seven data sets used contains observations rma-normalized gene-expression values. considered associated responses; binary—prognosis status —and three not—age tumor size grade binarized three non-binary responses categories response values least high average values average. responses unavailable observations eliminated observations leaving those kept ﬁrst save computational resources make problem harder. reduce features manageable number took highest variance. experimental results section demonstrate regression eﬀective tool predicting responses features cases number features large selection improve generalization performance. however noted yeast breast cancer data sets accuracy always goal; sometimes model interpretability important. indeed science entire goal regression often prediction rather discovering true model testing hypotheses whether pairs variables really linear relationship. instance econometrician them. thus order predict responses separately times. probably possible train predict responses simultaneously wanted avoid introducing errors changing code. hypothesis testing small number features straightforward given regression coeﬃcient compute statistic exceeds threshold reject null hypothesis coeﬃcient situation becomes slightly complicated many features whose coeﬃcients want examine. section reviews standard approaches multiple-testing problem section recasts light mdl. sets stage approach hypothesis testing section consider case single response variable suppose we’re trying determine features linearly correlated response. test feature signiﬁcance level overall probability we’ll falsely reject true null hypothesis much greater α—this problem multiple hypothesis testing. standard control so-called alpha inﬂation called bonferroni correction letting denote null hypotheses associated p-values reject boole’s inequality controls known family-wise error rate probability making false rejection level complete null hypothesis true. fact bonferroni controls fwer strong sense well subset null hypotheses probability falsely rejecting member subset members true also bounded bonferroni correction single-stage testing procedure which unfortunately testing larger number hypotheses reduces power rejecting especially problematic number hypotheses tested thousands tens thousands common with e.g. microarray fmri data. however statisticians developed several multistage testing procedures help overcome limitation conditioning rejection thresholds test statistics results rejections others. ﬁrst holm step-down procedure here denote p-values sorted increasing order corresponding null hypotheses. begin looking fails stop without rejecting anything. however reject move reject continue manner rejecting m−j+ fail reject hypothesis. like bonferroni holm method controls fwer strong sense independent dependent test statistics controls fwer independent test statistics level moreover simulation studies suggested remained true various multivariate-normal multivariate-gamma test statistics unfortunately unlike bonferroni holm procedures simes approach says nothing rejecting individual hypotheses complete null rejected although subsequently proposed limited procedures however benjamini hochberg pointed simes procedure could said control diﬀerent measure called false-discovery rate letting denote random variable number true null hypotheses rejected random variable number correctly rejected null hypotheses thus expected proportion falsely rejected null hypotheses. statistic ﬂexible fwer accounts total number hypotheses considered; instance hypotheses fwer would high proportion false rejections could still low. ﬂexibility large become standard bioinformatics neuroscience many ﬁelds. showed benjamini-hochberg step-up procedure controls independent test statistics conﬁguration false null hypotheses. extended result show procedure also controlled certain types positive dependency. included positively correlated normally distributed one-sided test statistics occur often instance gene-expression measurements various extensions procedure proposed. instance suggested step-down approach independent test statistics that dominating step-up procedure tended experimentally yield higher power. would control test-statistic correlation structure though often conservative necessary proposed resampling approach estimating distributions based data order gain increased power test statistics highly correlated. many modiﬁcations procedure forward since. however thesis stick original version given compares log-likelihoods data various models choosing complicated model suﬃciently lower negative log-likelihood. appendix shows that fact process equivalent standard statistical procedure known generalized likelihood-ratio test implied signiﬁcance level hypothesis testing diﬀers slightly regression however. goal regression minimize test-set error choosing highly informative features. penalizes feature several similar features correlated response stepwise regression likely include them. instance suppose true model nearly identical features. wanting waste model-coding bits regression would probably give model hypothesis testing features signiﬁcantly correlated response. example would hope include relevant features. this regress feature isolation; keep features that according deserve nonzero regression coeﬃcients. conceptually think for-loop features given iteration call mic-stepwise-regression function using regular matrix matrix contains current feature there’s catch though since for-loop searches potential features we’re eﬀectively hypothesis tests need incorporate sort multiple-testing penalty. describe motivate penalty perspective. section described scenario motivate ordinary regression sender wanted transmit data receiver sender receiver knew value associated matrix features. suppose instead diﬀerent receivers receiver knows values feature receivers want reconstruct sender transmit messages telling possibly using feature individually know. messenger hermes visits sender tells ground rules sender must transmit directly receiver information reconstruct given model. model information coeﬃcients) sender transmit hermes visit receiver tell appropriate receiver receiver least gets average reconstructing letting denote model sender tells coeﬃcient receiver otherwise contains costless intercept term regression coeﬃcient feature result something like series hypothesis tests features correspond rejected null hypotheses. idea that sender tell hermes coeﬃcients receiver coding involve penalty grows number possible features need protect alpha inﬂation multiple tests. suppose sender wants nonzero coeﬃcients features tell hermes transmit binary representation numbers using idealized bits one. then section sender spends bits specify coeﬃcient. resulting message hermes costs bits. general sender’s description length feature denote decrease residual coding cost would result using nonzero coeﬃcient feature minimizing equivalent following decision rule looking features decreasing order values feature model long interestingly appendix shows equivalent bonferroni correction implied fact according appendix taking cost coeﬃcient around stepwise regression corresponds around sender expects code coeﬃcient rather coding entire index specify feature advantageous scheme similar there scheme used convey subset nonzero response coeﬃcients partial mic; would describe subset features using code sender’s description case regression real-world hypothesis-testing problems often involve multiple responses. discuss task identifying subset wavelength features best predict chemical quantities expression quantitative trait loci mapping process looking correlations potentially thousands gene-expression transcripts genetic markers section reviews existing ways deal problem section describes approach. experimental comparisons various methods described section probably straightforward approach treat response separately apply bonferroni procedures section response time. we’ll refer approaches simply bonferroni respectively. according number early eqtl studies took approach applying single-transcript mapping methods transcript separately. however methods designed control false positives analyzing single transcript multiple tests across transcripts result inﬂated account greater number hypothesis tests performed responses might penalize procedure consists applying latter bonferroni threshold p-value matrix p-values we’ll call bonferronimatrix. eﬀectively we’re imagining matrix p-values long vector applying standard bonferroni correction that. similarly imagine turning matrix p-values single vector applying method we’ll call also allows approach bhmatrix. contrast harsher starting penalty we’re currently examining. describes conceptually similar approach called q-all used so-called q-values identify signiﬁcant marker-transcript correlations. we’re looking range signiﬁcance levels suﬃces examine bonferroni bonferronimatrix; we’ll show results bonferroni follows. true bhmatrix report methods. bhmatrix approach treat response separately since example response lots p-values pass harshest thresholds step-up procedure leave easier thresholds responses. doesn’t really share information across responses meaningful way. approach called ebarrays uses empirical bayes hierarchical model estimate prior underlying means transcript allowing stable inference across transcripts despite small sample size individually. goal determine marker transcripts show signiﬁcant association. originally designed sharing transcript information marker time method extended mixture markers model uses algorithm assign probabilities transcript correlates marker names suggest methods apply speciﬁcally eqtl problems though course empirical-bayes shrinkage framework applies generally. give example previous approaches sharing strength across responses developed hypothesis testing. ebarrays approach shares information across responses fails take advantage potentially important source shared strength namely features strongly associated response perhaps likely associated responses. case eqtl example might suspect type sharing across responses marker features corresponding trans-regulatory elements aﬀect expression many diﬀerent genes. hand approach section pick sharing features across responses propose method hypothesis testing mic. single-response version hypothesis testing section applied single-response regression feature individually also imagine applying multiple-response regression approach section feature individually order multiple-response hypothesis testing. amounts basically for-loop stepwise regression algorithm feature except costs coding features diﬀerent usual setup hermes imposed section consider bonferroni-style bh-style coding scheme using feature penalties bonferroni-mic evaluate feature time inclusion based whether stepwiseregressionmic would included nonzero coeﬃcients feature following exception. algorithm includes feature penalty number features given matrix. here give algorithm column time term would normally however coding scheme hermes imposed requires instead bits specify feature total number features pseudocode appears algorithm bh-mic slightly trickier cost adding feature depends many features already model. approach outlined algorithm loop features evaluate whether would include nonzero coeﬃcients feature-coding cost imposed -like term charged). ﬁlls possibly inﬂated optimal value zero rest. note zeroing rows suﬃcient; need back number nonzero coeﬃcients subsequently trim necessary. this evaluate cost keeping best currently nonzero features current number nonzero features qorig choose bonferroni-mic bh-mic algorithmic complexity dominated for-loop. section call stepwiseregressionmic required evaluations description length we’ve used denote number columns particular complexity within given for-loop iteration overall then algorithms evaluate description length times. consider single feature. section noted correspondence regression feature statistical likelihood-ratio test whether regression coeﬃcient nonzero. responses hypothesis testing searches subsets responses nonzero coeﬃcients feature. evaluation description length process interpreted hypothesis test explained appendix. particular evaluate description length subset responses nonzero we’re implicitly likelihood-ratio test null alternative hypotheses matrix true regression coeﬃcients feature responses. theory performs exponentially many tests subset though obviously highly correlated. interpret multiple-testing correction implicit hypothesis tests. section corresponding portions appendix pointed approximate correspondence particular penalties particular corrections instance log-likelihood space essentially bonferroni correction p-value space cost coeﬃcient approximately corresponds responses approximations somewhat rougher. reason diﬀerence dimension parameter spaces often greater case standard-normal approximation using appendix longer goes through. nevertheless expect bonferronibh-style penalties section roughly right things multiple responses; needn’t make match bonferroni procedures exactly. measure performance algorithms terms test-set error cross-validation experiments real data sets section when instead need judge well algorithm identiﬁes truly nonzero coeﬃcients fall back synthetic data. that’s real world doesn’t tell true matrix—we know make ourselves. thus section rely entirely synthetic data. however sometimes base synthetic data real-world data yeast growth data section scenarios below generated random instances data sets corresponding random βsim matrices taking training data points responses. synthetic data cheap evaluate test-set squares error error test data points. calculate error response sse/ ordinary-least-squares regression given response exactly features selected algorithm report precision recall coeﬃcient level calculate results separately response data standard errors represent standard deviations divided compare following feature-selection approaches truth oracle select exactly true features. bonfα=α response separately select features whose p-values response α/m. calculate p-value given feature response regressing response feature intercept evaluating p-value slope regression coeﬃcient. methods free parameters give point values precision recall. allow comparison then methods variety levels presenting results levels precision approximately matched mic; relative performance assessed based recall. tried discrete grid values precisions always match exactly took highest precision exceeding value. puts slight disadvantage comparing recall precision often slightly higher methods. tables below make bold results represent best performance among three bonferroni-style methods also among three bh-style methods. here best performance based observed average value diﬀerences often statistically signiﬁcant. created three data sets manner described section diﬀerence took features instead table shows results partial full independent scenarios. appears worse test-error methods except full scenario performed close truth. especially poor independent performance suggests probably fact harder time picking single nonzero coeﬃcients given βsim higher overhead cost course hypothesis-testing algorithms test error important metric. recall outperform competitors partial especially full scenarios. would expect performs worse independent created synthetic yeast growth data based described section explained below talking true coeﬃcients synthetic model requires features relatively uncorrelated. however transcript features original data highly correlated included marker features matrix. before made growth clusters matrix. created four types data sets varying degrees correlation among features basic data-generation process each describe below. algorithm described section cost bits coeﬃcient. resulting matrix contained next step construct true matrix βsim resembling identical based summary statistics fraction features nonzero coeﬃcients? average number nonzero coeﬃcients row? sample mean standard deviation nonzero coeﬃcient values? initialized empty matrix βsim walked rows time ﬂipping coin probability decide whether give nonzero coeﬃcients. drew number nonzero coeﬃcients poisson distribution rate distributed coeﬃcients randomly among columns. values coeﬃcients drawn independently normal distribution mean standard deviation σbβ. ﬁnished walking rows checked make sure total number nonzero coeﬃcients βsim within original started process over. covariance based covariance matrix real matrix. ﬁrst three variants synthetic data constructed simulated matrix xsim drawing multivariate-normal distribution consisted taking subscripts section figure plots respectively three variants involved normally distributed feature values even though consisted thus fourth variant took xsim original binary-valued matrix. figure correlation matrices marker features yeast growth data set. ﬁgures correspond left right diagonal shrinkage half-diagonal shrinkage shrinkage. correlation near yellow correlation near light blue correlation near dark blue correlation near relatively high correlation among features measures precision recall slightly misleading features truly correlated responses becomes unclear text explains. table shows results variant data set. general performs essentially indistinguishably methods. probably average number responses feature particular data implying isn’t substantial sharing features across responses. appear enough sharing doesn’t perform much worse methods case independent synthetic data table precision methods degrades signiﬁcantly correlation among features increases. fact numbers somewhat misleading notion true coeﬃcient becomes fuzzy features correlated. true features synthetic data algorithm returned real data set. since designed synthetic data imitate real might expect return similar patterns coeﬃcients cases. generating matrix serve βsim also allows randomization multiple instances synthetic data set. course downside simulation approach correlational structure original problem lost. generated data imposing particular βsim matrix nonzero coeﬃcients certain locations. instance response might given feature coeﬃcient causing correlation feature response. however feature correlated with features features also correlated response selecting well. isn’t necessarily wrong data really show correlation; arbitrariness choice zeros nonzeros βsim fault large apparent rate false positives. principle provides natural framework design penalized-regression criteria feature selection. case single response example characterized information-theoretic penalty bits feature selecting among total features. proposed extension criterion called case multiple responses. eﬃciently coding locations feature-response pairs features associated multiple responses allows sharing information across responses feature selection. method competitive with sometimes outperforms existing multitask learning algorithms terms prediction accuracy achieving generally sparser interpretable models. also viewed domain hypothesis testing correcting alpha inﬂation multiple tests. explained regression applied feature separately interpreted along lines standard bonferroni benjamini-hochberg feature-selection procedures. using approach extended hypothesis testing multiple responses allowing greater power selecting true coeﬃcients features signiﬁcantly shared across responses. information theory describes isomorphism probabilities code lengths idealized code length symbol similar though approximate relationship process statistical hypothesis testing namely tend introduce extra parameter model roughly cases hypothesis test would reject null hypothesis parameter zero. perhaps unsurprising tool model selection hypothesis testing rejecting models data poorly. might specify that given distributed i.e. might given distributed observed data authors prefer speak ﬁxed probability distributions parameters ranging diﬀerent parameter spaces case regression example entire real line. sense measures badness relative diﬀerent data sets take diﬀerent values according probability distribution. true unlikely large deﬁne threshold reject whenever exceeds threshold. known generalized likelihood ratio test many standard statistical hypothesis tests examples including t-test signiﬁcance regression coeﬃcient example single regression coeﬃcient introduces quantity common statistics deal instead following result belong type probability distribution satisfying certain smoothness conditions parameter space dimensionality parameter space dimensionality asymptotically chi-square degrees freedom equal particular probability distribution normal chi-square distribution asymptotic exact. single regression coeﬃcient chi-square distribution degree freedom distribution square standard normal random variable thus rewrite it’s worth reﬂecting correspondences approximate. indeed code length negative probability p-value probability then bonferroni rule rejecting equivalent rejecting looks basically however used exactly equal former comparison probability density function points latter area probability density function extreme regions. still often quite close practice. close? suppose observe value associated p-value stands cumulative distribution function. figure compares associated p-value case degree freedom; straight -degree line would perfect match approximation generally correct within factor curve compute implied complicated model letting suppose regress features separately. would obtain p-values could apply step-up procedure alternatively could evaluate feature negative log-likelihood ratio approach regression prediction originated jing zhou dean foster lyle ungar drafted paper outlining theory preliminary experiments. lyle suggested continue work project summer focusing hypothesis testing continued guidance lyle dean kept working research fall spring part thesis. january paramveer dhillon lyle submitted paper international conference machine learning highlighting experimental results section results section original thesis. addition names above thank robert stine phil everson advice statistical theory; qiao conversations lyle; dana pe’er laboratory providing yeast data set; adam ertel sandler making accessible breast cancer data set; knerr computing assistance; newhall rich wicentowski doug turnbull guidance writing thesis; santosh venkatesh honors thesis examiner several helpful comments corrections.", "year": 2009}