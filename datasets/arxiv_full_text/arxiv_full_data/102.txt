{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "tag": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.", "text": "neural machine translation recently proposed approach machine translation. unlike traditional statistical machine translation neural machine translation aims building single neural network jointly tuned maximize translation performance. models proposed recently neural machine translation often belong family encoder–decoders encode source sentence ﬁxed-length vector decoder generates translation. paper conjecture ﬁxed-length vector bottleneck improving performance basic encoder–decoder architecture propose extend allowing model automatically search parts source sentence relevant predicting target word without form parts hard segment explicitly. approach achieve translation performance comparable existing state-of-the-art phrase-based system task english-to-french translation. furthermore qualitative analysis reveals alignments found model agree well intuition. neural machine translation newly emerging approach machine translation recently proposed kalchbrenner blunsom sutskever unlike traditional phrase-based translation system consists many small sub-components tuned separately neural machine translation attempts build train single large neural network reads sentence outputs correct translation. proposed neural machine translation models belong family encoder– decoders encoder decoder language involve language-speciﬁc encoder applied sentence whose outputs compared encoder neural network reads encodes source sentence ﬁxed-length vector. decoder outputs translation encoded vector. whole encoder–decoder system consists encoder decoder language pair jointly trained maximize probability correct translation given source sentence. potential issue encoder–decoder approach neural network needs able compress necessary information source sentence ﬁxed-length vector. make difﬁcult neural network cope long sentences especially longer sentences training corpus. showed indeed performance basic encoder–decoder deteriorates rapidly length input sentence increases. order address issue introduce extension encoder–decoder model learns align translate jointly. time proposed model generates word translation searches positions source sentence relevant information concentrated. model predicts target word based context vectors associated source positions previous generated target words. important distinguishing feature approach basic encoder–decoder attempt encode whole input sentence single ﬁxed-length vector. instead encodes input sentence sequence vectors chooses subset vectors adaptively decoding translation. frees neural translation model squash information source sentence regardless length ﬁxed-length vector. show allows model cope better long sentences. paper show proposed approach jointly learning align translate achieves signiﬁcantly improved translation performance basic encoder–decoder approach. improvement apparent longer sentences observed sentences length. task english-to-french translation proposed approach achieves single model translation performance comparable close conventional phrase-based system. furthermore qualitative analysis reveals proposed model ﬁnds linguistically plausible alignment source sentence corresponding target sentence. probabilistic perspective translation equivalent ﬁnding target sentence maximizes conditional probability given source sentence i.e. maxy neural machine translation parameterized model maximize conditional probability sentence pairs using parallel training corpus. conditional distribution learned translation model given source sentence corresponding translation generated searching sentence maximizes conditional probability. recently number papers proposed neural networks directly learn conditional distribution neural machine translation approach typically consists components ﬁrst encodes source sentence second decodes target sentence instance recurrent neural networks used encode variable-length source sentence ﬁxed-length vector decode vector variable-length target sentence. despite quite approach neural machine translation already shown promising results. sutskever reported neural machine translation based rnns long shortterm memory units achieves close state-of-the-art performance conventional phrase-based machine translation system english-to-french translation task. adding neural components existing translation systems instance score phrase pairs phrase table re-rank candidate translations allowed surpass previous state-of-the-art performance level. here describe brieﬂy underlying framework called encoder–decoder proposed sutskever upon build novel architecture learns align translate simultaneously. although previous works used encode variable-length input sentence ﬁxed-length vector necessary even beneﬁcial variable-length vector show later. decoder often trained predict next word given context vector previously predicted words {y··· yt−}. words decoder deﬁnes probability translation decomposing joint probability ordered conditionals nonlinear potentially multi-layered function outputs probability hidden state rnn. noted architectures hybrid de-convolutional neural network used section propose novel architecture neural machine translation. architecture consists bidirectional encoder decoder emulates searching source sentence decoding translation noted unlike existing encoder–decoder approach probability conditioned distinct context vector target word context vector depends sequence annotations encoder maps input sentence. annotation contains information whole input sequence strong focus parts surrounding i-th word input sequence. explain detail annotations computed next section. context vector then computed weighted annotations alignment model scores well inputs around position output position match. score based hidden state j-th annotation input sentence. parametrize alignment model feedforward neural network jointly trained components proposed system. note unlike traditional machine translation alignment considered latent variable. instead alignment model directly computes soft alignment allows gradient cost function backpropagated through. gradient used train alignment model well whole translation model jointly. understand approach taking weighted annotations computing expected annotation expectation possible alignments. probability target word aligned translated from source word then i-th context vector expected annotation annotations probabilities αij. probability associated energy reﬂects importance annotation respect previous hidden state deciding next state generating intuitively implements mechanism attention decoder. decoder decides parts source sentence attention letting decoder attention mechanism relieve encoder burden encode information source sentence ﬁxedlength vector. approach information spread throughout sequence annotations selectively retrieved decoder accordingly. usual described reads input sequence order starting ﬁrst symbol last xtx. however proposed scheme would like annotation word summarize preceding words also following words. hence propose bidirectional successfully used recently speech recognition obtain annotation word concatenating forward hidden state backward annotation contains summaries preceding words following words. tendency rnns better represent recent inputs annotation focused words around sequence annotations used decoder alignment model later compute context vector evaluate proposed approach task english-to-french translation. bilingual parallel corpora provided comparison also report performance encoder–decoder proposed recently training procedures dataset models. contains following english-french parallel corpora europarl news commentary crawled corpora words respectively totaling words. following procedure described reduce size combined corpus words using data selection method axelrod monolingual data mentioned parallel corpora although possible much larger monolingual corpus pretrain encoder. concatenate news-test http//www.statmt.org/wmt/translation-task.html implementations available https//github.com/lisa-groundhog/groundhog. available online http//www-lium.univ-lemans.fr/˜schwenk/cslm_joint_paper/. news-test- make development evaluate models test consists sentences present training data. usual tokenization shortlist frequent words language train models. word included shortlist mapped special token apply special preprocessing lowercasing stemming data. train types models. ﬁrst encoder–decoder proposed model refer rnnsearch. train model twice ﬁrst sentences length words sentences length word encoder decoder rnnencdec hidden units each. encoder rnnsearch consists forward backward recurrent neural networks hidden units. decoder hidden units. cases multilayer network single maxout hidden layer compute conditional probability target word minibatch stochastic gradient descent algorithm together adadelta train model. update direction computed using minibatch sentences. trained model approximately days. model trained beam search translation approximately maximizes conditional probability sutskever used approach generate translations neural machine translation model. table list translation performances measured bleu score. clear table cases proposed rnnsearch outperforms conventional rnnencdec. importantly performance rnnsearch high conventional phrase-based translation system sentences consisting known words considered. signiﬁcant achievement considering moses uses separate monolingual corpus addition parallel corpora used train rnnsearch rnnencdec. figure four sample alignments found rnnsearch-. x-axis y-axis plot correspond words source sentence generated translation respectively. pixel shows weight annotation j-th source word i-th target word grayscale arbitrary sentence. three randomly selected samples among sentences without unknown words length words test set. motivations behind proposed approach ﬁxed-length context vector basic encoder–decoder approach. conjectured limitation make basic encoder–decoder approach underperform long sentences. fig. performance rnnencdec dramatically drops length sentences increases. hand rnnsearch- rnnsearch- robust length sentences. rnnsearch especially shows performance deterioration even sentences length more. superiority proposed model basic encoder–decoder conﬁrmed fact rnnsearch- even outperforms rnnencdec- table bleu scores trained models computed test set. second third columns show respectively scores sentences sentences without unknown word themselves reference translations. note rnnsearch- trained much longer performance development stopped improving. disallowed models generate tokens sentences unknown words evaluated proposed approach provides intuitive inspect alignment words generated translation source sentence. done visualizing annotation weights fig. matrix plot indicates weights associated annotations. positions source sentence considered important generating target word. alignments fig. alignment words english french largely monotonic. strong weights along diagonal matrix. however also observe number non-trivial non-monotonic alignments. adjectives nouns typically ordered differently french english example fig. ﬁgure model correctly translates phrase rnnsearch able correctly align jumping words looked word back time complete whole phrase strength soft-alignment opposed hard-alignment evident instance fig. consider source phrase translated hard alignment helpful translation must consider word following determine whether translated soft-alignment solves issue naturally letting model look example model able correctly translate observe similar behaviors presented cases fig. additional beneﬁt soft alignment naturally deals source target phrases different lengths without requiring counter-intuitive mapping words nowhere clearly visible fig. proposed model much better conventional model translating long sentences. likely fact rnnsearch require encoding long sentence ﬁxed-length vector perfectly accurately encoding parts input sentence surround particular word. privil`ege d’admission droit d’un m´edecin reconnaˆıtre patient l’hˆopital centre m´edical d’un diagnostic prendre diagnostic fonction ´etat sant´e. privil`ege d’admission droit d’un m´edecin d’admettre patient hˆopital centre m´edical pour effectuer diagnostic proc´edure selon statut travailleur soins sant´e l’hˆopital. type d’exp´erience fait partie initiatives disney pour prolonger dur´ee nouvelles d´evelopper liens avec lecteurs num´eriques deviennent plus complexes. previous example rnnencdec began deviating actual meaning source sentence generating approximately words point quality translation deteriorates basic mistakes lack closing quotation mark. genre d’exp´erience fait partie efforts disney pour prolonger dur´ee s´eries cr´eer nouvelles relations avec publics plateformes num´eriques plus plus importantes a-t-il ajout´e. conjunction quantitative results presented already qualitative observations conﬁrm hypotheses rnnsearch architecture enables reliable translation long sentences standard rnnencdec model. similar approach aligning output symbol input symbol proposed recently graves context handwriting synthesis. handwriting synthesis task model asked generate handwriting given sequence characters. work used mixture gaussian kernels compute weights annotations location width mixture coefﬁcient kernel predicted alignment model. speciﬁcally alignment restricted predict location location increases monotonically. main difference approach that modes weights annotations move direction. context machine translation severe limitation reordering often needed generate grammatically correct translation approach hand requires computing annotation weight every word source sentence word translation. drawback severe task translation input output sentences words. however limit applicability proposed scheme tasks. since bengio introduced neural probabilistic language model uses neural network model conditional probability word given ﬁxed number preceding words neural networks widely used machine translation. however role neural networks largely limited simply providing single feature existing statistical machine translation system re-rank list candidate translations provided existing system. instance schwenk proposed using feedforward neural network compute score pair source target phrases score additional feature phrase-based statistical machine translation system. recently kalchbrenner blunsom devlin reported successful neural networks sub-component existing translation system. traditionally neural network trained target-side language model used rescore rerank list candidate translations although approaches shown improve translation performance stateof-the-art machine translation systems interested ambitious objective designing completely translation system based neural networks. neural machine translation approach consider paper therefore radical departure earlier works. rather using neural network part existing system model works generates translation source sentence directly. conventional approach neural machine translation called encoder–decoder approach encodes whole input sentence ﬁxed-length vector translation decoded. conjectured ﬁxed-length context vector problematic translating long sentences based recent empirical study reported pouget-abadie paper proposed novel architecture addresses issue. extended basic encoder–decoder letting model search input words annotations computed encoder generating target word. frees model encode whole source sentence ﬁxed-length vector also lets model focus information relevant generation next target word. major positive impact ability neural machine translation system yield good results longer sentences. unlike traditional machine translation systems pieces translation system including alignment mechanism jointly trained towards better log-probability producing correct translations. tested proposed model called rnnsearch task english-to-french translation. experiment revealed proposed rnnsearch outperforms conventional encoder–decoder model signiﬁcantly regardless sentence length much robust length source sentence. qualitative analysis investigated alignment generated rnnsearch able conclude model correctly align target word relevant words annotations source sentence generated correct translation. perhaps importantly proposed approach achieved translation performance comparable existing phrase-based statistical machine translation. striking result considering proposed architecture whole family neural machine translation proposed recently year. believe architecture proposed promising step toward better machine translation better understanding natural languages general. challenges left future better handle unknown rare words. required model widely used match performance current state-of-the-art machine translation systems contexts. authors would like thank developers theano acknowledge support following agencies research funding computing support nserc calcul qu´ebec compute canada canada research chairs cifar. bahdanau thanks support planet intelligent systems gmbh. also thank felix hill bart merri´enboer jean pouget-abadie coline devin tae-ho kim. references axelrod domain adaptation pseudo in-domain data selection. proceedings conference empirical methods natural language processing pages association computational linguistics. bastien lamblin pascanu bergstra goodfellow bergeron bouchard bengio theano features speed improvements. deep learning unsupervised feature learning nips workshop. merrienboer gulcehre bougares schwenk bengio learning phrase representations using encoder-decoder statistical machine translation. proceedings empiricial methods natural language processing appear. merri¨enboer bahdanau bengio properties neural machine translation encoder–decoder approaches. eighth workshop syntax semantics structure statistical translation. appear. devlin zbib huang lamar schwartz makhoul fast robust neural network joint models statistical machine translation. association computational linguistics. forcada ˜neco recursive hetero-associative memories translation. mira moreno-d´ıaz cabestany editors biological artiﬁcial computation neuroscience technology volume lecture notes computer science pages springer berlin heidelberg. graves jaitly mohamed a.-r. hybrid speech recognition deep bidirectional lstm. automatic speech recognition understanding ieee workshop pages hermann blunsom multilingual distributed representations without word alignment. proceedings second international conference learning representations kalchbrenner blunsom recurrent continuous translation models. proceedings conference empirical methods natural language processing pages association computational linguistics. koehn marcu statistical phrase-based translation. proceedings conference north american chapter association computational linguistics human language technology volume naacl pages stroudsburg usa. association computational linguistics. curse sentence length neural machine translation using automatic segmentation. eighth workshop syntax semantics structure statistical translation. appear. schwenk continuous space translation models phrase-based statistical machine translation. boitet editors proceedings international conference computational linguistics pages indian institute technology bombay. schwenk dchelotte gauvain j.-l. continuous space language models statistical machine translation. proceedings coling/acl main conference poster sessions pages association computational linguistics. proposed scheme section general framework freely deﬁne instance activation functions recurrent neural networks alignment model here describe choices made experiments paper. activation function gated hidden unit recently proposed gated hidden unit alternative conventional simple units element-wise tanh. gated unit similar long short-term memory unit proposed earlier hochreiter schmidhuber sharing ability better model learn long-term dependencies. made possible computation paths unfolded product derivatives close paths allow gradients backward easily without suffering much vanishing effect therefore possible lstm units instead gated hidden unit described here done similar context sutskever state employing gated hidden units computed m-dimensional embedding word output reset gates represented -of-k vector simply column embedding matrix rm×k. whenever possible omit bias terms make equations less cluttered. update gates allow hidden unit maintain previous activation reset gates control much information previous state reset. compute logistic sigmoid function. step decoder compute output probability multi-layered function single hidden layer maxout units normalize output probabilities softmax function alignment model alignment model designed considering model needs evaluated times sentence pair lengths order reduce computation singlelayer multilayer perceptron rm×kx word embedding matrix. rn×n weight matrices. word embedding dimensionality number hidden units respectively. usual logistic sigmoid function. word embedding matrix target language. rn×m rn×n rn×n weights. again word embedding dimensionality number hidden units respectively. initial hidden state computed tanh table learning statistics relevant information. update corresponds updating parameters using single minibatch. epoch pass training set. average conditional log-probabilities sentences either training development set. note lengths sentences differ. j-th annotation source sentence rn×n rn×n weight matrices. note model becomes encoder–decoder decoder state context last generated word deﬁne probability target word random orinitialized recurrent weight matrices thogonal matrices. initialized sampling element gaussian distribution mean variance elements bias vectors initialized zero. weight matrix initialized sampling gaussian distribution mean variance update implementation requires time proportional length longest sentence minibatch. hence minimize waste computation every update retrieved sentence pairs sorted according lengths split minibatches. training data shufﬂed training traversed sequentially manner. admitting privilege right doctor admit patient hospital medical centre carry diagnosis procedure based status health care worker hospital. privil`ege d’admission droit d’un m´edecin vertu statut membre soignant d’un hˆopital d’admettre patient dans hˆopital centre m´edical d´elivrer diagnostic traitement. privil`ege d’admission droit d’un m´edecin reconnaˆıtre patient l’hˆopital centre m´edical d’un diagnostic prendre diagnostic fonction ´etat sant´e. rnnsearch- privil`ege d’admission droit d’un m´edecin d’admettre patient hˆopital centre m´edical pour effectuer diagnostic proc´edure selon statut travailleur soins sant´e l’hˆopital. privil`ege admettre droit d’un m´edecin d’admettre patient dans hˆopital centre m´edical pour effectuer diagnostic proc´edure fond´ee situation tant travailleur soins sant´e dans hˆopital. kind experience part disney’s efforts extend lifetime series build relationships audiences digital platforms becoming ever important added. type d’exp´erience entre dans cadre efforts disney pour ´etendre dur´ee s´eries construire nouvelles relations avec public grˆace plateformes num´eriques sont plus plus importantes a-t-il ajout´e. type d’exp´erience fait partie initiatives disney pour prolonger dur´ee nouvelles d´evelopper liens avec lecteurs num´eriques deviennent plus complexes. genre d’exp´erience fait partie efforts disney pour prolonger dur´ee s´eries cr´eer nouvelles relations avec publics plateformes num´eriques plus plus importantes a-t-il ajout´e. genre d’exp´erience fait partie efforts disney ´etendre dur´ee s´erie construire nouvelles relations avec public biais plates-formes num´eriques deviennent plus plus important at-il ajout´e. press conference thursday blair stated nothing video might constitute reasonable motive could lead criminal charges brought mayor. conf´erence presse jeudi blair afﬁrm´e qu’il avait rien dans cette vid´eo puisse constituer motifs raisonnables pouvant mener d´epˆot d’une accusation criminelle contre maire. lors conf´erence presse jeudi blair qu’il avait rien dans cette vid´eo pourrait constituer motivation raisonnable pouvant entraˆıner accusations criminelles port´ees contre maire. lors d’une conf´erence presse jeudi blair d´eclar´e qu’il avait rien dans cette vid´eo pourrait constituer motif raisonnable pourrait conduire accusations criminelles contre maire. lors d’une conf´erence presse jeudi blair d´eclar´e qu’il avait rien dans cette vido pourrait constituer motif raisonnable pourrait mener accusations criminelles portes contre maire. table translations generated rnnenc- rnnsearch- long source sentences selected test set. source sentence also show goldstandard translation. translations google translate made august", "year": 2014}