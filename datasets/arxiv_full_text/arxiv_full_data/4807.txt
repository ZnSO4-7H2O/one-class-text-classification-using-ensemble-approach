{"title": "Taxonomy grounded aggregation of classifiers with different label sets", "tag": ["cs.AI", "cs.LG"], "abstract": "We describe the problem of aggregating the label predictions of diverse classifiers using a class taxonomy. Such a taxonomy may not have been available or referenced when the individual classifiers were designed and trained, yet mapping the output labels into the taxonomy is desirable to integrate the effort spent in training the constituent classifiers. A hierarchical taxonomy representing some domain knowledge may be different from, but partially mappable to, the label sets of the individual classifiers. We present a heuristic approach and a principled graphical model to aggregate the label predictions by grounding them into the available taxonomy. Our model aggregates the labels using the taxonomy structure as constraints to find the most likely hierarchically consistent class. We experimentally validate our proposed method on image and text classification tasks.", "text": "ﬁers explicitly used. hierarchy agnostic classiﬁers either multi-class multi-label binary classiﬁers possibly trained subsets classes. paper address problem aggregating output multiple classiﬁers using pre-deﬁned class taxonomy since several applications need grounded references background taxonomies. motivating example consider task visual object recognition —given image task predict likely object image. considerable amount progress achieved computer vision community various pre-trained state-of-the-art classiﬁers available. since number possible objects quite large classiﬁers trained diﬀerent datasets class labels considerable amount research eﬀort also time takes weeks months train) spent training classiﬁers. hence interested reusing multiple classiﬁers given image aggregating scores predicting ﬁnal label. however combining pre-trained classiﬁers poses challenges list address paper. describe problem aggregating label predictions diverse classiﬁers using class taxonomy. taxonomy available referenced individual classiﬁers designed trained mapping output labels taxonomy desirable integrate eﬀort spent training constituent classiﬁers. hierarchical taxonomy representing domain knowledge diﬀerent from partially mappable label sets individual classiﬁers. present heuristic approach principled graphical model aggregate label predictions grounding available taxonomy. model aggregates labels using taxonomy structure constraints likely hierarchically consistent class. experimentally validate proposed method image text classiﬁcation tasks. several real-world classiﬁcation problems text categorization content classiﬁcation patent codes codes diseases etc.) classes predicted naturally organized large predeﬁned class hierarchy class taxonomy—typically tree however state-of-the-art results obtained classiﬁers typically ignore class hierarchy treat class separately. could taxonomy available training classihierarchically inconsistent predictions—since classiﬁers trained ignoring taxonomy class predictions hierarchically consistent. consider example classiﬁer trained bels mutually exclusive possible classiﬁer give high score doberman score though taxonomy implies doberman hence also receive high score. problem persists across diﬀerent classiﬁers—if classiﬁer predicts instance another doberman need aggregate classiﬁers hierarchically consistent way. diﬀerent classiﬁer accuracies—the individual classiﬁers diﬀerent accuracies accounted aggregating classiﬁers. moreover reported accuracies based classes trained also ignore hierarchy. model classiﬁer performance using taxonomy estimate accuracies using validation set. depth predicted class label using class taxonomy actually predict class label label sets used train classiﬁers. generalize notion label instance path taxonomy. path starts root taxonomy terminates class taxonomy. propose strategies decide terminate path give ﬁnal prediction. related work —there rich literature area hierarchical classiﬁcation survey) deals training classiﬁers explicitly accounting class hierarchy. however paper primarily concerned aggregating pretrained classiﬁers hierarchically consistent way. attempt re-train classiﬁers using taxonomy. proposed algorithms also used aggregate labels collected multiple annotators crowdsourcing. sophisticated techniques exist binary categorical ordinal labels best knowledge methods labels nouns grouped sets cognitive synonyms expressing distinct concept synsets interlinked means conceptualsemantic lexical relations. frequently encoded relation among synsets super-subordinate relation another area research related problem setting integrating label-sets case e-commerce catalog integration catalog integration problem mapping source product taxonomy containing textual descriptions products target master taxonomy various techniques involve jointly learning class mappings without data labeled labelsets estimating re-training master taxonomy using statistics and/or structure source taxonomy. speciﬁc formulation propose quite diﬀerent label-set mapping problem since want lazily combine predictions diﬀerently trained classiﬁers background taxonomy. motivating example mapping object classiﬁcation predictions wordnet want re-use eﬀort spent creating tuning existing classiﬁers view lens wordnet naturally describe objects images world knowledge. problem setting constituent classiﬁers free evolve change area expertise. thus ensemble meta-learning methods learn accuracy estimates dynamic model selection techniques apply setting. class taxonomies—in paper concerned classiﬁcation problems class labels organized hierarchically class taxonomies. hierarchy imposes parent-child is-a relation among classes—an instance belonging speciﬁc class also belongs ancestor classes. formally assumption classiﬁers multiclass classiﬁers trained without knowledge class hierarchy. also possible classiﬁer category parent class also separate category descendant class well score assigned class instance classiﬁer without loss generality assume scores probabilities generalization instance label paths taxonomy —given taxonomy true label instance completely speciﬁed classes leaf nodes hierarchy. however practice class label speciﬁed class taxonomy higher leaf node. example true label instance maybe doberman however class label speciﬁed annotator could hypernyms example dog. generalize notion label instance label path taxonomy. path starts root taxonomy terminates class taxonomy. paths leaf node completely speciﬁed. instance class label taxonomy consistency —since classiﬁers trained without knowledge taxonomy likely scores hierarchically consistent. example instance possibly higher score doberman dog. however taxonomy know doberman hence high score doberman. deﬁne following notion taxonomy consistency instance true class label taxonomy consistency implies taxonomy is-a relationship trained classiﬁers classiﬁer trained using subset classes ckj} score assigned class instance classiﬁer task aggregate scores estimate best path taxonomy pathi every instance true class label taxonomically consistent. could multiple paths path need necessarily leaf node ﬁrst present heuristic solution propagating classiﬁer scores upward particular class ancestors taxonomy navigating is-a hierarchy upwards. scores multiple classiﬁers node aggregated summing ﬁnal path estimated traversing taxonomy root terminating class based entropy children. speciﬁcally figure illustration heuristic score propagation algorithm diﬀerent classiﬁers classiﬁers assign following scores test instance ﬁnal scores class propagating scores ancestors summing also shown. ﬁnal predicted label path instance marked rectangles. note path terminates non-leaf class working dog. heuristic method assumes classiﬁers performance aggregates scores. section cast label aggregation problem inference problem appropriately deﬁned graphical model proposed graphical model kinds nodes discrete binary nodes corresponding hierarchically organized classes taxonomy continuous nodes corresponding classiﬁer scores class taxonomy corresponds binary discrete node true binary label instance class taxonomy. discrete nodes organized hierarchically is-a relationship deﬁned speciﬁed graphical model model parameters task likely conﬁguration class nodes given values classiﬁer nodes actually compute marginal probability class nodes traverse taxonomy using method described graphical model described mixed discrete-gaussian network mixed discretegaussian network consists discrete continuous nodes. models assumes conditional distribution continuous variables given discrete multivariate gaussian. networks exact inference algorithms exist permits local computation junction tree exact probabilities means variances. this reasonable model scores constructed linear combination many features. probabilistic classiﬁers since scores range ﬁrst apply logit inverse softmax transformation scores makes scores approximately normal. discrete labels—we also incorporate classiﬁers produce discrete labels. instead bi-normal distribution following parameters deﬁne conditional probability distribution classiﬁer node algorithm. algorithm eﬃcient iterative procedure estimate parameters presence missing/hidden data. unknown hidden true label missing data case. iteration algorithm consists steps expectation-step maximization-step. m-step involves maximization lower bound log-likelihood reﬁned e-step. speciﬁcally e-step obtain marginal probabilities class nodes given current estimate model parameters observed classiﬁer scores. m-step re-estimate model parameters given class labels taxonomy class nodes. diﬀerence estimatentry level categories—the proposed algorithm returns path taxonomy. ﬁnal label computed terminating path based entropy. however domains want terminate based domain speciﬁc criterion. example object recognition task want assign label object ‘entry level’ category–the labels people name object example entry level category rottweiler dog. appropriately modify termination strategy account suitable backing path using ideas datasets—for visual object recognition subset images imagenet ilsvrc detection challenge dataset dataset consists basic categories total wordnet synsets node taxonomy extract sub-hierarchy rooted basic category animal resulting taxonomy nodes. images categories completely non-overlapping aggregated images forms ﬁnal dataset experiments. text categorization experiments used benchmark reuters corpus volume news articles dataset hierarchical multi-labeled dataset thousand articles tagged along topics industries regions facets. used class industries taxonomy experiments. experimental setup—we data training validation testing. randomly select subset classes taxonomy train multi-class classiﬁer completely ignoring class taxonomy. object detection take activations sixth hidden layer deep convolutional neural network features train linear multi-class features text categorization used standard token pre-processing available dataset tfidf representation trained linear svm. classiﬁer trained using train split validation split used tune hyper-parameters. experimented classiﬁers operating diﬀerent class labels classiﬁers training sample belongs multiple labels training-set construction randomly assign instance classes order avoid confounding classiﬁer. ﬁnal model performance evaluated test split representation classes taxonomy. evaluation metrics—accuracy good metric evaluating performance various classiﬁers hierarchical relations among classes. classiﬁers operate subset labels taxonomy test possibly instances classes taxonomy. rich literature evaluation measures hierarchical classiﬁcation review). evaluation choose lowest common ancestor based precision recall f-score measures recommended measures essentially hierarchical versions precision recall f-score based actual predicted class. results—table shows mean hierarchical precision recall f-score test individual classiﬁers also proposed heuristic graphical model based aggregation algorithms visual object recognition text classiﬁcation tasks. graphical model inference used junction tree algorithm bayes toolbox. appropriate thresholds deciding terminal class tuned using validation set. heuristic algorithm path termination based entropy. graphical model terminal node decided based marginal probabilities. visual object recognition dataset three measures performance aggregation algorithms better equal best performing classiﬁer ensemble graphical model based approach outperforming heuristic approach. object recognition dataset also show results alternate termination stratbacking suitable entry level class alternate termination strategy gave improvements. text classiﬁcation dataset graphical model outperforms best classiﬁer terms precision large margin. achieved without modifying retraining constituent classiﬁers way. proposed solutions based heuristic score propagation taxonomy principled approach using graphical model. proposed algorithms experimentally validated real world problems visual object recognition text categorization. plan extend approach taxonomies cycles curated knowledge graphs ontologies. model implicity assumes classiﬁer label sets mappable classes taxonomy. plan integrate ideas catalog integration directly graphical model.", "year": 2015}