{"title": "EgoTransfer: Transferring Motion Across Egocentric and Exocentric  Domains using Deep Neural Networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Mirror neurons have been observed in the primary motor cortex of primate species, in particular in humans and monkeys. A mirror neuron fires when a person performs a certain action, and also when he observes the same action being performed by another person. A crucial step towards building fully autonomous intelligent systems with human-like learning abilities is the capability in modeling the mirror neuron. On one hand, the abundance of egocentric cameras in the past few years has offered the opportunity to study a lot of vision problems from the first-person perspective. A great deal of interesting research has been done during the past few years, trying to explore various computer vision tasks from the perspective of the self. On the other hand, videos recorded by traditional static cameras, capture humans performing different actions from an exocentric third-person perspective. In this work, we take the first step towards relating motion information across these two perspectives. We train models that predict motion in an egocentric view, by observing it from an exocentric view, and vice versa. This allows models to predict how an egocentric motion would look like from outside. To do so, we train linear and nonlinear models and evaluate their performance in terms of retrieving the egocentric (exocentric) motion features, while having access to an exocentric (egocentric) motion feature. Our experimental results demonstrate that motion information can be successfully transferred across the two views.", "text": "mirror neurons observed primary motor cortex primate species particular humans monkeys. mirror neuron ﬁres person performs certain action also observes action performed another person. crucial step towards building fully autonomous intelligent systems humanlike learning abilities capability modeling mirror neuron. hand abundance egocentric cameras past years offered opportunity study vision problems ﬁrst-person perspective. great deal interesting research done past years trying explore various computer vision tasks perspective self. hand videos recorded traditional static cameras capture humans performing different actions exocentric third-person perspective. work take ﬁrst step towards relating motion information across perspectives. train models predict motion egocentric view observing exocentric view vice versa. allows models predict egocentric motion would look like outside. train linear nonlinear models evaluate performance terms retrieving egocentric motion features access exocentric motion feature. experimental results demonstrate motion information successfully transferred across views. mirror neuron neuron ﬁres animal acts animal observes action performed another. thus neuron mirrors behavior other though observer acting. neurons directly observed figure main objective work learn mapping egocentric exocentric motion features. time-synchronized egocentric-exocentric pairs videos extract different types motion features views train different mapping models automatically learn relationship. achieving human-like learning abilities requires modeling mirror neuron phenomenon. words intelligent systems able relate visual information third person perspective ﬁrst person perspective vice versa. watching human running intelligent system able imagine visual world would look like system actually attempted act. believe perfect time taking ﬁrst step towards modeling concept computer vision standpoint. past years egocentric cameras provided opportunity study ﬁrst person vision widely extensively. thanks affordability wearable cameras smart glasses interesting research done ranging action recognition identiﬁcation localization. history computer vision however goes beyond past years. tremendous amount research conducted different areas computer vision traditional types videos collected using static cameras canonical oblique view. refer videos exocentric third-person videos. given fact egocentric vision relatively area compared exocentric vision amount available exocentric data drastically egocentric data. example several datasets computer vision community action activity recognition exocentric domain nearly many egocentric datasets. order take advantage vast amount knowledge exists exocentric domain need systematic adaptation exocentric information egocentric domain. study explore relationship egocentric exocentric visual motion transfer. words seek learn transformation motion features exocentric space egocentric space vice versa. collect dataset egocentric exocentric videos captured simultaneously body mounted egocentric static exocentric cameras capturing people performing diverse actions covering broad spectrum motions. divide pair videos time-synchronized short clips frames extract motion features clip view illustrated figure provide feature pairs views. train different linear non-linear mappings learn transformation views. testing performance models evaluate capability terms retrieving correct match view. test video pairs therefore feature pairs extracted simultaneously recorded exocentric videos. feature ﬁrst view view evaluate capability terms ﬁnding correct paired video/feature target set. words retrieve it’s corresponding video clip test set. evaluate analyze performance different mapping methods different scenarios. first person vision also known egocentric vision became increasingly popular vision community. research conducted past years including object detection activity recognition video summarization motion egocentric vision particular studied fundamental features ﬁrst person video analysis. costante explore convolutional neural networks learn best visual features figure testing process evaluating models. clip view ﬁrst extract motion features. apply models transform egocentric motion features exocentric space. exocentric videos ranked based feature similarity transformed egocentric features rank correct ground-truth exocentric video used evaluating performance. compare models terms ranking capability. predict camera motion egocentric videos. grauman propose learning-based approach detect user engagement using long-term egomotion cues. jayaraman learn feature mapping pixels video frame space equivariant various motion classes. proposed twin stream network architecture analyze appearance information motion information egocentric videos used features recognize egocentric activities. action activity recognition egocentric videos topics community. ogaki jointly used motion motion compute sequence global optical egocentric videos. poleg proposed compact convolutional neural network architecture long-term activity recognition egocentric videos extended egocentric video segmentation. singh used cnns end-to-end learning classiﬁcation actions using hand pose head motion saliency map. used gaze information addition features perform action recognition. proposed attention based approach activity recognition detecting visually salient objects. relationship egocentric top-view videos explored tasks human identiﬁcation temporal correspondence. relationship egocentric top-view information explored tasks human identiﬁcation semantic segmentation temporal correspondence. work relate different views motion considered knowledge transfer domain adaptation task. knowledge transfer used multi-view action recognition multiple exocentric views action related other. multiple exocentric views allows geometrical visual reasoning since nature data different views actor visible cameras. contrast paper aims automatically learn mappings drastically different views egocentric exocentric. best knowledge ﬁrst attempt relating domains transferring motion information. train linear baseline models different nonlinear models evaluate possibility learning mapping egocentric exocentric motion features. follows explain details mapping scheme alongside implementation details. main goal transform motion features source view target view egocentric exocentric. words learn mapping models egocentric exocentric space vice versa. shown ﬁgure datapoint pairs case training learn transformation maps view another i.e. estimates mapping function xego xexo datapoints fact spatiotemporal features hoof features capturing histogram oriented optical ﬂow. train linear non-linear mapping models using pairs. evaluate learned models test terms capability retrieving groundtruth paired feature view. shown ﬁgure ego/exocentric video extract motion feature descriptor retrieve correct paired video view. rank videos view. rank correct match metric evaluate different models different scenarios. extracting motion features represent short video clip view using motion feature. different motion features different levels complexity employed. simple feature histogram oriented optical also complicated spatiotemporal feature known convolutional neural networks proposed study mapping capacity features from/to egocentric videos to/from exocentric videos using different mapping methods. features feature descriptors computed using convolutional neural network order reduce computational complexity training models reduce dimensionality using principal component analysis train different linear models uniform transformation linear regression linear regression regularization baselines. direct matching might question feature descriptors would perform directly compared. words simply retrieve exocentric videos directly comparing egocentric query feature. answer whether complicated mappings necessary evaluate performance direct matching spaces. direct matching assumes uniform transformation across spaces. given source target domains totally different direct matching expected outperform chance signiﬁcantly. experiments also validate expectation direct matching always achieves near-random performances. linear regression tried training linear regression model source domain target domain. linear regression following form computed using closed form solution least squares optimization. experiments indicate linear regression consistently outperforms chance direct matching large margin suffers limits linear models. consistent edge compared direct matching suggests better mappings possible. regularized linear regression regularization shown improve regression models preventing overﬁtting data converging trivial solutions. tried regularized linear regression models evaluated accuracies well. experiments regularization improve accuracy considerably compared linear regression scenarios. figure non-linear mapping model fully connected layers relu activation batch normalization. network tries reconstruct features view another view. non-linear mapping reconstruction objective train non-linear mapping model containing layers fully connected layers relu activations batch normalization performed layer. please ﬁgure architecture designed purpose reconstructing target features source features used least square loss adam optimizer training. hoof features dimensionality fully connected layers features dimensionalities experiments show simple architecture able outperform linear model large margin cases. numbers training testing examples found table first train model using batch size epochs. epoch number validation loss minimum finally validation training train model scratch optimum number epochs. keras platform theano backend implement test network. two-stream classiﬁcation network two-stream classiﬁcation networks popular architectures tasks matching classiﬁcation. trained two-stream neural network requires pairs features input sends product non-linear transformation sigmoid function enforce notion probability output. choose output corresponding feature pairs noncorresponding pairs. intuition behind network assumption common space views non-linearly transformable therefore product maximized. shown ﬁgure dense layers relu activation batch normalization applied stream. hoof features dimensionalities dense layers stream features dimensionalities training positive negative figure two-stream classiﬁcation based network. dense layer dimenionalities hoof features dimensionalities again activations relu batch normalization performed layer. pairs needed. original training data contains positive pairs. generate negative pairs pick random non-correspondent features views. since negative examples drastically outnumber positive ones sample weights order balance training data. weights negative positive examples order non-linear mapping method ﬁrst train model using batch size epochs. epoch number validation loss minimum. finally validation training train model scratch optimum number epochs. optimizing network binary cross entropy loss. shown ﬁgure query clip source domain test match target domain extract motion features query video. trained linear non-linear models transform query source feature target domain. finally compare features extracted target transformed query feature rank based that. rank target videos evaluate performance mapping models terms ranking target videos. would ideally want correct match query video appear top. linear models nonlinear mapping reconstruction objective process straightforward. two-stream network feed query source feature paired target features acquire score denotes probability target feature matched best knowledge dataset containing simultaneously recorded egocentric exocentric wide range ﬁrst third person motions. therefore collect dataset containing video pairs. video pair contains egocentric exocentric video. pair videos temporally aligned cause temporal features correspond other. examples shown ﬁgure pair collected asking actor perform range actions covering broad range motions front exocentric camera wearing egocentric body-worn camera capturing actor’s motion ﬁrst person perspective. pair videos divided pairs temporally aligned short clips feature descriptors extracted clip. details number videos features used training testing included following table. order increase number training testing examples also ﬂipped versions egocentric side view videos also rotated versions top-view videos test consists paired videos source target view. feature source view attempt correct pair target view. transform feature target space using learned mapping models. test feature evaluate matching performance linear non-linear mapping methods terms ranking. compute area curve cumulative matching curve quantitative measure performance. evaluate mappings different features hoof four different scenarios including egocentric top-view top-view egocentric egocentric side-view side-view egocentric. shown ﬁgures transforming features top-view vice versa non-linear models drastically outperform linear models. particular two-stream classiﬁcation network achieves highest accuracy. further transferring features across egocentric side-view non-linear mapping method outperforms two-stream classiﬁcation network. summary quantitative results features found table mapping egocentric top-view figure shows cumulative matching curves mapping egocentric top-view vice versa. scenarios models outperform chance conﬁrms possibility ﬁnding mapping spaces. non-linear methods perform favorably compared linear models. two-stream classiﬁcation based network achieves highest accuracy. mapping egocentric side-view figure shows cumulative matching curves mapping egocentric top-view top-view egocentric. scenarios linear mappings outperform chance under-perform non-linear models. also non-linear mapping reconstruction objective achieves best performance. here evaluate retrieval performance mapping methods using features. figures show retrieval performance transforming features across side respectively. observed ﬁgure non-linear mapping outperforms linear mapping two-stream classiﬁcation model not. hand ﬁgure transform features egocentric side view non-linear mapping perform well linear methods. here two-stream network drastically outperforms both. generally seen using features non-linear models consistently outperform linear models. believe features extracted last fully connected layers dcnns offer fully meaningful independent information. makes suitable linear models. result every non-linear model necessarily able outperform linear model trained features. according this expect non-linear models estimate identity transformation. however shows hard non-linear fully connected layer learn identity transformation accurately. therefore additional figure examples video pairs collected dataset. pair contains egocentric exocentric video simultaneously recorded actor performing actions walking jogging running hand waving hand clapping boxing push-ups order capture wide variety motions. mapping egocentric top-view figure shows cumulative matching curves mapping egocentric top-view top-view egocentric. seen scenarios non-linear mapping achieves best performance. mapping egocentric side-view figure illustrates performance mapping egocentric side-view vice versa. here two-stream classiﬁcation network achieves best performance. inspired mirror neuron concept explored possibility transforming motion information across drastically different views egocentric exocentric showed possible learn transformation linear non-linear across spaces. observe depending scenario feature type linear models outperform non-linear models. opposite happen well. overall using hoof features transferring motion side view leads higher accuracy compared top-view. intuitively makes sense since side views often visually similar contain information regarding activity. table performance different mapping methods hoof features. linear mapping reconstruction objective achieves favorable result mapping egocentric side view two-stream network outperforms rest mapping egocentric top-view. mapping egocentric side view generally achieves higher accuracy compared egocentric top-view. intuitively justiﬁable given fact top-view drastic difference egocentric compared side view. table performance different mapping methods features. non-linear mapping reconstruction objective gives favorable results mapping egocentric top-view two-stream network outperforms rest mapping egocentric side-view. features applied data target domain. latter recorded egocentric video ofﬂine online better track person top-view surveillance camera. consider using sophisticated spatio-temporal features well domain adaptation task transfer approaches. work explored possibility transferring knowledge across egocentric exocentric domains. believe work stepping stone explore relationship domains possible applications action recognition identiﬁcation. figure mapping egocentric view top-view top-view egocentric view using hoof features. illustrated ﬁgures cases linear regression regularized linear regression perform better random. cases non-linear models drastically outperform linear methods. stream classiﬁcation network achieves highest auc. figure mapping egocentric view side-view side-view egocentric view using hoof features. cases linear regression regularized linear regression perform better random. non-linear models drastically outperform linear methods. however case non-linear mapping model outperforms stream classiﬁcation network. figure mapping egocentric view top-view top-view egocentric view using features. cases linear regression regularized linear regression perform better random. non-linear mapping outperforms linear models. two-stream classiﬁcation network perform better linear models. figure mapping egocentric view side-view side-view egocentric view using features. cases linear regression regularized linear regression perform better random. matsuo yamada ueno naito. attentionbased activity recognition egocentric video. ieee conference computer vision pattern recognition workshops june ogaki kitani sugano sato. coupling eye-motion ego-motion features ﬁrst-person activity recognition. ieee computer society conference computer vision pattern recognition workshops pages june singh arora jawahar. first person action ieee recognition using deep learned descriptors. conference computer vision pattern recognition june tran bourdev fergus torresani paluri. learning spatiotemporal features convolutional networks. ieee international conference computer vision pages ieee", "year": 2016}