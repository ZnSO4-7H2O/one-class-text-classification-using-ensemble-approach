{"title": "Iterative Alternating Neural Attention for Machine Reading", "tag": ["cs.CL", "cs.NE"], "abstract": "We propose a novel neural attention architecture to tackle machine comprehension tasks, such as answering Cloze-style queries with respect to a document. Unlike previous models, we do not collapse the query into a single vector, instead we deploy an iterative alternating attention mechanism that allows a fine-grained exploration of both the query and the document. Our model outperforms state-of-the-art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children's Book Test (CBT) dataset.", "text": "propose novel neural attention architecture tackle machine comprehension tasks answering cloze-style queries respect document. unlike previous models collapse query single vector instead deploy iterative alternating attention mechanism allows ﬁne-grained exploration query document. model outperforms state-of-the-art baselines standard machine comprehension benchmarks news articles children’s book test dataset. recently idea training machine comprehension models read understand answer questions text come closer reality principally factors. ﬁrst advent deep learning techniques allow manipulation natural language beyond surface forms generalize beyond relatively small amounts labeled data. second factor formulation standard machine comprehension benchmarks based cloze-style queries permit fast integration loops model conception experimental evaluation. cloze-style queries created deleting particular word natural-language statement. task guess word deleted. pragmatic approach recent work formed questions extracting sentence larger document. contrast considering stand-alone statement system required handle larger amount information possibly inﬂuence prediction missing word. contextual dependencies also injected removing word short human-crafted summary larger body text. abstractive nature summary likely demand higher level comprehension original text cases machine comprehension system presented ablated query document original query refers. missing word assumed appear document. encouraged recent success deep learning attention architectures propose novel neural attention-based inference model designed perform machine reading comprehension tasks. model ﬁrst reads document query using recurrent neural network then deploys iterative inference process uncover inferential links exist missing query word query document. phase involves novel alternating attention mechanism; ﬁrst attends parts query ﬁnds corresponding matches attending document. result alternating search back iterative inference process seed next search step. permits model reason different parts query sequential based information gathered previously document. ﬁxed number iterations model uses summary inference process predict answer. kadlec compress query single representation instead alternates attention query document obtain ﬁne-grained query representation within ﬁxed computation time. architecture tightly integrates previous ideas related bidirectional readers iterative attention processes obtains state-of-theart results machine comprehension datasets shows promise application broad range natural language processing tasks. advantages using cloze-style questions evaluate machine comprehension systems sufﬁcient amount training test data obtained without human intervention. corpora datasets. corpus generated well-known children’s books available project gutenberg. documents consist -sentence excerpts books. related query formed excerpt’s sentence replacing single word anonymous placeholder token. dataset divided four subsets depending type word replaced. subsets named entity common noun verb preposition. focus evaluation solely ﬁrst subsets i.e. cbtne cbt-cn since latter relatively simple demonstrated corpus generated news articles available website. documents given full articles themselves accompanied short bullet-point summary statements. instead extracting query articles themselves authors replace named entity within article summary anonymous placeholder token. model represented fig. workﬂow three steps. first encoding phase compute vector representations acting memory content input document query. next inference phase aims untangle complex semantic relationships linking document query order provide sufﬁciently strong evidence answer prediction successful. accomplish this iterative process that iteration alternates attentive memory accesses query document. finally prediction phase uses information gathered repeated attentions query document maximize probability correct answer. describe phases following sections. input encoding phase sequence words document query drawn vocabulary word represented continuous word embedding stored word embedding matrix |×d. sequence processed using recurrent neural network encoder gated recurrent units position input sequence takes input word embedding updates hidden state deﬁned figure model ﬁrst encodes query document means bidirectional networks. then deploys iterative inference mechanism alternates attending query encodings document encodings given query attended state. results alternating attention gated back inference gru. even encodings computed once query representation dynamic changes throughout inference process. ﬁxed number steps weights document attention used estimate probability answer modelled additional recurrent network. recurrent network iteratively performs alternating search step gather information useful predict answer. particular time step performs attentive read query encodings resulting query glimpse given current query glimpse extracts conditional document glimpse representing parts document relevant current query glimpse. turn attentive reads conditioned previous hidden state inference summarizing information gathered query document time inference uses glimpses update recurrent state thus decides information needs gathered complete inference process. recurrent state reset gate update gate respectively i{ruh} rh×d h{ruh} rh×h parameters sigmoid function elementwise multiplication. hidden state acts representation word context preceding sequence inputs x<i. order incorporate information future tokens choose process sequence reverse additional therefore encoding phase maps token contextual representation given concatenation forward backward hidden states denote contextual encodings word query document respectively. phase considered means uncover possible inference chain starts query document leads answer. inference network sigmoid output unit activation. fourth argument gate takes account multiplicative interactions query document glimpses making easier determine degree matching them. given query gate producing document gate producing inputs inference given reset version query document glimpses i.e. st−). intuitively model reviews query glimpse respect contents document glimpse vice versa. answer prediction ﬁxed number time-steps document attention weights obtained last search step used predict probability answer given document query formally follow apply pointer-sum loss train model used stochastic gradient descent adam optimizer initial learning rate batch size decay learning rate accuracy validation increase half-epoch i.e. batches batches initialize weights model sampling normal distribution following recurrent weights initialized orthogonal biases initialized zero. order stabilize learning clip gradients norm greater performed hyperparameter search embedding regularization inference steps embedding size encoder size inference size regularize model applying dropout rate input embeddings search gates sukhbaatar differences. first bilinear term instead simple product order compute importance query term current time step. simple bilinear attention successfully used second term allows bias attention mechanism towards words tend important across questions independently search st−. similar achieved original attention mechanism proposed without burden additional tanh layer. document attentive read alternating attention continues probing document given current query glimpse particular document attention weights computed based previous search state currently selected query glimpse note document attention also conditioned st−. allows model perform transitive reasoning document side i.e. previously obtained document information bias future attended locations particularly important natural language inference tasks gating search results order update recurrent state inference evolve basis information gathered current inference step i.e. st−) deﬁned however current query glimpse general document contain information speciﬁed query glimpse i.e. query document attention weights nearly uniform. include gating mechanism designed reset current query document glimpses case current search fruitful. formally implement gating mechanism element-wise multiplication rs+h gate takes form -layer feed-forward inputs query document attention mechanisms. found setting embedding regularization worked robustly across datasets. model implemented theano using keras library. computational complexity similar previous state-of-the-art models bidirectional encoder major bottleneck method computing document query encodings. alternating attention mechanism runs ﬁxed number steps orders magnitude smaller typical document query datasets repeated attentions require softmax locations typically fast recent architectures. thus computation cost comparable outperform latter models datasets tested. main result model sets stateof-the-art common noun category gaining points validation test best baseline reader performance partially reﬂected cbt-ne dataset. observe accuracy points validation reﬂect better performance test sits best baseline. cbtne missing word named entity appearing story likely less frequent common noun. found approximatively validation examples test examples contain answer never predicted training set. numbers considerably lower cbt-cn validation test examples respectively contain answer previously seen. ensembles fusing multiple models generally achieves better generalization. order investigate whether could help achieving better held-out performance cbt-ne adopt simple strategy average predictions models trained different random seeds case ensemble outperforms reader ensemble cbt-cn cbt-ne setting state-of-the-art task. cbt-ne achieves validation test performance accuracy points respectively cbt-cn shows additional fixed query attention order measure impact query attention step model constrain query attention weights uniform i.e. /|q| corresponds ﬁxing query representation average pooling bidirectional query encodings similar spirit previous work comparing line line query attention mechanism allows improvements points validation points test respect ﬁxing query representation throughout search process. similar scenario observed dataset. also report recent results stanford system came attention writearticle model slightly improves strong baseline percent validation percent test. note latter comparison inﬂuenced different training initialization strategies. first stanford uses glove embeddings pre-trained large external corpus. second system normalizes output probabilities candidate answers document. ensembles also report results using ensembled models. similarly single model case ensembles achieve state-of-the-art test performance validation test respectively outperforming previously published results. category analysis classiﬁed sample stories based type inference required guess answer. categories require local context matching around placeholder answer text exact match paraphrasing partial clue require higher reasoning skills multiple sentences ambiguous. example exact match examples question placeholder answer document share several neighboring exact words. table per-category performance stanford system. ﬁrst three categories require local context matching next global context matching coreference errors unanswerable questions tackled neural models perform similarly. seems iterative alternating attention inference better able solve difﬁcult examples ambiguous/hard. hypothesis that contrast stanford uses ﬁxedquery attention step iterative attention better explore documents queries. finally coreference errors includes examples critical coreference resolution errors make questions unanswerable. barrier achieving accuracies considerably estimate accurate ensemble model approaching near-optimal performance dataset. inspect query document attention weights example article dataset. title article dante turns grave italian language declines discusses decline italian language schools. plot shown figure locations attended query document left right column respectively. corresponds inference timestep ﬁrst step query attention focuses placeholder token local context generally important discriminate answer. model ﬁrst focuses entity corresponds greek article. point model still uncertain possible locations document query attention moves towards schools model hesitates italian european union satisfy query. step likely candidates european union rome timesteps unfold model learns needs important infer correct entity i.e. italian. query sits attended location document attention evolves become conﬁdent answer. that across examples query attention wanders near focuses placeholder location attempting discriminate identity using local context. particular datasets majority questions answered attending words directly neighbouring placeholder. aligns ﬁndings concerning state required reasoning inference levels dataset quite simple. would worthwhile formulate dataset placeholder harder infer using local neighboring words thereby necessitates deeper query exploration. finally across work ﬁxed number inference steps found using timesteps works well consistently across tested datasets. however hypothesize timesteps would beneﬁt harder examples. neural attention models applied recently sm¨org˚asbord machine learning natural language processing problems. include limited handwriting recognition digit classiﬁcation machine translation question answering caption generation general attention models keep memory states accessed learned attention policies. case memory represented document query contextual encodings. model closely related also applied question answering. pointer-style attention mechanism perform ﬁnal answer prediction proposed turn based earlier pointer networks however differently work perform attention step embed query single vector representation corresponding concatenation last state forward backward networks. knowledge embedding query single vector representation choice shared machine reading comprehension models. model repeated tight integration query attention document attention allows model explore dynamically parts query important predict answer focus parts document salient currently-attended query components. similar attempt attending different components query found model document processed query word. computationally intractable large documents since involves unrolling bidirectional recurrent neural network entire document multiple times. contrast model estimates query document encodings learn attend different parts encodings ﬁxed number steps. inference network responsible making sense current attention step respect gathered before. addition achieving state-ofthe-art performance technique also prove scalable alternative query attention models. finally iterative inference process shares similarities iterative hops memory networks model query representation updated iteratively although different components attended separately. moreover substitute simple linear update network. gating mechanism network made possible multiple steps attention propagate learning signal effectively back ﬁrst timestep. presented iterative neural attention model applied machine comprehension tasks. architecture deploys novel alternating attention mechanism tightly integrates successful ideas past works machine reading comprehension obtain state-of-the-art results three datasets. iterative alternating attention mechanism continually reﬁnes view query document aggregating information required answer query. multiple future research directions envisioned. plan dynamically select optimal number inference steps required example. moreover suspect shifting towards stochastic attention permit learn interesting search policies. finally believe model fully general applied straightforward tasks information retrieval.", "year": 2016}