{"title": "Stance Detection with Bidirectional Conditional Encoding", "tag": ["cs.CL", "cs.LG", "cs.NE", "68T50", "I.2.7"], "abstract": "Stance detection is the task of classifying the attitude expressed in a text towards a target such as Hillary Clinton to be \"positive\", negative\" or \"neutral\". Previous work has assumed that either the target is mentioned in the text or that training data for every target is given. This paper considers the more challenging version of this task, where targets are not always mentioned and no training data is available for the test targets. We experiment with conditional LSTM encoding, which builds a representation of the tweet that is dependent on the target, and demonstrate that it outperforms encoding the tweet and the target independently. Performance is improved further when the conditional model is augmented with bidirectional encoding. We evaluate our approach on the SemEval 2016 Task 6 Twitter Stance Detection corpus achieving performance second best only to a system trained on semi-automatically labelled tweets for the test target. When such weak supervision is added, our approach achieves state-of-the-art results.", "text": "stance detection task classifying attitude expressed text towards target hillary clinton positive negative neutral. previous work assumed either target mentioned text training data every target given. paper considers challenging version task targets always mentioned training data available test targets. experiment conditional lstm encoding builds representation tweet dependent target demonstrate outperforms encoding tweet target independently. performance improved further conditional model augmented bidirectional encoding. evaluate approach semeval task twitter stance detection corpus achieving performance second best system trained semi-automatically labelled tweets test target. weak supervision added approach achieves state–of-the-art results. goal stance detection classify attitude expressed text towards given target positive negative neutral. information useful variety tasks e.g. mendoza showed tweets stating actual facts afﬁrmed tweets related them tweets conveying false information predominantly questioned denied. paper focus novel stance detection task namely tweet stance detection towards previously unseen targets deﬁned semeval stance detection twitter task task rather difﬁcult ﬁrstly training data targets test secondly targets always mentioned tweet. example tweet realdonaldtrump honest voice expresses positive stance towards target donald trump. however stance annotated respect hillary clinton implicit target tweet expresses negative stance since supporting candidates party implies negative stance towards candidates parties. thus challenge twofold. first need learn model interprets tweet stance towards target might mentioned tweet itself. second need learn model without labelled training data target respect predicting stance. example above need learn model hillary clinton using training data targets. renders task challenging realistic scenario unlikely labelled training data target interest available. target initial state. show approach achieves better baseline independent lstm encoding tweet target. results improve bidirectional version model takes account context either side word encoded. context shared task would second best result except approach uses automatically labelled tweets test targets lastly bidirectional conditional encoding model trained data achieves state-of-the-art performance semeval stance detection twitter shared task consists subtasks task task task goal detect stance tweets towards targets given labelled training data test targets task focus paper goal detect stance respect unseen target donald trump labeled training/development data provided. systems need classify stance tweet positive negative neutral towards target. ofﬁcial metric reported shared task macroaveraged classes favor against. although none considered systems still need predict avoid precision errors classes. even though participants allowed manually label data test target donald trump allowed label data automatically. best-performing systems submitted task pkudblab litismind made this thus changing task weakly supervised seen target stance detection instead unseen target task. although goal paper present stance detection methods targets training data available show also used successfully weakly supervised framework outperform state-of-the-art secommon stance detection approach treat sentence-level classiﬁcation task similar however approach cannot capture stance tweet respect particular target unless training data available test targets. cases could learn tweet mentioning donald trump positive manner expresses negative stance towards hillary clinton. despite limitation baselines implemented support vector machine classiﬁer lstm network order assess whether successful incorporating target stance prediction. naive approach incorporate target stance prediction would generate features concatenating target words tweet. ignoring issue features would rather sparse classiﬁer could learn words target-dependent stance weights still assumes training data available target. order learn combine stance target tweet generalises unseen targets focus learning distributed representations ways combine them. following sections develop progressively proposed bidirectional conditional lstm encoding model starting independently encoding tweet target using lstms. here input vector time step denotes lstm memory output vector remaining weight matrices biases trainable parameters. concatenate output vector representations classify stance using softmax non-linear projection space three classes stance detection trainable weight matrices trainable class bias. model learns target-independent distributed representations tweets relies nonlinear projection layer incorporate target stance prediction. order learn target-dependent tweet representations conditional encoding previously applied task recognising textual entailment lstm encode target ﬁxed-length vector. then encode tweet another lstm whose state initialised representation target. finally last output vector tweet lstm predict stance target-tweet pair. formally sequence target word vectors sequence tweet word vectors start state bidirectional lstms shown learn improved representations sequences encoding sequence left right right left. therefore adapt conditional encoding model section bidirectional lstms represent target tweet using vectors them obtained reading target tweet left-to-right obtained reading right-to-left. achieve this initialise state bidirectional lstm reads tweet last state forward reversed encoding target bidirectional encoding allows model construct target-dependent representations tweet word considered leftright-hand side context taken account. order counter-balance relatively small amount training data available employ unsupervised pre-training initialising word embeddings used lstms appropriately trained wordvec model note embeddings used initialisation allow optimised training. detail train wordvec model corpus unlabelled tweets collected twitter keyword search november january plus tweets contained ofﬁcial semeval stance detection datasets unlabelled tweets collected contain targets considered shared task using keywords target namely hillary clinton trump climate femini aborti. note twitter allow regular expression search free text search disregarding possible word boundaries. combine large unlabelled corpus ofﬁcial training data train skip-gram wordvec model table data sizes available corpora. taska tr+dv part taska tr+dv tweets target hillary clinton only development. taskb autolab automatically labelled version taskb unlab. crawled unlab unlabelled tweet corpus collected finally ensure proposed neural network architectures contribute performance also word vectors wordvec develop bag-of-word-vectors baseline tweet target representations logistic regression classiﬁer regularization experiments performed semeval task corpus stance detection twitter report experiments different experimental setups unseen target setup main focus paper i.e. detecting stance tweets towards previously unseen targets. show conditional encoding reading tweets target-speciﬁc generalises unseen targets better baselines ignore target. next compare approach previous work weakly supervised framework show approach outperforms state-of-the-art semeval stance detection subtask corpus. table lists various corpora used experiments sizes. taska tr+dv ofﬁcial semeval twitter stance detection taska training development corpus contain instances targets legalization abortion atheism feminist movement climate change real concern hillary clinton. taska tr+dv part corpus contains hillary clinton tweets development purposes. taskb test taskb test corpus report results containing donald trump testing instances. taskb unlab unlabelled corpus containing donald trump tweets supplied task organisers taskb auto-lab* automatically labelled version small portion corpus weakly supervised stance detection experiments reported section finally crawled unlab* corpus collected unsupervised pre-training experiments ofﬁcial task evaluation script predictions post processed target contained tweet highest-scoring non-neutral stance chosen. motivated observation training data target-containing tweets express stance neutral. code used experiments available https//github.com/sheffieldnlp/stance-conditional. models used tweets labelled hillary clinton development tweets remaining four targets training. refer development setup models tuned using setup. labelled donald trump tweets used reporting ﬁnal results. based small grid search using development setup following settings lstm-based models chosen input layer size hidden layer size training epochs initial learning rate using adam optimisation dropout models trained using cross-entropy loss. relatively small hidden layer dropout help avoid overﬁtting. explained earlier challenge learn model without manually labelled training data test target using data task targets. order avoid using labelled data donald trump still development tune evaluate results unseen target setting show well conditional encoding suited learning targetdependent representations tweets crucially well representations generalise unseen targets. best performing method detable results unseen target stance detection development setup using bicond single separate embeddings matrices tweet target different initialisations velopment test setups bicond achieves respectively. notably concat learns independent encoding target tweets achieve improvements tweetonly learns representation tweets only. shows sufﬁcient take target account important learn target-dependent encodings tweets. models learn condition encoding tweets targets outperform baselines test set. worth noting bag-of-wordvectors baseline achieves results comparable tweetonly concat conditional encoding models tarcondtweet even though achieves signiﬁcantly lower performance test set. pre-trained word embeddings already useful stance detection. consistent ﬁndings works showing usefulness bag-of-word-vectors baseline related tasks recognising textual entailment bowman sentiment analysis eisner best result test setup bicond second highest reported result twitter stance detection corpus however ﬁrst third fourth best approaches achieved results automatically labelling donald trump training data. bicond unseen target setting outperforms third fourth best approaches large margin seen table results weakly supervised stance detection discussed section pre-training table shows effect unsupervised pre-training word embeddings wordvec skip-gram model furthermore results sharing representations tweets targets development set. ﬁrst results uniformly random embedding initialisation prefixed uses pre-trained skip-gram word embeddings whereas precont initialises word embeddings ones skipgram continues training during lstm training. results show that absence large labelled training dataset pretraining word embeddings helpful random initialisation embeddings. sing shows difference using shared separate embeddings matrices looking word embeddings. sing means word representations tweet target vocabularies shared whereas means different. using shared table stance detection test results weakly supervised setup trained automatically labelled pos+neg+neutral trump data reported ofﬁcial test set. target tweet tweet table shows results development bicond compared best unidirectional encoding model tweetcondtar baseline model concat split tweets contain target not. three models perform well target mentioned tweet less targets mentioned explicitly. case target mentioned tweet biconditional encoding outperforms unidirectional encoding unidirectional encoding outperforms concat. shows conditional encoding able learn useful dependencies tweets targets. previous section showed usefulness conditional encoding unseen target stance detection compared results internal baselines. goal experiments reported section compare participants semeval stance detection task consider unseen target setup submissions ranking ones task pkudblab litismind infufrgs considered different experimental setup. automatically annotated training data test target donald trump thus converting task weakly supervised seen target stance detection. pkudblab system uses deep convolutional neural network learns make -way predictions automatically labelled positive negative training data donald trump. neutral class predicted according rules applied test time. since best performing systems participated shared task consider weakly supervised setup compare proposed approach state-of-the-art using weakly supervised setup. note that even though pkudblab litismind inf-ufrgs also regular expressions label training data automatically resulting datasets available therefore develop automatic labelling method dataset publicly available code repository. weakly supervised test setup setup unlabelled donald trump corpus taskb unlab annotated automatically. purpose created small regular expressions based inspection taskb unlab corpus expressing positive negative stance towards target. regular expressions positive stance were tweet labelled positive positive expressions detected else negative negative expressions detected. neither detected tweet annotated neutral randomly chance. resulting corpus size stance shown table hyperparameters lstm-based models used unseen target setup described previous section. table lists results weakly supervised setting. including using unseen target setup compared state-of-the-art stance detection corpus. table lists baselines reported mohammad namely majority class baseline method using -gram bag-of-word character n-gram features extracted tweets used train -way classiﬁer. bag-of-word baselines achieve results comparable majority baseline shows difﬁcult task baselines extract features tweets svm-ngrams-comb tweetonly perform worse baselines also learn representations targets training conditional encoding models table stance detection test results compared state art. svm-ngrams-comb majority baseline reported mohammad pkudblab litismind zarrella marsh infufrgs dias becker automatically labelled stance detection data achieve state-of-the-art results. best result achieved bi-directional conditional encoding model shows models suitable unseen well seen target stance detection. stance detection previous work mostly considered target-speciﬁc stance prediction debates student essays task considered paper challenging stance detection debates because addition irregular language mohammad dataset offered withcontext e.g. conversational structure tweet metadata. targets also always mentioned tweets additional challenge distinguishes task analopen-domain ysis zhang related work rumour stance detection either requires training data rumour target rule-based thus potentially hard generalise. finally target-dependent stance detection task tackled paper different ferreira vlachos related concerned stance statement language towards another statement. conditional encoding conditional encoding applied related task recognising textual entailment using dataset half million training examples numerous different hypotheses. experiments show conditional encoding also successful relatively small training applied unseen testing target. moreover augment conditional encoding bidirectional encoding demonstrate added beneﬁt unsupervised pre-training word embeddings unlabelled domain data. paper showed conditional lstm encoding successful approach stance detection unseen targets. unseen target bidirectional conditional encoding approach achieves second best results reported date semeval twitter stance detection corpus. weakly supervised seen target scenario considered prior work approach achieves best results date semeval task dataset. show absence large labelled corpora unsupervised pretraining used learn target representations stance detection improves results semeval corpus. future work investigate challenge stance detection tweets contain explicit mentions target.", "year": 2016}