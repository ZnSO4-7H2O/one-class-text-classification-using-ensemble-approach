{"title": "A recurrent neural network without chaos", "tag": ["cs.NE", "cs.CL", "cs.LG"], "abstract": "We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "text": "introduce exceptionally simple gated recurrent neural network achieves performance comparable well-known gated architectures lstms grus word-level language modeling task. prove model simple predicable non-chaotic dynamics. stands stark contrast standard gated architectures whose underlying dynamical systems exhibit chaotic behavior. gated recurrent neural networks long short term memory network introduced hochreiter schmidhuber gated recurrent unit proposed prove highly effective machine learning tasks involve sequential data. propose exceptionally simple variant gated architectures. basic model takes form stands component vector words consider input sequence learned feature remains except time initialized corresponding response network impulse feature sequence relaxes toward zero. forget gate control rate relaxation. thus activates presented strong feature relaxes toward zero data present network strong feature. overall leads dynamically simple model activation patterns hidden states network clear cause predictable subsequent behavior. dynamics sort occur models. instead three popular recurrent neural network architectures namely vanilla lstm complex irregular unpredictable dynamics. even absence input data networks give rise chaotic dynamical systems. words presented null input data activation patterns hidden states necessarily follow predictable path. proposed network rather dull minimalist dynamics comparison; attractor zero state stands polar-opposite spectrum chaotic systems. perhaps surprisingly least light comparison proposed network performs well lstms grus word level language modeling task. therefore conclude ability form chaotic temporal dynamics sense describe section cannot explain success word-level language modeling tasks. next section review phenomenon chaos rnns synthetic examples trained models. also prove precise quantiﬁed description dynamical picture proposed network. particular show dynamical system induced proposed network never chaotic reason refer chaos-free network ﬁnal section provides series experiments demonstrate achieve results comparable lstm word-level language modeling task. together observations show architecture simple achieve performance comparable dynamically complex lstm. study rnns dynamical systems point-of-view brought fruitful insights generic features rnns shall pursue brief investigation lstm networks using formalism allows identify distinctions them. recall given mapping given initial time given initial state simple repeated iteration mapping deﬁnes discrete-time dynamical system. index represents current time point represents current state system. visited states ut+n deﬁnes forward trajectory forward orbit attractor dynamical system invariant attracts trajectories start sufﬁciently close attractors chaotic dynamical systems often fractal sets reason referred strange attractors. denotes input data point. example case gain insight underlying design architecture proves usefull consider trajectories behave inﬂuenced external input. lead consider dynamical system refer dynamical system induced recurrent neural network. timeinvariant system much tractable offers mean investigate inner working given architecture; separates inﬂuence input data produce essentially possible response model itself. studying trajectories inﬂuenced external data give indication ability given generate complex sophisticated trajectories own. shall shortly dynamical system induced excessively simple predictable trajectories converge zero state. words attractor zero state. sharp contrast dynamical systems induced lstm exhibit chaotic behaviors strange attractors. learned parameters describe data inﬂuence evolution hidden states time step. modeling perspective would occur scenario trained learned weak coupling speciﬁc data point hidden state time sense data inﬂuence small wjxt nearly vanish. hidden state transitions according refer bertschinger natschl¨ager study chaotic behavior simpliﬁed vanilla speciﬁc statistical model namely i.i.d. bernoulli process input data well speciﬁc statistical model namely i.i.d. gaussian weights recurrence matrix. subsection brieﬂy show lstm absence input data lead dynamical systems chaotic classical sense term figure depicts strange attractor dynamical system zero bias model parameters. weights randomly generated normal distribution standard deviation rounded nearest integer. figure obtained choosing initial state uniformly random plotting h-component iterates trajectories starting converge toward depicted attractor. resemblance attractor classical strange attractors h´enon attractor striking successive zooms branch lstm attractor figure reveal fractal nature. figure enlargement figure figure enlargement magenta figure structure repeats zoom practical consequence chaos long-term behavior forward orbits exhibit high degree sensitivity initial states figure provides example behavior dynamical system initial condition drawn uniformly random computed small amplitude perturbations adding small random number drawn uniformly component. iterated steps plotted h-component ﬁnal state trials figure collection ﬁnal states essentially ﬁlls entire attractor despite fact initial conditions highly localized around ﬁxed point. words time dynamical system small neighborhood around ﬁxed initial condition entire attractor. figure additionally illustrates sensitivity initial conditions points attractor itself. take initial condition attractor perturb nearby initial condition plot distance corresponding trajectories ﬁrst time steps. initial phase agreement trajectories strongly diverge. synthetic example illustrates potentially chaotic nature lstm architecture. show chaotic behavior occurs trained models well synthetically generated instances. take parameter values lstm hidden units trained figure small neighborhood around ﬁxed initial condition iterations mapped entire attractor. trajectories starting starting within another strongly diverge steps. figure -unit lstm trained penn treebank. absence input data system chaotic nearby trajectories diverge. presence input data system mostly driven external input. trajectories starting apart converge. penn treebank corpus without dropout data inputs zero corresponding induced dynamical system. trajectories starting nearby initial conditions computed figure plots ﬁrst component hidden state trajectories ﬁrst time steps. initial phase agreement forward trajectories strongly diverge. also trajectories exhibit typical aperiodic behavior characterizes chaotic systems. inputs vanish come actual word-level data behavior different. lstm longer autonomous system whose dynamics driven hidden states time dependent system whose dynamics mostly driven external inputs. figure shows ﬁrst component hidden states trajectories start initial conditions apart. sensitivity initial condition disappears instead trajectories converge toward steps. memory initial difference lost. overall experiments indicate trained lstm driven external inputs chaotic. presence input data lstm becomes forced system whose dynamics dominated external forcing. dynamical behavior dramatically different lstm. subsection start showing hidden states activate relax toward zero predictable fashion response input data. hand shows cannot produce non-trivial dynamics without inﬂuence data. other leads interpretable model; non-trivial activations hidden states clear cause emanating occurs component hidden state relaxes toward zero rate depends value component forget gate. overall leads following simple picture activates presented embedded input strong feature relaxes toward zero data present network strong feature. strength activation decay rate controlled component input forget gates. proof lemma elementary proof lemma using non-expansivity hyperbolic tangent i.e. tanh| triangle inequality obtain induced cfn. following lemma shows attractor dynamical system zero state. lemma starting initial state trajectory eventually converge zero state. limt→+∞ regardless initial state proof. deﬁnition clearly sequence deﬁned satisﬁes since sequence bounded sequence uθut exists ﬁnite using non-expansivity hyperbolic tangent obtain |ut| σ|ut−| conclude noting maximal values input gates involved layer network constant depending norms matrices sizes initial conditions previous levels. estimate shows lemma remains true multi-layer architectures. figure -layer -unit trained penn treebank. inputs zero i.e. time-point indicated dashed line. left plot slowest units ﬁrst layer. right plot slowest units second layer. second layer retains information much longer ﬁrst layer. inequality shows higher levels decay slowly remain non-trivial earlier levels decay quickly. illustrate behavior computationally simple experiment. take -layer -unit network trained penn treebank feed following input data ﬁrst inputs ﬁrst words test penn treebank; subsequent inputs zero. words layers select units decay slowest plot figure ﬁrst layer retains information time steps whereas second layer retains information steps. experiment conforms analysis indicates adding third fourth layer would potentially allow multi-layer architecture retain information even longer periods. section show despite simplicity network achieves performance comparable much complex lstm network word level language modeling task. datasets experiments namely penn treebank corpus text corpus consider one-layer two-layer cfns lstms experiments. train lstm networks similar fashion always compare models number parameters. compare performance without dropout show cases obtain similar results. also provide results published mikolov jozefowicz sukhbaatar sake comparison. thus model dropout hyperparameters. parameter controls amount dropout layers; parameter controls amount dropout inside gate. similar dropout strategy lstm sigmoid gates receive amount dropout. denotes approximate gradient loss respect weights estimated certain number presented examples. usual backpropagation time approximation estimating gradient unroll steps past neglect longer dependencies. experiments lstm networks unrolled steps take minibatches size case exact gradient update simply corresponds making step length direction steepest descent. search directions euclidean norm perform gradient clipping training. initialize weights except bias gates uniformly random initialize bias gates respectively beginning training initialize weights lstm exactly way; bias forget input gate initialized weights initialized uniformly initialization scheme favors information horizontal direction. importance careful initialization forget gate ﬁrst pointed gers emphasized jozefowicz finally initialize hidden states zero models. dataset construction. penn treebank corpus million words vocabulary size used code zaremba construct split dataset training validation test text corpus million characters vocabulary size used script mikolov construct split dataset training development experiments without dropout. tables provide comparison various recurrent network architectures without dropout evaluated penn treebank corpus text corpus. last rows table provide results lstm networks trained initialized manner described above. tried layer architectures reported best result. learning rate schedules used network described appendix. also report results published jozefowicz vanilla lstm network trained penn treebank million parameters finally report results published mikolov sukhbaatar various networks trained text. four networks lstm network mikolov number parameters lstm networks trained vanilla structurally constrained recurrent network end-to-end memory network units less parameters. nonetheless indicate performance table provide context. experiments dropout. table provides comparison various recurrent network architectures dropout evaluated penn treebank corpus. ﬁrst three rows report results published last four rows provide results lstm networks trained initialized strategy previously described. dropout rate chosen follows experiments parameters lstm; experiments parameters lstm. despite simple dynamics obtains results compare well lstm networks grus word-level language modeling. indicates might possible general build rnns perform well avoiding intricate uninterpretable potentially chaotic dynamics occur lstms grus. course remains seen dynamically simple rnns proposed perform well wide variety tasks potentially requiring longer term dependencies needed word level language modeling. experiments presented section indicate plausible path forward activations higher layers multi-layer decay slower rate activations lower layers. theory complexity long-term dependencies therefore captured using feed-forward approach rather relying intricate hard interpret dynamics lstm gru. overall simple model therefore potential mathematically wellunderstood. particular section reveals dynamics hidden states inherently interpretable lstm. mathematical analysis provides insights network presence absence input data obviously work needed complete picture emerge. hope investigation opens avenues inquiry understanding drive subsequent improvements. kyunghyun bart merri¨enboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. rafal jozefowicz wojciech zaremba ilya sutskever. empirical exploration recurrent network architectures. proceedings international conference machine learning strange attractor h´enon map. sake comparison provide figure depiction well-known strange attractor arising discrete-time dynamical system. generate pictures reproducing numerical experiments h´enon discrete dynamical system considered dimensional parameters obtain figure choosing initial state plotting iterates trajectories starting close origin time converge toward depicted attractor. successive zooms branch attractor reveal fractal nature. structure repeats fashion remarkably similar -unit lstm section zero bias model parameters. also successive zooms branch attractor reveal fractal nature. lstm forward trajectories dynamical system exhibit high degree sensitivity initial states. network sizes learning rate schedules used experiments. penn treebank experiment without dropout network hidden layers units total million parameters. lstm hidden layer units total million parameters well. also tried two-layer lstm million parameters result worse report table. text experiments lstm hidden layers hidden units total million parameters. also tried one-layer lstm million parameters result worse hidden layers units each total million parameters well. penn treebank experiment dropout parameters hidden layers units lstm parameters trained hidden layers units each. also tried one-layer lstm parameters similar slightly worse results two-layer architecture. network learning rate divided time validation perplexity decrease least initial learning rate chosen lstm.", "year": 2016}