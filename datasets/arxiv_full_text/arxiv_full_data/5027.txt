{"title": "Piecewise Linear Neural Network verification: A comparative study", "tag": ["cs.AI", "cs.LG"], "abstract": "The success of Deep Learning and its potential use in many important safety- critical applications has motivated research on formal verification of Neural Network (NN) models. Despite the reputation of learned NN models to behave as black boxes and the theoretical hardness of proving their properties, researchers have been successful in verifying some classes of models by exploiting their piecewise linear structure. Unfortunately, most of these approaches test their algorithms without comparison with other approaches. As a result, the pros and cons of the different algorithms are not well understood. Motivated by the need to accelerate progress in this very important area, we investigate the trade-offs of a number of different approaches based on Mixed Integer Programming, Satisfiability Modulo Theory, as well as a novel method based on the Branch-and-Bound framework. We also propose a new data set of benchmarks, in addition to a collection of pre- viously released testcases that can be used to compare existing methods. Our analysis not only allows a comparison to be made between different strategies, the comparison of results from different solvers also revealed implementation bugs in published methods. We expect that the availability of our benchmark and the analysis of the different approaches will allow researchers to develop and evaluate promising approaches for making progress on this important topic.", "text": "success deep learning potential many important safetycritical applications motivated research formal veriﬁcation neural network models. despite reputation learned models behave black boxes theoretical hardness proving properties researchers successful verifying classes models exploiting piecewise linear structure. unfortunately approaches test algorithms without comparison approaches. result pros cons different algorithms well understood. motivated need accelerate progress important area investigate trade-offs number different approaches based mixed integer programming satisﬁability modulo theory well novel method based branch-and-bound framework. also propose data benchmarks addition collection previously released testcases used compare existing methods. analysis allows comparison made different strategies comparison results different solvers also revealed implementation bugs published methods. expect availability benchmark analysis different approaches allow researchers develop evaluate promising approaches making progress important topic. despite success wide variety applications deep neural networks seen limited adoption safety-critical settings. main explanation lies reputation black-boxes whose behaviour predicted. current approaches evaluate quality trained models mostly rely testing using held-out data sets. however edsger dijkstra said testing shows presence absence bugs. deep learning models deployed applications autonomous driving cars need able enforce verify safety-critical behaviours. particularly illustrative instance limit understanding behaviour learned models lies existence adversarial examples small perturbations imperceptible human capable signiﬁcantly modifying predictions generated network despite performing well test set. several methods proposed make networks robust perturbed inputs. however clear methods successful reducing number adversarial examples capable reducing number adversarial examples currently known methods generate. know measure size region around training samples guaranteed contain adversarial examples. method verify simple hidden layer neural networks recently researchers able work non-trivial models taking advantage structure relu-based networks. even then works scalable large networks encountered real world problems. major roadblock area lack analysis success failure modes proposed approaches. remedy problem gather data test cases based existing literature parametrically explore space possible architectures. evaluate different published approaches implementing public version available generate ﬁrst experimental comparison published tools. addition introduce general formalism problem showing possible directions improvement well method showing signiﬁcantly better performance practical scenarios. additionally comparison also revealed bugs publicly available veriﬁcation software made evident contradictions results various methods. toy-example problem given figure domain want prove output hidden-layer network always satisfy property running example explain methods used comparison experiments. paper going focus piecewise-linear neural networks networks decompose polyhedra restriction linear function prevents including networks activation functions sigmoid tanh pl-nns allow linear transformations fullyconnected convolutional layers pooling units maxpooling activation functions relus various extensions leaky relu prelu words pl-nns represent majority networks used practice. note layers batchnormalization also preserve piecewise linearity test-time. properties going consider boolean formulas linear inequalities. although formulas deﬁne property function ouptut loss generality property involving variables could expressed function output different network including additional variables output. problem pl-nn veriﬁcation shown np-complete using reduction -sat unlikely polynomial time algorithm exist. therefore experimental comparison remains approach possible evaluate relative advantages different methods propose zakrzewski hein andriushchenko propose methods based second derivatives functions expressed networks guarantee output network doesn’t change much around points. however requires additional assumption layers networks twice differentiable pl-nn don’t satisfy. spectrum narodytska cheng solvers propose veriﬁcation methods specialised case binarized neural networks methods however don’t translate pl-nns. addition exact formal methods approaches proposed don’t provide complete veriﬁcation. pulina tacchella proposed method veriﬁcation networks sigmoid activation function. approximation non-linearities means method always return decision certain problems. bastani studied pl-nn context table evolution reluplex algorithm. cells corresponds value violating linear constraints orange cells corresponds value violating relu constraints. resolution violation linear constraints prioritised. obtaining robustness guarantees adversarial examples. scalability reasons added additional assumptions limiting domain considered points sharing activation pattern reference point. effectively circumvents non-linearities network. xiang huang rely discretisation perform layer layer analysis obtain guarantees output huang require user specify family possible changes xiang over-approximate output space layer leading existence undecidable properties making inadequate comparison. methods involve comparison leverage piecewise-linear structure pl-nn make problem tractable. shall network figure throughout illustrative example. methods compare follow general principle given property prove attempt discover counterexample would make property false. accomplished deﬁning variables corresponding inputs hidden units output network well constraints counterexample would satisfy. network figure variables would aout bout constraints would here input value hidden unit aout value relu. point satisfying constraints would counterexample property would imply possible drive output less. however problem unsatisﬁable counterexample exist implies property true. want emphasize requirements beyond saying counterexample could found necessary prove counter-examples exists. difﬁculty stems non-linear constraints explain method tackles problem. possible eliminate non-linearities encode help binary variables transform pl-nn veriﬁcation problem mixed integer program lomuscio maganti cheng advocate big-m encoding achieve this. example non-linearities equation replaced approach advantages. ﬁnal problem solve ends imposing integrality constraints inputs comes additional cost. prove useful input features network known necessarily integers methods imposing integer constraints would possible either proof would attempted relaxed version networks would done possible combinations integer inputs. solving np-hard performance methods going dependent quality solver used encoding. cheng proposes several methods obtain good encoding picking appropriate values order quality linear relaxation problem high possible. however problem still resolved general purpose solver question remains open whether solver knowing speciﬁc structure problem efﬁcient challenging benchmarks. katz presents procedure named reluplex verify properties neural network containing linear functions relu activation unit functioning solver using splittingon-demand framework principle reluplex always maintain assignment variables even constraints violated. starting initial assignment attempts violated constraints step. prioritises ﬁxing linear constraints illustrative example) using simplex algorithm even leads violated relu constraints seen step process shown table solution problem containing linear constraints exists shows counterexample search unsatisﬁable. otherwise linear constraints ﬁxed reluplex attempts violated relu time step table potentially leading newly violated linear constraints. process guaranteed converge guarantee progress non-linearities getting ﬁxed often split cases. generates sub-problems involving additional linear constraint instead linear one. ﬁrst solves problem aout second aout ain. worst setting problem split completely possible combinations activation patterns point sub-problems simple lps. ehlers also proposed approach based splitting problems possible phase activation function. unlike reluplex proposed tool named planet operates attempting assignment phase non-linearities. reusing notation introduced section assigns value true false variable verifying step feasibility partial assignment. opposed reluplex advantage easily extended networks containing maxpooling units. order detect incoherent assignments faster author proposes global linear approximation neural network. addition existing linear constraints linear constraints approximated sets linear constraints representing convex hull nonlinearities. relus replaced equations aout respectively output input relu pre-computed upper lower bounds relus input. feasible domain illustrated figure maxpooling units replaced constraints efﬁciently queried detect infeasibility automatically deducing implied assignments variables. values lower bounds upper bounds necessary deﬁne constraints built iteratively optimizing corresponding variable based constraints imposed previous layers. additional heuristics make infeasibility detection implied phase inference faster described original paper reluplex planet rely splitting mechanism relus guarantee progress fundamentally different reluplex drives search satisﬁability using simplex algorithm splits relu lazily unlock cases planet drives whole search eagerly making splits using solver making deductions based those. reluplex always maintains assignment variables even though doesn’t respect constraints end; planet maintains assignments phase nonlinearities. approaches never compared common benchmarks hard identify promising speciﬁc cases method outperforms other even though rely similar principles. veriﬁcation optimization present different approaching neural network veriﬁcation problem. whole satisﬁability problem transformed optimization problem decision obtained checking sign minimum. show boolean formula linear inequalities encoded additional layers network. assume property simple inequality case sufﬁcient network ﬁnal fully connected layer output weight bias global minimum network positive indicates original network consequence property true. output hand global minimum negative provides counter-example property. clauses speciﬁed using encoded using maxpooling unit; prop− clauses speciﬁed using erty equivalent encoded similarly property mini result formulate boolean formula linear inequalities output network sequence additional layers veriﬁcation problem would reduced global minimization problem. aside speciﬁc class remains hard problem. advantage formalism allowing prove complex properties containing several clauses single procedure rather decompose desired property separate queries done previous work finding exact global minimum necessary veriﬁcation advantage generating value. value positive correspond margin property satisﬁed. estimating robustness adversarial examples existing methods choose perform binary search maximum radius guaranteeing absence adversarial examples. optimization process would appropriate formalism here. branch bound optimization optimization algorithms stochastic gradient descent usual workhorses deep learning appropriate minimization problem. despite capable converging good local minima guaranalgorithm describes generic form branch-and-bound method. input domain repeatedly split sub-domains compute lower upper bounds minimum output best upper-bound found serve candidate global minimum. domain whose lower bound greater current global upper bound pruned away necessarily won’t contain global minimum iteratively splitting domains able compute tighter lower bounds. keep track global lower bound minimum taking minimum lower bounds sub-domains global upper bound global lower bound differ less small scalar consider converged. description veriﬁcation problem optimization pseudo-code algorithm generic would apply veriﬁcation problems beyond speciﬁc case pl-nn. obtain practical algorithm necessary specify elementary functions split takes argument domain returns partition domains domain choosing split function deﬁne shape domains operating potentially making computation bounds harder easier. compute {lower upper} bound compute bound minimum feasible domain domain. want lower bound high possible upper bound possible several approaches compute bounds employ keep tightest. experiments result minimising variable corresponding output network subject constraints linear approximation introduced ehlers lower bound simple random sampling practice necessary algorithm convergence veriﬁcation. negative global upper bound found corresponding input valid counter-example. similarly soon global lower bound goes zero know property veriﬁed. method implements different strategy worst-case analysis always indicate exponential runtimes approaches compared experimentally. attempt perform veriﬁcation three data sets veriﬁcation properties report comparison results. dimensions problems given table data sets compare four methods using protocol. done timeout hours maximum allowed memory usage single core machine ram. code data necessary replicate analysis available. success rate corresponds proportion properties solver returns correct answer timing terminated using much memory. compare performances method present results separately cases properties true false. means satisﬁability problem counter-example satisﬁable implying property false. case runtime corresponds time took exhibiting counter-example property. hand unsat corresponds time took prove problem infeasible counterexamples property could exist. methods returning exhibit counterexample disagreement solvers easily resolved evaluating network counterexample checking property output. criterion establish ground truth property except table know result unsat construction. reported bugs original authors method every time disagreement detected. case timeout runtime method counted maximum allowed time even though actual runtime would worse. result average runtime methods success rate would worse practice reported here. ﬁxing planet solver memory error encountered. give insights relative performance solver count number wins corresponds many times solver fastest solve property. relative difference runtime solvers less don’t count win. figure quality linear approximation depending size domain. compute value lower bound given domain centred around global minimum repeatedly shrink size domain. rebuilding completely linear approximation step allows create tighter lower-bounds thanks better opposed using constraints changing bounds input variables. effect even signiﬁcant deeper networks. note tool doesn’t support maxpooling units automatically convert maxpooling layers series linear layers relu activations. decompose element-wise maximum series pairwise maximum planet based version released author tool implemented using glpk solve linear programs modiﬁed version minisat drive search. wrote software convert directions input format reluplex planet. discovered memory issues original implementation reported author used ﬁxed version experiments. consists encoding satisﬁability problem mixed integer program using encoding non-linearity. exact encoding maxpooling relu found appendix method similar implemented lack availability open-sourced methods reimplemented approach python using gurobi solver. choose value relu made linear approximation planet. leads better values dataﬂow analysis discussed cheng even compared proposed heuristic selection several advantages encompasses information layers network rather layers; solves simple rather mips. based method described section pick strategy chooses domain currently smallest lower bound. split domain half across longest edge generate smaller domains split method. generate upper bounds minimum randomly sampling points considered domain minimise linear approximation network proposed lower bound. implementation python uses gurobi solve lps. note opposed approach taken ehlers building single approximation network rebuild approximation sub-domain. motivated observation shown figure demonstrate signiﬁcant improvements brings especially deep networks. airborne collision avoidance system data released katz neural network based advisory system recommending horizontal manoeuvres aircraft order table results acas data set. note properties solved methods therefore present sat/unsat breakdowns. reluplex boast comparable success rate ﬁnishes signiﬁcantly faster especially counterexamples. table results collisiondetection data set. solvers ﬁnished test cases reluplex erroneously classiﬁed three properties true. properties difference fastest methods inferior runtime didn’t count win. avoid collisions based sensor measurements. possible manoeuvres assigned score neural network action minimum score chosen. case complex property prove noaction minimal score counter-example search implemented series four satisﬁability problems does exist point score {weakleft weakright strongleft strongright} less score noaction? approach discussed section would able merge satisﬁability problems single one. experiments original strategy authors. results acas data shown table proposed method using branch bound performs best along criteria. compared second best method reluplex order magnitude faster exhibiting counter-examples twice fast proving correctness true properties. data largest number hidden units planet doesn’t manage exhibit counterexample property. hypothesise high number non-linearities strategy eager assignments phase planet disadvantaged opposed lazy approach reluplex bab. methods perform better unsat problem ones. postulate relatively small dimensionality input makes random testing capable easily getting good coverage discovering counterexamples quickly. collisiondetection data released authors planet network attempts predict whether vehicles parameterized trajectories going collide. total properties extracted problems arising binary search identify size region around training examples prediction network doesn’t change. planet best performing method benchmarks accompanied release tool. table shows fastest method properties. conversely becomes worst performing method especially true properties. however important note solvers ﬁnished signiﬁcantly timeout limit indicating data isn’t extremely challenging. data reluplex classiﬁed three false properties unsatisﬁable. evaluated counterexamples returned solvers using reluplex’s code conﬁrmed valid counterexamples. reported issue original authors reluplex identiﬁed numerical instabilities reason discrepancy. table results twinstream data set. reluplex returns erroneous results several properties numerical instabilities. planet performs best branch-and-bound struggle instances. parameters. networks contain separate streams streams architecture weights inputs. ﬁnal layer network computes difference outputs streams positive bias term refer margin. result output always equal value ﬁnal bias. networks attempt prove true property output network positive. generate networks random weights using glorot initialisation vary depth number hidden units stream number inputs value margin. note opposed data sets weights aren’t result optimization process therefore representative real use-cases. networks margin networks margin reluplex returns result although properties true construction. evaluating returned counterexamples using reluplex conﬁrms valid counterexamples caused numerical issues. also reported issues authors reluplex. surprisingly generic solver planet solver perform best data set. possible explanation heavy structure present problem methods explicitly representing decision phase non-linearity inherently advantage. generate plot showing evolution runtimes architectural parameters twinstream networks. curve represented corresponds ﬁxed conﬁguration parameters. allows assess impact factor performance solvers. planet relatively insensitive changes number inputs network branch-and-bound incurs signiﬁcantly higher run-times number inputs network rises. explained fact planet branch hidden units whose number kept constant here branches dimensions input. hand studying impact network depth planet much sensitive higher number layers unlike surprisingly performs better deeper networks. analysis figure doesn’t reveal surprising insights higher number non-linearities layer corresponds complex problem directly translates longer runtime solvers. figure reveals expected runtime branch-and-bound sensible amount property true compared solvers reason directly phases non-linearities. natural case properties higher margin corresponds simply cases possible stop counter-example search procedure faster. improvement formal veriﬁcation neural networks represents important challenge tackled learned models used safety critical applications. lack shared benchmarks researchers makes hard evaluate progress estimate promising research directions. gathered test cases existing literature proposed ones evaluated performance published methods allowed surface issues published tools offer informed view status ﬁeld. addition proposed conceptually simple method offers competitive performance realistic data available. method sufﬁciently general easily improvable better lower bound directly translate faster veriﬁcation. figure impact number inputs network performance solvers. branchand-bound performs signiﬁcantly worse input domain larger number dimension planet degrades gracefully. figure impact network depth performance solvers. deeper networks slower prove property. surprisingly branch-and-bound seems perform better deeper networks. figure impact number hidden units layer performance solvers. hidden units correspond higher number non-linearities consequently harder problem solve. methods worse similar fashion hidden units introduced. figure impact margin property true performance solvers. branch-and-bound subject improvement difﬁculty problem relaxed methods based explictly assigning values phase non-linearities less affected. chih-hong cheng frederik diehl yassine hamza gereon hinz georg n¨uhrenberg markus rickert harald ruess michael troung-le. neural networks safety-critical applications-challenges experiments perspectives. arxiv. maxpooling units either make choice decompose maxpooling series relu linear encoding. another solution encode maxpooling directly. constraint", "year": 2017}