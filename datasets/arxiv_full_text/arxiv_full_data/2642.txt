{"title": "Efficient Online Learning for Optimizing Value of Information: Theory  and Application to Interactive Troubleshooting", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We consider the optimal value of information (VoI) problem, where the goal is to sequentially select a set of tests with a minimal cost, so that one can efficiently make the best decision based on the observed outcomes. Existing algorithms are either heuristics with no guarantees, or scale poorly (with exponential run time in terms of the number of available tests). Moreover, these methods assume a known distribution over the test outcomes, which is often not the case in practice. We propose an efficient sampling-based online learning framework to address the above issues. First, assuming the distribution over hypotheses is known, we propose a dynamic hypothesis enumeration strategy, which allows efficient information gathering with strong theoretical guarantees. We show that with sufficient amount of samples, one can identify a near-optimal decision with high probability. Second, when the parameters of the hypotheses distribution are unknown, we propose an algorithm which learns the parameters progressively via posterior sampling in an online fashion. We further establish a rigorous bound on the expected regret. We demonstrate the effectiveness of our approach on a real-world interactive troubleshooting application and show that one can efficiently make high-quality decisions with low cost.", "text": "consider optimal value information problem goal sequentially select tests minimal cost efﬁciently make best decision based observed outcomes. existing algorithms either heuristics guarantees scale poorly moreover methods assume known distribution test outcomes often case practice. propose sampling-based online learning framework address issues. first assuming distribution hypotheses known propose dynamic hypothesis enumeration strategy allows efﬁcient information gathering strong theoretical guarantees. show sufﬁcient amount samples identify near-optimal decision high probability. second parameters hypotheses distribution unknown propose algorithm learns parameters progressively posterior sampling online fashion. establish rigorous bound expected regret. demonstrate effectiveness approach real-world interactive troubleshooting application show efﬁciently make high-quality decisions cost. optimal information gathering decision making central challenge artiﬁcial intelligence. classical approach decision making decision-theoretic value information needs optimal testing policy achieves maximal value information. informally goal optimal policy reduce uncertainty hidden state system question efﬁciently probing sequence tests observations make best decision minimal cost. example consider automated troubleshooting problem customer reaches contact center wants resolve problem cell phone. provide solution virtual agent customer questions regarding symptoms cellphone diagnose root-cause. here decision corresponds solution hidden state corresponds root-cause want learn about tests correspond questions symptoms hypothesis space consists full realizations tests. want develop virtual agent identify best solution customer minimal questions asked. optimization voi. optimal problem studied various contexts including active learning bayesian experimental design policy making probabilistic planning etc. refer interested readers detailed review related work. deriving optimal policies np-hard general however certain conditions approximation results known. particular test outcomes deterministic functions hidden state simple greedy algorithm namely generalized binary search guaranteed provide near-optimal approximation optimal policy terms cost results recently generalized decision making information gathering policies longer resolve uncertainty hidden state enough make optimal decision. problem known decision region determination problem relates problem learning optimal policy maximal utility problem aims resolving uncertainty amongst decisions. following line work javdani chen propose principled framework optimizing using surrogate objective functions. theoretical guarantees algorithms rely fact objective functions exhibit adaptive submodularity natural diminishing returns property generalizes classical notion submodularity adaptive policies. follows simple greedy policy provide near-optimal approximation optimal solution. limitations existing methods. many dataintensive decision making applications however evaluating surrogate objectives expensive. first assume underlying distribution hypotheses given. iteration needs perform greedy search tests myopically maximizes expected gain corresponding objective whose runtime depends linearly size support probability distribution hypotheses. however size hypothesis space growing exponentially number tests often computationally prohibitive work original distribution. second practice underlying distribution hypotheses often unknown requires estimated time. within online framework solving troubleshooting problem assume virtual agent possess perfect knowledge root-causes correspond symptoms. thus provide better diagnosis long virtual agent must engage customers answer explorative questions session spamming customer excessive queries. contribution. paper make contributions fronts. first assuming prior hypotheses given propose efﬁcient hypothesis enumeration scheme makes class adaptive submodular surrogates practically feasible still preserving strong theoretical guarantees. particular divide-and-conquer strategy generate probable hypotheses conditioning hidden state novel efﬁcient priority search procedure merge states compute marginal likelihood. comparison prior sampling scheme utilizes speciﬁc structure underlying model thereby offers increased efﬁciency better approximation guarantees. second contribution integrate hypothesis enumeration strategy optimizing online sequential information gathering framework conditional probabilities test outcomes given hidden states unknown learned data online fashion. instance troubleshooting conditional probabilities symptoms given root-cause might unknown. purpose employ posterior sampling approach decision-making session ﬁrst sample parameters conditional probability distributions according probabilities optimal hypothesis enumeration algorithm generate hypotheses session. establish rigorous bound expected regret algorithm. online learning strategy interpreted thompson sampling across multiple sessions interaction. several recent empirical simulations theoretical studies demonstrated effectiveness thompson sampling different settings. however different framework classical usage thompson sampling suggests choose action according probability optimal i.e. action maximizes reward expectation; whereas model action interpreted tests performed decision-making session. finally demonstrate online learning framework real-world troubleshooting platform. empirical results show efﬁciently submodular surrogate-based approaches dynamic hypothesis enumeration strategy achieving much better performance comparing existing commonlyused heuristics experiments online setting imply framework encourages efﬁcient exploration which combined hypothesis enumeration algorithm leads efﬁcient online learning optimal voi. formulating problem. random variable represents hidden state upon want make decision. reward making decision hidden state modeled utility function given tests binary outcomes; performing test reveals information incurs cost given cost function denote ground hypotheses; hypothesis corresponds possible realization outcomes tests words outcome test modeled deterministic function i.e. random variable corresponding outcome test random variable denote observed outcome test crucially assume xi’s conditionally independent given hidden state i.e. ofﬂine setting assume parameters distributions given. denote performed tests outcome vector deﬁne expected utility making decision observing value speciﬁc observations deﬁned maxd∈d i.e. maximum expected utility achievable acting upon observations decision deﬁne associated decision region voi} i.e. hypotheses optimal decision. formally policy partial mapping test-observation pairs tests. expected cost policy costav worst-case cost costwc maxh represents tests seen given hypothesis happens. goal problem optimal policy minimal cost upon termination exists least decision region contains hypotheses consistent observations acquired policy. formally seek hypotheses consistent equivalence class edge cutting algorithm. simplicity consider special case problem decision regions disjoint. case problem reduces equivalence class determination problem solved nearoptimally equivalence class edge cutting algorithm illustrated fig. employs edge-cutting strategy based weighted graph note algorithmic framework also directly applied general setting overlapping decision regions. vertices represent hypotheses edges link hypotheses want distinguish between. formally d=d{{h consists pairs root-causes corresponding different target decisions deﬁne weight funcselects test maximizes total expected weight edges unit cost. performance guarantee relies fact objective function adaptive submodular strongly adaptive monotone particular pmin minh∈h denote minimal prior probability hypothesis; expected cost bounded factor minimal expected cost. note compute exact objective enumerate hypotheses non-zero prior probability. total number hypotheses exponential respect number tests; hence direct application scale several hundreds tests more. facilitate efﬁcient optimization must consider effective sampling schemes explore hypothesis space. maintain conﬁdent hypotheses efﬁciently approximate objective. concretely consider following problem optimal hypothesis enumeration problem. assume prior hidden state known prior distribution hypotheses fully speciﬁed conditional probability distribution table test hidden state hypotheses sampled cpt. clearly ideal rich enough enclose promising candidates true underlying hypotheses compact enough excludes hypotheses extremely rare ensures feasibility algorithm. deﬁne coverage total probafigure illustration algorithm fig. shows illustrative example troubleshooting problem. example assume three possible troubleshooting decisions make corresponds many hypotheses goal identify best solution given problem quickly possible. initialize drawing edges pairs hypotheses mapped different decisions perform test observe result edges incident hypotheses cut/ removed. existing approaches generating hypotheses monte-carlo sampling often require large sample size reach certain coverage total probability mass. illustrate this consider simple multinomial distribution describes probability distribution four mutually exclusive hypotheses probabilities monte-carlo hypothesis generator simply samples hypotheses according probabilities require observe subset hypotheses cover least total probability mass conﬁdence level least need least sample average size cover rare observations. dynamic hypothesis enumeration. problem monte-carlo approach ignorant structure problem. instead method aims providing likely conﬁgurations covering pre-speciﬁed fraction total probability mass efﬁcient adaptive way. nutshell adaptively maintain pool hypotheses constitute small sample sufﬁcient coverage. particular hypothesis enumeration scheme consists modules algorithm locally enumerates likely hypotheses hidden state cover taking union hidden states least fraction total probability mass hypotheses; algorithm illustrated fig. provides global mechanism that observing test outcome adaptively ﬁlters inconsistent hypotheses re-generates hypotheses calling algorithm basic module hypothesis enumeration framework local hypothesis generator enumerates likely hypotheses given hidden state. incrementally builds directed acyclic graph hypotheses starting likely conﬁguration. step leaf nodes represent current candidate frontier i.e. hypotheses dominate candidate hypotheses terms likelihood. used generate remaining hypotheses children generation next likely hypothesis candimechanism date frontier identiﬁed children call algorithm generate l∗y; foreach determine next test {t}; observe {xt}; update filter inconsistent hypotheses remove test list available tests; output vectors decision highest uncertainty; hence ﬂipping sign test minimal effect overall likelihood. generator proceeds enumerate likely hypotheses corresponding given hidden state line children hypotheses generated follows. ﬁrst child last create switching last instance child hypothesis log-probability obtained λy+qn−pn. second child ﬁrst need locate right-most pair create switching instance child hypothesis associated log-probability computed index right-most pair. output algorithm produces ranked list likely hypotheses given log-probabilities log) exp) addition also produces residual frontier used seed list next iteration. generating likely hypotheses hidden state merge global compute marginal likelihoods. dynamically re-generate hypotheses observations made. step necessary order constantly guarantee sample covers least total remaining mass observations become available. shown algorithm global iterative ﬁltering re-sampling module consists global loop initializing ranked lists prior distribution hidden states iteratively performs following sequences operations first hidden state calls algorithm generate enough hypotheses covers least current mass i.e. might initially empty previous call algorithm case generator produces additional hypotheses starting frontier desired coverage achieved. step necessary inconsistent i.e. hidden states whose posterior distribution given zero. merge hypotheses associated hidservations state sample covers least fraction total mass consistent procedure followed performing identify next test performed assume enumerate hypotheses beginning experiment i.e. regenerate hypotheses observing outcome test. underlying true hypothesis included sampled construction algorithm guaranteed make optimal decision. otherwise small probability fails output optimal decision. theorem states trade-off size expected cost algorithm theorem suppose generated hypotheses coverage deﬁne ˜pmin minh∈ policy induced algorithm optimal policy original distribution cost performing tests. then holds costav costav moreover stop running cuts edges probability least costwc. hypothesis distribution theorem establishes bound cost greedy algorithm samples cost optimal algorithm total population quality bound depends well structure problem running greedy policy larger samples leads lower failure rate although ˜pmin might signiﬁcantly smaller small further adaptive re-sampling constantly maintain coverage posterior distribution similar reasoning show greedy policy adaptively-resampled posteriors yields lower failure rate greedy policy samples hypotheses start session. online setting exact decision making model unknown need learn model feedback. employ efﬁcient posterior sampling strategy described follows. suppose initially access prior model parameters example troubleshooting application assume beta prior parameters beginning session sample β)). particular pair simply generate parameter depend historical data. algorithm sequentially pick tests session decision making session over observe together test-outcome pairs. update distribution enter next conversation. algorithm details. suppose drawn session denote optimal policy w.r.t. distribution policy induced algorithm pmin minh∈ policies satisfy condition problem achieve hence theorem implies probability principle want design adaptive policy competitive hindsightsession optimal policy knows true distribution. deﬁne expected regret policy session respect true distribution true denotes expected utility policy w.r.t. distribution learned denotes optimal policy suppose given ﬁxed budget performing tests session. deﬁne expected regret incurred running policies sessions regret establish following bound expected regret running algorithm theorem mint ˜pmin notes minimal probability hypothesis sampled distributions denotes worst-case cost optimal algorithm sessions. then algorithm achieves expected regret prove theorem view optimal problem optimizing partially observable markov decision process repeated episodes ﬁxed horizon parameter regret bound corresponds number belief states pomdp. established connection interpret online learning problem reinforcement learning problem posterior sampling similar osband notice conservative bound doubly-exponential horizon. however practice number reachable belief states limited structure problem hence could smaller. case theorem implies expected regret algorithm limit bounded η/τ. experimental results figure online troubleshooting customers reach call center diagnosis devices. troubleshooting virtual agent resolves issues sequentially asking customer series diagnosis questions. data collected contact center agents knowledge workers solve complex troubleshooting problems mobile devices training data involve around root-causes tests binary outcomes. training data derived distribution prior distribution root-causes assume uniform. cause underlying hypothesis number decisions number root-causes plus extra decision give-up. intuitively root-causes result symptoms virtual agent cannot decide true root-cause therefore forward case human agent corresponding give-up decision. practice introducing giveup decision guarantees overlaps between decision regions. utility function corresponds cost mis-prediction speciﬁed business domain expert give-up; give-up decision optimal posterior distribution given test outcomes peak value higher information gathering full knowledge experiments coverage parameter addition also algorithm several different subroutines myopic value information information gain uncertainty sampling myopic criterion greedily picks test leading highest expected utility; criterion picks tests greedily maximizing reduction entropy decision regions fig. shows expected entropy increasing test budget. clearly myopic performs comparably worse others. fig. fig. report average label complexity average utility making decisions varying maximal number samples allowed root-cause. generate hypotheses extensively algorithms require tests order make decision; hand quality decisions also increases samples. behavior reasonable since samples excludes large amount good candidates turn leads poor utility. moreover improvement average query complexity using shows clear advantage using submodular surrogates kind sequential problem construction less myopic. online sequential information gathering evaluate online learning framework simulated test scenarios. initial distribution parameters beta priors αij/βij roughly proportional ratio number positive negative symptoms pairs) training set. inject noise estimates ﬂipping values small probability. experiments assume uniform prior distribution root-causes. fig. demonstrates behavior three algorithms. experiments maximal budget session/test scenario keep running policy till identiﬁes decision region exceeds budget. session report average label complexity sessions) average utility sessions). update estimators. observe cost much higher encourage exploration much online methods. static based initial sample without updates. full info versions algorithms ground truth used online versions algorithm update cpt’s. axis shows accumulated means different evaluation criteria. notable observations order first integrating levels sampling strategy average utility algorithms approach optimal utility time. second variants consistently outperform alternatives terms query complexity consistent results ofﬂine setting. bayesian experimental design troubleshooting. substantial research diagnosis troubleshooting using graphical models last decades particular using bayesian networks. research question perform efﬁcient inference. example nielsen focuses efﬁcient belief updating bayesian networks particular context troubleshooting; ricks mengshoel consider inference problem complex bayesian network structures multi-fault diagnosis. contrast focus orthogonal direction sense assume efﬁciently compute posterior distribution minimize cost troubleshooting policy. optimal voi. many greedy heuristics proposed optimizing voi; instance breese heckerman proposes myopic policy single fault troubleshooting problem chooses action/test minimizes expected cost ﬁxing device question. objective function viewed particular form myopic voi. unfortunately unlike greedy algorithms based submodular surrogates algorithms perform arbitrarily badly sampling-based bayesian inference. authors suggest rejection sampling approach estimate original distribution hypotheses problem. another work similar work spirit suggests generate hypotheses descending order prior likelihood approximate inference hybrid bayesian networks. comparison sampling scheme based speciﬁc structure underlying model potentially offers increased efﬁciency better approximation guarantees. sequential decision making pomdp. value information known reward function partially observable markov decision process belief state represents tests outcomes. unfortunately gives exponentially large state space. idea using samples speed planning explored e.g. pomcp based monte carlo tree search samples states action histories despot samples scenarios evaluation policies iteration. approaches rely ﬁnite policy evaluated. number tests increases require samples. consequently limited short planning horizons small state action spaces. online learning. many different schemes investigated online learning online decision making look different setting integrate sequential information gathering online learning framework wherein learn decision model time. purpose lift framework higher level sampling procedure whose goal sample appropriately decision model parameters. several recent empirical simulations theoretical studies demonstrated effectiveness thompson sampling different settings. however different framework classical usage thompson sampling suggests choose action according probability optimal i.e. action maximizes reward expectation. pursue practical efﬁcient approaches challenging problem optimal given prior hypothesis space propose novel hypothesis enumeration strategy efﬁcient optimization show compensates inefﬁciency popular submodular surrogate-based greedy algorithms still enjoying provable theoretical guarantees; prior unknown propose efﬁcient principled online learning framework expected regret long run. addition demonstrate promising empirical performance framework realworld troubleshooting platform. believe work important step towards practical robust adaptive information acquisition systems. would like thank anonymous reviewers christopher dance helpful comments. work done part andreas krause visiting simons institute theory computing. work supported part microsoft research faculty fellowship snsf early postdoc.mobility fellowship google european doctoral fellowship.", "year": 2017}