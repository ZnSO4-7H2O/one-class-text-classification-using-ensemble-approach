{"title": "Multi-task Neural Networks for QSAR Predictions", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Although artificial neural networks have occasionally been used for Quantitative Structure-Activity/Property Relationship (QSAR/QSPR) studies in the past, the literature has of late been dominated by other machine learning techniques such as random forests. However, a variety of new neural net techniques along with successful applications in other domains have renewed interest in network approaches. In this work, inspired by the winning team's use of neural networks in a recent QSAR competition, we used an artificial neural network to learn a function that predicts activities of compounds for multiple assays at the same time. We conducted experiments leveraging recent methods for dealing with overfitting in neural networks as well as other tricks from the neural networks literature. We compared our methods to alternative methods reported to perform well on these tasks and found that our neural net methods provided superior performance.", "text": "although artiﬁcial neural networks occasionally used quantitative structure-activity/property relationship studies past literature late dominated machine learning techniques random forests. however variety neural techniques along successful applications domains renewed interest network approaches. work inspired winning team’s neural networks recent qsar competition used artiﬁcial neural network learn function predicts activities compounds multiple assays time. conducted experiments leveraging recent methods dealing overﬁtting neural networks well tricks neural networks literature. compared methods alternative methods reported perform well tasks found neural methods provided superior performance. quantitative structure analysis/prediction studies attempt build mathematical models relating physical chemical properties compounds chemical structure. mathematical models could used inform pharmacological studies providing silico metholodogy test rank compounds desired properties without actual experiments. already qsar studies used extenstively predict pharmacokinetic properties adme toxicity. improvements methods could provide numerous beneﬁts drug development pipeline. learning qsar models requires three main steps generating training measured properties known compounds encoding information chemical structure compounds buildilng mathematical model predict measured properties encoded chemical structure. high throughput screening studies ideal collecting training data hundred thousands compounds tested routinely using equipment assay interest generating molecular descriptors compound structures also well studied problem various methods encoding relevant information compounds vectors numbers developed. descriptors measure various physical chemical toplogical properties compounds interest. descriptors computed machine learning statistics supply mathematical models used make predictions. particular machine learning techniques used matter great deal quality qsar predictions. qsar competition sponsored merck competitors used descriptors training data nevertheless machine learning techniques allowed winning team achieve relative accuracy improvement approximately merck’s internal baseline. applying machine learning algorithms qsar rich history. early work applying machine learning qsar tasks used linear regression models quickly supplanted bayesian neural networks approaches. recently random forests projection pursuit partial least squares support vector machines also applied successfully task. methods diﬀerent advantages disadvantages recent review). practitioners ﬁeld often partial methods that addition making accurate predictions allow variable selection users able assess whether chosen features indeed useful viewpoint informed chemist methods allow easy assessment uncertainty models. also important issue controlling overﬁtting although concern paramount qsar domains high-capacity machine learning models applied small data sets. random forests have thus well-suited requirements since readily overﬁt since provide simple measure variable importance. bayesian neural networks particulary suitable assessing uncertainty model predictions controlling overﬁtting. however computational demands bayesian neural networks necessitate small hidden layers using variable selection reduce input dimensionality. gaussian process regression models viewed inﬁnite hidden layer limit bayesian neural networks still quite computationally expensive often requiring time cubic number training cases non-bayesian neural networks also applied qsar often single small hidden layer past decade seen tremendous improvements training methods deep wide neural networks well renewed appreciation advantages deeper networks. deep neural networks highly successful variety diﬀerent tasks including speech recognition computer vision natural language processing methodological improvements along faster machines allowed researchers train much larger powerful models instead neural networks hidden layer hidden units neural networks regularly trained many hidden layers thousands hidden units resulting millions tunable parameters. networks large even trained small datasets without substantial overﬁtting using early stopping weight penalties newer techniques unsupervised pre-training dropout techniques avoiding overﬁtting large neural nets trained small datasets relatively inexpensive computationally especially compared fully bayesian approach. mentioned above neural qsar models trained work diﬀer used qsar literature variety respects. however important diﬀerence neural nets explore paper multi-task neural networks operate multiple assays simultaneously something rare literature. work aware multi-task neural nets qsar erhan variety diﬀerences work present here. particular used recently developed dropout technique control overﬁtting access target protein features. additionally erhan focussed exploring performance kernel-based jrank algorithm collaborative ﬁltering diﬀerent neural variants. motivation behind multi-task learning performance related tasks improved learning shared models. multi-task neural network models particularly appealing shared learned feature extraction pipeline multiple tasks. learning general features produce better models weights multi-task nets also constrained data cases sharing statistical strength. qsar tasks naturally lend multi-tasking assays often related diﬀerent compounds might share features. even assays tenuously related still governed laws chemistry might still important learn broadly useful higher-level features initial descriptors. paper apply multi-task learning qsar using various neural network models. leveraging recent developments outlined above. results show neural nets multi-tasking lead signiﬁcantly improved results baselines generated random forests. machine learning model basis approach qsar feedforward neural network neural networks powerful non-linear models classiﬁcation regression dimensionality reduction. neural network maps input vectors output vectors repeated compositions simpler modules called layers. internal layers re-represent input learn features input useful task. mathematically l-layer neural network vector valued function input vectors minimize cost function. standard regression problems output ylxn; target vectors dimensional. typical cost function regression mean squared error trained neural networks work using minibatched stochastic gradient descent momentum. small minibatch training cases used update velocity parameters direct application neural networks qsar modeling train neural data single assay using vectors molecular descriptors input recorded activities training labels. single-task neural approach although simple depends suﬃcient training data single assay model well. suspect data scarcity reason agressively regularized models random forests popular literature order leverage data multiple assays multi-task neural qsar architecture makes predictions multiple assays simultaneously. vectors descriptors still serve input network separate output unit assay. given compound typically observe small number output values since general expect compounds appear assays regularly. multi-task neural training perform backpropagation output layer weights incident output units whose value observe current training case. simplicity treat compound appears diﬀerent assays diﬀerent training cases descriptor vector diﬀerent outputs observed. since number compounds screened varies across assays na¨ıvely combining training cases available assays training multitask neural would bias objective function towards whatever assay compounds. handle issue controlling many training cases assay minibatch. example assays could create minibatches cases drawing cases random particular assay wish emphasize cases assays. martin presented another leveraging related assays called proﬁleqsar similar spirit multi-task neural approach many important diﬀerences. proﬁle-qsar treats assays side information uses single task methods complete assay/compound activity matrix previously studied assays. imputed measured activities particular compound side information assays become additional descriptors making predictions compound. unlike proﬁle-qsar multi-task neural approach imputation activities. another diﬀerence multi-task neural trains available assays potentially makes predictions available assays well learning shared feature extraction pipeline. deep neural networks neural networks multiple hidden layers highly successful recently numerous applications speech recognition capable learning complicated rapidly-varying non-linear functions also capable extracting hierarchy useful features input. many successful deep neural network models millions tunable parameters wide layers thousands units. contrast neural nets used qsar date tend single layer sometimes hidden units recent advances training regularizing wide deep neural nets computer hardware changed dominant approach training neural networks machine learning community training small nets incapable overﬁtting often underﬁt instead agressively regularizing wide deep neural networks many tunable parameters. important lesson recent successes deep learning that although deeper networks always perform better shallow ones practitioners need using models trade breadth depth vice versa since particular architecture cannot best problems. paper describe initial attempts leverage advances deep neural network methods qsar applications. assays become cheaper assay results accumulate machine learning models high capacity best performance. many models trained weights training cases careful training regularization still perform well. using wide and/or deep neural nets many tunable parameters makes avoiding overﬁtting especially crucial. qsar modeling often want large expressive models capable representing complex dependencies descriptors activities data also quite limited. regularization broadly construed subject much recent deep learning research. generative unsupervised pre-training powerful data-dependent regularizer brought attention deep neural models particular. early stopping training based validation error partially control overﬁtting limited eﬀectiveness little validation data networks trained high capacity. penalizing large weights model also help avoid overﬁtting although sophisticated techniques experience much useful. technique dropout dropout randomly corrupts activations neurons network training zeroing activities independently. intuitively eﬀect noise added dropout penalizes large weights result uncertain predictions hidden unit activations. another view dropout approximate model averaging exponentially numerous diﬀerent neural nets produced deleting random subsets hidden units inputs. also view multi-task neural nets regularizing weights shared across assays. since output weights assay-speciﬁc using data assays powerful avoiding overﬁtting. instead shifting weights towards zero penalty weights would multi-task neural nets shift weights hidden unit feature detectors towards values extract features useful qsar tasks. hidden layers multi-task neural network in-eﬀect learn higher level abstract molecular descriptors training objective encourages create features useful task. cytochrome family subfamily polypeptide cytochrome family subfamily polypeptide isoform cytochrome family subfamily polypeptide cytochrome family subfamily polypeptide cytochrome family subfamily polypeptide group streptokinase expression inhibition protein phosphatase catalytic subunit alpha isoform biochemical identify small molecule inhibitors tim- yeast identify small molecule inhibitors yeast identify inhibitors sentrin-speciﬁc protease identify inhibitors sentrin-speciﬁc protease identify inhibitors sentrin-speciﬁc protease identify inhibitors sentrin-speciﬁc proteases using caspase- selectivity assay idenﬁty inhibitors two-pore domain potassium channel identify inhibitors mdm/mdmx interaction inhibitor hits mitochondrial permeability transition pore inihibition trypanosoma cruzi nih/t toxicity identify molecules bind repeats experiments assay results deposited pubchem used diﬀerent assays selected least several closely related. table lists assays used experiments. included cellular biochemical assays cases used multiple related assays example assays diﬀerent families cytochrome enzymes. generated molecular descriptors dragon input various machine learning models. dragon generate diﬀerent descriptors several descriptors inapplicable compounds data sets. excluding inapplicable ones left molecular descriptors. descriptor z-score normalized compounds union assays. single task neural models also generated additional binary features thresholding single descriptors. selected descriptors thresholds decision nodes used boosted ensemble limited depth decision trees. using assays treated problem classiﬁcation task using active/inactive labels produced assay depositors. qsar prediction formulated classiﬁcation problem regression problem ranking problem. techniques equally applicable problem formulations paper consider binary classiﬁcation version problem. natural model predicted activities would virtual screening ultimately ranking problem. although models trained optimized classiﬁcation performance area curve performance metric since emphasizes ranking aspect problem relevant virtual screening applications. assay held random ligands test leaving remaining training set. used several classiﬁers implemented scikitslearn package baselines random forests gradient boosted decision tree ensembles logistic regression split training four folds trained model four times diﬀerent fold held validation data. average test aucs four models reporting test results. used performance validation data select best particular model family models. extent baseline models required metaparameter tuning performed tuning hand using validation performance. neural networks many important metaparameters including architectural metaparameters layer sizes hidden unit link functions optimization metaparameters learning rates momentum values regularization metaparameters dropout probabilities layer long train stopping learning rate annealing schedules. trained neural nets used rectiﬁed linear units well neural nets used sigmoidal units neural nets eight million parameters. order experimental protocol avoids many viccissitudes human expert judgement possible neural metaparameters using bayesian optimization validation auc. bayesian optimization type sequential model-based optimization ideally suited globally optimizing blackbox noisy functions parsimonious number function evaluations. metaparamter optimization bayesian optimization constructs model function mapping metaparameter settings validation suggests jobs based acquisition function balances exploration areas space model highly uncertainty areas space likely produce better results best far. used spearmint software implements bayesian optimization algorithms snoek particular bayesian optimization ﬁnds good model conﬁgurations eﬃciently makes strong validation error small validation sets quickly overﬁt validation data. however overﬁtting validation data hurt test performance neural models relative baselines fewer metaparameters tuned readily repeatably hand. table contains main results assays investigated. assays best neural achieves test exceeding best baseline result statistically signiﬁgant margin. measured statistical signiﬁgance t-tests using standard errors training models repeatedly bootstrap samples remaining assays statistically signiﬁgant diﬀerence best decesion tree ensemble baseline best neural net. assays logistic regression worst performing model. assays closely related assays available multi-task neural nets often performed better single-task counterparts even priori information particular assays related despite make good predictions assays simultaneously including unrelated ones. three assays decision tree baselines best relative neural models fewest training cases closely related assays dataset. since generally expect high-capacity multi-task neural models provide beneﬁts larger datasets related assays leverage three assays baselines comparitively better surprising. assays used contain several groups closely related assays. speciﬁcally cytochrome assays minor variations four assays inhibition sentrinspeciﬁc proteases related cellular assays inhibition particular yeast strains. assays strongly positively correlated simply merging data might work well sophisticated multi-task neural method leverage general positive negative feature-conditional correlations. fact related assays worked nearly identical gains multi-task neural single-task methods might simply reﬂect gains expected adding data single-task classiﬁer. investigate issue created additional datasets combined data multiple assays. example assay three related table table shows average test random forests gradient boosted decision tree ensembles single-task neural nets multi-task neural nets assay. multi-task neural nets trained make predictions assays once. best neural result particular assay better decision tree baselines diﬀerences statistically signiﬁcant boldface cases best decision tree baseline better neural nets diﬀerences statistically signiﬁcant. assays added training data training still testing data exclusively manner created combined datasets trained gbms them since gbms best performing non-neural model. table shows results datasets single-task gbms multi-task neural nets using assays gbms combining training data related assays. datasets multi-task neural nets exhibit statistically signiﬁgant improvement single-task combined training gbm. remaining three datasets statistically signiﬁgant diﬀerence performance best three models second best. table highlights results comparing singlemulti-task neural nets assays related assays collection. assays multi-task neural trained using assays obtain statistically signiﬁgant improvements test single-task neural nets. since multi-task neural models learn ignore assays making predictions particular assay worst somewhat weaker results single-task neural waste capacity irrelevant tasks. indeed results. series assays closely related enough gbms trained combined training four assays much better gbms primary assay’s training set. series assays trained combined training sets even better multi-task neural trained make predictions datasets although improvement statistically signiﬁgant. however multi-task neural addition make predictions unrelated assays told series assays treated problem. contrast series series assays gbms trained combined training sets worse gbms primary training set. assays however multi-task neural improves upon single task neural showing leverage information related assays nuanced way. practice often somewhat related assays nevertheless identical situation multi-task neural models stil provide beneﬁts unlike classiﬁer simply combining training sets diﬀerent assays. series assays although gbms combined training sets better gbms trained data primary assay multi-task neural better still. result demonstrates assays group suﬃciently contradictory confuse models combined training still gains multi-task neural net. interesting cases arise assays related positively correlated strongly enough simply combine single dataset. scenario multitask neural nets shine negative partial correlations. table multi-task neural nets compare favorably gbms using training sets combine related assays. bold entries correspond statistically signiﬁgant diﬀerences. table assays related assays collection multi-task neural nets typically provide statistically signiﬁgant improvements single-task neural nets. bold entries correspond statistically signiﬁgand diﬀerences. since allowed bayesian optimization train fully connected neural networks many hidden units single layer used variety methods prevent overﬁtting. bayesian optimization quickly learned enable dropout non-zero weight penalty. given assay best performing neural always used dropout preliminary hand-tuned experiments dropout seemed crucial well. unlike qsar work literature example winkler warns including many descriptors even contain relevant information advise performing feature selection reduce number input dimensions drastically. although descriptors necessary informative well-regularized properly trained neural networks handle thousands correlated input features. trained neural nets using descriptors well ones using informative input features assays using informative input descriptors degrage test much using fewer typically produced large unnecessary drop test auc. figure shows test representative assays. generated plot training best multi-task neural using relevant primary assay diﬀerent numbers input descriptors. assays used descriptor used changing number hidden layers consistent eﬀect. performed separate bayesian optimization runs three hidden layer multi-task neural networks hidden layer single task neural networks. single task neural nets adding second hidden layer little eﬀect best result bayesian optimization particular model class achieved test regardless number hidden layers. however although consistent trend multi-task nets allowing deeper models seemed occasionally important. table shows results multi-task nets diﬀerent depths. although optimal depth predictable across assays assays large diﬀerences performance multi-task neural nets diﬀerent depths. depth results somewhat contradicts experience merck molecular activity prediction contest found using neural networks hidden layer crucial almost cases. unfortunately since neither contest data descriptors used contest public available research additional experiments speculate cause discrepancy. several assays contest many compounds assays used pubchem larger datasets likely provide enough training information usefully additional hidden layers. larger datasets aﬀecting optimal neural depth configure test representative assays multi-task neural using diﬀerent numbers input features. given number input descriptors selected best features measured information gain. table multi-task neural network results neural nets diﬀerent numbers hidden layers. bolded best result statistically signiﬁgant diﬀerence test second best entry row. sistent depth mattering multi-task nets trained since data assays. contest used regression formulation task unlike experiments work used binary classiﬁcation formulation exaccerbating diﬀerence number bits information training labels. since focus eﬀorts trying many diﬀerent descriptor sets types descriptors dragon compute exist contest data. larger descriptors ones dragon computes might improve results. example open source rdkit software provides morgan ﬁngerprint descriptors current version dragon compute commercial software packages could descriptor sets important information also. natural eﬀective leveraging data multiple assays training neural network qsar model. however many experiments need done settle exactly best solve qsar problems neural networks. treating task binary classiﬁcation problem using potentially unreliable active/inactive decisions assay depositors pubchem best approach problem. virtual screening application notion active compound towards particular target essential plan perform future work ranking version problem. given eﬀectiveness state-of-the-art bayesian optimization software packages practitioners longer fear large number metaparameters sophisticated neural network models since even small datasets able good metaparameter settings automatically. also hope develop better ways implementing multi-task neural nets make additional information assays likely related well target features side information. given rapid progress research neural network methods also hope apply advances deep learning community qsar problems. would like acknowledge christopher jordan-squire geoﬀ hinton work team merck molecular activity challenge; without contest would started work project.", "year": 2014}