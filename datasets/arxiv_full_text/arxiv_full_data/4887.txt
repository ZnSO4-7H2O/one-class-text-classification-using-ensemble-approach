{"title": "Robust Spectral Inference for Joint Stochastic Matrix Factorization", "tag": ["cs.LG", "cs.AI"], "abstract": "Spectral inference provides fast algorithms and provable optimality for latent topic analysis. But for real data these algorithms require additional ad-hoc heuristics, and even then often produce unusable results. We explain this poor performance by casting the problem of topic inference in the framework of Joint Stochastic Matrix Factorization (JSMF) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist. We then propose a novel rectification method that learns high quality topics and their interactions even on small, noisy data. This method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality.", "text": "spectral inference provides fast algorithms provable optimality latent topic analysis. real data algorithms require additional ad-hoc heuristics even often produce unusable results. explain poor performance casting problem topic inference framework joint stochastic matrix factorization showing previous methods violate theoretical conditions necessary good solution exist. propose novel rectiﬁcation method learns high quality topics interactions even small noisy data. method achieves results comparable probabilistic techniques several domains maintaining scalability provable optimality. summarizing large data sets using pairwise co-occurrence frequencies powerful tool data mining. objects often better described relationships inherent characteristics. communities discovered friendships song genres identiﬁed co-occurrence playlists neural word embeddings factorizations pairwise cooccurrence information recent anchor word algorithms perform spectral inference co-occurrence statistics inferring topic models co-occurrence statistics calculated using single parallel pass training corpus. algorithms fast deterministic provably guaranteed sensitive observation noise small samples often producing effectively useless results real documents present problems probabilistic algorithms. cast general problem learning overlapping latent clusters joint-stochastic matrix factorization subset non-negative matrix factorization contains topic modeling special case. explore conditions necessary inference cooccurrence statistics show anchor words algorithms necessarily violate conditions. propose rectiﬁed algorithm matches performance probabilistic inference—even small noisy datasets—without losing efﬁciency provable guarantees. validating real synthetic data demonstrate rectiﬁcation produces better clusters also unlike previous work learns meaningful cluster interactions. matrix represent co-occurrence pairs drawn objects joint probability pair objects goal discover latent clusters approximately decomposing babt object-cluster matrix column corresponds cluster probability drawing object conditioned object belonging cluster cluster-cluster matrix represents joint probability pairs clusters. call matrices joint-stochastic correspondence joint distributions; column-stochastic. example applications shown table anchor word algorithms solve jsmf problems using separability assumption topic contains least anchor word non-negligible probability exclusively topic. algorithm uses co-occurrence patterns anchor words summary basis co-occurrence patterns words. initial algorithm theoretically sound unable produce column-stochastic word-topic matrix unstable matrix inversions. subsequent algorithm ﬁxes negative entries still produces large negative entries estimated topic-topic matrix shown figure proposed algorithm infers valid topic-topic interactions. metric structures co-occurrence matrices required successful factorization. rn×n joint-stochastic matrix constructed training examples contain subset objects. wish latent clusters factorizing column-stochastic matrix rn×k joint-stochastic matrix rk×k satisfying babt probabilistic structure. figure shows event space model. distribution pairs clusters generated ﬁrst stochastic process hyperparameter m-th training example contains total objects model views example consisting possible pairs objects. pairs cluster assignments sampled selected distribution actual object pair drawn respect corresponding cluster assignments note process explain training example generated model shows model understands objects training examples. following model views parameters rather random variables. primary learning task estimate estimate recover hyperparameter conditional independence factorization babt equivalent separability assumption cluster basis object matrix terms assume submatrix comprised bag-of-words assumption every object pair object example except itself. implication work better understanding self-co-occurrences diagonal entries co-occurrence matrix. rows indices diagonal. rows form non-negative basis space assumption implies rank+ rank. providing identiﬁability factorization assumption becomes crucial inference note jsmf factorization unique column permutation meaning speciﬁc ordering exists among discovered clusters equivalent probabilistic topic models statistical structure. distribution distributions cluster distribution sampled training example. saying i.i.d samples directly observable. deﬁning posterior cluster-cluster matrix geometric structure. though separability assumption allows identify even noisy observation need throughly investigate structure cluster interactions. eventually related much useful information co-occurrence corresponding anchor bases contains enabling best training data. doubly non-negative matrices entrywise non-negative positive semideﬁnite claim proof take vector thus proving analogous linearity expectation. relying double non-negativity equation implies low-rank structure also double non-negativity similar proof anchor word algorithms consider neither double non-negativity cluster interactions implication co-occurrence statistics. indeed empirical co-occurrence matrices collected limited data generally indeﬁnite full-rank whereas posterior co-occurrences must positive semideﬁnite low-rank. approach efﬁciently enforce double nonnegativity low-rankness co-occurrence matrix based geometric property posterior behavior. later clarify process substantially improves quality clusters interactions eliminating noises restoring missing information. section describe estimate co-occurrence matrix training data rectify low-rank doubly non-negative. decompose rectiﬁed preserves doubly non-negative structure cluster interaction matrix. making estimator unbiased. putting diag)) linearity expectation. rectifying co-occurrence unbiased estimator model reality matrices often differ mismatch model assumptions data error estimation limited data. computed generally full-rank many negative eigenvalues causing large approximation error. posterior co-occurrence must lowrank doubly non-negative joint-stochastic propose rectiﬁcation methods diagonal completion alternating projection modiﬁes diagonal entries becomes low-rank non-negative joint-stochastic; enforces modiﬁes every entry enforces properties well positive semi-deﬁniteness. empirical results strongly favor alternating projection defer details diagonal completion appendix. based desired property posterior co-occurrence seek project estimator onto joint-stochastic doubly non-negative rank matrices. alternating projection methods like dykstra’s algorithm allow project onto intersection ﬁnitely many convex sets using projections onto individual turn. setting consider intersection three sets symmetric matrices elementwise non-negative matrices normalized matrices norn whose entry equal positive semi-deﬁnite matrices rank psdn project onto three sets follows πpsdnk eigendecomposition matrix modiﬁed negative eigenvalues largest positive eigenvalues zero. truncated eigendecompositions computed efﬁciently projections likewise efﬁcient. norn convex psdn not. however show alternating projection non-convex still works certain conditions guaranteeing local convergence. thus iterating three projections turn convergence rectiﬁes desired space. show satisfy conditions convergence behavior section selecting basis ﬁrst step factorization select subset objects satisfy separability assumption. want best rows row-normalized co-occurrence matrix rows nearly convex hull selected rows. gramschmidt process select anchors computes pivoted decomposition utilize sparsity scale beyond small vocabularies random projections approximately preserve distances rows experiments pivoted algorithm exploits sparsity instead using random projections thus preserves deterministic inference. recovering object-cluster ﬁnding basis objects infer entry coefﬁcients reconstruct bayes’ rule {p}k i-th terms basis rows corresponding since there reason expect real data generated topics much less exactly latent topics. effectively random projections necessary either proper dimensions based multiple estimate thus main task step solve simplex-constrained infer coefﬁcients object. exponentiated gradient algorithm solve problem similar note step efﬁciently done parallel object. recovering cluster-cluster recovered minimizing babtf inferred generally many negative entries probabilistic interaction topics. project onto joint-stochastic matrices produces large approximation error. figure algorithm produces negative cluster co-occurrence probabilities. probabilistic reconstruction alone second panel) removes negative entries offdiagonals one. trying rectiﬁcation produces valid joint stochastic matrix. consider alternate recovery method leverages separability assumption. submatrix whose rows columns correspond selected objects diagonal submatrix rows corresponding approach efﬁciently recovers cluster-cluster matrix mostly based co-occrrurence information corresponding anchor basis produces negative entries stability diagonal matrix inversion. note principle submatrices matrix also psd; hence psdn psdk. thus recovered unbiased estimator rectiﬁcation. rectiﬁed anchor words algorithm alternating projection ﬁxes many problems baseline anchor words algorithm matching performance gibbs sampling maintaining spectral inference’s determinism independence corpus size. evaluate direct measurement matrix quality well indicators topic utility. text datasets nips full papers york times news articles. eliminate minimal list english stop words prune rare words based tf-idf scores remove documents fewer tokens vocabulary curation. also prepare non-textual item-selection datasets users’ movie reviews movielens dataset music playlists complete yes.com dataset. perform similar vocabulary curation document tailoring exception frequent stop-object elimination. playlists often contain songs multiple times users unlikely review movies once augment movie dataset review contains number movies based half-scaled rating information varies stars stars. statistics datasets shown table times experiment randomly permuting order objects using median results minimize effect different orderings. also iterations alternating psdn norn turn. probabilistic gibbs sampling mallet standard option iterations. metrics evaluated original rectiﬁed whereas inferred rectiﬁed later realized essentially approach previously tried able generate qualitative results. although report comparable results probabilistic algorithms algorithm fails many circumstances. algorithm prefers rare unusual anchor words form poor basis topic clusters consist high-frequency terms repeatedly shown upper third table contrast algorithm rectiﬁcation successfully learns themes similar probabilistic algorithm. also verify cluster interactions given third panel figure explain topics correlate other. similar visualize anchor words cooccurrence space figure shows embedding nips vocabulary blue dots selected anchor words red. ﬁrst plot shows standard anchor words original cooccurrence space. second plot shows anchor words selected rectiﬁed space overlaid original co-occurrence space. third plot shows anchor words second plot overlaid ap-rectiﬁed space. rectiﬁed anchor words provide better coverage spaces explaining able achieve reasonable topics even rectiﬁcation also produces better clusters non-textual movie dataset. cluster notably genre-coherent year-coherent clusters original algorithm. example verify cluster walt disney animations mostly cluster fantasy movies represented lord rings ﬁlms similar clusters found probabilistic gibbs sampling. baseline algorithm repeats pulp fiction silence lambs times. arora neuron layer hidden recognition signal cell noise neuron layer hidden cell signal representation noise neuron layer cell hidden signal noise dynamic neuron layer cell hidden control signal noise neuron layer hidden cell signal recognition noise paper neuron circuit cell synaptic signal layer activity control action dynamic optimal policy controller reinforcement recognition layer hidden word speech image cell ﬁeld visual direction image motion object orientation gaussian noise hidden approximation matrix bound examples probabilistic neuron cell visual signal response ﬁeld activity control action policy optimal reinforcement dynamic robot recognition image object feature word speech features hidden layer dynamic neuron recurrent noise gaussian approximation matrix bound component variables quantitative results. measure intrinsic quality inference summarization respect jsmf objectives well extrinsic quality resulting topics. lines correspond four methods baseline algorithm previous work without rectiﬁcation diagonal completion alternating projection gibbs gibbs sampling. anchor objects form good basis remaining objects. measure recovery error rectiﬁed matrix. reduces error almost cases effective although expect error decrease increase number clusters reducing recovery error ﬁxed choosing better anchors extremely difﬁcult subset selection algorithm decreased error good matrix factorization small element respect original matrix wise approximation error preserve information indicates lower correlation clusters. diagonal dominancy measures much cluster distinct speciﬁcity dominancy songs corpus lacks baseline results dominancy undeﬁned algorithm picks song occurs playlist basis object. case original construction hence zero diagonal element making dominancy nan. figure experimental results real dataset. x-axis indicates logk varies topics topics. whereas baseline algorithm largely fails small infer quality even large alternating projection ﬁnds better basis vectors also shows stable comparable behaviors probabilistic inference every metric. inter-topic dissimilarity ters given objects becomes uniform making similar counts average number objects cluster occur cluster’s objects. experiments validate gibbs yield comparably speciﬁc distinct topics baseline simply repeat corpus distribution table coherence words occur together frequently. produces results close gibbs sampling baseline metric correlates human evaluation clusters worse coherence actually better metric penalize repetition semi-synthetic experiments matches gibbs sampling outperforms baseline discrepancies topic quality metrics smaller real experiments speculate semi-synthetic data well-behaved real data explaining issues recognized previously. work? rectiﬁcation diagonals empirical matrix correct. bursty objects yield diagonal entries large; extremely rare objects occur document yield zero diagonals. rare objects problematic general corresponding rows matrix sparse noisy rows likely selected pivoted rare objects likely anchors matrix likely highly diagonally dominant provides uninformative picture topic correlations. problems exacerbated small relative effective rank early choice poor anchor precludes better choice later number documents small case empirical relatively sparse strongly affected noise. mitigate issue exhaustive grid search document frequency cutoffs informative anchors. model performance inconsistent different cutoffs search requires cross-validation case nearly impossible good heuristics dataset number topics. fortunately low-rank matrix cannot many diagonally-dominant rows since violates rank property. diagonal entries small relative off-diagonals since violates positive semi-deﬁniteness. anchor word assumption implies non-negative rank ordinary rank same algorithm ideally remove information wish learn; rather low-rank projection suppresses inﬂuence small numbers noisy rows associated rare words well correlated others projection recovers missing information diagonals. converge? enjoys local linear convergence initial near convergence point psdn super-regular strong regularity holds ﬁrst condition recall rectiﬁed pushing toward ideal convergence point inside intersection. since shown close desired.the proxregular sets subsets super-regular sets prox-regularity psdn sufﬁcient second condition. permutation invariant spectral symmetric matrices deﬁned prox-regular prox-regular |supp| since element exactly positive components others zero psdn deﬁnition locally unique almost everywhere satisfying second condition almost surely. checking third condition priori challenging expect noise empirical prevent irregular solution following argument numerical example expect converge locally linearly verify local convergence practice. empirically ratio average distances iterations always nytimes dataset datasets similar. note rectiﬁed result pushing empirical toward ideal approximation factors computed based co-occurrence shape could distant c∗’s provable guarantees hold better rectiﬁed jsmf speciﬁc structure-preserving non-negative matrix factorization performing spectral inference. exploit similar separable structure problmes. tackle hyperspectral unmixing problems assume pure pixels separability-equivalent computer vision. general without structures rescal studies tensorial extension similar factorization symnmf infers rather babt topic modeling performs spectral inference third moment tensor assuming topics uncorrelated. core algorithm rectify input co-occurrence matrix combined several recent developments. proposes regularization methods recovering better nonlinearly projects co-occurrence low-dimensional space t-sne achieves better anchors ﬁnding exact anchors space. performs multiple random projections low-dimensional spaces recovers approximate anchors efﬁciently divide-and-conquer strategy. addition work also opens several promising research directions. exactly anchors found rectiﬁed form better bases ones found original space since topic-topic matrix doubly non-negative joint-stochastic learn super-topics multi-layered hierarchical model recursively applying jsmf topic-topic co-occurrence references alan mislove bimal viswanath krishna gummadi peter druschel. know inferring user proﬁles online social networks. proceedings international conference search data mining york february omer levy yoav goldberg. neural word embedding implicit matrix factorization. nips arora moitra. learning topic models going beyond svd. focs sanjeev arora rong yonatan halpern david mimno ankur moitra david sontag yichen jamesp. boyle richardl. dykstra. method ﬁnding projections onto intersection convex sets hilbert spaces. advances order restricted statistical inference volume lecture notes statistics pages springer york moontae david mimno. low-dimensional embeddings interpretable anchor-based topic inference. proceedings conference empirical methods natural language processing pages association computational linguistics thang nguyen yuening jordan boyd-graber. anchors regularized adding robustness extensibility scalable topic-modeling algorithms. association computational linguistics david mimno hanna wallach edmund talley miriam leenders andrew mccallum. optimizing nascimento student member bioucas dias. vertex component analysis fast algorithm unmix hyperspectral data. ieee transactions geoscience remote sensing pages c´ecile gomez borgne pascal allemand christophe delacourt patrick ledru. n-findr method versus independent component analysis lithological identiﬁcation hyperspectral imagery. international journal remote sensing maximilian nickel volker tresp hans-peter kriegel. three-way model collective learning multi-relational data. proceedings international conference machine learning icml pages anima anandkumar dean foster daniel sham kakade yi-kai liu. spectral algorithm latent dirichlet allocation. advances neural information processing systems annual conference neural information processing systems proceedings meeting held december lake tahoe nevada united states. pages", "year": 2016}