{"title": "Domain-Independent Optimistic Initialization for Reinforcement Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "In Reinforcement Learning (RL), it is common to use optimistic initialization of value functions to encourage exploration. However, such an approach generally depends on the domain, viz., the scale of the rewards must be known, and the feature representation must have a constant norm. We present a simple approach that performs optimistic initialization with less dependence on the domain.", "text": "reinforcement learning common optimistic initialization value functions encourage exploration. however approach generally depends domain viz. scale rewards must known feature representation must constant norm. present simple approach performs optimistic initialization less dependence domain. factor action-value function policy sometimes feasible compute approximate values linear function approximation learned weights feature vector. function approximation adds difﬁculties optimistic initialization indirectly speciﬁes value state-action pairs choice approach circumvent requirement knowing reward scale normalize rewards ﬁrst nonzero reward seen i.e. rt/|rst|. optimistically initialize representing expectation reward size ﬁrst reward achieved next timestep. function approximation means initializing weights ensure e.g. /|φ|. however requires constant among states actions. feature vector binary-valued approach guaranteeing constant norm stack applied coordinate. achieves goal cost doubling number features. besides removes sparsity feature vector often exploited efﬁcient algorithms. mild form optimism. optimistic view might achieve reward step equal ﬁrst observed reward case initialize sparse reward domains common arcade learning environment mild form often sufﬁcient. challenges trade-off exploration exploitation. agent must choose taking action known give positive reward explore possibilities hoping receive greater reward future. context common strategy unknown environments assume unseen states promising states already seen. approach optimistic initialization values several algorithms rely estimates expected values states expected values actions given state optimistic initialization consists initializing estimates higher values likely true value. depend prior knowledge expected scale rewards. paper circumvents limitations presenting different optimistically initialize value functions without additional domain knowledge assumptions. next section formalize problem setting well framework. present optimistic initialization approach. also present experimental analysis method using arcade learning environment testbed. consider markov decision process time step agent state needs take action action taken agent observes state reward transition probability function agent’s goal obtain policy maximizes expected discounted return discount |rst| initializing initializing rst. shift alleviates knowing since requirement anymore. also even though deﬁned terms need know non-zero reward observed. episodic tasks shift encourage agents terminate episodes fast possible avoid negative rewards. avoid provide termination reward rend γt−k+ number steps episode maximum number steps. equivalent receiving reward additional steps forces agent look something better. evaluated approach different domains different reward scales different number active features. domains obtained arcade learning environment framework dozens atari games agent access time step game screen data besides additional reward signal. compare learning curves regular sarsa sarsa q-values optimistically initialized. used basic features sarsa parameters reported bellemare al.. basic features divide screen tiles check tile possible colours active totalling features. game freeway consists controlling chicken needs cross street avoiding cars score point episode lasts steps agent’s goal cross street many times possible. game poses interesting exploration challenge ramdom exploration requires agent cross street acting randomly dozens time steps. means frequently selecting action avoiding cars. looking results figure that expected optimistic initialization help since favours exploration speeding process learning positive reward available game. improvement sarsa learning rates best performance game private different domain. game agent supposed move right several screens avoid enemies avoid negative rewards. along path agent collect intermediate rewards ultimate goal reach goal obtaining much larger reward. optimistic initialization much reckless sense takes much time realize speciﬁc state good sarsa conservative. interestingly observe exploration huge beneﬁt game larger thus besides formal analysis shown approach behaves would expect optimistically initialized algorithms behave. increased agents’ exploration trade sometimes agent exploited negative reward hoping obtain higher return. algorithms implemented without needing rigorous domain knowledge know work unfeasible perform optimistic initialization transparent way. besides requiring adaptations speciﬁc domains approach hinder algorithm performance. authors would like thak erik talvitie helpful input throughout research. research supported alberta innovates technology futures alberta innovates centre machine learning computing resources provided compute canada westgrid.", "year": 2014}