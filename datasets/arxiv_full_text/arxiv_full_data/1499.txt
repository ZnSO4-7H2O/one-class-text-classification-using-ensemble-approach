{"title": "Whodunnit? Crime Drama as a Case for Natural Language Understanding", "tag": ["cs.CL", "cs.AI", "cs.CV"], "abstract": "In this paper we argue that crime drama exemplified in television programs such as CSI:Crime Scene Investigation is an ideal testbed for approximating real-world natural language understanding and the complex inferences associated with it. We propose to treat crime drama as a new inference task, capitalizing on the fact that each episode poses the same basic question (i.e., who committed the crime) and naturally provides the answer when the perpetrator is revealed. We develop a new dataset based on CSI episodes, formalize perpetrator identification as a sequence labeling problem, and develop an LSTM-based model which learns from multi-modal data. Experimental results show that an incremental inference strategy is key to making accurate guesses as well as learning from representations fusing textual, visual, and acoustic input.", "text": "paper argue crime drama exempliﬁed television programs crime scene investigation ideal testbed approximating real-world natural language understanding complex inferences associated propose treat crime drama inference task capitalizing fact episode poses basic question naturally provides answer perpetrator revealed. develop dataset based episodes formalize perpetrator identiﬁcation sequence labeling problem develop lstm-based model learns multi-modal data. experimental results show incremental inference strategy making accurate guesses well learning representations fusing textual visual acoustic input. success neural networks variety applications creation large-scale datasets played critical role advancing machine understanding natural language together modalities. problem assumed several guises literature reading comprehension recognizing textual entailment notably question answering based text model predicts input whether perpetrator mentioned not. formulation generalizes episodes crime series. speciﬁc identity number persons committing crime well type police drama consideration. advantageously incremental track model predictions beginning episode examine behavior e.g. often changes mind whether consistent predictions perpetrator identiﬁed. develop dataset based episodes contains goldstandard perpetrator mentions well viewers’ guesses perpetrator episode unfolds. sequential nature inference task lends itself naturally recurrent network modeling. adopt generic architecture combines one-directional long-short term memory network softmax output layer binary labels indicating whether perpetrator mentioned. based architecture investigate following questions experimental results dataset reveal multi-modal representations essential task hand boding well real-world natural language understanding. also show incremental inference strategy guessing perpetrator accurately although model tends research connections several lines work natural language processing computer vision generally multi-modal learning. review related literature areas below. language grounding recent years seen increased interest problem grounding language physical world. various semantic space models proposed learn meaning words based linguistic visual acoustic input variety cross-modal methods fuse techniques image text processing also applied tasks generating image descriptions retrieving images given natural language query another strand research focuses explicitly encode underlying semantics images making structural representations work shares common goal grounding language additional modalities. model however static learns representations evolve time. video understanding work video understanding assumed several guises generating descriptions video clips retrieving video clips natural language queries learning actions video tracking characters movies also aligned screenplays plot synopses books improving scene prediction semantic browsing. work uses low-level features establish social networks main characters order summarize movies perform genre figure excerpt script speakers shown bold spoken dialog normal font scene descriptions italics. gold-standard entity mention annotations color. perpetrator mentions green words referring entities red. classiﬁcation although visual features used mostly isolation cases combined audio order perform video segmentation semantic movie indexing datasets released recently include movies textual data. movieqa large-scale dataset contains movies questions accompanied candidate answers correct. movies dataset also contains subtitles video clips scripts plots text described video service narration service visually impaired. moviedescription related dataset contains sentences aligned video clips movies. scriptbase another movie database consists movie screenplays used generate script summaries. contrast story comprehension tasks envisaged movieqa moviedescription focus single cinematic genre access entire episodes opposed video-clips dvss data. rather answering multiple factoid questions solve single problem albeit inherently challenging humans machines. hension i.e. reading text answering questions open-domain question answering i.e. ﬁnding answer question large collection documents cloze question completion i.e. predicting blanked-out word sentence visual question answering another related task provide natural language answer question image. inference task viewed form question answering multi-modal data focusing type question. compared previous work machine reading visual question answering interested temporal characteristics inference process study understanding evolves incrementally contribution various modalities importantly formulation inference task sequence labeling problem departs conventional question answering allowing study humans models alike make decisions time. work make episodes u.s. show crime scene investigation vegas successful crime series ever made. fifteen seasons total episodes produced course ﬁfteen years. procedural crime series follows team investigators employed vegas police department collect evaluate evvideo using closed captions time-stamped provided form subtitles part video data. alignment screenplay closed captions non-trivial since latter contain dialogue omitting speaker information scene descriptions. ﬁrst used dynamic time warping approximately align closed captions dialogue scripts. heuristically time-stamped remaining elements screenplay allocating time spans spoken utterances. table shows descriptive statistics dataset featuring number cases episode length type crime among information. data annotated goals mind. firstly order capture characteristics human inference process recorded participants incrementally update beliefs perpetrator. secondly collected goldstandard labels indicating whether perpetrator mentioned. speciﬁcally participant watches episode record guesses perpetrator episode ﬁnished perpetrator revealed participant annotates entities screenplay referring true perpetrator annotations collected webinterface. recruited three annotators postgraduate students proﬁcient english none regular viewers. obtained annotations episodes snapshot annotation interface presented figure interface provides short description episode i.e. form one-sentence summary summaries adapted season summaries available wikipedia. annotator watches episode sequence three minute intervals. every three minutes video halts annotator prepaired ofﬁcial videos screenplays downloaded website hosting show transcripts. dataset comprises episodes approximately minutes long. episodes follow regular plot begin display crime crime scene. team recurring police investigators attempt reconstruct crime perpetrator. investigation multiple suspects emerge crime often committed single person eventually identiﬁed convicted. episodes feature unrelated cases. beginning episode team split investigator assigned single case. episode alternates scenes covering case stories typically overlap. figure displays small excerpt screenplay. readers unfamiliar script writing conventions note scripts typically consist scenes headings indicating scene shot character cues preface lines actors speak scene descriptions explain camera sees number cases case grissom catherine nick warrick investigate wealthy couple murdered house. case meanwhile sara sent local high school cheerleader found eviscerated football ﬁeld. even though annotation task described reﬂects individual rather gold-standard behavior report inter-annotator agreement means estimating variance amongst participants. computed using cohen’s kappa based three episodes annotated participants. overall agreement task also measured percent agreement minority class found reasonably good indicating despite individual differences process guessing perpetrator broadly comparable across participants. finally annotators trouble distinguishing utterances refer case achieving gold standard mention annotation watching entire episode annotator reads screenplay second time tags entity mentions knowing perpetrator. word script three radio buttons attached annotator selects word refers perpetrator suspect character falls neither classes majority words button selected. snapshot interface second layer annotations shown figure ensure consistency annotators given detailed guidelines constitutes entity. examples include proper names titles sented screenplay corresponding part episode watched. reading screenplay must indicate every sentence whether believe perpetrator mentioned. able monitor humans create discard hypotheses perpetrators incrementally. mentioned earlier episodes feature case. annotators signal sentence case belongs whether irrelevant order obtain ﬁne-grained picture human guesses annotators additionally asked press large button soon think know perpetrator i.e. time three episodes annotators perpetrator class entity annotations percent agreement perpetrators entities. high agreement indicates task well-deﬁned elicited annotations reliable. second pass various entities script disambiguated terms whether refer perpetrator individuals. note work tokenlevel gold standard annotations directly. model trained sentence-level annotations obtain token-level ones assumption sentence mentions perpetrator contains token does. formalize problem identifying perpetrator crime series episode sequence labeling task. like humans watching episode model presented sequence inputs corresponding sentence script assigns label indicating whether perpetrator mentioned sentence model fully incremental labeling decision based solely information derived previously seen inputs. could formalized inference task multi-label classiﬁcation problem labels correspond characters script. although perhaps intuitive multi-class framework results output label space different episode renders comparison model performance across episodes problematic. contrast formulation advantage directly applicable episode indeed crime series. sketch inference task shown figure core model one-directional long-short term memory network lstm cells variant recurrent neural networks complex figure overview perpetrator prediction task. model receives input form text images audio. modality mapped feature representation. feature representations fused passed lstm predicts whether perpetrator mentioned computational unit emerged popular architecture representational power effectiveness capturing long-term dependencies. lstms provide ways selectively store forget aspects previously seen inputs consequence memorize information longer time periods. input output forget gates ﬂexibly regulate extent inputs stored used forgotten. lstm processes sequence inputs utilizes memory slot hidden state incrementally updated time step given input previous latent state previous memory state latent state time also shown work well general sound classiﬁcation extract -dimensional mfcc feature vector every milliseconds video. input sentence sample mfcc feature vectors associated time interval concatenate chronological order acoustic input modality fusion model learns fuse multimodal input part overall architecture. general method obtain combination input modalities single modality inputs concatenated m-dimensional vector multiply vector weight matrix dimension m-dimensional bias pass result rectiﬁed linear unit experiments investigate type knowledge strategy necessary identifying perpetrator episode. order shed light former question compare variants model access information different modalities. examine different inference strategies comparing lstm three baselines. ﬁrst lacks ability ﬂexibly fuse multi-modal information second notion history classifying inputs independently third baseline rule-base system neither uses multi-modal inputs notion history. also compare lstm humans watching csi. report results describe setup comparison models detail. mentioned earlier input model consists sequence sentences either spoken utterances scene descriptions augment textual input multi-modal information obtained alignment screenplays video textual modality words sentence mapped -dimensional glove embeddings pretrained wikipedia gigaword word embeddings subsequently concatenated padded maximum sentence length observed data order obtain ﬁxed-length input vectors. resulting vector passed convolutional layer maxpooling obtain sentence-level representation word embeddings ﬁne-tuned training. visual modality obtain video corresponding time span covered sentence sample frame sentence center associated period. frame -dimensional visual feature vector using ﬁnal hidden layer pre-trained convolutional network optimized object classiﬁcation acoustic modality sentence extract audio track video includes sounds background music spoken dialog. obtain mel-frequency cepstral coefﬁcient features continuous signal. mfcc features originally developed context speech recognition trained model using adam stochastic gradient-descent mini-batches episodes. weights initialized randomly except word embeddings initialized pre-trained -dimensional glove vectors ﬁne-tuned training. trained networks epochs report best result obtained training. results averages runs network. parameters optimized using cross-validation splits. sentence convolution layer three ﬁlters sizes convolution returns -dimensional output. ﬁnal sentence representation obtained concatenating output three ﬁlters dimension size hidden representation merged crossmodal inputs lstm layer nodes. learning rate apply dropout probability model comparison conditional random fields probabilistic graphical models sequence labeling. comparison allows examine whether lstm’s long-term memory feature integration beneﬁcial sequence prediction. experimented variety features obtained best results input sentence represented concatenated word embeddings. also compared lstm multi-layer perceptron hidden layers softmax output layer. replaced lstm overall network structure keeping methodology sentence convolution modality fusion associated parameters ﬁxed values described section hidden layers relu activations layersize lstm. learning rate makes independent predictions element sequence. comparison table precision recall detecting minority class humans various systems. report results crossvalidation held-out data using textual visual auditory modalities. aside supervised models described developed simple rule-based system require access labeled data. system defaults perpetrator class sentence containing personal possessive reﬂexive pronoun words assumes every pronoun refers perpetrator. pronoun mentions identiﬁed using string-matching precompiled list pronouns. system cannot incorporate acoustic visual data. human upper bound finally compared model performance humans. annotation task participants annotate sentences incrementally watching episode ﬁrst time. annotations express belief whether perpetrator mentioned. evaluate ﬁrst-pass guesses gold standard figure precision ﬁnal episode test episodes cross-validation splits. show scores episode global averages episodes ordered increasing model precision. report precision recall minority class focusing accurately models identify perpetrator mentions. table summarizes results averaged across cross-validation splits truly held-out test episodes overall observe humans outperform comparison models. particular human precision superior whereas recall comparable exception high recall since assumes pronouns refer perpetrators. analyze differences model human behavior detail section regard lstm visual acoustic modalities bring improvements textual modality however contribution appears complementary. also experimented acoustic visual features without high-level textual information lstm converges towards predicting majority class only. results held-out test reveal model generalizes well unseen episodes despite trained relatively small data sample compared standards deep learning. lstm consistently outperforms nonincremental mlp. shows ability utilize information previous inputs essential task. intuitively plausible; order identify perpetrator viewers must aware plot’s development make inferences episode evolves. outperformed systems including rule-based pro. contrast utilizes sequential information cannot ﬂexibly fuse information different modalities exploit non-linear mappings like neural models. type input enabled predict perpetrator mentions concatenated word embeddings trained crfs audio visual features together word embeddings however models converged predicting majority class. suggests crfs capacity model complex long sequences draw meaningful inferences based them. achieves reasonable score achieves high recall expense precision. precision-recall tradeoff much balanced neural systems. section assess directly lstm compares humans asked identify perpetrator episode. speciﬁcally measure precision ﬁnal episode compare human performance lstm model uses three modalities. figure shows precision results test episodes average precision horizontal bars. figure human lstm behavior course episodes plots show cumulative true positives shown cumulatively individual counts interval statistics relating gold perpetrator mentions shown black. vertical bars show humans press button indicate identiﬁed perpetrator. achieved humans). results also show moderate correlation model humans episodes difﬁcult lstm also result lower human precision. episodes left plot precision special cases. ﬁrst revolves around suicide strictly speaking crime second mention perpetrator ﬁnal model guessing? next analyze model’s guessing ability compares humans. figure tracks model behavior course episodes across equally sized intervals. show cumulative development cumulative true positive counts true positive counts within interval bars indicate times annotators pressed button. figure shows humans outperform lstm precision humans cautious guessing perpetrator ﬁrst human guess appears around sentence ﬁrst model guess around sentence ﬁrst true mention around sentence humans guess perpetrator however precise consistent. interestingly model guesses start episode closely follow pattern gold-perpetrator mentions indicates early model guesses noise meaningful predictions. analysis human responses illustrated figure three annotators plot points episode press button indicate know perpetrator also show number times annotators pressed button individually interval cumulatively course episode. analysis reveals viewers tend press button towards unexpected since episodes inherently designed obfuscate identiﬁcation perpetrator. moreover figure suggests types viewers eager viewers like model guess early change mind often therefore press button frequently three within time interval cumulatively times normalized respect length. statistics averaged across cases annotator table excerpts episodes together model predictions. model conﬁdence illustrated darker shades corresponding higher conﬁdence. true perpetrator mentions highlighted blue. conversation involving true perpetrator. bottom conversation suspect perpetrator. episode) conservative viewers guess late press button less frequently notice statistics figure averages across several episodes annotator watched thus viewer behavior unlikely artifact individual episodes table provides evidence lstm behaves like eager viewer. presents time episode model correctly identiﬁes perpetrator ﬁrst time. seen minimum average identiﬁcation times lower lstm compared human viewers. table shows model predictions screenplay excerpts. illustrate degree model’s belief perpetrator mentioned color intensity. true perpetrator mentions highlighted blue. ﬁrst example model mostly identiﬁes perpetrator mentions correctly. second example identiﬁes seemingly plausible sentences which however refer suspect true perpetrator. incremental inference strategy ﬂexible access previously observed information. compared model humans guess cautiously beginning consistent predictions strong suspicion. lstm starts guessing earlier leading superior initial true-positive rates however cost consistency. many directions future work. beyond perpetrators consider suspects emerge disappear course episode. note obtained suspect annotations used experiments. also interesting examine model behaves out-of-domain i.e. tested crime series e.g. order. finally detailed analysis happens episode give rise deeper understanding enabling applications like video summarization skimming. acknowledgments authors gratefully acknowledge support european research council project summa also thank annotators anonymous tacl reviewers whose feedback helped improve present paper members edinburghnlp helpful discussions suggestions.", "year": 2017}