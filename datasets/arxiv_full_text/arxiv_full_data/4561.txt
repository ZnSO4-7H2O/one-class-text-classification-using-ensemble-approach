{"title": "Learning unbelievable marginal probabilities", "tag": ["cs.AI", "cs.LG"], "abstract": "Loopy belief propagation performs approximate inference on graphical models with loops. One might hope to compensate for the approximation by adjusting model parameters. Learning algorithms for this purpose have been explored previously, and the claim has been made that every set of locally consistent marginals can arise from belief propagation run on a graphical model. On the contrary, here we show that many probability distributions have marginals that cannot be reached by belief propagation using any set of model parameters or any learning algorithm. We call such marginals `unbelievable.' This problem occurs whenever the Hessian of the Bethe free energy is not positive-definite at the target marginals. All learning algorithms for belief propagation necessarily fail in these cases, producing beliefs or sets of beliefs that may even be worse than the pre-learning approximation. We then show that averaging inaccurate beliefs, each obtained from belief propagation using model parameters perturbed about some learned mean values, can achieve the unbelievable marginals.", "text": "loopy belief propagation performs approximate inference graphical models loops. might hope compensate approximation adjusting model parameters. learning algorithms purpose explored previously claim made every locally consistent marginals arise belief propagation graphical model. contrary show many probability distributions marginals cannot reached belief propagation using model parameters learning algorithm. call marginals ‘unbelievable.’ problem occurs whenever hessian bethe free energy positive-deﬁnite target marginals. learning algorithms belief propagation necessarily fail cases producing beliefs sets beliefs even worse pre-learning approximation. show averaging inaccurate beliefs obtained belief propagation using model parameters perturbed learned mean values achieve unbelievable marginals. calculating marginal probabilities graphical model generally requires summing exponentially many states np-hard general variety approximate methods used circumvent problem. popular technique belief propagation particular sumproduct rule message-passing algorithm performing inference graphical model though exact efﬁcient trees merely approximation applied graphical models loops. natural question whether compensate shortcomings approximation setting model parameters appropriately. paper prove sets marginals simply cannot achieved belief propagation. cases provide algorithm achieve much better results using ensemble parameters rather single instance. given variables given probability distribution data. would like construct model reproduces certain marginal probabilities particular x\\xi nodes relevant clusters x\\xα idα}. write collection here indexes sets interacting variables subset variables whose interaction characterized vector sufﬁcient statistics corresponding natural parameters assume without loss generality irreducible meaning cannot written linearly independent functions depend collect sufﬁcient statistics natural parameters vectors normally learning graphical model would parameters marginal probabilities match target. here however exact inference compute marginals. instead approximate inference loopy belief propagation match target. beliefs must normalized one. tree graphs beliefs exactly equal marginals graphical model loopy graphs beliefs ﬁxed points often good approximations marginals. guaranteed locally consistent xα\\xi necessarily globally consistent exist single joint distribution beliefs marginals resultant beliefs called pseudomarginals rather simply marginals. vector refer node factor beliefs produced belief propagation. despite limitations found empirically work well many circumstances. theoretical justiﬁcation loopy belief propagation emerged proofs stable ﬁxed points local minima bethe free energy free energies important quantities machine learning kullback-leibler divergence data model distributions expressed terms free energies models optimized minimizing free energies appropriately. given energy function gibbs free energy distribution coefﬁcients |ni| number factors neighboring node compensate overcounting single-node marginals overlapping factor marginals. tree-structured qi−di bethe entropy exact hence bethe free energy. loopy graphs bethe entropy isn’t really even entropy neglects statistical dependencies present factor marginals. nonetheless bethe free energy often close enough gibbs free energy minima approximate true marginals since stable ﬁxed points minima bethe free energy helped explain belief propagation often successful. emphasize bethe free energy directly depends marginals joint distribution write vector pseudomarginals pseudomarginal space convex satisfy positivity local consistency constraints wish correct deﬁciencies belief propagation identifying parameters produces beliefs matching true marginals target distribution since ﬁxed points stationary points simply parameters produce stationary point pseudomarginal space necessary condition reach ﬁxed point there. simply evaluate gradient zero solve note principle gradient could used directly minimize bethe free energy complicated function usually cannot minimized analytically contrast using solve parameters needed move beliefs target location. much easier since bethe free energy linear approach learning parameters described ‘pseudo-moment matching’ lq-element vector overcomplete representation pseudomarginals must obey local consistency constraints convenient express pseudomarginals terms minimal parameters smaller dimensionality using afﬁne transform well known converge ﬁxed points cannot realized marginals joint distribution. section show converse also true distributions whose marginals cannot realized beliefs couplings. cases existing methods learning often yield poor results sometimes even worse performing learning all. surprising view claims contrary state belief propagation pseudo-moment matching always reach ﬁxed point reproduces target marginals. technically ﬁxed points always stable thus reachable running belief propagation. deﬁnition marginals ‘unbelievable’ belief propagation cannot converge parameters. belief propagation converge target namely marginals zero gradient sufﬁcient bethe free energy must also local minimum requires positivedeﬁnite hessian subspace pseudomarginals satisﬁes local consistency constraints. since energy linear pseudomarginals hessian given second derivative bethe entropy projection constrains derivatives subspace spanned minimal parameters hessian positive deﬁnite evaluated parameters given give minimum target target cannot stable ﬁxed point loopy belief propagation. section calculate bethe hessian explicitly binary model pairwise interactions. theorem unbelievable marginal probabilities exist. proof. proof example. simplest unbelievable example binary graphical model xixj. symmetry marginals target nodes pairs substituting marginals appropriate bethe hessian gives matrix negative eigenvalue associated eigenvector symmetry marginals singlenode components pairwise components thus bethe free energy minimum marginals stable ﬁxed points occur local minima bethe free energy cannot reproduce marginals parameters. hence marginals unbelievable. unbelievable marginals exist actually quite common section graphical models multinomial gaussian variables least loops always pseudomarginals hessian positive deﬁnite hand marginals sufﬁciently small correlations believable guaranteed positive-deﬁnite bethe hessian stronger conditions described. pseudo-moment matching fails reproduce unbelievable marginals alternative gradient descent procedure learning analagous wake-sleep algorithm used train boltzmann machines original rule derived gradient descent kullback-leibler figure landscape bethe free energy binary graphical model pairwise interactions. slice bethe free energy along axis pseudomarginal space three different values parameters energy linear pseudomarginals varying parameters changes tilt free energy. remove local minima. second derivatives free energies identical. second derivative positive local minimum exist negative parameters produce local minimum. two-dimensional slice bethe free energy colored according minimum eigenvalue λmin bethe hessian. bethe wake-sleep learning beliefs proceed along toward target marginals stable ﬁxed points exist believable region target resides unbelievable region learning equilibrates ﬁxed points jump believable regions either side unbelievable zone. gibbs free energy using energy function cost function ‘bethe divergence’ replacing free energies bethe free energies evaluated true marginals beliefs obtained ﬁxed points data’s free energy depend beliefs β/∂b ﬁxed points belief propagation stationary points bethe free energy β/∂b consequently ∂dβ/∂b furthermore entropy terms free energies depend explicitly expectations sufﬁcient statistics pseudomarginals gradient forms basis simple learning algorithm. step learning belief propagation obtaining beliefs current parameters parameters changed opposite direction gradient learning rate. generally increases bethe free energy beliefs decreasing data hopefully allowing draw closer data marginals. call learning rule bethe wake-sleep algorithm. ﬁxed points. might re-initialize messages ﬁxed starting point choose random initial messages restart messages stopped previous learning step. experiments ﬁrst approach initializing constant messages beginning run. bethe wake-sleep learning rule sometimes places minimum true data distribution belief propagation give true marginals ﬁxed points. however reasons provided above cannot occur bethe hessian positive deﬁnite. bethe wake-sleep algorithm attempts learn unbelievable marginals parameters beliefs reach ﬁxed point instead continue vary time still learning reaches equilibrium temporal average beliefs equal unbelievable marginals. theorem bethe wake-sleep algorithm reaches equilibrium unbelievable marginals matched belief propagation ﬁxed points averaged equilibrium ensemble parameters. proof. equilibrium time average parameter changes zero deﬁnition substitution bethe wake-sleep equation directly implies deterministic mapping minimal representation pseudomarginals gives learning equilibrated ﬁxed points belief propagation occur right frequency averaged together reproduce target distribution exactly note none individual ﬁxed points close true marginals. call inference algorithm ensemble belief propagation ensemble produces perfect marginals exploiting constant small amplitude learning thus assumes correct marginals perpetually available. also works well learning turned parameters drawn randomly gaussian distribution mean covariance matched equilibrium distribution simulations always low-rank principle components needed good performance. gaussian ensemble quite accurate continued learning performance still markedly better available ﬁxed points. target within convex hull believable pseudomarginals learning cannot reach equilibrium eventually gets close remains consistent difference must increase without bound. though possible principle observe effect experiments. also equilibrium belief propagation learning iteration fails converge. experiments section concentrate ising model binary variables factors comprising individual variables pairs energy function jijxixj. sufﬁcient statistics various ﬁrst second moments xixj natural parameters jij. model target distributions model. parameterize pseudomarginals remaining probabilities linear functions values. positivity constraints local consistency constraints appear interactions ﬁnite inequality constraints active learning proceeds bethe wake-sleep algorithm causes parameters converge discrete limit cycle attempting learn unbelievable marginals. limit cycle projected onto ﬁrst principal components cycle. corresponding beliefs limit cycle projected onto ﬁrst principal components trajectory pseudomarginal space. believable regions pseudomarginal space colored cyan unbelievable regions yellow inconsistent pseudomarginals black. limit cycle average beliefs precisely equal target marginals average many ﬁxed points generated randomly perturbed parameters still produces better approximation target marginals individual believable ﬁxed points. even best amongst several ﬁxed points cannot match unbelievable marginals ensemble leads much improved performance figure shows fraction marginals unbelievable -node fully-connected ising models random coupling parameters negative eigenvalue. generated ising model targets using selected unbelievable ones evaluated performance ensemble various methods choosing parameters used exponential temporal message damping time steps mundamped e−/. fixed points declared messages changed less single time step. evaluated performance actual parameters generated target pseudomoment matching best-matching beliefs obtained time bethe wake-sleep learning. also measured performance parameter ensembles last iterations bethe wake-sleep learning parameters sampled gaussian mean covariance ensemble. belief propagation gave poor approximation target marginals expected model many strong loops. even learning could never correct marginals guaranteed selection unbelievable targets. ensemble belief propagation gave excellent results. using exact parameter ensemble gave orders magnitude improvement limited figure performance learning unbelievable marginals. fraction marginals unbelievable. marginals generated fully connected -node binary models random biases performance models pairwise couplings measured bethe divergence euclidean distance target generated selected unbelievability. bars represent central quartiles white line indicates median. models graphical model generated target distribution parameters pseudomoment matching beliefs best performance encountered bethe wake-sleep learning using exact parameters last iterations learning gaussian-distributed parameters ﬁrstsecond-order statistics studies also made bethe hessian draw conclusions belief propagation. instance hessian reveals ising model’s paramagnetic state becomes unstable large enough couplings another example hessian positive deﬁnite throughout pseudomarginal space bethe free energy convex thus unique ﬁxed point stronger interpretation appears underappreciated hessian positive deﬁnite pseudomarginals never ﬁxed point there parameters. might hope adjusting parameters belief propagation systematic paper could approximation perform exact inference. proved futile hope belief propagation simply never converge certain marginals. however also provided algorithm work ensemble belief propagation uses several different parameters different ﬁxed points averages results. approach preserves locality scalability make popular corrects defects cost running algorithm times. additionally raises possibility systematic compensation ﬂaws might exist mapping individual parameters ensemble parameters {θebp} could used ebp. especially clear application discriminative models like conditional random fields models trained known inputs produce known inferences generalize draw novel inferences novel inputs. belief propagation used learning model fail even known training examples happen unbelievable. overall performance suffer. ensemble remedy training failures thus allow better performance reliable generalization. paper addressed learning fully-observed models only marginals variables available training. unbelievable marginals exist models hidden variables well. ensemble work fully-observed case training require inference hidden variables wake sleep phases. important inference engine brain. inference hard neural computations resort approximations perhaps including belief propagation would undesirable neural circuits blind spots i.e. reasonable inferences cannot draw precisely occurs averaging models blind spot eliminated. brain synaptic weights ﬂuctuate variety mechanisms. perhaps ﬂuctuations allow averaging models thereby reach conclusions unattainable deterministic mechanism.", "year": 2011}