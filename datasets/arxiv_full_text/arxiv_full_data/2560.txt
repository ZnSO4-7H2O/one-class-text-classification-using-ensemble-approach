{"title": "Online Optimization Methods for the Quantification Problem", "tag": ["stat.ML", "cs.AI", "cs.IR", "cs.LG"], "abstract": "The estimation of class prevalence, i.e., the fraction of a population that belongs to a certain class, is a very useful tool in data analytics and learning, and finds applications in many domains such as sentiment analysis, epidemiology, etc. For example, in sentiment analysis, the objective is often not to estimate whether a specific text conveys a positive or a negative sentiment, but rather estimate the overall distribution of positive and negative sentiments during an event window. A popular way of performing the above task, often dubbed quantification, is to use supervised learning to train a prevalence estimator from labeled data.  Contemporary literature cites several performance measures used to measure the success of such prevalence estimators. In this paper we propose the first online stochastic algorithms for directly optimizing these quantification-specific performance measures. We also provide algorithms that optimize hybrid performance measures that seek to balance quantification and classification performance. Our algorithms present a significant advancement in the theory of multivariate optimization and we show, by a rigorous theoretical analysis, that they exhibit optimal convergence. We also report extensive experiments on benchmark and real data sets which demonstrate that our methods significantly outperform existing optimization techniques used for these performance measures.", "text": "estimation class prevalence i.e. fraction population belongs certain class useful tool data analytics learning ﬁnds applications many domains sentiment analysis epidemiology etc. example sentiment analysis objective often estimate whether speciﬁc text conveys positive negative sentiment rather estimate overall distribution positive negative sentiments event window. popular performing task often dubbed quantiﬁcation supervised learning train prevalence estimator labeled data. contemporary literature cites several performance measures used measure success prevalence estimators. paper propose ﬁrst online stochastic algorithms directly optimizing quantiﬁcationspeciﬁc performance measures. also provide algorithms optimize hybrid performance measures seek balance quantiﬁcation classiﬁcation performance. algorithms present signiﬁcant advancement theory multivariate optimization show rigorous theoretical analysis exhibit optimal convergence. also report extensive experiments benchmark real data sets demonstrate methods signiﬁcantly outperform existing optimization techniques used performance measures. quantiﬁcation deﬁned task estimating prevalence classes interest unlabeled given training items labeled according classes. quantiﬁcation ﬁnds natural application contexts characterized distribution drift i.e. contexts training data exhibit class prevalence pattern test data. phenomenon different reasons including inherent non-stationary character context class bias affects selection training data. †this work done shuai research associate qcri-hbku. ‡sanjay chawla leave university sydney. §fabrizio sebastiani leave consiglio nazionale delle ricerche italy. na¨ıve tackle quantiﬁcation classify count approach i.e. classify unlabeled item independently compute fraction unlabeled items attributed class. however good classiﬁer necessarily lead good quantiﬁer assuming binary case even false positives false negatives comparatively small quantiﬁcation accuracy might result signiﬁcantly different researchers study quantiﬁcation task right rather byproduct classiﬁcation. fact quantiﬁcation classiﬁcation disguise also seen fact evaluation measures different classiﬁcation need employed. quantiﬁcation actually amounts computing well estimated class distribution actual class distribution respectively denote true estimated prevalence); such natural evaluate quality function class f-divergences natural choice class kullback-leibler divergence deﬁned indeed frequently used measure evaluating quantiﬁcation note non-decomposable i.e. error make estimating cannot broken item-level errors. feature inherent feature measure evaluating quantiﬁcation. fact error made given unlabeled item impacts overall quantiﬁcation error depends items classiﬁed; e.g. unlabeled items generating additional false negative actually beneﬁcial overall quantiﬁcation accuracy measured function. fact measure choice quantiﬁcation non-decomposable lead structured output learners svmperf allow direct optimization non-decomposable functions; approach esuli sebastiani indeed based optimizing using svmperf. however minimizing objective quantiﬁcation regardless value fairly paradoxical. authors observed might lead generation unreliable quantiﬁers have result championed idea optimizing multi-objective measures combine quantiﬁcation accuracy classiﬁcation accuracy. using decision-tree-like approach minimizes |fp−fn| product |fn−fp| measure quantiﬁcation error measure classiﬁcation error; also optimizes measure combines quantiﬁcation classiﬁcation accuracy. svmperf provide recipe optimizing general performance measures serious limitations. svmperf designed directly handle applications large streaming data sets norm. svmperf also scale well multi-class settings time required method exponential number classes. paper develop stochastic methods optimizing large family popular quantiﬁcation performance measures. methods effortlessly work streaming data scale large datasets offering training times order magnitude faster approaches svmperf. quantiﬁcation methods. quantiﬁcation methods proposed years broadly classiﬁed classes namely aggregative non-aggregative methods. aggregative approaches perform quantiﬁcation ﬁrst classifying individual items intermediate step non-aggregative approaches require step estimate class prevalences holistically. methods fall former class latter class representatives sake simplicity assume quantiﬁcation tackled aggregative i.e. classiﬁcation individual items necessary intermediate step estimation class prevalences. note however necessary; non-aggregative approaches quantiﬁcation found within class aggregative methods distinction made methods ﬁrst general-purpose learning algorithms post-process prevalence estimates account estimation biases methods instead learning algorithms explicitly devised quantiﬁcation paper focus latter class methods. applications quantiﬁcation. application perspective quantiﬁcation especially useful ﬁelds inherently interested aggregate data care little individual cases. aside applications ﬁelds quantiﬁcation also used contexts diverse natural language processing resource allocation tweet sentiment analysis veterinary sciences quantiﬁcation independently studied within statistics machine learning data mining unsurprisingly given varied literature quantiﬁcation also goes different names counting class probability re-estimation class prior estimation learning class balance applications quantiﬁcation estimation class prevalences itself rather used improve accuracy tasks classiﬁcation. instance balikas quantiﬁcation model selection supervised learning tuning hyperparameters yield best quantiﬁcation accuracy validation data; allows hyperparameter tuning performed without incurring costs inherent k-fold cross-validation. saerens followed authors apply quantiﬁcation customize trained classiﬁer class prevalence exhibited test goal improving classiﬁcation accuracy unlabeled data exhibiting class distribution different training set. work chan seen direct application notion quantiﬁcation tune word sense disambiguator estimated sense priors test set. work also seen instance transfer learning since goal adapt word sense disambiguation algorithm domain different algorithm trained upon. stochastic optimization. discussed section goal paper perform quantiﬁcation directly optimizing online stochastic setting speciﬁc performance measures quantiﬁcation problem. recent advances seen much progress efﬁcient methods online learning optimization full information bandit settings works frequently assume optimization objective notion regret considered decomposable written expectation losses penalties individual data points. however performance measures quantiﬁcation multivariate complex structure form. recent progress towards developing stochastic optimization methods nondecomposable measures. however approaches satisfy needs problem. work addresses problem optimizing structured svmperf-style objectives streaming fashion requires maintenance large buffers result offers poor convergence. work narasimhan presents online stochastic methods optimizing performance measures concave pseudo-linear canonical confusion matrix predictor. however method requires computation gradients fenchel dual performance measures difﬁcult quantiﬁcation performance measures study nested structure. methods extend work provide convenient routines optimizing complex performance measures used evaluating quantiﬁcation. sake simplicity paper restrict analysis binary classiﬁcation problems linear models. denote space feature vectors label shall assume data points generated according ﬁxed unknown distribution denote proportion positives population algorithms training time receive training points sampled denote mentioned above present algorithms analyses learning linear model denote model space denote radii domain model space respectively. however note algorithms analyses extended learning non-linear models kernels well multi-class quantiﬁcation problems. however postpone discussion extensions expanded version paper. focus work shall optimization quantiﬁcation-speciﬁc performance measures online stochastic settings. concentrate performance measures represented functions confusion matrix classiﬁer. binary setting confusion matrix completely described terms true positive rate true negative rate classiﬁer. however initially develop algorithms reward functions surrogates values. done ease algorithm design analysis since values count-based form non-concave non-differentiable estimators. surrogates concave almost-everywhere differentiable. formally reward function assigns reward prediction data point true label data point given reward function model data point calculate rewards positive negative points. average expected value rewards treated surrogates respectively. note since ting i.e. denotes indicator function. sake convenience function concave lr-lipschitz takes values bounded range examples surrogate reward functions examples reward functions surrogates classiﬁcation accuracy indicator function inverted hinge loss function performance measures task quantiﬁcation requires estimating distribution unlabeled items across available classes binary setting. work target quantiﬁcation performance measures well hybrid classiﬁcation-quantiﬁcation performance measures. discuss turn. kullback-leibler divergence recent years performance measure become standard quantiﬁcation literature evaluation binary multiclass quantiﬁcation redeﬁne convenience. distributions values kld) range +∞.. note since distance function algorithms driven reward maximization uniformity will instead trying minimize maximize −kld; call latter negkld. normalized squared score measure quantiﬁcation accuracy introduced deﬁned }|s| ignoring normalization constants performance measure attempts reduce direct measure quantiﬁcation error. recall section several works advocated hybrid multi-objective performance measures balance quantiﬁcation classiﬁcation performance. measures typically take quantiﬁcation performance measure combine classiﬁcation performance measure. typically classiﬁcation performance measure sensitive class imbalance chosen balanced accuracy f-measure g-mean hybrid performance measures discussed literature presented below. classiﬁcation-quantiﬁcation balancing work introduced performance measure attempt compromise classiﬁcation quantiﬁcation accuracy. discussed section performance measure deﬁned weighted combination measure classiﬁcation accuracy pclass measure quantiﬁcation accuracy pquant. sake simplicity experiments adopt pclass pquant. however stress methods suitably adapted work choices pclass pquant. rithms. deﬁne refer reader tables details. bakld hybrid performance measure takes weighted average negkld; i.e. bakld performance measure gives user strong handle much emphasis placed quantiﬁcation much classiﬁcation performance. bakld experiments show methods offer attractive tradeoff two. particularly well-behaved performance measure since capable taking unbounded values within compact domain unit simplex. poses problem optimization algorithms point view convergence well numerical stability. solve problem computing distributions perform additive smoothing computing denotes smoothed version denominator normalizing factor. quantity often used smoothing factor adopt here. smoothed versions used place non-smoothed versions deﬁne hybrid performance measures constructed taking ratio classiﬁcation quantiﬁcation performance measures. exercise obtain performance measures mimic f-measure also pseudolinear performance measure ability methods directly optimize complex performance measures indicative utility terms freedom allow user design objectives datatask-speciﬁc manner. cqreward bkreward hybrid performance measures deﬁned cqreward +kld. notice performance measures optimized numerator i.e. large denominator small translates large cqreward small bkreward. clearly performance measures encourage good performance respect classiﬁcation quantiﬁcation penalize predictor either neglects quantiﬁcation better classiﬁcation performance round. past section seen introduce wide variety quantiﬁcation hybrid performance measures. these negkld q-measure already prevalent quantiﬁcation literature introduced bakld cqreward bkreward. discussed before exploring large variety performance measures demonstrate utility methods respect quantiﬁcation problem present newer ways designing hybrid performance measures give user expressivity tailoring performance measure task hand. also note performance measures extremely diverse complex structures. show negkld q-measure bakld nested concave functions speciﬁcally concave functions functions concave confusion matrix predictor. hand cqreward bkreward turn pseudo-concave functions confusion matrix. thus working different families performance measures here different properties requires different optimization techniques. previous discussion sections clariﬁes aspects efforts quantiﬁcation literature. firstly speciﬁc performance measures developed adopted evaluating quantiﬁcation performance including q-measure etc. secondly algorithms directly optimize performance measures desirable evidenced recent works works mentioned make tools optimization literature learn linear non-linear models perform quantiﬁcation. state efforts direction adopted structural approach optimizing performance measures great success however approach comes severe drawbacks. structural although signiﬁcant tool allows optimization arbitrary performance measures suffers drawbacks. firstly structural surrogate necessarily tight surrogate performance measures something demonstrated past literature lead poor training. importantly optimizing structural surrogate requires expensive cutting plane methods known scale poorly amount training data well unable handle streaming data. alleviate problems propose stochastic optimization algorithms directly optimize large family quantiﬁcation performance measures. methods come sound theoretical convergence guarantees able operate streaming data sets experiments demonstrate offer much faster accurate quantiﬁcation performance variety data sets. optimization techniques introduce crucial advancements ﬁeld stochastic optimization multivariate performance measures address families performance measures discussed concluding section nested concave performance measures pseudo-concave performance measures. describe turn below. table list nested concave performance measures canonical expressions terms confusion matrix denote values denote proportion positives negatives population. columns give closed form updates used steps algorithm name ﬁrst class performance measures deal concave combinations concave performance measures. formally given three concave functions deﬁne performance measure respectively denote either values surrogate reward functions therefor. examples performance measures include negative performance measure qmeasure described section table describes performance measures canonical form i.e. expressions terms values. describing algorithm nested concave measures recall notion concave fenchel conjugate concave functions. concave function fenchel conjugate deﬁned assume functions deﬁning performance measures γ-smooth constant true functions save function used deﬁnition quantiﬁcation measure. however carry smoothing step pointed section algorithm nemsis nested primal-dual stochastic updates require outer wrapper function inner performance measures step sizes feasible sets ensure classiﬁer data stream points position present algorithm nemsis stochastic optimization nested concave functions. algorithm gives outline technique. note direct application traditional stochastic optimization techniques nested performance measures considered possible discussed before. nemsis overcomes challenges exploiting nested dual structure performance measure carefully balancing updates inner outer levels. every time step nemsis performs four cheap updates. ﬁrst update primal ascent update model vector takes weighted stochastic gradient descent step. note step involves projection step model vectors denoted experiments deﬁned euclidean norm-bounded vectors projection could effected using euclidean normalization done time model vectors d-dimensional. weights descent step decided dual parameters functions nemsis updates dual variables three simple steps. fact line numbers executed closed form performance measures allows rapid updates. appendix simple derivations. state convergence proof nemsis. note despite complicated nature performance measures tackled nemsis still able recover optimal rate convergence known stochastic optimization routines. refer reader appendix proof theorem. proof requires careful analysis primal dual update steps different levels tying updates together taking account nesting structure performance measure. theorem suppose given stream random samples drawn distribution algorithm executed step sizes nested concave performance measure then universal constant average model output algorithm satisﬁes probability least table list pseudo-concave performance measures canonical expressions terms confusion matrix note denote proportion positives negatives population. related work narasimhan narasimhan proposed algorithm spade offered stochastic optimization concave performance measures. note although performance measures considered indeed concave difﬁcult apply spade directly since spade requires computation gradients fenchel dual function difﬁcult compute given nested structure function. nemsis hand requires duals individual functions much accessible. moreover nemsis uses much simpler dual update involve parameters fact closed form solution cases. spade hand performs dual gradient descent requires tuning another step length parameter. third beneﬁt nemsis achieves logarithmic regret respect dual updates whereas spade incurs polynomial regret gradient descent-style dual update. next class performance measures consider expressed ratio quantiﬁcation classiﬁcation performance measure. formally given convex quantiﬁcation performance measure pquant concave classiﬁcation performance measure pclass deﬁne performance measure assume performance measures pquant pclass positive valued. performance measures useful allowing system designer balance classiﬁcation quantiﬁcation performance. moreover form measure allows enormous amount freedom choosing quantiﬁcation classiﬁcation performance measures. examples performance measures include cqreward bkreward measures. introduced section represented canonical forms table performance measures constructed described above ratio concave convex measures called pseudo-concave measures. because although functions concave level sets still convex makes possible optimize efﬁciently. intuition behind this need introduce notion valuation function corresponding performance measure. passing note remark non-concavity performance measures nemsis cannot applied here. deﬁnition valuation pseudo-concave performance measure pclass pquant level deﬁned seen valuation function deﬁnes level sets performance measure. this notice positivity functions pquant pclass however since pclass concave pquant convex concave function close connection level sets notions valuation functions exploited give optimization algorithms pseudo-linear performance measures f-measure approaches treat valuation function form proxy surrogate original performance measure optimize hopes making progress respect original measure. taking approach performance measures yields natural algorithm optimizing pseudoconcave measures outline algorithm algorithm repeatedly trains models optimize valuations current level upgrades level itself. notice step algorithm concave maximization problem convex something done using variety methods following nemsis used implement step. also notice step deﬁnition valuation function carried simply setting turns linear rate convergence well-behaved performance measures. next result formalizes statement. note result similar arrived pseudo-linear functions. theorem suppose execute algorithm pseudo-concave performance measure quantiﬁcation performance measure always takes values range supw∈w optimal performance level excess error refer reader appendix proof theorem. theorem generalizes result general case pseudo-concave functions. note pseudo-concave functions deﬁned table care taken ensure quantiﬁcation performance measure satisﬁes drawback cannot operate streaming data settings requires concave optimization oracle. however notice performance measures table valuation function always least nested concave function. motivates nemsis solve inner optimization problems online fashion. combining online technique approximately execute step gives scan algorithm outlined algorithm optimization problems. refer reader appendix proof theorem. theorem suppose execute algorithm pseudo-concave performance measure pquant always takes values range epoch lengths following geometric rate increase constant cψζζr effective constant nemsis analysis inner invocations nemsis scan. also excess error model generated probability least moreover number samples consumed till point ignoring universal constants algorithm scan stochastic concave alternation require objective model space step sizes epoch lengths ensure classiﬁer repeat stream exhausted return related work narasimhan narasimhan also proposed algorithms stamp seek optimize pseudo-linear performance measures. however neither algorithms analyses transfer directly pseudo-concave setting. because exploiting pseudo-linearity performance measure stamp able convert problem sequence cost-weighted optimization problems simple solve. convenience absent mentioned above even creation valuation function scan still solve possibly nested concave minimization problem invoking nemsis procedure inner problem. proof technique used analyzing also makes heavy pseudo-linearity. convergence proof hand general guarantees linear convergence rate. data sets used following benchmark data sets machine learning repository ijcnn covertype adult letters cod-rna. also used following three real-world data sets cheminformatics drug discovery data challenge data breast cancer detection data pertaining protein-protein interaction prediction task case used data training remaining testing. methods compares proposed nemsis scan algorithms state-of-the-art one-pass minibatch stochastic gradient method svmperf technique techniques capable optimizing structural surrogates arbitrary performance measures modiﬁed implementations suitably adapt performance measures considered here. nemsis scan implementations used hinge-based concave surrogate. non-surrogate approaches also experimented variant nemsis scan algorithms dual updates computed using original count based values rather surrogate reward functions. refer version nemsis-ns. also developed similar version scan called scan-ns level estimation performed using tpr/tnr values. empirically observed non-surrogate versions algorithms offer superior stable performance surrogate versions. parameters parameters including step sizes upper bounds reward functions regularization parameters projection radii tuned values using held-out portion training treated validation set. step sizes base step length tuned step lengths mimic parameter setting setting buffer size number passes comparison negkld ﬁrst compare nemsis-ns nemsis baselines svmperf several data sets negative measure. results presented figure clear proposed algorithms comparable performance signiﬁcantly faster rate convergence. since svmperf batch/offline method important clarify compared online methods. case timers embedded inside svmperf code regular intervals performance current model vector evaluated. clear svmperf signiﬁcantly slower behavior quite erratic. proposed methods often faster pmb. three four data sets nemsis-ns achieves faster rate convergence compared nemsis. comparison bakld also used bakld performance measure evaluate trade-off nemsis offers quantiﬁcation classiﬁcation performance. weighting parameter bakld denoted cweight avoid confusion varied across grid; value nemsis used optimize bakld performance noted separately. results presented figure three data sets notice sweet spot tasks i.e. quantiﬁcation classiﬁcation simultaneously good performance. comparison varying class proportions next evaluated robustness algorithms across data sets varying different class proportions figure plot positive proposed baseline methods diverse datasets. again clear nemsis family algorithms better performance compared baselines demonstrating versatility across range class distributions. comparison varying drift next test performance nemsis family methods drifts class proportions train test sets. case retain original class proportion train vary class proportions test suitably sampling original positive negative test instances. included svmperf experiments took inordinately long time complete. seen figure adult letter data nemsis family fairly robust small class drifts. expected class proportions change large amount test algorithms perform poorly. comparison hybrid performance measures finally tested methods optimizing composite performance measures strike nuanced trade-off quantiﬁcation classiﬁcation performance. figures contains results nemsis methods optimizing q-measure figure contains results scan-ns optimizing cqreward proposed methods often signiﬁcantly better baseline terms accuracy running time. quantiﬁcation task estimating class prevalence problem settings subject distribution drift emerged important problem machine learning data mining. discussion justiﬁed necessity design algorithms exclusively solve quantiﬁcation task special emphasis performance measures kullbackleibler divergence considered facto standard literature. paper proposed family algorithms nemsis scan non-surrogate versions address online quantiﬁcation problem. abstracting negkld hybrid performance measures nested concave pseudo concave functions designed provably correct efﬁcient algorithms optimizing performance measures online stochastic setting. validated algorithms several data sets varying conditions including class imbalance distribution drift. proposed algorithms demonstrate ability jointly optimize quantiﬁcation classiﬁcation tasks. best knowledge ﬁrst work directly addresses online quantiﬁcation problem such opens novel application areas. authors thank anonymous reviewers helpful comments. thanks deep singh daljeet kaur faculty fellowship research-i foundation kanpur support. acknowledges support pixel s.r.l. sychip inc. murata japan.", "year": 2016}