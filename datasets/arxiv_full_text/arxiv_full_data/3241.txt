{"title": "Scalable $k$-NN graph construction", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "The $k$-NN graph has played a central role in increasingly popular data-driven techniques for various learning and vision tasks; yet, finding an efficient and effective way to construct $k$-NN graphs remains a challenge, especially for large-scale high-dimensional data. In this paper, we propose a new approach to construct approximate $k$-NN graphs with emphasis in: efficiency and accuracy. We hierarchically and randomly divide the data points into subsets and build an exact neighborhood graph over each subset, achieving a base approximate neighborhood graph; we then repeat this process for several times to generate multiple neighborhood graphs, which are combined to yield a more accurate approximate neighborhood graph. Furthermore, we propose a neighborhood propagation scheme to further enhance the accuracy. We show both theoretical and empirical accuracy and efficiency of our approach to $k$-NN graph construction and demonstrate significant speed-up in dealing with large scale visual data.", "text": "ﬁelds machine learning computer vision data mining bioinformatics internet search witnessed great success applying data-driven techniques neighborhood graphs widely adopted. examples include image object organization object retrieval face synthesis shape retrieval approximate nearest neighbor search manifold learning components hence suitable many situations contrast k-nn graphs shown especially useful practice therefore paper focuses problem constructing k-nn naive solution neighborhood graph construction exhaustively comparing pairs points takes time denoting number data points denoting dimensionality. prohibitively slow unsuitable large-scale applications. early research efforts conducted construct exact k-nn graphs however time complexity methods either grows exponentially respect dimensionality grows super-linearly respect number makes impractical large scale high-dimensional problems. nowadays research efforts construction generally simpler nearest neighbor search search algorithm solve neighborhood graph construction vice versa words neighborhood graph construcand-conquer methodology process consists stages recursively divide data small subsets merge neighborhood graphs subsets. using overlapped divisions paper propose approach construct approximate k-nn graphs emphasis efﬁciency accuracy justify approach theory empirical experiments large scale vision data. ﬁrst present multiple random divide-and-conquer approach construct approximate neighborhood graph. randomly hierarchically partition data points subsets neighboring peated several times increase chance neighboring points connected least random partition. multiple random partitions viewed exploit overlaps also utilized furthermore propose neighborhood propagation scheme i.e. propagating local approximate neighborhoods wider area achieve accurate neighborhood graph higher speed. observe several repetitions random partitions neighboring relationships generated random partition already appeared previous random partitions discovered approximate neighborhoods true neighboring points instead relatively large overlaps. points suggest propagate local neighborhood wider range expecting higher speed connecting neighboring points. experimental results show multiple division scheme superior existing neighborhood construction algorithms terms speed accuracy neighborhood propagation construction exact k-nn graphs extensively studied literature number algorithms proposed avoid high complexity divide-and-conquer method taking time presented algorithm expected time introduced worst-case time algorithm proposed time complexity methods grows exponentially data dimension makes quite inapplicable high-dimensional problems. recently proposed method empirically requires distance calculations low-dimensional cases calculations high-dimensional cases. algorithm works well low-dimensional data becomes inefﬁcient high-dimensional cases. spite rich previous literature efﬁcient algorithm high-dimensional straightforward solution constructing approximate k-nn graph apply nearest neighbor search algorithm indexing structure usually ﬁrst made organize data points search method based structure adopted handle upcoming queries. representative examples search methods partition-tree-based methods kd-trees random projection trees hashing based methods locality sensitive hashing however pointed existing search methods generally suffer unfavorable trade-off complexity indexing structure accuracy search. moveover search methods generally ignore fact graph construction query must data points. consequence unnecessary approximate neighborhood graph construction methods proposed recently following divide-and-conquer methodology approach divides data points three overlapped subsets predeﬁned overlapping ratio unites subgraphs constructed subsets together followed reﬁnement step inspecting neighbors neighbors divides data non-overlapped subsets additionally samples another subset overlaps subsets ﬁnally merges three graphs together. accuracy methods rely much difﬁcult balance efﬁciency accuracy especially large scale problems. approach proposed related approach clearly different adopts randomly rotated algorithm using morton ordering method works well low-dimensional data. presented incremental building neighborhood graphs algorithm mainly relative neighborhood graph inefﬁcient large scale problems. gave extensive theoretical analysis choose proper k-nn graphs better performance real applications problem different multiple random division scheme exploited problems. random kd-trees constructed boost indexing efﬁciency. random forest developed build ensemble classiﬁer consists many decision trees improve classiﬁcation performance. differently paper proposes adopt multiple random division technique build neighborhood graph. ﬁrst present multiple random divide-and-conquer approach build base approximate neighborhood graphs ensemble together achieve approximate neighborhood graph. introduce neighborhood propagation technique propagate local neighborhoods wider range order achieve accurate neighborhood graph higher speed. base approximate neighborhood graph unconnected graph subgraph corresponds group possibly neighboring points. words base approximate neighborhood graph corresponds partitioning data points. adopt divide-and-conquer methodology recursively partition points small subsets forming random partition tree. make nearby points subset hyperplanes hyperspheres partition data set. speciﬁcally divide point nonoverlapped subsets recursively conduct division process subsets cardinality subset smaller ﬁxed value brute-force manner adopted build neighborhood graph subset points. different single random division yields base approximate neighborhood graph containing serial isolated subgraphs unable connect point neighboring points lying different subgraphs. illustrated fig. neighborhood identiﬁed division represented ellipse. denote points neighborhood m-th division union multiple sets neighboring points written increasing number random divisions union cover true neighbors represented small green points fig. quality combined neighborhood graph improved. denote base approximate neighborhood graphs resulted division adjacent list adjm point combination achieved efﬁciently uniting adjacent lists {adjm}m random divisions. let’s consider situation random divisions illustrated fig. fig. fig. ﬁrst division yields isolated subsets second division yields isolated subsets fig. overlaps overlaps implies serves bridge connect isolated subgraphs constructed also serves role. turn also regarded roles connect subgraphs implementation data points divided many parts many times sufﬁcient overlaps make better connections among subgraphs. let’s consider point again. discussed before increasing number random divisions union becomes larger covers true neighbors although increases accuracy neighborhood graph also makes contribution random division smaller. words progress toward true neighborhood graph becomes slower becomes larger. validated experimental result shown fig. effective rate m-th division deﬁned hand suppose point previously-identiﬁed neighborhood common true neighboring point increase number random divisions probability identiﬁed neighboring point increases i.e. becomes larger. suggests neighboring point accessing neighbor expanding access previously-identiﬁed neighborhood conduct accesses gradually propagating neighborhoods best-ﬁrst manner. speciﬁcally ﬁrst expand neighborhood push best-ﬁrst strategy makes true neighbors ﬁrst discovered higher probability. propagation process stops queue empty maximum number visited points reached. process following present theoretic analysis show random divisions neighborhood propagation work well complexity analysis approach. detailed proofs found section lemma suppose random hyperplane partitions data points certain point true neighbors probability subset. single random partition depth tree. tree discovered xi’s neighbor probability random partitions discovered xi’s neighbor probability probability nk+)l−) let’s compare probabilities discovering true nearest neighbor partition tree neighborhood propagation. condition discovered neighbors previous discover neighboring relationship trees conclusions probability next tree stay grows; probability discover relationship propagation keeps increasing; probability next tree smaller propagation theorem suppose min{pij|hxi exact k-nn graph. random divisions ﬁrst-order neighborhood propagation true k-nn point discovered least probability assumption least neighboring point. following discusses time complexity. approach takes time multiple random divisions denotes number divisions time neighborhood propagation denotes maximum number visited points. large scale high-dimensional problem relatively small whole complexity multiple random divisions combined method written algorithm presented denoted virmajokif complexity reported algorithms named glue overlap take time time overlapping ratio suggested comparison theoretic time complexity approach smaller methods. random division. implementation chooses random principal directions perform random divisions make diameter subset small enough. theoretically shown principalcipal directions obtained using principal component analysis generate random principal directions rather computing principle direction whole subset points compute principal direction points randomly sampled subset. implementation principle direction computed lanczos algorithm compared space partitioning ways e.g. speedup. process multiple random divisions neighborhood propagation distances between pair points computed once would cost much especially highdistances computed. requiring distance pair points check distance evaluated hash table. time overhead operations hash table cost re-computation cost data sets. demonstrate proposed neighborhood graph construction algorithm sift features gist features. sift features collected caltech data recognition benchmark images extract maximally stable extremal regions image compute -dimensional sift feature mser. image randomly sample sift features data set. besides conduct experiments tinyimage imagenet data justify approach. similar global gist descriptor represent image dimensional vector describing texture within localized grid cells. dimension gist descriptor evaluation scheme. adopt accuracy measurement evaluate quality approximate graph. accuracy approximate k-nn graph deﬁned accuracy |e∩e| denotes direct edges graph denotes cardinality set. accuracy within range higher accuracy means better graph. exact neighborhood graph computed brute-force method running time four data sets given tab. report results approaches based multiple random divide-and-conquer combination subsequently followed neighborhood propagation. recursive division repeated till cardinality subset smaller neighborhood propagation triggered effective rate m-th random division deﬁned less threshold help hash table effective rate easily calculated affect efﬁciency algorithm. algorithms adjusted parameters make performance good possible. also present performance building neighborhood graphs searching random kd-trees performs better partition trees construction kd-trees cheap. algorithms .ghz desktop single thread. results. performance comparison shown fig. horizontal axis corresponds construction time vertical axis corresponds accuracy. test algorithms four data sets described before compute accuracy based different numbers neighbors denoted results clearly superiority algorithms algorithms. caltech approach multiple random divisions achieve accuracy graph least three times faster algorithms applying neighborhood propagation approach least times faster. targeted accuracy becomes higher number neighbors becomes larger turn efﬁcient kd-tree search still least three times worse multiple random division method required accuracy adopting neighborhood propagation superiority becomes even signiﬁcant dimensional cases. tinyimage approach achieves accuracy seconds within time accuracy methods comparing approach brute-force method approach achieves accuracy using brute-force time -dimensional sift features achieves accuracy using brute-force time -dimensional gist features shows constructing approximate neighborhood graph indeed saves signiﬁcant amount time face images organization. ﬁrst present application adopts neighborhood-based distance measure organize face images. rank-order distance shown good evaluate distance faces rank-order distance face images computed comparing neighboring faces requires ﬁrst constructing k-nn face graph. data face images experiment consists labeled face dataset labeled images distracter face dataset collected web. ﬁrst compute graph using approach. conduct neighborhood propagation step obtain k-nn graph rank-order distances evaluation adopt three metrics local label consistency ratio precision ndcg local label consistency ratio aims evaluate label face dominant among labels neighboring faces evaluated number faces label larger otherwise precision computed proportion faces among neighbors ndcg normalized discounted cumulative gain neighbors widely used various ranking tasks. tab. shows comparison average local label consistency ratio fig. shows comparisons average precision ndcg observations. hand neighborhood graph powerful useful better distance measure face images organization. hand performances almost same shows k-nn graph constructed approach accurate. object discovery. discovering objects large unlabeled image collections challenging problem show proposed approach effectively construct matching graph coarse similarity measure fast random divisions similarity measure accurate neighborhood propdata consists labeled images oxford data distracter images downloaded flickr. sift features extracted image. build vocabularies respectively visual words. image represented features k-dimensional histogram visual words indicates word occurrences weighted tf-idf bag-of-words representation visual words attaching spatial position word. dimensional features used build random divisions fast neighborhood graph construction high dimensional features spatial information used compute image matching spatial veriﬁcation accurate neighborhood propagation yielding matching graph. afﬁnity construct graph over-segments nodes deﬁne similarity over-segments proportion images over-segments -reciprocal nearest neighbors fast computed matching graph. ﬁnally apply afﬁnity propagation over-segment graph obtain ﬁnal clustering result. evaluate performance labeled object group cluster containing images object compute precision recall f-measure cluster. result given tab. objects discovered f-measure show theoretical empirical accuracy efﬁciency approach. ongoing work investigating learning scheme automatically trigger neighborhood propagation. lemma suppose random hyperplane partitions data points certain point true neighbors probability side. single random partition tree discovered xi’s neighbor probability depth tree. random partitions discovered xi’s neighbor probability used build binary partition probability also easily computed. example using entry satisﬁes gaussian distribution uniform random variable demonstrated hab) euclidean distance denotes probability density function absolute value gaussian distribution. probability larger dij. bi-partition data according median larger dij. case cosine distance random projection used build random partition tree computed proof. according lemma. previous random partition trees probability fail discover neighbors l-th tree neighboring relationship lemma proved basic multiplication discovered probability principle. theorem suppose min{pij|hxi exact k-nn graph data set. random divisions ﬁrst-order neighborhood propagation true k-nn point discovered least probability assumption least neighboring point proof. ways neighbor. first discover least partition according lemma. second fail discover trees probability probability discovers partition trees ﬁrst-order neighborhood propagation", "year": 2013}