{"title": "Two Projection Pursuit Algorithms for Machine Learning under  Non-Stationarity", "tag": ["cs.LG", "cs.AI"], "abstract": "This thesis derives, tests and applies two linear projection algorithms for machine learning under non-stationarity. The first finds a direction in a linear space upon which a data set is maximally non-stationary. The second aims to robustify two-way classification against non-stationarity. The algorithm is tested on a key application scenario, namely Brain Computer Interfacing.", "text": "grateful professor k.-r.müller provided opportunity gain research experience ida/machine learning laboratory addition employing without assistance would able complete degree bccn continue level digniﬁed manner. greatly appreciated treated equal member research group despite master’s student’s status assigned research tasks genuine scientiﬁc import interest. addition heartily thank franz kiraly paul bünau frank meinecke wojciech samek imparting knowledge machine learning neuroscience ﬁrst years berlin enthusiasm topics pursued thesis. chapters thesis addition result collaboration paul frank wojciech. addition would like thank franz kiraly wojciech samek again well alex schlegel danny pankin comments manuscript. moreover thank remaining numerous members machine learning group bccn discussed ideas relating neuroscience machine learning computer science mathematics. addition thank andrea gerdes vanessa casagrande margret franke assisting demanding task completing degree within allotted year period. finally would like thank parents continued support transition made ﬁeld girlfriend michaela helping lose sight idealism originally brought ﬁeld. introduction feature extraction change-point detection stationary subspace analysis finding non-stationary sources relationship statistical testing simulations synthetic data generation performance measure single linkage clustering symmetrized divergence measure weighted cusum changes variance kohlmorgen/lemm algorithm application fault monitoring setup conclusion description experiments results discussion experiments layout figures discussion results analysis datasets using group wise discussion analysis outlook non-stationarity stochastic process deﬁned loosely variability probability distribution time. conversely stationarity stochastic process corresponds constancy distribution. machine learning typical tasks include regression classiﬁcation system identiﬁcation. regression classiﬁcation guarantee generalization training test made assumption underlying process yielding training test sets non-stationary. thus quantiﬁcation nonstationarity algorithms choosing features stationary indispensable tasks. hand system identiﬁcation stationary maximally non-stationary subsystem often carries important signiﬁcance terms primitives domain consideration example neuroscience identiﬁcation subsystem dynamics synaptic weights neural network weights consisting subset weights whose distribution maximally non-stationary learning vital interest analysis neural substrate underlying learning process. classical literature machine learning statistical learning theory assumes samples used training parameters instance classiﬁers drawn single probability distribution rather possibly non-stationary process time series data taken stationary time. thus approaches learning relax stationarity assumption sparse present time within machine learning literature. particular ﬁrst algorithm stationary subspace analysis obtaining linear stationary projection data published recently since algorithms published reﬁne computationally assumptions develop maximum likelihood approach derive algebraic algorithm solution addition linear algorithms published applications example scsp brain computer interfacing however non-stationarity machine learning remains largely open problem instance general technique classiﬁcation non-stationarity proposed non-adaptive. otherwise covariate shift problem studied detail instance methods seek perform robust classiﬁcation non-stationarity remain fully investigated. present thesis’s contribution twofold ﬁrstly chapter present method based stationary subspace analysis addresses system identiﬁcation task described above namely identifying maximally non-stationary subsystem. subsequently apply method speciﬁc machine learning task namely change point detection. particular show using method based prior feature extraction step change point detection boosts performance three representative change point detection algorithms synthetic data data adapted real world recordings. note chapter consists part joint work present author authors following submitted paper chapter adapted version. secondly chapter address two-fold classiﬁcation problem non-stationarity propose method adapting commonly used method two-fold classiﬁcation namely linear discriminant analysis setting call resulting algorithm stationary linear discriminant analysis slda short. investigate properties performance algorithm simulated data data recorded brain computer interface experiments. finally tested algorithm perform rigorous investigation results obtained means statistical testing comparison base-line methods. please note addition chapter includes joint work wojciech samek. change point detection task appears broad range applications biomedical signal processing speech recognition industrial process monitoring fault state detection econometrics goal change point detection time points time series changes macroscopic state another. result time series decomposed segments similar behavior. change point detection based ﬁnding changes properties data moments spectral properties temporal structure changes w.r.t. certain patterns choice aspects depends particular application domain statistical type changes aims detect. informative uninformative directions change point detection. left panel shows figure observed bivariate time series pronounced changes visible. middle panel shows underlying sources exhibits clearly visible changes. right panel stationary sources much higher signal power informative non-stationary sources thus masks presence change-points observed data. empirical distributions windows time series estimating comparing probability densities diﬃcult statistical problem particularly high dimensions. often however many directions many cases high dimensional signal space uninformative change point detection exists subspace distribution data remains constant time subspace irrelevant change point detection increases overall dimensionality. moreover stationary components high signal power make change points invisible observer also detection algorithms. example change points visible time series depicted left panel figure even though exists direction two-dimensional signal space clearly shows change-points seen middle panel. however non-stationary contribution visible observed signal relatively power example also observe suﬃce select channels individually neither appears informative. fact many application domains biomedical engineering geophysical data analysis plausible data generated mixture underlying sources cannot measure directly. chapter show extract useful features change point detection ﬁnding non-stationary directions using variant stationary subspace analysis even though exists wide range feature extraction methods classiﬁcation regression date specialized procedure feature extraction general signal processing proposed change point detection. controlled simulations synthetic data show three representative change point detection algorithms accuracy signiﬁcantly increased prior feature extraction step particular data high dimensional. eﬀect consistent various numbers dimensions strengths change-points. application fault monitoring ground truth available show proposed feature extraction improves performance leads dimensionality reduction desired state changes clearly visible. moreover also show determine correct dimensionality informative subspace. remainder chapter organized follows. next section introduce feature extraction method based extension stationary subspace analysis. section contains results simulations section present application fault monitoring. conclusions outlined section feature extraction high-dimensional data shown useful improving performance subsequent learning algorithms derived features also understanding high-dimensional complex physical systems. many application areas computer vision bioinformatics text classiﬁcation deﬁning useful features fact main step towards successful machine learning. general feature extraction methods classiﬁcation regression tasks based maximizing mutual information features target explaining given percentage variance dataset choosing features maximize margin classes selecting informative subsets variables enumerative search however change-point detection dedicated feature extraction proposed unlike classical supervised feature selection target variable allows measure informativeness feature changepoint detection cannot tell whether feature elicits changes detect since usually ground truth available even feature extraction feasible following principle useful feature exhibit signiﬁcant distributional changes time. reducing dimensionality pre-processing step particularly beneﬁcial change-point detection task algorithms either explicitly implicitly make approximations probability densities directly compute divergence measure based summary statistics mean covariance segments time series hard problems whose sample complexities grow exponentially number dimensions. seen example presented figure selecting channels individually helpful lead suboptimal features. data non-stationary overall despite fact dimension seems stationary. moreover single non-stationary source expressed across large number channels. therefore sensible estimate linear projection data contains much information relating change-points possible. chapter demonstrate ﬁnding projection non-stationary directions using variant stationary subspace analysis signiﬁcantly increases performance change-point detection algorithms. remainder section ﬁrst review algorithm show extend towards ﬁnding non-stationary directions. show approach corresponds ﬁnding projection likely non-stationary terms statistical hypothesis test. stationary sources non-stationary sources unknown time-constant invertible mixing matrix. spaces spanned columns mixing matrix called sn-space respectively. note contrast independent component analysis independence assumption sources invert mixing model given samples mixed sources i.e. want estimate demixing matrix separates stationary non-stationary sources. applying time series yields estimated stationary non-stationary sources respectively submatrices rds×d estimated demixing matrix project estimated stationary non-stationary sources called s-projection n-projection respectively. estimated mixing matrix inverse estimated demixing matrix ˆb−. stationary resp. non-stationary nature sources unchanged. addition separation sn-sources unique adding stationary components non-stationary source leaves source non-stationary whereas converse true. n-projection identiﬁed arbitrary contributions stationary sources. hence cannot recover true n-sources true s-sources conversely identify true n-space true s-space. however order extract features changepoint detection recover true non-stationary sources instead non-stationary ones. algorithm depends deﬁnition stationarity s-projection aims satisfy. algorithms time series considered stationary mean covariance constant time i.e. pairs time points variant weak stationarity whereby time structure taken account. following concept stationarity algorithm ﬁnds s-projection minimizes diﬀerence ﬁrst moments estimated s-sources across epochs time series. thus samples divided non-overlapping epochs deﬁned index sets epoch mean covariance matrices estimated diﬀerence mean covariance matrix epochs measured using kullback-leibler divergence gaussians. objective function information theoretic diﬀerence epoch average epoch. since s-sources determined arbitrary linear transformation since global translation data change diﬀerence epoch distributions without loss generality data centered whitened implies assume average epoch’s mean covariance matrix order extract useful features change-point detection would like projection non-stationary sources. however algorithms merely estimate projection stationary sources choose projection non-stationary sources orthogonal found s-projection means stationary contributions projected estimated n-sources. case covariance sources orthogonal s-sources constant time implies information relating non-stationarity thereby lost following protocol obtain sources containing non-stationarity. constancy however always hold non-stationarity well reside changing covariance sn-sources. intuitively order directions lose information relating non-stationarity contained data need propose diﬀerent method simply taking orthogonal complement stationary projection. problem remains whether frame sensible deﬁning non-stationary sources independently deﬁnition stationary sources since non-stationary source remains non-stationary stationary source imposed onto source also clear sensible deﬁne non-stationarity data non-circularly guarantees superposition stationary noise onto non-stationary direction yields less non-stationary direction thus allows recover orthogonal case mentioned above; neither clear case information theoretic point view. therefore simply take deﬁnition non-stationary sources sources maximize loss function. deﬁnition makes trivially non-stationary sources unique; thus working deﬁnition non-stationarity taking measure optimized various independent justiﬁcations measure explore deeply here include fact minimum loss function consistent estimator stationary projection loss information theoretic sense independent parameterizations probability distribution underlying vector space yields positive results task hand thus rationale behind following approach thus taking loss deﬁnition non-stationarity non-stationary sources means optimizing n-projection instead s-projection. turn optimization problem ﬁrst analyze situation formally order develop intuition diﬀerence maximizing non-stationarity taking orthogonal complement stationary projection. figure left panel shows epoch covariance matrices non-stationarity conﬁned changes variance along direction hence non-stationary projection orthogonal true stationary projection case situation depicted right panel here covariance dimensions changes nonstationary projection non-stationary orthogonal complement true s-projection. consider ﬁrst simple example comprising stationary non-stationary source corresponding normalized basis vectors respectively angle spaces i.e. asan. consider arbitrary pair epochs projection maximizes diﬀerence mean variance univariate random variable represents stationary source univariate random variables model non-stationary sources epochs respectively. without loss generality assume true s-projection normalized order determine relationship true s-projection non-stationary projection write terms maximal ˆbnan i.e. orthogonal thus respect diﬀerence mean choosing n-projection orthogonal s-projection always optimal irrespective type distribution change epochs. clearly change covariance sn-sources epochs i.e. ∆σsn diﬀerence maximized left panel figure example. however covariance sn-sources vary i.e. |∆σsn| projection longer non-stationary. this consider derivative respect thus seen example clearly maximizing non-stationarity equivalent maximizing variance diﬀerence seen equivalent taking orthogonal complement stationary projection. course maximizing variance diﬀerences general appropriate deﬁnition non-stationarity loss comes play general measure. thus order projection non-stationary sources need maximize nonstationarity estimated n-sources. propose maximizing objective function n-projection section show maximizing objective function non-stationary sources understood statistical testing point-of-view also maximizes test statistic minimizes p-value rejecting null hypothesis estimated directions stationary. provide alternative rationale loss function maximize. addition interpretation loss function terms testing allow detect number actually non-stationary directions dataset. precisely maximize test statistic thereby minimizes p-value statistical hypothesis test compares models data null hypothesis epoch follows standard normal distribution alternative hypothesis epoch gaussian distributed individual mean covariance matrix. random variables modeling distribution data epochs. formally hypothesis written follows words statistical test tells whether reject simple model favor complex model decision based value test statistics whose distribution known null hypothesis since special case since parameter estimates obtained maximum likelihood likelihood ratio test statistic ratio likelihood data parameters maximum likelihood estimates. maximum likelihood estimates mean covariance matrices estimated n-sources respectively. probability density function multivariate gaussian distribution. likelihood ratio test statistic given simplicity right hand term testing hypothesis whether data generated normal distribution constant covariance mean assume w.l.o.g. resp. white application fault monitoring p-value test furnishes useful indicator number informative directions change point detection. speciﬁcally obtain upper bound optimal number directions change point detection stationary sources increasing number stationary sources test returns projection signiﬁcantly non-stationary. sources safely removed without loss informativeness change point detection. removing directions sacriﬁce information change point detection; hand depending particular data removing additional directions lead increases performance result reduced dimensionality. notice here procedure obtaining upper bound uses standard algorithm whereas ﬁnal preprocessing step change point detection optimize non-stationarity. demonstrate feasibility using test statistic select parameter display figure p-values obtained using ﬁxed value simulated data’s dimensionality number stationary sources ranging chosen parameter ranging conﬁdence level rejection null hypothesis projected data stationary returns correct average cases. simulation dataset synthesized length epochs. possible parameter setting stationary projection computed. finally value test statistic together p−value computed estimated stationary sources. dataset described section figure average p-values obtained realizations dataset setting actual number dimensions y-axis displays actual number x−axis displays value parameter used compute stationary projection using ssa. shows decision made conﬁdence level. displays choice made using decision rule choosing parameter occurs often. results show method picks correct parameter averages simulations thus represents feasible procedure choosing number non-stationary sources table overview simulations performed corresponding ﬁgures reporting results. tick denotes corresponding parameter varied experiment. cross denotes corresponding parameter kept ﬁxed. section demonstrate ability enhance segmentation performance three changepoint detection algorithms synthetic data setup. algorithms single linkage clustering divergence uses mean covariance test statistics cusum uses sequence hypothesis tests kohlmorgen/lemm using kernel density measure hidden markov model. segmentation algorithm compare performance baseline case dataset segmented without preprocessing case data preprocessed projecting random subspace case dataset preprocessed using ssa. compare performance respect following schemes parameter variation change-point detection algorithms test slcd kohlmorgen/lemm three parameter variation schemes tested. cusum second scheme apply method univariate method. setup realization dataset perform segmentation dataset estimated non-stationary sources preprocessing dataset dimensional random projection dataset. random projection acts comparison measure accuracy ssa-estimated non-stationary sources segmentation purposes. synthetic data evaluate performance change point detection methods generated linear mixture stationary non-stationary sources. data generated epoch-wise epoch ﬁxed length dataset consists concatenation epochs. stationary sources distributed normally epoch according source signals distributed according active model epoch; active model gaussian distributions covariance diagonal matrix whose eigenvalues chosen random thus covariances corresponding log-spaced values markov chain chosen way. transition models consecutive epochs follows experiments evaluate algorithms based estimation area curves across realizations dataset. true positive rate false positive rate deﬁned respect ﬁxed epochs constitute synthetic dataset; change-point occur epochs ﬁxed length. change-point algorithms test reports changes respect division epochs synthetic dataset thus well deﬁned. provides information relating range fpr. signal detection trade achieved depends operational constraints cancer diagnosis procedures must achieve high perhaps cost higher desirable fpr. network intrusion detection instance need compromise given computational demands high fpr. order assess detection performance across requirements provides informative measure integrates possible trade oﬀs. speciﬁcally algorithm accompanied parameter controls trade fpr. slcd number clusters cusum threshold likelihood ratio kohlmorgen/lemm parameter controlling readily state assigned model. case vary obtain aucs. single linkage clustering symmetrized distance measure simple algorithm change point detection however advantage eﬃciency segmentation based parameter independent distance matrix particular segmentation based single linkage clustering computes distance measure based covariance mean time windows estimate occurrence change-points algorithm consists following three steps based dissimilarity matrix single linkage clustering returns assignment epochs clusters change-point occurs neighbouring epochs belong cluster. figure illustration case signiﬁcantly improves single linkage clustering divergence panel displays true decomposition stationary non-stationary sources true change-points marked. middle panel displays change-points slcd ﬁnds entire data clearly change-points left undetected. bottom panel displays change-points found slcd estimated non-stationary sources. results simulations varying numbers non-stationary sources dataset channels shown figure ﬁrst panel. degree changes visible lower preprocessing signiﬁcantly outperforms baseline method even small number irrelevant stationary sources. results simulations varying number stationary dimensions non-stationary dimensions displayed figure second panel. small performance baseline preprocessing similar ssa’s performance robust respect addition higher numbers stationary sources i.e. noise directions. segmentations produced using preprocessing continue carry information relating change-points whereas baseline’s approaches corresponds accuracy randomly chosen segmentations. results simulations varying power non-stationary sources displayed figure third panel. performance baseline preprocessing improves increasing power change eﬀect evident lower preprocessing baseline. illustration case preprocessing signiﬁcantly outperforms baseline displayed figure estimated non-stationary sources exhibit clearer illustration change-points full dataset corresponding segmentation performances reﬂect fact. figure results simulations single linkage clustering symmetrized divergence left panel displays results ﬁxed dimensionality time series varying number stationary sources ..the middle panel displays results ﬁxed number non-stationary sources varying number stationary sources power change right panel displays results ﬁxed varying power change non-stationary sources. displays results terms area curve computed section error bars extend percentiles. statistical quality control cusum sequential analysis technique developed cusum widely used oldest methods change point detection; algorithm online method change point detection based series log-likelihood ratio tests. thus cusum algorithm detects change parameter process asymtotically optimal pre-change post-change parameters known case target value changing parameter unknown weighted cusum algorithm deﬁned direct extension cusum integrating parameter interval. following statistics constitutes likelihood ratios currently estimated parameter non-stationary process diﬀering target values integrated measure function serves weighting function possible target values changed parameter. principle algorithm thus applied multi-dimensional data. however extension cusum algorithm higher dimensions non-trivial integrating possible values covariance computationally expensive also various parameterizations lead likelihood function. given this test eﬀectiveness algorithm computing one-dimensional segmentations. particular compare segmentation performed dimensional projection chosen best segmentation individual dimensions respect hit-rate trial. accordance choose comprise ﬁxed uniform interval containing possible values process’s variance. approximate integral evenly spaced values interval. approximate stopping time setting figure left panel results varying numbers stationary sources displayed. weighted cusum preprocessing signiﬁcantly outperforms baseline values number non-stationary sources values number stationary sources. figure right panel results changes power change ergodic sections displayed outperforms baseline except values power level change detection schemes fail. simulations show represents method choosing dimensional subspace render uni-dimensional segmentation methods applicable higher dimensional datasets resulting segmentation method dimensional derived non-stationary source simpler parametrize eﬃcient. true dimensionality non-stationary part information loss observed. kohlmorgen/lemm algorithm ﬂexible non-parametric multivariate method applied online oﬄine operation modes. distinctive kohlmorgen/lemm algorithm kernel density estimator rather simple summary statistic used estimate occurence changepoints. particular algorithm based standard kernel density estimator gaussian kernels estimation optimal segmentation based hidden markov model speciﬁcally estimate densities arbitrary epochs dataset gaussian kernels deﬁne distance measure epochs l-norm yielding figure results simulations cusum. left panel displays results ﬁxed number non-stationary sources ﬁxed power change varying number stationary sources. right panel displays results ﬁxed varying power change non-stationary sources. displays results terms area curve computed section error bars extend percentiles. here corresponds point epoch point epoch formula distance measure derived analytically based expression kernels used density estimation simpliﬁes computations calculating densities explicitly details). ﬁnal segmentation based distance matrix generated epochs calculated respect distance measure weighted cusum possible deﬁne algorithms whose sensitivity distributional changes reporting change-points related value parameter controls probability transitions states ﬁtting hidden markov model. however shown case change-points known also derive algorithm returns exactly number change-points simulations evaluate performance ﬁrst variant full range parameters obtain curve. addition choose parameter according rule thumb given sets proportional mean distance data point nearest neighbours dimensionality data evaluated sample set. exact implementation test based papers details follows preprocessing improves segmentation obtained using kohlmorgen/lemm algorithm three schemes parameter variation dataset. particular area varying ﬁxed displayed figure ﬁrst panel area varying ﬁxed displayed figure second panel area varying power change non-stationary sources ﬁxed displayed figure third panel ranging increments additional interest varying ﬁxed performance segmentation preprocessing superior higher values implies improvement change point detection kohlmorgen/lemm algorithm reduction dimensionality informative estimated n-sources outweighs diﬃculty problem estimating n-sources presence large number noise dimensions. figure results simulations kohlmorgen/lemm.the left panel displays results ﬁxed dimensionality time series varying number stationary sources ﬁxed power change ..the middle panel displays results ﬁxed number non-stationary sources varying number stationary sources ﬁxed power change right panel displays results ﬁxed varying power change non-stationary sources. displays results terms area curve computed section error bars extend percentiles. section apply feature extraction technique fault monitoring. dataset consists multichannel measurements machine vibration. machine investigation pump driven electromotor. incoming shaft reduced speed delaying gear-combinations measurements repeated identical machines ﬁrst shows progressed pitting gears second machine virtually fault free. rotating speed driving shaft measured tachometer. pump data semi-synthetic insofar juxtapose non-temporally consecutive sections data pump conditions. sections data ﬁrst second machine spliced randomly together yield dataset time points seven channels. illustration dataset displayed figure figure pump dataset machine investigation pump driven electromotor. measurements made machine vibration seven sensors. data alternates conditions normal functionality pitting gears. preprocessed using division dataset equally sized epochs number stationary sources ranging dimensionality dataset subsequently kohlmorgen/lemm algorithm preprocessed data using window size separation datapoints non-overlapping epochs. parameter choice select parameter number stationary sources following scheme measure stationarity optimize given loss function equation compute estimated projection stationary sources using ﬁrst half data available computed loss function estimated stationary sources second half compared result values loss function obtained dataset obtained randomly permuting time axis. random permutation produce average approximately stationary sources regardless non-stationarity present estimated stationary sources addition measure information relating non-stationarity lost choosing number stationary sources deﬁne baseline-normalized integral stationary error follows here denotes loss function given equation original dataset stationary parameter measure random permutation dataset. using notation equation right hand side evaluated specifying free parameter motivation bnise random ﬂuctuations corresponding sample sizes even stationary dataset measured slightly non-stationary loss function. thus measure deviation loss dataset hand expected loss stationary dataset estimated using dataset shuﬄed time axis. diﬀerence normalized standard deviation losses estimated shuﬄes. results scheme segmentation given figure observe clearly visible diﬀerence expected loss function value small sample sizes loss function value present estimated stationary sources. similarly looking p-values observe reject hypothesis estimated s-sources stationary whereas higher values reject hypothesis. implies test eﬀectiveness scheme segmentation evaluated preprocessing possible values values obtained using parameter choices preprocessing compared baseline case displayed figure increase performance preprocessing robust measured values respect varying choices parameter long chosen note that although dataset hand exists information relating change-points frequency spectrum taken time information cannot used bring baseline method onto preprocessing ssa. display results figure comparison. here segmentation based -dimensional spectrogram evaluated individual channel dataset computed. best performance channels segmentation spectrograms lower worst performance achieved entire dataset without using spectral information without ssa. figure pump dataset schemes selecting parameter left measure bnise increasing values right value error function compared randomly generated data. bottom left performances various values bottom left p-values estimated s-sources. unsupervised segmentation identiﬁcation time series hard problem even univariate case received considerable attention science industry broad applicability ranging process control ﬁnance biomedical data analysis. high dimensional segmentation problems diﬀerent figure pump dataset segmentations computed using kohlmorgen/ lemm number change-points speciﬁed baseline corresponds segmentation without preprocessing. middle panel displays segmentation preprocessing. bottom panel displays real changepoints superimposed dataset. figure pump dataset performance spectograms computed individual channels datatset. spectrogram computed window length datapoints overlap datapoints. frequency band windows used compute timeseries size subsystems multivariate time series exhibit clearer informative signals segmentation others. shown beneﬁcial decompose overall system stationary non-stationary parts means non-stationary subsystem determine segmentation. generic approach yields excellent results simulations expect proposed dimensionality reduction useful wide range datasets task discarding irrelevant stationary information independent dataset-speciﬁc distribution within informative non-stationary subspace. moreover using preprocessing highly versatile combined subsequent segmentation method. applications made along lines present thesis eﬀective non-stationary part data visible mean covariances. method considered thus made applicable general datasets whose changes consist spectrum temporal domain data computing score function preprocessing step classiﬁcation widely studied task machine learning less well studied however classiﬁcation assumption training test sets diﬀer distribution. situation training test sets diﬀer arises naturally drawn non-stationary time series. particular classiﬁcation non-stationarity poses serious challenge instance design brain computer interfaces wider setting classiﬁcation assumption diﬀerence distribution test training domains without assumption drawn single time series studied example moreover online learning studied contribution murata following however assume training test distributions drawn single time series. particular focus classiﬁcation problem non-stationarity whereby class distributions epoch modeled gaussians diﬀering means. algorithm proposed non-stationarity literature aims robustify classiﬁcation non-stationarity setting. exactly present chapter. progress made however brain computer interfacing literature classiﬁcation non-stationarity. brain computer interfacing non-stationarity imposed artifacts learning related adaptation view improving classiﬁcation perfomance contributions present classiﬁcation based variance based adaptation based methods classiﬁcation non-stationarity. method presented adapts standard prior feature extraction step namely yield scsp i.e. stationary csp. functions assumption variance class distributions diﬀer projection data sought maximizes diﬀerence variance classes. linear discriminant subsequently trained data consisting variances within single trials data thus extracted variances features suitable classiﬁcation stationarity assumption assumption classes diﬀer covariance. scsp adapts methodology extracting features discriminative variance simultaneously stationary. approach shown alleviate extent problem non-stationarity bci. drawback scsp method speciﬁcally tailored extracting discriminative features class distributions diﬀer covariance thus scsp highly relevant classiﬁcation brain computer interfacing necessarily extend general classiﬁcation setting. particular scsp applicable class distributions diﬀer epoch necessarily covariance. address problem designing corresponding method below. suppose therefore instead assume features separable non-stationary; suppose further classes discriminable given probability without processing modeled mean covariance covariance classes identical linear discriminant analysis correct ansatz. propose method improves classiﬁcation non-stationary setting assume homoscedastic gaussian assumption valid within transient time window data non-stationary longer time frame; algorithm classiﬁcation direction obtained optimization loss function whose minimization rewards separability penalizes non-stationarity. test resulting algorithm section section data since represents case-study non-stationarity poses serious problem subsequently results rigorously analyzed tested. furthermore brieﬂy consider theoretical background classiﬁcation non-stationarity section question arises attempting design algorithms order extract discriminative features stationary? answer general. firstly explicit class information. argue choose epochs intelligently thus boost classiﬁcation equal numbers samples class used epoch thus fashion used look stationary directions. however approach fails whenever non-stationarity present leaves mixture distribution classes stationary. avoid respect deﬁciency brought unmodiﬁed application treating class separately deriving loss function stipulates distribution class joint distribution classes stationary zero loss function resulting algorithm aptly named group-wise ssa. approach however drawback that although diﬀerences classes treated non-stationarities guarantee class information preserved features thus derived. second drawback group-wise non-stationarities projected data detrimental classiﬁcation. figure illustrates diﬃculty using classiﬁcation figure illustrates difﬁculty using lda. ﬁrst case select stationary direction direction contains discriminative information. second case chooses discriminative direction training data however not-orthogonal non-stationary direction. assume non-stationarity present transition training testing poorly reﬂected training non-stationarity then provides highly suboptimal solution. thus. neither suﬃciently cater classiﬁcation non-stationarity uniﬁed manner. address deﬁcits section standard approach training classiﬁer linear discriminant analysis ﬁrst proposed fisher’s landmark article outputs hyperplane decision bias given classes modeled mean covariances given classes normally distributed identical figure ﬁgure provides illustration unsuitability classiﬁcation. ellipses correspond class covariances class epochs. y-direction corresponds direction chosen discriminative. x-direction non-stationary provides discriminative information. thus illustration demonstrates using discard information important classiﬁcation. here refer class means class covariances. decision rule sign. according ansatz computed direction maximizes ratio within class covariances class covariance ratio maximized thus optimizing projection maximize ration often fail robust classiﬁcation nonstationarity ratio take non-stationarity accound; however ratio incorporated error function described following section. figure ﬁgure provides illustration demonstrating unsuitability classiﬁcation non-stationarity. ellipses schematics covariances dimensional space class epochs. diagonal direction discriminative direction. y-direction stationary discriminative thus provide robust classiﬁcation direction solution. thus illustration demonstrates using choose non-stationary directions provide poor generalization error test set. propose loss function ﬁnds hyperplane which ideal scenario performs classiﬁcation robust non-stationarities. incorporate ratio loss function call method slda stands ‘stationary’. thus slda derived classiﬁcation performed direction given equation using bias described below. hyperplane decision boundary performs dimensional projection data makes decision based threshold thus interest data stationary possible dimensional projection. implies that although data non-stationary need single dimension discriminative stationary succeed task. using slda choose direction classiﬁcation using trade loss function based ratio used catering non-stationarity. class decision subsequently threshold lda. kullback-leibler divergence average epoch mean epoch gaussian estimated epoch data assumption normalized means covariances. parameters denote empirical mean covariance class evaluated training data. thus course ﬁxed cross-validation folds omitted direction simply selected direction maximizing function select practice small sensible parameter values chosen hand best chosen cross validation. simulations divided subgroups. ﬁrst subgroup described section designed check improvements observed using slda implicit regularization robustness instance gradient based optimization. details.) robustness refers ability estimator produce drastically diﬀerent results presence outliers details). order test whether regularization provided result gradient based optimization compare closed form maximum obtained gradient based optimization. call method obtaining gradlda. various simulations within regularization robustness analysis section examine exactly phenomena expected comparison performance gradlda lda. ﬁrst simulation data generated mixture sources approximately equal separation. second simulation uses dataset ﬁrst including outliers. third simulation tests performance using datasets directions data discriminative problem ﬁnding good discriminative direction harder. fourth simulation tests transition occurs moving dataset similar hard dataset similar simple. second group simulations tests performance slda choosing speciﬁc discriminative stationary directions within dataset non-stationary non-discriminative stationary directions also present. ﬁrst simulation uses three epochs dimensions whereas second simulation uses epochs dimensions. motivation section simply test accuracy slda picking stationary discriminative subspace without considering complicated issue whether implies higher generalization issue discuss section .... simulations stationary discriminative direction also non-stationary discriminative directions stationary non-discriminative directions. case test performance whether slda eﬀective picking stationary discriminative direction. third ﬁnal group simulations designed test diﬀerence test training distributions necessary order guarantee stationary discriminative direction chosen slda provides classiﬁcation direction produces higher test error direction chosen lda. ﬁrst simulation investigates diﬀerence slda ﬁrst simulations test presence regularization robustness eﬀects gradient based versus standard closed form lda. particular following simulations reported upon simple training data consists points drawn randomly distinct gaussians class test data points class. source classes generated class i.i.d. samples class dimensionality data test performance terms classiﬁcation error gradlda i.e. slda results displayed left panel figure outliers next frame data includes outliers added data used previous simulation outliers added sparsely random time points uniformly class; outliers generated samples gaussian mean times size original data added data class. before training data consists points class test data points class. dimensionality data results displayed right panel figure hard investigate diﬀerence gradlda follows variance class source. sources chosen class means diﬀering remaining source vary degree separation observed. simulation outliers included. results reported classiﬁcation errors displayed bottom left panel figure tapered diﬃculty ﬁnal simulation section retain data setup previous simulation hold separability separable source constant whilst increasing separability second third separable sources remaining source means separated before. results displayed bottom right panel figure cases data randomly mixed orthogonally. second simulations investigate simple non-stationary data setups evaluate performance terms angle normal vectors decision hyperplanes stationary directions within data sets. case epoch distributions gaussian variance source. ﬁrst source stationary separation classes. second source separation stationary; classes drawn gaussians unit variance means respectively. third dimension separation nonstationarity; data dimension consists epochs; outer epochs drawn normal distributions unit variance means respectively. inner epoch drawn gaussian distribution class mean randomly chosen uniform distribution class randomly chosen uniform distribution corresponds non-stationarity level. data subsequently mixed orthogonally. particular increase number dimensions investigate again simplicity nonstationarity mean. sources orthogonally consist samples classes. case source epoch samples drawn class. epoch source independent random numbers mean classes. remaining sources stationary. addition source generated separation means classes follows. epoch classes source distributed according gaussians data randomly mixed orthogonally. realization data compute directions wlda wslda measure angle projection second source ssep. epochs contains data points class. exact parameter values follows results realizations data value varying trade-oﬀ parameter non-stationarity level displayed terms subspace angle figure finally assess relationship non-stationarity observed training data nonstationarity observed transfer training test data terms classiﬁcation test error s-space small ﬁrst transfer simulations retain second data used fourth simulations using ﬁrst epochs training data comprising data points class. ﬁnal epoch generated held back test comprising data points class parameter ﬁxed varied addition parameter value ﬁxed. evaluate average classiﬁcation performance range s-space large second transfer simulations retain data previous simulation diﬀerence enlarge number signiﬁcantly separable stationary directions exchange snoise independent copy ssep additionally increase addition parameter reduced decreased results simulations described section displayed figure comparison gradlda ﬁrst data show multiple discriminative directions data gradlda outperformed lda; left panel ﬁgure. similarly second data right panel slda provides robustness eﬀect presence outliers. thus gradlda necessarily regularize robustify through instance early stopping. however results third fourth data sets show classiﬁcation problem diﬃcult improvements classiﬁcation performance possible using gradlda. thus regularization eﬀects obtained using gradlda classiﬁcation task diﬃcult. note evaluate quality performance slda ﬁrst terms angles subspaces rather terms classiﬁcation accuracy. classiﬁcation accuracy dependent diﬀerence distribution training test sets given training data choose test data arbitrarily. produce test data induces arbitrarily test performance corresponding solution slda solution example frame data sets whereby slightly nonstationary subspace training corresponds highly non-stationary subspace testing. achieved choosing non-stationarity testing reﬂect level non-stationarity observed training figure thus evaluation scheme based classiﬁcation accuracy ﬁxed distribution non-stationarity data dependent lacks objectivity. figure ﬁgure displays results simulations described section .... left panel displays results ﬁrst simulation demonstrates outperforms gradlda stationary data generated sources uniformly separable. right panel displays results second simulation shows gradlda robustify outliers. bottom left panel displays results third simulation bottom right displays results fourth simulation show discriminative subspace smaller regularization eﬀects obtained using gradlda. performances reported given terms errorates y-axis. boxplots whiskers describe full extent data corresponds lower quartile median upper quartile. bottom left panel x-values represent level separation single separable source whereas second panel x-values represent level separation second third separable sources. second simulation figure data-set used slda values hyperparameter achieve lower angles discriminative stationary source data lda. non-stationarity training grows however improves since high level non-stationarity implies discriminability. subtlety note that parameters epoch distributions non-stationarity directions data space drawn probability distribution non-stationarity ignored provided enough epochs available. example assumes data epoch distributed according gaussian distribution non-stationarity consists non-stationarity mean distribution data obtained integrating distribution parameters mean also gaussian figure ﬁgure displays accuracy degrees stable separable direction direction chosen resp. slda simulation described section .... goal pick single direction separable stationary three dimensional space contains non-separable stationary directions separable non-stationary directions. x-axis displays non-stationarity level corresponds text corresponds roughly average deviation epoch means mean epochs. y-axis corresponds accuracy methods choosing correct direction measured degrees. ﬁgure higher non-stationarity level accurate slda high non-stationarity levels inaccurate. however non-stationarity level high becomes less inaccurate lower separation induced non-stationary source non-stationarity. thus case whereby non-stationarity construed draws parameters probability distribution given enough data extra allowance non-stationarity made. thus slda ansatz requires formulation invalid. addition slda classiﬁcation topographies non-stationarities constant time i.e. spatial location non-stationary sources figure ﬁgure displays accuracy radians stable separable direction direction chosen resp. slda simulation described section data consists dimensions composed single stationary separable source separable non-stationary source four noise sources. goal recover single stationary separable source. x-axis displays non-stationarity level corresponds average deviation epoch means mean epochs. axis corresponds varying values trade-oﬀ parameter slda loss function color displayed panels corresponds accuracy radians choosing correct direction methods. non-stationarity levels slda chooses correct direction smaller values higher non-stationarity levels slda higher values chooses correct direction. similarly results displayed figure accurate higher non-stationarity levels lower separation average induced non-stationary directions data setup. aﬀecting classiﬁcation test training data. requirements taken together constitute strong assumptions data generating process. requirement investigated classiﬁcation simulations. simulations section aimed achieving partial intuition subtleties discussed section .... particular ﬁxed parameter value trade-oﬀ parameter achieves high subspace accuracy subspace simulations observe figure necessarily entail improvement slda classiﬁcation task even arbitrary non-stationarity. rather shown that stationary discriminative directions signiﬁcantly separable non-stationary discriminative direction improvement possible. hand number stationary directions discriminative non-stationary moderately discriminative direction improvement possible using slda lda. addition seems gradlda fares even worse case suggests gradlda loses classiﬁcation accuracy poor optimization. suggests addition improvement case plausible given tidier optimization approach slda. example slda yields improvement displayed figure whilst direction chosen training data displays clear separation opposite true test data. hand separation direction chosen slda training data clear distributions remain stable transition test data. figure ﬁgure displays classiﬁcation accuracy resp. slda whilst varying transfer non-stationarity training test. panels data consists sources mixed orthogonally. sources non-stationary separable several noise directions resp. sources separable stationary. section details data setup. panel xaxis displays non-stationarity level corresponds average deviation epoch means mean epochs whilst y-axis corresponds classiﬁcation error achieved. left hand ﬁgure displays classiﬁcation accuracy discriminative subspace dimension approximately whilst right hand ﬁgure discriminative subspace dimension approximately n-source provides less discriminative information simulation corresponding left hand panel. evaluate performance method slda data combined preprocessing making reduction ﬁxed number hcsp features classiﬁcation. data consist subjects taken study baseline procedure classiﬁcation using follows standard template processing motor imagery classiﬁcation algorithm find discriminative frequency band. compute number hcsp feature directions. trial compute variance giving data points trial class. train classiﬁer entire training data consisting data points. test classiﬁer’s performance test set. slda procedure deﬁned algorithm addition test selection cross validation algorithm although argued nature problem unsuitable method classiﬁcation include results completeness check. particular preprocess figure ﬁgure displays example y-direction non-stationary x-direction stationary; although y-direction higher separation training x-direction extent separable stationary. case black decision line yield lower error testing solution blue. find discriminative frequency band. compute number hcsp feature directions. trial compute variance giving data points trial class. train slda classiﬁer entire training data consisting data points. test classiﬁer’s performance test set. extra sanity check test rlda rlda works regularizing covariance estimates extreme eigenvalues occur estimating covariance small sample sizes. testing diﬀerence performance stationarity-penalizing method slda regularizing method rlda check increases performance implicit regularization covariance estimates. steps displayed algorithm another sanity check test following method perform gradient based optimization slda trade parameter unity often refer approach \"gradlda\". steps displayed algorithm find discriminative frequency band. compute number hcsp feature directions using csp. trial compute variance giving data points trial class. perform k-fold classiﬁcation training data trade-oﬀ parameter settings fold find discriminative frequency band. compute number hcsp feature directions using csp. trial compute variance giving data points trial class. train classiﬁer entire training data consisting data points using regularized ﬁnal sanity check test performance following method instead performing slda penalizing non-stationarity test performance trade-oﬀ fisher error function random homogeneous polynomial degree coeﬃcients putatively discriminative direction rationale testing randlda data dependent property improvement using slda possible trade random error function also achieve performance increases. therefore include random trade comparison’s sake. find discriminative frequency band. compute number hcsp feature directions using csp. trial compute variance giving data points trial class. train slda classiﬁer entire training data consisting data points using test classiﬁer’s performance test set. compute random matrix find discriminative frequency band. compute number hcsp feature directions using csp. trial compute variance giving data points trial class. train randlda classiﬁer entire training data consisting data points. test classiﬁer’s performance test set. test classiﬁcation performance methods subjects available report results below. particular hcsp number epochs chosen evaluating stationarity slda guarantees determinacy dimensional projection hcsp choose cross validation slda ssa. training data case consists trials class test data consists trials class. figure displays individual diﬀerences subject performances selection methods tested setting hcsp figure displays average performance methods selection parameter settings. figure displays quantiles improvement performance comparisons made individually figure figure displays example slda induces large improvement lda. figure displays corresponding scalp plots subject figure displays scalp plots slda solutions subject slda performed worst. corresponding values results displayed figure show application leads signiﬁcant improvement performance baseline method despite choice parameter cross validation. although improves performance subset individual trials preprocessing leads signiﬁcant increase overall performance classiﬁcation. therefore concentrate comparison baseline method csp+lda henceforth. table p-values motor imagery data sets. p-values given obtained using sided paired wilcoxon sign rank test using bonferroni correction factor equal number tests performed. test tests null hypothesis eﬀectively algorithm left hand column performs better algorithm middle column. table corresponds single comparison methods. corresponding p-values displayed right hand column. table sided paired t-tests using bonferroni correction factor equal number tests performed. test tests null hypothesis eﬀectively algorithm left hand column performs better algorithm middle column. corresponds comparison methods. corresponding p-values displayed right hand column. method yielding highest performance slda datasets observed gradlda randlda show decrease performance slda not. however improvement slda over instance gradlda signiﬁcant whether slda’s incorporation non-stationarity considerations fact contributes observed improvement also doubtful light data analysis section observe nonstationary directions training test non-stationary direction within training reﬂect poorly. suppose therefore slda’s eﬀectiveness here rest stationarity incorporation must suggest alternative explanations improvement observed. data correctly modeled gaussians covariance regularization using optimal shrinkage parameter optimal since optimal gaussians. claim improvement solely regularization equivalent claiming gaussian assumption false. regularization sole explanation improvement argued implausible addition takes simulations section account. particular discriminative direction data regularization eﬀects lower. often case data sets hand since feature extraction step extracts number discriminative directions. additionally regularization eﬀects never observed exceed simulation whereas performance increases observed exceed thus suppose regularization fully explain improvement using slda randlda gradlda. possible explanations? candidate explanation improvement depends data speciﬁc non-stationarities training test data. particular observe data analysis section highly non-random dependencies non-stationary directions discriminative. according possibility data dependent feature implies slight deviation direction chosen using results increase performance. increase performance using gradlda randlda using attributed slight diﬀerence direction chosen using gradient based optimization. proposed explanation suggest slda gradient based optimization returns slightly diﬀerent direction classiﬁcation produced either local mimima stochastic nature gradient based approach. slight diﬀerence direction solution accounts increase performance data dependency. section gain insight non-stationarity present datasets. understand improvement performance observed despite using putatively suboptimal classiﬁcation methods task non-stationarity correlated direction highest discriminability training data case choosing slightly diﬀerent nonetheless discriminative direction chosen results stationary direction training test data. ﬁrst step analysis compute non-stationary direction training test data non-stationary direction within training data using group wise gain insight consistent attempt infer stationarity directions training test data using training data method used follows results displayed figure firstly results show attempt infer distributions non-stationarity diﬃcult best purely basis training data. results show reject hypothesis angle directions random highly signiﬁcant conﬁdence level using kolgomorov-smirnov test testing whether sample angles directions measured drawn distribution angles one-dimensional subspaces deﬁned vectors drawn dimensional multivariate uniform distribution i.e. distribution deﬁned details experimental setup. visual areas thus provide information training available testing thus diﬀerence discriminative direction presents mode non-stationarity susceptible extraction training data alone. implies often non-stationary direction coincides largely discriminative direction training data. direction simultaneously non-stationary direction deviation away direction chosen highly probably stationary. although appear data pecularity merely artifact experimental setup phenomenon whereby diﬀerent states subject including discriminative information occur training test phases pertinent design brain computer interface. task therefore remains examine structure systematic diﬀerence selected paradigms. demonstrated slda leads signiﬁcant improvements performance classiﬁcation standard approach cannot explained result implicit eigenvalue-based regularization. stated above deﬁning slda derive linear classiﬁer robust non-stationarity assumption classes coarsely modeled gaussians. noted speciﬁc assumptions must fulﬁlled slda work successfully non-stationarity training must constant topography test training phases non-stationary distribution time. however fulﬁlled observed slda leads improvement classiﬁcation performance. particular successfully showed highly non-random patterns non-stationarity. challenge remains therefore describe explain exactly type non-stationarity must present make improvement possible whether understanding levered design methods yield higher improvements face non-stationarity. exciting task understanding domain speciﬁc non-stationarity also inviting road research. figure ﬁgure displays results given diﬀerences error-rates percent individual subjects methods slda rlda gradlda randlda slda cross validation processing motor-imagery subjects. case comparison displayed title corresponding panel. x-axis corresponds individual motor imagery subjects y-axis corresponds diﬀerence performance methods compared. results show slda yields improvement cannot explained terms implicit eigenvalue spectrum related regularization explained terms diﬀerent form regularization otherwise improvement observed randlda. slda highest mean accuracy presence fewer lower tail outliers. figure ﬁgure displays results given error-rates scatter plotted percent methods slda rlda gradlda randlda slda cross validation processing motor imagery subjects. diagonal represents increase performance comparison made displayed title methods classiﬁcation errors displayed y-axes. corresponds single subject. plots show improvement slda marked error rate subject high although improvements performance observed lower error-rates also. figure ﬁgure displays quantiles improvement motorimagery subjects classiﬁcation accuracy methods slda rlda gradlda randlda slda cross validation percentage terms. comparison drawn displayed x-axis. y-values display corresponding increases accuracy. figure upper panel ﬁgure displays mean average classiﬁcation accuracy motor imagery data sets percentage terms slda rlda gradlda randlda slda cross validation. x-values represent individual methods y-values corresponding accuracies. lower panel displays corresponding medians. whilst slda obtains highest mean error randlda obtains highest median error. figure panels display diﬀerence modulo hyperplane training testing single motor imagery subject bottom panels display diﬀerence modulo slda hyperplane training testing subject. blue lines display decision boundary classiﬁer. clarity plot principal component pooled data normal vector respective hyperplane. whilst class distributions shift signiﬁcantly towards hyperplane solution shift less pronounced slda. figure ﬁgure displays scalp plots slda solutions subject whose improvement greatest using slda upper panel ﬁeld pattern displayed whereas bottom panel diﬀerence displayed. diﬀerence topography striking right central parietal electrode positions. figure ﬁgure displays scalp plots slda solutions subject whose improvement inferior using slda upper panel ﬁeld pattern displayed whereas bottom panel diﬀerence displayed. diﬀerence topography striking right central frontal electrode positions. figure ﬁgure displays angles boxplot motor imagery subjects selection dimensional subspaces contained within features. left hand column represents angle chosen training data direction maximizing non-stationarity training test data. second column displays quantiles angles non-stationary direction within training data non-stationary direction training test data. third column displays results would expect subspaces chosen random comparison’s sake. results show that little correlation non-stationarities within training training test correlation subspace chosen direction maximizing training test non-stationarity. leaves particular suitability slda data sets doubtful. thesis contributed research non-stationarity machine learning proposing testing linear projection algorithms dealing non-stationarity. inspired stationary subspace analysis. ﬁrst algorithm maximizes non-stationarity projection dataset thus suited preprocessing method high-dimensional sequential change-point detection although application principle limited such conjecture additionally maximizing non-stationarity prove interest analysis neural data resulting learning paradigm experimental settings. second algorithm proposed yields improvements non-stationarity brain computer interfaces. although cannot conclude improvement performance observed results non-stationarity penalization identiﬁed using method developed slda features datasets represent valuable prior information coping non-stationarity. blankertz kawanabe tomioka hohlefeld nikulin k.r. müller. invariant common spatial patterns alleviating nonstationarities brain-computer interfacing. advances neural information processing systems d.e. gustafson a.s. willsky j.y. wang m.c. lancaster j.h. triebwasser. ecg/vcg rhythm diagnosis using statistical signal analysis. part identiﬁcation persistent rhythms. part identiﬁcation transient rhythms. ieee trans. biomedical engineering bme-– hara kawahara washio bünau. stationary subspace analysis generalized eigenvalue problem. proceedings international conference neural information processing theory algorithms volume part iconip’ pages berlin heidelberg springerverlag. kohlmorgen lemm. dynamic on-line segmentation sequential data. t.g. dietterich becker ghahramani editors advances neural information processing systems pages press lecun boser j.s. denker henderson r.e. howard hubbard l.d. jackel. handwritten digit recognition back-propagation network. touretzky editor neural information processing systems volume morgan kaufman j.s. morris k.r. coombes koomen e.a. baggerly kobayashi. feature extraction quantiﬁcation mass spectrometry biomedical applications using mean spectrum. bioinformatics noboru murata motoaki kawanabe andreas ziehe klaus-robert müller shun ichi amari. on-line learning changing environments applications supervised unsupervised learning. neural networks niedermeyer lopes silva. electroencephalography basic principles clinical applications related ﬁelds. lippincott williams wilkins walnut street philadephia l.r. rabiner. tutorial hidden markov models selected applications speech recognition. waibel editors readings speech recognition pages morgan kaufmann wojcikiewicz vidaurre kawanabe. stationary common spatial patterns towards robust classiﬁcation non-stationary signals. acoustics speech signal processing ieee international conference pages ziehe k.-r. muller nolte b.-m. mackert curio. artifact reduction magnetoneurography based time-delayed second-order correlations. ieee transactions biomedical engineering", "year": 2011}