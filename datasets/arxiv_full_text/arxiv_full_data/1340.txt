{"title": "Sparse Deep Stacking Network for Image Classification", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Sparse coding can learn good robust representation to noise and model more higher-order representation for image classification. However, the inference algorithm is computationally expensive even though the supervised signals are used to learn compact and discriminative dictionaries in sparse coding techniques. Luckily, a simplified neural network module (SNNM) has been proposed to directly learn the discriminative dictionaries for avoiding the expensive inference. But the SNNM module ignores the sparse representations. Therefore, we propose a sparse SNNM module by adding the mixed-norm regularization (l1/l2 norm). The sparse SNNM modules are further stacked to build a sparse deep stacking network (S-DSN). In the experiments, we evaluate S-DSN with four databases, including Extended YaleB, AR, 15 scene and Caltech101. Experimental results show that our model outperforms related classification methods with only a linear classifier. It is worth noting that we reach 98.8% recognition accuracy on 15 scene.", "text": "snnm input layer non-linearly mapped hidden layer using projection matrix sigmoid activation function linearly mapped output layer matrix clearly discriminative ability trained minimizing least squares error output vector label vector. moreover snnm fast infer hidden representation calculating projection multiplication nonlinear transformation. following stacked scheme many snnm modules stacked build deep stacking network previously named deep convex network recently received increasing attentions successful application speech classiﬁcation information retrieval additionally attractive snnm’s batch-mode nature offers potential solution insurmountable problem scalability dealing virtually unlimited amount training data available nowadays therefore extend image classiﬁcation. despite dsn’s success speech classiﬁcation framework also several limitations. first conventional used sigmoid activation function nonlinear hidden layer although sigmoid widely used literature suffers number drawbacks example training slow random initialization solution stuck poor local solution good predictive performance fact antwo types activation functions. hyperbolic tangent applied training deep neural networks. suffers problems sigmoid functions. recent proposal rectiﬁer linear unit observed method useful object recognition often trains signiﬁcantly faster second sparse representations play role image classiﬁcation power learn good robust features noise train gabor-like ﬁlters model higher-order features evidently sparse representations promising results image classiﬁcation furthermore considerable evidence brain percentage neurons active sparse coding learn good robust representation noise model higher-order representation image classiﬁcation. however inference algorithm computationally expensive even though supervised signals used learn compact discriminative dictionaries sparse coding techniques. luckily simpliﬁed neural network module proposed directly learn discriminative dictionaries avoiding expensive inference. snnm module ignores sparse representations. therefore propose sparse snnm module adding mixed-norm regularization sparse snnm modules stacked build sparse deep stacking network experiments evaluate s-dsn four databases including extended yaleb scene caltech. experimental results show model outperforms related classiﬁcation methods linear classiﬁer. worth noting reach recognition accuracy scene. well-known sparse representations number theoretical practical advantages computer vision machine learning particular sparse coding techniques promising results image classiﬁcation e.g. face recognition digit classiﬁcation. sparse coding generative model important extract sparse representations. however sparse coding expensive inference algorithm label training data. although researchers supervised signals learn compact discriminative dictionaries expensive inference algorithm still problem. since train dictionaries using labels directly learn discriminative dictionaries avoiding expensive inference? fortunately simpliﬁed neural network module directly train discriminative dictionaries fast calculate representations. copyright association advancement artiﬁcial intelligence rights reserved. reasonable consider sparse representations snnm modules. however conventional techniques training snnm completely ignores sparse representations. generally achieved penalizing non-zero activation hidden units deviation expected activation hidden units neural networks. moreover neural networks local dependencies hidden units make hidden units better modeling observed data snnm module restricted connections within hidden layer exhibit dependencies. fortunately hidden units without increasing connections divided non-overlapping groups capturing local dependencies among hidden units local dependencies implemented using regularization upon activation possibilities hidden units snnm module. light argument paper exploits sparse deep stacking network image classiﬁcation. s-dsn obtained stacking sparse snnm modules consider activation function relu sigmoid; group sparse penalties penalize hidden representations snnm modular. s-dsn many explicit advantages. first compared sparse coding technique one-layer s-dsn learn projection dictionaries lead faster inference. second compared s-dsn extract sparse representations learning good features image classiﬁcation. last sdsn retain scalable structure dsn. conform advantages s-dsn image classiﬁcation extensive experiments performed four databases including extended yaleb scene caltech. compared multiple related methods experiments show model gets better classiﬁcation results benchmark methods. particular reach recognition accuracy scene. architecture originally presented literature deng explore original strategy building deep networks based stacking layers basic snnm modules take simpliﬁed form multilayer perceptron. mathematically describe follow target vectors rc×n input data vectors arranged form columns rd×n formally basic module lower-layer weight matrix denoted rd×l connects linear input layer nonlinear hidden layer. upper-layer weight matrix denoted rl×c connects nonlinear hidden layer linear output layer. outputs upper-layer rl×n hidden layer outputs convex solution accentuates role convex optimization learning output network weights basic module many basic modules often stacked another form deep model. speciﬁcally input units higher module include output units lowest module optionally input feature obtaining higher-order information data recently extended tensor-dsn which’s basic module replace linear hidden representation output bilinear mapping. retains scalable structure provides higher-order feature interactions missing dsn. s-dsn sparse case dsn. stacking operation s-dsn described general paradigm output vector lower module original input vector form expanded input vector higher module dsn. modular architecture s-dsn different dsn. consider sigmoid function relu function; sparse penalties added hidden units modular architecture. second faster moving towards direction ﬁnds optimal points deterministic nonlinear relationship used compute gradient. plugging criterion least squares objective rewritten however regularization used objective function gradient sdsn complicated. simplify gradient assume sdsn equivalent zero. similar sdsn. second term derive gradient obtain neural networks sparse representations advantageous classiﬁcation moreover group sparse representations learn statistical dependencies hidden units lead better performance implement dependencies averagely divide hidden units nonoverlapping groups restrain dependencies within groups force hidden units group compete luckily mixed-norm regularization conducted modular architecture achieve group sparse representations. following consider mixed-norm regularization follows algorithms learning lower-layer weight matrix first given ﬁxed current optimized using gradient descent algorithm minimize squared error objective s-dsn architecture. summarize optimization s-dsn algorithm capturing spare representation data paper proposes s-dsn implemented penalizing hidden unit activations rectifying negative outputs hidden units activations. simple structure module s-dsn still retains computational advantage parallelism scalability learning parameters. present experimental results four databases extended yaleb database face database caltech scene categories. extended yaleb database database contains frontal face images people. images person. original images cropped normalized pixels. database database consists color images people. person face images taken sessions. images include facial variations including different illumination conditions different expressions different facial disguises following standard -scene data compiled several researchers contains total images falling categories number images category ranging categories include living room bedroom kitchen highway mountain according four databases preprocessed extended yaleb database face database face image projected onto n-dimensional feature vector randomly generated matrix zero-mean normal distribution. dimension random-face feature extended yaleb dimension face face databases n-dimensional features image normalized caltech database ﬁrst extract sift descriptors patches densely sampled image dense grid -pixels stepsize; extract spatial pyramid feature based extracted sift features three grids size train codebook spatial pyramid standard k-means clustering scene category database compute spatial pyramid feature using four-level spatial pyramid sift-descriptor codebook size finally spatial pyramid features reduced dimensions pca. matrix parameters initialized small random values sampled normal distribution zero mean standard deviation simchoplicity regularization parameter chosen regularization parameter chosen group number chosen experiments train epochs number hidden units number layers data experiment repeated times random selected training testing images average precision reported. rest paper denote s-dsn indicates s-dsn sigmoid function; s-dsn indicates s-dsn relu function; dsn- s-dsn- s-dsn respectively indicate one-layer s-dsn s-dsn; dsn- s-dsn- s-dsn- respectively indicate two-layer s-dsn sdsn. table hoyer’s sparseness measures extended yaleb databases. train samples category extended yaleb rest testing. databases number hidden units group sizes s-dsn s-dsn number layers extended yaleb used dsn; used s-dsn. extended yaleb used dsn; used s-dsn. sparseness comparisons presenting classiﬁcation results ﬁrst show sparseness s-dsn s-dsn compared dsn. hoyer’s sparseness measure ﬁgure sparse representations learned s-dsn s-dsn dsn. measure good properties interval normalized scale. value close means zero components vector. perform comparisons extended yaleb databases results reported table sparseness results show s-dsn s-dsn higher sparseness higher recognition accuracy. table compares network s-dsn s-dsn dsn. observe average sparseness layers s-dsn average sparseness layers s-dsn contract average sparseness layers average databases. seen s-dsn learn sparser representations. space reasons figure visualizes activation probabilities ﬁrst hidden layer computed sdsn given image test figure activation probabilities ﬁrst hidden layer computed s-dsn database. activation probabilities normalized dividing maximum activation probabilities. compare s-dsn lc-ksvd algorithms reported state-of-the-art results databases. experimental results summarized table table respectively. s-dsn achieves better results lc-ksvd src. table s-dsn- better lc-ksvd improvement extended yaleb. table s-dsn- s-dsn- also better lc-ksvd improvement respectively. addition compare lc-ksvd terms computation time classifying test image. s-dsn faster inference directly learn projection dictionaries. shown table s-dsn times faster lc-ksvd. scene category following common experimental settings randomly choose images class training data rest test data. experiments used dsn; used s-dsn; used s-dsn. figure effect number hidden units used sdsn s-dsn recognition accuracy. examine performance proposed s-dsn changes varying number hidden units. randomly select images category training data rest test data. consider settings number hidden units changes compare results dsn. reported results figure approaches maintain high classiﬁcation accuracies outperform model. increasing number hidden units accuracy system improves sdsn s-dsn dsn. effects number layers deep framework utilizes multiple-layers feature abstraction better representation images. tables check effect varying number layers classiﬁcation accuracy improves number layers increases. addition compared dictionary learning approaches s-dsn faster inference deep architecture. moreover s-dsn good performance image classiﬁcation. paper present improved model s-dsn image classiﬁcation. s-dsn constructed stacking many sparse snnm modules. sparse snnm module lower-layer weights upper-layer weights solved using convex optimization gradient lc-ksvd deepsc state-of-the-art approaches scspm itdl ispr+ifv sr-lsr decaf dsfl+decaf detailed comparison results shown table compared lc-ksvd s-dsn-’s performance much better since makes improvement. also registers improvement deep models deepsc decaf dsfl+decaf dsn. table shows sdsn performs best among existing methods. confusion matrix s-dsn shown figure misclassiﬁcation errors industrial store higher others. caltech following common experimental settings train samples category test rest. space reasons give parameters training samples category used dsn; used s-dsn; used s-dsn. evaluate approach using spatial pyramid features compare lc-ksvd deepsc approaches scspm lrsc lclr average classiﬁcation rates reported table results s-dsn- outperforms competing dictionary learning approaches including lc-ksvd lrsc src; improvement. s-dsn also regdescent algorithm. s-dsn extract sparse representations random face features spatial pyramid features image classiﬁcation. experimental results show s-dsn yields good classiﬁcation results four public databases linear classiﬁer. work partially supported national science fund distinguished young scholars grant nos. project chinese ministry education grant program fundamental research funds central universities program changjiang scholars innovative research team university irt.", "year": 2015}