{"title": "A Change-Detection based Framework for Piecewise-stationary Multi-Armed  Bandit Problem", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "The multi-armed bandit problem has been extensively studied under the stationary assumption. However in reality, this assumption often does not hold because the distributions of rewards themselves may change over time. In this paper, we propose a change-detection (CD) based framework for multi-armed bandit problems under the piecewise-stationary setting, and study a class of change-detection based UCB (Upper Confidence Bound) policies, CD-UCB, that actively detects change points and restarts the UCB indices. We then develop CUSUM-UCB and PHT-UCB, that belong to the CD-UCB class and use cumulative sum (CUSUM) and Page-Hinkley Test (PHT) to detect changes. We show that CUSUM-UCB obtains the best known regret upper bound under mild assumptions. We also demonstrate the regret reduction of the CD-UCB policies over arbitrary Bernoulli rewards and Yahoo! datasets of webpage click-through rates.", "text": "multi-armed bandit problem extensively studied stationary assumption. however reality assumption often hold distributions rewards change time. paper propose change-detection based framework multiarmed bandit problems piecewise-stationary setting study class change-detection based policies cd-ucb actively detects change points restarts indices. develop cusum-ucb pht-ucb belong cd-ucb class cumulative page-hinkley test detect changes. show cusum-ucb obtains best known regret upper bound mild assumptions. also demonstrate regret reduction cd-ucb policies arbitrary bernoulli rewards yahoo datasets webpage click-through rates. multi-armed bandit problem introduced thompson models sequential allocation presence uncertainty partial feedback rewards. extensively studied turned fundamental many problems artiﬁcial intelligence reinforcement learning online recommendation systems computational advertisement classical multi-armed bandit problem decision maker needs choose independent arms obtains associated reward sequence time slots characterized unknown reward distribution rewards independent identically distributed goal bandit algorithm implemented decision maker minimize regret time slots deﬁned expectation difference between total rewards collected playing highest expected reward total rewards obtained algorithm. achieve goal decision maker faced exploration versus exploitation dilemma trade-off exploring environment copyright association advancement artiﬁcial intelligence rights reserved. proﬁtable arms exploiting current empirically best often possible. problemdependent regret lower bound algorithm classical bandit problem shown robbins several algorithms proposed proven achieve regret thompson sampling \u0001n-greedy upper conﬁdence bound variants bandit policies found bubeck cesa-bianchi although stationary multi-armed bandit problem well-studied unclear whether achieve regret non-stationary environment distributions rewards change time. setting often occurs practical problems. example consider dynamic spectrum access problem communication systems. here decision maker wants exploit empty channel thus improving spectrum usage. availability channel dependent number users coverage area. number users however change dramatically time therefore non-stationary stochastic process. hence availability channel also follows distribution unknown varies time. address changing environment challenge non-stationary multi-armed bandit problem proposed literature. main approaches deal non-stationary environment passively adaptive policies actively adaptive policies first passively adaptive policies unaware changes happen update decisions based recent observations order keep track current best arm. discounted introduced kocsis szepesv´ari geometric moving average samples applied index shown achieve regret upper-bounded number change points time based analysis d-ucb also proposed analyzed sliding-window algorithm updates index based observations within provide regret upper bound cd-ucb class given change detection performance. cusum obtain upper bound mean detection delay lower bound mean time false alarms show regret cusum-ucb best knowledge note results generalized bounded interval. rewards {xt}t≥ modeled sequence independent random variables potentially different distributions unknown decision maker. denote expectation reward time slot i.e. highest expected reward time slot denoted maxi∈k min{µt minimum difference time slots expected rewards best best arm. policy algorithm chooses next play based sequence past plays obtained rewards. performance policy measured terms regret. regret plays deﬁned expected total loss playing suboptimal arms. denote regret policy plays number times played best ﬁrst plays. moving window ﬁxed length. regret sw-ucb exp.s also achieves regret bound uniform exploration mixed standard algorithm. similarly besbes zeevi proposed rexp algorithm restarts algorithm periodically. shown regret upperbounded denotes total reward variation budget time increased regret rexp comes adversarial nature algorithm assumes environment changes every time slot worst case. second actively adaptive policies adopt change detection algorithm monitor varying environment restart bandit algorithms alarm. adapteve proposed hartland employs pagehinkley test detect change points restart policy. also used adapt window length sw-ucl extension sw-ucb multi-armed bandit gaussian rewards. however regret upper bounds adapt-eve adaptive swucl still open problems. works closely related work regard instances change-detection based framework. highlight contributions provide analytical result framework. mellor shapiro took bayesian view non-stationary bandit problem stochastic model dynamic environment assumed bayesian online change detection algorithm applied. similar work hartland theoretical analysis change-point thompson sampling still open. exp.r combines drift detector achieves regret efﬁcient change rate intuition behind actively adaptive policies utilize balance exploration exploitation bandit algorithms change point detected environment stays stationary while often true real world applications. observation motivates construct change-detection based framework class actively adaptive policies developed good theoretical bounds good empirical performance. main contributions follows. propose change-detection based framework piecewise-stationary bandit problem consists change detection algorithm bandit algorithm. develop class policies cd-ucb uses bandit algorithm. design instances supi∈k µt+| piecewise-stationary environment consider notion piecewise-stationary environment mannor distributions rewards remain constant certain period abruptly change unknown time slots called breakpoints. number breakpoints time {∃i∈kµt=µt+}. addition make three assumption ensures shortest interval successive breakpoints greater enough samples estimate mean before change happens. note assumption equivalent notions abruptly changing environment used garivier moulines switching environment mellor shapiro however different adversarial environment assumption environment changes time. make similar assumption assumption mannor detectability paper. assumption exists known parameter µt+| assumption excludes inﬁnitesimal mean shift reasonable practice detecting abrupt changes bounded certain threshold. assumption distributions arms bernoulli distributions. assumption also used literature assumption empirical average bernoulli grid points min{ \u0001)m/m \u0001)m/m t}\\{} minimal non-trival expectation closest grid point deﬁne minimal arms mini∈k change-detection based framework consists components change detection algorithm bandit algorithm shown figure time bandit algorithm outputs decision based past observations bandit environment. environment generates corresponding reward observed bandit algorithm change detection algorithm. change detection algorithm monitors distribution sends positive signal restart note denotes ﬂoor function denotes since bandit algorithms well-studied bandit setting remains change detection algorithm works bandit environment. change point detection problems well studied e.g. book however change detection algorithms applied context quite different bandit setting. challenges adapting existing change detection algorithms bandit setting. unknown priors context change detection problem usually assumes prior distributions change point known. however information unknown decision maker bandit setting. even though simple methods estimating priors applying change detection algorithm like analytical results literature. insufﬁcient samples bandit feedback setting decision maker observe time. however change detection algorithms running parallel since associated change detection procedure monitor possible mean shift. change detection algorithms arms hungry samples time. decision maker feed change detection algorithms intentionally change detection algorithm miss detection opportunities enough recent samples. section introduce change-detection based policy addresses issue insufﬁcient samples. develop tailored cusum algorithm bandit setting overcome issue unknown priors. finally combine cusum algorithm algorithm cusum-ucb policy speciﬁc instance change-detection based framework. performance analysis provided section observed sample step random walk. random walk designed positive mean drift change point negative mean drift without change. hence cusum signals change random walk crosses positive threshold propose tailored cusum algorithm works bandit setting. speciﬁc ﬁrst samples yk)/m. construct random walks negative mean drifts change point positive mean drifts change. particular design two-sided cusum algorithm described algorithm upper random walk monitoring possible positive mean shift. step upper random walk. cusum-ucb policy ready introduce cusum-ucb policy cd-ucb policy cusum change detection algorithm. particular takes parallel cusum algorithms cd-ucb. formal description cusum-ucb found algorithm provided section supplementary material. pht-ucb policy introduce another instance cd-ucb algorithm running change detection algorithm named pht-ucb. viewed variant algorithm replacing time returns alarm breakpoint. given change detection algorithm employ control algorithm cd-ucb policy shown algorithm clarify useful notations follows. last time alarms restarts time number valid observations time denoted total number valid observations decision maker. sample average conﬁdence padding term. particular parameter controls fraction plays exploit feed change detection algorithm. large drive algorithm linear regret performance small limit detectability change detection algorithm. discuss choice sections tailored cusum algorithm change detection algorithm observes sequence independent random variables online manner outputs alarm change point detected. context traditional change detection problem assumes parameters known density function addition sampled distribution breakpoint. mean change point. cusum algorithm originally proposed proven optimal detecting abrupt changes sense worst mean detection delay basic idea cusum algorithm take function next theorem show result cusum used detect abrupt change. note expectations exclude ﬁrst time slots. theorem assumptions expected detection delay expected number false alarms algorithm satisfy summing result theorems obtain regret upper bound cusum-ucb policy. best knowledge ﬁrst regret bound actively adaptive policy bandit feedback setting. theorem assumptions cusum-ucb policy achieves remark choices parameters depend knowledge common non-stationary bandit literature. example discounting factor d-ucb sliding window size sw-ucb depend knowledge batch size rexp depends knowledge denotes total reward variation. practically viable reward change rate regular accurately estimate based history. remark shown garivier moulines policy aplower bound problem section analyze performance part proposed algorithm bandit algorithm change detection algorithm first present regret upper bound result cd-ucb given change detection guarantee. independent interest understanding challenges non-stationary environment. second provide performance guarantees modiﬁed cusum algorithm terms mean detection delay expected number false alarms time then combine results provide regret upper bound cusum-ucb. proofs presented supplementary material. theorem assumption cd-ucb policy achieves recall regret cd-ucb policy uppere]. therefore given parameter values performance change detection algorithm obtain regret upper bound change detection based bandit algorithm. letting obtain following result. corollary regret cd-ucb rπcd-ucb e)). remark oracle algorithm detects change point properties achieve regret recovers regret result man. note change detection algorithm proposed mannor achieves properties side observations available. next proposition introduce result algorithm conditional expected detection delay conditional expected number false alarms given note expectations exclude ﬁrst slots initial observations. proposition recall tuning parameter algorithm assumptions conditional expected detection delay false alarms conditional expected number satisfy regrets time horizon shown figure although assumptions violated cusum-ucb pht-ucb outperform policies. polynomial order regret non-linear least squares method curves model resulting exponents exp.r d-ucb rexp swucb exp.s cusum-ucb pht-ucb table summarizes regret upper bounds existing proposed algorithms non-stationary setting constant policy smaller regret term respect compared sw-ucb. evaluate existing proposed policies three nonstationary environments synthetic dataset real-world dataset yahoo yahoo dataset collected user click traces news articles. pht-ucb similar adapt-eve different adapt-eve ignores issue insufﬁcient samples includes heuristic methods dealing detection points. simulation parameters tuned around based ﬂipping environment. suggest practitioners take approach choices corollary minimizing regret upper bound rather regret. parameters cusum-ucb pht-ucb compare performances cusum pht. parameters listed table section appendix. note obtained based prior knowledge datasets. baseline algorithms tuned similarly knowledge take average regret trials synthetic dataset. synthetic datasets flipping environment. consider arms ﬂipping environment stationary expected reward ﬂips values. arms associated bernoulli distributions. particular change points note equivalent vary within interval compare regrets d-ucb sw-ucb cusum-ucb verify remark reason results algorithms omitted. shown figure cusum-ucb outperforms d-ucb sw-ucb. addition cusum-ucb sw-ucb increases decreases. regret curves shown figure curves model resulting exponents d-ucb rexp sw-ucb exp.r exp.s cusumucb pht-ucb respectively. passively adaptive policies d-ucb swucb rexp receive linear regret time. cusum-ucb pht-ucb achieve much better performance show sublinear regret active adaptation changes. another observation cusum-ucb outperforms pht-ucb. reason behind yahoo dataset frequent breakpoints switching environment thus estimation test drift away detects change turn results detection misses higher regret. yahoo experiment repeat experiment regret curves shown figure curves model resulting exponents d-ucb rexp sw-ucb exp.r exp.s cusum-ucb pht-ucb respectively. passively adaptive policies d-ucb sw-ucb rexp receive linear regret time. cusum-ucb pht-ucb show robust performance larger scale experiment. propose change-detection based framework multiarmed bandit problems non-stationary setting. study class change-detection based policies cd-ucb provide general regret upper bound given performance change detection algorithms. develop cusum-ucb pht-ucb actively react environment detecting breakpoints. analytically show regret cusum-ucb lower regret bound existing policies non-stationary setting. best knowledge ﬁrst regret bound actively adaptive policies. finally demonstrate cusum-ucb outperforms existing policies extensive experiments arbitrary bernoulli rewards real world dataset webpage click-through rates. respectively. regret cusum-ucb pht-ucb shows better sublinear function time compared policies. another observation pht-ucb performs better cusumucb although could regret upper bound pht-ucb. reason behind test stable reliable switching environment. yahoo dataset yahoo experiment yahoo published benchmark dataset evaluation bandit algorithms dataset user click news articles displayed yahoo front page given arrival user goal select article present user order maximize expected click-through rate reward binary value user click. purpose experiment randomly select articles list permutations possible articles overlapped time most. recover ground truth expected clickrates articles take approach mellor shapiro click-through rates estimated dataset taking mean article’s click-through rate every time ticks shown figkocsis szepesv´ari discounted ucb. pascal challenges workshop robbins asymptotically efﬁcient adaptive allocation rules. advances applied mathematics langford wang unbiased ofﬂine evaluation contextual-bandit-based news article proceedings fourth recommendation algorithms. international conference search data mining acm. karatzoglou gentile collaborative proceedings international ﬁltering bandits. sigir conference research development information retrieval acm. lorden procedures reacting change distribution. annals mathematical statistics mellor shapiro thompson sampling switching environments bayesian online change detection. proceedings sixteenth international conference artiﬁcial intelligence statistics page biometrika pollard convergence stochastic processes. springer. srivastava reverdy leonard surveillance abruptly changing world multiarmed bandits. decision control ieee annual conference ieee. sutton barto reinforcement learning introduction volume press cambridge. thompson likelihood unknown probability exceeds another view evidence samples. biometrika c.-y.; hong y.-t.; c.-j. tracking best expert non-stationary stochastic environments. advances neural information processing systems yahoo webscope program. sandbox.yahoo.com/catalog.php?datatype= r&did=. mannor piecewise-stationary bandit problems side observations. proceedings annual international conference machine learning acm.", "year": 2017}