{"title": "Recurrent Highway Networks", "tag": ["cs.LG", "cs.CL", "cs.NE"], "abstract": "Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with 'deep' transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which extend the LSTM architecture to allow step-to-step transition depths larger than one. Several language modeling experiments demonstrate that the proposed architecture results in powerful and efficient models. On the Penn Treebank corpus, solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters. On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.", "text": "many sequential processing tasks require complex nonlinear transition functions step next. however recurrent neural networks \"deep\" transition functions remain difﬁcult train even using long short-term memory networks. introduce novel theoretical analysis recurrent networks based geršgorin’s circle theorem illuminates several modeling optimization issues improves understanding lstm cell. based analysis propose recurrent highway networks extend lstm architecture allow step-to-step transition depths larger one. several language modeling experiments demonstrate proposed architecture results powerful efﬁcient models. penn treebank corpus solely increasing transition depth improves word-level perplexity using number parameters. larger wikipedia datasets character prediction rhns outperform previous results achieve entropy bits character. network depth central importance resurgence neural networks powerful machine learning paradigm theoretical evidence indicates deeper networks exponentially efﬁcient representing certain function classes bianchini scarselli references therein). sequential nature recurrent neural networks long credit assignment paths deep time. however certain internal function mappings modern rnns composed units grouped *equal contribution zürich switzerland swiss idsia nnaisense switzerland. correspondence julian zilly <jzillyethz.ch> rupesh srivastava <rupeshidsia.ch>. layers usually take advantage depth example state update time step next typically modeled using single trainable linear transformation followed non-linearity. unfortunately increased depth represents challenge neural network parameters optimized means error backpropagation deep networks suffer commonly referred vanishing exploding gradient problems since magnitude gradients shrink explode exponentially backpropagation. training difﬁculties ﬁrst studied context standard rnns depth time proportional length input sequence arbitrary size. widely used long short-term memory architecture introduced speciﬁcally address problem vanishing/exploding gradients recurrent networks. vanishing gradient problem also becomes limitation training deep feedforward networks. highway layers based lstm cell addressed limitation enabling training networks even hundreds stacked layers. used feedforward connections layers used improve performance many domains speech recognition language modeling variants called residual networks widely useful many computer vision problems paper ﬁrst provide mathematical analysis rnns offers deeper understanding strengths lstm cell. based insights introduce lstm networks long credit assignment paths time also space called recurrent highway networks rhns. unlike previous work deep rnns model incorporates highway layers inside recurrent transition argue superior method increasing depth. enables substantially powerful trainable sequential models efﬁciently signiﬁcantly outperforming existing architectures widely used benchmarks. recent years common method utilizing computational advantages depth recurrent networks stacking recurrent layers analogous using multiple hidden layers feedforward networks. training stacked rnns naturally requires credit assignment across space time difﬁcult practice. problems recently addressed architectures utilizing lstm-based transformations stacking general method increase depth step-to-step recurrent state transition tick several micro time steps step sequence method adapt recurrence depth problem learn parameters memories previous events standard deep nonlinear processing. notable graves reported improvements simple algorithmic tasks using method performance improvements obtained real world data. pascanu proposed increase recurrence depth adding multiple non-linear layers recurrent transition resulting deep transition rnns deep transition rnns skip connections rnns). powerful principle architectures seldom used exacerbated gradient propagation issues resulting extremely long credit assignment paths. related work chung added extra connections states across consecutive time steps stacked also increases recurrence depth. however model requires many extra connections increasing depth gives fraction states access largest depth still faces gradient propagation issues along longest paths. compared stacking recurrent layers increasing recurrence depth signiﬁcantly higher modeling power rnn. figure illustrates stacking layers allows maximum credit assignment path length hidden states time steps apart recurrence depth enables maximum path length allows greater power efﬁciency using larger depths also explains architectures much difﬁcult train compared stacked rnns. next sections address problem head focusing mechanisms lstm using design rhns suffer difﬁculties. figure comparison stacked depth deep transition recurrence depth operating sequence time steps. longest credit assignment path hidden states time steps deep transition rnns. denote total loss input sequence length represent output standard time rn×m rn×n input recurrent weight matrices bias vector point-wise non-linearity. describes dynamics standard rnn. derivative loss respect parameters network expanded using chain rule input bias omitted simplicity. obtain conditions gradients vanish explode. temporal jacobian maximal bound σmax largest singular value norm jacobian centered around diagonal values radius j=j=i |aij| equal absolute values non-diagonal entries example geršgorin circles referring differently initialized rnns depicted figure using understand relationship entries possible locations eigenvalues jacobian. shifting diagonal values shifts possible locations eigenvalues. large off-diagonal entries allow large spread eigenvalues. small off-diagonal entries yield smaller radii thus conﬁned distribution eigenvalues around diagonal entries aii. values initialized standard deviation close spectrum largely dependent also initially centered around example geršgorin circle could corresponding circle figure magnitude eigenvalues |λi| initially likely substantially smaller additionally employing commonly used weight regularization also limit magnitude eigenvalues. alternatively entries initialized large standard deviation radii geršgorin circles corresponding increase. hence spectrum possess eigenvalues norms greater resulting exploding gradients. radii summed size matrix larger matrices associated larger circle radius. consequence larger matrices initialized correspondingly smaller standard deviations avoid exploding gradients. general unlike variants lstm rnns direct mechanism rapidly regulate jacobian eigenvalues across time steps hypothesize efﬁcient necessary learning complex sequence processing. proposed initialize identity matrix small random values off-diagonals. changes situation depicted result identity initialization indicated circle figure initially since spectrum described centered around ensuring gradients less likely vanish. however ﬂexible remedy. training eigenvalues easily become larger resulting exploding gradients. conjecture reason extremely small learning rates used figure illustration geršgorin circle theorem. geršgorin circles centered around diagonal entries aii. corresponding eigenvalues within radius absolute values non-diagonal entries aij. circle represents exemplar geršgorin circle initialized small random values. circle represents identity initialization diagonal entries recurrent matrix small random values otherwise. dashed circle denotes unit circle radius together provides conditions vanishing gradients note depends activation function e.g. |tanh| logistic sigmoid. similarly show spectral radius greater exploding gradients emerge since description problem terms largest singular values spectral radius sheds light boundary conditions vanishing exploding gradients illuminate eigenvalues distributed overall. applying geršgorin circle theorem able provide insight problem. geršgorin circle theorem square matrix rn×n highway layers enable easy training deep feedforward networks adaptive computation. outputs nonlinear transforms associated weight matrices whtc. typically utilize sigmoid nonlinearity referred transform carry gates since regulate passing transformed input carrying original input highway layer computation deﬁned denotes element-wise multiplication. recall recurrent state transition standard described propose construct recurrent highway network layer multiple highway layers recurrent state transition formally whtc rn×m rhtc rn×n represent weights matrices nonlinear transform gates layer biases denoted bhtc denote intermediate output layer layer recurrence depth described studied greff jozefowicz retains essential components lstm multiplicative gating units controlling information self-connected additive cells. however layer naturally extends extending lstm model complex state transitions. similar highway lstm layers variants constructed without changing basic principles example ﬁxing gates always open coupling gates done experiments paper. simpler formulation layers allows analysis similar standard rnns based gct. omitting inputs biases temporal jacobian ∂y/∂y layer recurrence depth given note directly transformed ﬁrst highway layer recurrent transition layer layer’s output previous time step. subsequent highway layers process outputs previous layers. dotted vertical lines figure separate multiple highway layers recurrent transition. equation captures inﬂuence gates eigenvalues compared situation standard seen layer ﬂexibility adjusting centers radii geršgorin circles. particular limiting cases noted. carry gates fully open transform gates fully closed results i.e. eigenvalues since geršgorin circle radius shrunk diagonal entry limiting case eigenvalues simply gates vary eigenvalues dynamically adjusted combination limiting behaviors. figure schematic showing computation within layer inside recurrent loop. vertical dashed lines delimit stacked highway layers. horizontal dashed lines imply extension recurrence depth stacking layers. transformations described equations respectively. takeaways analysis follows. firstly allows observe behavior full spectrum temporal jacobian effect gating units expect learning multiple temporal dependencies real-world data efﬁciently sufﬁcient avoid vanishing exploding gradients. gates layers provide versatile setup dynamically remembering forgetting transforming information compared standard rnns. secondly becomes clear effect behavior jacobian highly non-linear gating functions facilitate learning rapid precise regulation network dynamics. depth widely used method expressive power functions motivating multiple layers transformations. paper extending layers using highway layers favor simplicity ease training. however expect cases stacking plain layers transformations also useful. finally analysis layer’s ﬂexibility controlling spectrum furthers theoretical understanding lstm highway networks variants. feedforward highway networks jacobian layer transformation takes place temporal jacobian analysis. highway layer allows increased ﬂexibility controlling various components input transformed carried. ﬂexibility likely reason behind performance improvement highway layers even cases network depth high experiments setup work carry gate coupled transform gate setting similar suggestion highway networks. coupling also used recurrent architecture. reduces model size ﬁxed number units prevents unbounded blow-up state values leading stable training imposes modeling bias suboptimal certain tasks output non-linearity similar lstm networks could alternatively used combat issue. optimization wikipedia experiments bias transform gates towards closed start training. networks single hidden layer since interested studying inﬂuence recurrence depth stacking multiple layers already known useful. detailed conﬁgurations experiments included supplementary material. regularization rhns like rnns suitable regularization essential obtaining good generalization rhns practice. adopt regularization technique proposed interpretation dropout based approximate variational inference. rhns regularized technique referred variational rhns. penn treebank word-level language modeling task report results without weighttying input output mappings fair comparisons. regularization independently proposed inan khosravi press wolf architecture designed enable optimization recurrent networks deep transitions. therefore primary experimental veriﬁcation seek whether rhns higher recurrence depth easier optimize compared alternatives preferably using simple gradient based methods. figure swarm plot optimization experiment results various architectures different depths next step prediction chorales dataset. point result optimization using random hyperparameter setting. number network parameters increases depth kept across architectures depth. architectures random search unable good hyperparameters depth increased. ﬁgure must viewed color. network sizes chosen total number network parameters increases recurrence depth increases remains across architectures. hyperparameter search conducted sgd-based optimization architecture depth combination fair comparisons. absence optimization difﬁculties larger networks reach similar better loss value compared smaller networks. however swarm plot figure shows dt-rnn dt-rnn become considerably harder optimize increasing depth. similar feedforward highway networks increasing recurrence depth adversely affect optimization rhns. examine effect recurrence depth train rhns ﬁxed total parameters recurrence depths ranging word level language modeling penn treebank dataset preprocessed mikolov hyperparameters used train model. depth show test perplexity best model based performance validation figure additionally also report results model trained regularization. cases test score improves recurrence depth increases best layer model reducing weight decay improves results recurrence depth increases layers \"width\" network decreases units since number parameters kept ﬁxed. thus results demonstrate even small datasets utilizing parameters increase depth yield large beneﬁts even though size \"state\" reduced. table compares result best published results dataset. directly comparable baseline variational lstm+wt differs network architecture size models. rhns outperform single models well previous ensembles also beneﬁt regularization similar lstms. solely analyzed architecture found reinforcement learning hyperparamater search zoph achieves better results. task experiment next symbol prediction challenging hutter prize wikipedia datasets text enwik unicode symbols total respectively. size complexity datasets allow stress learning generalization capacity rhns. train various variational rhns recurrence depth units hidden layer obtaining state-of-the-art results. text table validation test perplexity recent state word-level language models penn treebank dataset. model uses feedforward highway layers transform character-aware word representation feeding lstm layers. dropout indicates regularization used zaremba applied input output recurrent layers. variational refers dropout regularization based approximate variational inference. rhns large recurrence depth achieve highly competitive results highlighted bold. model rnn-lda cache conv.+highway+lstm+dropout lstm+dropout variational lstm variational lstm pointer sentinel-lstm variational lstm augmented loss variational neural architecture search base variational neural architecture search base neural architecture search base validation/test model units recurrence depth achieved. similarly enwik validation/test achieved model hyperparameters. difference models size embedding layer size character set. table table show rhns outperform previous best models text enwik signiﬁcantly fewer total parameters. detailed description networks provided supplementary material. analyze inner workings rhns inspection gate activations effect network performance. recurrence depth optimized chorales dataset figure shows mean transform gate activity layer time steps example sequences. note gates biased towards zero initialization layers utilized trained network. gate activity ﬁrst layer recurrent transition typically high average indicating least layer recurrent transition almost always utilized. gates layers varied behavior dynamically switching activity time different sequence. similar feedforward case highway layers rhns perform adaptive computation i.e. effective amount transformation dynamically adjusted sequence time step. unlike general methods mentioned section maximum depth limited recurrence depth layer. concrete description computations feedforward networks recently offered terms learning unrolled iterative estimation description carries rhns ﬁrst layer recurrent transition computes rough estimation memory state change figure test perplexity penn treebank word-level language modeling using rhns ﬁxed parameter budget increasing recurrence depth. increasing depth improves performance layers. mean activations transform gates recurrence depth different sequences activations averaged units highway layer. high value indicates layer transforms inputs particular time step larger extent opposed passing input next layer contributions layers towards network performance quantiﬁed lesioning experiment highway layer time gates pushed towards carry behavior setting bias large negative value resulting loss training measured. change loss biasing layer measures contribution network performance. rhns ﬁrst layer recurrent transition contributes much overall performance compared others removing layer general lowers performance substantially recurrent nature network. plot obtained results included supplementary material. developed analysis behavior rnns based geršgorin circle theorem. analysis provided insights ability gates variably inﬂuence learning simpliﬁed version lstms. introduced recurrent highway networks powerful model designed take advantage increased depth recurrent transition retaining ease training lstms. experiments conﬁrmed theoretical optimization advantages well improved performance well known sequence modeling tasks. acknowledgements research partially supported project intuitive natural prosthesis utilization snsf grant advanced reinforcement learning thank klaus greff sjoerd steenkiste wonmin byeon steunebrink many insightful discussions. grateful nvidia corporation providing dgx- computer idsia part pioneers research award. bianchini monica scarselli franco. complexity neural network classiﬁers comparison shallow deep architectures. ieee transactions neural networks boulanger-lewandowski bengio vincent modeling temporal dependencies high-dimensional sequences application polyphonic music generation transcription. arxiv e-prints june kyunghyun merriënboer bart gulcehre caglar bahdanau dzmitry bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoderdecoder statistical machine translation. arxiv preprint arxiv. hochreiter bengio frasconi schmidhuber gradient recurrent nets difﬁculty learning long-term dependencies. kremer kolen field guide dynamical recurrent neural networks. ieee press robinson fallside utility driven dynamic error propagation network. technical report cued/finfeng/tr. cambridge university engineering department schmidhuber jürgen. reinforcement learning markovian non-markovian environments. advances neural information processing systems morgan-kaufmann srivastava rupesh greff klaus schmidhuber juergen. training deep networks. advances neural information processing systems curran associates inc. werbos paul system modeling optimization proceedings ifip conference york city august september chapter applications advances nonlinear sensitivity analysis springer berlin heidelberg williams complexity exact gradient computation algorithms recurrent neural networks. technical report nuccs-- boston northeastern university college computer science kept enable learning long-term dependencies regularize variational dropout used. ﬁrst second model used dropout probabilities input embedding output layer input hidden units rhn. larger third model used dropout probabilities input embedding output layer input hidden units rhn. weights initialized uniformly range initial bias transform gate facilitate learning early training. similar penn treebank experiments gradients re-scaled norm whenever value exceeded. embedding size weight-tying used. text wikipedia text dataset split training/validation/test splits characters similar recent work. trained rhns stacked layers recurrent state transition. units units resulting networks parameters respectively. initial learning rate learning rate decay unit model units model used epochs. training performed mini-batches sequences length model units model units weight decay activation previous sequence kept enable learning long-term dependencies regularize variational dropout used dropout probabilities input embedding output layer input hidden units model units. model units used dropout probabilities input embedding output layer input ﬁnally dropout probabilities hidden units rhn. weights initialized uniformly range initial bias transform gate facilitate learning early training. similar penn treebank experiments gradients rescaled norm whenever value exceeded. embedding size weight-tying used. lesioning experiment figure shows results lesioning experiment section experiment conducted recurrence depth trained chorales dataset part optimization experiment subsection dashed line corresponds training error without lesioning. x-axis denotes index lesioned highway layer y-axis denotes likelihood network predictions. following paragraphs describe precise experimental settings used obtain results paper. reproducing results penn treesource code bank https//github.com/julian/recurrenthighwaynetworks github. optimization experiments compare rhns deep transition rnns deep transition rnns skip connections -rnns) introduced pascanu random hyperparamter settings architecture depth. number units layer recurrence ﬁxed recurrence depths respectively. batch size training maximum epochs performed stopping earlier loss improve epochs. tanh used activation function nonlinear layers. random search initial transform gate bias sampled {−−−} initial learning rate sampled uniformly finally weights initialized using gaussian distribution standard deviation sampled uniformly experiments optimization performed using stochastic gradient descent momentum momentum penn treebank penn treebank text corpus comparatively small standard benchmark language modeling. pre-processing data used code based gal’s extension zaremba’s implementation. study inﬂuence recurrence depth trained compared rhns layer recurrence depth total budget parameters. leads hidden state sizes ranging units. batch size ﬁxed sequence length truncated backpropagation learning rate learning rate decay starting epochs weight decay maximum gradient norm dropout rates chosen embedding layer input gates hidden units output activations. weights initialized uniform distribution best -layer model obtained lowering weight decay improved results. enwik wikipedia enwik dataset split training/validation/test splits characters similar recent work. trained three different rhns. stacked layers recurrent state transition units resulting network parameters. second stacked layers recurrence units total parameters third stacked layers units total parameters. initial learning rate learning rate decay epochs used. large model stacked layers units used learning rate decay ensure proper convergence. training performed mini-batches sequences length weight decay ﬁrst model two. activation previous sequence", "year": 2016}