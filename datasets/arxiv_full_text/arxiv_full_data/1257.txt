{"title": "DarkRank: Accelerating Deep Metric Learning via Cross Sample  Similarities Transfer", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "We have witnessed rapid evolution of deep neural network architecture design in the past years. These latest progresses greatly facilitate the developments in various areas such as computer vision and natural language processing. However, along with the extraordinary performance, these state-of-the-art models also bring in expensive computational cost. Directly deploying these models into applications with real-time requirement is still infeasible. Recently, Hinton etal. have shown that the dark knowledge within a powerful teacher model can significantly help the training of a smaller and faster student network. These knowledge are vastly beneficial to improve the generalization ability of the student model. Inspired by their work, we introduce a new type of knowledge -- cross sample similarities for model compression and acceleration. This knowledge can be naturally derived from deep metric learning model. To transfer them, we bring the \"learning to rank\" technique into deep metric learning formulation. We test our proposed DarkRank method on various metric learning tasks including pedestrian re-identification, image retrieval and image clustering. The results are quite encouraging. Our method can improve over the baseline method by a large margin. Moreover, it is fully compatible with other existing methods. When combined, the performance can be further boosted.", "text": "mitigate problem many model acceleration methods proposed. roughly categorized three types network pruning model quantization knowledge transfer. network pruning iteratively removes neurons weights less important ﬁnal prediction; model quantization decreases representation precision weights activations network thus increases computation throughput; knowledge transfer directly trains smaller student network guided larger powerful teacher. among methods knowledge transfer based methods practical. compared methods mostly need tailor made hardwares implementations archive considerable acceleration without bells whistles. knowledge distill variants dominant approaches among knowledge transfer based methods. though utilize different forms knowledges knowledges still limited within single sample. namely methods provide precise supervision sample teacher networks either classiﬁer intermediate feature level. however methods miss another valuable treasure relationships across different samples. kind knowledge also encodes structure embedded space teacher networks. moreover naturally objective metric learning since usually utilizes similar instance level supervision. elaborate motivation sequel depict method fig. upper right corner shows student better captures similarity images transferring. digit similar ranked higher. witnessed rapid evolution deep neural network architecture design past years. latest progresses greatly facilitate developments various areas computer vision natural language processing. however along extraordinary performance state-of-theart models also bring expensive computational cost. directly deploying models applications real-time requirement still infeasible. recently hinton shown dark knowledge within powerful teacher model signiﬁcantly help training smaller faster student network. knowledge vastly beneﬁcial improve generalization ability student model. inspired work introduce type knowledge cross sample similarities model compression acceleration. knowledge naturally derived deep metric learning model. transfer them bring learning rank technique deep metric learning formulation. test proposed darkrank method various metric learning tasks including pedestrian re-identiﬁcation image retrieval image clustering. results quite encouraging. method improve baseline method large margin. moreover fully compatible existing methods. combined performance boosted. metric learning basis many computer vision tasks including face veriﬁcation pedestrian reidentiﬁcation. recent years end-to-end deep metric learning method learns feature representation guide metric based losses achieved great success. factor success deep metric learning methods powerful network architectures. nevertheless along powerful features deeper wider networks also bring heavier computation burden. many real-world applications like autonomous driving system latency critical limited hardware resources. figure network architecture darkrank method. student network trained standard classiﬁcation loss contrastive loss triplet loss well similarity transfer loss proposed test proposed method various metric learning tasks. method signiﬁcantly improve performance student networks. applied jointly existing methods better transferring performance. deep metric learning different traditional metric learning methods focus learning mahalanobis distance euclidean space high dimensional kernel space deep metric learning usually transforms features dnns compare samples euclidean space directly. despite rapid evolution network architectures loss functions metric learning still popular research topic. point metric learning separate inter-class embeddings reduce intra-class variance. classiﬁcation loss variants learn robust features help separate samples different classes. however out-of-sample identities performance cannot guaranteed since explicit metric induced approach. another drawback classiﬁcation loss projects samples label direction embedding space thus ignores intra-class variance. veriﬁcation loss popular alternative directly encodes similarity dissimilarity supervisions. weakness veriﬁcation loss tries enforce hard margin anchor negative samples. restriction strict since images different categories look similar other. imposing hard margin samples hurts learnt representation. triplet loss variants overcome disadvantage imposing order embedded triplets instead. triplet loss exact reﬂection desired retrieval results positive samples closer anchor negative ones. good performance requires careful design sampling training procedure. related work includes center loss maintains shifting template class reduce intra-class variance simultaneously drawing template sample towards other. besides loss function design introduce smoothness metric space respect data manifold prior. another closely related method listmle. unlike listnet name states listmle aims maximizing likelihood ground truth ranking formal deﬁnition follow motivation depict framework fig. along intuitive illustration explain motivation work. example query digit relevant digits irrelevant digits. training supervision original student network successfully rank relevant digits front irrelevant ones. however query similar digits. simply using hard labels totally ignores dark knowledge. however knowledge crucial generalization ability student models. powerful teacher model reﬂect similarities embedded space. consequently propose transfer cross sample similarities improve performance student networks. formulation denote embedded features mini-batch embedding function choice depends problem hand image data text data. denote embedded features student networks similarly teacher networks. sample mini-batch anchor query rest samples mini-batch candidates x··· xn}. construct similarity score function based euclidean distance embeddings. parameters score function control scale contrast different embeddings knowledge transfer model acceleration compression bucila ﬁrst proposed approximate ensemble classiﬁers single neural network. recently hinton revived idea name knowledge distill. insight comes softened probabilities output classiﬁers encode accurate embedding sample label space one-hot labels. consequently addition original training targets proposed soft targets teacher networks guide training student networks. process transfers precise supervision signal student networks therefore improves generalization ability. subsequent works fitnets attention transfer neuron selectivity transfer tried exploit knowledges intermediate feature maps cnns improve performance. instead using forward input-output pairs czarnecki tried utilize gradients respect input teacher network knowledge transfer sobolev training. paper exploit unique type knowledge inside deep metric learning model cross sample similarities train better student network. learning rank learning rank refers problem given query rank list samples according similarities. learning rank methods divided three types pointwise pairwise listwise according assembling samples. pointwise approaches directly optimize relevance label similarity score query candidate; pairwise approaches compare relative relevance similarity candidates. representative works pairwise ranking include ranking lambda rank listwise methods either directly optimize ranking evaluation metric maximize likelihood groundtruth rank. listnet listmle fall category. paper introduce listwise ranking loss deep metric learning utilize transfer soft similarities candidates query student models. section review listnet listmle classical listwise learning rank methods introduced document retrieval task. methods closely related proposed method elaborated sequel. that propose methods transfer soft transfer hard transfer. soft transfer method construct probability distributions possible permutations mini-batch based eqn. then match distributions divergence. hard transfer method simply maximize likelihood ranking highest probability teacher model. formally cuhk cuhk large scale data person re-identiﬁcation. contains images identities. identity captured cameras different views. author provides detected handcropped annotations. conduct experiments detected data since closer real world scenarios. furthermore follow training evaluation protocol report rank- performance ﬁrst standard split. market market contains images identities. images collected different camera views. follow training evaluation protocol report mean average precision rank- accuracy single multiple query settings. cub-- caltech ucsd birds-- dataset contains images bird species. following setting train network ﬁrst species perform image retrieval clustering rest species standard recall metrics reported. implementation details choose inception-bn teacher network nin-bn student network. networks pre-trained imagenet lsvrc image classiﬁcation dataset. ﬁrst remove fully connected layers speciﬁc pre-trained task globally average pool features. output connected fully connected layer followed normalization layer generate ﬁnal embeddings. large margin softmax loss directly connected fully connected layer. losses including proposed transfer loss built upon normalization layer. figure illustrates architecture system. margin large margin softmax loss margin triplet veriﬁcation loss. loss weights veriﬁcation triplet large margin softmax loss respectively. choose stochastic gradient descent method momentum optimization. learning rate inception-bn nin-bn weight decay train model epochs shrink learning rate factor epochs. batch size person reid tasks resize input images randomly crop ﬁrst construct possible cross view positive image pairs randomly shufﬂe start epoch. image retrieval clustering resize input images randomly crop addition images horizontal direction randomly training tasks. implement method mxnet train model scratch experimenting soft transfer considers possible rankings. helpful several rankings similar probability. however possible ranking total. feasible large. whereas hard transfer considers possible ranking labeled teacher. demonstrated experiments hard transfer good approximation soft transfer sense much faster long lists similar performance. overall loss function training student networks consists losses ground-truth loss teacher knowledge. speciﬁc combine large margin softmax loss veriﬁcation loss triplet loss proposed darkrank loss either soft hard variant. section test performance darkrank method several metric learning tasks including person re-identiﬁcation image retrieval clustering compare several baselines closely related works. also conduct ablation analysis inﬂuence hyperparameters method. figure selected results visualization darkrank transfer market. border color image denotes relation query image. help teacher’s knowledge student model learns better distance metric capture similarities images. compared methods introduce models baselines compared experiments. despite soft hard darkrank methods proposed also test following methods combination methods knowledge distill since classiﬁcation loss included model test knowledge distill softened softmax target. according temperature loss weight softmax knowledge distill method. formally deﬁned direct match distances query candidates straightforward form cross sample similarities knowledge. directly match distances output teacher student models baseline. formally matching loss deﬁned table. directly matching distances teacher student model marginal improvement original student model. reason student model struggles match exact distances teacher’s limited capacity. method soft hard variants make signiﬁcant improvements original model. could similar satisfactory results. discussed formulation hard variant great computational advantage soft training thus preferable practitioners. moreover synergy performance student model improved. complementary results demonstrate method indeed transfers interinstance knowledge teacher network ignored contrast since rank information reveals relative distance query candidate provide much details absolute distance metric space. distances candidates query close associated probabilities permutations also close makes hard distinguish good ranking ranking. introduce contrast parameter sharpen differences scores. test different values cuhk validation model performance peaks. figure shows details. loss weight training process important balance transfer loss original training loss. loss weight transfer loss according results fig. note also reveals performance model quite stable large range supervised learning achieved great success computer vision majority collected data remains unlabeled. tasks like self-supervised learning class level supervision available. supervision signal purely comes pairwise similarity. knowledge transfer methods like hard cases. advantage method utilize instance level supervision thus available supervised unsupervised tasks. another well-known instance level method fitnet directly matches embeddings student teacher loss. compare transfer performance fitnet without darkrank. shown table. fitnet achieves similar performance method alone. combined method signiﬁcant improvement achieved. result proves method utilizes different kind information complimenting existing intra-instance methods. scaling factor constraining embeddings unit hyper-sphere standard setting metric learning methods person reid recent work shows small embedding norm hurt representation power embeddings. compensate introducing scaling factor test different values goal image clustering group images categories according visual similarity. image retrieval ﬁnding similar images gallery given query image. tasks rely heavily embeddings learnt model since similarity image pair generally calculated based euclidean mahalanobis distance embeddings. metrics adopted image clustering nmi. harmonic mean precision recall. normalized mutual information reﬂects correspondence candidate clustering ground-truth clustering dataset. mutual information entropy respectively. ranges higher value indicates better correspondence. choose recall percentage returned images belongs category query image metric image retrieval task. networks hyper-parameters stated implementation details section. present image retrieval clustering results cub-- table. results show method achieves signiﬁcant margin recall metrics. shows method generally applicable various kinds metric learning tasks. summarize complexity performance teacher student network table. speed tested pascal titan mxnet don’t optimize implementation testing. note that ﬁrst work studies knowledge transfer deep metric learning model choose off-the-shelf network architectures rather deliberately designing them. even though still achieve wall time acceleration minor performance loss. believe beneﬁt latest network design philosophy achieve even better speedup. paper proposed type knowledge cross sample similarities model compression acceleration. fully utilize knowledge modiﬁed classical listwise rank loss bridge teacher networks student networks. knowledge transfer student model signiﬁcantly improve performance various metric learning tasks. moreover combining transfer methods exploit intra-instance knowledge performance teachers students narrowed. particularly without deliberately tuning network architecture method achieves three times wall clock speedup minor performance loss off-the-shelf networks. believe preliminary work provides possibility knowledge transfer based model acceleration. future would like exploit cross sample similarities general applications beyond deep metric learning.", "year": 2017}