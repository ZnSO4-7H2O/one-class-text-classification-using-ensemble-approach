{"title": "Toward Deeper Understanding of Neural Networks: The Power of  Initialization and a Dual View on Expressivity", "tag": ["cs.LG", "cs.AI", "cs.CC", "cs.DS", "stat.ML"], "abstract": "We develop a general duality between neural networks and compositional kernels, striving towards a better understanding of deep learning. We show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space. Hence, though the training objective is hard to optimize in the worst case, the initial weights form a good starting point for optimization. Our dual view also reveals a pragmatic and aesthetic perspective of neural networks and underscores their expressive power.", "text": "develop general duality neural networks compositional kernels striving towards better understanding deep learning. show initial representations generated common random initializations suﬃciently rich express functions dual kernel space. hence though training objective hard optimize worst case initial weights form good starting point optimization. dual view also reveals pragmatic aesthetic perspective neural networks underscores expressive power. neural network learning underpinned state empirical results numerous applied machine learning tasks nonetheless neural network learning remains rather poorly understood several regards. notably remains unclear training algorithms good weights learning impacted network architecture activations role random weight initialization choose concrete optimization procedure given architecture. start analyzing expressive power subsequent random weight initialization. motivation empirical success training algorithms despite inherent computational intractability fact optimize highly non-convex objectives potentially many local minima. result shows random initialization already positions learning algorithms good starting point. deﬁne object termed computation skeleton describes distilled structure feed-forward networks. skeleobtained certain non-linear compositions according skeleton’s structure. show representation generated random initialization suﬃciently rich approxiaddition explaining part success ﬁnding good weights study provides appealing perspective neural network learning. establish tight connection between network architectures dual kernel spaces. connection generalizes several previous constructions demonstrate dual view gives rise design principles supporting current practice suggesting ideas. outline points. current theoretical understanding learning. understanding neural network learning particularly recent successes commonly decomposes following research questions. though still complete previous work provides understanding questions standard results complexity theory imply essentially functions interest expressed network moderate size. biological phenomena show many relevant functions expressed even simpler networks similar convolutional neural networks dominant tasks today. barron’s theorem states even two-layer networks express rich functions. question classical recent results statistical learning theory show number examples grows comparison size network empirical loss must close population loss. contrast ﬁrst question rather poorly understood. learning algorithms succeed practice theoretical analysis overly pessimistic. direct interpretation theoretical results suggests going slightly deeper beyond single layer networks e.g. depth networks hidden units hard predict even marginally better random finally note recent empirical successes prompted surge theoretical work around learning compositional kernels connections networks. idea composing kernels repeatedly appeared throughout machine learning literature instance early work sch¨olkopf grauman darrell inspired deep networks’ success researchers considered deep composition kernels fully connected two-layer networks correspondence kernels neural networks random weights examined notably rahimi recht proved formal connection kernel. work extended include polynomial kernels well kernels several authors explored ways extend line research deeper either fully-connected networks convolutional networks work sets common foundation expands ideas. extend analysis fully-connected convolutional networks rather broad family architectures. addition prove approximation guarantees network corresponding kernel general setting. thus extend previous analyses applies fully connected two-layer networks. finally connection analytical tool reason architectural design choices. denoted dimensional sphere denoted provide brief overview reproducing kernel hilbert spaces sequel merely introduce notation here. hilbert space slightly non-standard notation ball radius denote refer vector input’s coordinate denote scalar entry. though notation slightly non-standard uniﬁes input types seen various domains. example binary features encoded taking case {±}n. meanwhile images audio signals often represented bounded continuous numerical values—we assume full generality values match setup above embed circle e.g. represent category unit vector sd−. large input space output space based sample drawn i.i.d. distribution supervised learning problem speciﬁed output length loss function goal predictor whose loss small. empirical loss commonly used proxy loss regression problems correspond instance squared loss binary classiﬁcation captured zero-one loss hinge loss standard extensions multiclass case. loss l-lipschitz convex convex every neural network learning. deﬁne neural network vertices weighted directed acyclic graph whose nodes denoted edges weight function denoted sole role would dictate incoming outgoing edges associated activation function respect gaussian measure normalized supervised learning problem network input nodes output nodes denoted network together weight vector {wuv deﬁnes predictor whose prediction given propagating forward network. formally deﬁne output subgraph node finally hokw). also refer internal nodes hidden units. output layer sub-network consisting output neurons along incoming edges. representation induced network network obtained removing output layer. representation function induced weights hrepw. given sample learning algorithm searches weights small empirical loss yi). popular approach randomly initialize weights variant stochastic gradient method improve weights direction lower empirical loss. kernel learning. function reproducing kernel simply kernel every matrix positive semi-deﬁnite. kernel induces hilbert space functions corresponding norm kernel corresponding space normalized given function though formal deﬁnition neural networks skeletons appear identical make conceptual distinction role analysis rather diﬀerent. accompanied weights neural network describes concrete function whereas skeleton stands topology common several networks well kernel. underscore diﬀerences note skeletons naturally compact networks. particular examples skeletons paper irreducible meaning nodes restrict attention skeletons problems outputs denote number non-input nodes tions. skeleton rather basic aggregates inputs single step. input next layer aggregates partial results. altogether corresponds networks single one-dimensional convolutional layer followed fully connected layer. two-dimensional counterparts skeletons correspond networks common visual object recognition. number output nodes induces neural network architecture. recall inputs ordered sets vectors sd−. deﬁnition computation skeleton consider input coordinates deﬁne following neural network input node corresponding input neurons weight internal node labeled activation neurons activation weight addition output neurons identity activation weight edge viuj whenever every output node neuron connected output neurons term -fold realization also deﬁne r-fold realization note notion replication parameter corresponds terminology convolutional networks number channels taken convolutional layer number hidden units taken fully-connected layer. figure illustrates -realizations skeleton coordinate dimension -realization network single convolutional layer channels stride width followed three fully-connected layers. global replication parameter realization used brevity; straightforward next deﬁne scheme random initialization weights neural network similar often done practice. employ deﬁnition throughout paper whenever refer random weights. deﬁnition random initialization neural network multivariate gaussian uv∈e weight sampled independently normal distribution mean variance dδ/δ) input neuron δ/σu otherwise. architectures convolutional nets weights shared across diﬀerent edges. again straightforward extend results cases simplicity assume explicit weight sharing. computation skeletons reproducing kernels addition networks’ architectures computation skeleton also deﬁnes normalized kernel corresponding norm functions norm property small obtained certain simple compositions functions according structure deﬁne kernel introduce dual activation dual kernel. denote multivariate gaussian section shows indeed kernel every activation adheres square-integrability requirement. fact continuous kernel dual activation. note normalized normalized. show section dual activations closely related hermite polynomial expansions used calculate duals activation functions analytically. table lists examples normalized activations corresponding dual following deﬁnition gives kernel corresponding skeleton normalized activations. deﬁnition computation skeleton normalized activations output node every node inductively deﬁne kernel follows. input node corresponding coordinate deﬁne non-input node deﬁne show later indeed kernel every skeleton understand moderate norm functions show section functions obtained certain simple compositions according feed-forward structure intuition tions relu input space {±}n. skeleton comprising single fully connected layer comprising convolutional layer skeleton unnormalized activations corresponding kernel kernel skeleton stride width log. followed single fully-connected layer. kernel takes form y/n). symmetric kernel therefore functions small norm essentially low-degree polynomials. instance bound norm functions. case space contains multiplino-longer hrs. moreover property holds true regardless choice activation function. hand contains functions whose dependence adjacent input coordinates complex. includes instance function symmetric depends adjacent coordinates xi+q. furthermore functions also hrs. main results review main results. compositional kernel upshots initialization approximates kernel sense result holds twofold. first proper rescaling show κs). then also show functions obtained composing bounded linear functions approximately bounded-norm functions words functions expressed varying weights last layer approximately bounded-norm functions simplicity restrict analysis case also conﬁne analysis either bounded activations bounded ﬁrst second derivatives relu activation. extending results broader family activations left future work. remaining sections hide universal constants. note many activations c-bounded constant particular popular sigmoid-like functions tanh tan− satisfy boundedness requirements. next introduce terminology parallels representation layer kernel space. concretely network whose obtained representation dividing output neuron empirical kernel corresponding deﬁned ψw). also deﬁne empirical kernel space corresponding hκw. concretely note activation assume depth logarithmic quadratic dependence depth. however requires /depth. theorem skeleton relu activations. random initialization remaining theorems l-lipschitz loss distribution denote cardinality support distribution. note bounded instance number bits used represent element following notion approximation. deﬁnition distribution space \u0001-approximates space w.r.t. every theorem skeleton c-bounded activations. random initialization required bounds polynomial. analogously relu activation bound polynomial even without restricting depth. however polynomial growth theorems rather large. improving bounds proving optimality left future work. deﬁnition computation skeleton normalized activations output node every node follows. input node corresponding inductively deﬁne kernel non-input node deﬁne coordinate deﬁne well-known found chapter similar textbooks. hilbert space functions reproducing kernel hilbert space abbreviated rkhs kernel space every linear functional recall deﬁne activations square integrable functions w.r.t. gaussian measure. thus hermite polynomials form orthonormal basis space activations. particular activation uniquely described basis hermite polynomials compositional kernel spaces describe details compositional kernel spaces. skeleton norrest section study functions norm. particular show indeed normalized kernel. recall deﬁned inductively equation recursion describes means generating kernel form another kernel. since kernels correspond kernel spaces also prescribes operator produces kernel space reason using notation becomes clear sequel. space obtained starting spaces corresponding input nodes propagating according structure node operation applied. hence understand need understand operation well spaces corresponding variable fwhv termed direct average deﬁned equation ˜κv) resulting kernel space denoted according deﬁned resulting kernel space denoted next analyze operations. extension kernel space. normalized kernel space kernel bixi function. shortly function dual activation function. extension w.r.t. denoted kernel space corresponding kernel µ)). lemma function indeed kernel. furthermore following properties hold. next discuss examples activations calculate dual activation kernel. note dual exponential activation calculated duals step relu activations calculated here derivations diﬀerent prove useful future calculations duals activations. collapsing tower fully connected layers. conclude section discuss case deep networks. setting taken illustrative purposes might surface building networks numerous fully connected layers. indeed deep architectures aware employ consecutive fully connected layers. hence limiting properties understood limit case identity function. therefore simply linear kernel. assume neither identity negation. following claim shows point-wise limit corresponding degenerate kernel. here third inequality follows form fact −bi|ρ|i −bi|ρ|. moreover since inequalities must strict. properties follows lemma finally show note second derivative ibiρi− non-negative hence convex particular intersects x-axis either inﬁnitely many times assume identity rule option inﬁnitely many intersections. also since know least intersection hence intersections conclude intersection proof main results applies activations decent i.e. well-behaved sense deﬁned sequel. show c-bounded activations well relu activation decent. ﬁrst need extend deﬁnition dual activation kernel proof. measure concentration property follows standard concentration bounds sub-exponential random variables remains show )-lipschitz ﬁrst calculate exact expression expression already calculated proof. node denote normalized representation sub-skeleton rooted analogously denotes empirical kernel network. output node still κuw. given ﬁxed node denote here convention enough show probability least nodes well-initialized. ﬁrst note input nodes well-initialized construction since next show given incoming nodes ready conclude proof. u|s| ordered list nodes accordance depth starting shallowest nodes ending output node. denote event well-initialized. need show using induction inequality qδ|s| hypothesis holds. assume claim δ|s| proof. denote maxi suppose stochastic gradient decent sample w.r.t. loss learning rate projections onto ball radius namely start iteration choose random perform update iterations loss expectation would particular exists sequence gradient steps attains solution update adds subtracts current solution. hence written weighted xi’s coeﬃcient theorem distribution -lipschitz loss kernel i.i.d. samples probability least have role initialization training. results surface question extent random initialization accounts success neural networks. mostly leave question future research would like point empirical evidence supporting important role initialization. first numerous researchers practitioners demonstrated random initialization similar scheme analyze crucial success neural network learning suggests starting arbitrary weights unlikely lead good solution. second several studies show contribution optimizing representation layers relatively small example competitive accuracy cifar- stl- mnist mono datasets achieved optimizing merely last layer furthermore saxe show performance training last layer quite correlated training entire network. eﬀectiveness optimizing solely last layer also manifested popularity random features paradigm finally studies show metrics induced initial fully trained representations substantially diﬀerent. indeed giryes demonstrated mnist cifar- datasets distances’ histogram diﬀerent examples barely changes moving initial trained representation. imagenet dataset diﬀerence pronounced still moderate. role architecture. using skeletons compositional kernel spaces reason functions network actually learn rather merely express. explain retrospect past architectural choices potentially guide future choices. consider example task object recognition. appears intuitive supported visual processing mechanisms mammals order perform object recognition ﬁrst processing stages conﬁned local receptive ﬁelds. then result local computations applied detect complex shapes combined towards prediction. processing scheme naturally expressed convolutional skeletons. dimensional version example demonstrates usefulness convolutional networks vision speech applications. rationale described pioneered lecun colleagues alas mere fact network express desired functions guarantee actually learn them. using example barron’s theorem claim visionrelated functions expressed fully connected layer networks networks inferior convolutional networks machine vision applications. result mitigates gap. first enables original intuition behind convolutional networks order design function spaces provably learnable. second detailed example also explains convolutional networks perform better fully connected networks. give examples. first suppose skeleton fully connected layer dual activation followed additional fully connected layer dual activation straightforward verify layers replaced single layer dual second example concerned relu activation common activations used practice. theory suggests somewhat surprising explanation usefulness. first dual kernel relu activation enables expression non-linear functions. however property holds true many activations. second theorem shows even quite deep networks relu activations random initialization approximates corresponding kernel. lack proof time writing conjecture property holds true many activations. special relu? well additional property relu positive homogeneous algorithms sensitive initialization. initialization similar approaches used practice encompasses small correction form multiplication small constant depends activation. activations ignoring correction especially deep networks results large change generated representation. relu activation robust changes. note similar reasoning applies max-pooling operation. future work. though formalism fairly general mostly analyzed fully connected convolutional layers. intriguing questions remain analysis max-pooling recursive neural network components dual perspective. algorithmic side seen whether framework help understanding procedures dropout batch-normalization beside studying existing elements neural network learning would interesting devise architectural components inspired duality. concrete questions concerned quantitative improvements main results. particular remains open whether dependence made polynomial quartic dependence improved. addition interesting right improving bounds underscore eﬀectiveness random initialization generating dimensional embeddings compositional kernel spaces. randomly generating embeddings also considered currently working design analysis random features rahimi recht would like thank yossi arjevani elad eban moritz hardt elad hazan percy liang nati linial recht shai shalev-shwartz fruitful discussions comments suggestions.", "year": 2016}