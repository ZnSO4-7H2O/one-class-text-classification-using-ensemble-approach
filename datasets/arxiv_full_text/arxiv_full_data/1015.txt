{"title": "Biologically Inspired Radio Signal Feature Extraction with Sparse  Denoising Autoencoders", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Automatic modulation classification (AMC) is an important task for modern communication systems; however, it is a challenging problem when signal features and precise models for generating each modulation may be unknown. We present a new biologically-inspired AMC method without the need for models or manually specified features --- thus removing the requirement for expert prior knowledge. We accomplish this task using regularized stacked sparse denoising autoencoders (SSDAs). Our method selects efficient classification features directly from raw in-phase/quadrature (I/Q) radio signals in an unsupervised manner. These features are then used to construct higher-complexity abstract features which can be used for automatic modulation classification. We demonstrate this process using a dataset generated with a software defined radio, consisting of random input bits encoded in 100-sample segments of various common digital radio modulations. Our results show correct classification rates of > 99% at 7.5 dB signal-to-noise ratio (SNR) and > 92% at 0 dB SNR in a 6-way classification test. Our experiments demonstrate a dramatically new and broadly applicable mechanism for performing AMC and related tasks without the need for expert-defined or modulation-specific signal information.", "text": "decision threshold optimal theoretical bayesian perspective minimizes chance wrong classiﬁcation. however often high computational complexity requires careful design selection signal noise models. feature-based uses expert-selected designed signal ﬁlters based known characteristics expected modulations decision tree associated thresholds determine detected modulation family methods require substantial design-side knowledge modulation properties make speciﬁc assumptions regarding environmental noise. contrast task animal moving within natural environment. animal sensory systems vision audition evolved millions years detect identify respond novel events could pose threat indicate reward. result sound sight observed animals make immediate decision classify friend neutral; perform task without explicit model expert knowledge environment. instead rely previously learned low-level environmental features generate activity diﬀerent layers neurons within sensory cortex information propagates layers cortex concepts neurons sensitive become abstract decisions based hierarchical features allow animal make friend-foe decision decision made without prior knowledge exact input properties presence noise corruption; further process naturally suited non-cooperative environments. receptive ﬁelds organisms often possess striking features explained simple statistical approaches. many mammalian visual systems receptive ﬁelds spatially localized oriented scaledependent gabor-like ﬁlters; features generally recovered searching orthogonal basis functions space natural images. appears important criterion biological receptive ﬁelds maximize statistical independence basis functions method accomplishing derive sparse over-complete basis particular basis function highly selective small number environmental features visual systems manifests speciﬁcity neural populations particular stimuli. also indicative eﬃcient implementation terms energy reaction time compared classiﬁcation important task modern communication systems; however challenging problem signal features precise models generating modulation unknown. present biologically-inspired method without need models manually speciﬁed features thus removing requirement expert prior knowledge. accomplish task using regularized stacked sparse denoising autoencoders method selects eﬃcient classiﬁcation features directly in-phase/quadrature radio signals unsupervised manner. features used construct higher-complexity abstract features used automatic modulation classiﬁcation. demonstrate process using dataset generated software deﬁned radio consisting random input bits encoded -sample segments various common digital radio modulations. results show correct classiﬁcation rates signalto-noise ratio -way classiﬁcation test. experiments demonstrate dramatically broadly applicable mechanism performing related tasks without need expert-deﬁned modulation-speciﬁc signal information. ﬁcult task bridges signal detection creation useful information received signals. task even challenging non-cooperative noisy environment realistic channel properties even prior knowledge modulations detected. information available classiﬁcation generally feasible existing methods require prior information regarding modulation mechanism. broadly automatic modulation classiﬁcation techniques fall categories likelihood-based featurebased likelihood function received signal belonging modulation used create likelihood ratio compared pre-determined here demonstrate biologically-inspired artiﬁcial neural network conﬁgured recreates gabor-like receptive ﬁelds trained natural images generate useful information regarding nonbiological sensory input case in-phase quadrature signals acquired radio frequency spectrum. architecture ingests whitened signals uses stacked sparse denoising autoencoders adaptively generate primitive complex features uses features perform automatic modulation classiﬁcation. requires timepoints make classiﬁcation training rapidly output ssda high-dimensional representation responds uniquely signals modulation type. system translates output human-readable modulation labels using training softmax multi-layer perceptron classiﬁer. approach diﬀers signiﬁcantly current approaches automatic modulation classiﬁcation modelexpert-free ingest side requires example signals detect future signals modulated manner. stands sharp contrast methods require expert input stage. methodologies compared fig. tested methods variety modulations additive white gaussian noise channels biologicallyinspired algorithm demonstrates performance comparable existing methods yielding correct classiﬁcation signal-to-noise ratio suggests dramatically diﬀerent approach successful challenging environments scenarios. although modulated radio signals many ways quite diﬀerent ‘signals’ biological neural systems evolved detect process nonetheless propagate environment produces many biologically relevant sources noise reasonable whether principles biological sensing lead useful alternatives existing statistical approaches radio signal processing. demonstrate architecture generate useful receptive ﬁelds allow classiﬁcation methods better discriminate amongst classes; receptive ﬁelds arise unsupervised sparse encoding input represent equivalent useful primitive features within space man-made radio signals unsupervised feature selection pre-training diﬀerentiates neuralnetwork based methods makes method generally applicable broad range input modalities potentially including images network data ﬂows time-varying processes. likelihood-based modulation classiﬁcation requiring priori knowledge probability distribution function received waveform conditioned details modulation. biologically-inspired automatic modulation classiﬁcation requirement observed signals labeled modulations. figure comparison workﬂows automatic modulation classiﬁcation. existing methodologies shown note require expert situational knowledge instead uses observational data. consider system accepts signal emits prediction ﬁxed number nmod modulation families encodes letting true modulation characterize represents full joint distribution probability correct prediction given signal neural network consists series pre-trained sparse denoising autoencoders followed fully-connected softmax classiﬁer layer. starting signal sample vector described previous section compute input units ﬁrst autoencoder values given values hidden layer units calculated according here non-linear activation function operates element-wise argument stochastic corruptor adds noise according noise model input. non-deterministic corrupt sample vector diﬀerent ways every time passed training output layer autoencoder discarded hidden layer activations used input layer next autoencoder. hypothesized overly sparse compact representation would unable distinguish identical modulations shifted time. thus number neurons ﬁrst second layers chosen fully sparse activation constraints would still signiﬁcant number neurons active given sample explored seven total architectures summarized table iii. included simple softmax classiﬁer two-layer without pre-training one-layer denoising autoencoders two-layer autoencoders deep ssda regularization exact number neurons chosen arbitrarily conform available computing resources. prevent learning trivial mapping either layer-to layer dimensionality sparsity constraint altered pair layers. parameters single autoencoder weight matrix bias vectors unsupervised training process adjusting parameters output layer reproduces input precisely possible also subjecting constraint designed encourage sparse activation hidden layer units encourage hidden layer unit activations remain near except small fraction. overall cost function single autoencoder layer binary produced randomly choosing byte values waveforms’ input. binary data modulated in-phase quadrature samples using methods on-oﬀ keying gaussian frequency-shift keying gaussian minimumshift keying diﬀerential binary phase-shift keying diﬀerential quadrature phase-shift keying orthogonal frequency-division multiplexing modulation samples sent bladerf upconverted carrier frequency. conﬁgured loop-back mode signal sent received within device’s circuitry external antenna. arrangement provides added realism incorporating upconversion radio eﬀects without unwanted thirdparty signals could pollute controlled testing. signal sampling rate number samples symbol consistent every modulation type except ofdm. contrast modulation techniques ofdm encodes data multiple carrier frequencies simultaneously within symbol modulates carrier frequency independently. experiment used existing ofdm signal processing component operates symbol rate diﬀerent conﬁgurations sample rate. rate identical transmission reception signal. received signal downconverted radio resulting samples stored analysis. data ﬁles need arranged format structure neural network. data split segments consisting nspv samples samples vector. segment composed interleaved values sample forming vector length ×nspv symbols. vectors thus vector contains nspv placed sets train test modulation type positions within random. parameter nspv identical modulation type experiments described paper. speciﬁc values parameters shown table description samples symbol nsps samples vector nspv number training vectors train number training vectors modulation number test vectors test minimize equation using batch stochastic gradient descent overall architecture shown fig. test several variations stacked autoencoder architecture described below parameters hold ﬁxed experiments listed table assess performance system realistic channel model altered test data additive white gaussian noise data conﬁgurations used input purely feed-forward mode system re-trained modulation classiﬁcation output evaluated. awgn added signal modulation types resulting signal-to-noise ratio matches given value. necessary since modulation type sampled radio diﬀerent average power levels. signal modulation sets {smod} added noise power pnoise number sample vectors particular modulation individual signal sample vector length factor chosen matches desired snr. examples modulation data addition noise shown fig. note modulations exhibited similar transmitted power exception slightly larger. hidden layer activations autoencoder supplied input another autoencoder leading stacked architecture. denote input hidden output units single autoencoder layer respectively. process forward propagation entire network autoencoders proceeds sequentially according conduct sequential unsupervised training individual autoencoder layers using stochastic gradient descent batch size adagrad method based data described previously. parameters used training listed table measured overall classiﬁcation accuracy architecture. architectures varied number layers types cost enforced training; used cost non-sparse activation penalty cost weight magnitude penalty architectures included softmax perceptron single layer sparse denoising autoencoder without weight decay costs two-layer sparse denoising autoencoder without layer sparsity costs weight decay costs. also tested deep ssda layers. architectures chosen study eﬀects adding additional regularizations ability system classify radio modulations. results architecture summarized table iii. architectures performed approximately orders magnitude better softmax classiﬁer alone test absence noise. regularization sparsity constraints number training examples required obtain convergence increased particular architecture required signiﬁcantly time converge others. however oﬀset increased performance presence channel noise. follow unsupervised pre-training phase supervised ﬁne-tuning. phase organize pretrained autoencoders purely feed-forward multilayer perceptron according equation additional ﬁnal layer interpreting ﬁnal output vector multilayer perceptron probability distribution modulation families supervised learning attempts minimize negative log-likelihood function additional regularization term encourage model retain sparsely activating features learned unsupervised phase. regularization term value depending desired experiment conﬁguration. explicitly list samples total number layers output layer indicates weight matrix layers loss function multi-layer perceptron given figure architecture input data neuron organization output classiﬁcation. autoencoders conﬁgured sparsity across neurons noise corruption layers. softmax conﬁgured regularization constraint reﬁnement autoencoder weights. classiﬁer performance presence channel noise ability classify modulations signalto-noise ratios crucial abilities successful algorithm. tested system’s performance measuring function snr. method degrades gracefully decreases approaches random chance performance conﬁguration shown fig. summarized table iii. single-layer weight decay substantially improved generalization classiﬁer higher noise levels prevented network converging reasonable level accuracy allow better data occur added second layer improved overall performance. however two-layer sparse stacked denoising autoencoder weight decay sparsity costs layer performs signiﬁcantly better across snrs error rate performance better closest comfigure classiﬁcation error test function noise. example test gaussian noise produce desired signal-to-noise ratio presenting example neural network. conﬁgurations correspond table iii. classiﬁcation error random guess indicated horizontal dotted line. value corresponds perfect classiﬁcation. conﬁguration substantially worse performance included plot. confusion matrix per-type classiﬁcation accuracy although good indication classiﬁer performance overall also interested identifying speciﬁc modulations less challenging method. this construct confusion matrix dimension nmod nmod consisting values plot confusion matrix classiﬁer highest overall performance fig. snrs signals on-oﬀ keying easiest classify virtually none misclassiﬁed noise levels. remaining modulation families error decreases; classiﬁer tends over-predict gmsk expense types. another error confusion dqpsk dbpsk modes high noise. parallel this also examine precision sensitivity classiﬁer modulation family function snr. true predicted class label respectively sample precision classiﬁer class highest performance classiﬁer fig. results conﬁrm on-oﬀ keying extremely robust noise classiﬁcation system precision modulation family falls noise increases. however sensitivity varies much strongly across diﬀerent modulation families. particular observe sensitivities ranging depending modulation type. performance characterization comparison system traditional methods challenging modulation classiﬁcation large diverse ﬁeld study. however recent review paper automatic modulation classiﬁcation makes valiant eﬀort summarizing comparing likelihood-based featurebased classiﬁcations footing. review -way classiﬁcation task bpsk qpsk modulations average-likelihood ratio test method observed. performance exceeds method alrt classiﬁcation strongly tied modulation type; discrimination task required obtain classiﬁer. worse cases model parameters exact carrier phase unknown performance begins drop snrs required achieve quasi-alrt classiﬁer choosing modulations. similarly methods perform well high snrs suﬀer signiﬁcant performance variations diﬀerent modulation types. figure confusion matrix modulation family classiﬁed architecture signal-to-noise ratio visualizations empirically determined joint distribution equation environmentally-corrupted non-cooperative entirely novel signals would allow method perform modulation classiﬁcation methods discussed would entirely unfeasible. additionally method evaluated sequences symbols timepoints; substantially fewer timepoints existing methods makes system likely valuable classiﬁcation dynamically shifting environments. results indicate unsupervised pretraining crucial task. observed exploring overall classiﬁcation performance ssda network multi-layer perceptron trained dropout regularization without pretraining. conﬁgured network dropout layer regularization architecture suggested architecture initially failed converge ﬁrst epochs. performed sweep characterize parameter sensitivity architecture. found convergence model highly sensitive learning rate; change could cause model improvement random chance. choosing learning rate indicated parameter search trained number epochs pre-trained architectures. although initial convergence rate similar convergence became asymptotic error rate asymptotic behavior observed stochastic gradient descent momentum learning rules adaptive subgradient results agreement work performed also indicate challenge using simple machine learning models perform amc. results indicate although possible conﬁgure would converge task relative robustness system signiﬁcantly reduced performance system competitive compared performance using methods well ann-based methods crucially unlike existing methods prior knowledge modulation design characteristics completely unnecessary method; duplicate performance observed must merely obtain list signals known diﬀerent other. unreasonable expect crude methods signal discrimination could used build training dataset diﬃculty parameter selection increases. using unsupervised pre-training able substantially reduce parameter sensitivity improve total training time accuracy. regularization typically prescribed neural networks prevent overﬁtting improve generalization. unsupervised pre-training also considered form regularization used starting point ﬁnal generalization error reduced however observed that task regularization assists classifying exemplars corrupted eﬀects found training set. demonstrated examining classiﬁcation performance architectures dataset corrupted additive white gaussian noise typical challenge radio-frequency propagation testing. classifying test samples test corrupted noise heavily regularized pre-trained network tested exhibited best overall performance. absence noise best performance observed unconstrained single-layer architecture quantify performance presence noise examine required achieve performance speciﬁc e.g. classiﬁcation error measure unconstrained single-layer network poorer performance requiring reach addition second layer constraints results modest improvement sparse pre-training regularization included constraints performance achieved represents improvement unconstrained single-layer network. corresponds -fold increase maximum noise level given detection rate. addition sparsity appears crucial performance increase result forcing selection valuable receptive ﬁelds seen fig. incrementally released forms regularization performance noisy data decreased. particularly useful aspect implementation ssdas; propagation digitally-transmitted radio signals real environments presents signiﬁcant modeling challenge ability method compensate model-free noise highly desirable. performance single-layer architecture also indicates addition regularizations drawbacks must compensated for; without second layer fully regularized single-layer network converge adequately high performance levels however generalize better alone presence noise. must rely limited selection receptive ﬁelds small network strong constraints enough neurons active adequately represent necessary features classiﬁcation. primitive features however remain intact signal corruption thus allow higher low-snr performance. also tested deeper architecture additional layers would improve overall classiﬁcation outweigh regularization eﬀects reduce generalization untrained environmental noise. prior work deep neural network architectures typical adding layer improves performance less noise-free conditions agrees results deeper model consisted architecture additional layers subject similar sparsity contraints interestingly model converged high accuracy quickly. thus addition additional pre-trained layers resulted rapidly converging highly accurate classiﬁer. unfortunately conﬁguration also performed substantially worse exposed signals awgn channel. hypothesize somewhat desirable form overﬁtting; adding additional layers classiﬁer becomes highly tuned properties input somewhat inﬂexible. improve generalization could explore convolutional networks provide strong regularization using deeper representation achieve high accuracy. possible network achieve rapid convergence seen deep ssda without loss performance presence unmodeled noise. insights come studying classiﬁer begins fail noisy conditions show fig. fig. confusion matrix shows full distriselected snrs precision bution sensitivity curves show full behavior marginal distribution. recall precision measures within samples predicted given modulation fraction actually modulation. sensitivity measures within samples actually given modulation fraction predicted modulation. precision class high correctly identify single example class; sensitivity class high assign every sample class. results show degradation performance noise random; example classiﬁcation system systematically over-predicts gmsk moreover degradations simply magniﬁcations errors exist noise ofdm clearest example this system loses sensitivity modulation much slowly families. behavior likely indicator crosstalk receptive ﬁelds classiﬁcation system. traditional architecture would rely features selected speciﬁc modulation family system learns features used classifying multiple families single feature vector might play role reconstructing identifying gmsk ofdm example manner vectors fail noisy versions diﬀerent target families reﬂected performance degrade uniformly family. possible mitigation potential crosstalk simple adding neurons autoencoder layers increase number possible receptive ﬁelds system learns. unsupervised feature extraction raises important question sort signal features system becoming sensitive receptive ﬁelds autoencoder system simply weights input layer target layer describe input maximally excites target neuron. features thought primitive features input. began verifying network would produce gabor-like receptive trained cifar image dataset aligns nicely work presented papers including work dimensionality reduction encoding although analyze second-layer receptive ﬁelds chose utilize sparsity order beneﬁt improvements receptive ﬁelds observed researchers studying receptive ﬁelds sparse restricted boltzmann machine models visual cortex fig. present characteristic inputs modulation scheme; fig. present selection layer receptive ﬁelds. layer receptive ﬁelds appear exhibit strong alignment exact modulation type symbol pattern; although would made neat explanation perhaps surprising. outside scope paper analysis spectral temporal characteristic autonomously determined receptive ﬁelds would provide deeper understanding modulation features system learning. receptive ﬁelds shown fig. represent signiﬁcant diﬀerence research prior studies artiﬁcial neural network based automatic modulation schemes. researchers used singlemulti-layer anns achieve impressive performance methods required expert construction features speciﬁc modulation. thus methods could described feature weighting. feature weighting signiﬁcant contribution although reliance detailed knowledge signal characteristics makes inﬂexible. unsupervised methods present allow much greater ﬂexibility terms incorporating unusual characteristics figure examples layer receptive ﬁelds derived table number indicates neuron receptive ﬁeld corresponds. receptive ﬁeld consists input maximally activates target neuron corresponds extracted features learned network. environmental noise accommodating signals detailed model available adapting changes environmental signal characteristics time on-line learning techniques. based methods also actively researched within radio front-end processing stages. example multilayer perceptron channel equalization eﬀorts orthogonal potentially complementary ours. task automatic modulation classiﬁcation radio signal data conceptually similar tasks related ﬁelds phoneme word detection speech processing although domain presents unique challenges terms sample rate robustness noisy environments. also note recent work acoustic modeling deep networks found signiﬁcant improvements possible leveraging layers autoencoder units proof-ofconcept architecture present likely permit many optimizations. additional improvements come form convolutional autoencoders seen fig. low-level features time-shifted variants other. implies convolutional application features streaming inputs provide performance computational cost improvements. another possible route towards improved performance especially application streaming online analysis implementation architecture spiking neural network. spiking neural networks another step towards biologically-relevant systems seek represent information discrete temporal events much like biological nervous systems action potentials. snns natively represent information contained signal timing higher resolution clocked systems equivalent sophistication open much larger parameter space encoding information. provide opportunities unsupervised learning optimization eﬃcient bandwidth usage implemented architecture demonstrated near-identical performance task described full spiking simulation. results focus companion paper work. presented fundamentally diﬀerent addressing challenge automatic modulation classiﬁcation radio frequency domain. method biologically-inspired feature extraction feature abstraction labeling demonstrate principles animal sensory systems applied achieve useful performance task. however nothing fundamentally important using samples even -dimensional input vectors. method essentially sensor-agnostic sensory system providing classiﬁcation noisy environments. important note perform exhaustive hyperparameter searches optimizations. rather searching optimal network task approached task modulation classiﬁcation biologically-inspired perspective. computer vision research realization experimentally measured receptive ﬁelds similar gabor ﬁlters discovery sparse coding natural images generates similar ﬁelds crucial result. additional work recovering biologically-relevant receptive ﬁelds diﬀerent sensory modalities diﬀerent dimensionality-reduction techniques also crucial step towards research present conﬁgured network that given natural scene data would produce well-known gabor-like receptive ﬁelds. prediction veriﬁed experiments system produces biologically-relevant receptive ﬁelds biologically-relevant task would also produce useful receptive ﬁelds non-biological task. extent optimization used results network capable useful performance automatic modulation classiﬁcation. results diﬀer much prior work neuralnetwork processing time-varying signals focusing narrowly ingesting waveform data rather spectrogram ﬁlter bank features extracting useful features later tasks. thus taken ﬁrst step road called demonstrated even relatively simple networks useful processing radio signals extremely limited samples presence environmental noise. results also diﬀer prior work make expert knowledge construct eﬀective features adapt signals propagation environment competitive performance. opens opportunities eﬃcient increasingly complex electromagnetic signaling environment. also hope research lead application additional biological inspiration problem sensing processing currently researching spiking neural networks creating complete perception electromagnetic environment. also demonstrate biologically-inspired feature extraction form sparsity unsupervised pretraining enhance neural-network even noise conditions modeled training data. demonstrate persistence level sparsity constraints increase general performance classiﬁer improves environmental conditions classiﬁer speciﬁcally trained. noise explored architectures successfully converge perform similarly well biologically motivated principles result system performs markedly better environmental noise. particularly interesting light prevailing explanations sparse coding principle among robustness environmental noise. results indicate principle still valid useful problem domains rarely associated sensing natural organisms. believe important broadly applicable conclusion work biologically-inspired sensing principles implemented using hierarchical neural networks require biologically-inspired input. suggests areas machine human perception limited beneﬁt application methods propose.", "year": 2016}