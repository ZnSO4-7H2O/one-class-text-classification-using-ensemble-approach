{"title": "SAM: Semantic Attribute Modulation for Language Modeling and Style  Variation", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "This paper presents a Semantic Attribute Modulation (SAM) for language modeling and style variation. The semantic attribute modulation includes various document attributes, such as titles, authors, and document categories. We consider two types of attributes, (title attributes and category attributes), and a flexible attribute selection scheme by automatically scoring them via an attribute attention mechanism. The semantic attributes are embedded into the hidden semantic space as the generation inputs. With the attributes properly harnessed, our proposed SAM can generate interpretable texts with regard to the input attributes. Qualitative analysis, including word semantic analysis and attention values, shows the interpretability of SAM. On several typical text datasets, we empirically demonstrate the superiority of the Semantic Attribute Modulated language model with different combinations of document attributes. Moreover, we present a style variation for the lyric generation using SAM, which shows a strong connection between the style variation and the semantic attributes.", "text": "paper presents semantic attribute modulation language modeling style variation. semantic attribute modulation includes various document attributes titles authors document categories. consider types attributes ﬂexible attribute selection scheme automatically scoring attribute attention mechanism. semantic attributes embedded hidden semantic space generation inputs. attributes properly harnessed proposed generate interpretable texts regard input attributes. qualitative analysis including word semantic analysis attention values shows interpretability sam. several typical text datasets empirically demonstrate superiority semantic attribute modulated language model different combinations document attributes. moreover present style variation lyric generation using shows strong connection style variation semantic attributes. language generation considered task artiﬁcial intelligence ﬁeld language modeling task aims present word distributions text sequences considered degenerated text generation task generates word step. traditional language generation approaches phrase templates related generation rules. language modeling task counting-based n-gram method broadly used. methods conceptually simple hard generalize like humans. later bengio developed feed-forward neural network language model mikolov used recurrent neural network train language model. beneﬁts large-scale corpora modiﬁed gating functions long-short term memory gated recurrent unit recurrent neural network demonstrated good capability modeling word probabilities widely used method language modeling language generation nevertheless often criticized incapable capturing long-term dependency resulting losing important contextual information. shown language models enhanced speciﬁc long-term contextual information including document topics bag-of-words contexts neural cache etc. several speciﬁc text structure considered rnnlms hierarchical sentence sequences tree-structured texts dialog contexts aforementioned models main text sequences modeled vastly-accessible attributes documents ignored. interestingly document attributes implicitly convey global contextual information word distributions vastly-accessible reading main texts daily reading speaking. document titles compact abstracts carefully chosen authors keynote speakers. labels tags speciﬁc categories assigned experienced editors. authorships reﬂect writing styles. vastly-accessible attributes predict word distributions better moreover generation perspective several previous works generate designed outputs scratch single semantic attribute however semantic attributes incorporated time incapable meet huge complexity text generation task. paper consider diversity semantic attributes attention mechanism conjoin semantic attribute joint embedding. hence semantic attribute modulation brings ﬂexible generate texts choose different combinations attributes. strong semantic information conveyed attributes text generations interpretable regard different combinations input attributes. ﬂexibility text style variation replacements semantic attributes. interesting example please jason mraz rewrite lyric ‘last kiss’. paper present semantic attribute modulation language modeling style variation. consider vastly-accessible semantic language attributes extract attribute embedding. speciﬁcally adopt types semantic attributes title attribute category attribute. title attribute encoder title embedding. category embedding model learns shared embedding documents speciﬁc category. then generate outputs attention mechanism diversity attribute embeddings. semantic attribute modulated language model obtains better per-word prediction results vanilla rnnlm without sam. improved word predictions highly related semantic attributes therefore interpretable humans. moreover present lyric generation task lyric variation derived semantic attributes. text generation conditioned semantic attribute ﬂexible attribute selection. learned attribute replaced input output style variation. interesting lyric style variations examples demonstrate ﬂexibility sam. incorporating semantic attribute modulation language model gets better word prediction results several text datasets. better word predictions highly related semantic attribute hence interpretable humans. take alphago news article york times concrete example given title ‘google’s computer program beats se-dol tournament’ main text words ‘google’ ‘program’ ‘go’ could predicted easily. given author attribute ‘choe sang-hun’ pulitzer prizewinning south korean journalist better predict words ‘south-korean’ ‘go’. words ahead recurrent neural network build word probabilities time step transition gating function reads word updates hidden state continuous vector representation input vector embedding matrix. probability next possible word vocabulary computed models always criticized lacking capacity long-term sequential dependence resulting unsatisfactory performance modeling contextual information. several previous works tried capture contextual information using previous contexts. contextual representation extracted contexts generation process rnnlm main texts documents semantic attributes titles authorships tags sentiments convey important semantic information. section present semantic attribute modulation originated attention mechanism diversity attributes. then language modeling style variation language generation. given semantic attribute modulated representation generative process model words document. title often carefully chosen author compact abstract document. given m-length title sequence recurrent neural network extract hidden state every title word dimension title word hidden state since title words equal contribution whole context embedding attention mechanism title attribute obtain different title representation different main text words weighted category attributes commonly used daily writing speaking. useful category attributes include document categories authorships sentiments etc. formulate category attribute vector embedding category attribute counted encoder vector language generation style variation semantic embedding extractions obtain semantic attribute embeddings {ck}. leverage importance attribute main content word adopt another semantic attribute attention mechanism learn semantic attribute embedding different main text words incorporate obtained semantic attributes framework. using attribute attention mechanism transition hidden state reads current word also semantic attribute embedding speciﬁcallywe concatenate semantic attribute embedding input word embedding vector thus hidden states update seen figure. build semantic attribute modulated language generation model. semantic attributes considered inputs designed generation outputs. comparing semantic attributes corresponding outputs interpretable users. moreover considering attributes reﬂect text styles realize text style variation replacing related attributes. give generated variations typical lyrics experiment part. neural machine translation neural machine translation uses encoder-decoder network generate speciﬁc response encoder network reads source texts language encodes continuous embeddings. decoder network translates another language. also used generate poems encoding keywords similar work generating texts given useful attributes. difference work uses semantic attribute attention modulation extract semantic embedding instead encoder-decoder framework. contextual work related several contextual language modeling works. titles keywords represented bag-of-words used build conditional rnnlm model. work involved text attributes could model discrete attributes. discrete attributes review rates document categories also used control content generation variational auto-encoder based model generator-discriminator scheme also used generating controllable texts input attributes limited discrete categories. several major advantages paper methods. first adopt diverse attribute including widely used category attributes. semantic information brings interpretability sam. second better attribute representation method including semantic attention mechanism ﬂexibility attention mechanism. third replacing semantic attributes model realize style variation lyric generation. section ﬁrst show semantic attribute modulated language model gets better word predictions. extensive qualitative analyses demonstrate interpretability word predictions regard input evaluate proposed language model semantic attribute attention different datasets different attribute combinations. among datasets ttnews xlyrics titles imdb collected ourselves. future plans release collected corpora resolving copyright issues. detailed statistics table. penn treebank penn treebank commonly-used dataset evaluating language models texts derived wall street journal preprocessed corpus mkb+ training tokens vocabulary size topic model analyze corpus topic number assign label document topic assignment largest weight. analysis category attribute discussions seen appendix bbcnews bbcnews formal english news dataset contains news articles collected gsbt. bbcnews documents class labels business entertainment politics sport technology. imdb movie reviews imdb movie reviews movie review corpus training reviews testing reviews. note mdp+ provide review titles collected titles according provided links. ttnews ttnews chinese news dataset crawled several major chinese media. ttnews news articles million tokens vocabulary size document contains title author annotations. consider several variants proposed methods different combinations semantic attributes. detail consider language modeling category attribute title attribute title attribute plus category attribute. order realize style variation generations consider generating lyrics original title attribute fake author attribute. training adam method initial learning rate maximize loglikelihood early-stop method based validation log-likelihood. dimension word embedding http//www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz http//mlg.ucd.ie/datasets/bbc.html http//ai.stanford.edu/\\begingroup\\let\\relax\\relax\\endgroup represents conventional model without semantic attribute sam-cat model category attribute. seen results adding semantic category attribute sam-cat outperforms baseline models achieving lower perplexities. document titles carefully chosen authors summarize document contents attract attention readers. part incorporate title attribute take advantage implicit word distribution represented title. four corpora task. bbcnews ttnews formal published corpora imdb movie review corpus xlyrics lyric corpus. implement -gram model conventional model corpus without titles. rnn-state conventional rnnlm model title’s last hidden state initialization. means title considered ﬁrst sentence included prediction per-word perplexities. rnn-bow conventional rnnlm model incorporated bag-of-words representation title time step reimplementation sam-title-att method model title attribute attention mechanism. adding title’s last hidden state sam-title-att initialization sam-title-attstate method. show word prediction perplexity results table. rnn-based models title embedding better perplexity results. moreover sam-title better rnn-state added title information would disappear several nonlinear gating functions. attention-based title attribute performs better without attention. attention mechanism provides different importance weights title words. generally model title attribute performs better bbcnews compared imdb. believe result caused different genres datasets. order make title attribute useful titles able convey reﬁned summaries documents. bbcnews formal news corpus written professional journalists usually titles higher quality imdb corpus. semantic attribute attention conjoin attributes sufﬁx ‘au’ means method incorporates author categorical attribute maintains method notations used previous part. show word prediction perplexity results several attribute combinations table. ttnews xlyrics datasets incorporating title author attributes better single one. order discover sam-cat outperforms traditional methods dataset table. demonstrate words category largest least perplexity changes table. mark words strong semantic information speciﬁc category bold. example politics category adding category attribute words largest prediction improvement generally related politics ‘ireland’ ‘bush’ ‘chairman’. words largest prediction degeneration generally semantic meaning related politics ‘gm’ ‘stock’ ‘orders’. words least word prediction change generally function words ‘both’ ‘many’ ‘but’. word prediction changes categories similar politics category. results ﬁnance category table. show results categories appendix. space limit. investigate attention values control importance weights attributes visualize attention values figure. color depth shows attention weights. rectangles show title word ‘microsoft’ large effect content words ‘software’ ‘unauthorized’. title word ‘move’ large effect content word ‘prove’. example shows attention mechanism works ﬂexible selection attributes. many downstream applications language modeling enhanced proposed semantic attributes. machine translation semantic attributes could also titles authors categories. speech recognition task semantic attributes include dialect speaker. language generation tasks question-answering poem/lyric generation possible attributes titles authors even styles. model perform lyric generation based title author attributes. given original lyric generate title fake author. several amazing generation results differences highly related title attribute. give concrete examples left examples appendix. english example fig. original lyric last kiss popular song taylor swift country style. changing authorship jason mraz generate love song looks likes rock lyric. styles lyrics tally styles singers. chinese example fig. original lyric face sentimental love song written xiaotian wang recalling past love. changing authorship lovely sisters trending chinese band generate joyful love song happiness falling love. paper propose semantic attribute modulation language modeling style variation. main idea take advantage vastly-accessible meaningful attributes generate interpretable texts. model adopts diversity semantic attributes including titles authors categories. attention mechanism model automatically scores attributes ﬂexible embeds attribute representations hidden feature spaces generation model inputs. diversity input attributes make model powerful interpretable semantic attribute attention mechanism brings ﬂexibility whole model. extensive experimental demonstrates effectiveness interpretability ﬂexible semantic attribute modulated language generation model. future interested exploring attributes semantic meaning language model task. addition lyric generation task language generation tasks also model utilize semantic attributes. possible example incorporate geographic position attribute speech recognition task model dialects.", "year": 2017}