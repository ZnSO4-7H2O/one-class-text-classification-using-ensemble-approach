{"title": "Non-iterative Label Propagation on Optimal Leading Forest", "tag": ["cs.LG", "cs.AI", "H.2.8"], "abstract": "Graph based semi-supervised learning (GSSL) has intuitive representation and can be improved by exploiting the matrix calculation. However, it has to perform iterative optimization to achieve a preset objective, which usually leads to low efficiency. Another inconvenience lying in GSSL is that when new data come, the graph construction and the optimization have to be conducted all over again. We propose a sound assumption, arguing that: the neighboring data points are not in peer-to-peer relation, but in a partial-ordered relation induced by the local density and distance between the data; and the label of a center can be regarded as the contribution of its followers. Starting from the assumption, we develop a highly efficient non-iterative label propagation algorithm based on a novel data structure named as optimal leading forest (LaPOLeaF). The major weaknesses of the traditional GSSL are addressed by this study. We further scale LaPOLeaF to accommodate big data by utilizing block distance matrix technique, parallel computing, and Locality-Sensitive Hashing (LSH). Experiments on large datasets have shown the promising results of the proposed methods.", "text": "ponder possible reasons limitations argue crux models treat relationship among neighboring data points peer-to-peer. data points considered equal signiﬁcant represent class gssl objective functions optimizing data point equal priority. however peer-to-peer relationship questionable many situations. example data point lies centering location space class representative power diverges central location even k-nn neighborhood. paper grounded partial-order-relation assumption neighboring data points equal status label leader contribution followers assumption intuitively reasonable since saying known company keeps. labels peripheral data change model parameter selection labels core data much stable. fig. illustrates idea. fig. partial-order-relation assumption label center regarded contribution labels followers. therefore safely infer herein left unlabeled point triangle right pentagram. paper proposes non-iterative label propagation algorithm taking previous research work namely local density based optimal granulation starting point. lodog input data organized optimal number subtrees. every non-center node subtrees parent join microcluster parent belongs subtrees called leading tree. proposed method label propagation optimal leading forest performs label propagation structure relatively independent subtrees forest rather traditional nearest neighbor graph. abstract—graph based semi-supervised learning intuitive representation improved exploiting matrix calculation. however perform iterative optimization achieve preset objective usually leads efﬁciency. another inconvenience lying gssl data come graph construction optimization conducted again. propose sound assumption arguing that neighboring data points peer-to-peer relation partial-ordered relation induced local density distance data; label center regarded contribution followers. starting assumption develop highly efﬁcient non-iterative label propagation algorithm based novel data structure named optimal leading forest major weaknesses traditional gssl addressed study. scale lapoleaf accommodate data utilizing block distance matrix technique parallel computing localitysensitive hashing experiments large datasets shown promising results proposed methods. unlabeled data generated sampled tremendous size data era. reason semisupervised learning increasingly drawing interests attention machine learning society. among variety many model streams graph-based reputation easily understood visual representation convenient improve learning performance exploiting corresponding matrix calculation. therefore research works regard e.g. existing gssl models apparent limitations. models usually need solve optimization problem iterative fashion hence efﬁciency. models difﬁculty delivering label bunch data solution unlabeled data derived specially given graph. newly included data graph changed whole iterative optimization process required again. wang proposed hierarchical method address granularity dilemma adding series intermediate granular anchor layer ﬁnest original data coarsest anchor layer underlying philosophy still assumptions. slightly different assumptions proposed novel concept graph harmoniousness integrates feature learning label learning framework objective function term also needs obtain local optimal solution alternately running iterative optimizing procedure variables. distance band-width parameter. deﬁnition leading node δ-distance. nearest neighbor higher local density called {dij|ρj leading node formally denoted short. dili called δ-distance simply store array named {ρi} deﬁnition leading tree ≤i≤n arrow start i\\{r} xli. thus arrows form tree node tends join cluster belongs unless makes center. tree called leading tree. deﬁnition partial order suppose deﬁnition center potential. denote potential selected center computed intuitively object large large would great chance center collection data. performed subtrees edges consideration much sparse nearest neighbor graph; subtrees relatively independent other massive label propagation computation easier parallelized size samples huge; lapoleaf performs label propagation non-iterative fashion high efﬁciency. overall lapoleaf algorithm formulated simple empirical evaluations show promising accuracy high efﬁciency. rest paper organized follows. section brieﬂy reviews related work. model lapoleaf presented details section iii. section describes method scale lapoleaf data. section analyzes computation complexity discusses relationship researches section describes experimental study. reach conclusion section vii. suppose undirected graph denoted vertices edges mapping edge real number gssl takes input data vertices graph places edge vertices similar correlated. basic idea gssl propagating labels labeled samples unlabeled constructed graph. propagation strength edge proportion weight wij. almost existing gssl works fundamental assumptions. called clustering assumption meaning samples cluster labels. clustering assumption usually used labeled sample set. called manifold assumption means similar samples similar labels. manifold assumption used labeled unlabeled data. starting assumptions gssl usually aims optimizing objective function terms. however concrete components different gssl models vary. example objective function label indication matrix; label labeled data. proposed anchor graph regulation approach predict label data point locally weighted average labels anchor points function following principle proposed local density based optimal granulation method build justiﬁable granules accurately efﬁciently lodog construct optimal disconnecting corresponding leading tree optimal number subtrees. optimal number derived minimizing objective function here number igs; parameter striking balance experimental evidence semantic; points included granule; returns cardinality set; strictly monotonically increasing function used adjust magnitude well match distcost. function automatically selected group common functions logarithm functions linear functions power functions exponential functions; root granule leading tree. used lodog construct optimal leading forest dataset. readers referred details lodog. deﬁnition optimal leading forest leading trees constructed dataset using lodog method. leading trees collectively called optimal leading forest. lapoleaf ﬁrst makes global optimization construct performs label propagation subtrees. following aforementioned partial relation assumption relationship children parent formulated stage label propagation lapoleaf guided formula. label vector parent kclassiﬁcation problem. label vector i-th child w.r.t. current parent. k-th element equals others equal zero represents class label k-th class regression problems simply scalar value. popi population data points merged node subtree node derived information granule granulation methods local sensitive hashing others. granulation performed construction popi assigned constant fig. diagram non-iterative label propagation subtrees fnlt. gets label weighted summation label computed likely cascade fashion. root unlabeled subtree. situation borrow label labeled either borrow operation transitively carried previous stages roots subtrees guaranteed labeled. also guidance unlabeled children label information top-down fashion. deﬁnition unlabeled node. node subtree unlabeled node label vector zero vector. otherwise i.e. label vector least element greater zero node called labeled node deﬁnition unlabeled subtree. subtree called unlabeled subtree every node tree labeled. otherwise i.e. leading tree contains least labeled node tree called labeled subtree since label parent regarded contribution children propagation process required start bottom subtree. label vector unlabeled children initialized vector therefore contribute label parent. layer index node ready bottom-up propagation start execute parallel fashion labeled subtrees. proposition propagation root labeled subtree must labeled. parent labeled least child labeled corresponding round propagation. propagation progressing sequentially along bottom-up direction root parent layer. therefore proposition obviously holds. root root labeled data rare unevenly distributed would unlabeled subtrees. case must borrow label information labeled subtrees. label root stable nodes root unlabeled subtree borrow label information root labeled subtree however must requirements keep consistence partial order assumption required superior nearest root formally labeled roots. exists particular conclude root whole leading tree constructed labeled. guarantee every unlabeled root successfully borrow label need guarantee labeled. parent children previous stages root nodes subtrees labeled. propagation labels propagated top-down fashion i.e. labels sequentially propagated layer bottom layer process parallelized independent subtrees. need consider situations parent children unlabeled. here simply assign li=lp assignment directly satisﬁes matter value takes. parent without loss generality assume ﬁrst children labeled children unlabeled. situation generate virtual parent replace original labeled children. using part //preparing olf; compute distance matrix dist compute local density compute leading nodes δ-distance compute representation power using split using objective function return generate double-moon dataset data points moon illustrate main stages lapoleaf helping readers build intuitive impression method. labeled points randomly selected moon. constructed ﬁrst step using steps described part algorithm here parameters lodog {percent root subtree marked yellow face edge. easily observed edges appear much sparser gssl methods based nearest neighbors propagation nodes subtrees ﬁrstly tagged layer index. sub-tree greatest height layers. bottom-up label propagation root labeled subtree becomes labeled. nodes path initial labeled node corresponding root labeled well. nodes labeled now. unlabeled subtrees remain unchanged. fig. shows propagation stage unlabeled root borrowed labeled nearest neighboring root higher density. green arrows show label borrowing information arrow head indicating label owner. fig. illustrative example lapoleaf doublemoon dataset. constructed dataset. propagation. propagation. green arrows indicate borrower owner unlabeled root borrows label another root. roots labeled stage. propagation. color saturation reﬂects value maximal element label vector. closer value higher saturation color. salient advantage lapoleaf obtain label datum time. leading tree structure incrementally updated time lodog algorithm time updated time. label propagation takes time. interested reader refer previous work provided detailed description algorithm incrementally updating node leading tree. also provided therein proof correctness algorithm. scale model lapoleaf data context propose approaches. uses parallel computing platform divide-and-conquer strategy obtain exact solution approximate approach based localitysensitive hashing problem confronted three aspects divideand-conquer strategy. computing distance matrix time complexity. computation needs accessing elements whole distance matrix data also complexity distances centers prepared advance propagation stage since memory single computer able accommodate whole distance matrix large dataset distances centers retrieved directly whole distance matrix. apart three parts steps lapoleaf linear usually could single machine. compute distance matrix parallel distance matrix puts considerable burden computation time capacity memory. example computed required memory capacity distance matrix samples even distance stored ﬂoat bytes. fig. whole dataset divided subsets whole distances pair points computed tree parts. ﬁrst parts correspond full connections within subgraphs respectively third part corresponds full connections within complete bipartite graph number subsets generalized note although balance size subset necessarily equal. therefore always square matrix square matrix. although computing distance matrix complexity positive message mainstream manufacturers scientiﬁc computing softwares made great efforts accelerate matrix operations. distance metric l-norm instead computing distance objects pair pair formulate distances full connections parts bipart graph theorem small datasets instances attributes matlab matrix computing runs times faster pairwise distance computing. theorem euclidean distance matrix full connections within complete bipartite graph given elementmentioned above adequate computers given large dataset exact lapoleaf reasonable merge closely neighboring data points bucket employing techniques basic idea nearest located neighbors high probability share hash code away data points likely collide. different distance metrics need different hash funcideally distance matrix arbitrarily-sized data computed provided enough computers. however computers adequate dataset hand turn second approach lsh. computing nneigh parallel additive characteristic local density whole vector computed fully parallel fashion computing node whole matrix split belt stored separately different computing nodes. suppose blocks distance matrix samples where random vector sign function. improved work introducing gramschmidt orthogonalization process random vector group form representing unit named super-bit. investigating step algorithm except calculation distance matrix requires exactly basic operations steps lapoleaf linear time complexity size compared llgc hagr lapoleaf much efﬁcient listed table table size number iterations; number classes; number points layer. empirical evaluation section veriﬁed analysis. also worthwhile compare efﬁciency lxnew. lapoleaf lxnew require linear time complexity w.r.t. size existing olf. however traditional gssl methods lxnew requires running time long labeling data points prepare distance matrix centers propagation stage needs access distances pair centers denoted distcenters. distance matrix stored centralized computer distcenters extracted directly whole distance matrix however stored distributed system divided blocks denoted stored different computing node index range instances whose distances indicated ∗bsize+ b∗bsize]. usually bsize except last matrix block. label propagation lapoleaf heuristic algorithm lacks optimization objective. hence offers mathematical guarantee achieve best solution stage. however argue optimization moved forward construction stage. since obtained optimal partial ordered structure whole dataset believe iteration optimization regards data peer-to-peer relation longer compulsory. difference lapoleaf gssl methods. meanwhile lapoleaf regarded improved version k-nn. k-nn nearest neighbors considered spherical-shaped information granule unlabeled data assigned label voting strategy. parameter user result quite sensitive choice contrast lapoleaf information granules arbitrarily-shaped leading trees size tree automatically decided data lapoleaf usually sizes different. better captures nature data distribution label propagation reasonably designed lapoleaf constantly outperforms knn. efﬁciency effectiveness lapoleaf evaluated real world datasets among three small data machine learning repository larger scaled. information datasets shown table small datasets used demonstrate effectiveness lapoleaf datasets used show scalability lapoleaf measures parallel computing locality-sensitive hashing small sized datasets namely iris wine yeast shown lapoleaf achieves competitive accuracy efﬁciency much higher compared classical semi-supervised learning methods linear discriminant analysis neighborhood component analysis semi-supervised discriminant analysis framework learning propagability parameter conﬁguration experimental details distance metric chosen preprocessing method datasets listed table iii. improve accuracy gssl improve efﬁciency getting paradigm iterative optimization achieve minimum value objective function. lapoleaf exhibits high efﬁciency example completes whole process within seconds iris dataset mentioned personal computer. mnist dataset contains handwriting digits images total digit emphasize effectiveness lapoleaf itself directly original pixel data learning features since distance matrix oversized applied divideand-conquer technology described section iv-a. whole dataset equally divided subsets size distance matrix block computing matrix blocks vector group nneigh} constructed running lodog algorithm single machine. parameters intermediate results datasets mnist activity detailed last rows table iii. objective function values choosing mnist data shown fig. labeled samples randomly chosen digit accuracies achieved lapoleaf stateof-the-art method hierarchical anchor graph regularization listed table lapoleaf achieves competitive accuracy mnist data. however highlight lapoleaf efﬁciency. lapoleaf complete whole learning process including construction three stages label propagation within minutes personal computer. time consumptions detailed table accelerators gyroscopes built smart phones smart watches. since many different models phones watches sampling frequency accuracy therefore different data collected heterogeneous. dataset contains observations attributes each. since empirical cumulative distribution function feature reported outperform task compute ecdf feature original data future learning. conventionally time frame second overlapping ratio since major sampling frequency include observations frame compute ecdf move forward observations ecdf features computed. time granularity activity recognition half second. segment number resulting dimensionality feature ecdf reduced size activity data rows large number samples features employ tackle problems time. sb-lsh method empirically parameter depth super-bit number super-bits reduce amount data rows merging collided samples bucket. example resultant number hash buckets subset subject smart phone model nexus compared ecdf feature rows’ number number samples contained hash bucket treated weight compute construction. lapoleaf activity data preprocessing stage activity data transformed rows categorical features. consists series hash bucket number weight number ecdf rows share hash bucket number sequence. distance matrix computed based ecdf features bucket sequence rather hash codes themselves cosine distance used. parameters .x). parameter conﬁguration leads number subtrees constructed propagation sequentially taking randomly selected labeled ecdf features labeled data ﬁnal accuracy achieved lapoleaf .±.. existing gssl methods weaknesses. efﬁciency iterative optimization process inconvenience predict label newly arrived data. paper ﬁrstly made sound assumption neighboring data points equal positions lying partial-ordered relation; label center regarded contribution followers. based assumption previous work named lodog non-iterative semi-supervised approach called lapoleaf proposed. lapoleaf exhibits salient advantages much higher efﬁciency sate-of-the-art models keep accuracy comparable. deliver labels newly arrived data time complexity number data forming optimal leading forest enable lapoleaf accommodate data proposed exact divide-and-conquer approach approximate locality-sensitive-hashing method. theoretical analysis empirical validation shown effectiveness efﬁciency lapoleaf. plan extend datar immorlica indyk mirrokni locality-sensitive hashing scheme based p-stable distributions proceedings twentieth annual symposium computational geometry belhumeur hespanha kriegman eigenfaces ﬁsherfaces recognition using class speciﬁc linear projection ieee transactions pattern analysis machine intelligence vol. stisen blunck bhattacharya prentow kjærgaard sonne jensen smart devices different assessing mitigatingmobile sensing heterogeneities activity recognition proceedings conference embedded networked sensor systems hammerla kirkham andras ploetz preserving statistical characteristics accelerometry data using empirical cumulative distribution proceedings international symposium wearable computers received b.s. beijing jiaotong university beijing m.s. tianjin normal university tianjin china ph.d. candidate southwest jiaotong university chengdu china. research interests include data mining granular computing machine learning. published number papers refereed international journals ieee transactions cybernetics information sciences etc. guoyin wang received b.s. m.s. ph.d. degrees xian jiaotong university xian china respectively. worked university north texas university regina canada visiting scholar since working chongqing university posts telecommunications currently professor director chongqing laboratory computational intelligence director national international scientiﬁc technological cooperation base data intelligent computing dean graduate school. president international rough sets society chairman steering committee irss vice-president chinese association artiﬁcial intelligence. author books editor dozens proceedings international national conferences reviewed research publications. research interests include rough granular computing knowledge technology data mining neural network cognitive computing.", "year": 2017}