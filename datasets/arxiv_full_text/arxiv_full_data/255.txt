{"title": "FiLM: Visual Reasoning with a General Conditioning Layer", "tag": ["cs.CV", "cs.AI", "cs.CL", "stat.ML"], "abstract": "We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.", "text": "work show general model architecture achieve strong visual reasoning method introduce film feature-wise linear modulation. film layer carries simple feature-wise afﬁne transformation neural network’s intermediate features conditioned arbitrary input. case visual reasoning film layers enable recurrent neural network input question inﬂuence convolutional neural network computation image. process adaptively radically alters cnn’s behavior function input question allowing overall model carry variety reasoning tasks ranging counting comparing example. film thought generalization conditional normalization proven highly successful image stylization speech recognition visual question answering demonstrating film’s broad applicability. paper expands upon shorter report contribution show film strong conditioning method showing following visual reasoning tasks film models achieve state-of-the-art across variety introduce general-purpose conditioning method neural networks called film feature-wise linear modulation. film layers inﬂuence neural network computation simple feature-wise afﬁne transformation based conditioning information. show film layers highly effective visual reasoning answering image-related questions require multi-step high-level process task proven difﬁcult standard deep learning methods explicitly model reasoning. speciﬁcally show visual reasoning tasks film layers halve state-of-theart error clevr benchmark modulate features coherent manner robust ablations architectural modiﬁcations generalize well challenging data examples even zero-shot. ability reason everyday visual input fundamental building block human intelligence. argued artiﬁcial agents learn complex structured process necessary build aspects reasoning compositionality relational computation however model made general-purpose components could learn visually reason architecture would likely widely applicable across domains. understand general-purpose architecture exists take advantage recently proposed clevr dataset tests visual reasoning question answering. examples clevr shown figure visual question answering general task asking questions images line datasets generally focus asking diverse simpler questions images often answerable single glance. datasets number effective generalpurpose deep learning models emerged visual question answering however tests clevr show general deep learning approaches struggle learn structured multi-step reasoning particular methods tend copyright association advancement artiﬁcial intelligence rights reserved. film robust; many film model ablations still outperform prior state-of-the-art. notably close link normalization success conditioned afﬁne transformation previously untouched assumption. thus relax conditions method applied. film models learn little data generalize complex and/or substantially different data seen during training. also introduce novel film-based zeroshot generalization method improves validates film’s generalization capabilities. feature-wise linear modulation film learns adaptively inﬂuence output neural network applying afﬁne transformation film network’s intermediate features based input. formally film learns functions output function input arbitrary functions neural networks. modulation target neural network’s processing based input neural network input case multi-modal conditional tasks. cnns thus modulate per-feature-map distribution activations based agnostic spatial location. practice easier refer single function outputs vector since example often beneﬁcial share parameters across efﬁcient learning. refer single function film generator. also refer network film layers applied feature-wise linearly modulated network film-ed network. film layers empower film generator manipulate feature maps target film-ed network scaling down negating them shutting selectively thresholding more. feature conditioned independently giving film generator moderately ﬁne-grained control activations film layer. film requires parameters modulated feature scalable computationally efﬁcient conditioning method. particular film computational cost scale image resolution. model film model consists film-generating linguistic pipeline film-ed visual pipeline depicted figure film generator processes question using gated recurrent unit network hidden units takes learned dimensional word embeddings. ﬁnal hidden state question embedding model predicts residual block afﬁne projection. ﬁxed feature extractor outputs conv layer resnet pre-trained imagenet match prior work clevr image features processed several model film-ed residual blocks feature maps ﬁnal classiﬁer. classiﬁer consists convolution feature maps global max-pooling two-layer hidden units outputs softmax distribution ﬁnal answers. film-ed resblock starts convolution followed convolution architecture depicted figure turn parameters batch normalization layers immediately precede film layers off. drawing prior work clevr visual reasoning concatenate coordinate feature maps indicating relative spatial position image features resblock’s input classiﬁer’s input facilitate spatial reasoning. notably prior work examined whether afﬁne transformation must placed directly normalization. rather prior work includes normalization method name instructive purposes implementation details. investigate connection film normalization ﬁnding strictly necessary afﬁne transformation occur directly normalization. thus provide uniﬁed framework methods film well normalization-free relaxation approach broadly applied. beyond many connections film conditioning methods. common approach used example conditional dcgans concatenate constant feature maps conditioning information convolutional layer input. though parameter efﬁcient method simply results feature-wise conditional bias. likewise concatenating conditioning information fully-connected layer input amounts feature-wise conditional bias. approaches wavenet conditional pixelcnn directly conditional feature-wise bias. approaches equivalent film compare film experiments section. reinforcement learning alternate formulation film used train game-conditioned deep q-network play atari games though film neither focus work analyzed major component. methods gate input’s features function input rather separate conditioning input. methods include lstms sequence modeling convolutional sequence sequence machine translation even imagenet winning model squeeze excitation networks approach amounts feature-wise conditional scaling restricted film consists scaling shifting unrestricted. experiments section show effect restricting film’s scaling visual reasoning. noteworthy general approach feature modulation effective across variety settings architectures. even broader links film methods. example film viewed using network generate parameters another network making form hypernetwork also film potential ties conditional computation mixture experts methods specialized network subparts active per-example basis later provide evidence film learns selectively highlight suppress feature maps based conditioning information. methods select sub-network level film selects feature level. domain visual reasoning leading method program generator execution engine model approach consists sequenceto-sequence program generator takes question outputs sequence corresponding tree composadam weight decay batch size batch normalization relu throughout film-ed network. model uses image-question-answer triplets training withdata augmentation. employ early stopping based validation accuracy training epochs maximum. further model details appendix. empirically found film large capacity many architectural hyperparameter choices added regularization. stress model relies solely feature-wise afﬁne conditioning question information inﬂuence visual pipeline behavior answer questions. approach differs classical visual question answering pipelines fuse image language information single embedding element-wise product concatenation attention and/or advanced methods film viewed generalization conditional normalization methods. replaces parameters feature-wise afﬁne transformation typical normalization layers introduced originally learned function conditioning information. various forms proven highly effective across number domains conditional instance norm adaptive instance norm image stylization dynamic layer norm speech recognition conditional batch norm general visual question answering complex scenes guesswhat? work complements seek show feature-wise afﬁne conditioning effective multi-step reasoning understand underlying mechanism behind success. table clevr accuracy baselines competing methods film. denotes extra supervision program labels. denotes data augmentation. denotes training pixels. able neural modules three layer residual block. tree neural modules assembled form execution engine predicts answer image. modular approach part line neural module network methods end-to-end module networks also tested visual reasoning. models strong priors explicitly modeling compositional nature reasoning training additional program labels i.e. ground-truth step-by-step instructions correctly answer question. end-to-end module networks build model biases per-module hand-crafted neural architectures speciﬁc functions. approach learns directly visual textual input withadditional cues specialized architecture. relation networks another leading approach visual reasoning succeed explicitly building comparison-based prior. carry pairwise comparisons location extracted convolutional features image including lstm-extracted question features input mlp. element-wise resulting comparison vectors form another vector ﬁnal classiﬁer predicts answer. note computational cost scales quadratically spatial resolution film’s cost independent spatial resolution. notably since concatenate question features input form feature-wise conditional biasing explained earlier conditioning approach related film. first test model visual reasoning clevr task trained film models analyze film learns. second explore well model generalizes challenging questions clevr-humans task. finally examine film performs fewshot zero-shot generalization settings using clevr compositional generalization test. appendix provide error analysis model. code available https//github.com/ethanjperez/film. clevr task clevr synthetic dataset tuples images contain d-rendered objects various shapes materials colors sizes. questions multi-step compositional nature shown figure range counting questions comparison questions words long. answers word possible answers. programs additional supervisory signal consisting step-by-step instructions filter shape relate count answer question. baselines compare following methods discussed detail related work section q-type baseline predicts based question’s category. lstm predicts using question. cnn+lstm prediction cnn-extracted stacked attention networks linear prediction cnn-extracted image feature lstm-extracted question features combined rounds soft spatial attention end-to-end module networks program generator execution engine methods separate neural networks learn separate subfunctions assembled question-dependent structure results film achieves overall state-of-the-art clevr shown table outperforming humans previous methods including using explicit models reasoning program supervision and/or data augmentation. figure visualizations distribution locations model uses globally max-pooled features ﬁnal predicts from. film correctly localizes answer-referenced object question-referenced objects accurately answers incorrectly questions images used match methods using extra supervision film roughly halves state-of-the-art error note using pre-trained image features input viewed form data augmentation film performs equally well using pixel inputs. interestingly pixel model seems perform better lower-level questions image features model seems perform better higher-level questions film layers learn? understand film visually reasons visualize activations observe result film layers. also histograms t-sne patterns learned film parameters themselves. figures appendix visualize effect film single feature level. activation visualizations figure visualizes distribution locations responsible globally-pooled features model’s ﬁnal classiﬁer uses predict answers. images reveal film model predicts using features areas near answer-related question-related objects high clevr accuracy also suggests. ﬁnding highlights appropriate feature modulation indirectly results spatial modulation regions question-relevant features large activations regions not. observation might explain film outperforms stacked attention next best method explicitly built reasoning signiﬁcantly film appears carry many spatial attention’s beneﬁts also inﬂuencing feature representation. figure also suggests film-ed network carries reasoning throughout pipeline. example film-ed network localized answer-referenced object alone classiﬁer. bottom example film-ed network retains classiﬁer features objects referred answer referred question. latter example provides evidence ﬁnal carries reasoning using film extract relevant features reasoning. film parameter histograms analyze lower level film uses question condition visual pipeline plot values predicted validation shown figure detail appendix values take advantage sizable range varying respectively. values show sharp peak showing film learns question shut signiﬁcantly suppress whole feature maps. simultaneously film learns upregulate much selective feature maps high magnitude values. furthermore large fraction values negative; since model uses relu film cause signiﬁcantly different activations pass relu downstream layers also values negative suggesting film also uses selective activations pass relu. show later film’s success largely architecture-agnostic examining particular model gives insight inﬂuence film learns exert figure t-sne plots ﬁrst last film layers -film layer network. film parameters cluster low-level reasoning functions ﬁrst layer high-level reasoning functions last layer. film parameters t-sne plot figure visualize film parameter vectors random validation points t-sne. analyze deeper -resblock version model similar validation accuracy -resblock model better examine film layers different layers hierarchy behave. first last layer film grouped low-level high-level reasoning functions necessary answer clevr questions respectively. example film parameters equal color query color close ﬁrst layer apart last layer. true shape size material questions. conversely equal shape equal size equal material film parameters grouped last layer split ﬁrst layer likewise high level groupings integer comparison querying. ﬁndings suggest film layers learn sort function-based modularity without architectural prior. simply end-to-end training film learns handle different types questions differently also different types question sub-parts differently; film model works low-level high-level processes proper approach. models fewer film layers patterns also appear less clearly; models must begin higher level reasoning sooner. ablations using validation conduct ablation study best model understand film learns visual reasoning. show results test time ablations figure architectural ablations table varied model depths table without hyperparameter tuning architectural ablations model depths outperform prior state-of-the-art training image-question-answer triplets supporting film’s overall robustness. table also shows using validation results statistically signiﬁcant. effect test effect separately trained model constant another models accuracy drop respectively; film learn condition visual reasoning either biasing scaling alone albeit well conditioning together. result also suggests important compare importance series test time ablations best fullytrained model. first replace mean across training set. ablation effect removes conditioning information parameters test time model trained here accuracy drops procedure results drop. large difference suggests that practice film largely conditions rather next analyze performance increasingly gaussian noise best model’s film parameters test time. noise gamma hurts performance signiﬁcantly more showing film’s higher sensitivity changes corroborating relatively greater importance moid many models feature-wise multiplicative gating likewise also limit using tanh. restrictions hurt performance roughly much removing conditioning entirely training thus film’s ability scale features large magnitudes appears contribute success. limiting also hurts performance validating value film’s capacity negate zero feature maps. conditional normalization perform ablation study placement film evaluate relationship normalization film conditional normalization approaches assume. unfortunately difﬁcult accurately decouple effect film normalization simply training corresponding model without normalization normalization signiﬁcantly accelerates regularizes improves neural network learning include results completeness. however substantial performance drop moving film layers different parts model’s resblocks; even reach upper best model’s performance range placing film post-normalization relu resblocks. thus decouple name normalization clarity regarding fundamental effectiveness method comes from. demonstrating conditioning mechanism closely connected normalization open doors applications settings normalization less common rnns reinforcement learning promising directions future work film. repetitive conditioning understand contribution repetitive conditioning towards film model success train film models successively fewer film layers. models fewer film layers even single film layer deviate best model’s performance revealing model reason answer diverse questions successfully modulating features even once. observation highlights capacity even film layer. perhaps film layer pass enough question information enable carry reasoning later network place hierarchical conditioning deeper film models appear use. leave in-depth investigation matter future work. spatial reasoning examine film models approach spatial reasoning train version best model architecture image features convolutions without feeding coordinate feature maps indicating relative spatial position model. global max-pooling near model model cannot transfer information across spatial positions. notably model still achieves high accuracy indicating film models able reason space simply spatial information contained single location ﬁxed image features. residual connection removing residual connection causes larger accuracy drops. since global max-pooling operation near network ﬁnding suggests best model learns primarily features locations repeatedly important throughout lower higher levels reasoning make ﬁnal decision. higher accuracies models film modulating features inside residual connections rather outside residual connections supports hypothesis. model depth table shows model performance number resblocks. film robust varying depth less resblock backing earlier theory film-ed network reasons throughout pipeline. clevr-humans human-posed questions assess well visual reasoning models generalize realistic complex free-form questions clevr-humans dataset introduced dataset contains human-posed questions clevr images along corresponding answers. number samples limited training figure examples clevr-humans introduces words concepts. ﬁne-tuning clevr-humans clevr-trained model reason obstruction superlatives reﬂections still struggles hypothetical scenarios also learned human preference primarily identify objects shape validation testing. questions collected amazon mechanical turk workers prompted questions likely hard smart robot answer. result clevr-humans questions diverse vocabulary complex concepts. method test film clevr-humans take best clevr-trained film model ﬁne-tune filmgenerating linguistic pipeline alone clevr-humans. similar prior work update visual pipeline clevr-humans mitigate overﬁtting small training set. results model achieves state-of-the-art generalization clevr-humans ﬁne-tuning shown table indicating film well-suited handle complex diverse questions. figure shows examples clevr-humans film model answers. ﬁne-tuning film outperforms prior methods smaller margin. ﬁne-tuning film reaches considerably improved ﬁnal accuracy. particular gain accuracy made film upon ﬁne-tuning greater made models; film adapts dataefﬁciently using small clevr-humans dataset. state-of-the-art method program generator execution engine ﬁne-tuning prior work pg+ees explains neural module network method struggles questions cannot well approximated model’s module inventory contrast film freedom modulate existing feature maps fairly ﬂexible ﬁne-grained operation novel ways reason concepts. results thus provide evidence beneﬁts film’s general nature. clevr compositional generalization test test well models learn compositional concepts generalize clevr-cogent introduced dataset synthesized clevr contains conditions condition cubes gray blue brown yellow cylinders green purple cyan; condition cubes cylinders swap color palettes. conditions contain spheres colors. clevr-cogent thus indicates model answers clevr questions memorizing combinations traits learning disentangled general representations. results train best model architecture condition report accuracies conditions ﬁne-tuning figure results indicate film surpasses visual reasoning models learning general concepts. film learns better compositional generalization even pg+ee explicitly models compositionality trained program-level supervision specifically includes ﬁltering colors ﬁltering shapes. sample efﬁciency catastrophic forgetting show sample efﬁciency forgetting curves figure film achieves prior state-of-the-art accuracy much ﬁne-tuning data. however film model still suffers catastrophic forgetting ﬁne-tuning. zero-shot generalization film’s accuracy condition much higher suggesting film memorized attribute combinations extent. example model learns bias cubes cyan learning training bias helps minimize training loss. overcome bias develop novel film-based zero-shot generalization method. inspired word embedding manipulations e.g. king woman queen test linear manipulation extends reasoning film. compute many cyan cubes there? linear combination questions film parameter space many cyan spheres there? many brown cubes there? many brown spheres there?. model correctly count cyan cubes. show another example method figure evaluate method validation using parser automatically generate right combination questions. test previously reported clevr-cogent film models method show results figure method overall accuracy gain training testing zero-shot generalization method could applied questions questions model accuracy starts jumps ﬁne-tuning accuracy between zero-shot original approaches identical likewise ﬁne-tuning. note difference predicted film parameters methods negligible likely causing similar performance. achieve improvements without speciﬁcally training model zero-shot generalization. method simply allows film take advantage concept disentanglement training. also observe convex combinations film parameters i.e. between many cyan things there? many brown things there? often monotonically interpolates predicted answer answers endpoint questions. results highlight limited extent ﬂexibility film parameters meaningful manipulations. implemented method many limitations. however approaches word embeddings representation learning zero-shot learning applied directly optimize analogy-making film-ed network could directly train procedure backpropagation. learned model could also replace parser. avenues promising future work. figure clevr-cogent example. combination concepts blue cylinder training set. zero-shot method computes original question’s film parameters linear combination three questions’ film parameters method corrects model’s answer rubber metal. show model achieve strong visual reasoning using general-purpose feature-wise linear modulation layers. efﬁciently manipulating neural network’s intermediate features selective meaningful manner using film layers effectively language modulate carry diverse multi-step reasoning tasks image. ablation study suggests film resilient architectural modiﬁcations test time ablations even restrictions film layers themselves. notably provide evidence film’s success closely connected normalization previously assumed. thus open door applications approach settings normalization less common rnns reinforcement learning. ﬁndings also suggest film models generalize better sample efﬁciently even zero-shot foreign challenging data. overall results investigation film case visual reasoning complement broader literature demonstrates success film-like techniques across many domains supporting case film’s strength simply within single domain general versatile approach. thank developers pytorch open-source code implementation based off. thank mohammad pezeshki dzmitry bahdanau yoshua bengio nando freitas hugo larochelle laurens maaten joseph cohen joelle pineau olivier pietquin j´er´emie mary c´esar laurent chin-wei huang layla asri smith james ough helpful discussions justin johnson clevr test evaluations. thank nvidia donating dgx- computer used work. also acknowledge frqnt chist-era iglu project coll`ege doctoral lille nord france cper nordpas calais/feder data advanced data science technologies funding work. lastly thank acronymcreator.net acronym film.", "year": 2017}