{"title": "Medical Diagnosis From Laboratory Tests by Combining Generative and  Discriminative Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "A primary goal of computational phenotype research is to conduct medical diagnosis. In hospital, physicians rely on massive clinical data to make diagnosis decisions, among which laboratory tests are one of the most important resources. However, the longitudinal and incomplete nature of laboratory test data casts a significant challenge on its interpretation and usage, which may result in harmful decisions by both human physicians and automatic diagnosis systems. In this work, we take advantage of deep generative models to deal with the complex laboratory tests. Specifically, we propose an end-to-end architecture that involves a deep generative variational recurrent neural networks (VRNN) to learn robust and generalizable features, and a discriminative neural network (NN) model to learn diagnosis decision making, and the two models are trained jointly. Our experiments are conducted on a dataset involving 46,252 patients, and the 50 most frequent tests are used to predict the 50 most common diagnoses. The results show that our model, VRNN+NN, significantly (p<0.001) outperforms other baseline models. Moreover, we demonstrate that the representations learned by the joint training are more informative than those learned by pure generative models. Finally, we find that our model offers a surprisingly good imputation for missing values.", "text": "beijing university posts telecommunications beijing china machine learning department carnegie mellon university pittsburgh center speech language technologies tsinghua university beijing china petuum pittsburgh *eric.xingpetuum.com primary goal computational phenotype research conduct medical diagnosis. hospital physicians rely massive clinical data make diagnosis decisions among laboratory tests important resources. however longitudinal incomplete nature laboratory test data casts signiﬁcant challenge interpretation usage result harmful decisions human physicians automatic diagnosis systems. work take advantage deep generative models deal complex laboratory tests. speciﬁcally propose end-to-end architecture involves deep generative variational recurrent neural networks learn robust generalizable features discriminative neural network model learn diagnosis decision making models trained jointly. experiments conducted dataset involving patients frequent tests used predict common diagnoses. results show model vrnn+nn signiﬁcantly outperforms baseline models. moreover demonstrate representations learned joint training informative learned pure generative models. finally model offers surprisingly good imputation missing values. introduction misdiagnosis diagnostic decision made inaccurately widely occurs. approximately million adults experiencing diagnosis errors every year half could harmful many adult patients intensive care unit misdiagnosis annually major source misdiagnosis sub-optimal interpretation usage clinical data nowadays physicians overwhelmed large amount medical data including laboratory tests vital signs clinical notes medication prescriptions etc. among kinds clinical data laboratory tests play important role. according american clinical laboratory association laboratory tests guide diagnostic decisions. unfortunately comprehensively understanding laboratory test results discovering underlying clinical implications easy. incorrect interpretation laboratory tests major breakdown point diagnostic process reasons laboratory tests difﬁcult understand two-fold. first missing values pervasive. typical certain time point small subset laboratory tests examined leaving values tests missed. data missing prevents physicians getting full picture patients’ clinical states leading sub-optimal decisions. second laboratory test values complex multivariate time-series structure in-hospital stay multiple laboratory tests examined particular time test examined multiple times different time points. multivariate temporal data exhibits complicated patterns along dimensions time tests. learning patterns highly valuable diagnosis technically challenging. work study leverage ability machine learning automatically distilling patterns complex noisy incomplete irregular laboratory test data address above-mentioned issues build end-to-end diagnostic model assist diagnosis decisions. previous studies applied perform diagnosis based laboratory tests. approaches three major tasks handling missing values discovering patterns multivariate time-series predicting diseases often conducted separately. however three tasks tightly coupled mutually beneﬁt other. hand better imputation missing values leads discovery informative patterns boosts accuracy diagnosis. hand model training supervision diagnosis provides guidance pattern discovery inﬂuences imputation missing values tailoring discovered patterns imputed values suitable diagnosis task. performing tasks separately fails consider synergistic relationships hence leads sub-optimal solution. another limitation exists previous studies often proposed discriminative structure cannot well address missing value problem learn generalizable patterns principle. contributions paper develop end-to-end deep neural model perform diagnosis based laboratory tests. model seamlessly integrates three tasks together imputing missing values discovering patterns multivariate time-series data predicting diseases perform jointly. model combines major learning paradigms machine learning generative learning discriminative learning generative learning component utilized deal missing values discover robust generalizable patterns discriminative learning component used predicting diseases based patterns discovered generative learning. evaluate proposed model patient visits demonstrate model achieves signiﬁcantly better diagnosis performance baseline models better imputation missing values better discovery patterns laboratory test data. related works lasko proposed gaussian process model longitudinal electronic medical records used standard auto-encoder learn hidden features inputs ghassemi introduced multi-task gaussian process model clinical data miotto took advantage denoising auto-encoder learn hidden representations used representations make diagnose recent years recurrent neural network demonstrated superiority modeling longitudinal data like natural language speech signals also utilized medical diagnosis. lipton proposed medical diagnosis model based long short-term memory recurrent neural networks obtained performance better strong baselines work missing values addressed heuristic forwardback-ﬁlling. later work continued lstm-based diagnosis used missing value indicator part inputs. found missing value patterns help diagnosis performance choi proposed ’doctor used previous diagnoses predict future diagnosis another work neural attention mechanism introduced proposed diagnosis model based stacked lstm gradient boosting trees introduced make learned features interpretable later work modiﬁed structure gated recurrent units deal incomplete inputs work based discriminative models cannot address missing value problem well principle. generative model usually works better aspect. typical generative model gaussian mixture model missing values easily addressed expectation-maximum algorithm instance marlin employed gmms discovery patterns clinical data conducted mortality prediction generative models however mostly shallow linear gaussian. recently researchers proposed several deep generative models e.g. variational auto-encoder variational compared conventional generative models vrnn model complex conditional distributions hence representing complex patterns. work utilize deep generative models. however generative models task-oriented therefore propose end-to-end approach combines advantages types models train joint fashion. seen either generative learning discriminative target guidance discriminative learning generative target regularization. data preparation data used study opened mimic-iii publicly available. derived laboratory tests patients. contains in-hospital outpatient records. in-hospital episode corresponding icd- codes study primary diagnoses considered. amounts different diagnoses unique laboratory tests. diagnoses tests quite rare limit study frequent diagnoses frequent laboratory tests. group test results ﬁnally temporal sequences in-hospital records labeled disease lengths temporal sequences range focus latest days. figure shows number samples disease ids. random split dataset times time keep proportion training development testing sets %%%. hence numbers samples three sets respectively. tests valued discrete categories like abnormal normal. change categories integers like abnormal normal. test results normalized z-normalization i.e. values test subtract mean divided standard deviation. note patient cannot every test every missing values pervasive data. figure present example patient’s laboratory test records. seen missing values. simple statistics show whole dataset average missing value rate i.e. average laboratory tests values patient’s one-day record. experiments initially impute missing values applying z-normalization mean values change zero. zero imputation equals mean imputation. moreover since models situated neural network framework zero inputs introduce additional bias computation. note baseline models zero-imputation behaves solution missing value problem models behaves indicators missing values missing values addressed deep generative models. model architectures given i.i.d data {xn}n temporal sequence length i.e. ...xt}. dimension input features. meanwhile class label purpose predict class labels accurately. speciﬁcally longitudinal laboratory test records patient one-day record. primary diagnosis. propose models study denoted vae+nn vrnn+nn former static model demonstrate contribution deep generative models latter temporal model extends deep generative learning approach learn long-term temporal dependency. vae+nn model address temporal records simply averaging vectors time points i.e. average. although averaging operation alleviates missing value problem extent rate missing values still high deal missing values capture complex patterns data propose vae+nn model generative model used handle missing values discover patterns standard neural network used classiﬁer shown figure prior centered isotropic multivariate gaussian distribution i.e. assume ﬁxed also follows gaussian distribution suppose different dimensions independent generation deﬁned figure example patient’s laboratory test records. axis corresponds laboratory tests used. axis denotes time records. green means value otherwise missing. vrnn+nn better capture long-term dependency clinical records previous work introduced task however internal transition structure entirely deterministic cannot efﬁciently model complexity highly structured sequential data. therefore chung extended idea recurrent framework modeling high-dimensional sequences leading variational propose vrnn+nn model shown figure involves vrnn generate sequential hidden features model make decisions based average hidden features. prior latent variable conditional probability observations. assume prior distribution follows gaussian distribution different dimensions independent i.e. covariance matrix diagonal. therefore prior distribution deﬁned follows. represents mean standard variance prior distribution respectively. again assume different dimensions independent conditional probability observation written inference learning vae+nn involving neural networks makes true posterior intractable. resort variational inference methods approximate true posterior deﬁned fashion also discriminative loss classiﬁcation cross entropy true disease model predicted posterior probability. denotes parameters discriminative network. baselines section build several baseline models comparative study denoted ae+nn rnn+nn respectively. ae+nn models used compare vae+nn model we’d like deep generative models better performance representing single feature vectors. rnn+nn model similar model structure previous studies used compare vrnn+nn model. ae+nn demonstrate superiority present ae+nn baseline based standard auto-encoder shown figure similar structure deterministic less generative. model also combine loss mean squared error training objective also missing items included loss function. summary model learning target deﬁned follows rnn+nn rnn+nn model shown figure model processes temporal features average hidden state used input also denotes recurrent computation lstm cells. model formulated follows implement details experiments models implemented tensorﬂow feed-forward neural networks hidden layer relu activations. size hidden layer adam optimizer learning rate learning rate decay trade parameter experiments. evaluation metrics score generally score deﬁned ∗precision∗recall precision+recall precision number correct positive results divided number positive results recall number correct positive results divided number positive results returned. since task -class classiﬁcation three kinds scores micro-f macro-f macro-f-weighted micro-f computed ﬂatten macro-f arithmetic mean scores different classes. macro-f-weighted weighted mean scores different classes weight class deﬁned number samples class testing set. score ranges best performance achieves score equals simulate blind prediction i.e. equal probabilities different classes results micro-f macro-f macro-f-weighted area curve. similarly also three aucs evaluation micro-auc macro-auc macro-aucweighted also ranges best performance achieves equals blind prediction aucs equal results table shows diagnosis performances three sets experiments. values performances different models diagnosis task measured different variants values aucs. additionally test joint training table three sets diagnosis performances. contains performances models baselines; middle shows performances model features derived different models; bottom compares performance vrnn+nn rnn+nn typical heuristic imputation methods. results presented mean value standard deviation fashion. result better representations compared unsupervised generative models representations derived vae+nn vrnn vrnn+nn used train model diagnosis decision. performances presented middle set. finally bottom compare vrnn+nn model’s ability dealing missing values heuristic imputation methods investigate four imputation methods here. zero default approach baseline models last&next mean nocb three best known imputation methods according previous study last&next average last known next known values; mean mean patient’s values after; nocb next observation carried backward. since deep generative models reconstruct input data conjecture vrnn+nn model potential impute missing values better. test conjecture ﬁrst randomly drop values original data trained vrnn+nn impute intentionally dropped values. results terms shown table values heuristic imputation methods also presented. paired t-test results methods shown well. discussion diagnosis performances shown table observed considering temporal dependencies e.g. vrnn diagnosis perform signiﬁcantly improved conﬁrming long-term dependency important property clinical data. secondly involving deep generative models provides consistent performance improvement compared improvement provided generative modeling observed dealing single averaged feature vectors improvement signiﬁcant. indicates data missing problem average vectors severe zero imputation reasonably good. dealing temporal vector sequences improvement highly signiﬁcant understandable data missing problem serious case. according results classiﬁer different features representations learned vae+nn performs signiﬁcantly better learned vae; similarly representations learned vrnn+nn better learned vrnn. implies joint training considers classiﬁcation target lead better feature learning. surprising learning task-oriented. comparison different missing value imputation methods shown bottom table demonstrates vrnn+nn model outperforms heuristic imputation methods conﬁrming generative model principled missing data treatment. conclusion drawn explicitly results shown table performance different imputation methods compared imputation error directly. advantage data imputation quite useful practice. example help complete incomplete laboratory test data give rough range missing values assist physicians better analysis. limitations future works limitations exist current study need addressed future work. first consider frequent diagnoses leaving rest diseases untested. severe data imbalance different diseases. diseases sample. even diseases selected study data distribution still quite biased. shown figure disease samples disease samples. unbalanced data casts challenge model training resultant models tend score higher frequent labels infrequent ones. study didn’t apply additional preprocessing like over-sampling under-sampling second looking vrnn+nn model performance disease shown figure scores aucs different diseases vary dramatically. diagnoses like alcohol withdrawal single liveborn without cesarean section quite accurate others like gram-neg septicemia upper lobe lung obtain scores close phenomenon implies diseases difﬁcult detect model. result limitation frequent laboratory tests deﬁciency clinical data. laboratory tests considered clinical information included future study. third used in-hospital records diagnosis labels also outpatient test results mimic-iii posses labels. future work investigate semi-supervised learning enhance capability model utilizing unlabelled data. conclusion longitudinal incomplete noisy laboratory test data casts difﬁculty automatically medical diagnosis. study proposed utilize deep sequential generative model form vrnn deal missing data problem learn complex temporal patterns clinical data generative model trained jointly back-end discriminative model making diagnosis decision. leads end-to-end system takes advantage generative learning discriminative learning. experiments show vrnn+nn model signiﬁcantly surpasses baselines non-temporal version vae+nn model. also deep generative models help imputing missing values clinical laboratory tests distilling informative patterns diagnosis combination generative discriminative models leads improvement feature learning diagnosis decision. future work involves addressing data unbalance utilizing unlabeled data. references singh meyer thomas frequency diagnostic errors outpatient care estimations three large observational studies involving adult populations. qual. saf. ./bmjqs--. marlin kale khemani wetzel unsupervised pattern discovery electronic health care data using probabilistic clustering models. proceedings sighit international health informatics symposium author contributions statement s.z. p.x. conceived designed study. s.z. processed data performed experiments. s.h. p.x. d.w. wrote paper. d.w. e.p.x. take responsibility paper co-senior authors. authors reviewed manuscript. additional information supplementary information accompanies paper. competing ﬁnancial interests authors declare competing ﬁnancial interests.", "year": 2017}