{"title": "SKYNET: an efficient and robust neural network training tool for machine  learning in astronomy", "tag": ["astro-ph.IM", "cs.LG", "cs.NE", "physics.data-an", "stat.ML"], "abstract": "We present the first public release of our generic neural network training algorithm, called SkyNet. This efficient and robust machine learning tool is able to train large and deep feed-forward neural networks, including autoencoders, for use in a wide range of supervised and unsupervised learning applications, such as regression, classification, density estimation, clustering and dimensionality reduction. SkyNet uses a `pre-training' method to obtain a set of network parameters that has empirically been shown to be close to a good solution, followed by further optimisation using a regularised variant of Newton's method, where the level of regularisation is determined and adjusted automatically; the latter uses second-order derivative information to improve convergence, but without the need to evaluate or store the full Hessian matrix, by using a fast approximate method to calculate Hessian-vector products. This combination of methods allows for the training of complicated networks that are difficult to optimise using standard backpropagation techniques. SkyNet employs convergence criteria that naturally prevent overfitting, and also includes a fast algorithm for estimating the accuracy of network outputs. The utility and flexibility of SkyNet are demonstrated by application to a number of toy problems, and to astronomical problems focusing on the recovery of structure from blurred and noisy images, the identification of gamma-ray bursters, and the compression and denoising of galaxy images. The SkyNet software, which is implemented in standard ANSI C and fully parallelised using MPI, is available at http://www.mrao.cam.ac.uk/software/skynet/.", "text": "gravitational astrophysics laboratory nasa goddard space flight center greenbelt greenbelt astrophysics group cavendish laboratory thomson avenue cambridge kavli institute cosmology madingley road cambridge abstract present ﬁrst public release generic neural network training algorithm called skynet. efﬁcient robust machine learning tool able train large deep feedforward neural networks including autoencoders wide range supervised unsupervised learning applications regression classiﬁcation density estimation clustering dimensionality reduction. skynet uses ‘pre-training’ method obtain network parameters empirically shown close good solution followed optimisation using regularised variant newton’s method level regularisation determined adjusted automatically; latter uses second-order derivative information improve convergence without need evaluate store full hessian matrix using fast approximate method calculate hessian-vector products. combination methods allows training complicated networks difﬁcult optimise using standard backpropagation techniques. skynet employs convergence criteria naturally prevent overﬁtting also includes fast algorithm estimating accuracy network outputs. utility ﬂexibility skynet demonstrated application number problems astronomical problems focusing recovery structure blurred noisy images identiﬁcation gammaray bursters compression denoising galaxy images. skynet software implemented standard ansi fully parallelised using available http//www.mrao.cam.ac.uk/software/skynet/. words methods data analysis methods statistical modern astronomy increasingly faced problem analysing large complicated multidimensional data sets. analyses typically include tasks data description interpretation inference pattern recognition prediction classiﬁcation compression many more. performing tasks machine learning methods. accessible accounts machine learning astronomy example mackay ball brunner moreover machine learning software easily used astronomy python-based astroml package c-based fast artiﬁcial neural network library recently started become available. learning unsupervised learning. supervised learning goal infer function labeled training data consist training examples. example ‘properties’ ‘labels’. properties known ‘input’ quantities whose values used predict values labels considered ‘output’ quantities. thus function inferred mapping properties labels. learned mapping applied datasets values labels known. supervised learning usually subdivided classiﬁcation regression. classiﬁcation labels take discrete values whereas regression labels continuous. astronomy example using multifrequency observations supernova lightcurve determine type classiﬁcation problem since label discrete whereas using observations determine energy output supernova explosion regression problem since label continuous. classiﬁcation also used obtain distribution output value would normally treated regression problem. demonstrated bonnett measuring redshifts cfhtlens. particularly important recent application regression supervised learning astrophysics cosmology acceleration statistical analysis large data sets context complicated models. analyses typically performs many evaluations likelihood function describing probability obtaining data different sets values model parameters. problems particular cosmology function evaluation take tens seconds making analysis computationally expensive. performing regression supervised learning infer replace mapping model parameters likelihood value reduce computation required likelihood evaluation several orders magnitude thereby vastly accelerating analysis unsupervised learning data labels. precisely quantities associated data item divided properties labels lack ‘causal structure’ inputs assumed beginning outputs causal chain difference supervised learning. instead observations considered causal chain assumed begin ‘latent’ variables. unsupervised learning infer number and/or nature latent variables ﬁnding similarities data items. enables summarize explain features dataset. common tasks unsupervised learning include density estimation clustering dimensionality reduction. indeed cases dimensionality reduction used pre-processing step supervised learning since classiﬁcation regression sometimes performed reduced space accurately original space. astronomical example unsupervised learning might wish multifrequency observations lightcurves supernovae determine many different types supernovae contained alternatively data also includes type supernova might wish determine properties combination properties lightcurves important determining type photometrically reduced property combinations could used instead original lightcurve data perform supernovae classiﬁcation regression analyses mentioned above. intuitive well-established approach machine learning supervised unsupervised based artiﬁcial neural networks loosely inspired structure functional aspects brain. consist group interconnected nodes processes information receives passes product nodes weighted connections. constitute non-linear statistical data modeling tool used represent complex relationships inputs outputs patterns data capture statistical structure unknown joint probability distribution observed variables. general structure arbitrary many machine learning applications performed using feed-forward nns. networks structure directed input layer nodes passes information output layer zero many ‘hidden’ layers between. network able ‘learn’ mapping inputs outputs given training data make predictions outputs input data. moreover universal approximation theorem assures accurately precisely approximate mapping given form. useful introduction found mackay astronomy feed-forward applied various machine learning problems years nonetheless widespread astronomy limited difﬁculty associated standard techniques backpropagation training networks many nodes and/or numerous hidden layers often necessary model complicated mappings numerous inputs outputs modern astronomical applications. paper therefore present ﬁrst public release skynet efﬁcient robust neural network training tool able train large and/or deep feed-forward networks. skynet able achieve using combination ‘pre-training’ method hinton osindero obtain network weights close good optimum training objective function followed optimisation weights using regularised variant newton’s method based developed memsys software package particular second-order derivative information used improve convergence without need evaluate store full hessian matrix using fast approximate method calculate hessianvector products skynet implemented standard ansi programming language parallelised using mpi. also note skynet already combined multinest produce blind accelerated multimodal bayesian inference package generic completely automated tool greatly accelerating bayesian inference problems multinest fully-parallelised implementation nested sampling extended handle multimodal highly-degenerate distributions. astrophysical bayesian inference problems multinest typically reduces number likelihood evaluations required order magnitude more compared standard mcmc methods bambi achieves substantial gains speeding evaluation likelihood replacing trained regression neural network. bambi proceeds ﬁrst using multinest obtain speciﬁed number samples model parameter space uses input skynet train network likelihood function. convergence optimal weights network’s ability predict likelihood values within speciﬁed tolerance level tested. fails sampling continues using original likelihood enough samples made training performed again. network trained sufﬁciently accurate predictions used place original likelihood function future samples multinest. typical problems cosmology example using network reduces likeli skynet also used train recurrent neural networks application networks discussed future work. version skynet parallelised gpus using cuda currently development. weights biases values wish determine training vary huge range non-linear mappings inputs outputs possible. fact universal approximation theorem states three layers approximate continuous function given accuracy long activation function locally bounded piecewise continuous polynomial although functions would work well tanh). increasing number hidden nodes achieve accuracy risk overﬁtting training data. activation functions also proposed rectiﬁed linear function wherein max{ ‘softsign’ function argued former removes need pre-training serves better model biological neurons. ‘softsign’ similar tanh slower approach asymptotes autoencoders speciﬁc type feed-forward neural network containing hidden layers inputs mapped themselves i.e. network trained approximate identity operation; hidden layer used typically referred ‘deep’ autoencoder. networks typically contain several hidden layers symmetric central layer containing fewer nodes inputs basic diagram autoencoder shown fig. three inputs mapped three symmetrically-arranged hidden layers nodes central layer. autoencoder thus considered half-networks part mapping inputs central layer second part mapping central layer values outputs parts called ‘encoder’ ‘decoder’ respectively either reduced ‘feature variables’ embodied nodes central layer variables general non-linear functions original input variables. determine dependence feature variable turn simply decoding corresponding value varied; feature variable obtains curve original data space. conversely collection feature values central layer hood evaluation time seconds less millisecond allowing multinest complete analysis much rapidly. bonus analysis user also obtains network trained provide likelihood evaluations near peak needed subsequent analyses. public release skynet also make bambi publically available. structure paper follows. section describe general structure feed-forward including particular special case networks called autoencoders used performing non-linear dimensionality reduction. section present procedures used skynet train networks types. skynet applied machine learning examples section including regression task classiﬁcation task dimensionality reduction task using autoencoders. also apply skynet problem classifying images handwritten digits mnist database widely-used benchmarking test machine learning algorithms. application skynet astronomical machine learning examples presented section including regression task determine projected ellipticity galaxy blurred noisy images galaxy ﬁeld star; classiﬁcation task based simulated gamma-ray burst detection pipeline swift satellite determine given source parameters detected; dimensionality reduction task using autoencoders compress denoise galaxy images. finally present conclusions section multilayer perceptron feed-forward neural network simplest type network consists ordered layers perceptron nodes pass scalar values layer next. perceptron simplest kind node maps input vector scalar output {wi} parameters perceptron called ‘weights’ ‘bias’ respectively. -layer consists input layer hidden layer output layer shown fig. outputs nodes hidden output layers given following equations non-linearity essential allowing network model non-linear functions. expand include hidden layers iterate connection hidden layer next time using activation function ﬁnal hidden layer connect output layer using relation training wish optimal network weights biases maximise accuracy predicted outputs. however must careful avoid overﬁtting training data lead inaccurate predictions inputs network trained. training data inputs outputs provided user approximately cent used actual training remainder retained validation used determine convergence avoid overﬁtting. ratio gives plenty information training still leaves representative subset data checks made. prudent ‘whiten’ data training network. whitening normalises input and/or output values easier train network starting initial weights small centred zero. network weights ﬁrst last layers ‘unwhitened’ training network able perform mapping original inputs outputs. transforms chosen user wish whiten inputs training data. whitening normally performed separately input calculated across inputs related. mean standard deviation minimum maximum would computed inputs training data items. chosen whitening transform also used whitening outputs. since transforms consist subtracting offset multiplying scale factor easily performed reversed. unwhiten network weights inverse transform applied offset scale determined source input node target output node. outputs classiﬁcation network whitened since already probabilities might reasonably termed feature vector input data. autoencoders therefore provide intuitive approach non-linear dimensionality reduction constitute natural generalisation linear methods principal component analysis independent component analysis widely used astronomy. indeed antoencoder single hidden layer linear activation functions shown identical topic explored section worth noting encoding input data feature variables also useful performing clustering tasks; illustrated section autoencoders however notoriously difﬁcult train since objective function contains broad local maximum output average value inputs nonetheless difﬁculty overcome pretraining methods discussed section important choice training number nodes hidden layers. optimal number organisation layers complicated dependence number training data points number inputs outputs complexity function trained. choosing nodes mean unable learn relationship highest possible accuracy; choosing many increase risk overﬁtting training data also slow training process. using empirical evidence theoretical considerations suggested optimal architecture approximating continuous function hidden layer containing nodes number input nodes. serra-ricart also empirical support suggestion. choice allows network model form mapping function without unnecessary work. practice better over-estimate number hidden nodes required. described section skynet performs basic checks prevent over-ﬁtting additional training time associated hidden nodes large penalty optimal network obtained early attempt. case given particular problem optimal network structure terms number hidden nodes distributed layers determined comparing denote network weights biases collectively network parameter vector skynet considers parameters random variables posterior distribution given training started random initial state state determined ‘pre-training’ procedure discussed below. former case network training begins setting random values network parameters sampled normal distribution zero mean variance latter case skynet makes pre-training approach developed hinton osindero obtains network weights biases close good solution network objective function. method originally devised autoencoders mind based model restricted boltzmann machines generative model learn probability distribution inputs. consists layer input nodes layer hidden nodes shown figure case inputs hidden layer back treated symmetrically weights adjusted number ‘epochs’ gradually reducing reproduction error. model autoencoder rbms ‘stacked’ rbm’s hidden layer input next. initial case nn’s inputs ﬁrst hidden layer; repeated ﬁrst second hidden layer central layer reached. network weights ‘unfolded’ using transpose symmetric connections decoding half provide decent starting point full training begin. shown fig. weights matrices deﬁned pre-training. training performed using contrastive divergence procedure summarised following steps sampling indicates setting value node probability calculated otherwise. likelihood also depends hyperparameters describe standard deviation outputs likelihood encodes well characterised given parameters able reproduce known training data outputs. modulated prior assumed form hyperparameter plays role regularisation parameter optimisation since determines relative importance prior likelihood. prior also seen -norm penalty. form likelihood depends type network trained. number outputs number training data examples nn’s predicted output vector input vector network parameters hyperparameters {σi} describe standard deviation outputs. classiﬁcation problems skynet uses continuous outputs interpreted probabilities inputs belongs particular output class. achieved applying softmax transformation output values non-negative unity namely scenario true predicted output values probabilities true outputs zero except correct output value unity. classiﬁcation networks hyper-parameters appear log-likelihood. pre-training approach also used general feed-forward networks. layers weights except ﬁnal connects last hidden layer outputs pretrained ﬁrst half symmetric autoencoder. however network weights unfolded; instead ﬁnal layer weights initialised randomly would done without pre-training. network ‘learns inputs’ mapping outputs. shown greatly reduce training time multiple problems glorot bengio erhan note autoencoder pre-trained activation function central hidden layer made linear activation function ﬁnal hidden layer outputs made sigmoidal. general feed-forward networks pre-trained continue original activation functions. simply default settings user freedom alter suit speciﬁc problem. first initial values hyperparameters set. values user either true output values whitened values difference settings magnitude error used. algorithm calculates large initial estimate total number weights biases rate user default deﬁnes size ‘conﬁdence region’ gradient. expression sets larger regularisation magnitude gradient likelihood larger. relates amount ‘smoothing’ required steepness function smoothed. rate factor denominator allows increase damping smaller conﬁdence regions value gradient. results smaller conservative steps likely result increase function value results steps required reach optimal weights. training proceeds using adapted form truncated newton optimisation algorithm described below calculate step taken iteration. following step adjustments made another step calculated. first updated multiplying value serves assure convergence value equals number unconstrained data points problem. similarly updated probability maximised current parameters procedures described detail gull skilling hobson gradient hessian matrix second derivatives evaluated purposes function log-posterior distribution parameters hence represents gaussian approximation posterior. hessian log-posterior regularised hessian log-likelihood function prior whose magnitude provides regularisation. deﬁne hessian matrix log-likelihood identity matrix. regularisation parameter interpreted controlling level ‘conservatism’ gaussian approximation posterior. particular regularisation helps prevent optimisation becoming trapped small local maxima smoothing function explored. also aids reducing region conﬁdence gradient information make less likely step results worse parameters. ideally seek step using principle iterating stepping procedure eventually bring local maximum moreover newton’s method important property scale-invariant namely behaviour unchanged linear rescaling parameters. methods without property often problems optimising poorly scaled parameters. however major practical difﬁculties standard newton’s method. first hessian loglikelihood guaranteed positive semi-deﬁnite. thus even addition damping term derived log-prior full hessian log-posterior also invertible. second even invertible inversion prohibitively expensive number parameters large case even modestly-sized neural networks. address ﬁrst issue replace hessian form gauss–newton approximation guaranteed positive semi-deﬁnite deﬁned regression likelihood classiﬁcation likelihood respectively particular approximation used differs classical gauss–newton matrix retains second derivative information. second avoid prohibitive expense calculating inverse instead solve iteratively using conjugate-gradient algorithm requires matrix-vector products vector avoid even computational burden calculating storing hessian principle products form easily computed using ﬁnite differences cost single extra gradient evaluation using identity approach however subject numerical problems. therefore instead calculate products using stable efﬁcient procedure applicable involves additional forward backward pass network beyond initial ones required gradient calculation. puts. computationally cheap method estimating suggested mackay whereby adds gaussian noise true outputs training data trains network noisy data. performing many realisations networks’ predictions average predictions absence added noise. moreover standard deviation predictions provide good estimate accuracy original network’s predictions. since train networks using original trained network starting point re-training noisy data fast. additionally evaluating ensemble predictions measure accuracy computationally intensive network evaluations simple perform done less millisecond. explicitly steps method addition steps skynet includes option user random gaussian offsets parameters training performed data offset training moving optimisation potential local maximum posterior distribution network parameters size offset must chosen problem; this recommend using value thus noise training data saved network parameters training network whose posterior maximum near exactly original network’s. method compared described graff determining accuracy predictions likelihood used bambi. although method described requires overhead time training additional networks small compared speed gains possible. indeed method’s accuracy computations require less millisecond opposed tenths second method used previously. consequently faster method described incorporated public release version bambi leading around orders magnitude increase speed reported graff following iteration optmisation algorithm posterior likelihood correlation error squared values calculated training data validation data correlation network outputs deﬁned output means output variables training data; functional dependencies dropped brevity. correlation provides relative measure well predicted outputs match true ones. practice correlations output averaged together give average correlation network’s predictions. average error-squared network outputs deﬁned might expect optimisation proceeds steady increase values posterior likelihood correlation negative error squared evaluated training validation data. eventually however algorithm begin overﬁt resulting continued increase quantities evaluated training data decrease evaluated validation data. divergence behaviour taken indicating algorithm converged optimisation terminated. user choose four quantities listed used determine convergence although default error squared since include hyperparameters calculation less prone problems zeros correlation. note correlation error-squared functions discussed also provide quantitative measures compare performance different network architectures terms number hidden nodes distributed layers. network size complexity increased point reached minimal gains achieved increasing correlation reducing error-squared. therefore network architecture achieve peak performance equally well-suited. practice wish smallest simplest network minimizes risk overﬁtting time required training. figure correlation error-squared values function number hidden nodes obtained converged architecture ramped sinc function regression problem. perform regression data items divided randomly items training validation. simple problem network single hidden layer containing nodes whiten input output data using network pre-trained. optimal value determined comparing correlation error-squared networks different numbers hidden nodes. results presented fig. shows correlation increases error-squared decreases reach hidden nodes measures level off. thus adding additional nodes beyond number improve accuracy network. network hidden nodes obtain correlation greater cent; comparison true predicted outputs case shown figure consider classiﬁcation problem based threeway classiﬁcation data created radford neal testing algorithms training. data four variables drawn times standard uniform distribution two-dimensional euclidean distance less point placed class otherwise class neither conditions true class note values play part classiﬁcation. gaussian noise zero mean standard deviation added input values. approximately percent data used training remaining cent validation. network single hidden layer containing nodes whiten input output data using network pretrained. full network thus architecture three output nodes give probabilities figure comparisons true predicted values obtained converged architecture training data validation data ramped sinc function regression problem. optimal value determined comparing correlation error-squared networks different numbers hidden nodes. results shown fig. correlation increases error-squared decreases reach hidden nodes measures level off. network hidden nodes total cent training data points cent validation points correctly classiﬁed. summary classiﬁcation results network given table results compare well neal’s original results similar classiﬁcations based applying original criteria directly data points noise added. figure shows data true classiﬁcations. dimensionality reduction common task astronomy usually performed using principal component analysis variants. eigenvalues eigenvectors correlation matrix centred data found. eigenvector directions deﬁne variables mutually-orthogonal linear combinations original variables describing data item. dimensionality reduction achieved keeping combinations corresponding largest eigenvalues. limited however orthogonal projections recent interest independent component analysis still constructs linear combinations original variables relaxes condition orthogonality specifically ﬁnds directions projections data onto directions maximum statistical independence either minimisation mutual information maximization non-gaussianity. provide quick comparison autoencoders traditional ﬁrst consider examples data points drawn single multivariate gaussian distribution assumed using theoretical covariance matrix given eigenvalues eigenvectors. basic check cases perform main step calculating eigenvalues eigenvectors sample covariance matrix resulting data points match assumed closely. ﬁrst example draw data points two-dimensional correlated gaussian. simple case ﬁrst train autoencoder single hidden layer. moreover effect dimensionality reduction place node hidden layer full network architecture whitening input output data using performed. pre-training also used even small autoencoder network easy optimiser fall large local maximum output average inputs. shows original data curve traced data space performs decoding value feature variable single central layer node varied limits obtained encoding data. might expect curve approximates eigenvector larger eigenvalue covariance matrix data. curve exactly straight line non-linearity activation function hidden layer output layer inﬂuenced particular realisation data analysed. noted dimensionality reduction performed conversely encoding data point obtain corresponding feature value central layer node rather performing pca-like projection data space. resulting error-squared correlation antoencoder percent respectively. pursue comparison further also train similar manner autoencoder nodes single hidden layer full network architecture although clearly longer relevant dimensionality reduction. shows original data together curves traced data space performs decoding varies feature vectors respectively central layer nodes. that ﬁrst case recovers curve similar shown whereas second case curve approximates eigenvector data covariance matrix smaller eigenvalue. before neither curve exactly straight line non-linearity activation function. moreover curves intersect right-angles would case principal component directions. resulting correlation error-squared antoencoder percent respectively. note latter close percent would expect two-dimensional data set. second example demonstrate ability determine optimal number nodes single hidden layer autoencoder redundant information provided. accomplish this draw data points -dimensional correlated gaussian figure original data points drawn two-dimensional correlated gaussian distribution together curve traced performing decoding varies single feature value central layer trained autoencoder architecture curves traced performing decoding varies feature vectors respectively central layer trained autoencoder architecture cases feature values varied within limits obtained encoding data. ‘rotate’ points -dimensional space. thus data point form points dimensional hyperplane space. similar manner above train autoencoders architecture varied error-squared correlation resulting networks given table expected three hidden nodes used correlation close percent adding improve results. note that desired ‘rank’ feature variables according amount decrease error-squared increase correlation analogous ranking eigenvectors according eigenvalues pca. consider another two-dimensional example data drawn distribution different single multivariate gaussian assumed pca. particular data distributed partial ring centred table error-squared correlation autoencoder networks architecture applied data points drawn -dimensional correlated gaussian distribution ‘rotated’ -dimensional space. data plotted light blue points figure although noiseless data fully determined single parameter clear linear dimensionality reduction method would unable represent data accurately single component. indeed dominant principal component would along straight horizontal line passing point easily veriﬁed practice. projections onto direction clearly distinguish data points x-coordinate lying opposite sides symmetry line. show however possible represent data well using single variable taking advantage non-linearity antoencoder. slightly challenging example train autoencoder three hidden layers single node central layer perform dimensionality reduction. thus full network architecture whitening input output data applied using network pretrained. optimal value determined comparing correlation error-squared networks different numbers hidden nodes. results shown fig. shows optimal performance reached beyond signiﬁcant improvement results adding hidden nodes. results obtained antoencoder architecture shown figure panel shows curve traced performing decoding varies single feature value central layer trained autoencoder architecture clearly follows ring structure closely. curve traced allowed vary range encountered training shown sees curves extend short distance ring curve extending either gap. conversely bottom panel shows encoded feature value obtained compared true angle input data. that expected strong monotonic relationship variables. example consider two-dimensional case data drawn distribution different single multivariate gaussian rather simulating long curving degeneracy focus distribution possessing multiple modes. particular data generated four equal gaussian modes standard deviation directions means respectively; example also originally presented serra-ricart data plotted light blue points figure case intuitively obvious extent data represented using single variable. clear however linear dimensionality reduction method would unable represent data accurately single component. indeed case principal directions along diagonal lines degrees passing point equal eigenvalues either direction used perform dimensionality reduction. projection onto line degrees clearly distinguish data points given line perpendicular direction thereby conﬂating data points modes ﬁgure. thus resulting histogram projected values data points contain three peaks show however possible distinguish four modes using single variable makes non-linear nature autoencoders. train autoencoder architecture using whitening input pre-training. optimal value determined comparing correlation error-squared networks different numbers hidden nodes. investigating values performed best small margin. results network shown figure panel shows curve traced performing decoding varies single feature value central layer trained autoencoder; curve traces path centre outskirts four modes corresponds distinct range feature values. illustrated bottom panel shows histogram encoded feature values obtained data set. histogram contains four clear peaks corresponding modes original data distribution. setting appropriate threshold values feature variable classify points belonging modes cent accuracy mnist database handwritten digits subset larger collection available nist consists training validation images handwritten digits. digit sizenormalised centred pixel greyscale image. images publicly available online along information generation data results previous analyses researchers. data become standard testing figure original data points curve traced performing decoding varies single feature value central layer trained autoencoder architecture curve traced allowed vary range encountered training shown true angle training data points versus encoded feature values. table error rates classiﬁcation networks different architectures trained identify handwritten digits greyscale images mnist database. networks inputs outputs. dimensionality reduction mnist data also performed. particular large deep autoencoder networks trained hidden layers ++++ hidden layers networks inputs outputs corresponding image size. thus dimensionality reduction feature variables respectively represents signiﬁcant data compression. network able obtain average total error squared obtained average total error squared values comparable obtained hinton salakhutdinov thus despite reducing dimensionality input data used pixels non-linear feature variables reduced basis sets retain enough information original images reproduce within small errors. also demonstrates skynet capable training large deep autoencoder networks. mentioned previously dimensionality reduction sometimes used prelude supervised-learning task classiﬁcation since latter sometimes performed accurately reduced space original data space. illustrate approach using figure original data points curve traced performing decoding varies single feature value central layer trained autoencoder architecture histogram encoded feature values obtained input data; four separate peaks visible corresponding four gaussian modes labelled. machine learning algorithms. sample digits shown figure learning task identify correctly digit written image. although easy task human brain quite challenging machine learning application. several classiﬁcation networks varying complexity trained problem. cases pre-training used hidden layers inputs whitened using transformation network inputs outputs class assigned image highest output probability. results obtained summarised table error rates deﬁned fraction incorrectly classiﬁed calculated validation images. note networks large deep nonetheless well trained using skynet. results compared referenced mnist website yield error rates pertable error rates classiﬁcation networks different architectures trained autoencoder feature values identify handwritten digits mnist database. classiﬁcation networks inputs outputs. training images passed autoencoder obtain encoded feature values image. feature values used train classiﬁcation network identify handwritten digits. resulting error rates listed table networks different numbers hidden layers nodes. particular comparing table resulting classiﬁcations nearly accurate best-performing network trained full images despite reducing dimensionality input data pixels non-linear feature variables reducing number classiﬁcation network parameters several orders magnitude. finally massive dimensionality reduction feature variables performed images training large deep autoencoder hidden layers expected network signiﬁcantly less able reproduce original images average total error squared advantage plot feature values obtained images provide simple illustration clustering. scatterplot shown fig. validation images points colour-coded according true digit contained image; ﬁgure compared ﬁgure hinton salakhutdinov signiﬁcant overlap digits similar shapes digits occupy distinct regions parameter space section perform simple comparison skynet alternative algorithm training case simple sinc problem section compare fann library. library developed several years thus features interfaces implemeneted thus skynet. however training performed standard backpropagation techniques ﬁrstorder nature. containing nodes found fann’s optimal predictions time equivalent skynet. however fann performed approximately order magnitude steps parameter space reach solution feature prevent overﬁtting larger trained furthermore running multiple times skynet produces similar time results consistently fann times varied greatly often converge result. lastly fann requires user create main function setup network trained read data perform training save network. comparison skynet seeks make functions easier user asking input settings formatted data. additional functionality implemented future releases useful simple tool provided now. mapping dark matter challenge presented www.kaggle.com simpliﬁed version great challenge problem data item consists greyscale images galaxy star respectively. pixel value image drawn poisson distribution mean equal underlying intensity value pixel. moreover images convolved same unknown point spread function. learning task pair images predict ellipticities underlying true galaxy image thus constitutes regression problem. training data contains image pairs challenge data contains image pairs. sample galaxy star image pair shown challenge solutions validation data kept secret participating teams submitted predictions. details challenge descriptions results found kitching table root mean square errors ellipticity predictions networks different architectures evaluated image pairs mapping dark matter challenge. networks outputs galaxy ellipticities vary range details data found challenge’s webpage also gives unweighted quadrupole moments formula calculating ellipticities image. competition organisers note however formula provide good esimates true galaxy ellipticities since account pointspread function noise. skynet train several regression networks takes galaxy star images inputs produces estimates true galaxy ellipticities outputs. following approach used original challenge quality network’s predictions measured root mean squared error predicted ellipticities challenge data pairs images. clearly better predictions result lower values rmse. size dataset meant training large networks computationally expensive. therefore demonstration train relatively small networks used three different data sets full galaxy star images; full galaxy image centrally cropped star image; full galaxy image alone. training data provided consisting image pairs cent used training networks remaining percent used validation. rmse values trained networks different architecture evaluated challenge dataset given table rmse values obtained even naive ﬁrst approach using full dataset inputs quite good; comparison standard software package sextractor produced rmse score test data uwqm method scores table increasing number hidden nodes beyond improve network accuracy slowly. results however improved easily simply reducing number inputs without affecting information content example cropping star images central pixels yield dataset simple change increases accuracy ellipticity predictions thereby lowering rmse. increasing number nodes single hidden layer adding extra hidden layer yield improving predictions although rate improvement quite gradual. nonetheless indicates complex networks could improve accuracy ellipticity predictions. best result obtained networks investigated rmse compares well competition winners achieved rmse using mixture methods included nns. note score produced using immediate ‘out-of-the-box’ application skynet involves different architectures trained data found hidden layer conﬁgurations performed equally well classiﬁcation task. thus results presented section network hidden layers applied blind grbs. since outputs probabilities investigate quality classiﬁcation function threshold probability required class claim detection. discussed feroz marshall hobson compute expected number total detections correct detections false detections well derived statistics function without knowing true classiﬁcations label probability detection blind expected total number total grbs expected number correctly predicted true expected number falsely predicted false given following figure plot actual expected completeness purity. clear actual expected curves another minimal differences. thus without knowing true classiﬁcations grbs detected obtain desired completeness purity levels ﬁnal sample. information also plot actual expected receiver operating characteristic curves curve originated signal detection theory reliable choosing optimal threshold value well comparing binary classiﬁers. curve plots true positive rate false positive rate function threshold value. perfect classiﬁer table however reducing number inputs removing star images altogether leads signiﬁcant increase rmse. expected since absence star images allow infer pointspread function sufﬁciently well predict underlying galaxy ellipticities accurately. finally note results could potentially improved ﬁtting proﬁles images using parameters training would reduce number inputs orders magnitude. alternatively could train autoencoder dimensionally-reduce image feature variables; would vastly reduce number network inputs also potentially alleviate effect noise images. investigations however postponed future publication. long gamma-ray bursts almost indicators corecollapse supernovae deaths massive stars. ability determine intrinsic rate events function redshift essential studying numerous aspects stellar evolution cosmology. swift space telescope multi-wavelength detector currently observing hundreds grbs however swift uses complicated combination triggering criteria identifying grbs makes difﬁcult infer intrinsic rate. indeed studies approximate swift trigger algorithm single detection threshold lead biasses inferring intrinsic rate function redshift. investigate issue further recent study lien performed monte carlo analysis generated mock sample grbs using rate luminosity function wanderman piran processed entire simulated swift detection pipeline applying full swift trigger criteria determine grbs would detected. found resulting measured rate function redshift followed closely true swift described fynbo ﬁnding consistent mock sample simulated trigger pipeline good approximations reality. analysis however quite computationally expensive since determining detected swift requires minute single cpu. goal replace simulated swift trigger pipeline classiﬁcation determine microseconds whether given detected. training data pre-computed mock sample grbs lien particular divide sample randomly training validation ﬁnal blind test perform ﬁnal evaluations. inputs total luminosity redshift energy peak together arrival swift detector size light curve emitting frame parameters grb’s band function spectrum background intensity four different energy ranges angle arrival detector total ﬂux. softmaxed outputs correspond using wish determine well detection rate respect redshift reproduced since relationship deriving scientiﬁc results. figure show event counts function redshift classiﬁer original simulated swift trigger pipeline. clear sets counts agree well. bottom part ﬁgure calculate measure error within redshift computing ‘true’ number grbs detected full simulated swift pipeline number obtained using classiﬁcation original detection counting essentially poissonian process intrinsic normalized error remain within bins error introduced similarly exceed magnitude. note networks used obtain results trained hours thereafter make accurate determination whether detected swift microseconds instead minutes computation time required full simulated swift trigger pipeline. astronomical data analysis data often contains great deal redundant information. simply usually many pixels astronomical image distinct features identiﬁed object imaged pixels independent measures structure. able compress data removing redundancies instead quantify distinct features present efﬁcient subsequent analyses. ﬁnding features data performing compression denoising autoencoders. already shown autoencoders able represent non-linear features data reduce number variables used describe value closer intrinsic dimensionality data. figure actual expected curves classiﬁer predicts whether detected swift. curve traces true versus false positive rates probability threshold varies illustrated inset log-log plot. curve connects whereas random classiﬁer yield curve diagonal line connecting directly. general larger area underneath curve powerful classiﬁer. figure expected actual curves classiﬁer close small deviations occuring false positive rates; noted actual curve better expected one. curves also indicate test quite powerful predicting grbs detected swift. using completeness purity curves make decision appropriate value use. require certain level completeness regardless false positives require minimal level contamination ﬁnal sample. alternatively curve derive optimal value pth. example value curve intersects diagonal line connecting table mean rmse values autoencoder reconstructions galaxy cropped star images challenge data set. pixel values range normalized rmse errors divided square root original pixel value reconstructed. previous analysis presented section measuring galaxy ellipticities challenge data images provides good example. original galaxy images contain pixels clearly independent measurements. additionally even cropped star images contain non-independent pixels. autoencoder compress images orders magnitude thousands input variables tens measure ellipticities. perform compression autoencoders single hidden layer trained since images contain relatively features simpler networks require less time train. pretraining used examples. input data pixels galaxy accompanying star. since noise pixel poissonian report table rmse autoencoder outputs also rmse normalised original pixel value. values listed table correspond mean values obtained collection images comprising training data set. values obtained image another mutually consistent autoencoder nodes hidden layer image-to-image variations signiﬁcantly larger differences means. sees table slight improvement observed using hidden nodes. indeed taking account image-to-image variation predictions larger networks statistically indistinguishable smaller ones. therefore represent images well feature values. consider original construction images galaxy represented parameters star parameters means that without noise parameters needed describe images completely. reﬂected ability autoencoder perform ‘majority’ features. additional features produce marginal decreases rmse ﬁne-tuning and/or ﬁtting noise data. looking pixel-to-pixel comparisons ﬁnds large part error coming fainter pixels part distinctly external galaxy. therefore galaxy described even accurately numbers presented indicate. plot comparison typical galaxy/star pair shown figure corresponding origfigure pixel-by-pixel comparison typical galaxy/star image pair original autoencoder reconstructed images. larger errors occurring smaller true pixel values associated background. used autoencoder single hidden layer nodes. illustrate nature feature vectors constructed network decoding central layer values etc. autoencoder hidden nodes obtain corresponding output images. plot images figure galaxy/star example shown figure shape features clearly seen although greyscale reversed images brighter structures. reversal accounted network assigning original features negative weights positive bias. trained autoencoders investigate using compressed feature values rather original images determine ellipticities galaxies. since number inputs decreased larger hidden layers regression network analysis figure features vectors obtained decoding etc. central layer autoencoder central layer nodes. shown extracted galaxy star features. greyscale reversed actual values. results show extra information given regression networks trained feature values autoencoder acted disadvantage predicting galaxy ellipticities. networks trained features however accuracies predicted ellipticities better even obtained using full original images cases. demonstrates power able eliminate redundant information noise thereby improve accuracy analysis. also observe adding unnecessary complexity structure makes difﬁcult algorithm global maximum. method dimensionality reduction also eliminates noise performing measurements clearly applied wide range astronomical applications. examples include classiﬁcation supernovae type measurements galaxies stars spectra. described efﬁcient robust neural network training algorithm called skynet made freely available academic purposes. generic tool capable training large deep feed-forward networks including autoencoders applied supervised unsupervised machine learning tasks astronomy regression classiﬁcation density estimation clustering dimensionality reduction. skynet employs combination pre-training followed iterative reﬁnement network parameters using regularised variant newton’s optimisation algorithm incorporates second-order derivative information without need even compute store hessian matrix. linear sigmoidal activation functions provided user choose between. skynet adopts converﬁrst demonstrate capabilities skynet examples regression classiﬁcation dimensionality reduction using autoencoder networks apply classic machine learning problem handwriting classiﬁcation determining digits mnist database. astronomical context skynet applied regression problem measuring ellipticity noisy convolved galaxy images mapping dark matter challenge; classiﬁcation problem identifying gamma-ray bursters detectable swift satellite; dimensionality reduction problem compressing denoising images galaxies. case straightforward skynet produces networks perform desired task quickly accurately typically achieve results competitive machine learning approaches tailored required task. future development skynet expand upon many current features introduce ones. working include activation functions pooling nodes convolutional diversity outputs robust support recursive nns. addition improving speed efﬁciency training algorithm itself. however skynet current state already useful tool performing machine learning astronomy. authors thank john skilling providing useful advice early stages algorithm development. also thank lien providing data used seciton work utilized three different high-performance computing facilities different times initial work performed cosmos viii altix supercomputer funded sgi/intel hefce pparc authors thank andrey kaliazin assistance; early work also utilized darwin supercomputer university cambridge high performance computing service provided dell inc. using strategic research infrastructure funding higher education funding council england; later work utilised discover system nasa center climate simulation nasa goddard space flight center. currently supported nasa postdoctoral fellowship ridge associated universities completed portion work funded gates cambridge scholarship university cambridge. supported research fellowship leverhulme newton trusts. auld bridges hobson m.p. mnras ball n.m. brunner r.j. int. mod. phys. bergstra desjardins lamblin bengio technical report d´epartement dinformatique recherche op´erationnelle universit´e montr´eal. glorot bordes bengio proceedings fourteenth international conference artiﬁcial intelligence statistics journal machine learning research eds. gordon dunson gull s.f. skilling quantiﬁed maximum entropy memsys users’ manual. maximum entropy data consultants ltd. bury edmunds suffolk http//www.maxent.co.uk/ lien sakamoto gehrels palmer graziani proceedings international astronomical union longo tagliaferri andreon mining proceedings mpa/eso/mpe workshop eds. banday zaroubi bartelmann murtagh neural comput. pascanu bengio arxiv. pearlmutter b.a. neural comput. sanger t.d. neural networks schraudolph n.n. neural comput. serra-ricart calbet garrido gaitan skilling conference series tagliaferri neural networks tagliaferri longo andreon capozziello donalek giordano neural nets italian workshop neural nets eds. apolloni marinaro tagliaferri", "year": 2013}