{"title": "Monitoring Term Drift Based on Semantic Consistency in an Evolving  Vector Field", "tag": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Based on the Aristotelian concept of potentiality vs. actuality allowing for the study of energy and dynamics in language, we propose a field approach to lexical analysis. Falling back on the distributional hypothesis to statistically model word meaning, we used evolving fields as a metaphor to express time-dependent changes in a vector space model by a combination of random indexing and evolving self-organizing maps (ESOM). To monitor semantic drifts within the observation period, an experiment was carried out on the term space of a collection of 12.8 million Amazon book reviews. For evaluation, the semantic consistency of ESOM term clusters was compared with their respective neighbourhoods in WordNet, and contrasted with distances among term vectors by random indexing. We found that at 0.05 level of significance, the terms in the clusters showed a high level of semantic consistency. Tracking the drift of distributional patterns in the term space across time periods, we found that consistency decreased, but not at a statistically significant level. Our method is highly scalable, with interpretations in philosophy.", "text": "‡information technologies institute centre research technology hellas thessaloniki greece §institute applied biosciences centre research technology hellas thessaloniki greece energy machine learning related latter often based minimizing constrained multivariate function loss function. concepts feature space energy minima representing cost classiﬁcation decision energy minimizing process. suggests machine learning must identify concepts minima since potential energy physics carried ﬁeld respective topological mapping concepts naturally something energy work capacity. general process practically isomorphic theory reaction paths potential hypersurface leading probabilistic composition chemical compounds computational chemistry believe evolving ﬁelds metaphor simulate category formation semantic content legitimate approach. furthermore attractor networks establishing quasi-continuous ﬁeld capable processing word sentence meaning link considerations study neural networks. current line thought semantic continuity hypothesis i.e. assumption vocabulary modelled term space consists actual potential word content former mapped observable locations latter ﬁlling so-called lexical gaps them. linguistics offers innumerable examples existence gaps language lacks spelled i.e. actualized content contrast another one. continuity best modelled evolving ﬁeld actual potential word content constantly dislocated time. dislocations actual positions embedding potential contexts change offering rich texture semantic substance quasi charging actual term locations discharged potential ones. line thought applies vector space model sentence content given timestamped data measure dislocations called semantic drift important indicator ongoing language change prominently affecting monitoring novelties document indexing terminology. abstract—based aristotelian concept potentiality actuality allowing study energy dynamics language propose ﬁeld approach lexical analysis. falling back distributional hypothesis statistically model word meaning used evolving ﬁelds metaphor express timedependent changes vector space model combination random indexing evolving self-organizing maps monitor semantic drifts within observation period experiment carried term space collection million amazon book reviews. evaluation semantic consistency esom term clusters compared respective neighbourhoods wordnet contrasted distances among term vectors random indexing. found level signiﬁcance terms clusters showed high level semantic consistency. tracking drift distributional patterns term space across time periods found consistency decreased statistically signiﬁcant level. method highly scalable interpretations philosophy. modeling semantic content statistical analysis prominently means computational theoretical linguistics quietly inspired physics chemistry past decades. strictly metaphoric basis idea compare language rule-based system domains natural science like systems innovative model design. endeavours typically back kinds physical phenomena i.e. attraction acting like gravity force non-polar roots system attraction repulsion based polarity like electromagnetism. word meaning sentence meaning show statistical behaviour compliant idea non-polar polar binding forces allowing latent analytic thinking category building many areas including natural language processing bioinformatics networks quantum theory chemometry clearly similarity meaning attractor difference meaning repellent organizing principles conceptual processing cannot ignore interesting ahead explore implications observation. consider word semantics behaviour linguistic signs dual nature i.e. intertwined form content leading emergence conceptual categories objects ultimately applicability artiﬁcial neural networks machine learning. mathematical object feature categorization neural networks depends concept similarity fundamental binding force brieﬂy review measures semantic relatedness express thematic coherence linguistics relevant text processing prominent theories word meaning distributional hypothesis referential theory word semantics according ﬁrst meaning depends word i.e. contextual whereas second referential i.e. goes back convention expressed e.g. deﬁnitions ontology entries. habitual word context clearly implies agreements sense certain word forms used certain contexts dependency approaches. automated systems assign score semantic relatedness given pair terms calculated relatedness measure. absolute score typically irrelevant own; important measure assigns higher score term pairs humans think related comparatively lower scores term pairs less related distributional similarity predecessors back long building notion term dependency structures derived therefrom underlying distributional hypothesis often cited explaining word meaning enters information processing attempts utilize lexical resources purpose used sole source word semantics information retrieval inherent exploitation term occurrences notably term frequency-inverse document frequency measure co-occurrences including multiple-level term co-occurrences hand referential approach relies days lexical resources. lexical resource computer science structure captures semantic relations among terms quasi charging word occurrences context external information. reason combining approaches statistical techniques typically suffer sparse data problem perform poorly terms relatively rare. hybrid methods attempt address problem supplementing sparse data information lexical database semantic network differentiate weights edges connecting node child nodes needs consider link strength speciﬁc child link. situation corpus statistics contribute. following types resources commonly used measuring semantic similarity terms dictionaries semantic networks wordnet thesauri modelled roget’s thesaurus ontologies. general practice evaluating effectiveness information retrieval text categorization models measures like recall precision accuracy many ongoing work build semantic spaces distributional compositional semantics representing word sentence meaning locations high-dimensional space phrase sentence component binding recursive matrix-vector spaces tensor product circular holographic reduced representation routinely used models representation semantic content documents compared ideal state language provided human standards interpretation inherent evaluation method using geometry probability vehicle meaning i.e. building medium language aims maximizing similarity human standard statistical reconstruction. hypothetic original correlate spoken language called mental state internal state neuroscience recalls language thought hypothesis philosophy also called mentalese. joint element whereas language mental phenomenon assumed continuous uttered mathematically modelled representations discrete. duplicity returns hidden metaphysics traditional mentalist recent generalist-universalist theories language language tool operated something deeper thought reason logic cognition functions line biological-neurological mechanisms common human beings moreover linguistic school thought orthogonal theories called neohumboldtian ﬁeld theories word meaning goes back dual model discrete distributions related content called lexical semantic ﬁelds based language underpinned assumption conceptual ﬁelds mind. then lexical ﬁeld related words outward manifestation underlying conceptual ﬁeld total conceptual ﬁelds describes one’s world view another unrelated school thought saussure’s structural linguistics language mental grammar rule specifying ideal content pronunciation whereas speech stands exempliﬁcation rules important symptom lexical ﬁelds regions related content separated lexical gaps. nonexistent names things could exist rules particular language indicate possible conceptual distinctions mapped actual language mother’s father father’s father called grandfather english father’s brother distinguished mother’s brother called uncle. language-speciﬁc discontinuities semantic content play prominent role methodology. assumption products mind continuous mapping spoken language discrete goes back ultimately aristotle’s metaphysics. this existence reality described total components conceivable potentiality plus observable-measurable actuality names latent manifest capacity existents induce change. therefore current thinking existence consists layers potentiality actuality importantly ascribes ﬁeld nature mental experience potentiality layer indirectly perceive actualized values events. model departing idea similarity instances semantic ﬁeld logical next question coherent groups instances ﬁeld? relating term similarity semantic consistency domain restriction hypothesis answers question based ﬁltering away extracted false sense relations semantically related terms extracted corpus tend semantically coherent. semantic domains used ﬁlters integrating pattern-based distributional approaches capture characteristic properties semantic relations syntagmatic properties terms given relation tend co-occur texts mostly connected speciﬁc lexical-syntactic patterns pattern connects terms is-a aspect captured using pattern-based approach; domain properties semantic relation among terms holds belong semantic domain semantic domains sets terms characterized similar distributional properties corpus. approach detailed hand recent reincarnation semantic consistency distant supervision method identify reliable instances noisy instances inspecting whether instance located semantically consistent region. ﬁrst model local subspace around instance sparse linear combination training instances estimate semantic consistency exploiting characteristics local subspace context semantic dynamics growing body literature semantic drift language-related version abrupt parameter value changes data mining called concept drifts semantic drift mean features ontology concepts gradually change knowledge domain evolves alternatively different user communities reinterpret concept different context risk concepts lose rhetorical descriptive applicative power general sense topic important beyond linguistic implications especially managing semantic interoperability federations; respective research date focused generation semantic build vector space model random indexing able closely track changes evolving text collection. project space two-dimensional surface clusters shifts apparent emergent selforganizing maps; projection preserves local topology high-dimensional space allows model dynamic semantic ﬁelds. wordnet-based referential similarity measures evaluate semantic consistency also detect semantic drifts time. refer figure outline. build tfidf vector space model corpus provides foundation distributional semantic distance measures. basic tfidf space known extremely sparse nonzero elements. latent semantic analysis latent semantic indexing measures semantic information co-occurrence analysis corpus reduces dimensionality solves problem sparsity. dimension vector space reduced singular value decomposition. random indexing similar idea rely computationally intensive matrix decomposition. makes random indexing much scalable technique practice. instead ﬁrst constructing huge co-occurrence matrix using separate dimension reduction phase random projection builds incremental word space model random projection technique described three-step operation first document corpus assigned unique randomly generated representation called index vector. index vectors sparse high-dimensional ternary means dimensionality order hundreds consist small number randomly distributed values rest elements vectors then context vectors produced scanning text time word occurs context context’s d-dimensional index vector added context vector word question. words thus represented d-dimensional context vectors effectively words’ contexts. comparing term vectors random indexed space similarity measure euclidean distance cosine dissimilarity enables quantitative framework semantic analysis. self-organizing two-dimensional grid artiﬁcial neurons. neuron associated weight vector matches dimension training data. take instance training data closest weight vector pull closer data instance. also pull weight vectors nearby neurons closer data instance decreasing weight best matching unit. repeat procedure every training instance. constitutes epoch. repeat process second epoch smaller neighbourhood radius lower learning rate adjusting weight vectors. eventually neighbourhood function decreases extent training might stop. time needed train grows linearly data size grows linearly number neurons som. resulting network reﬂects local topology high-dimensional space emergent self-organizing maps contain much larger number target nodes embedding thus capture topology original space accurately using toroid avoids edge effects. nodes emergent self-organizing correspond terms; nodes best matching units special role identify semantic content terms. rest nodes interpolation semantic ﬁeld. since ﬁeld continuous nature toroid maps planar would introduce artiﬁcial discrete cut-off edges. structure described above extensively deployed tasks related determining semantic similarity terms primarily automatic text analysis artiﬁcial intelligence applications. towards direction many semantic similarity metrics proposed grouped four different categories path-based content-based feature-based hybrid metrics. path-based metrics similarity terms depends relative position taxonomy well length path linking concepts. representative examples deploying path-based measures include content-based metrics based information content available concept common information concepts share similar are. examples belonging category include feature-based metrics based properties ontology obtaining similarity value. common characteristics concepts similar are. relationships similar terms taxonomy also taken consideration. related approaches classical model proposed tversky recent approach presented wordnet large lexical database english created maintained princeton university publicly available research commercial users free charge latest version wordnet’s popularity arguably lies fact that besides merely offering short deﬁnitions usage examples contained nouns verbs adverbs adjectives also introduces certain types semantic relations terms. examples relations include synonymy hyper/hyponymy former assume terms compared come reference ontology latter compare terms different ontologies. since easy directly compare structure information content different ontologies case cross ontology similarity typically employs hybrid feature-based methods foundation measuring drift based random indexing subsequent tfidf spaces ﬁxed random seed. following method able derive subsequent low-dimensional spaces compared another. train emergent self-organizing term vector spaces ﬁrst period continue training lower learning rate arrive smoothly changing dynamics. potential source confusion point time-like variable iterations epochs training esom unrelated temporal aspects corpus. time always refer time related corpus period refer documents belonging certain time interval corpus. epoch hand refers training rounds esom. experiments based large text corpus tfidf spaces random indices created. random indices used generate sequence self-organizing maps. analysed topology maps consistency drifts. book reviews literary genre equivalent abstracts scientiﬁc articles cross-pollinated idea crowdsourcing underlying recommender systems i.e. summary article produced professional abstracting indexing service potentially many summaries item written users part professional part contributors. sense approaches represent user feedback. methodological perspective nature condensed semantic content them book reviews processing falls category e.g. text summarization hand combined sentiment analysis hand. blend represent interesting scalable resource complex semantic content neuromorphic studies. experiments described based stanford’s amazon book reviews data publicly available part university’s snap project. data spanned period years included approximately book reviews march every item data included product user information ratings well plain text content description. collection contained degenerate time stamps corresponding instances discarded. used lucene information retrieval software library builds inverted index interpreted row-major sparse representation term-document matrix. used semanticvectors package reducing dimensionality space. training emergent selforganizing maps used somoclu implementation semantic similarity metrics based java several semantic relatedness/similarity algorithms presented section iii-c. works wordnet constitutes improved version perl-based wordnet-similarity-.. experiments described subsequently deployed representative path-based semantic similarity method imminent future plan investigate behaviour methods note experiments open source. subsection aimed assessing whether proximity terms toroid plane indicates strong underlying semantic similarity. focus lied neurons term assigned interested average similarities within neurons. towards direction order evaluate consistency approach case single time period initially randomly divided terms period clusters terms groups computed average semantic similarity; then based derived average similarities determined empirical probability distribution. latter good approximation normal distribution. enabled make t-test evaluating signiﬁcance similarity described subsequently. idea assumed terms within neuron constituted random group total population. hence average similarities within neurons followed normal distribution mean equal empirical mean distribution variance equal based assumption empirical variance performed -sided t-test predeﬁned level signiﬁcance order assess whether average within neuron statistically signiﬁcantly greater empirical mean considered three generic cases neurons containing terms neurons containing terms ﬁnally neurons containing terms. three cases http//lucene.apache.org/ last accessed jan’ https//code.google.com/p/wsj/ last accessed jan’. http//wn-similarity.sourceforge.net/ last accessed feb’. https//github.com/peterwittek/concept drifts repeatedly performed -sided t-test every neuron calculated percentage cases could reject null hypothesis. percentage greater indicated number samples high average similarity much greater expected based assumption average similarities followed normal distribution level signiﬁcance comparing percentage level signiﬁcance t-test. cases rejected deduced overall level similarity terms within neurons statistically signiﬁcantly greater expected based initial assumption. meant terms within neuron demonstrated greater similarity comparison random group terms indicating grouping made sense. table displays percentages three cases different levels signiﬁcance three periods p-values derived application binomial test displayed since negligible. holds percentage much greater every case. {pair harvard scorpio monsignor misrepresentation}. intuitively displays apparent semantic similarity contrast terms included not. would expect would rejected rejected indeed case. however cases semantic similarity terms within neuron apparent null hypothesis rejected. chosen path-based semantic similarity metric thus leave future work application path-based information content-based methods. robustness statistical approach based initial assumption population averages followed normal distribution tested empirically. however believe groundwork interesting future investigations. several periods overall similarity within neuron converges. experiment paper involved three periods repeated process described previous subsection. calculated percentages cases rejected null hypothesis t-test three periods displayed table observed table slight decrease percentages period period. assess whether decrease important performed test proportional comparison resulting p-values shown table indicate decrease statistically signiﬁcant every case. means indeed slight differences demonstrate divergence. periods would needed order investigate whether convergence study imminent future. recently increased attention paid models evolving semantic content something represented dynamic vector ﬁeld. approach based hypothesis semantic continuity vocabulary allowing manifest latent word content mapped lexical forms word meaning term space behaving like energy constructing conceptual categories. spite simple design analyse book reviews years three periods t-test conﬁrmed overall level similarity terms within esom grid neurons signiﬁcantly higher expected i.e. neuron content statistically consistent. project received funding european union’s seventh framework programme research technological development demonstration grant agreement pericles. dar´anyi wittek grateful dominic widdows trevor cohen assistance semantic vectors package. authors would also like thank julian mcauley granting access amazon data set. baker computational approaches study language change language linguistics compass vol. baroni lenci distributional memory general framework corpus-based semantics computational linguistics vol. blacoe kasheﬁ lapata quantum-theoretic approach distributional semantics proceedings naacl-hlt- conference north american chapter association computational linguistics human language technologies june bulskov knappe andreasen measuring similarity conceptual querying flexible query answering systems ser. lecture notes computer science andreasen motro christiansen larsen eds. springer vol. bylesj¨o rantalainen cloarec nicholson holmes trygg opls discriminant analysis combining strengths pls-da simca classiﬁcation journal chemometrics vol. cohen widdows schvaneveldt rindﬂesch discovery distance farther journeys predication space proceedings bibmw- ieee international conference bioinformatics biomedicine workshops saussure course general linguistics deerwester dumais furnas landauer harshman indexing latent semantic analysis journal american society information science vol. dong hussain chang hybrid concept similarity measure model ontology environment move meaningful internet systems workshops ser. lecture notes computer science meersman herrero dillon eds. vol. springer mccarthy gaylord measuring word meaning context computational linguistics vol. pad´o structured vector space model word meaning context proceedings emnlp- conference empirical methods natural language processing october fodor language thought. harvard university press gliozzo pennacchiotti pantel domain restriction hypothesis relating term similarity semantic consistency proceedings naacl-hlt- conference north american chapter association computational linguistics human language technologies april semantic consistency local subspace based method distant supervised relation extraction proceedings acl- annual meeting association computational linguistics june house linguistic relativity translation amsterdam studies theory history linguistic science vol. jiang conrath semantic similarity based corpus statistics lexical taxonomy proceedings rocling- international conference research computational linguistics kanerva kristofersson holst random indexing text samples latent semantic analysis proceedings cogsci- annual conference cognitive science society vol. kohonen self-organizing maps. springer kontostathis pottenger framework understanding latent semantic indexing performance information processing management vol. taira bashyam kangarloo ﬁeld theoretical approach medical natural language processing ieee transactions information technology biomedicine vol. tang sentiment-speciﬁc representation learning document-level sentiment analysis proceedings wsdm- international conference search data mining. trier sprachliche feld neue jahrbucher wissenschaft varelas voutsakis raftopoulou petrakis milios semantic similarity methods wordnet application information retrieval proceedings widm- annual international workshop information data management. widdows ferraro semantic vectors scalable open source package online technology management application lrec- international conference language resources evaluation palmer verb semantics lexical selection proceedings acl- annual meeting association computational linguistics. association computational linguistics yang citationrank algorithm inheriting google technology designed highlight genes responsible serious adverse drug reaction bioinformatics vol. zhou wang model semantic similarity measuring wordnet proceedings iske- international conference intelligent system knowledge engineering vol. lesk automatic sense disambiguation using machine readable dictionaries tell pine cone cream cone? proceedings sigdoc- annual international conference systems documentation bandar mclean approach measuring semantic similarity words using multiple information sources ieee transactions knowledge data engineering vol. july lord stevens brass goble investigating semantic similarity measures across gene ontology relationship sequence annotation. bioinformatics vol. mcauley leskovec hidden factors hidden topics understanding rating dimensions review text proceedings recsys- conference recommender systems. morris beghtol hirst term relationships contribution text semantics information literacy lexical cohesion proceedings cais- annual conference canadian association information science peat willett limitations term co-occurrence data query expansion document retrieval systems journal american society information science vol. resnik using information content evaluate semantic similarity taxonomy proceedings ijcai- international joint conference artiﬁcial intelligence vol. august rodr´ıguez egenhofer determining semantic similarity among entity classes different ontologies ieee transactions knowledge data engineering vol. socher huval manning semantic compositionality recursive matrix-vector spaces proceedings emnlp-conll- joint conference empirical methods natural language processing computational natural language learning. association computational linguistics", "year": 2015}