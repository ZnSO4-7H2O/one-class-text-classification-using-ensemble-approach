{"title": "ForestHash: Semantic Hashing With Shallow Random Forests and Tiny  Convolutional Networks", "tag": ["cs.CV", "stat.ML"], "abstract": "Hash codes are efficient data representations for coping with the ever growing amounts of data. In this paper, we introduce a random forest semantic hashing scheme that embeds tiny convolutional neural networks (CNN) into shallow random forests, with near-optimal information-theoretic code aggregation among trees. We start with a simple hashing scheme, where random trees in a forest act as hashing functions by setting `1' for the visited tree leaf, and `0' for the rest. We show that traditional random forests fail to generate hashes that preserve the underlying similarity between the trees, rendering the random forests approach to hashing challenging. To address this, we propose to first randomly group arriving classes at each tree split node into two groups, obtaining a significantly simplified two-class classification problem, which can be handled using a light-weight CNN weak learner. Such random class grouping scheme enables code uniqueness by enforcing each class to share its code with different classes in different trees. A non-conventional low-rank loss is further adopted for the CNN weak learners to encourage code consistency by minimizing intra-class variations and maximizing inter-class distance for the two random class groups. Finally, we introduce an information-theoretic approach for aggregating codes of individual trees into a single hash code, producing a near-optimal unique hash for each class. The proposed approach significantly outperforms state-of-the-art hashing methods for image retrieval tasks on large-scale public datasets, while performing at the level of other state-of-the-art image classification techniques while utilizing a more compact and efficient scalable representation. This work proposes a principled and robust procedure to train and deploy in parallel an ensemble of light-weight CNNs, instead of simply going deeper.", "text": "tasks seems surprising much modest efforts invested related often difﬁcult problems image multimodal contentbased retrieval generally similarity assessment large-scale databases. problems arising primitives many computer vision tasks becoming increasingly important exponentially increasing information. semantic similarity-preserving hashing methods recently received considerable attention addressing need part signiﬁcant memory computational advantage representations. methods learn embed data points space binary strings; thus producing compact representations constant sub-linear search time; critical options low-cost truly data. embedding considered hashing function data translates underlying similarity collision probability hash generally similarity codes hamming metric. examples recent similarity-preserving hashing methods include locality-sensitive hashing kernelized version spectral hashing sparse hash kernel-based supervised hashing anchor graph hashing selftaught hashing deep supervised hashing profound similarity problems semantic hashing binary classiﬁcation numerous classiﬁcation techniques adapted former task. example multiple state-of-the-art supervised hashing techniques like hashing sparsehash hdml based deep learning methodologies. besides deep learning random forest another popular classiﬁcation technique recently shown great success large variety classiﬁcation tasks pose estimation object recognition however best knowledge random forests used construct semantic hashing schemes therefore don’t enjoy advantages compact efﬁcient codes. mainly acting hashing functions random forest fails prehash codes efﬁcient data representations coping ever growing amounts data. paper introduce random forest semantic hashing scheme embeds tiny convolutional neural networks shallow random forests near-optimal informationtheoretic code aggregation among trees. start simple hashing scheme random trees forest hashing functions setting visited tree leaf rest. show traditional random forests fail generate hashes preserve underlying similarity trees rendering random forests approach hashing challenging. address this propose ﬁrst randomly group arriving classes tree split node groups obtaining signiﬁcantly simpliﬁed two-class classiﬁcation problem handled using light-weight weak learner. random class grouping scheme enables code uniqueness enforcing class share code different classes different trees. non-conventional low-rank loss adopted weak learners encourage code consistency minimizing intra-class variations maximizing inter-class distance random class groups. finally introduce information-theoretic approach aggregating codes individual trees single hash code producing near-optimal unique hash class. proposed approach signiﬁcantly outperforms state-ofthe-art hashing methods image retrieval tasks largescale public datasets performing level state-of-the-art image classiﬁcation techniques utilizing compact efﬁcient scalable representation. work proposes principled robust procedure train deploy parallel ensemble light-weight cnns instead simply going deeper. figure foresthash embeds tiny convolutional neural networks shallow random forests. foresthash consists shallow random trees forest usually depth tree split node arriving classes randomly partitioned groups signiﬁcantly simpliﬁed two-class classiﬁcation problem sufﬁciently handled light-weight weak learner usually layers. visited tree leaf rest. simultaneously pushing data point trees depth obtain -bit hash codes. random grouping classes enables code uniqueness enforcing class shares code different classes different trees. non-conventional low-rank loss adopted weak learners encourages code consistency minimizing intra-class variations maximizing inter-class distance random class groups. obtained foresthash codes serve efﬁcient compact image representation image retrieval classiﬁcation. serve underlying similarity inconsistency hash codes generated tree class data; also lacks principled aggregating hash codes produced individual trees single longer code. paper propose foresthash scheme. shown figure proposed foresthash designed provide consistent unique hashes images semantic class embedding tiny convolutional neural networks shallow random forests. start simple hashing scheme random trees forest hashing functions setting visited tree leaf rest. enable hashing scheme ﬁrst introduce random class grouping randomly partition arriving classes groups tree split node. class random grouping enables code uniqueness enforcing class share code different classes different trees also produces signiﬁcantly reduced twoclass problem sufﬁciently handled light-weight cnn. adopt non-conventional low-rank loss weak learners encourage code consistency minimizing intra-class variations maximizing inter-class distance random class groups thereby preserving similarity. low-rank loss based assumption high-dimensional data often small intrinsic dimension. consequently data lowdimensional subspace arranged columns single matrix matrix approximately low-rank. section show learn linear transformation subspaces using matrix nuclear norm optimization criterion. discuss experimentally theoretically learned transformation simultaneously minimizes intra-class variation maximizes inter-class separation. show kernelization deep learning note similarity assessments image pairs using foresthash codes involve single instruction modern architectures e.g. popcnt sse. even native software implementation still observe three orders magnitude speedup original features. finally proposed information-theoretic aggregation scheme provides principled combine hashes independently trained random tree forest. aggregation process discussed section performed efﬁciently greedy still achieves nearoptimal solution submodularity mutual information criterion optimized. discuss unsupervised supervised hash aggregation. section show comprehensive experimental evaluation proposed representation scheme demonstrating signiﬁcantly outperforms state-of-the-art hashing methods large-scale image multi-modal retrieval tasks. ﬁrst discuss simple random forest hashing scheme independently trained random trees hashing functions setting visited tree leaf rest. also show hashes forest often fail preserve desired intra-class similarity. random forest ensemble binary decision trees tree consists hierarchically connected split nodes leaf nodes. split node corresponds weak learner evaluates arriving data point sending left right child based weak learner binary outputs. leaf node stores statistics data points arrived training. testing decision tree returns class posterior probability test sample forest output often deﬁned average tree posteriors. tree different algorithms like grow tree relying termination criteria; also avoid post-training operations tree pruning. thus tree depth consists tree leaf nodes indexed breadth-ﬁrst order. training introduce randomness forest combination random sampling randomized node optimization thereby avoiding duplicate trees. discussed training tree different randomly selected decreases risk overﬁtting improves generalization classiﬁcation forests signiﬁcantly reduces training time. given classes introduce node randomness randomly partitioning classes arriving binary split node categories. pedagogic hashing scheme constructed follows data point pushed tree reaching corresponding leaf node. simply leaf nodes visited rest. ordering bits predeﬁned node order e.g. breadth-ﬁrst order obtain -bit hash code always containing exactly random forest consisting trees depth point simultaneously pushed trees obtain -bit hash codes. hashing scheme several obvious characteristics advantages first training hashing processes done parallel achieve high computational efﬁciency modern parallel hardware. second multiple hash codes obtained forest independently trained tree potential inherit boosting effect random forest i.e. increasing number trees increases accuracy finally scheme guarantees -sparsity hash codes tree. however hashes forest fail preserve underlying data similarity. classiﬁcation forest originally designed ensemble posterior obtained averaging large number trees thus boosting classiﬁcation accuracy conﬁdent class posteriors required individual trees. however lack conﬁdent class posteriors individual trees obtain highly inconsistent hashes individual tree class data. also obvious combine hashes different trees given target code length. inconsistency hash codes prevents standard random forest directly adopted hashing being codes critical large-scale retrieval. inconsistency becomes severe increasing tree depth leaf nodes available distribute class samples. problem solved simply increasing number trees longer total length. example -bit inconsistency allowed -bit hash code hamming ball already contains codes. principled required combine hashes tree. choose combine hashes different trees simply concatenating averaging thresholding voting. however principles behind heuristics obvious might loose control code length sparsity even binarity. follows address problems. propose random class grouping scheme followed near-optimal code aggregation enforce code uniqueness class. also adopt non-conventional low-rank loss weak learners encourage code consistency. random class grouping random class grouping scheme ﬁrst introduced randomly partition arriving classes groups tree split node. random class grouping serves main purposes first multi-class problem signiﬁcantly reduced two-class classiﬁcation problem split node sufﬁciently handled lightweight weak learner. second random class grouping enforces class share code different classes different trees allows information-theoretic aggregation developed sequel later produce nearoptimal unique hash code class. low-rank loss non-conventional low-rank loss adopted weak learners forest. consider s-dimensional data points belonging classes random class grouping simplicity denoted positive negative. stack points columns matrices respectively. ||a||∗ denote nuclear norm matrix i.e. singular values. nuclear norm known convex envelope matrix rank unit ball matrices following result helps motivate per-node classiﬁer lemma matrices dimensions denote column-wise concatenation. then ||||∗ ||a||∗ ||b||∗ equality holding column spaces orthogonal. based lemma loss function reaches minimum column spaces classes become orthogonal applying learned transformation equivalently reaches minimum subspaces classes maximally opened transformation i.e. smallest principal angle simultaneously minimizing ﬁrst spaces equals figure synthetic two-class examples illustrating properties learned low-rank transformation. transformed respectively. classes deﬁned {blue cyan} {yellow red}. kernel applied transform nuclear norm terms helps reduce variation within classes. synthetic examples presented figure illustrate properties learned transformation. trivial solution avoided good initialization. splitting functions. random class grouping two-class classiﬁcation problem split node. stack training data points class columns matrices respectively. training i-th split node denote arriving training samples weight matrix successfully learned minimizing reasonable assume classes belong low-dimensional subspace distance used classify previously unseed points. k-svd learn pair dictionaries classes minimizing optimization. optimize low-rank loss function using gradient descent subgradient nuclear norm matrix computed follows uσvt decomposition matrix columns corresponding eigenvalues larger predeﬁned threshold. following subgradient nuclear norm evaluated simpliﬁed form kernelization. sufﬁcient number tree splits could potentially handle non-linearity data classiﬁcation. work limited number splits preferred tree e.g. depth encourage short codes insufﬁcient modeling data non-linearity well. moreover rely tree splits modeling non-linearity still obtain less conﬁdent class posteriors explained. low-rank loss particularly effective data approximately linear subspaces improve ability handling generic data effective data points inner product space prior optimize low-rank loss. given data point create nonlinear ...; computing inner product between ﬁxed points randomly drawn training set. inner products computed kernel function satisfy mercer conditions; note explicit representation required. examples kernel functions include polynomial kernels radial basis function kernels variance given data points mapped data denoted learn weight matrix minimizing deep networks. kernelization shows simple effective non-linear mapping learner ultimate handling intricate data. gradient descent optimization discussed above possible implement following function ||φ||∗ ||φ||∗−|| φ]||∗ figure angles deep features learned validation cifar- using vgg-. additional low-rank loss. standard softmax loss. lowrank loss intra-class variations among features collapsed inter-class features orthogonal particularly preferred tree split node. low-rank loss layer general deep networks denotes mapping deep network. experimental experience low-rank loss reports comparable performance standard softmax loss used standalone classiﬁcation loss small classiﬁcation problems. however together softmax observed consistent classiﬁcation performance improvements popular architectures challenging datasets. figure low-rank loss intra-class variations among features collapsed inter-class features orthogonal. property particularly beneﬁtial tree split node. information-theoretic code aggregation training random tree low-rank loss learner produce consistent hashes similar data points propose information-theoretic approach aggregate hashes across trees unique code data class. labels usually unavailable available small subset data unsupervised aggregation allows exploiting available data. also explain labels available incorporated supervised hash aggregation. note code aggregation step learned training cost testing. unsupervised aggregation. consider random forest consisting trees depth hash codes obtained training samples denoted {bi}m codes generated i-th tree henceforth denoted code blocks. given target hash code length objective select code blocks maximizing mutual information selected remaining codes general problem maximizing submodular functions np-hard reduction max-cover problem. however motivated propose simple greedy algorithm approximate solution start iteratively choose next best code block provides maximum increase mutual information i.e. denotes conditional entropy. intuitively ﬁrst term forces different already selected codes second term forces representative among remaining codes. deﬁning covariance matrix ij-th entry equal hamming distance efﬁciently evaluated closed form detailed proved greedy algorithm gives polynomial-time approximation within optimum napier’s constant. based similar arguments near-optimality approach guaranteed forest size sufﬁciently larger supervised aggregation. class labels available training samples upper bound bayes error hashing codes given bound minimized maximized. thus discriminative hash codes obtained maximizing similarly unsupervised case maximize using greedy algorithm initialized iteratively choosing next best code block provides maximum mutual information increase i.e. evaluated entropy measures involve computation probability density functions efﬁciently computed counting frequency unique codes note number unique codes usually small learned transformation step. terms evaluated using different samples exploit labeled unlabeled data. parameters suggested estimated ratio maximal information gained code block respective criteria i.e. maxi exploiting diminishing return property ﬁrst greedily selected code block based need evaluated leads efﬁcient process ﬁnding selecting using semantic information gives hash model less robust e.g. overﬁts training data model also concerning actual code representation. shown experiments unsupervised supervised aggregation approaches promote unique codes class further improvements uniﬁed. multimodal hashing extend foresthash multimodal similarity learning approach. often challenging enable similarity assessment across modalities example searching corpus consisting audio video text using queries modalities. foresthash framework easily extended hashing data multiple modalities single space. training multimodal data arrives tree split node simply enforce random class partition modalities learn modality dictionary pair independently using shared class partition. training splitting function dominant modality evaluated arriving data point; testing based modality arriving point corresponding splitting function acts independently. shown section foresthash significantly outperforms state-of-the-art hashing approaches cross-modality multimedia retrieval tasks. present experimental evaluation foresthash image retrieval tasks using standard hashing benchmarks cifar- image dataset mnist image dataset wikipedia image document dataset cifar- challenging dataset labeled color images different object categories table retrieval performance different hashing methods cifar-. method network network all-cnn deeply supervised fractalnet resnet resnet stochastic depth foresthash -bit foresthash -bit foresthash -bit table error rates cifar- image classiﬁcation benchmark. foresthash performs level state-of-the-art image classiﬁcation techniques utilizing compact representation. discussed section low-rank weak learner tree split node allowed various implementations. without particular speciﬁcation -dimensional kernelization assumed. table shows network structures light-weight learners adopted experiments. note shallow tree preexperiment cifar- using reduced size training. adopt setup image retrieval experiments used images class training; testing disjoint test images evenly sampled classes query remaining images. images used inputs foresthash learners dimensional gist descriptors used compared methods including foresthash kernel. table summarizes retrieval performance various methods cifar- reduced training using mean precision recall hamming radius hash lookup. compared methods code provided authors; sparsehash reproduce results unsupervised rest hashing schemes supervised. report performance foresthash using random unsupervised supervised semi-supervised hash aggregation schemes respectively. observe proposed information-theoretic code aggregation provides effective combine hashes different trees showing beneﬁts unify unsupervised supervised aggregation. also observe using softmax loss learners leads performance degradation. reduced training complex learner structures show obvious advantage. general proposed foresthash shows signiﬁcantly higher precision recall compared methods. supervised hashing methods hdml fasthash report excellent performance hdml deep learning based hashing method fasthash boosted trees based method. adopt experimental setting i.e. training disjoint query split mnist data. hashing method assessed retrieval precision recall radius shown table using training samples foresthash kernel shows comparable performance hdml fasthash better tsh. foresthash two-layer signiﬁcantly outperform compared methods. assume labels available small subset data often case retrieval system. number lafigure forest boosting effect using foresthash codes. foresthash shows principled robust procedure train deploy parallel ensemble light-weight cnns instead simply going deeper. ferred; deeper tree becomes less preferred retrieval loses robustness gained randomness. tree depth assumed default section. practice choice tree depth also depends target code length level parallelism supported hash tree trained deployed independently parallel. ﬁrst adopt cifar- protocol popular among many deep-learning based hashing methods e.g. ofﬁcial cifar- train/test split used; namely images used training gallery images query. table reports retrieval performance comparisons multiple hashing methods. foresthash simplest two-layer learner table already signiﬁcantly outperforms state-of-the-art methods. given large size training retrieval performance increases using complex network structures learners e.g. cnn. superior retrieval performance foresthash codes table easily explained lowrank loss properties figure boosting effect random forest figure foresthash shows principled robust procedure train deploy parallel ensemble light-weight cnns instead simply going deeper. shown table foresthash performs level state-of-the-art image classiﬁcation techniques e.g. resnet utilizing -bit repredocuments. article-image pairs annotated label semantic classes. enable fair comparison adopted provided features images text table shows mean average precision scores cross-modality image retrieval using text queries. proposed foresthash signiﬁcantly outperforms stateof-the-art hashing approaches cross-modality multimedia retrieval tasks. note mm-nn cm-nn table deep learning motivated hashing methods. examples cross-modality text queries top- images retrieved shown figure using foresthash note text information used compose query foresthash retrieves images category query text. foresthash signiﬁcantly outperforms codes least shorter. considering importance compact computationally efﬁcient codes introduced random forest semantic hashing scheme extending random forest beyond classiﬁcation large-scale multimodal retrieval incommensurable data. proposed scheme consists forest random class grouping low-rank loss information-theoretic code aggregation scheme. using matrix nuclear norm optimization criterion low-rank loss simultaneously reduce variations within classes increases separations classes. thus hash consistency among similar samples enforced random tree. information-theoretic code aggregation scheme provides nearly optimal combine hashes generated different trees producing unique code sample category applicable training regimes ranging totally unsupervised fully supervised. note proposed framework combines fundamental fashion kernel methods random forests cnns hashing. method shows exceptional effectiveness preserving similarity hashes signiﬁcantly outperforms state-of-the-art hashing methods large-scale singlemulti-modal retrieval tasks. beled samples reduces class respectively retrieval performance deep learning boosted tree-based hashing degrades dramatically methods require dense training learn rich parameters. subspace assumption behind low-rank loss known robust regime labeled training examples class foresthash signiﬁcantly outperforms state-of-the-art methods reduced training cases. note training hashing time foresthash reported time tree order emphasize fact different trees trained deployed independently easily done parallel experiments conducted mnist following enabling comparison hashing methods implementation accessible. split mnist dataset training containing samples disjoint query samples. table reports hamming ranking performance measured mean average precision code lengths proposed foresthash signiﬁcantly outperforms hashing methods. performed cross-modality retrieval experiment following wikipedia dataset. purpose demonstrate foresthash natively supports crossmodality. wikipedia dataset contains total", "year": 2017}