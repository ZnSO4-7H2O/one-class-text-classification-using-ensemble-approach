{"title": "Efficient Exploration through Bayesian Deep Q-Networks", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We propose Bayesian Deep Q-Network (BDQN), a practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model. This layer can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari games in Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, the double deep Q network (DDQN).", "text": "propose bayesian deep q-network practical thompson sampling based reinforcement learning algorithm. thompson sampling allows targeted exploration high dimensions posterior sampling usually computationally expensive. address limitation introducing uncertainty output layer network bayesian linear regression model. layer trained fast closed-form updates samples drawn efﬁciently gaussian distribution. apply method wide range atari games arcade learning environments. since bdqn carries efﬁcient exploration able reach higher rewards substantially faster baseline double deep network central challenge design algorithms scale enormous inﬁnite state spaces efﬁciently balance exploration exploitation environments. much exciting advances deep scale enormous domains employ simple exploration strategies ε-greedy often highly inefﬁcient. though large body work efﬁcient exploration relevant domain small enough represented lookup tables value function much less work scaling exploration several papers combine generalization strategic exploration optimism-under-uncertainty involve explicit implicit bonuses rewards based uncertainty reward dynamics values. university california irvine stanford university caltech amazon. correspondence kamyar azizzadenesheli <kazizzaduci.edu> emma brunskill <ebruncs.stanford.edu> animashree anandkumar <animakumargmail.com>. pling thompson sampling bayesian approach involves maintaining prior distribution environment models updated observations made interaction environment. choose action sample posterior belief drawn action selected maximizes expected return sampled belief. interestingly posterior sampling decision making also studied ﬁeld psychology setting involves sampling reward dynamics model performing planning using sampled models compute optimal action current state thompson sampling approaches observed often empirically work signiﬁcantly better optimistic approaches contextual bandit settings small mdps still maintains strong preserve state-of-theart performance bounds. large mdps sampling model performing planning model computationally intractable. therefore form function approximation required help scale ideas thompson sampling. help address this introduced randomized least-squares value iteration rlsvi involves combining linear value function approximation bayesian regression order able sample value function weights distribution. authors prove strong regret bounds rlsvi tabular basis function used rlsvi scalable large-scale deep neural networks. combine beneﬁts thompson sampling style approaches deep networks generalization scale osband introduced bootstrapped-ensemble approach trains several models parallel approximate posterior distribution. works suggest using posterior parameters node network employ variational approximation noisy network however mostly approaches lead modest gains atari benchmarks equaling substantial beneﬁts combining optimism-under-uncertainty deep neusurprizingly paper show simple approach extends randomized least-squares value iteration method deep neural networks yield substantial gains atari benchmarks. speciﬁcally combine deep neural network bayesian linear regression last layer network. work also related concurrently developed approach levine perform least squares temporal difference learning deep neural network uses ε-greedy exploration learned function also demonstrate modest gains atari benchmarks. results show performing bayesian regression instead sampling result yield substantial beneﬁt indicating higher data efﬁciency last layer leveraging explicit uncertainty representation value function substantial beneﬁt. speciﬁcally introduce bayesian deep qnetworks combines deep network bayesian linear regression model last layer. linearity choosing gaussian prior derive closed-form analytical update approximated posterior distribution functions. also draw samples efﬁciently gaussian distribution. exploration performed sampling learned gaussian posterior instantiate values best action selected. test bdqn wide range arcade learning environment atari games compare results implementation ddqn bdqn ddqn share architecture follow target objective differ used select actions ddqn uses ε-greedy bdqn performs bayesian linear regression last layer samples parameters resulting distributions selects best action sample. also compare results reported results number state-of-the-art approaches. proposed approach several beneﬁts– simplicity targeted exploration– yields performance often substantially better existing optimism-based state deep approaches. exploration-exploitation trade-off deeply investigated litera. jaksch investigates regret analysis mdps optimism face uncertainty principle deployed guarantee high probability regret upper bound. azizzadenesheli table bdqn ddqn number times steps written last column. ﬁrst column presents score ratio bdqn ddqn steps provided last column. second column score ratio bdqn number steps last column compared score ddqn† reported scores ddqn hasselt running samples evaluation time third column respect human score reported mnih worth noting design evaluation phase bdqn deploys order propose high probability regret upper bound partially observable mdps using spectral methods furthermore bart´ok tackles general case partial monitoring games provides minimax regret guarantee polynomial certain dimensions problem. multi-arm bandit compelling empirical pieces evidence thompson sampling provide better results optimism-under-uncertainty approaches state performance bounds preserved natural adaptation algorithm posterior sampling ﬁrst proposed strens also shown good frequentist bayesian performance guarantees even though theoretical addresses exploration exploitation trade-offs problems still prominent empirical reinforcement learning research empirical side recent success video games sparked ﬂurry research interest. following success deep atari games board game many researchers begun exploring practical applications deep reinevitably psrl posterior sampling policy value computationally intractable large systems psrl easily leveraged high dimensional problems. remedy failings osband consider randomized value functions approximate posterior samples value function computationally efﬁcient manner. show suitable linear value function approximation using approximated bayesian linear regression randomized least-squares value iteration method remain statistically efﬁcient still scalable large-scale deep neural networks. combat shortcomings osband suggests bootstrapped-ensemble approach trains several models parallel approximate posterior distribution. works suggest using variational approximation q-networks noisy network however approaches signiﬁcantly increase computational cost neither approach produced much beyond modest gains atari games. interestingly bayesian approach technique learning neural network deployed object recognition image caption generation signiﬁcant advantage veriﬁed snoek work present another alternative approach extends randomized least-squares value iteration method deep neural networks approximate posterior bayesian linear regression last layer neural network. approach several beneﬁts e.g. simplicity robustness targeted exploration importantly method much effective predecessors terms sample complexity ﬁnal performance. concurrently levine proposes least squares temporal difference learns linear model feature representation order estimate q-function ε-greedy exploration employed improvement tested atari games provided. games common games bdqn outperform factor drop-out another randomized exploration method proposed ghahramani osband investigates sufﬁciency estimated uncertainty hardness driving suitable exploitation stated before spite novelties proposed methods mentioned section neither them including based approaches produced section enumerate beneﬁts εgreedy strategies. show strategies exploit uncertainties expected returns design randomized exploration ε−greedy strategies disregard useful information exploration. order make balance exploration exploitation explores actions higher estimated return higher probability. order exploit estimated uncertainties dedicates higher chance explore action uncertainty increases. fig. expresses agent’s estimated values uncertainties available actions given state ε−greedy strategy mostly focuses greedy action action based strategy randomizes mostly actions utilizes approximated expected returns uncertainties frequency explores actions hand ε−greedy strategy explores actions actions agent almost sure expected returns frequent sub-greedy actions increases samples complexity. moreover ε−greedy strategy requires deep network approximate value sub-greedy actions equally good therefore dedicates network capacity accurately estimate values sub-greedy actions equally good instead focusing actions higher promising estimated value. therefore ends accurate enough estimation good actions compared greedy action. study value-based deep e.g. network following target value updated occasionally. therefore based strategy estimate posterior distribution adaptively follows target values. commonly used technique deep moving window replay buffer store recent experiences. based agent tries actions builds belief return actions given current target values possible later target value suggests high expected return actions. since replay buffer bounded moving window lack samples actions pushes posterior belief actions prior belief time agent tries order update belief. fig. shows lack samples action replay buffer increases uncertainty action randomized strategy starts explore over. means adaptive change target value respectively objective limited replay buffer bdqn agent never conﬁdent figure cartoon ε-greedy. crosses target values diamonds mean estimated q-values blue intervals uncertainties ε-greedy strategy mostly chooses greedy action action explore actions much actions randomizes mostly actions barely explore actions bdqn computes posterior using recent experiences replay buffer. therefore lack samples action increases uncertainty estimated value bdqn explores again. exploration crucial since target value changes time. maze. general based strategy advances explorationexploitation balance making trade-off expected returns uncertainties ε−greedy strategy ignores information. another beneﬁt ε-greedy described using fig. consider deterministic episodic maze game episode length shortest pass start destination. agent placed start point beginning episode goal state reach destination receive reward otherwise reward consider agent given q-functions true q-function within optimistic function set. agent supposed maximizes average return. worth noting agent task good function function set. situation randomizes q-functions high promising returns relatively high uncertainty including true q-function. agent picks true q-function increases posterior probability q-function matches observation. agent chooses functions predict deterministically wrong values posterior update functions zero. therefore agent choose functions again i.e. ﬁnds true q-function transferring information posterior update helps agent optimal fast. ε-greedy agent even though chooses true function beginning time step randomizes action probability therefore takes exponentially many trials order target game. inﬁnite horizon γ-discounted tuple state space action space transition kernel accompanied reward function time step environment state called current state agent needs make decision policy. given current state action environment stochastically proceed successor state probability distribution provides stochastic reward mean agent objective optimize overall expected discounted reward policy stochastic mapping states actions expectation respect randomness distribution initial state transition probabilities stochastic rewards policy stationary distribution optimal average return optimal policy respectively. denote average discounted reward policy starting state taking action ﬁrst place. optimal policy deterministic mapping state actions i.e. maxa know transition kernel reward function advance therefore cannot solve posed bellman equation directly. order tackle problem property minimizing bellman residual given q-function proposed consists consec). here utive samples behavioral policy furthermore carries idea introduce deep q-network q-functions parameterized deep network. improve quality estimate back propagation loss using update following describe setting used ddqn. order reduce bias estimator introduce target network qtarget target value γqtarget maxa′ loss regression problem minimizes estimated loss minimize distance target ddqn agent updates qtarget network setting network pursues regression target value provides biased estimator target. show extend randomized leastsquares value iteration method combine deep neural network. result viewed coarse approximation representing uncertainty q-function guide exploration. output deep network feature representation layer parametrized efﬁciently approximate distribution q-values uncertainty values captured. common assumption feature representation suitable linear classiﬁcation regression therefore therefore building linear model features suitable choice q-functions approximated linear transformation deep neural network features i.e. given pair state-action φθ⊤wa consequently mentioned previous section target value generated using target model. target model follows structure model contains φtarget denotes feature representation target network wtarget denotes target linear model applied target feature representation. inspired ddqn given tuple experience predicted value pair }|a| {wa}|a| {ξa}|a| bdqn agent interacts environment applying actions proposed i.e. ats. utilize notion experience replay buffer agent stores recent experiences. agent draws every sample steps optimally respect drawn weights. inner loop algorithm draw minibatch data replay buffer loss update target network every target steps θtarget period bayes target agent updates posterior distribution using larger minibatch data drawn replay buffer wtarget mean posterior sample respect updated posterior. algorithm gives full description bdqn. apply bdqn variety atari games using arcade learning environment openai baseline ddqn algorithm evaluate bdqn measures sample complexity score. furthermore implementations coded mxnet framework network architecture input network part bdqn tensor rescaled averaged channels last four observations. ﬁrst convolution layer ﬁlters size stride second convolution layer ﬁlters size stride last convolution layer ﬁlters size followed fully connected layer size layer this. therefore deploying space features approximate posterior distribution model parameter well posterior distribution q-functions using corresponding target values. gaussian models order make posterior update computationally tractable closed form common approximation make prior likelihood choices conjugates other. therefore given pair vector drawn gaussian prior given target value generated following model; given experience replay buffer construct disjoint datasets action ∪a∈ada tuples action cardinality interested approximated posterior distribution correspondingly p|da) following standard bayesian linear regression equations adjusted setting encourage readers familiar bayesian linear regression skip derivation. action corresponding dataset construct matrix rd×da concatenation feature column vectors {φ}da concatenation target values therefore posterior distribution follows choice hyper-parameters bdqn values target mean posterior distribution weights covariances draw posterior. ﬁxed target randomly initialize parameters network part bdqn train using rmsprop learning rate momentum inspired discount factor number steps target updates target steps weights re-sampled posterior distribution every sample steps. update network part bdqn every steps uniformly random sampling mini-batch size samples replay buffer. update posterior distribution weight every bayes target using mini-batch size entries sampled uniformly form replay buffer. experience replay contains recent transitions. hyper-parameters equivalent ones setting. furthermore part bdqn noise variance variance prior weights sample size posterior update period bayes target posterior sampling period sample. optimize hyper-parameters simple fast cheap non-exhaustive hyper-parameter tuning procedure using pre-trained model game assault. simplicity cheapness hyper parameter tuning proves robustness superiority bdqn exhaustive hyper-parameter search likely provide even better performance. details hyper parameters tuning provided apx. order compare fairness sample usage argue apx. network part bdqn corresponding part ddqn observe number samples part bdqn uses times fewer samples compared corresponding last layer ddqn apx. baselines implemented ddqn architecture match bdqn implementation. also aimed implement couple deep methods employ strategic exploration. unfortunately encountered several implementation challenges. illustrate performance approach instead extracted best reported results number state-of-the-art deep methods include table note perfect comparison sometimes additional details included papers mean hard compare reported results tried report ﬁnal performance bootstrapped exploration count based exploration noisynet. reproducibility released codes trained models https//github.com/kazizzad/bdqn-mxnet-gluon. since experiments expensive also released recorded arrays returns order make possible others compare bdqn without running experiments again. moreover bootstrap implementation also available https//github.com/kazizzad/bootstrap-dqn table comparison scores sample complexities. sample complexity represents number samples bdqn requires human score number number samples bdqn requires score ddqn scores ddqn borrowed original ddqn paper hasselt reported running samples evaluation time bootstrap pixel reactor scores borrowed original papers. noisynet since score noisy ddqn reported score closest model noisydqn scores. human scores borrowed mnih worth noting design evaluation phase bdqn. results results provided fig. table. bdqn performs best across majority games stated number samples even typically performing much better several methods trained much longer. note comparisons bootstrap noisynet viewed lightly since reported results algorithms generally trained substantially longer reactor outperformed bdqn three games alien atlantis enduro trained identical number time steps. worth noting scores ddqn reported during leaning phase example ddqn gives score learning phase setting zero mostly gives score also report number samples take bdqn reach human scores ddqn scores apx. observe bdqn immediately learns signiﬁcantly better policies targeted exploration much shorter period time. since bdqn game atlantis promise jump around time step times order make sure coincidence apx. fig. game pong experiment longer period plotted beginning order observe difference. games experiment samples since reached plateau. work proposed bdqn practical based algorithm provides targeted exploration computationally efﬁcient manner. involved making simple modiﬁcations ddqn architecture replacing last layer bayesian linear regression. gaussian prior obtained fast closed-form updates posterior distribution. demonstrated signiﬁcantly faster training much better performance many games compared reported results wide number state-ofthe-art baselines. computational limitations algorithm games remains interesting issue explore performance combine work bdqn randomize last layer model bayesian linear regression framework train alternate training layers network. alternative approach train end-to-end using stochastic optimization approaches could signiﬁcantly speed training retaining computational efﬁciency ddqn. work considered value based approaches deep plan explore advantages based exploration policy gradient based approaches future.", "year": 2018}