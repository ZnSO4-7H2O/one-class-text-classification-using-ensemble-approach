{"title": "Regret Minimization for Partially Observable Deep Reinforcement Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state. We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (Malm\\\"o) first-person navigation benchmarks.", "text": "peter sergey levine kurt keutzer berkeley research department electrical engineering computer sciences university california berkeley {phjsvlevinekeutzer}eecs.berkeley.edu deep reinforcement learning algorithms estimate state state-action value functions shown effective variety challenging domains including learning control strategies image pixels. however algorithms estimate state state-action value functions typically assume fully observed state must compensate partial non-markovian observations using ﬁnite-length frame-history observations recurrent networks. work propose deep reinforcement learning algorithm based counterfactual regret minimization iteratively updates approximation cumulative clipped advantage function robust partially observed state. demonstrate several partially observed reinforcement learning tasks class algorithms substantially outperform strong baseline methods pong single-frame observations challenging doom minecraft ﬁrst-person navigation benchmarks. many reinforcement learning problems practical interest property partial observability observations state generally non-markovian. despite importance partial observation real world value function-based methods q-learning generally assume markovian observation space. hand monte carlo policy gradient methods assume markovian observations many practical policy gradient methods introduce markov assumption using critic state-dependent baseline order improve sample efﬁciency. consider deep reinforcement learning methods learn state state-action value function. common workaround problem partial observation learn value functions space ﬁnite-length frame-history observations assumption frame-histories sufﬁcient length give environment approximate appearance full observability. learning play atari games images deep q-learning algorithms concatenate last observed frames video screen buffer input state-action value convolutional network. non-markovian tasks amenable ﬁnite-length frame-histories; recurrent value functions incorporate longer potentially inﬁnite histories cost solving harder optimization problem. develop methods learn variant value function robust partial observability? contribution model-free deep reinforcement learning algorithm based principle regret minimization require access markovian state. method learns policy estimating cumulative clipped advantage function approximation type regret central partial information game-solving algorithms draw primary inspiration counterfactual regret minimization cfr+ hence call algorithm advantage-based regret minimization evaluate approach three visual reinforcement learning domains pong varying framehistory lengths ﬁrst-person games doom minecraft doom minecraft exhibit ﬁrst-person viewpoint dimensional environment appear non-markovian even frame-history observations. method offers substantial improvement prior methods partially observable environments doom minecraft method learn well-performing policies within million simulator steps using visual input frame-history observations. deep reinforcement learning algorithms demonstrated achieve excellent results range complex tasks including playing games continuous control prior deep reinforcement learning algorithms either learn state state-action value functions learn policies using policy gradients perform combination using actor-critic architectures policy gradient methods typically need assume markovian state tend suffer poor sample complexity inability off-policy data. methods based learning q-functions replay buffers include off-policy data accelerating learning however learning q-functions bellman error minimization typically requires markovian state space. learning observations images inputs might markovian. prior methods proposed mitigate issue using recurrent critics q-functions learning q-functions depend entire histories observations. heuristics concatenation short observation sequences also used however changes increase size input space increasing variance make optimization problem complex. method instead learns cumulative advantage functions depend current state still handle non-markovian problems. form advantage function update resembles positive temporal difference methods additionally update rule modiﬁed cumulative q-function resembles average q-function used variance reduction q-learning. cases theoretical foundations method based cumulative regret minimization motivation substantively different. previous work ross ross bagnell connected regret minimization reinforcement learning imitation learning structured prediction although counterfactual regret minimization. regression regret matching based closely related idea directly approximate regret linear regression model however linear model limited representation compared deep function approximation. section review algorithm counterfactual regret minimization closely follow version described supplementary material bowling except notation reinforcement learning appropriate. consider setting extensive game. players numbered additional player considered chance player simulate random events. time step game player chooses action deﬁne following concepts notation strategies strategy i-th player probability distribution action conditioned information denote strategy proﬁle players denote strategy proﬁle players except i-th player. sequence probabilities probability reaching sequence players follow additionally probability reaching conditioned ρπ−i contain contributions already reached. similarly deﬁne respectively i-th player players except i-th. values value terminal sequence i-th player. expected notation denotes preﬁx denotes action performed observed. counterfactual value calculation assumes i-th player reaches upon reaching always chooses consider learning scenario t-th iteration players follow strategy proﬁle i-th player’s regret iterations deﬁned terms i-th player’s optimal strategy counterfactual regret shown majorize regret described learning algorithm iteration strategy updated using regret matching applied counterfactual regret previous iteration cfr+ consists modiﬁcation instead calculating full counterfactual regret instead counterfactual regret recursively positively clipped yield clipped counterfactual regret comparing equation equation difference previous iteration’s counterfactual regret positively clipped recursion. one-line change cfr+ turns yield large practical improvement performance algorithm also associated regret bound cfr+ strong bound cfr+ formulated imperfect information extensive-form games naturally generalized partially observed stochastic games since stochastic game always represented extensive form. -player partially observed stochastic game simply pomdp observation space mapping information sets observations rewrite counterfactual value kind stationary observation-action value π|o→a assumes agent follows policy except observing action always performed posit approximation π|o→a usual action value function valid observations π|o→a rarely seen trajectory. approximating recurrence terms familiar value functions cumulative clipped advantage function ordinary advantage function evaluated policy advantage-based regret minimization resulting reinforcement learning algorithm updates policy regret match cumulative clipped advantage function equation suggests outline batch-mode deep reinforcement learning algorithm. t-th sampling iteration batch data collected sampling trajectories using current policy followed processing steps next iteration’s policy using equation implement equation deep function approximation deﬁne value function approx well target value function imations learnable parameters. cumulative clipped advantage function represented vπt. within sampling iteration value functions ﬁtted using stochastic gradient descent sampling minibatches performing gradient steps. state-value function minimize n-step temporal difference loss moving target essentially using estimator deep deterministic policy gradient minibatch similar loss additional target reward bonus incorporates previous iteration’s cumulative clipped advantage max). regression targets deﬁned terms n-step returns intuition behind arm? understand algorithm achieves bound average regret accessing partially observed information sets. arm’s effectiveness partially observable domains seen inherited cfr. mechanics updates similar on-policy value function estimation learns modiﬁed on-policy q-function transitions added reward bonus max) reward bonus kind optimistic inertia favor observation-action pairs. second view comes comparing convergence rates reinforcement learning algorithms. cfr+ average regret bounds ultimately derived regret matching szepesv´ari proved convergence rate q-learning l∞-norm assuming ﬁxed exploration strategy depends condition number ratio minimum maximum state-action occupation frequencies describes balanced exploration strategy partial observability leads imbalanced exploration confounding states perceptual aliasing q-learning negatively affected. knowledge known regret bound q-learning. policy gradient method suitable baseline learning rate η-dependent regret bound derived stochastic gradient method; assuming parameter norm bound gradient estimator second moments setting learning rate policy gradient achieves average regret bound comparing algorithms policy gradient methods tuned learning rate schedule efﬁcient cfr+. additionally q-learning bounds q-function l∞-norm strictly worse algorithms’ average regret bounds. table summary. note remains implemented theory function approximation sampling tabular enumeration; ordinary q-function instead stationary q-function; n-step bootstrapped values instead full returns value function estimation. waugh address function approximation noisy version generalized blackwell’s condition even original implementation used sampling place enumeration refer reader bellemare in-depth discussion stationary q-function although full returns guaranteed unbiased non-markovian settings quite common practical algorithms trade strict unbiasedness favor lower variance using n-step returns variations thereof", "year": 2017}