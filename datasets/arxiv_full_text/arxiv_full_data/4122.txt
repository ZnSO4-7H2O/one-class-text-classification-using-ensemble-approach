{"title": "Efficient Multi-Scale 3D CNN with Fully Connected CRF for Accurate Brain  Lesion Segmentation", "tag": ["cs.CV", "cs.AI"], "abstract": "We propose a dual pathway, 11-layers deep, three-dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. The devised architecture is the result of an in-depth analysis of the limitations of current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an efficient and effective dense training scheme which joins the processing of adjacent image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze the development of deeper, thus more discriminative 3D CNNs. In order to incorporate both local and larger contextual information, we employ a dual pathway architecture that processes the input images at multiple scales simultaneously. For post-processing of the network's soft segmentation, we use a 3D fully connected Conditional Random Field which effectively removes false positives. Our pipeline is extensively evaluated on three challenging tasks of lesion segmentation in multi-channel MRI patient data with traumatic brain injuries, brain tumors, and ischemic stroke. We improve on the state-of-the-art for all three applications, with top ranking performance on the public benchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient, which allows its adoption in a variety of research and clinical settings. The source code of our implementation is made publicly available.", "text": "propose dual pathway -layers deep three-dimensional convolutional neural network challenging task brain lesion segmentation. devised architecture result in-depth analysis limitations current networks proposed similar applications. overcome computational burden processing medical scans devised eﬃcient eﬀective dense training scheme joins processing adjacent image patches pass network automatically adapting inherent class imbalance present data. further analyze development deeper thus discriminative cnns. order incorporate local larger contextual information employ dual pathway architecture processes input images multiple scales simultaneously. post-processing network’s soft segmentation fully connected conditional random field eﬀectively removes false positives. pipeline extensively evaluated three challenging tasks lesion segmentation multi-channel patient data traumatic brain injuries brain tumors ischemic stroke. improve state-of-theart three applications ranking performance public benchmarks brats isles method computationally eﬃcient allows adoption variety research clinical settings. source code implementation made publicly available. segmentation subsequent quantitative assessment lesions medical images provide valuable information analysis neuropathologies important planning treatment strategies monitoring disease progression prediction patient outcome. better understanding pathophysiology diseases quantitative imaging reveal clues disease characteristics eﬀects particular anatomical structures. example associations diﬀerent lesion types spatial distribution extent acute chronic sequelae traumatic brain injury still poorly understood however growing evidence quantiﬁcation lesion burden insight functional outcome patients moen additionally exact locations injuries relate particular deﬁcits depending brain structure aﬀected warner sharp line estimates functional deﬁcits caused stroke associated extent damage particular parts brain lesion burden commonly quantiﬁed means volume number lesions biomarkers shown related cognitive deﬁcits. example volume white matter lesions correlates cognitive decline increased risk dementia clinical research multiple sclerosis lesion count volume used analyse disease progression eﬀectiveness pharmaceutical treatment kappos finally accurate delineation pathology important case brain tumors estimation relative volume tumor’s sub-components required planning radiotherapy treatment follow-up quantitative analysis lesions requires accurate lesion segmentation multi-modal three-dimensional images challenging task number reasons. heterogeneous appearance lesions including large variability location size shape frequency make diﬃcult devise eﬀective segmentation rules. thus highly non-trivial delineate contusions edema haemorrhages sub-components brain tumors proliferating cells necrotic core arguably accurate segmentation results obtained manual delineation human expert tedious expensive time-consuming impractical larger studies introduces inter-observer variability. additionally deciding whether particular region part lesion multiple image sequences varying contrasts need considered level expert knowledge experience important factors impact segmentation accuracy. hence clinical routine often qualitative visual inspection best crude measures like approximate lesion volume number lesions used order capture better understand complexity brain pathologies important conduct large studies many subjects gain statistical power drawing conclusions across whole patient population. development accurate automatic segmentation algorithms therefore become major research focus medical image computing potential oﬀer objective reproducible scalable approaches quantitative assessment brain lesions. figure illustrates challenges arise devising computational approach task automatic lesion segmentation. ﬁgure summarizes statistics shows examples brain lesions case representative pathologies brain tumors ischemic stroke. lesions occur multiple sites varying shapes sizes image intensity proﬁles largely overlap non-aﬀected healthy parts brain lesions focus interest. example stroke lesions similar hyper-intense appearance flair sequences wmls schmidt generally diﬃcult derive statistical prior information lesion shape appearance. hand applications expectation spatial conﬁguration segmentation labels example hierarchical layout sub-components brain tumors. ideally computational approach able adjust application speciﬁc characteristics learning example images. multitude automatic lesion segmentation methods proposed last decade several main categories approaches identiﬁed. group methods poses lesion segmentation task abnormality detection problem example employing image registration. early work prastawa recent ones schmidt doyle align pathological scan healthy atlas lesions detected based deviations tissue appearance patient atlas image. lesions however cause figure heterogeneous appearance lesions poses challenges devising discriminative models. lesion size varies signiﬁcantly large focal small diﬀused lesions alignment manual lesion segmentations reveals wide spatial distribution lesions areas likely others. shows average normalized intensity histograms diﬀerent channels cases database healthy injured tissue. observe large overlap distributions healthy non-healthy tissue. large structural deformations lead incorrect segmentation incorrect registration. gooya parisot alleviate problem jointly solving segmentation registration tasks. showed registration together low-rank decomposition gives by-product abnormal structures sparse components although precise enough detection small lesions. abnormality detection also proposed within image synthesis works. representative approaches weiss using dictionary learning using patch-based approach. idea synthesize pseudo-healthy images compared patient scan allow highlight abnormal regions. context cardoso present generative model image synthesis yields probabilistic segmentation abnormalities. another unsupervised technique proposed erihov saliency-based method exploits brain asymmetry pathological cases. common advantage methods require training dataset corresponding manual annotations. general approaches suitable detecting lesions rather accurately segmenting them. successful supervised segmentation methods brain lesions based voxel-wise classiﬁers random forests. representative work geremia lesions employing intensity features capture appearance region around voxel. zikic combine generative gaussian mixture model obtain tissue-speciﬁc probabilistic priors framework adopted multiple works representative pipelines brain tumors tustison works incorporate morphological contextual features better capture heterogeneity lesions. also incorporate brain structure segmentation results obtained multi-atlas label propagation approach provide strong tissue-class priors random forests. tustison additionally markov random field incorporate spatial regularization. mrfs commonly used encourage spatial continuity segmentation mitra although methods successful appears modeling capabilities still signiﬁcant limitations. conﬁrmed results recent challenges also experience experimentation approaches. time deep learning techniques emerged powerful alternative supervised learning great model capacity ability learn highly discriminative features task hand. features often outperform hand-crafted pre-deﬁned feature sets. particular convolutional neural networks krizhevsky applied promising results variety biomedical imaging problems. ciresan presented ﬁrst implementation two-dimensional segmentation neural membranes. based work followed related approach methods zikic havaei pereira latter best performing automatic approach brats challenge methods based cnns used extensively computer vision applications natural images. here segmentation brain scan achieved processing slice independently arguably non-optimal volumetric medical image data. despite simplicity architecture promising results obtained methods indicate potential cnns. fully cnns come increased number parameters signiﬁcant memory computational requirements. previous work discusses problems apparent limitations employing medical imaging data roth incorporate contextual information multiple works used cnns three orthogonal patches roth lyksborg work structural brain segmentation brebisson montana extracted large patches multiple scales image combined small single-scale patches order avoid memory requirements fully networks. reasons discouraged cnns slow inference computationally expensive convolutions. contrast hybrid variants brebisson montana cnns fully exploit dense-inference sermanet technique greatly decreases inference times discuss section employing dense-inference cnns brosch urban reported computation times seconds approximately minute respectively processing single brain scan. even though size developed networks limited factor directly related network’s representational power results brain tumor segmentation respectively promising. performance cnns signiﬁcantly inﬂuenced strategy extracting training samples. commonly adopted approach training image patches equally sampled class. this however biases classiﬁer towards rare classes result over-segmentation. counter this cire¸san proposes train second samples class distribution close real oversample pixels incorrectly classiﬁed ﬁrst stage. secondary training stage also suggested havaei retrain classiﬁcation layer patches extracted uniformly image. practice stage training schemes prone overﬁtting sensitive state ﬁrst classiﬁer. alternatively dense training used train network multiple voxels single image optimisation step brosch ronneberger introduce severe class imbalance similarly uniform sampling. weighted cost functions proposed latter works alleviate problem. brosch manually adjusted sensitivity network method become diﬃcult calibrate multi-class problems. ronneberger ﬁrst balance cost class eﬀect similar equal sampling adjust speciﬁc task estimating diﬃculty segmenting pixel. propose eﬃcient hybrid training scheme utilizing dense training sampled image segments analyze behaviour adapting class imbalance segmentation problem hand. analyze depth development deeper thus discriminative computationally eﬃcient cnns. exploit utilization small kernels design approach previously found beneﬁcial networks impacts cnns even more present adopted solutions enable training deeper networks. employ parallel convolutional pathways multi-scale processing solution eﬃciently incorporate local contextual information greatly improves segmentation results. demonstrate generalization capabilities system without signiﬁcant modiﬁcations outperforms state-of-the-art variety challenging segmentation tasks ranking results miccai challenges isles brats. furthermore detailed analysis network reveals valuable insights powerful black deep learning cnns. example found network capable learning complex high level features separate gray matter cerebrospinal ﬂuid anatomical structures identify image regions corresponding lesions. additionally extended fully-connected conditional random field model kr¨ahenb¨uhl koltun ﬁnal post-processing cnn’s soft segmentation maps. overcomes limitations previous models handle arbitrarily large neighborhoods preserving fast inference times. best knowledge ﬁrst fully connected medical data. facilitate research encourage researchers build upon results source code lesion segmentation method including fully connected made publicly available https//biomedia.doc.ic.ac.uk/software/deepmedic/. proposed lesion segmentation method consists main components produces highly accurate soft segmentation maps fully connected imposes regularization constraints output produces ﬁnal hard segmentation labels. main contributions work within component describe ﬁrst following. cnns produce estimates voxel-wise segmentation labels classifying voxel image independently taking neighborhood i.e. local contextual image information account. achieved sequential convolutions input multiple ﬁlters cascaded layalso referred channels. every group neurons detects particular pattern i.e. feature channels previous layer. pattern deﬁned kernel weights associated neurons m-th l-th layer arranged grid activations constitute image result convolving previous layer’s channels -dimensional kernel applying non-linearity kernel matrix learned hidden weights input ﬁrst layer correspond channels original input image instance multi-sequence scan brain. concatenation kernels viewed -dimensional kernel convolving concatenated channels intuitively expresses neurons higher layers combine patterns extracted previous layers results detection increasingly complex patterns. activations neurons last layer correspond particular segmentation class labels hence layer also referred classiﬁcation layer. neurons thus grouped segmentation classes. activations position-wise softmax function produces predicted posterior exp) class form soft segmentation maps probabilities. activation c-th classiﬁcation position baseline network depicted figure baseline consists four layers kernels feature extraction leading receptive ﬁeld size classiﬁcation layer implemented convolutional kernels enables eﬃcient denseinference. network segments input predicts multiple voxels simultaneously shift receptive ﬁeld input. number receptive ﬁeld layer given product strides kernels layers preceding work unary strides used larger strides downsample unwanted behaviour accurate segmentation. thus system receptive ﬁeld neuron classiﬁcation layer corresponds image common patch-wise classiﬁcation setting input patch size provided network outputs single prediction central voxel. case classiﬁcation layer consists size networks implemented fully-convolutionals capable dense-inference performed input size greater provided case dimensions increase according includes classiﬁcation output multiple predictions simultaneously stride cnn’s receptive ﬁeld input predictions equally trustworthy long receptive ﬁeld fully contained within input captures original content i.e. padding used. strategy signiﬁcantly reduces computational costs memory loads since otherwise repeated computations convolutions voxels overlapping patches avoided. optimal performance achieved whole image scanned forward pass. memory constraints allow case large networks large number need cached volume tiled multiple image-segments larger individual patches small enough memory. analyzing exploit dense-inference technique training ﬁrst main contribution work present commonly used setting cnns trained patch-by-patch. random patches size extracted training images. batch formed samples processed network training iteration stochastic gradient descent step aims alter network’s parameters weights biases order maximize likelihood data equally minimize cross entropy cost function label central voxel scalar value predicted posterior class regularization terms omitted simplicity. multiple sequential optimization steps diﬀerent batches gradually lead convergence. larger training batch sizes preferred approximate overall data accurately lead better estimation true gradient sgd. however memory requirement computation time increase batch size. limitation especially relevant cnns dozens patches processed within reasonable time modern gpus. overcome problem devise training strategy exploits dense inference technique image segments. following image segment size greater given input network i={xyz} training batches formed segments extracted training images cost function case dense-training becomes s-th segment batch true labels true label v-th voxel predicted voxels respectively. output corresponding position classiﬁcation softmax function. eﬀective batch size increased factor withcorresponding increase computational memory requirements earlier discussed sec. notice hybrid scheme commonly used training individual patches dense training scheme whole image latter problematic apply training large cnns volumes high resolution memory limitations. appealing consequence scheme sampling input segments provides ﬂexible automatic balance distribution training samples diﬀerent segmentation classes important issue directly impacts segmentation accuracy. speciﬁcally build training batches extracting segments training images probability centred foreground background voxel figure consider network receptive ﬁeld densely-applied depicted lesion-centred image segments size relatively background captured larger segments around smaller lesions. alleviating class-imbalance. note predicted voxels segment class something occurs segment sampled region near class boundaries hence sampling rate proposed hybrid method adjusts true distribution segmentation task’s classes. speciﬁcally smaller labelled object background voxels captured within segments centred foreground voxel. implicitly yields balance sensitivity speciﬁcity case binary segmentation tasks. multi-class problems rate diﬀerent classes captured within segment centred foreground reﬂects real relative distribution foreground classes adjusting frequency relatively background. deeper networks greater discriminative power additional non-linearities better quality local optima however convolutions kernels computationally expensive comparison variants hampers addition layers. additionally architectures larger number trainable parameters layer adding clcl− weights model. number layer size kernel respecl tive spatial dimension. overall makes network increasingly prone over-ﬁtting. order build deeper architecture adopt sole small kernels faster convolve contain less weights. design approach previously found beneﬁcial classiﬁcation natural images eﬀect even drastic networks. compared common kernel choices urban prasoon baseline smaller kernels reduce element-wise multiplications factor approximately reducing number trainable parameters factor. thus deeper network variants implicitly regularised eﬃcient designed simply replacing layer common architectures layers smaller kernels figure replacement depicted layer kernels successive layers using kernels introduces additional nonlinearity without altering cnn’s receptive ﬁeld. additionally number weights reduced required convolutions however deeper networks diﬃcult train. shown forward backwards propagated signal explode vanish care given retain variance occurs every successive layer variance signal multiplied number weights neuron layer connected input variance layer’s weights. better preserve signal initial training stage adopt scheme recently derived relu-based networks initialize kernel weights system sampling normal distribution phenomenon similar nature hinders network’s performance internal covariate shift occurs throughout training weight updates deeper layers result continuously changing distribution signal higher layers hinders convergence weights. speciﬁcally training iteration weight updates cause deviation variance weights. next iteration signal ampliﬁed inﬂuencing signal deviation ampliﬁed exponential number dimensions. reason problem affects training cnns severely conventional systems. countering adopt recently proposed batch normalisation technique hidden layers allows normalization activations every optimization step order better preserve signal. segmentation voxel performed taking account contextual information captured receptive ﬁeld centred voxel. spatial context providing important information able discriminate voxels otherwise appear similar considering local appearance. follows increase cnn’s receptive ﬁeld requires bigger kernels convolutional layers increases computation memory requirements. alternative would pooling however leads loss exact position segmented voxel thus negatively impact accuracy. order incorporate local larger contextual information second pathway operates down-sampled images. thus dual pathway simultaneously processes input image multiple scales higher level features location within brain learned second pathway detailed local appearance structures captured ﬁrst. pathways decoupled architecture arbitrarily large context processed second pathway simply adjusting down-sampling factor size pathways independently adjusted according computational capacity task hand require relatively less ﬁlters focused down-sampled context. preserve capability dense inference spatial correspondence activations last convolutional layers pathways ensured. networks unary kernel strides used proposed architecture requires every shifts receptive ﬁeld normal resolution input shift performed down-sampled input. hence required dimensions size input second pathway figure multi-scale convolutional pathways. kernels pathways size neurons last layers pathways thus receptive ﬁelds size voxels. inputs pathways centered image location second segment extracted down-sampled version image factor second pathway processes context actual area size voxels. deepmedic proposed -layers architecture results replacing layer depicted pathways kernels number size depicted relation required dimensions input segments resolutions extracted centered image location. up-sampled match dimensions concatenated together. hidden layers combining multi-scale features ﬁnal classiﬁcation shown fig. integration multi-scale parallel pathways architectures non-unary strides discussed appendix combining multi-scale features found beneﬁcial recent works ronneberger whole images processed network applying number convolutions down-sampling processing various scales. decoupled pathways allow arbitrarily large context provided avoiding need load large parts volume memory. additionally architecture extracts features completely independently multiple resolutions. features learned ﬁrst pathway retain ﬁnest details involved processing resolution neighboring voxels share substantial spatial context soft segmentation maps produced tend smooth even though neighborhood dependencies modeled directly. however local minima training noise input images still result spurious outputs small isolated regions holes predictions. employ fully connected post-processing step achieve structured predictions. describe below capable modeling arbitrarily large voxel-neighborhoods also computationally eﬃcient making ideal processing multi-modal medical scans. zj]. corresponding energy penalty given function deﬁned arbitrary feature space feature vectors pair voxels. kr¨ahenb¨uhl koltun observed penalty function deﬁned linear combination gaussian kernels model lends eﬃcient inference mean ﬁeld approximation expressing message passing convolutions gaussian kernels space feature vectors extended work original authors implemented version processing multi-modal scans. make gaussian kernels operate feature space deﬁned voxel coordinates intensities c-th modality-channel voxel smoothness kernel deﬁned diagonal covariance matrix elements conﬁgurable parameters axis. parameters express size shape neighborhoods homogeneous labels encouraged. appearance kernel similarly. additional parameters interpreted strongly enforce homogeneous appearance input channels voxels area spatially deﬁned identically labelled. finally conﬁgurable weights deﬁne relative strength factors. section present series experiments order analyze impact main contributions justify choices made design proposed -layers multi-scale architecture referred deepmedic. starting baseline discussed sec. ﬁrst explore beneﬁt proposed dense training scheme investigate deeper models evaluate inﬂuence multi-scale dual pathway finally compare method corresponding variants assess beneﬁt processing context. following experiments conducted using dataset multi-channel mris described detail later sec. here images randomly split validation training images each. sets used analyses. monitor progress segmentation accuracy training extract random patches regular intervals equal numbers extracted validation images. patches uniformly sampled brain region order approximate true distribution lesions healthy tissue. full segmentation validation datasets performed every epochs mean dice similarity coeﬃcient determined. details conﬁguration networks provided appendix compare proposed dense training method commonly used training schemes -layers baseline ﬁrst common scheme trains patches extracted uniformly brain region second scheme samples patches equally lesion figure comparison commonly used methods training patches uniformly sampled brain region equally sampled lesion background proposed scheme cubic segments side length also equally sampled lesion background. varied observe eﬀect. left right percentage training samples extracted lesion class mean accuracy sensitivity speciﬁcity calculated uniformly sampled validation patches ﬁnally mean segmentation validation datasets. progress throughout training plotted. lesions small puni achieves high voxelwise accuracy speciﬁc sensitive opposite case peq. method achieves eﬀective balance resulting better segmentation reﬂected higher dsc. background class. refer schemes puni peq. results shown fig. show correlation sensitivity speciﬁcity percentage training samples come lesion class. performs poorly over-segmentation puni better classiﬁcation background class leads high mean voxel-wise accuracy since majority corresponds background particularly high scores under-segmentation evaluate dense training scheme train multiple models varying sized image segments equally sampled lesions background. tested sizes segments upwards models referred side length cubic segments. fair comparison batch sizes experiments adjusted similar memory footprint lead similar training times compared training samples. increase size training segments further quickly reach balance sensitivity speciﬁcity puni results improved segmentation expressed dsc. segment size hyper-parameter model. observe increase performance increasing segment size quickly levels similar performance obtained wide range segment sizes allows easy conﬁguration. remaining experiments models trained segments size figure mean accuracy validation samples segmentations validation images obtained shallow baseline deep variant smaller kernels. training plain deeper model fails overcome adopting initialization scheme combined batch normalization leads enhanced variants. deep+ performs signiﬁcantly better shallow+ similar computation time thanks small kernels. dense training whole volume inapplicable experimental settings memory limitations previously shown give similar results training uniformly sampled patches referred deep. training latter however utterly fails model making predictions corresponding background class. problem related challenge preserving signal propagates deep networks variance gets multiplied variance weights previously discussed sec. causes weights models initialized commonly used scheme comparison initialization scheme derived preserving signal initial stage training results higher values overcomes problem. preservation signal obtained employing batch normalization. results enhanced -layers model refer deep+ using enhancements shallow model yields shallow+. signiﬁcant performance improvement deep+ shallow+ shown fig. result greater representational power deeper network. models need similar computational times highlights beneﬁts utilizing small kernels design cnns. although deeper model requires sequential computations faster smaller kernel size. figure mean accuracy validation samples segmentation validation images obtained single-scale model dual pathway architecture also trained single-scale model larger capacity similar capacity deepmedic. deepmedic yields best performance capturing greater context bigdeep+ seems suﬀer over-ﬁtting. ﬁnal version proposed network architecture referred deepmedic built extending deep+ model second convolutional pathway identical ﬁrst one. hidden layers added combining multi-scale features classiﬁcation layer resulting deep network -layers input segments second pathway extracted images down-sampled factor three. thus network capable capturing context area original image receptive ﬁeld lower-resolution pathway doubling computational memory requirements single pathway cnn. comparison recent systems proposed lesion segmentation pereira receptive ﬁeld limited voxels. figure shows improvement deepmedic achieves single pathfig. show representative visual examples model deep+. improvement using multi-scale cnn. finally conﬁrm performance increase accounted additional context additional capacity deepmedic. build single-scale model doubling -layers deep+ adding hidden layers. -layers deep wide model referred bigdeep+ number parameters deepmedic. performance model improved showing signs over-ﬁtting. acquired brain scans often anisotropic. case sequences dataset acquired lower axial resolution except isotropic mprage. perform series experiments investigate behaviour networks assess beneﬁt processing context setting. deepmedic converted setting third dimension kernel one. information surrounding context axial plane inﬂuences classiﬁcation voxel. segments given input dimensionality feature maps decreases memory required. allows developing variants increased width depth size training batch similar requirements version valid candidates model selection practical scenarios. assess various conﬁgurations present representatives table along performance. best segmentation among investigated variants achieved -layers multi-scale network reaching figure cases severe dataset showing representative improvements using multi-scale approach. left right flair sequence manually labeled lesions predicted soft segmentation obtained single-scale model prediction multi-scale deepmedic model. incorporation greater context enables deepmedic identify processes area within larger lesions spurious false positives signiﬁcantly reduced across image bottom. average validation fold. decline achieved version deepmedic indicates importance processing context even settings acquired sequences resolution along certain axis. segmentation tasks including challenging clinical data patients traumatic brain injuries brain tumors ischemic stroke. quantitative evaluation comparisons state-of-the-art reported tasks. sixty-six patients moderate-to-severe required admission neurosciences critical care unit addenbrooke’s hospital cambridge underwent imaging using -tesla siemens magnetom trio within ﬁrst week injury. ethical approval obtained local research ethics committee written assent consultee agreement obtained patients. structural sequences manually annotated flair sequences separate labeling lesion type. nine patients presence hyperintense white matter lesions felt chronic nature also annotated. artifacts example signal loss secondary intraparenchymal pressure probes also noted. purpose study focus binary segmentation abnormalities within brain tissue. thus merged classes correspond intra-cerebral abnormalities single lesion label. extra-cerebral pathologies epidural subdural hematoma treated background. excluded datasets corrupted flair images cases lesions found case because major scanning artifact corrupting images. results total cases used quantitative evaluation. brain masks obtained using robex tool images resampled registered space using atlas grabner bias ﬁeld correction used preliminary results showed negatively aﬀect lesion appearance. image intensities normalized zero-mean unit variance reported improves results network conﬁguration training network architecture corresponds described sec. i.e. dual-pathway -layers deep cnn. training data augmented adding images reﬂected along sagittal axis. make network invariant absolute intensities also shift intensities channel every training segment standard deviation intensities brain mask corresponding image. network regularized using dropout rate convolutional layers addition rate used last layers. network evaluated -fold cross-validation subjects. conﬁguration parameters fully connected determined conﬁguration experiment using random-search randomly selected subjects database predictions preliminary version corresponding model. subjects reshuﬄed -folds used subsequent evaluation. random forest baseline done best competitive baseline comparison. employ context-sensitive random forest similar model presented zikic brain tumors except apply forest images without additional tissue speciﬁc priors. train forest trees maximum depth larger size improve results. training data points approximately equally sampled lesion background classes optimal balance empirically chosen. hundred randomized cross-channel features evaluated split node maximum oﬀsets sizes folds training test sets used approach. table summarizes results tbi. signiﬁcantly outperforms random forest baseline relatively overall values indicate diﬃculty task. randomness training local minima network converges diﬀerent training sessions errors produce diﬀer clear unbiased errors network form ensemble three similar networks aggregating output averaging. ensemble yields better performance metrics also allows investigate behaviour network focusing biased errors. fig. shows obtained table performance deepmedic ensemble three networks database. comparison provide results random forest baseline. values correspond mean numbers bold indicate signiﬁcant improvement post-processing ensemble subject relation manually segmented predicted lesion volume. network capable segmenting cases small lesions although performance less robust cases even small errors large inﬂuence metric. investigation predicted lesion volume important biomarker prognostication shows network neither biased towards lesion background class promising results even cases small lesions. furthermore separately evaluate inﬂuence post-processing fully connected crf. shown table yields improvements classiﬁers. eﬀects prominent performance primary segmenter degrades shows robustness regulariser. fig. shows three representative cases. brain tumors evaluate system data brain tumor segmentation challenge training consists cases high grade cases grade glioma corresponding reference segmentations provided. segmentations include following tumor tissue classes necrotic core edema non-enhancing enhancing core. test consists cases grade revealed. reference segmentations test hidden evaluation carried online system. evaluation four predicted labels merged figure achieved ensemble three networks datasets. manually segmented predicted lesion volumes note logarithmic scale. continuous lines represent mean values. outlying subject presents small lesions successfully segmented also vascular ischemia. case database latter pathology networks fail segment lesion seen training. diﬀerent sets whole tumor core enhancing tumor subject four sequences available flair t-contrast datasets pre-processed organizers provided skull-stripped registered common space resampled isotropic resolution. dimensions volume minimal pre-processing normalizing brainnetwork conﬁguration training modify deepmedic architecture handle multi-class problems extending classiﬁcation layer feature maps rest conﬁguration remains unchanged. enrich dataset sagittal reﬂections. opposite experiments employ intenfor interpretation results note that best knowledge cases enhancing tumor class present manual segmentation considered zeros calculation average performance evaluation platform lowering upper bound class. figure three examples application system database. capable precise segmentation small large lesions. second depicts common mistakes observed. contusion near edge brain under-segmented possibly mistaken background. bottom shows worst cases representative challenges segmenting tbi. post-surgical sub-dural debris mistakenly captured brain mask. network partly segments abnormality celebral lesion interest. sity perturbation dropout convolutional layers network require much regularisation large database. network trained image segments extracted equal probability centred whole tumor healthy tissue. distribution classes captured training scheme provided appendix conﬁguration multi-class problem challenging global parameters consistently improve segmentation classes. instead merge four predicted probability maps single whole tumor post-processing. reﬁnes boundaries tumor background additionally removes isolated false positives. similarly experiments conﬁgured random subset training images reshuﬄed subsequent -fold cross validation. table average performance system training data brats computed online evaluation platform comparison submissions visible time manuscript submission. presenting teams submitted half cases. numbers bold indicate signiﬁcant improvement according two-sided paired quantitative results application deepmedic ensemble three similar networks training data presented table latter oﬀer improvement albeit fairly small since performance deepmedic already rather high task. also shown results previous works reported online evaluation platform. various settings vary among submissions pre-processing pipeline number folds used cross-validation. still appears system performs favourably compared previous state-of-the-art including semi-automatic system bakas latest challenge method pereira based grade-speciﬁc cnns requires visual inspection tumor identiﬁcation grade user prior segmentation. examples segmentations obtained method shown fig. deepmedic behaves well preserving hierarchical structure tumor account large context processed multi-scale network. table shows results method brats test data. results submissions accessible. decrease performance possibly inclusion test images vary signiﬁcantly training data cases acquired clinical centers provide training images something conﬁrmed organisers. note performance gains obtained larger case. indicates conﬁguration overﬁtted training database also robust factors variation acquisition sites complements nicely sensitive cnn. table average performance system test cases brats computed online evaluation platform. numbers bold indicate signiﬁcant improvement according two-sided paired participated ischemic stroke lesion segmentation challenge system achieved best results among participants sub-acute ischemic stroke lesions training phase challenge datasets made available along figure examples deepmedic’s segmentation evaluation training datasets brats cyan necrotic core green oedema orange non-enhancing core enhancing core. satisfying segmentation tumor regardless motion artefacts certain sequences. worst cases over-segmentation observed. false segmentation flair hyper-intensities oedema constitutes common error deepmedic. testing stage teams provided datasets evaluation. test data acquired clinical centers provided training images. corresponding expert segmentations hidden results submitted online evaluation network conﬁguration training conﬁguration network employed described kamnitsas main diﬀerence conﬁguration used tumors employed relatively smaller number low-resolution pathway. choice signiﬁcantly inﬂuence accuracy generally small siss lesions allowed lower computational cost. similar experiments evaluate network -fold cross validation training datasets. data augmentation sagittal reﬂections. testing phase challenge trained ensemble three networks training cases aggregate predictions averaging. performance system training data shown table signiﬁcant improvement achieved structural regularisation oﬀered although could partially accounted overﬁtting training data crf’s conﬁguration. examples visual inspection shown fig. table performance system training data isles-siss competition. values correspond mean numbers bold indicate signiﬁcant improvement according two-sided paired t-test metric testing phase challenge formed ensemble three networks coupled fully connected crf. submission ranked ﬁrst indicating superior performance challenging task among submissions. table shows results along entries halme among participating methods havaei layers convolutions. method perfomed less well challenging task points advantage oﬀered context large ﬁeld view deepmedic thanks multi-scale processing representational power deeper networks. important note decrease performance comparison training set. methods performed worse data coming second clinical center including method feng machine-learning based. highlights general diﬃculty current approaches applied multi-center data. implemented using theano library training session requires approximately nvidia titan using cudnn eﬃcient architecture deepmedic also allows models trained gpus memory. note although dimensions volumes processed databases allow dense training whole volumes size network dense inference whole volume still possible requires forward-pass thus less memory. fashion segmentation volume takes less seconds requires memory. tiling volume multiple segments size allows inference gpus less three minutes. fully connected implemented extending original source code kr¨ahenb¨uhl koltun implementation fast capable processing ﬁve-channel brain scan three minutes. speed-up could achieved implementation figure examples segmentations performed system training datasets isles system capable satisfying segmentation large smaller lesions. common mistakes performed challenge diﬀerentiating stroke lesions white matter lesions. presented deepmedic architecture automatic lesion segmentation surpasses state-of-the-art challenging data. proposed novel training scheme computationally eﬃcient also oﬀers adaptive partially alleviating inherent class-imbalance segmentation problems. analyzed beneﬁts using small convolutional kernels cnns allowed develop deeper thus discriminative network without increasing computational cost number trainable parameters. discussed challenges training deep neural networks adopted solutions latest advances deep learning. furthermore proposed eﬃcient solution processing large image context parallel convolutional pathways multi-scale processing alleviating main computational limitations previous cnns. finally presented ﬁrst application fully connected medical data employed post-processing step reﬁne network’s output method also shown promising processing natural images design proposed system well suited processing medical volumes thanks generic nature. capabilities deepmedic employed capturing patterns exceed networks locally connected random ﬁelds models commonly used previous work. time system eﬃcient inference time allows adoption variety research clinical settings. generic nature system allows straightforward application diﬀerent lesion segmentation tasks without major adaptations. best knowledge system achieved highest reported accuracy cohort patients severe tbi. comparison improved reported performance pipeline important note latter work focused segmentation contusions system shown capable segmenting even small diﬀused pathologies. additionally pipeline achieved state-of-the-art performance public benchmarks brain tumors stroke lesions believe performance improved taskdata-speciﬁc adjustments instance pre-processing results show potential generically designed segmentation system. applying pipeline tasks laborious process reconﬁguration crf. model improved system’s performance statistical signiﬁcance investigated tasks profoundly performance underlying classiﬁer degrades proving ﬂexibility robustness. finding optimal parameters task however challenging. became obvious task multi-class tumor segmentation. tumor’s substructures vary signiﬁcantly appearance ﬁnding global parameters yields improvements classes proved diﬃcult. instead applied binary fashion. model conﬁgured separate parameters class. however larger parameter space would complicate conﬁguration further. recent work zheng showed particular casted neural network parameters learned regular gradient descent. training end-to-end fashion neural network would alleviate discussed problems. explored part future work. discriminative power learned features indicated success recent cnn-based systems matching human performance domains previously considered ambitious silver analysis automatically extracted information could potentially provide novel insights facilitate research pathologies little prior knowledge currently available. attempt illustrate this explore patterns learned automatically lesion segmentation tasks. visualize activations deepmedic’s processing subject database. many appearing patterns diﬃcult interpret especially deeper layers. fig. provide examples intuitive explanation. interesting ﬁndings network learns identify ventricles white gray matter. reveals diﬀerentiation tissue type beneﬁcial lesion segmentation. line ﬁndings literature segmentation performance traditional classiﬁers signiﬁcantly improved incorporation tissue priors zikic intuitive diﬀerent types lesions aﬀect diﬀerent parts brain depending underlying mechanisms pathology. rigorous analysis spatial cues extracted network reveal correlations well deﬁned yet. similarly intriguing information extracted low-resolution pathway. process greater context neurons gain additional localization capabilities. activations certain form ﬁelds surrounding areas brain. patterns preserved deepest hidden layers indicates beneﬁcial ﬁnal segmentation believe cues provide spatial bias system instance large contusions tend occur towards front sides brain furthermore interaction multi-resolution features observed hidden layer follows concatenation pathways. network learns weight figure scan deepmedic’s segmentation. earlier deeper layers ﬁrst convolutional pathway. features learnt low-resolution pathway. last hidden layers combine multi-resolution features towards ﬁnal segmentation. output pathways preserving resolution certain parts show details others assumption low-resolution pathway provides rough localization large pathologies brain areas challenging segment reserves rest network’s capacity learning detailed patterns associated detection smaller lesions structures ambiguous areas. ﬁndings exploration lead believe great potential lies fusing discriminative power deep black knowledge acquired years targeted biomedical research. clinical knowledge available certain pathologies spatial priors white matter lesions. previously engineered models proven eﬀective tackling fundamental imaging problems brain extraction tissue segmentation bias ﬁeld correction. show network capable automatically extracting information. would interesting however investigate structured ways incorporating existing information priors network’s feature space simplify optimization problem letting specialist guide network towards optimal solution. although neural networks seem promising medical image analysis making inference process interpretable required. would allow understanding network fails important aspect biomedical applications. although output bounded range commonly referred probability convenience true probability bayesian sense. research towards bayesian networks aims alleviate limitation. example recent work ghahramani show model conﬁdence estimated sampling dropout mask. general point made performance drop observed system applied test datasets brats isles comparison cross-validated performance training data. cases subsets test images acquired clinical centers diﬀerent ones training datasets. diﬀerences scanner type acquisition protocols signiﬁcant impact appearance images. issue multi-center data heterogeneity considered major bottleneck enabling large-scale imaging studies. speciﬁc approach general problem medical image analysis. possible making invariant data heterogeneity learn generative model work supported epsrc first grant scheme partially funded framework programme european commission work supported medical research council program grant national institute health research biomedical research centre cambridge technology platform funding provided department health. supported imperial college london scholarship programme. vfjn supported health foundation/academy medical sciences clinician scientist fellowship. supported nihr senior investigator award. gratefully acknowledge support nvidia corporation donation titan gpus research. integration multi-scale parallel pathways architectures solely unary kernel strides proposed described sec. required up-sampling low-resolution features performed simple repetition experiments. found suﬃcient following hidden layers learning combine multi-scale features. case architectures strides greater unary last convolutional layers pathways receptive ﬁelds strides respectively. preserve spatial correspondence multi-scale features enable network dense inference dimensions input segments chosen brought dimensions sequential resampling networks main description system presented sec. models discussed work outside sec. fully cnns. architectures presented table b.a. prelu nonlinearity trained using rmsprop optimizer nesterov momentum value regularisation applied. train networks dense-training batches segments size exceptions experiments batch sizes adjusted along segment sizes achieve similar memory footprint training time batch. weights shallow -layers networks initialized sampling normal distribution initial learning rate deeper models weight initialisation scheme scheme increases signal’s variance settings leads rmsprom decreasing eﬀective learning rate. counter this accompany increased initial learning rate throughtraining learning rate models halved whenever convergence plateaus. dropout rate employed last hidden layers -layers deep models. networks table presents representative examples conﬁgurations employed experiments discussed sec. width depth batch size adjusted total required memory similar version deepmedic. wider deeper variants ones presented show greater performance. possible reason number ﬁlters enough extraction limited information ﬁeld view deep multi-scale variant already suﬃcient application. presented models regularized since less parameters variants. dmdpatch trained momentum initial learning rate rest setting increased performance. rest hyper parameters deepmedic. table network architectures investigated sec. ﬁnal validation accuracy achieved corresponding experiments. architectures. columns left right model’s name number parallel identical pathways number feature maps convolutional layers number feature maps hidden layer follows concatenation pathways dimensions input segment normal resolution pathways batch size ﬁnally average achieved validation fold. conﬁguration details provided appendix table real distribution classes training data brats along distribution captured proposed training scheme segments size extracted centred tumor healthy tissue equal probability. relative distribution foreground classes closely preserved imbalance comparison healthy tissue automatically alleviated.", "year": 2016}