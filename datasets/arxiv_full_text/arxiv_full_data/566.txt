{"title": "Efficient Exact Gradient Update for training Deep Networks with Very  Large Sparse Targets", "tag": ["cs.NE", "cs.CL", "cs.LG"], "abstract": "An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size D (e.g. 200 000). Computing the equally large, but typically non-sparse D-dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating the D x d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efficient handling of large sparse network inputs is trivial, the case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d^2) per example instead of O(Dd), remarkably without ever computing the D-dimensional output. The proposed algorithm yields a speedup of D/4d , i.e. two orders of magnitude for typical sizes, for that critical part of the computations that often dominates the training time in this kind of network architecture.", "text": "important class problems involves training deep neural networks sparse prediction targets high dimension occur naturally e.g. neural language models learning word-embeddings often posed predicting probability next words among vocabulary size computing equally large typically non-sparse d-dimensional output vector last hidden layer reasonable dimension incurs prohibitive computational cost example updating output weight matrix computing gradient needed backpropagation previous layers. efﬁcient handling large sparse network inputs trivial case large sparse targets thus sidestepped approximate alternatives hierarchical softmax sampling-based approximations training. work develop original algorithmic approach which family loss functions includes squared error spherical softmax compute exact loss gradient update output weights gradient backpropagation example instead remarkably without ever computing d-dimensional output. proposed algorithm yields speedup i.e. orders magnitude typical sizes critical part computations often dominates training time kind network architecture. many modern applications neural networks deal data represented representable large sparse vectors. representations arise natural language related tasks dimension vector typically size vocabulary also sparse user-item matrices collaborative-ﬁltering applications. trivial handle large sparse inputs neural network computationally efﬁcient manner forward propagation update input weight matrix backpropagation correspondingly sparse. contrast training large sparse prediction targets problematic even target sparse computation equally large network output corresponding gradient update huge output weight matrix sparse thus computationally prohibitive. practical problem ever since bengio ﬁrst proposed using neural network learning language model case computed output vector represents probability next word size considered vocabulary becoming increasingly large modern applications several approaches proposed attempt address difﬁculty essentially sidestepping fall categories sampling selection based approximations consider compute tiny fraction output’s dimensions sampled random heuristically chosen. reconstruction sampling dauphin efﬁcient biased importance sampling jean noise contrastive estimation mnih kavukcuoglu mikolov fall category. recent approximate maximum inner product search based locality sensitive hashing techniques select good candidate subset. compared initial problem considering output dimensions kinds approaches crude approximations. present work instead investigate actually perform exact gradient update corresponds considering outputs implicitly computationally efﬁcient manner without actually computing outputs. approach works relatively restricted class loss functions simplest linear output squared error common choice multiclass classiﬁcation softmax loss part class alternative spherical softmax also yield normalized class probabilities. simplicity presentation focus squared error online setting later brieﬂy mention extension minibatches general class loss functions. concerned gradient-descent based training deep feed-forward neural network target vectors high dimension sparse i.e. comparatively small number elements target vector non-zero. ksparse vector typically stored represented compactly numbers corresponding pairs network trained targets naturally equally large output layer dimension also optionally allow input network similarly high dimensional sparse vector dimension din. large sparse target output input suppose network’s intermediate hidden layers smaller typically manageable dimension mathematical notation vectors denoted using lower-case letters e.g. considered column-vectors; corresponding vectors denoted transpose e.g. matrices denoted using upper-case letters e.g. transpose column figure computational problem posed large sparse targets. dealing sparse input efﬁciently trivial forward backward propagation phases easily achieved however case large sparse targets. incur prohibitive computational cost output layer forward propagation gradient backpropagation weight update require accessing elements large output weight matrix. network architecture consider standard feed forward neural network architecture depicted figure input vector rdin linearly transformed linear activation input weight matrix rd). typically followed non-linear transformation yield representation ﬁrst hidden layer ﬁrst hidden layer representation similarly transformed number subsequent non-linear layers e.g. obtain last hidden layer representation obtain ﬁnal d-dimensional network output output weight matrix main focus work. finally network’s d-dimensional output compared d-dimensional target vector associated input using squared error yielding loss training procedure architecture typical multi-layer feed forward neural network architecture linear output layer squared error loss. parameters trained gradient descent using gradient backpropagation rumelhart lecun efﬁciently compute gradients. procedure shown figure given example training pair pass forward propagation proceeds outlined above computing hidden representation hidden layer turn based previous ﬁnally network’s predicted output associated loss pass gradient backpropagation works opposite direction starting propagating back gradients upstream network. corresponding gradient contributions parameters collected along straightforward associated speciﬁcally similarly input layer output layer parameters updated gradient descent step positive learning-rate. similarly output layer main focus here easy straightforward efﬁciently compute forward propagation backpropagation weight update part input layer large din-dimensional k−sparse input vector appropriate sparse representation. speciﬁcally suppose represented pair vectors length contains integer indexes associated real values elements forward propagation input layer sparse representation positions elements together value makes cheap compute even though huge full matrix rows need visited summed compute precisely sparse representation operation written d-dimensional vector making gradient update input layer suppose able gradients ﬁrst hidden layer activations corresponding gradient-based update form gradient vector input layer weights simply ηx)t rank-one update again rows associated non-zero entries need modiﬁed. precisely operation written ηvk∇a making operation rather computing ﬁnal output incurs prohibitive computational cost since full matrix. note a-priori reason representation sparse even would fundamentally change problem since extremely large supposed reasonably sized already. computing residual associated squared error loss incurs additional cost. gradient need backpropagate lower layers rank-one update updates elements incurs prohibitive computational cost. large three operations prohibitive fact sparse seen perspective doesn’t help since neither sparse. computationally efﬁcient algorithm performing exact online previously proposed workarounds approximate stochastic sampling. propose different approach results exact same efﬁcient gradient update remarkably without ever compute large output suppose that have network input example computed last hidden representation forward propagation. network’s dimensional output principle compared high dimensional target corresponding squared error loss section computing direct naive would prohibitive computational complexity computing output full matrix typically non-sparse similarly backpropagate gradient network need compute gradient loss respect last hidden layer representation again compute directly manner computational complexity would prohibitive provided maintained up-to-date matrix reasonable size cheaply maintained section rewrite operations perform terms leveraging k-sparse representation target vector altogether computational cost several orders magnitude cheaper prohibitive direct approach. gradient squared error loss respect output layer weight matrix corresponding gradient descent update would wnew positive learning rate. again computed manner induces prohibitive computational complexity compute output residual update elements elements must accessed update. surface seems hopeless. achieve exact update trick newh operation needed ensuing rank-one easy compute update thanks k-sparsity thek rows associated non-zero elements accessed updated sited rows modify naive update note factored representation implicitly terms entered computation previous paragraph need adapted slightly becomes rather computational complexity. doesn’t change overall complexity computations. already seen cheaply maintain up-to-date following update similarly following updates need keep up-to-date needed efﬁciently compute loss gradient shown updates equations equivalent implicitly updating wnew translates following update seen efﬁciently compute cost gradient respect well updating performing bookkeeping algorithm describes detailed algorithmic steps together equations derived above. proposed algorithm requires operations whereas standard approach required operations. take state precisely proposed algorithm computing loss gradient updates require roughly operations whereas standard approach required roughly operations. overproposed algorithm change corresponds computational speedup factor expected speedup thus note advantage computational complexity also memory access. example standard approach needs access change elements matrix whereas proposed approach accesses much smaller number elements well three matrices overall substantially faster algorithm which implicitly nevertheless perform exact gradient update standard approach. want emphasize approach completely different simply chaining linear layers performing ordinary gradient descent updates these would result prohibitive computational complexity standard approach ordinary separate gradient updates uand would equivalent ordinary gradient update update equation time lead become ill-conditioned. prevent this regularly monitor conditioning number. either smallest largest singular value moves outside acceptable range bring back appropriate rank- update algorithm also straightforwardly extended minibatch case yields theoretical speedup factor respect standard naive approach. needs careful order keep computation reasonably efﬁcient depending size minibatch efﬁcient solve corresponding linear equation minibatch scratch rather updating woodbury equation approach detailed linear output squared error easily extended slightly exotic loss functions basically loss function expressed using squared norm whole output vector compute cheaply. family loss functions include standard softmax includes so-called spherical softmax remains seen practice approach performs computationally whether lose something using limited family loss functions. implemented version using blas parallel version using cublas proposed algorithm. evaluated implementations training word embeddings simple neural language models probability next word given preceding n-gram learned neural network. used nvidia titan black .ghz experiments billion word dataset composed billions words belonging vocabulary millions words. evaluated resulting word embeddings recently introduced simlex- score measures similarity words. also compared approach unfactorised versions two-layer hierarchical softmax. figure illustrate practical speedup approach output layer only. figure shows models much faster train softmax models converge slightly lower simlex- scores. table summarizes speedups different output layers tried gpu. also emprically veriﬁed proposed factored algorithm learns exact model weights figure timing different algorithms. time taken forward backward propagations output layer including weight update minibatch size different sizes vocabulary gpu. input size ﬁxed timing layer hierarchical softmax efﬁcient implementation also provided comparison. right plot log-log scale. expected timings factorized versions independent size vocabulary. introduced algorithmic approach efﬁciently compute exact gradient updates training deep networks large sparse targets. remarkably complexity algorithm independent target size allows tackling large problems. implementation yield similar speedups theoretical thus used practical applications could explored work. particular neural language models seem good candidates. remains unclear using loss function log-softmax affect quality resulting word embeddingsm research carried direction. extensions approach possible losses simple squared error also empirically investigated light particular log-spherical-softmax. figure left practical theoretical speedups different sizes vocabulary ﬁxed input size practical unfact fact speedup similar theoretical one. right evolution simlex- score obtained different models function training time softmax models zero hidden-layer models large sparse target models hidden layers. best architectures retained cases extra non-linear layers help compensate lack softmax. models converge slightly lower scores similar speed hierarchical softmax model signiﬁcantly faster softmax models. gutmann hyvarinen. noise-contrastive estimation estimation principle unnormalized statistical models. proceedings thirteenth international conference artiﬁcial intelligence statistics andriy mnih koray kavukcuoglu. learning word embeddings efﬁciently noise-contrastive estimation. c.j.c. burges bottou welling ghahramani k.q. weinberger editors advances neural information processing systems pages curran associates inc. anshumali shrivastava ping asymmetric sublinear time maximum inner product search ghahramani welling cortes n.d. lawrence k.q. weinberger editors advances neural information processing systems pages curran associates inc. robert cowell zoubin ghahramani editors proceedings tenth international workshop artiﬁcial intelligence statistics pages society artiﬁcial intelligence statistics yann lecun. learning processes asymmetric threshold network. bienenstock fogelmansoulié weisbuch editors disordered systems biological organization pages springer-verlag berlin houches ciprian chelba tomas mikolov mike schuster thorsten brants phillipp koehn tony robinson. billion word benchmark measuring progress statistical language modeling. interspeech annual conference international speech communication association singapore september pages james bergstra olivier breuleux frédéric bastien pascal lamblin razvan pascanu guillaume desjardins joseph turian david warde-farley yoshua bengio. theano math expression compiler. proceedings python scientiﬁc computing conference oral presentation. frédéric bastien pascal lamblin razvan pascanu james bergstra goodfellow arnaud bergeron nicolas bouchard yoshua bengio. theano features speed improvements. deep learning unsupervised feature learning nips workshop algorithm derived online gradient relatively straightforward extend case minibatches containing examples still yield theoretical speedup factor respect standard naive approach. want careful order keep computation reasonably efﬁcient. minibatch version presented below update based woodbury equation depending size minibatch become efﬁcient solve corresponding linear equations minibatch scratch every time rather inverting matrix. case won’t need maintain all. update time lead becoming ill-conditioned. simultaneously update updated numerically start diverge true numerical precision. thus important prevent form happening i.e. make sure stays well conditioned ensure numerical stability algorithm. present progressively reﬁned strategies achieving this. operation doesn’t affects product implicit matrix remains unchanged affect restore perfectly well conditioned identity matrix. computing extremely costly operation possible want avoid next paragraphs develop efﬁcient strategy. becoming ill-conditioned singular values time becoming large and/or small. deﬁne singular values ordered decreasing order. conditioning number deﬁned become overly large becomes large and/or becomes small. restoring system pristine state shown previous paragraph effect brings back singular values back instead possible computationally less costly correct needed singular values fall outside safe range. often need occasionally correct singular value determined offending singular value corresponding singular vectors correcting singular value i.e. effectively bringing back operation. point apply corrective steps problematic singular values needed rather blindly needlessly inefﬁciently correcting basic full restoration explained previous paragraph. chosen safe range singular values procedures given output layer parameters concision enlist parameters explicitly parameter list. procedure singular-stabilize gets called every ncheck gradient updates doesn’t change singular vectors singular value. also changes symetrically remains unchanged. computing matrix required above costs roughly elementary operations since offending singular values typically smallest largest wasteful compute singular values every time. possibly cheaper alternative power iteration method largest singular value associated singular vector similarly u−to obtain smallest singular value iteration power iteration method requires operations iterations sufﬁce. experiments ﬁxed power iterations. also probably critical power iteration method fully convergence correcting along approximate offending singular vector direction sufﬁcient purpose ensuring numerical stability. reﬁnement loop ﬁnding smallest singular value power iteration method correcting calling fix-singular-value small repeat smallest singular value inside acceptable range. similarly largest singular values. note principle need ever invert scratch nevertheless proved necessary regularly ensure doesn’t stray much correct value numerical imprecisions. inverting using gaussian-elimination costs roughly operations reasonable won’t affect computational complexity often every training examples practice recompute scratch every time check singular value stabilization.", "year": 2014}