{"title": "SNN: Stacked Neural Networks", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "It has been proven that transfer learning provides an easy way to achieve state-of-the-art accuracies on several vision tasks by training a simple classifier on top of features obtained from pre-trained neural networks. The goal of this work is to generate better features for transfer learning from multiple publicly available pre-trained neural networks. To this end, we propose a novel architecture called Stacked Neural Networks which leverages the fast training time of transfer learning while simultaneously being much more accurate. We show that using a stacked NN architecture can result in up to 8% improvements in accuracy over state-of-the-art techniques using only one pre-trained network for transfer learning. A second aim of this work is to make network fine- tuning retain the generalizability of the base network to unseen tasks. To this end, we propose a new technique called \"joint fine-tuning\" that is able to give accuracies comparable to finetuning the same network individually over two datasets. We also show that a jointly finetuned network generalizes better to unseen tasks when compared to a network finetuned over a single task.", "text": "figure conceptual state deep learning space presented trade time consuming taskspeciﬁc neural network training targeted speciﬁcally speciﬁc datasets versus fast neural network training transfer learning obtain reasonable performance relatively generalizable neural networks. goal paper evaluate performance network architecture named stacked neural networks leverage fast training speed transfer learning considerably increasing accuracy transfer learning generating better features. since razavian al.’s work several state-of-the-art pretrained neural networks made publicly available caffe model include networks googlenet places initial studies suggested networks nonoverlapping mis-classiﬁcation behavior. observation leads believe combining combination networks improve classiﬁcation compensating shortcoming networks. valuable understand whether single combination neural networks generalizable datasets different combination networks provide optimal results proven transfer learning provides easy achieve state-of-the-art accuracies several vision tasks training simple classiﬁer features obtained pre-trained neural networks. goal project generate better features transfer learning multiple publicly available pretrained neural networks. propose novel architecture called stacked neural networks leverages fast training time transfer learning simultaneously much accurate. show using stacked architecture result improvements accuracy state-of-the-art techniques using pre-trained network transfer learning. second project make network ﬁnetuning retain generalizability base network unseen tasks. propose technique called joint ﬁnetuning able give accuracies comparable ﬁnetuning network individually datasets. also show jointly ﬁnetuned network generalizes better unseen tasks compared network ﬁnetuned single task. transfer learning general framework trains pre-trained neural network task network trained. amazingly razavian shown transfer learning pre-trained neural network outperform traditional hand-tuned approaches several tasks including coarse-grained detection ﬁne-grained detection attribute detection. work razavian point it’s features. figure depicts trade obtaining high classiﬁcation accuracy deep networks highly trained particular dataset long time period transfer learning produce decent classiﬁcation accuracy short training time period. order reach target work present novel method leveraging higher going further short note terminology. paper term transfer learning mean training classiﬁer features extracted pre-trained networks without changing networks. hand ﬁne-tuning mean changing layers network better given task. ensembling recognized simple boost performance vision task averaging scores multiple networks together. however tasks image-to-sentence retrieval features desirable instead score predeﬁned classes. thus tackle problem generating better features rather simply improving transfer learning accuracy ensembling. work show combination several network features novel technique call stacking offers better accuracy many vision tasks. also evaluate various combinations networks network combinations offer best accuracy across different datasets whether single combination networks offers substantially higher accuracy across board. also evaluate effect using ensemble stacked networks rather single stacked network order boost performance transfer learning even further. observe ensembling provide substantial boost performance offered stacking. also examine effects ﬁne-tuning generalizability features output network. observe signiﬁcant drop generalization performance network ﬁne-tuned speciﬁc task. however show joint ﬁnetuning single network different tasks actually create network accuracies close individually ﬁne-tuned networks. also show features jointly ﬁne-tuned network significantly higher generalizability unseen tasks ﬁnetuned network single task. however observe jointly ﬁne-tuned network still signiﬁcantly underperforms baseline using pre-trained network features only. section details experiments area. -layers -layers architecture proposed simonyan uses deep network smaller convolution ﬁlters size obtain stateof-the-art accuracies imagenet challenge. layer network features layer. googlenet architecture used szegedy places network created zhou architecture alexnet trained places dataset instead imagenet enable better performance scene-centric tasks. layer places features layer. network network network architecture used uses neural networks layer transfer function instead convolution followed non-linearity. pool layer features. lisa trafﬁc sign dataset dataset contains annotations video frames captured roads. image labeled trafﬁc signs visible images well location sign. covers trafﬁc signs. section formally deﬁne stacked neural networks discuss studies conducted construct novel deep learning framework improving state-ofthe-art prediction accuracy deep neural networks stacked neural networks deﬁned combination publicly available neural network architectures whose features extracted intermediate layer network concatenated together form larger feature set. figure illustrates idea detail. concatenated feature vector used train classiﬁer layer consists optional dropout layer afﬁne layer loss function. impact dropout figure stack publicly available neural network architectures. features generated network combined uniﬁed feature vector. vector used classify dropout afﬁne layers. deﬁne combination multiple networks stacked neural network layer discussed detail section figure shows convolutional neural networks members s-nn combination cnn’s also considered s-nn. instance {googlenet vgg} {nin places vgg} examples network -network s-nn’s. evaluate effect different network combinations section showing s-nn’s deliver higher classiﬁcation accuracy singlenetwork structures. shown literature ensemble multiple independently trained networks improve prediction accuracy reducing classiﬁcation error rate. combination s-nn’s produces feature vector scores. improve classiﬁcation accuracy studied effect ensemble mean scores ﬁnal network. figure shows number s-nn’s whose scores combined ensemble score. arbitrary group s-nn’s used generate ensemble score choose compute score stacking s-nn network combination subsets. example given s-nn containing three networks {nin googlenet} take ensemble score network subsets {nin} {vgg} {googlenet} {nin vgg} {nin googlenet} {vgg googlenet} {nin vgg} {nin googlenet}. method forming enfigure aimed answer question shows best experimental results combinations s-nn’s datasets. s-nn combination evaluated using different hyperparameter sweeps list hyperparamters used work included table evaluated validation accuracy single-network network -network -network stack combinations. -network experiments dropout layer -network experiments dropout layer discovered dropout becomes important training element number networks larger two. case networks however experimented network performance without dropout layer. figure points networks case s-nn figure best performing s-nn’s different datasets many several different hyperparameters. snn’s network included. number networks combinations evaluated. table hyperparameters used proﬁle classiﬁcation accuracy network combinations used s-nn experiments. best classiﬁcation accuracy obtained hyperparameter combinations reported data point figure figure aimed answer question shows possible network combinations represents accuracy degradation compared best s-nn combination -networks datasets. figure indicates single-network case googlenet generalizable least mean standard deviation accuracy degradation. -network case vgg+googlenet make best network. sensible choice network among best publicly available networks constructed based different architectural assumptions making relatively uncorrelated misclassiﬁcation behavior standpoint. -network case vgg+googlenet+places best s-nn. network case vgg+vgg+googlenet+places form best classiﬁcation choice indicating poor generalization ability cnn. figure horizontal axis shows possible s-nn architectures. vertical axis shows ensemble accuracy architecture combinations subsets. example subsets {nin vgg} used compute ensemble accuracy {nin vgg} {vgg} {nin}. network ensembles deliver highest generalization accuracy -network cases highlighted gray. figure focuses extracting best results obtained dataset different experimental settings. compares single-network case s-nn without dropout s-nn dropout training. shows best accuracies single-network ensembles well stacked ensemble model shown earlier. singlenetwork ensemble refers taking mean scores individual networks order construct ﬁnal score. interestingly enough results superior previous s-nn approach without score ensembles. further improve performance stack ensemble mode also includes scores s-nn architectures evaluated figure order generate even accurate classiﬁcations. black lines figure show state-of-the-art accuracies given dataset data augmentation applied dashed black lines show state-of-theart accuracies without data augmentation authors able identify publications explicitly showing state-of-the-art classiﬁcation results lisa dataset. performed data augmentation part study believe surpass state-of-the-art performance results even datasets results solid line moment. figure shows confusion matrices standalone classiﬁcation performance googlenet places cnn’s running scene dataset. avoid clutter figure illustrates classes. here draw number interesting insights results comparing similar confusion cells different matrices figure confusion matrix scene dataset three independent cnn’s along s-nn model. dataset classes. avoid clutter matrices consider classes. value cell corresponds number instances image actual class classiﬁed main diagonal represents correct classiﬁcation. show loss generalizability ﬁne-tuning ﬁrst ﬁne-tuned task create network vgga independently task create network vggb. subsequently evaluated transfer learning accuracy task vgga. compared accuracy vggb task results summarized table comparison shows indeed ﬁne-tuning task reduced transfer-learning accuracy task drastically thus interesting question whether possible ﬁne-tune single network give accuracies similar individually ﬁnetuned networks tasks figure best performance results generated across tests done. dashed line represent state-of-the-art without data augmentation solid lines represent state-ofthe-art data augmentation. misclassiﬁcation happens times times googlenet times places. combined s-nn misclassiﬁcations. shows error correction power networks features compensating other’s weakness. effect also observed misclassiﬁcation s-nn zero googlenet misclassiﬁes images cell. googlenet times places vgg. despite presence places network s-nn confuses images cell. shows despite presence networks completely eliminate type image misclassiﬁcation s-nn score googlenet dominates overall outcome often. section potential solution problem discussed. misclassiﬁcation never happens single-net cases. however stack features shows misclassiﬁcation cell. negligible misclassiﬁcation case shows stack features adversarial effect ﬁnal outcome. section potential solution problem discussed. part work also looked generalization network features suffers result ﬁnetuning network given task. perform experiments considered three tasks different domains food- scene caltech network architecture shown figure trained single network concatenation multiple dataset inputs minibatch. minibatch consisted images task task output features network independent linear classiﬁer layers tasks. ﬁnal loss taken softmax losses classiﬁer layers. denote jointly trained network vggab. table shows resulting accuracy approach. seen accuracy vggab task close accuracy vgga accuracy task close vggb thus inferred indeed possible gain best vgga vggb single network. seen that indeed generalization capabilities jointly ﬁnetuned network higher. although jointly ﬁnetuned network still reach transfer learning capability baseline network alone succeed mitigating degradation coming individual ﬁnetuning. suspect reason behind phenomenon jointly-ﬁnetuned network forced ﬁnetune towards general properties tasks rather extremely speciﬁc particular task. shown training neural networks deeper layers improve classiﬁcation accuracy. s-nn’s explained previous sections targeted toward agile training high generalization accuracy reasonable hypothesize trained together collective classiﬁcation error would drop. instead using data networks pretrained independently allow backward propagation training method broadcast classiﬁer gradients networks. since networks iterate gradients parallel inﬂuence other’s weight bias values. limited computation capability available unable generate results step. continue working scheme access powerful machines. previous section jointly tuning network tasks give accuracies comparable individual ﬁnetuning. also individual ﬁnetuning destroys generalization capabilities original network. facts bring next question generalization capability jointly tuned network? analysis confusion matrices available datasets made realize combination network features s-nn helps reduce errors also case networks introduce excessive confusion error rate s-nn. reduce adversarial impacts s-nn plan combine features applying weights network feature. weight coefﬁcients dependent prediction accuracy network classifying given dataset. instance googletnet shows weaker classiﬁcation performance relative places relative contribution googlenet features reduced. feature vector network multiplied scalar value value computed dividing classiﬁcation accuracy accuracy network best result. instance assume s-nn consisting {googlenet vgg}. googlenet individual classiﬁcation accuracy respectively googlenet features multiplied respectively stacking features. data augmentation improved classiﬁcation accuracy diversifying features. literatures show substantial improvement scene oxford flowers datasets networks trained using data augmentation. short time technique believe substantially boost prediction accuracy datasets all. inspired notion ensembles presented hinton study proves combining multiple powerful networks leads substantial performance gains. also proves training single network multiple datasets deliver better generalization accuracy. next milestone would like tackle evaluate possibility building numerous small fast-to-train networks trained multiple datasets stacked s-nn’s. call stack wimpy neural networks. technique interesting fronts. first multiple wimpy s-nn’s well powerful network like vgg. second architecture help reduce computation overhead demand recent deep neural networks enabling much parallelizable network architecture ability conveniently ofﬂoaded onto multiple computation units work presented stacked neural networks novel technique extracting higher generalization accuracy state-of-the-art neural networks public domain. evaluated various stack combinations discovered ﬁve-cnn stack delivers best accuracy stack cnn’s deliver similar accuracy gains consuming much less computation power. also presented classiﬁcation accuracy improvements generating ensemble s-nn’s. combination furthermore evaluated effect training multiple datasets network. interestingly enough concluded possible jointly ﬁnetune single network multiple datasets still obtain accuracies alsimilar individual ﬁnetuning networks. also show jointly ﬁnetuned networks better generalization capabilities individually ﬁnetuned variants.", "year": 2016}