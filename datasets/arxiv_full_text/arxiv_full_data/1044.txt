{"title": "Memory Augmented Neural Networks with Wormhole Connections", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Recent empirical results on long-term dependency tasks have shown that neural networks augmented with an external memory can learn the long-term dependency tasks more easily and achieve better generalization than vanilla recurrent neural networks (RNN). We suggest that memory augmented neural networks can reduce the effects of vanishing gradients by creating shortcut (or wormhole) connections. Based on this observation, we propose a novel memory augmented neural network model called TARDIS (Temporal Automatic Relation Discovery in Sequences). The controller of TARDIS can store a selective set of embeddings of its own previous hidden states into an external memory and revisit them as and when needed. For TARDIS, memory acts as a storage for wormhole connections to the past to propagate the gradients more effectively and it helps to learn the temporal dependencies. The memory structure of TARDIS has similarities to both Neural Turing Machines (NTM) and Dynamic Neural Turing Machines (D-NTM), but both read and write operations of TARDIS are simpler and more efficient. We use discrete addressing for read/write operations which helps to substantially to reduce the vanishing gradient problem with very long sequences. Read and write operations in TARDIS are tied with a heuristic once the memory becomes full, and this makes the learning problem simpler when compared to NTM or D-NTM type of architectures. We provide a detailed analysis on the gradient propagation in general for MANNs. We evaluate our models on different long-term dependency tasks and report competitive results in all of them.", "text": "caglar gulcehre montreal institute learning algorithms universite montreal montreal canada sarath chandar montreal institute learning algorithms universite montreal montreal canada recent empirical results long-term dependency tasks shown neural networks augmented external memory learn long-term dependency tasks easily achieve better generalization vanilla recurrent neural networks suggest memory augmented neural networks reduce eﬀects vanishing gradients creating shortcut connections. based observation propose novel memory augmented neural network model called tardis controller tardis store selective embeddings previous hidden states external memory revisit needed. tardis memory acts storage wormhole connections past propagate gradients eﬀectively helps learn temporal dependencies. memory structure tardis similarities neural turing machines dynamic neural turing machines read write operations tardis simpler eﬃcient. discrete addressing read/write operations helps substantially reduce vanishing gradient problem long sequences. read write operations tardis tied heuristic memory becomes full makes learning problem simpler compared d-ntm type architectures. provide detailed analysis gradient propagation general manns. evaluate models diﬀerent long-term dependency tasks report competitive results them. recurrent neural networks neural network architectures designed handle temporal dependencies sequential prediction problems. however well known rnns suﬀer issue vanishing gradients length sequence dependencies increases long short term memory units proposed alternative architecture handle long range dependencies better vanilla rnn. simpliﬁed version lstm unit called gated recurrent unit proposed proven successful number applications even though lstms grus attempt solve vanishing gradient problem memory architectures stored single hidden vector done hence accessing information past still diﬃcult. words lstm models limited ability perform search past memories needs access relevant information making prediction. extending capabilities neural networks memory component explored literature diﬀerent applications diﬀerent architectures memory augmented neural networks neural turing machines dynamic diﬀerentiable neural computers external memory store information mann’s controller learn read write external memory. show here general possible particular manns explicitly store previous hidden states memory provide shortcut connections time called wormhole connections look history states controller. learning read write external memory using neural networks gives model freedom ﬂexibility retrieve information past forget store information memory. however addressing mechanism read and/or write operations continuous access diﬀuse especially early training. hurt especially writing operation since diﬀused write operation overwrite large fraction memory step yielding fast vanishing memories hand discrete addressing used discrete d-ntm able perform search past prevents using straight backpropagation learning choose address. investigate gradients wormhole connections introduced controller eﬀects results show wormhole connections created controller mann signiﬁcantly reduce eﬀects vanishing gradients shortening paths signal needs travel dependencies. also discuss manns generalize sequences longer ones seen training. discrete d-ntm controller must learn read write external memory additionally also learn reader/writer synchronization. make learning challenging. spite diﬃculty gulcehre reported discrete d-ntm learn faster continuous d-ntm babi tasks. provide formal analysis gradient manns based discrete addressing justify result. paper also propose mann based discrete addressing called tardis tardis memory access based tying write read heads main characteristics tardis follows tardis simple memory augmented neural network model represent long-term dependencies eﬃciently using external memory small size. tardis represents dependencies hidden states inside memory. show theoretically experimentally tardis ﬁxes large extent problems related long-term dependencies. model also store sub-sequences sequence chunks memory. consequence controller learn represent high-level temporal abstractions well. tardis performs well several structured output prediction tasks veriﬁed experiments. idea using external memory attention justiﬁed concept mental-time travel humans occasionally solve daily tasks. particular cognitive science literature concept chronesthesia known form consciousness allows human think time subjectively perform mental time-travel tardis inspired ability humans allows look past memories plan future using episodic memory. neural network architectures external memory represent memory matrix form time step model read write external memory. whole content external memory considered generalization hidden state vector recurrent neural network. instead storing information single hidden state vector model store matrix higher capacity targeted ability substantially change small subset memory time step. neural turing machine example mann reading writing memory. model outline subsection describe basic structure tardis tardis mann external memory matrix rk×q number memory cells dimensionality cell. model controller read write external memory every time step. read memory controller generates read reading operation typically achieved computing weights product read weights memory resulting content vector one-hot vector dot-product tardis uses discrete addressing hence chooses cells memory matrix controller generates write weights also vector discrete addressing. omit biases equations cell memory linear projection previous hidden states conditioning controller’s hidden state content read memory interpreted creating short-cut connections across time written time read help gradients across time. possible discrete addressing used read write operations. however main challenge model learn proper read write mechanisms write hidden states previous time steps useful future predictions read right time step. call reader/writer synchronization problem. instead designing complicated addressing mechanisms mitigate diﬃculty learning properly address external memory tardis side-steps reader/writer synchronization problem using following heuristics. ﬁrst time steps model writes micro-states cells memory sequential order. memory becomes full eﬀective strategy terms preserving information stored memory would replace memory cell read micro-state generated hidden state controller conditioned memory cell read. model needs perfectly retain memory cell overwritten controller principle learn copying read input write output pseudocode details memory update algorithm tardis presented algorithm addressing mechanism similar d-ntm memory matrix tardis disjoint address section rk×a content section rk×c rk×q however unlike d-ntm address vectors ﬁxed random sparse vectors. controller reads address content parts memory write content section memory. norm applied equation simple feature-wise computation centering divisive variance normalization. normalization step makes training easier usage vectors. introduction usage vector help attention mechanism choose diﬀerent memory cells based frequency accesses cell memory. example memory cell rarely accessed controller next time step learn assign weights memory cells looking usage vector. controller learn access mechanism further order prevent model learn deﬁcient addressing mechanisms e.g. reading memory cell increase memory capacity model decrease probability last read memory location subtracting logit forget gate input gate output gate respectively. scalar reset gates control magnitude information ﬂowing memory previous hidden states cell lstm controlling information lstm cell gates allow model store sub-sequences chunks sequences memory instead entire context. micro-state lstm particular time step summary information stored lstm controller model. attending cells memory contains previous micro-states lstm model explicitly learn restore information past. controller learn represent high-level temporal abstractions creating wormhole connections memory illustrated figure example model takes token ﬁrst timestep stores representation ﬁrst memory cell address second timestep controller takes input writes second memory cell address furthermore gater blocks connection third timestep controller starts reading. receives input figure time step controller takes memory cell read hidden state previous timestep ht−. then generates controls contribution internal dynamics controller’s state memory becomes full discrete addressing weights generated controller used read write memory. predict target model reads ﬁrst memory cell micro-state stored. reading computes hidden-state writes micro-state ﬁrst memory cell. length path passing microstates would wormhole connection would skip timestep. regular single-layer ﬁxed graphical representation linear-chain considering connections recurrent states temporal axis. however tardis ﬂexible terms learn directed graphs diverse structures using wormhole connections reset gates. directed graph tardis learn recurrent states degree vertex depends number cells stored memory. work focus variation tardis controller maintains ﬁxed-size external memory. however possible memory grows respect length input sequences would scale diﬃcult train discrete addressing. figure tardis’s controller learn represent dependencies among inputs tokens choosing cells read write creating wormhole connections. represents input controller timestep hidden state controller rnn. section explain train tardis language model. language modeling example application. however would like highlight tardis also applied complex sequence sequence learning tasks. consider training examples example sequence length every time-step model receives input one-hot vector size equal size vocabulary produce output also one-hot vector size equal size vocabulary learnable parameters single layer combines deep fusion task loss would categorical cross-entropy targets model-outputs. super-script denotes variable output sample training set. however discrete decisions taken memory access every time-step makes model diﬀerentiable hence need rely approximate methods computing gradients respect discrete address vectors. paper explore approaches reinforce straight-through estimator reinforce likelihood-ratio method provides convenient simple estimating gradients stochastic actions. paper focus application reinforce sequential prediction tasks language modelling. example timestep interested maximizing expected return whole episode deﬁned below ideally would like compute gradients equation however computing gradient expectation feasible. would monte-carlo approximation compute gradients using reinforce sequential prediction task written equation training reinforce auxiliary cost training models reinforce diﬃcult variance imposed gradients. recent years researchers developed several tricks order mitigate eﬀect high-variance gradients. proposed also variance normalization reinforce gradients. log-likelihood prediction timestep. initial experiments showed reinforce reward structue often tends under-utilize memory mainly rely internal memory lstm controller. especially beginning training model decrease loss relying memory controller cause reinforce increase log-likelihood random actions. training reinforce challenging high variance gradients gumbel-softmax provides good alternative straight-through estimator reinforce tackle variance issue. unlike instead annealing temperature ﬁxing model learns inverse-temperature single scalar output conditioned hidden state controller. neural turing machine related class architecture model. ntms proven successful terms generalizing longer sequences sequences trained also shown eﬀective terms solving algorithmic tasks gated models lstms. however limitations design choices. controller’s lack precise knowledge contents information contents memory overlap. memory augmented models also known complicated yields diﬃculties terms implementing model training controller information sequence operations information frequency read write access memory. tardis tries address issues. gulcehre proposed variant called dynamic learnable location based addressing. d-ntm used continuous addressing discrete addressing. discrete d-ntm related tardis sense models discrete addressing memory operations. however discrete d-ntm expects controller learn read/write also learn reader/writer synchronization. tardis synchronization problem since reader writer tied. proposed sparse access memory mechanism ntms seen hybrid continuous discrete addressing. uses continuous addressing selected top-k relevant memory cells. recently graves proposed diﬀerentiable neural computer successor ntm. rocktäschel proposed models generate weights attend previous hidden states rnn. however since models attend whole context computation attention ineﬃcient. grave proposed cache based memory representation stores last states memory similar traditional cache-based models model learns choose state memory prediction language modeling tasks section analyze gradients external memory also investigate eﬃciency terms dealing vanishing gradients problem first describe vanishing gradient problem describe external memory model deal sake simplicity focus vanilla rnns entire analysis analysis extended lstms. analysis also assume weights read/write heads discrete. consider mann contents memory linear projections previous hidden states described equation assume reading writing operation discrete addressing. content read memory time step correspond memory location rate gradients vanishing time depends length sequence passes rtt. typically lesser length sequence passing qtt. thus gradients vanish lesser rate rnn. particular rate would strictly depend length shortest paths long enough dependencies gradients longer paths would still vanish. rate reaches zero strictly smaller rate reaches zero ideal memory access reach zero. hence unlike vanilla rnns equation states upper bound norm jacobian reach zero mann ideal memory access. theorem consider memory augmented neural network memory cells sequence length hidden state controller stored diﬀerent cells memory. prediction time step long-term dependency prediction independent tokens appear memory reading mechanism perfect model suﬀer vanishing gradients back-propagate input sequence longest-dependency would proof interested gradients propagating jacobians i.e. controller learns perfect reading mechanism time step would read memory cell hidden state time step stored thus following jacobians deﬁned equation rewrite jacobians equation ﬁrst terms might vanish grows. however singular values third term change grows. result gradients propagated necessarily vanish time. however order obtain stable dynamics network initialization matrices important. analysis highlights fact external memory model optimal read/write mechanism handle long-range dependencies much better rnn. however applicable discrete addressing read/write operations. d-ntm still learn read write scratch challenging optimization problem. tardis tying read/write operations make learning become much simpler model. particular results theorem points importance coming better ways designing attention mechanisms memory. controller mann able learn memory eﬃciently. example cells memory remain empty never read. controller overwrite memory cells read. result information stored overwritten memory cells lost completely. however tardis avoids issues construction algorithm. analyse lengths depth untrained models model assign uniform probability read write memory cells. give better idea untrained model uses memory beginning training. wormhole connection created reading memory cell writing cell tardis. example figure actual path length memory cell creates shorter path length call length actual path length shorter path created wormhole connection tmem. consider tardis model cells memory. tardis access memory cell uniformly random probability accessing random cell expected length shorter path created wormhole connections would proportional number reads writes memory cell. tardis reader sequence. verify result simulating read write heads tardis figure figure ﬁgures visualized expected path length memory cells sequence length memory size simulations. shows results tardis shows simulation mann uniformly random read write heads. consider mann separate read write heads accessing memory discrete uniformly random fashion. call umann. compute expected length shorter path created wormhole connections umann. read write head weights sampled multinomial distribution uniform probability memory cells respectively. index memory cell read timestep memory cell deﬁned below recursive function computes length path created wormhole connections cell. analysis shows tardis uniform read head maintains expected length shorter path created wormhole connections umann completely avoids reader/writer synchronization problem. expectation decay proportionally tmem whereas decay proportional ideal memory access rate reaches zero would strictly smaller rate reaches zero. hence equation upper bound norm jacobian vanish much smaller rate. however result assumes dependencies prediction relies accessible memory cell read controller. figure assuming prediction depends wormhole connection shorten path creating connection wormhole connection directly create connection create shorter paths gradients without vanishing. ﬁgure consider case wormhole connection created connections skips tokens general case consider mann writer ﬁlls memory cells sequential manner reader chooses memory cell uniformly random. call model urmann. assume dependency timesteps shown figure taken uniformly probability read address invoked time greater equal case expected shortest path length wormhole connection would still would scale well. reader well trained could pick exactly path length consider paths length less equal form figure also then shortest path length using wormhole connection connects state state paths realized leave distribution length shortest path open question. however probability hitting short path increases exponentially probability read interval probability figure illustrates wormhole connections creater shorter paths. figure show expected length path travelled outside wormhole connections obtained simulations decreases size memory decreases. particular urmann tardis trend close exponential. shown figure also inﬂuences total length paths travelled timestep well. writing memory using weights sampled uniform probability memory cells memory eﬃciently approaches compare particular ﬁxing writing mechanism seems useful. figure simulations tardis mann uniform read write mechanisms mann uniform read write head ﬁxed heuristic simulations assume dependency timestep simulations diﬀerent memory sizes model. plot show results expected length shortest path timestep plots size memory gets larger models length shortest path decreases dramatically. plot show expected length shortest path travelled outside wormhole connections respect diﬀerent memory sizes. tardis seems memory eﬃciently compared models particular size memory small creating shorter paths. graves shown lstms generalize well sequences longer ones seen training. whereas mann d-ntm shown generalize sequences longer ones seen training tasks. state lstm network utilizes unbounded history input sequence result parameters optimized using maximum likelihood criterion sequences lengths training examples. however n-gram language model suﬀer issue. comparison n-gram would input context ﬁxed window size markov property latent space. argued below claim trained mann also learn ability generalize sequences longer length ones appear training modifying contents memory reading regular minimize negative log-likelihood objective function targets using unbounded history represented hidden state model parametrized conditional distribution prediction timestep mann would learn assume represents dependencies depends input sequence represents dependencies limited context window contains paths shorter sequences seen training set. property claim manns d-ntm tardis generalize longer sequences easily. experiments penntreebank show tardis language model trained minimize log-likelihood test model yields close results. hand fact best results babi dataset obtained feedforward controller similarly feedforward controller used solve tasks also conﬁrms hypothesis. result written memory read becomes important able generalize longer sequences. preliminary study performance model consider character-level language modelling. evaluated models penn treebank corpus based train valid test used task using layer-normalization recurrent dropout also used sota results task. using layer-normalization recurrent dropout improves performance signiﬁcantly reduces eﬀects overﬁtting. train models adam sequences length show results table addition regular char-lm experiments order conﬁrm hypothesis regarding ability manns generalizing sequences longer ones seen training. trained language model learns using softmax layer described equation however measure performance test used softmax layer gets auxiliary cost deﬁned reinforce equation model trained reinforce auxiliary cost. table model’s performance using however using becomes small enough conﬁrm assumption table character-level language modelling results penn treebank dataset. tardis gumbel softmax straight-through estimator performs better reinforce performs competitively compared sota task. notiﬁes reset gates subsection introduce pen-stroke based sequential multi-digit mnist prediction task benchmark long term dependency modelling. also benchmark performance lstm tardis challenging task. recently introduced mnist stroke classiﬁcation task also provided dataset consisted stroke sequences representing skeleton digits mnist dataset. mnist digit image represented sequence quadruples {dxi eosi eodi}t number strokes deﬁne digit denotes oﬀset previous current stroke eosi binary valued feature denote stroke eodi another binary valued feature denote digit. original dataset ﬁrst quadruple contains absolute value instead oﬀsets without loss generality starting position experiments. digit represented strokes average task predict digit stroke sequence. dataset proposed incremental sequence learning consider multi-digit version dataset benchmark models handle long term dependencies. speciﬁcally given sequence pen-stroke sequences task predict sequence digits corresponding pen-stroke sequences given order. challenging task since requires model learn predict digit based pen-stroke sequence count number digits remember generate order seeing strokes. experiments consider versions task digit sequences respectively. generated training data points randomly sampling digits training mnist dataset. similarly generated validation test data points randomly sampling digits validation test mnist dataset respectively. average length stroke sequences tasks respectively. figure illustration sequential mnist strokes task multiple digits. network ﬁrst provided sequence strokes information mnist digits input prediction network tries predict mnist digits seen. model tries predict predictions previous time steps back network. ﬁrst time step model receives special <bos> token model ﬁrst time step prediction starts. benchmark performance lstm tardis task. models receive sequence strokes sequence expected generate sequence digits followed particular <bos> token. tasks illustrated figure evaluate models based per-digit error rate. also compare performance tardis reinforce tardis gumbel softmax. models trained number updates early stopping based per-digit error rate validation set. results versions task reported table-. table tardis performs better lstm three versions task. also tardis gumbel-softmax performs slightly better tardis reinforce consistent experiments. graves proposed associative recall copy tasks evaluate model’s ability learn simple algorithms generalize sequences longer ones seen training. trained tardis model features address features memory content part model. used model hidden state size model uses memory size train model adam used learning rate show results model table tardis model able solve tasks gumbel-softmax reinforce. bowman proposed task test machine learning algorithms’ ability infer whether given sentences entail contradict neutral other. however task considered long-term dependency task premise hypothesis presented model sequential order also explored rocktäschel model learn dependency relationship hypothesis premise. model ﬁrst reads premise hypothesis hypothesis model predicts whether premise table table consider model successful copy associative recall validation cost lower sequences maximum length seen training. threshold determine whether model successful task hypothesis contradicts entails. model proposed rocktäschel applies attention previous hidden states premise reads hypothesis. sense model still considered task-speciﬁc architectural design choice. tardis baseline lstm models include task-speciﬁc architectural design choices. table compare results diﬀerent models. model performs signiﬁcantly better models. however recently shown architectural tweaks possible design model speciﬁcally solve task achieve test accuracy paper propose simple eﬃcient memory augmented neural network model perform well algorithmic tasks realistic tasks. unlike previous approaches show better performance real-world tasks language modelling snli. also proposed task measure performance models dealing long-term dependencies. provide detailed analysis eﬀects using external memory gradients justify reason manns generalize better sequences longer ones seen training set. also shown gradients vanish much slower rate external memory used. theoretical results encourage studies direction developing better attention mechanisms create wormhole connections eﬃciently. thank chinnadhurai sankar suggesting phrase \"wormhole connections\" proof-reading paper. would like thank dzmitry bahdanau comments feedback earlier version paper. would like also thank developers theano developing powerful tool scientiﬁc computing theano development team acknowledge support following organizations research funding computing support nserc samsung calcul québec compute canada canada research chairs cifar. supported fqrnt-pbeee scholarship.", "year": 2017}