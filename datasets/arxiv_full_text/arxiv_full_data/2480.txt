{"title": "Efficient Reinforcement Learning in Deterministic Systems with Value  Function Generalization", "tag": ["cs.LG", "cs.AI", "cs.SY", "stat.ML"], "abstract": "We consider the problem of reinforcement learning over episodes of a finite-horizon deterministic system and as a solution propose optimistic constraint propagation (OCP), an algorithm designed to synthesize efficient exploration and value function generalization. We establish that when the true value function lies within a given hypothesis class, OCP selects optimal actions over all but at most K episodes, where K is the eluder dimension of the given hypothesis class. We establish further efficiency and asymptotic performance guarantees that apply even if the true value function does not lie in the given hypothesis class, for the special case where the hypothesis class is the span of pre-specified indicator functions over disjoint sets. We also discuss the computational complexity of OCP and present computational results involving two illustrative examples.", "text": "consider problem reinforcement learning episodes ﬁnite-horizon deterministic system solution propose optimistic constraint propagation algorithm designed synthesize eﬃcient exploration value function generalization. establish true value function lies within known hypothesis class selects optimal actions dime episodes dime apply even special case span pre-speciﬁed indicator functions introduction growing body work eﬃcient reinforcement learning provides algorithms guarantees sample computational eﬃciency references therein). literature highlights point eﬀective exploration scheme critical design eﬃcient reinforcement learning algorithm. particular popular exploration schemes \u0001-greedy boltzmann knowledge gradient require learning times grow exponentially number states and/or planning horizon aforementioned literature focusses tabula rasa learning; algorithms learn little prior knowledge transition probabilities rewards. algorithms require learning times grow least linearly number states. despite valuable insights generated design analysis algorithms limited practical import state spaces contexts practical interest enormous. need algorithms generalize past experience order learn make eﬀective decisions reasonable time. much work reinforcement learning algorithms generalize references therein). algorithms come statistical computational eﬃciency guarantees though noteworthy exceptions discuss. number results treat policy-based algorithms references therein) goal select high-performers among pre-speciﬁed collection policies learning progresses. though interesting results produced line work entails quite restrictive assumptions make strong guarantees. another body work focuses model-based algorithms. algorithm proposed kearns koller factored model observed data makes decisions based ﬁtted model. authors establish sample complexity bound polynomial number model parameters rather number states algorithm computationally intractable diﬃculty solving factored dps. lattimore propose novel algorithm case true environment known belong ﬁnite compact class models shows sample complexity polynomial cardinality model class model class ﬁnite \u0001-covering-number model class compact. though result theoretically interesting model classes interest \u0001-covering-number enormous since typically grows exponentially number free parameters. ortner ryabko establish regret bound algorithm applies problems continuous state spaces h¨older-continuous rewards transition kernels. though results represent interesting contribution literature couple features regret bound weaken practical implications. first regret grows linearly h¨older constant transition kernel contexts practical relevance grows exponentially number state variables. second dependence time becomes arbitrarily close linear dimension state space grows. pazis parr also consider problems continuous state spaces. assume q-functions lipschitz-continuous h¨older-continuous establish sample complexity bound. though results interesting signiﬁcant sample complexity bound log-linear covering number state-action space also typically grows exponentially number free parameters practical problems. reinforcement learning linear systems quadratic cost treated abbasi-yadkori szepesv´ari method proposed shown realize regret grows square root time. result interesting property desirable best knowledge expressions derived regret analysis exhibit exponential dependence number state variables further aware computationally eﬃcient implementing proposed method. work extended ibrahimi address linear systems sparse structure. here eﬃciency guarantees scale gracefully number state variables sparsity technical assumptions. popular approach generalization applied reinforcement learning literature involves ﬁtting parameterized value functions. approaches relate closely supervised learning learn functions state-action pairs value though diﬀerence value inﬂuenced action observed delayed feedback. advantage model learning approaches that given ﬁtted value function decisions made without solving often intractable control problem. promising direction though currently lack theoretical results provide attractive bounds learning time value function generalization. relevant paper along lines studies eﬃcient reinforcement learning value function generalization kwik framework reduces problem eﬃcient kwik online regression. however authors show solve general kwik online regression problem eﬃciently even clear whether possible. thus though result littman interesting provide provably eﬃcient algorithm general reinforcement learning problems. however worth mentioning provided solution kwik online regression deterministic linear functions. discuss later seen special case coherent learning problems consider section important challenge remains couple exploration value function generalization provably eﬀective particular establish sample computational eﬃciency guarantees scale gracefully planning horizon model complexity. paper make progress direction. start simple context restrict attention deterministic systems evolve ﬁnite time horizons consider episodic learning agent repeatedly interacts system. solution problem propose optimistic constraint propagation computationally eﬃcient reinforcement learning algorithm designed synthesize eﬃcient exploration value function generalization. estabquantiﬁes complexity hypothesis class. corollary result regret bounded function constant time linear problem horizon eluder dimension. aforementioned result perspective useful relate lines work. consider ﬁrst broad area reinforcement learning algorithms value functions linear combinations ﬁxed basis functions even hypothesis class contains true value function guarantees algorithms eﬃciently learn make near-optimal decisions. hand result implies attains near-optimal performance time scales linearly number basis functions. consider specialized context deterministic linear system quadratic cost ﬁnite time horizon. analysis abbasi-yadkori szepesv´ari leveraged produce regret bounds scale exponentially number state variables. hand using hypothesis space literature agnostic reinforcement learning hypothesis class prior work area produced interesting algorithms insights well bounds performance loss associated potential limits convergence convergence eﬃciency guarantees. results build reported earlier paper published proceedings conference addition establishing theoretical results present computational results involving illustrative examples synthetic deterministic markov chain inverted pendulum control problem considered lagoudakis compare least-squares value iteration classical reinforcement learning algorithm. experiments performance orders magnitude better lsvi. worth mentioning inverted pendulum example consider case small stochastic disturbances additive control. result shows that though designed deterministic systems might also work well stochastic environments especially magnitude stochastic disturbances small. finally worth pointing reinforcement learning algorithms often used approximate solutions large-scale dynamic programs system models known. known mean that given suﬃcient compute power determine expected single-period rewards transition probabilities desired level accuracy absence additional empirical data. contexts need statistical learning challenges purely computational. nevertheless reinforcement learning algorithms make popular solution techniques problems algorithm results also serve contributions ﬁeld approximate dynamic programming. speciﬁcally prior approximate dynamic programming algorithms linear combination basis functions value function even optimal value function within span come guarantees near-optimal policy computed eﬃciently. paper establish guarantee ocp. episodic reinforcement learning deterministic systems consider class reinforcement learning problems agent repeatedly interacts unknown discretetime deterministic ﬁnite-horizon markov decision process interaction referred episode agent’s objective maximize expected cumulative reward episodes. system identiﬁed sextuple state space states. action selected system state period reward realized; furthermore state transitions episode terminates period episode begins. initial state episode represent history actions observations multiple episodes often index variables episode period. example denote state action period episode count total number steps policy sequence functions mapping policy deﬁne value function optimal value function deﬁned policy throughout paper restrict attention systems said optimal admit optimal policies. note restriction incurs loss generality value function know anything else system function reward function sequence initial states reinforcement learning algorithm generates action based observations made period episode including states actions rewards observed previous episodes earlier current episode well note episode. quantify performance reinforcement learning algorithm deﬁne \u0001-suboptimal sample complexity algorithm moreover reinforcement learning number episodes algorithm sample eﬃcient given setting reasonable choice worst-case \u0001-suboptimal sample complexity algorithm small setting. note reward ineﬃcient exploration schemes proceeding worth pointing reinforcement learning problem proposed above number popular exploration schemes give rise exponentially large regret. even tabula rasa case boltzmann \u0001-greedy exploration schemes example lead worst-case regret exponential and/or |s|. also notice paper assume state transition model deterministic system unknown. literature references therein) consider settings state transition model known reward function unknown establish exploration schemes similar boltzmann exploration achieve regret example consider deterministic system illustrated figure node represents state arrow corresponds possible state transition. state space state transitions hand agent takes action state state transitions state absorbing. assume reward realized upon transition node reward realized upon transition node take horizon equal action consecutive time periods. starting special knowledge system default estimates period-state-action value boltzmann \u0001-greedy discover reward opportunity random wandering requires |s|− episodes expectation. translates lower bound expected regret dramatically reduce regret optimism. particular learning agent begins initial estimate period-state-action value incentivizes selection actions tried reduce dependence regret linear situation becomes complex however agent generalizes across period states and/or actions. generalization means altering value estimate period-state-action triple based observations made others. incorrect generalization turn optimistic estimate pessimistic one. algorithm generalizes manner prevents happening. establish retaining optimism guarantees regret. constraint propagation takes input state space action space horizon hypothesis class candidates algorithm maintains sequence subsets sequence scalar upper bounds summarize constraints past experience suggests ruling hypotheses. constraint sequence speciﬁed state action period interval takes form upper bound constraint given sequence constraints upper bounds s.t. constraint higher priority priority since boltzmann exploration \u0001-greedy exploration randomized exploration schemes measure performance lsvi boltzmann/\u0001-greedy exploration expected regret. symbol regret expected regret since regret deﬁned paper viewed special case expected regret. prove lemma constraint appended rule therefore sequence sets generated algorithm progresses decreasing contains intersection. agnostic case constraints finite state/action tabula rasa case. ﬁnite state action spaces represented vector without special prior knowledge natural |s|·|a|·h. polytopic prior constraints. consider aforementioned example suppose prior knowledge lies particular polytope. polytope linear systems quadratic cost classical control model known positive denoting positive finite hypothesis class. consider context prior knowledge well approximated element ﬁnite hypothesis class. ﬁnite hypothesis class apply ocp. scenario particular interest perspective learning theory. note context entails agnostic learning accommodated ocp. linear combination features. often eﬀective hand-select features mapping aiming compute weights lies span span. note features. apply here would state aggregation. special case linear combination features case discussed above. speciﬁcally state-action space period partitioned disjoint subsets ztzt··· ztkt choose features indicator functions partition ztk’s. sigmoid. known rewards received upon transitioning terminal state could sparse linear combination features. another case potential interest encoded sparse linear combination large number features φ··· particular suppose |s||a|×k l-norm worth mentioning deﬁned assumes action maximizing supq∈qc exists iteration. note assumption always holds action space ﬁnite diﬃcult modify algorithm addresses cases finally compare classical reinforcement learning algorithms. worth mentioning ﬁnite state/action tabula rasa case equivalent q-learning algo. sample eﬃciency optimistic constraint propagation establish results concerning sample eﬃciency ocp. results bound \u0001-suboptimal sample complexities appropriate choices obviously sample complexity bounds must depend complexity hypothesis class. such begin deﬁning eluder dimension introduced russo notion hypothesis class complexity use. functions equal equal further said independent respect dependent respect eluder dimension dime length longest sequence elements every element independent predecessors. note dime zero inﬁnity straightforward show dime dime. based results finite state/action tabula rasa case. |s|·|a|·h dime |s|·|a|· polytopic prior constraints. polytope dimension |s|·|a|·h dime linear systems quadratic cost positive semideﬁnite quadratics finite hypothesis space. dime |q|− linear combination features. theorem bounds -suboptimal sample complexity coherent learning cases. theorem follows exploration-exploitation lemma asserts episode either delivers optimal reward introduces constraint reduces eluder dimension hypothesis class consequently experience coherent hypothesis class ﬁnite eluder dimension. finite state/action tabula rasa case. apply case deliver suboptimal performance episodes. furthermore |rt| regret r|s||a|h polytopic prior constraints. apply case deliver sub-optimal performance episodes. furthermore |rt| regret rhd. linear systems quadratic cost apply case deliver finite hypothesis class case. assume agent prior knowledge ﬁnite hypothesis class. apply case deliver sub-optimal performance episodes. furthermore |rt| regret span. episodes. furthermore |rt| regret notice proceeding worth pointing feature distinguishes reinforcement learning algorithms makes sample eﬃcient presented coherent hypothesis class updates feasible candidates conservative manner never rues always uses optimistic estimates feasible guide action. constraints conﬂicting since satisﬁes constraints. episode deﬁne note deﬁnition episode sequence elements null furthermore element independent predecessors. moreover last period episode s.t. independent respect show analysis ﬁrst period episode value state-action-period triple learned perfectly. based notions notations. first state space time horizon action space hypothesis class denote ﬁnite-horizon deterministic system state space action space horizon notice reinforcement learning algorithm takes input knows coherent hypothesis class ﬁnite-horizon deterministic systems adaptively chooses deterministic system speciﬁcally assume beginning episode adversary adaptively chooses initial state period episode agent ﬁrst chooses action based algorithm adversary adaptively chooses state-action-time triples speciﬁes rewards state transitions subject constraints lemma exist state space action space hypothesis class dime matter agent adaptively chooses actions adversary adaptively choose |rt| agent achieve sub-optimal rewards least episodes supt regret rhk. proof lemma provide constructive proof lemma speciﬁcally construct state space action space recall thus constructed above hence optimal q-function represented vector specifying hypothesis class ﬁrst deﬁne matrix kh×k follows. denote corresponding state-action-time triple indicator vector index zeros everywhere else. obviously rank choose span thus dime rank beginning episode adversary chooses initial state episode etc. episode adversary adaptively chooses reward function follows. agent takes action period episode initial state adversary otherwise adversary notice adversary completes construction deterministic system episode note constructed deterministic system speciﬁcally straight thus constructed deterministic finally show constructed deterministic system satisﬁes lemma obviously |rt| furthermore note agent achieves sub-optimal rewards cumulative regret ﬁrst episodes thus supt regret khr. also applied agnostic learning cases cases performance depend complexity also distance subsection present results applied special agnostic learning case span pre-speciﬁed indicator functions disjoint subsets. speciﬁcally assume state-action space period partitioned disjoint subsets ztzt··· ztkt denote indicator function partition theorem bounds ρh-suboptimal sample complexity state aggregation case. similar theorem theorem also follows exploration-exploitation lemma asserts episode either delivers near-optimal reward approximately determines disjoint subset outline proof theorem subsection detailed analysis provided appendix. immediate corollary bounds regret. modiﬁed episode. furthermore system ﬁxed initial state condition proposition holds episode hold subsequent episodes consequently performance losses subsequent episodes worth mentioning sample complexity bound regret bounds subsection derived assumption partitions state-action spaces given. important problem practice choose optimal number state-action partitions. many approaches choose approach formulate regret constraint selection algorithm updates qc’s function class speciﬁed eqn. speciﬁcally denote coeﬃcient indicator function assume belongs partition then speciﬁed constraint equivalent induction straightforward episode represented computational eﬃciency optimistic constraint propagation brieﬂy discuss computational complexity ocp. typical complexity analysis optimization algorithms assume basic operations include arithmetic operations comparisons assignment measure computational complexity terms number basic operations period. first worth pointing general hypothesis class general action space computing supq∈qc requires solving possibly intractable optimization selecting action maximizes supq∈qc intractable. further number constraints number operations period grow however tractably small special structures represented weight vector characterized linear inequalities furthermore constraints form also linear inequalities hence episode characterized polyhedron supq∈qc computed solving linear programming problems. assume observed numerical value encoded bits solved karmarkar’s algorithm following proposition bounds computational complexity. construct constraint selection algorithm. requires sorting constraints comparing upper bounds positions sequence operations) checking whether times. note checking whether requires solving feasibility problem variables constraints. choose action ajt. note supq∈qc computed solving variables constraints thus derived solving lps. compute constraint ujt. note computed solving variables constraints computed solving variables constraints. assume observed numerical value encoded bits karmarkar’s algorithm solve variables constraints number bits input karmarkar’s algorithm hence requires operations solve thus computational complexities ﬁrst second third steps respectively. hence computational complexity |c|d.b) operations period. q.e.d. computationally eﬃcient parameters tractably small. note bound proposition worst-case bound term incurred need solve lps. special cases computational complexity much less. instance state aggregation case computational complexity operations period. constraint selection algorithm subset available constraints. instance coherent learning case discussed section constraint selection algorithm chooses constraints lead strict reduction eluder algorithms performance bounds derived section still hold. finally general agnostic learning case naive approach maintain time window constraints section present computational results involving illustrative examples system presented example inverted pendulum problem considered lagoudakis compare least-squares value iteration classical reinforcement learning algorithm. feature mapping choose span consider coherent learning case notice lsvi boltzmann/\u0001-greedy exploration applied problem node ﬁrst visited. estimates period-state-action value thus discussed section expectation take agent |s|− episodes ﬁrst reach node moreover lower bounds regret speciﬁed equation hold computational experiment choose simulate time period vary given construct features follows period choose vector ones sample φt··· i.i.d. gaussian distribution notice ensures coherent hypothesis class. apply example above-speciﬁed gains lsvi boltzmann/\u0001-greedy exploration problem. hand figure shows regret scales linearly results also indicate upper bound derived theorem tight problem small. inverted pendulum show signiﬁcantly outperforms lsvi \u0001-greedy exploration reinforcement learning formulation inverted pendulum problem. system dynamics inverted pendulum cart described equation wang angular position pendulum vertical angular velocity .m/s gravity constant mass pendulum mass cart length pendulum force applied cart note respectively derivatives respect time. similarly lagoudakis simulate nonlinear system step size action space actual input system noisy. speciﬁcally action selected actual input system random variable independently drawn uniform distribution unif initial state system also independently drawn unif. notice dynamic system deterministic consider reinforcement learning setting agent learns control inverted pendulum fall hour repeatedly interacting episodes. reward episode length time inverted pendulum falls capped hour. also assume agent know system dynamics reward function. apply lsvi form state aggregation problem. particular state space problem grid angular position space uniformly intervals; grid angular velocity space compare lsvi \u0001-greedy exploration purely randomized policy. notice though lsvi signiﬁcantly outperforms purely randomized policy performance unsatisfactory since cases cumulative reward episodes less hours indicating ﬁrst episodes average time length pendulum falls less seconds. figure compare best lsvi observe cases performance orders magnitude better lsvi. also note performances lsvi worse case finally would like emphasize system dynamics stochastic case however magnitude stochastic disturbances small relative magnitude control thus though motivated developed framework reinforcement learning deterministic systems might also perform well reinforcement learning problems stochastic environments especially magnitude stochastic disturbances small. conclusion proposed novel reinforcement learning algorithm called optimistic constraint propagation synthesizes eﬃcient exploration value function generalization episodic reinforcement learning deterministic systems. shown true value function lies given hypothesis class selects optimal actions dime episodes dime eluder dimension also established sample eﬃciency asymptotic performance guarantees state aggregation case special agnostic learning case span pre-speciﬁed indicator functions disjoint sets. also discussed computational complexity presented computational results involving illustrative examples. results demonstrate dramatic eﬃciency gains enjoyed relative lsvi boltzmann \u0001-greedy exploration. finally brieﬂy discuss possible directions future research. possible direction propose variant reinforcement learning inﬁnite-horizon discounted deterministic systems. note inﬁnite-horizon discounted problem bounded rewards eﬀective horizon eﬀective horizon similar sample complexity/regret bounds derived inﬁnite-horizon discounted problems. another possible direction design provably sample eﬃcient algorithms general agnostic learning case discussed paper. important problem design eﬃcient algorithms reinforcement learning mdps. though many provably eﬃcient algorithms proposed tabula rasa case problem references therein) however design algorithms value function generalization currently still open. thus interesting direction future research extend variant problem. useful terminologies notations. index i··· denote subvector associated sequence vectors θ··· linearly l-independent predecessors denote deﬁne rank l-rank independent respect exist s.t. based deﬁnition function space exist k-sparse vectors s.t. thus independent respect exist k-sparse vectors s.t. proposition min{n l-full-rank rank proof consider sequence matrix rows φ··· length index since l-full-rank thus linearly independent thus linearly dependent since result holds thus linearly l-dependent φ··· rank hand since l-full-rank choose sequence matrix rows φ··· length index linearly independent. thus φ··· sequence matrix rows s.t. every element linearly l-independent predecessors. thus rank rank q.e.d. thus min{n k-full-rank dime rank consequently dime dime last equality follows deﬁnition part lemma episode speciﬁcally since last period episode s.t. independent respect thus dependent respect lemma episode q∈qcj last equality follows fact notice deﬁnition sequence elements every element independent predecessors respect hence deﬁnition eluder dimension |zj| implies note combining inequality bellman equation summing inequalities conditions proposition hold. note prove conditions proposition imply note deﬁnition ujh− ljh− function function transits transits inﬁnity might also transit ﬁnite values episode transiting ﬁnite values partition transition occur null] q.e.d. azar mohammad gheshlaghi alessandro lazaric emma brunskill. regret bounds reinforcement learning policy advice. machine learning knowledge discovery databases. springer bartlett peter ambuj tewari. regal regularization based algorithm reinforcement learning weakly communicating mdps. proceedings conference uncertainty artiﬁcial intelligence gergely andr´as gy¨orgy csaba szepesv´ari. adversarial stochastic shortest path problem unknown transition probabilities. international conference artiﬁcial intelligence statistics. ryzhov ilya warren powell. approximate dynamic programming correlated bayesian beliefs. communication control computing annual allerton conference ieee strehl lihong eric wiewiora john langford michael littman. model-free reinforcement learning. proceedings international conference machine learning.", "year": 2013}