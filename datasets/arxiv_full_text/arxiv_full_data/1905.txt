{"title": "Grounding of Textual Phrases in Images by Reconstruction", "tag": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "Grounding (i.e. localizing) arbitrary, free-form textual phrases in visual content is a challenging problem with many applications for human-computer interaction and image-text reference resolution. Few datasets provide the ground truth spatial localization of phrases, thus it is desirable to learn from data with no or little grounding supervision. We propose a novel approach which learns grounding by reconstructing a given phrase using an attention mechanism, which can be either latent or optimized directly. During training our approach encodes the phrase using a recurrent network language model and then learns to attend to the relevant image region in order to reconstruct the input phrase. At test time, the correct attention, i.e., the grounding, is evaluated. If grounding supervision is available it can be directly applied via a loss over the attention mechanism. We demonstrate the effectiveness of our approach on the Flickr 30k Entities and ReferItGame datasets with different levels of supervision, ranging from no supervision over partial supervision to full supervision. Our supervised variant improves by a large margin over the state-of-the-art on both datasets.", "text": "abstract. grounding arbitrary free-form textual phrases visual content challenging problem many applications human-computer interaction image-text reference resolution. datasets provide ground truth spatial localization phrases thus desirable learn data little grounding supervision. propose novel approach learns grounding reconstructing given phrase using attention mechanism either latent optimized directly. training approach encodes phrase using recurrent network language model learns attend relevant image region order reconstruct input phrase. test time correct attention i.e. grounding evaluated. grounding supervision available directly applied loss attention mechanism. demonstrate eﬀectiveness approach flickr entities referitgame datasets diﬀerent levels supervision ranging supervision partial supervision full supervision. supervised variant improves large margin state-of-the-art datasets. language grounding visual data interesting problem studied computer vision natural language processing communities. grounding done diﬀerent levels granularity coarse e.g. associating paragraph text scene movie e.g. localizing word phrase given image work focus latter scenario. many prior eﬀorts area focused rather constrained settings small number nouns ground contrary want tackle problem grounding arbitrary natural language phrases images. parallel corpora sentence/visual data provide localization annotations annotation process costly. propose approach learn localize phrases relying phrases associated images without bounding annotations fig. without bounding annotations training time approach grounder ground free-form natural language phrases images. training latent attention approach reconstructs phrases learning attend correct box. test time attention model infers grounding phrase. semi-supervised fully supervised variants fig. main idea approach shown fig. ﬁrst consider scenario localization supervision available. given images paired natural language phrases want localize phrases bounding image propose model learns attend bounding proposal based selected bounding reconstructs phrase. second part model able predict correct phrase ﬁrst part model attended correctly learned without additional bounding supervision. method based ground reconstruction loss hence named grounder. additional supervision integrated model adding loss function directly penalizes incorrect attention reconstruction step. test time evaluate whether model attends correct bounding box. propose novel approach grounding textual phrases images operate supervision modes grounding annotations available. evaluate grounder approach flickr entities referitgame datasets show unsupervised variant better prior work supervised approach signiﬁcantly outperforms state-of-theart datasets. interestingly semi-supervised approach eﬀectively exploit small amounts labeled data surpasses supervised variant exploiting multiple losses. grounding natural language images video. grounding language images approach based markov random field aligns cuboids words. however limited nouns object classes relevant indoor scenes. uses conditional random field ground speciﬁcally designed scene graph query image. grounds dependencytree relations image regions using multiple instance learning ranking objective. simpliﬁes objective maximum score replaces dependency tree learned recurrent network. works evaluated grounding discuss quantitative comparison section recently presented dataset flickr entities augments flickrk dataset bounding boxes noun phrases present textual descriptions. report localization performance proposed embedding approach. proposes deep structure-preserving embedding image-sentence retrieval also applies phrase localization formulated ranking problem. spatial context recurrent convnet approach caption generation framework score phrase proposal boxes select highest probability. advantage approach applicability unsemi-supervised training regimes. believe approach encoding phrase optimizes better objective grounding scoring phrase text generation pipeline fully-supervised regime empirically show advantage attempts localize relation phases type subject-verb-object large scale order verify correctness relying detectors language grounding limited small nouns. object co-localization focuses discovering detecting object images videos without bounding annotation image/video level labels works similar respect amount supervision focus discrete classes approach handle arbitrary phrases allows localization novel phrases. also works propose train detectors wide range concepts using image-level annotated data image search e.g. approaches complementary sense obtaining large scale concept detectors little supervision however tackle complex phrases e.g. blond left focus work. attention vision tasks. recently diﬀerent attention mechanisms applied range computer vision tasks. general idea given visual input e.g. features given moment might want focus part e.g. attend speciﬁc subset features integrates spatial attention image captioning pipeline. consider variants soft hard attention meaning latter case model allowed pick single location ﬁrst attention weights distributed multiple locations. adapts soft-attention mechanism attends bounding proposals word time generating image captioning. relies similar mechanism perform temporal attention selecting frames video description task. uses attention mechanism densely label actions video sequence. approach relies soft-attention mechanism similar apply language grounding task attention helps select bounding proposal given phrase. bi-directional mapping. model phrase ﬁrst mapped image region attention image region mapped back phrase reconstruction. conceptual similarity previous work idea bi-directional mapping domain another. autoencoders input data ﬁrst mapped compressed vector encoding reconstructed decoding. uses bi-directional mapping visual features words words visual features recurrent neural network model. idea generate descriptions visual features reconstruct visual features given description. similar model also learn associate input text visual features attending image region rather reconstructing directly words. linguistic community proposed autoencoder generates latent structures given language input reconstructs input latent structures application e.g. part-of-speech tagging. goal approach ground natural language phrases images. speciﬁcally ground phrase image means region image corresponds phrase. subset e.g. segment bounding box. core insight method bidirectional correspondence image region phrase describing correct grounding textual phrase result image region human would describe using phrase i.e. possible reconstruct phrase based grounded image region. thus idea approach learn ground phrase reconstructing phrase automatically localized region. fig. gives overview approach. works parts ﬁrst part aims attend relevant region based phrase second part tries reconstruct phrase region attended ﬁrst phase. therefore training reconstruct text phrase model learns ﬁrst ground phrase image generate phrase region. fig. visualizes network structure. test time remove phrase reconstruction part ﬁrst part phrase grounding. described pipeline extended accommodate partial supervision i.e. ground-truth phrase localization. integrate additional loss model directly optimizes correct attention prediction fig. fig. model learns grounding textual phrases images little full supervision localization grounding part reconstruction part. training model distributes attention single several boxes learns reconstruct input phrase based boxes attends test time grounding part used. following present details parts approach learning attend correct region given phrase learning reconstruct phrase attended region. simplicity without loss generality refer single bounding box. following describe details model attention attention mechanism used model inspired similar soft attention formulations however inputs attention predictor single words rather multi-word phrases consequently also doubly stochastic attention used normalize attention across words. phrases dealing might complex thus require good language model represent them. choose long short-term memory network phrase encoder shown eﬀective various language modeling tasks e.g. translation encode query phrase word word lstm obtain representation phrase using hidden state ﬁnal time step number phrases batch. loss activates training sample ground-truth attention value otherwise zero. ground truth annotations deﬁne loss function learn parameters weakly supervised manner. next section describe deﬁne loss aiming reconstruct phrase based boxes attended test time calculate value selected ground truth rˆj. idea phrase reconstruction model learn reconstruct phrase attended boxes. given attention distribution boxes compute weighted visual features attention weights att) distribution phrases conditioned input visual feature. approach phrase generation inspired effectively used lstm generating image descriptions based visual features. given visual feature learns predict word sequence {wt}. time step model predicts distribution next word conditioned input visual feature previous words. single lstm layer feed visual input ﬁrst time step. lstm phrase encoder well decoder. although could potentially approaches phrases lower dimensional semantic space clear would reconstruction without recurrent network given train encoding decoding end-to-end. importantly entire grounding+reconstruction model trained single deep network back-propagation maximizing likelihood ground truth phrase generated reconstruction deﬁne training loss batch size ﬁrst discuss experimental setup design choices implementation present quantitative results test sets flickr entities referitgame datasets. best results outperform state-of-the-art datasets signiﬁcant margin. figures show qualitatively well ground phrases images. evaluate grounder datasets flickr entities referitgame flickr entities contains bounding boxes images associated natural language phrases. phrases dataset correspond multiple boxes e.g. men. consistency cases consider union boxes ground truth. images validation testing training. referitgame dataset contains regions images. regions associated natural language expressions constructed disambiguate described objects. bounding boxes provided test split namely images testing; rest split training validation images. obtain bounding proposals image using selective search flickr entities edge boxes referitgame dataset. semi-supervised fully supervised models obtain ground-truth attention selecting proposal overlaps groundtruth overlap thus fully supervised model trained available training phrase-box pairs proposal boxes exist. flickr entities visual representation rely network trained imagenet extract dimensional feature fully connected layer. also consider network ﬁne-tuned object detection pascal trained using fast r-cnn following refer features vgg-cls vggdet respectively. ﬁne-tune representation task reduce computational memory load however model trivially allows back-propagation image representation likely would lead further improvements. referitgame dataset vgg-cls features additional spatial features provided concatenate refer obtained feature vgg+spat. language encoding decoding rely lstm variant implemented caﬀe initialize randomly jointly train grounding task. experiments adam solver adaptively changes learning rate training. train models epochs flickr entities/referitgame dataset respectively pick best iteration validation set. flickr entities using vgg-cls features. regularization. applying regularization parameters important best performance unsupervised model. introducing weight decay improve accuracy contrast supervision available introduce batch normalization phrase encoding lstm visual feature leads performance improvement particular supervised scenario. layer initialization. experiment diﬀerent ways initialize layer parameters. conﬁguration works best using uniform initialization lstm msra convolutional layers xavier layers. switching xavier msra initialization convolutional layers improves accuracy unsupervised model report performance approach multiple levels supervision table last line table report proposal upper-bound accuracy namely presence correct among proposals unsupervised training. start unsupervised scenario i.e. phrase localization ground-truth used training time. approach relies vgg-cls features able achieve accuracy. note network trained imagenet seen bounding annotations training time. vgg-det ﬁne-tuned detection performs better achieves accuracy. improve taking sentence constraint account. namely unlikely diﬀerent phrases sentence grounded box. thus post-process attended boxes jointly process phrases sentence greedily select highest scoring phrase cannot selected twice. allows reach accuracy vgg-cls vgg-det. currently sentence constraint simple post processing step test time would interesting include sentence level constraint training part future work. compare unsupervised deep fragments approach note report grounding performance allow direct comparison work. best case evaluation deep fragments also relies detection boxes features achieve accuracy overall ranking objective seen complimentary reconstruction objective. might possible part future work combine objectives learn even better models without grounding supervision. supervised training. next look fully supervised scenario. accuracy achieved scrc recent approach achieves vgg-det features. approach using vggcls features achieves accuracy signiﬁcantly improving prior works vgg-cls. improve result impressive using vgg-det features. semi-supervised training. finally move semi-supervised scenario. notation annot. means annotated data used. described section parameter controls weight attention loss latt reconstruction loss lrec. estimate value validation iterations. found need higher weight latt little supervision available. e.g. supervision supervision fact cases labeled instances contribute latt instances contribute lrec. integrating available annotated data model signiﬁcantly improve accuracy accuracy increases providing annotations reaching vgg-cls vgg-det using annotations. ablation semi-supervised model evaluated supervised model using respective annotated data. observed consistent improvement semi-supervised model supervised model. intrestingly using available supervision lrec still helps improve performance supervised model intuition latt single correct bounding train deep fragments model flickr dataset evaluate flickr entities ground truth phrases boxes. trained deep fragments model achieves .%/.% recall image annotation/search compared .%/.% reported large number dependency tree fragments sentence matched proposal boxes rather average noun phrases sentence flickr entities make best case study favor ground-truth phrase take maximum overlapping dependency tree fragments compute matched boxes ground truth take highest iou. lrec also learn overlapping boxes high best overlap. results phrase type. flickr entities dataset provides type phrase annotation phrase analyze table unsupervised approach well phrases like people animals vehicles worse clothing body parts. could confusion people clothing body parts. address this could jointly model phrases spatial relations model. body parts also challenging type detect proposal upper-bound supervised model vgg-cls features outperforms types except body parts instruments vgg-det better similar types. semi-supervised model brings signiﬁcant performance improvements particular body parts. last column report accuracy novel phrases i.e. ones appear training data. phrases approach maintains high performance although lower overall accuracy. shows learned language representation eﬀective allows transfer unseen phrases. summary flickr entities. unsupervised approach performs similar better fully supervised methods incorporating small amount supervision allows outperform also vgg-cls features used. best supervised model achieves surpassing previously reported results including semi-supervised model eﬃciently exploits reconstruction loss lrec allows outperform supervised model. provide reference numbers baselines lrcn caffe-k reported lrcn baseline using image captioning model lrcn trained mscoco score likely query phrase generated proposal box. caffe-k large scale object classiﬁer trained imagenet distinguish classes. predicts class proposal constructs word synonyms class-name based wordnet obtained word compared query phrase projected joint vector space. approaches unsupervised w.r.t. phrase bounding annotations. table reports results approach well vgg+spat features unsupervised training. unsupervised scenario grounder performs competitive lrcn caffe-k baselines achieving accuracy. note case vgg+spat perform similarly. supervised training. supervised scenario compare best prior work dataset scrc reaches accuracy. supervised approach uses identical visual features signiﬁcantly improves performance semi-supervised training. moving semi-supervised scenario demonstrates performance improvements similar ones observed flickr entities datset. even small amount supervision signiﬁcantly improves performance annotations achieve outperforming supervised model. summary referitgame dataset. unsupervised model slightly improves prior work semi-supervised version eﬀectively learn labeled training instances supervision achieves improving large margin overall performance referitgame dataset signiﬁcantly lower flickr entities. attribute facts. first training referitgame rather small compared flickr second proposal upperbound referitgame signiﬁcantly lower flickr entities complex nature described objects stuﬀ image regions. provide qualitative results flickr entities dataset figure compare unsupervised supervised approaches vgg-det features. supervised approach visibly improves localization quality unsupervised approach nevertheless able localize many phrases correctly. figure presents qualitative results referitgame dataset. show predictions supervised approach well ground-truth boxes. diﬃculty task presented examples including failures bottom row. requires good language understanding order correctly ground complex phrases. order ground expressions like nearest left person right would need additionally model relations objects interesting direction future work. work address challenging task grounding unconstrained natural phrases images. consider diﬀerent scenarios available bounding supervision training time namely none little full supervision. propose novel approach grounder learns localize phrases images attending correct proposal reconstructing phrase able operate supervision scenarios. unsupervised scenario competitive better related work. semi-supervised approach works well small portion available annotated data takes advantage unsupervised data outperform purely supervised training using amount labeled data. outperforms state-of-the-art flickr entities referitgame dataset respectively. approach rather general could applied regions segmentation proposals instead bounding proposals. possible extensions include constraints within sentences training time jointly reason multiple phrases take account spatial relations them. acknowledgements. marcus rohrbach supported fellowship within fitweltweit-program german academic exchange service work supported darpa afrl muri award awards iis- iis- berkeley artiﬁcial intelligence research lab.", "year": 2015}