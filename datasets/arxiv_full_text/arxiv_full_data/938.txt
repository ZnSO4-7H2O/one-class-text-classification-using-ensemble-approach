{"title": "Simple, Efficient, and Neural Algorithms for Sparse Coding", "tag": ["cs.LG", "cs.DS", "cs.NE", "stat.ML"], "abstract": "Sparse coding is a basic task in many fields including signal processing, neuroscience and machine learning where the goal is to learn a basis that enables a sparse representation of a given set of data, if one exists. Its standard formulation is as a non-convex optimization problem which is solved in practice by heuristics based on alternating minimization. Re- cent work has resulted in several algorithms for sparse coding with provable guarantees, but somewhat surprisingly these are outperformed by the simple alternating minimization heuristics. Here we give a general framework for understanding alternating minimization which we leverage to analyze existing heuristics and to design new ones also with provable guarantees. Some of these algorithms seem implementable on simple neural architectures, which was the original motivation of Olshausen and Field (1997a) in introducing sparse coding. We also give the first efficient algorithm for sparse coding that works almost up to the information theoretic limit for sparse recovery on incoherent dictionaries. All previous algorithms that approached or surpassed this limit run in time exponential in some natural parameter. Finally, our algorithms improve upon the sample complexity of existing approaches. We believe that our analysis framework will have applications in other settings where simple iterative algorithms are used.", "text": "sparse coding basic task many ﬁelds including signal processing neuroscience machine learning goal learn basis enables sparse representation given data exists. standard formulation non-convex optimization problem solved practice heuristics based alternating minimization. recent work resulted several algorithms sparse coding provable guarantees somewhat surprisingly outperformed simple alternating minimization heuristics. give general framework understanding alternating minimization leverage analyze existing heuristics design ones also provable guarantees. algorithms seem implementable simple neural architectures original motivation olshausen field introducing sparse coding. also give ﬁrst eﬃcient algorithm sparse coding works almost information theoretic limit sparse recovery incoherent dictionaries. previous algorithms approached surpassed limit time exponential natural parameter. finally algorithms improve upon sample complexity existing approaches. believe analysis framework applications settings simple iterative algorithms used. sparse coding dictionary learning consists learning express input vectors image patches linear combinations small number vectors chosen large dictionary. signal processing wide variety signals turn sparse appropriately chosen basis neuroscience sparse representations believed improve energy eﬃciency brain allowing neurons inactive given time. machine learning imposing sparsity constraint representation useful avoid over-ﬁtting. additionally methods sparse coding thought tool feature extraction basis number important tasks image processing segmentation retrieval de-noising super-resolution aroracs.princeton.edu princeton university computer science department center computational intractability. work supported part grants ccf- ccf- ccf- dms- simons investigator award simons collaboration grant. ronggemicrosoft.com microsoft research england. tengyucs.princeton.edu princeton university computer science department center computational intractability. work supported part grants ccf- ccf- ccf- dms- simons investigator award simons collaboration grant. function used encourage sparsity. function nonconvex unknown. paper well subsequent work chooses larger allows greater ﬂexibility adapting representation data. remark sparse coding confused related usually easier problem ﬁnding sparse representations given coding matrix variously called compressed sensing sparse recovery candes candes olshausen field also gave local search/gradient descent heuristic trying minimize nonconvex energy function gave experimental evidence produces coding matrices image patches resemble known features portion visual cortex. related paper authors olshausen field places sparse coding familiar generative model setting whereby data points assumed probabilistically generated according model noise samples appropriate distribution unknown code. deﬁne surprisingly maximum likelihood-based approaches seem unnecessary practice local search/gradient descent energy function hard constraints works well related algorithms aharon k-svd engan fact methods eﬀective sparse coding considered practice solved problem even though polynomial time algorithm eﬃcient algorithms neural algorithms. recently rapid progress designing polynomial time algorithms sparse coding provable guarantees adopt generative model viewpoint sketched above. surprising success simple descent heuristics remained largely unexplained. empirically heuristics perform running time sample complexity solution quality algorithms observation fact starting point current work. course famous example simplex ellipsoid linear programming reminds much challenging analyze behavior empirically successful algorithm design polynomial time algorithm scratch sparse coding simple intuitive heuristics important another reason beyond algorithmic eﬃciency appear implementable neural architectures. since neural computation also deep learning proven diﬃcult analyze general analyzing sparse coding thoroughly seems natural ﬁrst step theory. algorithm close relative olshausen-field algorithm thus inherits neural implementability; appendix discussion. present rigorous analysis simple energy minimization heuristic side beneﬁt yields bounds running time sample complexity sparse coding better algorithms recent papers. adds recent literature analyzing alternating minimization jain hardt netrapalli work setting convex program known work setting known convex program runs time exponential natural parameter barak assuming µ-incoherent former gave algorithm works sparsity n/−γ/µ running time agarwal gave algorithm works sparsity either n//µ n//µ depending particular assumptions model. works also analyze alternating minimization assume starts estimate column-wise needs order useful estimate. however case algorithm runs exponential time. sample complexity algorithms also rather large least much larger. give simple eﬃcient algorithms based alternating minimization whose column-wise error decreases geometrically work sparsity n//µ remark even empirically alternating minimization appear work much beyond bound. work following family generative models casual reader think drawn distribution independent coordinates. even simpler setting —which polynomial time algorithms using independent component analysis—we know rigorous analysis heuristics like olshausen-field. models natural since original motivation behind sparse coding discover code whose representations property coordinates almost independent. relax requirements above expense restricting sparsity detail tradeoﬀs. section give general framework analyzing alternating main theorems minimization. instead thinking algorithm trying minimize known nonconvex function view trying minimize unknown convex function. various update rules shown provide good approximations gradient unknown function. lemma lemma lemma examples. leverage framework analyze existing heuristics design ones also provable guarantees. section prove additionally give neural architecture implementing algorithm appendix best knowledge ﬁrst neurally plausible algorithm sparse coding provable convergence. algorithm based modiﬁcation carefully project components along column currently updated. complement theorems revisiting olshausen-field rule analyzing variant section however analysis complex need bound quadratic error terms. uses convex programming. remains give method initialize iterative algorithms. give approach based pair-wise reweighting prove returns estimate -near high probability. additional beneﬁt within logarithmic factor information theoretic threshold sparse recovery incoherent dictionaries donoho gribonval nielsen previous known algorithms approach arora surpass sparsity barak time exponential natural parameter. moreover algorithms simple describe implement involve basic operations. believe framework applications beyond sparse coding could used show simple iterative algorithms powerful contexts well suggesting ways analyze them. describe framework analyzing alternating minimization. generic scheme interested given algorithm alternates updating estimates heuristic minimizing non-convex function penalty function hard constraint. crucial step compute gradient respect take step opposite direction update throughout paper learning rate needs appropriately. challenge analyzing general algorithm identify suitable measure progress— called lyapunov function dynamical systems control theory show improves step measure progress algorithms tion. independent paper balakrishnan proposes similar framework analysing algorithms hidden variable models. diﬀerence condition really geometry objective function though property direction movement. therefore ﬂexibility choose diﬀerent decoding procedures. ﬂexibility allows closed form obtain useful functional form setup reminiscent stochastic gradient descent moves direction whose expectation gradient known convex function. contrast function unknown furthermore expectation true gradient bias. bias able prove algorithms reach approximate optimum error whose magnitude determined bias. make bias negligible using complicated algorithms. consider general iterative algorithm trying desired solution step starts guess computes direction updates estimate ηgs. natural progress measure global optimum convex function speciﬁcally α-strongly convex /-smooth -correlated also refer bias. theorem suppose satisﬁes deﬁnition satisﬁes maxt convex standard approach take step direction gradient project iteration namely replace projb closest point euclidean distance. well-known projb therefore obtain following remains derive functional form various update rules show rules move direction approximately points direction desired solution sgn] section coordinate-wise sign function. return olshausen-field update rule analyze variant section using approximate projected gradient descent. finally design update rule section carefully project components along column currently updated. eﬀect replacing error term another results update rule negligible bias. main steps showing update rules framework given lemma lemma lemma algorithm update usual approach solve sparse recovery problem respect current code matrix however many standard basis pursuit algorithms penalty) diﬃcult analyze error code itself. part solution closed form terms code matrix. instead take much simpler approach solving sparse recovery problem uses matrix-vector multiplication followed thresholding particular thresholdc/t thresholdc/ keeps non-zero coordinates magnitude least decoding rule recovers signs support correctly provided column-wise δ-close section give initialization procedure based pair-wise reweighting prove works high probability. section independent interest since algorithm used even settings known could help solve another problem practice model selection. lemma design analyze neurally plausible algorithm sparse coding given algorithm give neural architecture implementing algorithm appendix fact simple algorithm provably works sheds light sparse coding might accomplished nature. throughout paper work following measure closeness deﬁnition δ-close permutation choice signs -near addition too. natural measure since hope learn columns relabeling sign-ﬂips. analysis assume throughout identity permutation family generative models invariant moreover norm note piqi scaling constant hence formula expect proof since -near δ-close invoke lemma conclude high probability sgn. event indicator function event. subsection analyze variant olshausen-field update rule. however quadratic error terms arise expressions derive bounding challenging. also need make stronger assumptions distributional model distinct qiii qiii theorem suppose -near lemma gave expression describes update direction assumptions generative model. make crucial lemma order prove -correlated moreover lemma show as+− together auxiliary assume iteration algorithm takes inﬁnite number samples prove corresponding simpliﬁed version theorem proof theorem highlights essential ideas behind proof theorem found appendix proof throughout proof ﬁxed omit superscript simplify notations. assumption already component need show norm extra term small enough. first bound norm triangle inequality large theory practice terms initialize alternating minimization. usual approach randomly populate columns samples often work know analyse them. give novel method initialization show succeeds high probability. defer proof theorem appendix idea following main lemma. invoke lemma several times order analyze algorithm verify whether supports share common element show sparsity requires ideas alternating minimization appears break going beyond down. mysterious properties alternating minimization also left explore random initialization works. heuristics information theoretically optimal terms sample complexity? finally analyse energy minimization contexts well?", "year": 2015}