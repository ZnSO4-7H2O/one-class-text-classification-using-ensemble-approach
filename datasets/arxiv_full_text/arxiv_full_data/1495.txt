{"title": "Best of Both Worlds: Transferring Knowledge from Discriminative Learning  to a Generative Visual Dialog Model", "tag": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "We present a novel training framework for neural sequence models, particularly for grounded dialog generation. The standard training paradigm for these models is maximum likelihood estimation (MLE), or minimizing the cross-entropy of the human responses. Across a variety of domains, a recurring problem with MLE trained generative neural dialog models (G) is that they tend to produce 'safe' and generic responses (\"I don't know\", \"I can't tell\"). In contrast, discriminative dialog models (D) that are trained to rank a list of candidate human responses outperform their generative counterparts; in terms of automatic metrics, diversity, and informativeness of the responses. However, D is not useful in practice since it cannot be deployed to have real conversations with users.  Our work aims to achieve the best of both worlds -- the practical usefulness of G and the strong performance of D -- via knowledge transfer from D to G. Our primary contribution is an end-to-end trainable generative visual dialog model, where G receives gradients from D as a perceptual (not adversarial) loss of the sequence sampled from G. We leverage the recently proposed Gumbel-Softmax (GS) approximation to the discrete distribution -- specifically, an RNN augmented with a sequence of GS samplers, coupled with the straight-through gradient estimator to enable end-to-end differentiability. We also introduce a stronger encoder for visual dialog, and employ a self-attention mechanism for answer encoding along with a metric learning loss to aid D in better capturing semantic similarities in answer responses. Overall, our proposed model outperforms state-of-the-art on the VisDial dataset by a significant margin (2.67% on recall@10). The source code can be downloaded from https://github.com/jiasenlu/visDial.pytorch.", "text": "present novel training framework neural sequence models particularly grounded dialog generation. standard training paradigm models maximum likelihood estimation minimizing cross-entropy human responses. across variety domains recurring problem trained generative neural dialog models tend produce ‘safe’ generic responses contrast discriminative dialog models trained rank list candidate human responses outperform generative counterparts; terms automatic metrics diversity informativeness responses. however useful practice since deployed real conversations users. work aims achieve best worlds practical usefulness strong performance knowledge transfer primary contribution end-to-end trainable generative visual dialog model receives gradients perceptual loss sequence sampled leverage recently proposed gumbel-softmax approximation discrete distribution speciﬁcally augmented sequence samplers coupled straight-through gradient estimator enable end-to-end differentiability. also introduce stronger encoder visual dialog employ self-attention mechanism answer encoding along metric learning loss better capturing semantic similarities answer responses. overall proposed model outperforms state-of-the-art visdial dataset signiﬁcant margin source code downloaded https//github.com/jiasenlu/visdial.pytorch fundamental goal artiﬁcial intelligence development perceptually-grounded dialog agents speciﬁcally agents perceive understand environment communicate understanding humans agents natural language. last years neural sequence models emerged dominant paradigm across variety setting datasets text-only dialog recently visual dialog agent must answer sequence questions grounded image requiring reason visual content dialog history. standard training paradigm neural dialog models maximum likelihood estimation equivalently minimizing cross-entropy ‘ground-truth’ human response. across variety domains recurring problem trained neural dialog models tend produce ‘safe’ generic responses ‘not sure’ don’t know’ text-only dialog can’t see’ can’t tell’ visual dialog reason emergent behavior space possible next utterances dialog highly multi-modal face highly multi-modal output distributions models ‘game’ latching head distribution frequent responses nature tend generic widely applicable. safe generic responses break dialog tend disengage human conversing agent ultimately rendering agent useless. clear novel training paradigms needed; focus paper. promising alternative training proposed recent work sequence-level training neural sequence models speciﬁcally using reinforcement learning optimize taskspeciﬁc sequence metrics bleu rouge cider unfortunately case dialog existing automatic metrics correlate poorly human judgment renders alternative infeasible dialog models. paper inspired success adversarial training propose train generative visual dialog model produce sequences score highly discriminative visual dialog model discriminative dialog model receives input candidate list possible responses learns sort list training dataset. generative dialog model aims produce sequence rank highest list shown fig. note proposed approach inspired adversarial training number subtle crucial differences generative adversarial networks unlike traditional gans novelty setup discriminator receives list candidate responses explicitly learns reason similarities differences across candidates. process learns task-dependent perceptual similarity learns recognize multiple correct responses feature space. example shown fig. right given image dialog history question bird?’ besides ground-truth answer not’ also assign high scores options valid responses question including generated ‘not see’. interaction responses captured similarity learned embeddings. similarity gives additional signal leverage addition loss. sense proposed approach viewed instance ‘knowledge transfer’ employ metric-learning loss function self-attention answer encoding mechanism makes particularly conducive knowledge transfer encouraging perceptually meaningful similarities emerge. especially fruitful since prior work demonstrated discriminative dialog models signiﬁcantly outperform generative counterparts useful since necessarily need list candidate responses rank available dialog dataset real conversations user. context work aims achieve best worlds practical usefulness strong performance knowledge transfer. primary technical contribution end-to-end trainable generative visual dialog model generator receives gradients discriminator loss sequence sampled note challenging output sequence discrete symbols naïvely amenable gradient-based training. propose leverage recently proposed gumbel-softmax approximation discrete distribution speciﬁcally recurrent neural network augmented sequence samplers coupled straight-through gradient estimator enables end-to-end differentiability. results show ‘knowledge transfer’ approach indeed successful. speciﬁcally discriminator-trained outperforms mle-trained recall visdial dataset essentially improving state-of-the-art recall recall. moreover generative model produces diverse informative responses side contribution speciﬁc application introduce novel encoder neural visual dialog models maintains separate memory banks visual memory another textual memory outperforms encoders used prior work. gans sequence generation. generative adversarial networks shown effective models wide range applications involving continuous variables recently also used discrete output spaces language generation e.g. image captioning dialog generation text generation either viewing generative model stochastic parametrized policy updated using reinforce figure model architecture proposed model. given image history question discriminator receives additional input candidate list possible responses learns sort list. generator aims produce sequence discriminator rank highest list. right block score different candidate answers. note multiple plausible responses score high. image coco dataset discriminator providing reward continuous relaxation discrete variables gumbel-softmax enable backpropagating response discriminator subtle signiﬁcant differences w.r.t. application motivation approach. prior works discriminator generator trained tandem scratch. goal discriminator settings primarily discriminate ‘fake’ samples ‘real’ samples contrast would like transfer knowledge discriminator generator. start pre-trained models suited task transfer knowledge improve keeping ﬁxed. show experiments procedure results producing diverse samples close embedding space ground truth perceptual similarity learned also draw connections work energy based without adversarial training aspect. energy case deep metric-learning based scoring mechanism instantiated visual dialog application. modeling image text attention. models tasks intersection vision language e.g. image captioning visual question answering visual dialog typically involve attention mechanisms. image captioning attending relevant regions image attending relevant image regions alone co-attending image regions question words/phrases context visual dialog uses attention identify utterances dialog history useful answering current question. however modeling image entire image embedding used obtain answer. contrast proposed encoder hciae localizes region image help reliably answer question. particular addition history question guiding image attention visual dialog encoder also reasons history identifying relevant regions image. allows model implicitly resolve co-references text ground back image. followup question round visual dialog agent needs return valid response question. given problem setup broad classes methods generative discriminative models. generative models visual dialog trained maximizing log-likelihood ground truth answer sequence hand discriminative models receive encoding input models effectively additional input list candidate answers learn sort list. thus design cannot used test time without list candidates available. section describe approach transfer knowledge discriminative visual dialog model generative visual dialog model fig. shows overview approach. given input image dialog history question encoder converts inputs joint representation generator takes input produces distribution answer sequences recurrent neural network word answer sequence gumbel-softmax sampler sample answer token distribution. negative discriminator it’s standard form takes ground-truth answer answers ti}n− t·)) embedding function. enable communication similarity nearly dialogs involve least pronoun. means model correctly answer question would require reliable mechanism co-reference resolution. common approach encoder architecture attention mechanism implicitly performs co-reference resolution identifying portion dialog history help answering current question using holistic representation image. intuitively would also expect answer also localized regions image consistent attended history. motivation propose novel encoder architecture shown fig. encoder ﬁrst uses current question attend exchanges history question attended history attend image obtain ﬁnal encoding. speciﬁcally spatial image features rd×k convolution layer cnn. encoded lstm vector simultaneously previous round history encoded separately another rd×t. conditioned question lstm embedding model attends history. attended representation history question embedding concatenated used input attend image vector elements rt×d parameters learned. attention weight convex combination columns weighted history. attended history feature appropriately elements query vector attended image feature similar manner. subsequently three components used obtain ﬁnal embedding discriminator loss discriminative visual dialog models produce distribution candidate answer list maximize log-likelihood correct option loss function needs conducive knowledge transfer. particular needs encourage perceptually meaningful similarities. therefore metric-learning multi-class n-pair loss deﬁned attention based lstm encoder answer. attention help discriminator better deal paraphrases across answers. attention weight learnt -layer lstm output time step. n-pair loss objective encourages learning space ground truth answer scored higher options time encourages options similar ground truth answers score better dissimilar ones. means that unlike multiclass logistic loss options correct different correct option overly penalized thus useful providing reliable signal generator. fig. example. follwing regularize norm embedding vectors small. high-level approach transferring knowledge follows repeatedly queries answers generates input embedding feedback update itself. update goal update parameters score higher correct answer learned embedding scoring function. formally perceptual loss aims optimize given embedding function learned discriminator intuitively updating generator parameters minimize interpreted learning produce answer sequence ‘fools’ discriminator believing answer score higher human response straightforward sample answer generator perform forward pass discriminator naïvely possible backpropagate gradients generator parameters since sampling discrete symbols results zero gradients w.r.t. generator parameters. overcome this leverage recently introduced continuous relaxation categorical distribution gumbel-softmax distribution concrete distribution intuitive level gumbel-softmax approximation uses called ‘gumbel-max trick’ reparametrize sampling categorical distribution replaces argmax softmax obtain continuous relaxation discrete random variable. formally denote k-ary categorical denote samples standard gumbel distribution e−e−g. sample concrete distribution produced following transformation temperature parameter control close samples concrete distribution approximate one-hot encoding categorical variable illustrated fig. augment lstm sequence samplers. speciﬁcally position answer sequence sampler sample answer token conditional distribution. coupled straight-through gradient estimator enables end-to-end differentiability. speciﬁcally forward pass discretize samples discrete samples backward pass continuous relaxation compute gradients. experiments held temperature parameter ﬁxed experiments dataset setup. evaluate proposed approach visdial dataset collected pairing subjects amazon mechanical turk chat image. person assigned role ‘questioner’ ‘answerer’. worker sees single line text describing image image remains hidden questioner. task questions hidden image imagine scene better. second worker sees image caption answers questions. workers take turns asking answering questions rounds. perform experiments visdial containing dialogs coco-train coco-val images total dialog question-answer pairs. split train test manner consistent caption considered ﬁrst round dialog history. evaluation protocol. following evaluation protocol established retrieval setting evaluate responses round dialog. speciﬁcally every question visdial coupled list candidate answer options models asked sort evaluation purposes. uses score rank answer options uses log-likelihood options ranking. models evaluated standard retrieval metrics mean rank recall mean reciprocal rank human response returned sorted list. pre-processing. truncate captions/questions/answers longer words respectively. build vocabulary words occur least times train resulting words. training details experiments lstms single layer hidden state. vgg- representation image. ﬁrst rescale images pixels take output last pooling layer image feature. adam optimizer base learning rate pre-train using standard epochs supervised training based epochs. following regularize norm embedding vectors small. subsequently train combination discriminative perceptual loss loss. found including important encouraging generate grammatically correct responses. results analysis baselines. compare proposed techniques current state-of-art generative discriminative models developed speciﬁcally introduced encoding architectures late fusion hierarchical recurrent encoder memory network trained generative discriminative decoder. compare models. approaches. present variants approach systematically study individual contributions training procedure novel encoder self-attentive answer encoding metric-loss hciae-g-dis generative model proposed encoder trained mixed discriminator loss forms best generative model. comparing model hciae-g-mle establishes improvement discriminative training. hciae-d-mle discriminative model proposed encoder trained standard discriminative cross-entropy loss. answer candidates encoded using lstm comparing variant discriminative baselines establishes improvement encoder discriminative setting. hciae-d-np discriminative model proposed encoder trained n-pair discriminative loss answer candidates encoded using lstm comparing variant hciae-d-mle establishes improvement n-pair loss. hciae-d-np-att discriminative model proposed encoder trained n-pair discriminative loss using self-attentive answer encoding. comparing variant hciae-d-np establishes improvement self-attention mechanism encoding answers. main results hciae-g-dis ﬁnal generative model ‘bells whistles’ hciae-g-dis uniformly performs best metrics outperforming previous state-of-art model mn-g shows importance knowledge transfer discriminator beneﬁt encoder architecture. knowledge transfer encoder understand relative importance proposed history conditioned image attentive encoder knowledge transfer compared performance hciae-g-dis hciae-g-mle uses proposed encoder without feedback discriminator. comparison highlights points ﬁrst hciae-g-mle improves current state-of-art method conﬁrming beneﬁts encoder. secondly importantly performance lower hciae-g-dis conﬁrming modiﬁcations encoder alone sufﬁcient gain improvements answer generation; knowledge transfer greatly improves metric loss self-attentive answer encoding purely discriminative setting ﬁnal discriminative model also beats performance corresponding state-of-art models n-pair loss used discriminator helpful knowledge transfer also improves performance discriminator improvements obtained using answer attention mechanism leads additional albeit small gains discriminator performance recall model training happens follows independently train generative model hciae-g-mle discriminative model hciae-d-np-att. hciae-g-mle initialization generative model updated based feedback hciae-d-np-att results ﬁnal hciae-g-dis. performed experiments answer following questions happens continue training hciae-d-np-att adversarial setting? particular continue training maximizing score ground truth answer minimizing score generated answer effectively setting adversarial training regime −lg. resulting discriminator hciae-gan signiﬁcant drop performance seen table. perhaps expected hciae-gan updates parameters based answers ground truth generated sample wrecks structure hciae-d-np-att previously learned leveraging additional incorrect options. happens continue structure-preserving training hciae-d-np-att? addition providing hciae-d-np-att samples fake answers also include incorrect options negative answers structure learned discriminator preserved. hciae-d-np-att continues train loss case small improvement performance additional computational overhead training discriminator supersedes performance improvement. also note hciae-d-np-att gets worse dialog task. might wonder train visual dialog? formulating task setting would involve training tandem providing feedback whether response generates real fake. found particularly unstable setting main reasons first consider case ground truth answer generated answers same. happens answers typically short ‘cryptic’ case train provide feedback answer labeled positive negative. second cases ground truth answer descriptive generator provides short answer quickly become powerful enough discard generated samples fake. case able provide information better task. experience suggests discriminator consider ‘gans visual dialog’ setting merely focused differentiating fake real. needs able score similarity ground truth answers. scoring mechanism provides reliable feedback fact show previous results pre-trained captures structure ingredient sharing knowledge adversarial training central. table present couple qualitative examples compares responses generated g-mle g-dis. g-mle predominantly produces ‘safe’ less informative answers ‘yes’ can’t tell’. contrast proposed model g-dis less frequently often generates diverse informative responses. conclusion generative models dialog typically trained objective. result tend latch safe generic responses. discriminative models hand shown signiﬁcantly outperform generative counterparts. however discriminative models deployed dialog agents real user canned candidate responses available. work propose transferring knowledge powerful discriminative visual dialog model generative model. leverage gumbel-softmax approximation discrete distribution –speciﬁcally augmented sequence samplers coupled gradient estimator end-to-end differentiability. also propose novel visual dialog encoder reasons image-attention informed history dialog; employ metric learning loss along self-attentive answer encoding enable discriminator learn meaningful structure dialog responses. result generative visual dialog model signiﬁcantly outperforms state-of-the-art.", "year": 2017}