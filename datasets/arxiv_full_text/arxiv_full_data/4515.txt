{"title": "Support Vector Machine Classification with Indefinite Kernels", "tag": ["cs.LG", "cs.AI"], "abstract": "We propose a method for support vector machine classification using indefinite kernels. Instead of directly minimizing or stabilizing a nonconvex loss function, our algorithm simultaneously computes support vectors and a proxy kernel matrix used in forming the loss. This can be interpreted as a penalized kernel learning problem where indefinite kernel matrices are treated as a noisy observations of a true Mercer kernel. Our formulation keeps the problem convex and relatively large problems can be solved efficiently using the projected gradient or analytic center cutting plane methods. We compare the performance of our technique with other methods on several classic data sets.", "text": "propose method support vector machine classiﬁcation using indeﬁnite kernels. instead directly minimizing stabilizing nonconvex loss function algorithm simultaneously computes support vectors proxy kernel matrix used forming loss. interpreted penalized kernel learning problem indeﬁnite kernel matrices treated noisy observations true mercer kernel. formulation keeps problem convex relatively large problems solved eﬃciently using projected gradient analytic center cutting plane methods. compare performance technique methods several standard data sets. support vector machines become central tool solving binary classiﬁcation problems. critical step support vector machine classiﬁcation choosing suitable kernel matrix measures similarity data points must positive semideﬁnite formed gram matrix data points reproducing kernel hilbert space. positive semideﬁnite condition kernel matrices also known mercer’s condition machine learning literature. classiﬁcation problem becomes linearly constrained quadratic program. here present algorithm classiﬁcation using indeﬁnite kernels i.e. kernel matrices formed using similarity measures positive semideﬁnite. interest indeﬁnite kernels motivated several observations. first certain similarity measures take advantage application-speciﬁc structure data often display excellent empirical classiﬁcation performance. unlike popular kernels used support vector machine classiﬁcation similarity matrices often indeﬁnite necessarily correspond reproducing kernel hilbert space. discussion.) particular application classiﬁcation indeﬁnite kernels image classiﬁcation using earth mover’s distance discussed zamolotskikh cunningham similarity measures protein sequences smith-waterman blast scores indeﬁnite provided hints constructing useful positive semideﬁnite kernels decribed saigo transformed positive semideﬁnite kernels good empirical performance example). tangent distance similarity measures ∗orfe department princeton university princeton rlussalumni.princeton.edu †orfe department princeton university princeton aspremonprinceton.edu preliminary version paper appeared proceedings neural information processing systems described simard haasdonk keysers invariant various simple image transformations also shown excellent performance optical character recognition. finally sometimes impossible prove kernels satisfy mercer’s condition numerical complexity evaluating exact positive kernel high proxy kernel used instead example). cases method allows bypass limitations. objective derive eﬃcient algorithms directly indeﬁnite similarity measures classiﬁcation. work closely follows spirit recent results kernel learning kernel matrix learned linear combination given kernels result explicitly constrained positive semideﬁnite. problem numerically challenging bach adapted algorithm solve case kernel written positively weighted combination kernels. setting here never numerically optimize kernel matrix part problem solved explicitly means complexity method substantially lower classical kernel learning algorithms closer practice algorithm used sonnenberg formulate multiple kernel learning problem bach semi-inﬁnite linear program solve column generation technique similar analytic center cutting plane method here. several methods proposed dealing indeﬁnite kernels svms. ﬁrst direction embeds data pseudo-euclidean space haasdonk example formulates classiﬁcation problem indeﬁnite kernel minimizing distance convex hulls formed categories data embedded space. nonseparable case handled manner using reduced convex hulls. discussion geometric interpretations svm.) another direction applies direct spectral transformations indeﬁnite kernels ﬂipping negative eigenvalues shifting eigenvalues reconstructing kernel original eigenvectors order produce positive semideﬁnite kernel zamolotskikh cunningham example). another option reformulate either maximum margin problem dual order indeﬁnite kernel convex optimization problem. reformulation suggested replaces indeﬁnite kernel identity matrix maintains separation using linear constraints. method achieves good performance convexiﬁcation procedure hard interpret. directly solving nonconvex problem sometimes gives good results well haasdonk oﬀers guarantees performance. work instead directly transforming indeﬁnite kernel simultaneously learn support vector weights proxy mercer kernel matrix penalizing distance proxy kernel original indeﬁnite one. main result kernel learning part problem solved explicitly meaning classiﬁcation problem indeﬁnite kernels simply formulated perturbation positive semideﬁnite case. formulation interpreted penalized kernel learning problem uncertainty input kernel matrix. sense indeﬁnite similarity matrices seen noisy observations true positive semideﬁnite kernel learn kernel increases generalization performance. complexity standpoint original classiﬁcation problem indeﬁnite kernel nonconvex penalization detail results convex problem hence solved eﬃciently guaranteed complexity bounds. paper organized follows. section formulate main classiﬁcation result section describe three detail interpretation penalized kernel learning problem. algorithms solving problem. section discusses several extensions main results. finally section test numerical performance methods various data sets. section modify kernel learning problem formulate penalized kernel learning problem indeﬁnite kernels. also detail framework applies kernels satisfy mercer’s condition. given kernel matrix vector labels diag matrix diagonal formulate kernel learning problem lanckriet authors minimize upper bound misclassiﬁcation probability using given kernel upper bound generalized performance measure order learn optimal kernel achieves good generalization performance. restricted convex subsets constant trace show problem reformulated convex program. restrictions reduce tractable optimization problems semideﬁnite quadratically constrained quadratic programs. goal solve problem similar restricting distance proxy kernel used classiﬁcation original indeﬁnite similarity measure. performance measure dual classiﬁcation problem hinge loss quadratic penalty. positive semideﬁnite problem convex quadratic program. suppose given indeﬁnite kernel matrix formulate instance problem restricting positive semideﬁnite kernel matrix given neighborhood original kernel matrix solve variables parameter controls distance original matrix proxy kernel kernel learning problem problem infeasible small values replace hard constraint penalty distance proxy kernel original indeﬁnite similarity matrix solve instead ﬁrst note problem convex optimization problem. inner minimization problem convex conic program also pointwise minimum family concave quadratic functions solution inner problem concave function hence outer optimization problem also convex details). thus concave maximization problem subject linear constraints therefore convex problem result inner kernel learning optimization problem solved closed form. variable rank matrix coeﬃcients yiαiαjyj. problem cast eigenvalue optimization problem variable letting eigenvalue decomposition column write reformulation problem appears chen authors move inner minimization problem constraints following semi-inﬁnite quadratically constrained linear program explicit solution optimal kernel given projection penalized rank-one update indeﬁnite kernel cone positive semideﬁnite matrices. tends inﬁnity rank-one update less eﬀect limit optimal kernel given zeroing negative eigenvalues indeﬁnite kernel. means indeﬁnite kernel contains small amount noise best positive semideﬁnite kernel framework positive part indeﬁnite kernel. limit tends inﬁnity also motivates heuristic transforming kernel testing set. since negative eigenvalues training kernel thresholded zero limit transformation occur test kernel. hence measure generalization performance update entries full kernel corresponding training instances rank-one update resulting optimal solution threshold negative eigenvalues full kernel matrix zero produce mercer kernel test set. variables quadratic program variables seen earlier feasible solution produces corresponding proxy kernel plugging kernel problem allows compute upper bound optimum value problem solving simple quadratic program variables result used bound duality track convergence. detail algorithms used solve problem maximizes nondiﬀerentiable concave function subject convex constraints. optimal point always exists since feasible bounded nonempty. numerical stability algorithms quadratically smooth objective compute gradient. ﬁrst describe simple projected gradient method numerically cheap iterations less predictable performance practice. show apply analytic center cutting plane method whose iterations numerically complex converges linearly. completeness also describe exchange method chen used solve problem numerical bottleneck quadratically constrained linear program solved iteration. gradient calculating gradient objective function requires computing eigenvalue decomposition matrix form +ρααt given matrix derivative eigenvalue respect given brieﬂy recall computed eﬃciently case details). refer reader kulis another kernel learning example using method. given eigenvalue decomposition changing basis problem reduced decomposition diagonal plus rank-one matrix ρuut first updated eigenvalues determined solving secular equations done explicit solution eigenvectors corresponding eigenvalues stable eigenvalues approximated. instability circumvented computing vector approximate eigenvalues exact eigenvalues matrix ρˆuˆut computing stable eigenvectors explicitly steps done time. ρˆuˆut close enough original matrix eigenvalues eigenvectors stable approximations true values. finally eigenvectors original matrix computed stable eigenvectors ρˆuˆut updating eigenvalue decomposition reduced procedure plus matrix multiplication complexity gradient computation. note eigenvalues symmetric matrices diﬀerentiable multiplicities greater discussion) subgradient used instead gradient algorithms detailed here. lewis shows compute approximate subdiﬀerential k-th largest eigenvalue symmetric matrix. used form regular subgradient objective function concave construction. projected gradient method takes steepest descent step projects point back onto feasible region example). choose initial point algorithm proceeds algorithm here assumed objective function diﬀerentiable method eﬃcient projection step numerically cheap. complexity iteration breaks follows step requires eigenvalue decomposition computed plus matrix multiplication described above. experiments stepsize indeﬁnitesvm perturbsvm iteration number. good stepsize crucial performance must chosen separately data rule thumb. note line search would costly would require multiple eigenvalue decompositions recalculate objective multiple times. step projection onto region solved explicitly sorting vector entries cost stopping criterion. compute duality using results candidate kernel iteration solve problem simply means solving problem positive semideﬁnite kernel produces upper bound hence bound suboptimality current solution. complexity. number iterations required method reach target precision grows nesterov complete discussion. analytic center cutting plane method reduces feasible region iteration using computed evaluating subgradient objective function analytic center current feasible volume reduced region converges target precision. method require diﬀerentiability. write ﬁrst localization optimal solution. method described algorithm complete treatment cutting plane methods). complexity iteration breaks follows step step computes analytic center polyhedron solved operations using interior point methods example. step simply updates polyhedral description. includes gradient computation plus matrix multiplication. objective function analytic center problem. computing hessian easy requires matrix multiplication form diagonal. restricting number constraints rule thumb; raising limit increases iteration complexity decreasing increases required number iterations. stopping criterion. upper bound computed maximizing ﬁrst order taylor approximation points ellipsoid covers computed explicitly. complexity. accpm provably convergent iterations using elimination keeps complexity localization bounded. schemes available slightly diﬀerent complexities bound achieved goﬃn vial using approximate centers example. algorithm considered chen order solve problem falls class algorithms called exchange methods methods iteratively solve problems constrained ﬁnite subset inﬁnitely many constraints solution iterate gives improved lower bound maximization problem. subproblem solved iteration number constraints used approximate inﬁnitely many constraints problem initial solution found solving problem input indeﬁnite kernel. algorithm proceeds algorithm below. complexity iteration breaks follows step step solved analytically using theorem eﬃcient calculation made algorithms using procedure plus matrix multiplication. step previous point optimal feasible respect constraint case feasible inﬁnitely many constraints original problem hence also optimal. step. step requires solving qclp number quadratic constraints equivalent number iterations. shown chen qclp written regularized version multiple kernel learning problem lanckriet number constraints equivalent number kernels mkl. eﬃcient methods solve many kernels active area research recently rakotomamonjy there authors gradient method solve reformulation problem smooth maximization problem. objective value gradient computation requires computing support vector machine hence iteration requires several computations speeded using warm-starting. furthermore chen prune inactive constraints iteration order decrease number constraints qclp. complexity. rate convergence known algorithm duality given chen shown monotonically decrease. ﬁrst algorithms discussed implemented matlab cases indeﬁnite positive semideﬁnite kernels downloaded authors’ webpages package called indeﬁnitesvm. penalty parameter one-dimensional implementation. package makes libsvm code chang produce suboptimality bounds track convergence. matlab implementation exchange method uses mosek solve problem compared projected gradient method section section extend results kernel methods namely support vector regressions one-class support vector machines. addition apply method using mercer kernels practicality indeﬁnite kernels classiﬁcation similarly motivates using indeﬁnite kernels support vector regression extend formulations section linear ǫ-insensitive loss proof follows directly theorem slight diﬀerence vector labels appear optimal kernel. plugging resulting formulation rewritten convex eigenvalue optimization problem variable again proxy kernel given produced feasible solution plugging proxy kernel problem allows compute upper bound optimum value problem solving support vector regression problem. variables inner minimization problem identical indeﬁnite optimal kernel form given corollary plugging gives another convex eigenvalue optimization problem central motivation indeﬁnite kernels classiﬁcation would also like analyze happens mercer kernel used input case learn another kernel decreases upper bound generalization performance produces perturbed support vectors. interpret input noisy kernel such achieve suboptimal performance. input kernel best kernel observe framework achieves optimal performance tends inﬁnity otherwise simply learn better kernel using ﬁnite classic problem given fourth order penalty support vectors. testing framework need transform kernel support vectors perturbed. case computing gradient longer requires eigenvalue decompositions iteration. experimental results shown section notice theorem generalization theorem constructing rank-one penalty matrix simply assign penalties training point. componentwise penalty formulation also extended true kernels. theorem simpliﬁes rank-one update section compare generalization performance technique methods applying classiﬁcation indeﬁnite similarity measures. also examine classiﬁcation performance using mercer kernels. conclude experiments showing convergence algorithms. experiments mercer kernels libsvm library. compare method classiﬁcation indeﬁnite kernels several kernel preprocessing techniques discussed earlier. ﬁrst three techniques perform spectral transformations indeﬁnite kernel. ﬁrst called denoise here thresholds negative eigenvalues zero. second transformation called takes absolute value eigenvalues. last transformation shift adds constant eigenvalue making positive. details. also implemented modiﬁcation suggested nonconvex quadratic objective function made convex replacing indeﬁnite kernel identity matrix. kernel appears linear inequality constraints separate data. finally compare results direct classiﬁcation original indeﬁnite kernel ﬁrst experiment data usps handwritten digits database hull using indeﬁnite simpson score one-sided tangent distance kernel compare digits. tangent distance transformation invariant measure—it assigns high similarity image slightly rotated shifted instances—and known perform well data set. experiments symmetrize one-sided tangent distance using square mean tangent distance deﬁned haasdonk keysers make similarity measure negative exponentiation. also consider simpson score task much cheaper compute ﬁnally analyze three data sets repository using indeﬁnite sigmoid kernel. data randomly divided training testing data. apply -fold cross validation average accuracy recall measures determine optimal parameters kernel inputs. train model full training optimal parameters test independent test set. table provides summary statistics data sets including minimum maximum eigenvalues training similarity matrices. observe simpson highly indeﬁnite one-sided tangent distance kernel nearly positive semideﬁnite. spectrum sigmoid kernels varies greatly across examples sensitive sigmoid kernel parameters. table compares accuracy recall average denoise shift modiﬁed direct indeﬁnite algorithm described work. based interpretation section indeﬁnite expected perform least well denoise; denoise good transformation cross-validation choose high penalty makes indeﬁnite denoise nearly equivalent. rank-one update provides ﬂexibility transformation similarities concerning data points easily classiﬁed modiﬁed rank-one update. interpretation speciﬁc rank-one update currently known. however chen recently proposed spectrum modiﬁcations similar manner indeﬁnite svm. rather perturb entire indeﬁnite similarity matrix perturb spectrum directly allowing improvements denoise well transformations. also note indeﬁnite might perform better sparse kernels rank-one update allow inference hidden relationships. observe indeﬁnite performs comparably usps examples relatively easy classiﬁcation problems. expected classiﬁcation using tangent distance outperforms classiﬁcation simpson score mentioned above simpson score cheaper compute. also note table summary statistics various data sets used experiments. usps data comes usps handwritten digits database data sets taken repository. refers simpson kernel one-sided tangent distance kernel sigmoid kernel. training testing sets divided randomly. notice simpson kernels mostly highly indeﬁnite one-sided tangent distance kernel nearly positive semideﬁnite. sigmoid kernel highly indeﬁnite depending parametrization. statistics sigmoid kernels refer optimal kernel parameterized cross validation indeﬁnite svm. spectrums based full kernel i.e. combining training testing data. documented classiﬁcation results usps data perform multi-classiﬁcation perform binary classiﬁcation. classiﬁcation data sets sigmoid kernels diﬃcult indeﬁnite technique outperforms least measures across three data sets. using time linear gaussian kernels usps data compare classiﬁcation performance using regular penalized kernel learning problem section call perturbsvm here. also test techniques positive semideﬁnite kernels formed using noisy usps data sets pixel normalizing case perturbsvm seen optimally denoised support vector machine classiﬁcation. cross-validate training test independent group examples used experiments above. optimal parameters classiﬁcation unperturbed data used train classiﬁers perturbed data. results summarized table results show perturbsvm performs least well almost cases. expected noise decreased generalization performance experiments. except usps---gaussian example value selected highest possible test perturbsvm outperforms least measure; implies support vectors perturbed improve classiﬁcation. overall zero moderate noise present perturbsvm improve performance regular shown. much noise present however modiﬁed uniform noise normalized performance techniques comparable. table indeﬁnite performs favorably highly indeﬁnite simpson kernels. performance comparable nearly positive semideﬁnite one-sided tangent distance kernel. comparable performance sigmoid kernels consistent indeﬁnite across data sets. performance measures accuracy average algorithms data sets created randomly perturbing four usps data sets used above. average results standard deviation displayed figure semilog scale expected accpm converges much faster higher precision iteration requires solving linear program size gradient projection method converges faster beginning stalls higher precision however iteration requires rank update eigenvalue decomposition. table performance measures usps data using linear gaussian kernels. unperturbed refers classiﬁcation original data noisy refers classiﬁcation data perturbed uniform noise. perturb perturbs support vectors improve generalization. however performance lower techniques presence high noise. figure convergence plots accpm projected gradient method random subsets usps-ss-- data accpm converges linearly higher precision gradient projection method converges faster beginning stalls higher precision. accpm compare siqclp method chen figure shows total runtime average iteration runtime varying problem dimensions example usps data simpson kernel. experiments averaged random data subsets tolerance duality gap. projected gradient method increasing increases number iterations converge; notice average time iteration vary siqclp also requires iterations converge higher however average iteration time seems less higher clear pattern seen varying note number iterations required varies widely function chosen kernel stepsize. results accpm siqclp shown dimensions respectively suﬃciently demonstrates projected gradient method eﬃcient. accpm clearly suﬀers complexity analytic center problem iteration. however improvements made siqclp implementation using regularized version eﬃcient solver solve problem rather mosek. siqclp also useful makes connection indeﬁnite formulation multiple kernel learning. observed experiments duality found siqclp tighter upper bound duality used projected gradient method. could potentially used create better stopping condition however complexity derive tighter duality much higher compute current figure total time versus dimension average time iteration versus dimension using projected gradient accpm indeﬁnitesvm siqclp number iterations convergence varies smallest dimension largest dimension example uses simpson kernel usps data. used improve generalization performance potentially noisy mercer kernels well extend kernel methods support vector regression one-class support vector machines. give provably convergent algorithms solving problem relatively large data sets. initial experiments show method fares quite favorably compared techniques handling indeﬁnite kernels framework limit provides clear interpretation heuristics. grateful m´aty´as sustik rank-one update eigenvalue decomposition code jianhui chen jieping siqclp matlab code. would also like acknowledge support grant dms- grant ses- career award peek junior faculty fellowship howard wentz junior faculty award.", "year": 2008}