{"title": "Learning Mixtures of Submodular Shells with Application to Document  Summarization", "tag": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "abstract": "We introduce a method to learn a mixture of submodular \"shells\" in a large-margin setting. A submodular shell is an abstract submodular function that can be instantiated with a ground set and a set of parameters to produce a submodular function. A mixture of such shells can then also be so instantiated to produce a more complex submodular function. What our algorithm learns are the mixture weights over such shells. We provide a risk bound guarantee when learning in a large-margin structured-prediction setting using a projected subgradient method when only approximate submodular optimization is possible (such as with submodular function maximization). We apply this method to the problem of multi-document summarization and produce the best results reported so far on the widely used NIST DUC-05 through DUC-07 document summarization corpora.", "text": "introduce method learn mixture submodular shells large-margin setting. submodular shell abstract submodular function instantiated ground parameters produce submodular function. mixture shells also instantiated produce complex submodular function. algorithm learns mixture weights shells. provide risk bound guarantee learning large-margin structured-prediction setting using projected subgradient method approximate submodular optimization possible apply method problem multi-document summarization produce best results reported widely used nist duc- duc- document summarization corpora. nonnegative weights also submodular. long history operations research game theory econometrics electrical engineering still beginning studied machine learning variety tasks including sensor placement structure learning graphical models document summarization social networks problem learning submodular functions also recently addressed. example asked make polynomial number queries unknown submodular function constructs possible?. among many results show even adaptive queries monotone functions canlearn better approximation given ﬁxed submodular function. similarly addressed submodular function learning problem learning theory perspective given distribution subsets. provide strong negative results including approximate setting within constant factor. general therefore learning submodular functions hard. incremental value decreases context considered grows submodular functions share number properties common convex concave functions including wide applicability generality multiple options representation closure number common operators example weighted collection wifi submodular learning conical mixture ruled approximate guarantees. mixtures might span large submodular functions depending diversity component set. call problem learning submodular mixtures. depends local parts could employ dynamic programming integer programming algorithm optimal solution. another approach explore combinatorial structures upon eﬃcient algorithms available. examples sort include hungarian method maximum weighted bipartite matching chu-liu-edmonds algorithm optimal branchings. alternatively sequel could resort approximate solutions approximation guarantees. beneﬁts using submodular score functions. first submodular functions natural expressive capability modeling decisions beyond local linear interactions among parts. submodular functions allow global direct interactions structured object unlike score functions must decompose way. second expressive power require prohibitive computation would arbitrary score function. reason submodular maximization problem even many constrained settings solved eﬃciently near-optimally rigorous performance guarantees every valid value since weights non-negative score function also submodular. call submodular shell abstracting submodular functions characterized ground score function called submodular shell mixture shell component mixture. note parameters associated particular ground parameters shell apply pair ground value. ground parameters associated particular ground parameters shell apply ground-set vector pair shell form instantiated. might learn example particular value produces good shell instantiation shell particular sense submodular shell might seen form structured submodularity. moreover might wifαi based data consisting training tuples form s)}t used objective involves instantiated mixture shells. note here training tuples consist sequence diﬀerent ground sets goal training tuples given ﬁnite conical weights optimize objective. course training ground sets identical learning submodular shell mixtures reduces learning submodular mixtures. practice however training ground sets usually identical. e.g. extractive document summarization training data could documents corresponding human summaries ground sets sentences constitute documents identical training samples. particular docuintroduce method whereby submodular shell mixtures learnt max-margin structuredprediction setting provide risk-bound guarantee approximate submodular maximization apply method extractive document summarization problem mentioned abstract yields extremely good results. subset yxxx valid. example sentence parse trees subset possible parse trees yxxx might valid. oﬀers little solace however yxxx score function measures good mixture truncation functions. moreover sums concave functions applied cardinality functions represented mixtures truncation functions functions expressed modular function nonnegative linear combinations truncation functions. submodular functions introduced document summarization also seen mixture submodular shells components either coverage diversity shell instantiated particular document. general using rich enough families shell components submodular shell mixture could expressive representing large family submodular functions. moreover another advantage using submodular shell mixture representations that since assume component given component weights unknown learning problem addressed using well-established methods. learning shell mixtures apply learnt mixture structured problems even diﬀerent underlying ground sets. might many ways learning shell mixtures paper take large-margin approach standard structured prediction. words want minimize risk making predictions using submodular shell mixture score function. course standard maximization learning structured prediction approximate case since maximizing submodular functions np-hard moreover need identify valid loss function dose violate submodularity. wish also ensure learnt parameters quality guarantees done below. discuss learning process section introduce learning problem improve clarity. given training instances yyy)}t drawn independently distribution pairs restricted yxxx pair corresponds ﬁnite ground vxxx parameters βxxx βxxx also ﬁnite collection submodular shells {fαi}m instantiated submodular function fαixxx submodular function. also given loss function xxxyyy measures loss predicting ˆyyy yxxx true label yxxx. empirical risk minimization problem conical mixture risk e∼dxxxyyy)] minimized. important realize learnt mixture coeﬃcients shells instantiated weighted submodular functions. fairly rich subclass submodular functions instantiated submodular shell mixtures components simpler submodular shells. consider example truncation functions mixtures represent many submodular functions interest. fact many submodular functions written mixtures truncation functions. example coverage type functions canonical examples submodular functions written submodular mixtures. presolved eﬃciently. problem term precisely matches prediction problem whose parameters trying learn also additional term corresponding loss. tractability therefore depends tractability prediction problem also form loss functions. using submodular shell mixtures score function approximate inference performance guarantee. therefore must perform approximate inference problem well. thus general refer approximate learning. since form inference dominant subroutine many learning algorithms structured prediction natural good approximate inference techniques make learning problem tractable. learning submodular shell mixtures inevitably must approximate learning since using expressive class score functions need decompose inference problem intractable exactly. using approximate inference drop-in replacement exact inference learning however could mislead learning algorithm result poorly learnt models. analyzed pointed approximate learning could fail even approximate inference method approximation guarantees. general therefore problematic assume arbitrary choice approximate inference method lead useful results learning method expects exact feedback. choosing compatible inference learning procedures therefore crucial. following leverage approximation guarantees submodular optimization performance approximate learning submodular shell mixtures bounded way. possible bounding investigate degree approximate parameters would obtained exact learning since parameters oﬀer little utility good prediction cannot made them. alternatively focus quality prediction obtained approximately learned model. particular seek bound risk gap. diﬀerence expected loss predictions approximate scheme exact methods. many algorithms proposed large margin learning problem including based exponentiated gradient method dual extragadient method cutting-plane algorithm subgradient descent method adopt subgradient descent algorithm learn submodular shell mixtures illustrated algorithm vectors takes dimension-wise maximum note preserve submodularity component weights must non-negative. thus simply project weights non-negative orthant whenever updates easy show updates followed projection onto non-negative region aﬀect convergence correctness algorithm point projected back convex moved closer every point including optimal points. algorithm needs solve note modular submodular ρ-approximate inference algorithm also apply loss augmented inference near-optimal solution eﬃciently. however mentioned above approximate inference algorithm used learning algorithm good approximation score might suﬃcient possible learning fail even rigorous approximate guarantees hand ratliﬀ show subgradient algorithm robust approximate settings risk experienced training γ-approximate subgradients bounded. risk predictions using models learnt exact inference. thus better approximation less additional risk using approximate lai. exact inference used additional risk shrinks zero. note theorem applies loss function score function necessary submodular. addition assumptions made better bound might possible additional risk shrinks zero grows. hand objective monotone submodular hence must analyze whether using good approximate inference lead good approximate learning case. note types approximation inference algorithms namely undergenerating overgenerating approximations. ating approximation algorithm always solution rithm loopy belief propagation instances undergenerating approximation algorithms. relaxation methods e.g. linear programming relaxation overgenerating approximation algorithms. note undergenerating algorithms usually produce solutions within feasible region problem. overgenerating algorithms hand generate solutions might outside feasible region. therefore solutions found overgenerating algorithms sometimes need mapped back feasible region order produce feasible solution during approximation guarantee longer holds cases learning submodular shell mixtures particularly interested undergenerating algorithms since greedy algorithm undergenerating algorithms oﬀers near-optimal solutions submodular maximization certain constraints generalization bounds approximate learning cutting-plane algorithms either undergenerating overgenerating inferences shown subgradient descent methods generalization analysis available overgenerating cases know generalization analyses available approximate learning undergenerating subgradient methods. oﬀer risk bounds approximate learning undergenerating subgradient methods. practice moreover approximation factor greedy algorithm submodular maximization usually close could expect little additional risk using algorithm approximate inference learning submodular shell mixtures. submodular mixtures could applied many structured prediction problems practical interest. paper apply submodular shell mixture learning extractive document summarization case study. paper show rouge score submodular score used pyramid method manual evaluation metrics used recent summarization track also monotone submodular. remaining question design good submodular function summarization. bilmes proposed class submodular functions models coverage well diversity summary. paper generalize class submodular functions propose submodular shell mixtures document summarization. produce variety submodular shells. parameter submodular shell takes form document given rewards sentence computed diversity shell component instantiated submodular function measures diversity summary particular document. extractive document summarization seen subset selection problem given ground sentences task extractive document summarization selecting subset sentences best represents whole document. words models quality summary. known problem submodular maximization subject knapsack constraints np-complete however monotone submodular solved eﬃciently near-optimally theoretical guarantee greedy algorithms always force submodular leading objective function optimized well might hand poorly represent given problem. attractive property submodularity like convexity continuous domain arises naturally many applications. applications document summarization. pointed many well-established methods including widely used maximum margin relevance method actually correspond submodular optimization. moreover shown commonly used rouge score automatic summarization evaluation monotone submodular giving evidence submodular modular loss function objective function loss augmented inference along submodular shell mixture score function resulting objective function submodular function plus supermodular function. algorithm available approximately optimize submodular function supermodular function performance guarantees usually exist algorithms therefore using one-minus-rouge loss function greedy algorithm longer provides near-optimal solution applied non-submodular objective risk bound shown theorem longer holds. n-grams occur documents number times n-gram occurs documents non-negative weight n-grams covered human reference summary. instead counting respect reference summary rouge counts n-grams candidate summary w.r.t. complement reference summaries. intuitively want summary cover many reference n-grams possible high rouge-score; similar large overlapping little possible n-grams human references. sense rouge measures portion many n-grams complement reference n-grams covered comparing summaries size smaller rouge better. best case i.e. human reference itself rouge equal obviously poor summary would also rouge equal empty summary. worth noting rouge makes sense comparing summaries close budget. fortunately summarization algorithms consume every budget order consume much information possible budget constraint. summaries produced rouge oﬀers fair indicator quality smaller loss value larger number reference n-gram overlaps therefore better summary. form well known submodular facility location function deﬁned partition ground set. thus call clustered facility location. summary contains multiple elements cluster element largest singleton reward regarded representative cluster reward representative counted ﬁnal score. diminishes returns choosing elements cluster therefore c-facility submodular. widely used evaluation criteria summarization rouge score basically submodular function counts n-gram recall rate human summaries. candidate summary number times n-gram occurs reference summary frouge-n submodular shown cannot used loss function since basically measures accuracy rather loss. greedy algorithm solving summarization problems greedy nature algorithm always outputs summary candidates whose costs close budget therefore rouge serve reasonable surrogate loss function learning submodular shell mixtures summarization tasks. major advantage using rouge loss function course algorithmic. submodular learning analysis theorem relies fact ρ-approximation algorithm available loss augmented information. since submodular score function summarization objective submodular loss function submodular. fortunately similar frouge-n proposed loss summarization rouge also monotone submodular therefore submodular shell mixture learning summarization exactly budgeted submodular maximization problem eﬃcient near-optimal algorithms available. consequently theoretical analyses section apply. recently guestrin studied linear submodular bandits problem online learning setting optimizing general class feature-rich submodular utility models diversiﬁed retrieval theoretical result based assumption award function special form. raman also propose online learning model algorithm learning rankings exploiting feedback maximize submodular utility measure. approach analysis could also applied online setting focus analysis paper batch learning. submodular utility models however seen special instances submodular shell mixtures. closely related work large-margin learning submodular score functions extractive document summarization studied. representation submodular score function again turns special case submodular shell mixture. moreover submodular shell mixture framework general ﬂexible framework proposed although authors claim approach applies submodular summarization models many submodular functions useful summarization linear either pairwise similarity singleton importance. example diversity reward function concave function sums singleton rewards even using linear model singleton rewards score function non-linear parameters since weights linearly extracted thus algorithms apply. viewing feature input submodular component hand preserves linearity parameters whereas algorithm applies. moreover representing score function using submodular shell mixture expressive shown section cutting plane algorithm one-minusrouge f-measure loss function used learn model weights. loss augmented inference greedy algorithm introduced approximately optimize objective. objective function however submodular non-submodular loss function use. therefore inference longer guaranteed near-optimal performance approximate learning cutting plane algorithm longer guaranteed also sipos apparently neglect fact learned similarity non-negative necessary ingredient preserve submodularity learned score function. simple ensure constrain weights non-negative submodular shell mixtures project weights non-negative orthant evaluated approach nist’s data demonstrate results generic query-focused summarization. data created national institution standard technology evaluation standardized benchmark evaluations document summarization researchers continue publish results data sets. generic summarization tasks. used duc- training submodular shell mixture learning. total document clusters duc- task therefore training examples total. used submodular shell mixture ﬁdelity components task. particular function pairwise sentence similarity sentence used three types sentence similarities. ﬁrst cosine similarities unigram bigram tf-idf vectors respectively. third similarity cosine similarity vector generated latent semantic analysis. similarity measures thus ﬁdelity components total. used algorithm rouge-like loss learn submodular shell mixture. rouge- results shown table result learned submodular shell mixture signiﬁcantly outperforms previous reported results. note experiment used non-standard setup duc- data divided training development test sets documents test set. therefore results reported directly comparable results follow standard evaluation setup. since summarization evaluations concentrated query-focused summarization. tested approach summarization duc- duc- duc- data. particular used duc- training duc- task duc- training duc- task duc- training duc- task. used diversity components clustered facility location components ﬁdelity components form submodular shell mixture query-focused summarization. three clusterings diﬀerent numbers clussingleton rewards used query-independent query-dependent singleton rewards. queryindependent reward simply summation pairwise similarities elements query-dependent reward simply used number terms sentence overlaps query weighting used therefore total clustered facility location components. used three curvatures diversity components gives diversity components total. additional ﬁdelity component components total. compared results reported literature know approach achieves best results reported duc- duc- duc-. paper propose notion learning submodular shells abstract submodular functions instantiated submodular functions given input structured prediction task. given training instances subgradient descent learn mixture coeﬃcients submodular shells instantiated weighted submodular functions. show submodular shell mixture expressive risk learning bounded approximate inference possible. applied task document summarization approach achieves best results reported standardized benchmark takes query-focused extractive summarization tasks.", "year": 2012}