{"title": "Faster Training of Very Deep Networks Via p-Norm Gates", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "A major contributing factor to the recent advances in deep neural networks is structural units that let sensory information and gradients to propagate easily. Gating is one such structure that acts as a flow control. Gates are employed in many recent state-of-the-art recurrent models such as LSTM and GRU, and feedforward models such as Residual Nets and Highway Networks. This enables learning in very deep networks with hundred layers and helps achieve record-breaking results in vision (e.g., ImageNet with Residual Nets) and NLP (e.g., machine translation with GRU). However, there is limited work in analysing the role of gating in the learning process. In this paper, we propose a flexible $p$-norm gating scheme, which allows user-controllable flow and as a consequence, improve the learning speed. This scheme subsumes other existing gating schemes, including those in GRU, Highway Networks and Residual Nets as special cases. Experiments on large sequence and vector datasets demonstrate that the proposed gating scheme helps improve the learning speed significantly without extra overhead.", "text": "abstract—a major contributing factor recent advances deep neural networks structural units sensory information gradients propagate easily. gating structure acts control. gates employed many recent state-of-the-art recurrent models lstm feedforward models residual nets highway networks. enables learning deep networks hundred layers helps achieve record-breaking results vision however limited work analysing role gating learning process. paper propose ﬂexible p-norm gating scheme allows usercontrollable consequence improve learning speed. scheme subsumes existing gating schemes including highway networks residual nets special cases. experiments large sequence vector datasets demonstrate proposed gating scheme helps improve learning speed signiﬁcantly without extra overhead. deep neural networks becoming method choice vision speech recognition deep nets represent complex data efﬁciently shallow ones non-linear hidden layers deep networks theoretically model functions higher complexity nonlinearity however learning standard feedforward networks many hidden layers notoriously difﬁcult likewise standard recurrent networks suffer vanishing gradients long sequences making gradient-based learning ineffective. major reason many layers non-linear transformation prevent data signals gradients ﬂowing easily network. forward direction data outcome change data signals lead change outcome leading poor credit assignment problem. backward direction large error gradient outcome propagated back data signals. result learning stops prematurely without returning informative mapping data outcome. several effective methods tackle problem. ﬁrst line work non-saturated nonlinear transforms rectiﬁed linear units whose gradients non-zero large portion input space. another approach also increases level linearity information propagation gating gates extra control neural units part information pass channel. learnable played important role state-of-the-art feedforward although details architectures differ share common gating scheme. speciﬁcally activation vector size computational step index hidden layer feedforward networks time step recurrent networks. updating follows following rule non-linear transformation gates point-wise multiplication. part previous activation vector copied vector. thus update nonlinear part linear part nonlinear part keeps transforming input complex output whilst linear part retains part input pass across layers much easier. linear part effectively prevents gradient vanishing even hundreds layers. example highway networks trained layers previous impossible feedforward networks. updating rule opens room study relationship gates limited work direction. existing work includes residual networks hence plays role residual. lstm explicit relation gates. work reported leads less parameters compared lstm. paper focuses later aims address inherent drawback linear relationship. particular approaches rate approaches rate prevent information passing early. propose ﬂexible p-norm gating scheme following relationship holds gated recurrent units sections ii-a ii-b respectively. notational convention bold lowercase letters vectors capital letters matrices. sigmoid function scalar deﬁned slight abuse notation vector denote vector rule applies function vector operator used denote element-wise multiplication. feedforward networks recurrent networks index denote computational steps layers feedforward networks time step recurrent networks. shown fig. architectures quite similar except extra input available step. training deep feedforward networks remains difﬁcult several reasons. first number parameters grows depth network leads overﬁtting. second stack multiple non-linear functions makes difﬁcult information gradients pass through. highway networks modiﬁcations resolve problems parameters shared layers leading compact model activation function modiﬁed adding sigmoid gates information lower layers pass linearly through. fig. illustrates highway network layer. ﬁrst modiﬁcation requires hidden layers hidden units bottom layer identical standard feedforward networks. second modiﬁcation deﬁnes candidate hidden state usual non-linear transform gates sigmoid functions independent summed unit element-wise e.g. latter option used paper highway networks part called carry behavior makes information layers pass easily network. behavior also allows back-propagation compute gradient directly input. effect networks deep recurrent neural networks recurrent neural network extension feedforward networks variable-length input sequence output sequence allows self-loop connections shared parameters across steps sequence. vanilla rnns activation function current input previous hidden state highway network feedforward neural network maps input vector outcome standard feedforward network consists hidden layers activation layer non-linear function lower layer regardless long achievable since often modelled logistic function. activation ﬁnal hidden layer loads information past without forgetting. note imagenet winner residual network special case used vector-data classiﬁcation tasks evaluate highway networks p-norm gates. used hidden layers dimensions each. models trained using standard stochastic gradient descent epochs mini-batch datasets used large datasets miniboone particle identiﬁcation sensorless drive diagnosis ﬁrst binary classiﬁcation task data taken miniboone experiment used classify electron neutrinos muon neutrinos second dataset extracted motor current different classes. table reports data statistics. training curves fig. shows training curves training sets. loss function measured negative-log likelihood. training costs decrease converge much faster ones miniboo dataset training needs epochs reach nats needs nearly epochs reach value. pattern similar sensorless dataset training loss epochs losses reach value epochs respectively. training largely unsuccessful report here. prediction prediction results validation sets reported table evaluate learning speed report number training epochs reach certain benchmark different values also report results epochs. miniboo dataset reach benchmark f-score needs https//archive.ics.uci.edu/ml/datasets/miniboone+particle+identiﬁcation https//archive.ics.uci.edu/ml/datasets/dataset+for+sensorless+drive+diagnosis gated recurrent units gated recurrent unit extension vanilla rnns illustration) suffer vanishing gradient problem. step compute candidate hidden state follows reset gate controls information previous state candidate hidden state. close previous hidden state ignored candidate hidden state reset current input. grus update hidden state using rule difference gate function current input used. linear relationship gates assumed relationship enables hidden state previous step copied partly current step. hence linear interpolation candidate hidden state previous hidden states. prevents gradients vanishing captures longer dependencies input sequence. remark highway networks grus considered simpliﬁed versions long short-term memory linear interpolation consecutive states grus less parameters. empirical experiments revealed grus comparable lstm efﬁcient training plays updating role plays forgetting role computational sequence. since relationship linear gets closer closer rate. learning gates might become specialized discriminative same-rate convergence block information lower layer passing high rate. learning speed suffer result. dynamics relationship gates function interesting. increases amount information passing linear part. concrete linear gates relationship portion information passing step. passing portion visualization fig. illustrates channels gates open layers different value randomly chosen data instance test miniboo. recall sec. ii-c control amount information non-linearity part linearity part respectively clear larger value gates open. interestingly values channels gate larger ones gate values model seems prefer linearity part. interestingly gradual change gates layers although gates layers directly linked. lower layers gates uniform informative near evaluate p-norm grus compare results different values task language modeling character-level modeling level attracted great interest recently generalizability language usage compact state space ability capture sub-word structures. speciﬁcally given sequence characters models probability next character quality model measured bits-per-character test −logp model trained maximizing validation. sentences length longer characters used ﬁrst characters. model trained hidden units epochs sentences mini-batch. results fig. reports training curves results validation epochs. clear ﬁgures model performs best among choices learning speed model quality. give concrete example indicated horizontal lines figs. learning reaches nats epoch learning reaches training loss model quality test data model achieves bits-per-character epochs faster model epochs. demonstrated highway nets grus training faster however question whether larger value always implies better results faster training? example activation ﬁnal hidden layer contains copy ﬁrst layer candidate states makes magnitude hidden states properly controllable deeper networks. evaluate effectiveness conducted experiments miniboo dataset networks depths found model works well values hidden layers. number layers increase model works well suggests proper control hidden state norms needed deep networks widely open gates. less gate parameter set. open direction modify internal working gates make informative assist regularizing hidden states following ﬁndings sec. iii-c also recent work zhang delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation proceedings ieee international conference computer vision hinton deng dahl a.-r. mohamed jaitly senior vanhoucke nguyen sainath deep neural networks acoustic modeling speech recognition shared views four research groups signal processing magazine ieee vol. kumar irsoy bradbury english pierce ondruska gulrajani socher anything dynamic memory networks natural language processing arxiv preprint arxiv. trends machine learning vol. glorot bordes bengio deep sparse rectiﬁer networks proceedings international conference artiﬁcial intelligence statistics. jmlr w&cp volume vol. merriënboer gulcehre bahdanau bougares schwenk bengio learning phrase representations using encoder-decoder statistical machine translation emnlp still partial piece-wise linearity desirable nonlinearity complex functions. time helps prevent activation units saturated gradients vanishing making gradient-based learning possible deep networks main idea p-norm gates allow greater data signals gradients many computational steps. leads faster learning demonstrated experiments. remains less clear dynamics relationship linearity gate nonlinearity gate hypothesize that least earlier stage learning larger gates help improve credit assignment allowing easier gradient communication outcome error unit. since gates learnable amount linearity function approximator controlled automatically. paper introduced p-norm gates ﬂexible gating scheme relaxes relationship nonlinearity linearity gates state-of-the-art deep networks highway networks residual networks grus. p-norm gates make gates generally wider larger thus increase amount information gradient passing networks. demonstrated p-norm gates major settings vector classiﬁcation tasks highway networks sequence modelling grus. extensive experiments consistently demonstrated faster learning caused ways control linearity relationship linearity gate nonlinearity gate possible scheme could monotonic relationship gates also remains open validate idea lstm memory cells lead compact model", "year": 2016}