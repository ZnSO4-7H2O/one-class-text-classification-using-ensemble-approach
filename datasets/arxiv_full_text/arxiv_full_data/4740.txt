{"title": "A Learning Algorithm based on High School Teaching Wisdom", "tag": ["cs.AI", "cs.LG"], "abstract": "A learning algorithm based on primary school teaching and learning is presented. The methodology is to continuously evaluate a student and to give them training on the examples for which they repeatedly fail, until, they can correctly answer all types of questions. This incremental learning procedure produces better learning curves by demanding the student to optimally dedicate their learning time on the failed examples. When used in machine learning, the algorithm is found to train a machine on a data with maximum variance in the feature space so that the generalization ability of the network improves. The algorithm has interesting applications in data mining, model evaluations and rare objects discovery.", "text": "learning algorithm based primary school teaching learning presented. methodology continuously evaluate performance network train examples repeatedly fail until examples correctly classiﬁed. empirical analysis data show algorithm produces good training data improves generalization ability network unseen data. algorithm interesting applications data mining model evaluations rare objects discovery. large uniform random sampling likely pick examples mostly seen class regions close mean gaussian distributions representing classes. logic selection that machine trained mean distributions likely well rest data clustering around acceptable many cases sparse representation examples within class unlikely examples collected random sampling process. problem addressed case speech recognition using turing’s estimate diﬀerent levels sampling based reestimate sparseness data used identify good training data. many algorithms preparing good training data found literature optimal data selection training neural network continues area active research eﬃciency machine learning algorithm reﬂected generalization ability deﬁned ability algorithm give accuracy produces training data similar unseen test data. however generalization ability known depend explicitly quality training data thus selection good sample training network plays crucial role evaluation. random sampling data validation data estimate quality learning popular machine learning community. data size philip department physics st.thomas another popular approach prepare good training data based wilson’s editing scheme k-nn classiﬁer used identify outliers incorrectly labeled examples training sample scheme sample retained training data k-nn classiﬁer able correctly predict class sample even removing training data. wilson’s editing basis subsequent development editing algorithms multi-edit citation editing supervised clustering editing etc. paper alternate optimal data selection criteria proposed which start null training instead removing failed entries training sample identify objects test data training data. contrast objects pass classiﬁcations retained test sample. outcome opposite wilson’s editing scheme. here instead clean training sample training done outliers boundary examples data. prevent network incorrectly learning outliers suﬃcient good examples added training data network correctly predicts class object even outliers present training data. argument based information theory perspective training sample carry information relating possible variants real world situation. proposed scheme much common primary schooling considered decisive learning period educational stream student. teachers stage give personal attention students compensate learning inabilities repeatedly training failures. repeated training believed enable student gradually comprehend underlying concepts learning progress. continued evaluation basis questionnaire dictations used teacher estimate learning curve individual students. information used select topics emphasized next teaching cycle. paper similar procedure optimal selection training examples machine learning algorithm attempted. understood machine learning algorithms make predictions basis ﬂexible evaluation criteria usually function connection weights favor possible outcomes given input pattern. training process adjusts criteria reduce overall error predictions network global minimum. although common practice ignore marginal diﬀerences evaluations simply consider maximum score possible outcome possible estimate score values conﬁdence measure predictions example bayesian classiﬁers every prediction associated bayesian posterior probability gives quantitative estimate conﬁdence prediction made. paper discusses optimal training data selection algorithm diﬀerence boosting neural network scheme readily implemented. dbnn software source code available bayesian learning though powerful computationally intensive conditional dependence parameters posterior computations. naive approach assume conditional independence features total probability computed product individual probabilities. known naive bayes classiﬁcation. tutorial introduction naive bayes major limitation naive bayes completely fails even typical problem. would like whether possible impose conditional independence data without loss accuracy posterior computation. answer positive explain consider problem independent input features either although inputs independent variables class ﬁxed value selected feature imposes constraint value feature take. thus class ﬁrst feature value feature take value vice versa. represented words replace independent input features gate pair variables formed combining it’s dependent variables feature become conditionally independent overall posterior probability thus computed product individual probabilities. interestingly coming non-binary real value features criteria applied replace binary values probability density distribution features. computation probability density simpliﬁed binning feature vector give histogram distribution. histogram approximates true distribution size small. however number bins can’t arbitrarily chosen. elementary statistics tells binning approximate distribution suﬃcient number samples bin. thus many practical situan represents tions taking size number discrete values taken variable good choice. another possibility start small number bins increase number until classiﬁcation seems reasonably separate classes data. usually done features continuous implementation code straightforward. initially deﬁnes size feature constructs memory model count= bins. feature vector training sample read depending fall count incremented. also linked separate bins associated target classes data. bins matrix structure rows representing features column representing histogram distributions bins given cin. since class always known training sample depending value features appropriate bins class incremented. repeated examples training data read. expressed features conditionally independent partitions. thus possible compute likelihood class produce feature value given intersection binning procedure makes diﬃcult deﬁne prior domain knowledge alone. distributions distinct prior taken unity. many practical situations overlap distribution classes. dbnn thus estimates prior data during training process replacing weight function updated training progress. given bayes theorem takes form represents number possible classes. initially uniform small value assumed iteration whenever classiﬁcation fails incremented small value. increment weight incorrectly classiﬁed example feature value falling given represents computed bayesian probability actual class incorrectly represented class. since computed shown value always positive bounded upper lower limits number iterations increment fraction independently chosen. typical value iterations increment factor used paper. boosting weightage function proportional diﬀerence probabilities gives name diﬀerence boosting network. since dbnn uses bayesian probability central rule decision making computed posterior probability gives conﬁdence network predictions. events equally likely posterior outcomes hence low. class problem means class problem examples described paper posterior probability ranges hereafter referred cause. ﬁgure overlapping regions likelihood examples blue higher likelihood examples shown red. result happen examples incorrectly classiﬁed. available observation objects prior alone cannot compensate correct classiﬁcation examples region. even observations large number examples blue region class dominate bayesian posterior estimation result incorrect classiﬁcation examples. overcome suppress prominence blue examples likelihood estimation diﬀerences features become significant bayesian classiﬁcation. means selection criteria deﬁned minimum number examples class required correct classiﬁcation objects class added training sample training demonstrate better performance data random samples whole data used training classiﬁer. ideal training data example figure thus blue examples removed diﬀerences features become signiﬁcant classiﬁcation. although might appear straightforward sampling comes multiple feature data addition sample multiple effects selection could become nontrivial optimisation problem. context argued primary school teaching method going step time compensating complications iteration converge optimal training data ﬁnite number rounds. attempted odsa algorithm described next. figure distribution samples shown tails partially overlapping other. clean examples sample represented green region blue regions represent samples belonging diﬀerent classes feature values. dominance objects shown blue likely objects region classiﬁed class blue objects resulting wrong identiﬁcation objects shown red. bayesian classiﬁers ability marginalise outliers apply occam’s razer principle default. need optimal training data? train entire data marginalise outliers? appears straightforward solution works case outliers. outliers issue here. sparsely represented examples? illustrated ﬁgure long distributions distinct possible make correct predictions call clean examples. ﬁgure green regions represents distribution clean samples data. classiﬁed without error high conﬁdence using bayesian theorem estimate posterior probability prodfigure projection optimally selected training samples class problem feature space. test data samples shown blue optimally selected representative training samples shown green pink dots. seen selected training examples mostly boundaries stated text. examples class bounded lower limit round bounded lower limit means number iterations increase ﬂuctuations likelihood decreases asymptotically algorithm eventually converge. procedure repeated unseen samples correctly classiﬁed. noted training data need give classiﬁcation accuracy more. because process removed diﬃcult boundary examples outliers test data added training data. odsa algorithm given algorithm picking example fails maximum conﬁdence test data adding training sample crucial step here. iteration example added training data failed example looks similar object another class. process collect boundary examples test data training data. outliers incorrect labels? outliers incorrectly labeled examples added hood estimate classes even subtle diﬀerences features help correct classiﬁcation data. consider classiﬁcation problem label object possible predeﬁned classes basis observed pattern features. outcomes equally likely conﬁdence prediction generally called prior. start estimates prior randomly pick data example class initial training sample train network. since confusing examples training accuracy training data close trained network used predict class objects data training samples taken. samples used training similar entire data test data give classiﬁcation accuracy comparable training data. however practice fraction test examples correctly predicted. stage search predicted samples select example class added training data. criteria selection identify failures. since competing example incorrectly identiﬁed examples previous round likelihood examples halved round. failures round likely caused examples. criteria failed sample class added training data. every step maximum samples removed test data added training data. third round alteration likelihood addition result optimally selected training data initialization; remove example class test data training data; gives training sample examples start with. train classiﬁer training data test classiﬁer test data errors prediction training data cause good examples classiﬁed wrongly network. algorithm subsequent iterations failed good examples looks similar training data. repeated likelihood class good examples eventually marginalise eﬀect outliers fewer good examples. major diﬀerence proposed method wilson’s editing scheme. teacher-student scenario teacher would correct misunderstanding causes student come wrong conclusions. typical example training samples selected class problem decision surface nonlinear shown figure important point noted training done optimally selected samples. rest data unseen network training epochs. brief regulating likelihood estimates sample selection method computing prior diﬀerence boosting algorithm identiﬁes examples used train data much better would possible random selection perhaps wilson’s editing algorithm good examples used training. like algorithm learning curve produced network also similarity human learning curves. practical problems boundaries classes would fuzzy become difﬁcult even human experts conﬁdently classify examples regions. need vagueness class deﬁnitions; rather could feature estimation inaccuracies regions feature space. example consider automated classiﬁcation deep images survey project. unlike conditions laboratory astronomer case infer classify objects basis appearance increase iterations rapid decline prediction accuracy training sample similar increase prediction accuracy test sample time major boundaries classes identiﬁed network accounts improved prediction accuracies test data. learning progresses sudden spikes appear learning curve test samples causing prediction accuracies fall intermittently. mostly caused addition incorrectly labeled objects training data. since algorithm picks objects failed maximum conﬁdence turns incorrectly labeled objects picked class boundaries deﬁned. however long incorrect labeling intentional number likelihood correct examples increases bayesian algorithm start ignoring network regains classiﬁcation accuracy subsequent iterations. training accuracy represented pseudo-learning curve however continue remain time almost incorrect labels getting gathered words level ineﬃciency network algorithm rather caused incorrect labels assigned dataset prediction results compared. substantiated increasing accuracy test data much larger training data. although test accuracy continues rise close noted spikes reappear second time causing degradation accuracy test sample. examining data selected times showed mostly network’s attempt redeﬁne boundary classes based updated training data. iterations continue network eventually predict samples test data correctly time boundary examples false label examples collected training sample. result accuracy optimal training data less equal test data. realistic estimate accuracy training selected figure learning curves typical classiﬁcation problem high degree uncertainty probable incorrect labeling training sample shown. pseudo-learning curve shown green real-learning curve shown red. smooth curve plotted show trends epoch learning. note spikes learning process. example million objects test sample thousand objects training sample last iteration shown. convenience divide learning curves groups namely pseudo-learning curve real-learning curve. pseudo-learning curve learning accuracy obtained training sample real test sample. initially training examples pseudo-learning level close reallearning level much lower. gradually failed examples added training sample classiﬁcation accuracy training data start decline test data start climb. figure illustrates learning curve case data high degree uncertainty labeling well estimation feature parameters. typical many practical situations space research details relevant current discussion. here landsat satellite data repository used illustrate advantage scheme. database consists multi-spectral values pixels neighborhoods satellite image classiﬁcation associated central pixel neighborhood. predict classiﬁcation given multi-spectral values. sample database class pixel coded number represented numbers respectively referring data show optimal data selection algorithm used generate training data maximum information density pattern. ﬁrst example train dbnn supplied training data test test dataset. optimal data selection training sample train dbnn testing done test dataset provided repository. optimally selected data contains information original training data expect comparable accuracy test data test rounds. result table shows accuracies cases comparable unseen test sample conﬁrming information contained original training data packed smaller subset generated data selection algorithm. accuracy obtained original training sample comparable values reported literature bayesian belief networks fact slight improvement test accuracy odsa controlled addition samples training data explained section second example training test datasets optimal data selection algorithm produce training data again maximum information density pattern. quality algorithm tested entire dataset individual training test datasets repository. results shown table found remarkable improvement best result dataset literature attained network. similar analysis done datasets repository best known prediction accuracies dataset literature along prediction accuracies training sample sizes given table classiﬁcation catalog preparation could application using machine learning tool much interesting useful application study outliers objects labeled relatively lower conﬁdence levels. mentioned before bayesian rule gives precise measure conﬁdence every prediction. conﬁdence level indicates well class test object deﬁned given evidences brieﬂy discuss table optimal data selection algorithm performed original training sample landsat satellite data repository. table shows prediction accuracies obtained unseen test sample original training data optimally selected subset. note improved training test accuracies optimally selected training subset. optimal selection compensated outliers usually adverse eﬀect network performance. table optimal data selection algorithm performed combined training test samples landsat satellite data repository. table shows prediction accuracies obtained entire data original test data original training data sets. remarkable improvement overall prediction accuracies higher accuracy ever reported dataset. original best test odsa data size accuracy name shuttle random heart random german random iris random glass w.breast cancer random random ecoli random haberman ionosphere random table best classiﬁcation accuracy reported literature datasets repository accuracy obtained dbnn trained optimally selected training data shown. cases provided training data odsa algorithm otherwise entire data used. either case training done optimally selected sample rest data unused training epochs. quasars deep objects look like stars reality extremely active galactic centers ﬂashing intense amounts radiation shines rest galaxy. although ﬁrst discovered radio-band wavelengths known many radio quiet quasars well. astronomers believe objects massive black hole center swallowing surrounding matter gravitationally pulled speeds close light resulting emission intense radiation. doppler eﬀect relativistic velocities seen spectra quasars conﬁrmatory test identity. limitation spectroscopy that since objects distant therefore faint integration time obtain reasonably good spectrum objects prohibitably large even modern large telescopes highly sensitive devices. example sloan digital survey extensive surveys discovered highest number quasars less archived images spectrum. simpler alternative obtaining spectrum image objects diﬀerent band-pass ﬁlters consider relative ﬁlter astronomers call colour object. quasar colours like spectra diﬀerent objects. since could many ﬁlters many colours also produced object. example sdss ﬁlters hence produce four independent colour images object. thus possible input colours neural network train spectroscopically identiﬁed objects that later colour information given input network correctly predict identity object without need spectra. star like objects. also spectral redshift expansion universe introduces drift colours quasars degrades classiﬁcation eﬃciency regions shifted colours becomes identical apparent colours types objects sky. causes odsa algorithm many samples overlapping regions training data minimize overall error classiﬁcation. increased number similar examples reduces likelihood objects training epoch bayesian network assigns lower conﬁdence value objects belonging overlapping patches feature space. identiﬁcation regions tells observer limitations overcome improving observational parameters. figure shows histogram deep images classiﬁed sdss data release function redshift using methodology described region selected known high concentration quasars. plot left blue correctly identiﬁed quasars quasars incorrectly classiﬁed network. stars since galaxy redshift zero hence appear single line histogram. plot right histogram plotted removing objects conﬁdence less equal removed almost failed examples plot. however noted failed examples alone good number correctly classiﬁed objects also removed process. cost conﬁdence important point noted bels. conﬁdence also could mean presence totally diﬀerent class objects dataset. since identiﬁed sample objects much easier identify classes objects data figure plot shows failed objects plot shows histogram removing conﬁdence predictions. since objects fail conﬁdence fewer ﬁgure. plots also reveal limitation sdss colours redshift ranges error patches appear. information used design newer ﬁlters produce additional colours compensate instrumental limitations. another practical application method evaluation models. models predict observationally falsiﬁable results. great example veriﬁcation bending light glances surface proof correctness model gravity given einstein’s general relativity. address diﬀerent problem illustrate application machine learning tools evaluate theoretical models light observations. model evaluations trivial predict precise values veriﬁed suﬃcient accuracy; case bending light presence massive object like sun. however many areas science precise measurements either possible hidden observer. example night thousands stars appear diﬀerent colours magniastronomers studied stars detail broadly classiﬁed seven classes since interested classiﬁcation schemes call instead astronomical names. theoretical models explain star distinct. task check accuracy given model basis available observations. nontrivial problem. star observe unique way. realistic measurement unambiguously identify class object spectra. since stars part galaxy assume eﬀect redshift ignored case. even then intragalatic dust stellar reminiscence distort observed spectra makes simple comparisons impractical. learning algorithm described comes useful tool situations. method straightforward. stated example stellar classiﬁcation theoretical model predict possible variants spectral types stars. mentioned above spectra belong illustrated example samples generated diﬀerent stellar models tested spectra real stars. these good model spectra used study. network trained subset examples found suﬃcient correctly classify remaining data. result testing network entire model spectral data shown table last line table gives actual number objects model data. last column gives total count model network. ideal case numbers reality unlikely intrinsic diﬀerences within class objects factors like dust observational limitations aﬀecting data. however noted model data objects correctly identiﬁed. objects case comparable table confusion matrix real data shown table seen network could produce accuracy samples least classes although classes also wrongly classiﬁed number examples cases small validate model. result table illustrates contradictions theory observations. given test sample size small discrepancy could argued matter chance. contrary observation have possibilities model inaccurate. feature representation good enough. issues nontrivial problems cannot answer likely unless detailed investigations done astronomy part. however simple rule thumb machine learning people reduces likelihood option correct. logic simple. features used represent sample ineﬃcient would obtained good classiﬁcation model spectral data. situation opposite here. network correctly classiﬁed examples class model data failed case real data. demands closer look goodness model. pertinent question obviously would related extent information used update given model. answer this second look problem. seen network performed reasonably well model spectra. means models distinct. failures occurred replaced model spectra actual spectra. wrong classiﬁcation means actual spectra looked similar model spectra diﬀerent class incorrectly assigned network. words although models distinct modeling failed comprehend distinctive features actual spectra. since examples failed models incorrectly identiﬁed with analysis averaged residual spectra obtained subtracting expected obtained model spectra actual spectra failed example should principle table confusion matrix showing class labels data network real data. reduced accuracy obtained contrast model spectra indicative missing features model. highlight wrong priorities missing features models. information used update theoretical model object found accuracy obtained real spectra comparable accuracy obtained spectra created model. paper details optimal data selection algorithm selecting complete training sample neural network. following high school teaching pattern shown algorithm able select valid information hidden dataset. interesting applications network discovery science model evaluations discussed. author wish thank editor referees positive comments suggestions improve manuscript. thanks inter university centre astronomy astrophysics pune providing computing facility professors ajit kembhavi ranjan gupta support preparation manuscript respective authors providing data used paper. work funded indian space research organization respond grant isro/res///-", "year": 2010}