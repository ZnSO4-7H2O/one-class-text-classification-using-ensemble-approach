{"title": "An Improved EM algorithm", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this paper, we firstly give a brief introduction of expectation maximization (EM) algorithm, and then discuss the initial value sensitivity of expectation maximization algorithm. Subsequently, we give a short proof of EM's convergence. Then, we implement experiments with the expectation maximization algorithm (We implement all the experiments on Gaussion mixture model (GMM)). Our experiment with expectation maximization is performed in the following three cases: initialize randomly; initialize with result of K-means; initialize with result of K-medoids. The experiment result shows that expectation maximization algorithm depend on its initial state or parameters. And we found that EM initialized with K-medoids performed better than both the one initialized with K-means and the one initialized randomly.", "text": "paper firstly give brief introduction expectation maximization algorithm discuss initial value sensitivity expectation maximization algorithm. subsequently give short proof em's convergence. then implement experiments expectation maximization algorithm experiment expectation maximization performed following three cases initialize randomly; initialize result k-means; initialize result k-medoids. expectation maximization algorithm depend initial state parameters. found initialized k-medoids performed better initialized k-means initialized randomly. maximum likelihood estimation parameter estimation method widely applied statistics maximum likelihood estimation condition given independently identically distributed points samples know specific form distribution samples drawn beforehand. maximum likelihood estimation finds real parameters distribution maximizing probability transformed logarithmic form thus much easier compute derivative object function however cases know label sample get. example given coins randomly choose flip upward side coin head tail. suppose know coin chosen flipping object estimate probability head upward coin condition flipping don’t know coin chosen. solved expectation maximization algorithm mainly deals parameter estimation latent hidden variables. coin flipping problem hidden variable coin choose flipping rather don’t know probability choosing specific coin algorithm applied various areas. shepp resorted algorithm indirectly emission tomography image reconstruction. feder applied algorithm maximum likelihood active noise cancellation carson used algorithm image segmentation. kriegel multi-instance problem algorithm performed better k-medoid algorithm three real world data sets. knowledge researchers initialize algorithm k-medoids. experiment result show algorithm initialized k-medoids performs better initialized k-means initialized randomly. paper organized follows. section introduce briefly. section analyze sensitivity algorithm initialization. section present convergence algorithm. section show experiment result gives analysis. finally section conclude work point future direction work. suppose given d-dimensional samples x={x collected clusters groups classes xi∈rd. know cluster comes from i.e. labels samples latent variables. object estimate parameter probability density function maximizes probability density function. usually represents parameter set. example dimensional normal distribution represents mean samples j-th cluster represents variance samples j-th cluster. paper discuss expectation maximization algorithm clustering classification represent latent variable. generally take numbers making ycomposed several then complete probability formula respect find relative better lower bound generalized expectation maximization algorithm chose make increase iteration. easy formula denominator nothing thus neglect maximize formula. section analyze sensitivity expectation maximization algorithm respect parameters probability density function number total clusters clapped i.e. make number clusters invariant value. intuitively initialize parameters different values experiment performance expectation maximization algorithm differ other. here analyze initial value sensitivity performance according experiment hard analyze mathematically directly. intuitively clusters deal satisfy following conditions conclusion function local maximums besides global maximum thence initialize parameters different values expectation maximization algorithm converge local maximum unless initial value initialized close true parameters generally hard practice. monotonously increasing obvious bounded. convergence expectation maximization algorithm holds theorem bounded sequence converged mathematical analysis presenting experiment result first give brief introduction k-means algorithm usually used clustering given samples clusters k-means algorithm algorithm give means clusters initially specific rule change means rule make means close true means. experiment stop k-means algorithm means change more. besides k-means algorithm another algorithm k-medoids slightly like k-means algorithm. also implement experiment algorithm initialized k-medoids give brief introduction estimate regard generalized reciprocal matrix regard generalized absolute value matrix regarding generic variable calculation partial derivative respect proof detail readers referred experiment stop parameters change smaller specific given threshold. experiment results appendix give intuitive proof analyze section considering expectation maximization algorithm initialize parameters random values performance expectation maximization algorithm inclined perform poorly. initialize parameters result k-means algorithms k-medoids algorithm. implement experiment times case conclude expectation maximization algorithm performs well combining k-means algorithm k-medoids algorithm i.e. initialize parameters expectation maximization result k-means k-medoids algorithm respectively performs better. find initialized k-means k-medoids improve em's performance lower dimensions obvious. introduction expectation paper first give brief maximization algorithm method different maximum likelihood estimation dealing problems latent hidden variables. prove convergence algorithm briefly. bases above implement experiment k-means algorithms expectation maximization algorithm give analysis. experiment result show k-means algorithm expectation maximization algorithm sensitive initial value future adaptive generalized algorithm need proposed improve classification rate performance. experiment implement experiment initializing parameters according result k-means algorithm results show performs better randomly initializing parameters. since k-means k-medoids improve em's performance easy think applying clustering algorithm initialize algorithm. carson belongie greenspan malik blobworld image segmentation using expectation-maximization application image querying. patternanalysisandmachineintelligence ieeetransactionson bilmes gentle tutorial algorithm application parameter estimation gaussian mixture hidden markov models. internationalcomputerscienceinstitute figure case four clusters -dimension space represent mean every clusterand represent initial mean case k-means performs well performs poorly.", "year": 2013}