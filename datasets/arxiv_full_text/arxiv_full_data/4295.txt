{"title": "On the information in spike timing: neural codes derived from  polychronous groups", "tag": ["q-bio.NC", "cs.NE", "stat.ML"], "abstract": "There is growing evidence regarding the importance of spike timing in neural information processing, with even a small number of spikes carrying information, but computational models lag significantly behind those for rate coding. Experimental evidence on neuronal behavior is consistent with the dynamical and state dependent behavior provided by recurrent connections. This motivates the minimalistic abstraction investigated in this paper, aimed at providing insight into information encoding in spike timing via recurrent connections. We employ information-theoretic techniques for a simple reservoir model which encodes input spatiotemporal patterns into a sparse neural code, translating the polychronous groups introduced by Izhikevich into codewords on which we can perform standard vector operations. We show that the distance properties of the code are similar to those for (optimal) random codes. In particular, the code meets benchmarks associated with both linear classification and capacity, with the latter scaling exponentially with reservoir size.", "text": "abstract—there growing evidence regarding importance spike timing neural information processing even small number spikes carrying information computational models signiﬁcantly behind rate coding. experimental evidence neuronal behavior consistent dynamical state dependent behavior provided recurrent connections. motivates minimalistic abstraction investigated paper aimed providing insight information encoding spike timing recurrent connections. employ information-theoretic techniques simple reservoir model encodes input spatiotemporal patterns sparse neural code translating polychronous groups introduced izhikevich codewords perform standard vector operations. show distance properties code similar random codes. particular code meets benchmarks associated linear classiﬁcation capacity latter scaling exponentially reservoir size. great deal variability axonal delays results interesting phenomenon terms polychronization. figure illustrates effect axonal delays neurons provide inputs neuron axons delays respectively. suppose neuron acts coincidence detector ﬁring spikes neurons arrive time given difference axonal delays happens spike neuron launched spike neuron network neurons connected variable delay axons interesting patterns ﬁrings space time izhikevich terms polychronous groups result individual spikes initiated small number neurons. classical models computational neuroscience based rate coding. existing state machine learning also implicitly models rate coding real value input output neuron serving abstraction ﬁring rate. signiﬁcant experimental evidence considerable speculation importance spike timing computational models less established. paper propose investigate idealized model order derive insight neural codes constructed spike timing information. sensory information inherently embedded spatiotemporal patterns. experimental evidence suggests neurons show dynamical behavior even stimulus ﬁxed shows primary visual cortex acts like fading memory whose current activity contain much information previous frame’s content current frame shows network responds differently odor proceeded odor consistent dynamical state dependent behavior provided recurrent connections. thus recurrent connections aspect model considered here strongly neuro-plausible candidates cortical modeling since bring temporal context providing state-dependent dynamics. paper whether polychronization provide basis concrete computational framework. tradition information theory consider simplest possible model captures basic features problem. consider input neurons connected reservoir neurons depicted figure axonal delays input neurons reservoir neurons reservoir neurons randomly chosen consecutive values. reservoir neuron coincidence detector ﬁring least spikes pre-synaptic neurons fall time bin. input spike pattern corresponding space-time pattern reservoir interpret encoding input pattern. figure shows example reservoir response particular input pattern. thus encoded input space-time pattern output space-time pattern higher-dimensional space. dimension increase input output spaces separate patterns input hence provides channel code. information embedded reservoir’s space-time pattern read interested encoding input spatiotemporal pattern responses reservoir neurons. input neuron connected reservoir neurons chosen random. reservoir neurons internally connected other reservoir neuron providing input reservoir neurons chosen random. thus average in-degree drr. simplicity synapses reservoir neuron taken excitatory role inhibitory synapses shaping neural codes setting certainly great interest future work. reservoir neuron ﬁres least incoming spikes arrive given time slot. numerical results. also make drastic abstraction spike timing dependent plasticity model. speciﬁcally assume that learning phase synapses equal strength incoming spikes line time sufﬁce make neuron ﬁre; enables instant learning input patterns. learning phase over unused incoming synapses zero. incorporating sophisticated stdp mechanisms compatible continual learning forgetting model subject future work. input patterns span times recurrent connections reservoir imply response input last time units. principle reverberant effects duration spikes reservoir neurons could indeﬁnitely long practice capture relevant information within ﬁnite horizon denote thz. simulations employ reservoir space-time response array tqth entry reservoir neuron spikes time otherwise thz. order translate reservoir spatiotemporal information purely spatial domain count ﬁrings reservoir neurons duration horizon results n-dimensional codeword vector entry given computational viewpoint mapping input pattern hash temporal coding rate coding resulting neural code familiar vector space study conventional techniques. since opens path efﬁcient classical computation learning discussion paper devoted spatial coding. standpoint neuro-plausibility intriguing experimental evidence regarding tuning synaptic integration within cortex various degrees sophistication. paper show even simplest approach integrate number spikes neuron relatively large time horizon provides powerful neural code scales well reservoir size readout mechanism maps space-time input patterns spatial output patterns integrating outputs reservoir neurons predeﬁned intervals. resulting spatial codeword real-valued vector dimension discuss properties spatial codes paper. interested codes scale reservoir size main results summarized follows analysis reveals achieve good properties terms linear separability memorization capacity output degree input neurons scale linearly output degree reservoir neurons remain constant scale reservoir size. veriﬁed scaling laws terms ability neural network encode spatio-temporal patterns facilitate classiﬁcation using linear classiﬁer. speciﬁcally shown network placement codewords enable achieve fundamental benchmarks linear separability established cover also veriﬁed that scaling weight distance properties spatial codes scale manner similar random codes. enable argue number patterns reliably distinguished scales exponentially observe that simple readout model small perturbations input pattern lead large changes output. thus system well-matched one-shot learning memorization describe system model depicted figure detail discuss basic properties parameter choices. minimal idealization spike timing information consider input neurons emitting single spike time chosen thus possible input patterns corresponding bits information. input therefore represented binary array rest substantial fraction large distances even there). sensitivity drawback generalization positive feature one-shot learning pattern memorization. course think discrete time model approximating continuous-time system given time resolution response change abruptly timing perturbations smaller resolution. computational perspective view reservoir neurons mapping spatio-temporal stimuli applied input neurons spatial output codewords. context model means neural network input space k-dimensional vectors output space n-dimensional vectors problem constructing optimal maps ﬁnite input patterns n-dimensional output feature space addressed classic paper cover cover considers classiﬁcation problem patterns represented features given binary labels shows fraction total possible labels classiﬁed linear classiﬁer given ρmaxpp equality achieved patterns general positions this mean every subset patterns correspond linearly independent vectors note cover’s result considers equality proof easily adapted show points general positions increase fraction linearly classiﬁable labels. important feature function ρmaxpp exhibits steep change values close values close seen figure ρmaxpp ploted function since ρmaxpp monotonically increasing function preceding result shows perform richer order neural code properly utilize available dimensions scale desirable characteristics discussed section must ensure large fraction reservoir neurons stimulated input pattern. firings reservoir neurons occur direct stimulation input well recurrent internal connections excessive number ﬁrings internal connections lead found less discrimination across neural codewords. therefore scale internal degrees reservoir scale degrees input nodes ensure sufﬁcient stimulation. ﬁring probability reservoir neuron direct response input spikes equivalent coincidence probability spike arrival times denoted tarrival ti`δi number input synapses. model input spikes axonal delays independent random variables distribution hence tarrival symmetric triangular distribution range coincidence probability uniform random variables described well known birthday problem probability least coincidence among realizations random varible distributed probability approximated distribution interest triangular rather uniform coincidence probability ﬁnite range distribution bounded uniform distribution range thus coincidence probability random variable tarrival spikes coming input synapses lower bounded ppi; using expression leads rule thumb order maintain constant ﬁring probability. since synapses coming input layer uniformly connected reservoir neurons bernoulli random variables mean epiq order input spikes propagate reservoir. figure indicates mapping input patterns space-time spatial codes chaotic output hamming distances euclidean distances large even input hamming distance small several values parameters respectively. comparison also include fraction patterns/labels linearly classiﬁed random feature vectors gaussian entries general positions figure contains plot similar figure instead presenting curves several values consider several values input-to-reservoir degree dir. second ﬁgure shows threshold mapping effective. speciﬁcally input neuron connected least reservoir neurons neural network essentially achieves optimal separability linearly classify essentially labeling patterns provided p{.. input neuron connected reservoir neurons degradation performance network still classify labelings provided p{.. however connectivity fraction patterns linearly classiﬁed becomes exceedingly small. thus neural network produces good code sufﬁcient connectivity threshold longer effective. well predicted approximation reservoir neuron ﬁring probability given approximations connectivity respectively. corresponding numbers simulations respectively yield similar qualitative conclusions. detailed analysis speciﬁc threshold required trigger enough activity within reservoir interesting topic future work. discussion shows that appropriate inputto-reservoir connectivity neural network almost optimal terms maximizing probability linear classiﬁcation. perhaps surprisingly show also performs well terms achieving small classiﬁcation error rate patterns linearly separable output space demonstrate this present figure classiﬁcation error rates obtained experiments used construct figure cases obtained optimality terms probability linear classiﬁcation error rates match would obtained random feature vectors gaussian entries. linear separability results preceding section show proposed system produces neural codewords general position similar produced random codes. course number linearly separable codewords scales linearly remove constraint linear separability whether number patterns reliably distinguish scales exponentially formulating communication problem follows possible input patterns messages corresponding bits information. pattern encoded vector dimension using spatial code hence code rate bits/dimension. increase code dimension keeping code rate ﬁxed codewords corresponding exponential scaling memory. keeping codewords well separated? information-theoretic terms well separated means that codeword perturbed noise impairments maximum likelihood decoding probability error tends zero gets large. given input pattern corresponding codeword probability maximum likelihood decoding different codeword bounded e´β||x´y|| standard noise models gaussian poisson. show shortly pairwise distance squared codewords grows linearly concentrating around mean thus union bound error probability conditioned correct codeword given figure gives simulation results showing mean variance squared distances scale linearly apply chebyshev’s inequality infer squared distances indeed concentrate around mean implies that appropriate choice parameters make error probability tend zero. codewords random code. components i.i.d. independent across codewords mean variance distance squared clearly scale linearly addition however p||x er||x y||sq{ stdpx tends standard gaussian central limit theorem. neural code also appears exhibit property figure shows histograms distance-squared exhibit small gaussian-like deviations around mean. note ﬁgure normalized distance codewords interpreted robustness code noise monotonic function code rate. explain behavior ﬁrst note simulations epiq kdir{n implies ﬁring probability reservoir neurons increasing code rate small enough neurons large neurons time. scenarios lead less discrimination among codewords. thus exists sweet spot code rate provides discrimination adjusting input-to-reservoir connectivity. prior models trying utilize spike timing include tempotron attempts binary classiﬁcation different spike timing patterns using perceptron-like structure fig. mean variance distance squared scale linearly distance squared concentrates around mean exhibits gaussian-like deviations default network settings chronotron attempts train towards desired output spike timing pattern. possible mechanisms realizing machines discussed rich history continued research experimental neuroscience showing importance spike timing; well references chaotic nature mappings induced spike timing pointed observations interpreted cast doubt whether spike timing provides robust enough signals. model abstraction well-accepted spiking neural network models embraces chaotic mappings resulting spike timing shows produce good channel codes could provide basis robust memory. note izhikevich also makes interesting case role polychronous groups working memory abstraction rich enough produce polychronous groups without requiring detailed models continuous-time neural dynamics used there. prior attempts abstraction include howard consider network coincidence detectors similar ours detailed insight neural codes generated networks provided. terms computational models based polychronous groups prior work focused least synapses adaptive delays. paughammoisy consider supervised learning reservoir computing model similar ours crucial difference allow adjustment delays reservoir neurons readout. principle compensate chaotic nature input reservoir adapting delays drawbacks point view computational neuroscience neuromorphic design izhikevich hoppenstead propose polychronous wavefront computation model transponders suitable coincidence wavefronts initiated spikes transponders order implement speciﬁc function location transponders must programmed. best knowledge ﬁrst work conduct detailed examination neural codes obtained polychronous groups. model falls within general framework reservoir computing since reservoir ﬁxed properties followed readout mechanism design ﬂexibility. however differs fundamentally existing reservoir computing models liquid state machines echo state networks essence ﬁxed nonlinear ﬁlters input state internal state read adaptive linear transformation however nonlinear mappings systems smooth unlike chaotic maps associated coincidence detection variable delays system. reservoir computing model proposed enables translation information spike timing good random like neural codes standard vector space. worth exploring architectural variants layered reservoirs reservoirs geometric connectivity constraints. speciﬁc readout mechanism propose simple could number ways going binary space-time code deﬁned ﬁring patterns reservoir neurons vector spatial code. role inhibitory connections shaping code also interesting topic. provide basic analysis neural code interest explore robustness noise models speciﬁcally related interesting note spike timing plays critical role number neuromorphic hardware designs form address event representation analog operations thresholding encode information spikes spatiotemporal locations spikes processing control units digital bus. first proposed early used build silicon retinas number research groups work raises question whether might possible replace explicit digital encoding space-time location spikes subsequent digital processing power-efﬁcient analog embeddings processing. authors would like thank professor bruno olshausen helpful advice comments. work supported part national science foundation grants cns- eccs- systems nanoscale information fabrics starnet centers sponsored marco darpa institute collaborative biotechnologies grant wnf-- u.s. army research ofﬁce. nikoli´c haeusler singer maass temporal dynamics information content carried neurons primary visual cortex advances neural information processing systems broome jayaraman laurent encoding decoding overlapping odor sequences neuron vol. clevenson watkins majorization birthday inequality mathematics magazine vol. gutig sompolinsky tempotron neuron learns spike timing-based decisions nature neuroscience vol. march albers westkott pawelzik perfect associative learning spike-timing dependent plasticity advances neural information processing systems. press jaeger echo state approach analysing training recurrent neural networks german national research center information technology tech. rep. report sivilotti wiring considerations analog vlsi systems application ﬁeld-programmable networks ph.d. dissertation california institute technology pasadena boahen point-to-point connectivity neuromorphic chips using address events ieee transactions circuits systems analog digital signal processing vol. nero bardallo serrano-gotarredona linaresbarranco ﬁve-decade dynamic-range ambient-light-independent calibrated signed-spatial-contrast retina .-ms latency optional time-to-ﬁrst-spike mode ieee transactions circuits systems regular papers vol.", "year": 2018}