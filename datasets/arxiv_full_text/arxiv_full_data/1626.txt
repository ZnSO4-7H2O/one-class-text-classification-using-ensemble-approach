{"title": "Traversing Knowledge Graph in Vector Space without Symbolic Space  Guidance", "tag": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "Recent studies on knowledge base completion, the task of recovering missing facts based on observed facts, demonstrate the importance of learning embeddings from multi-step relations. Due to the size of knowledge bases, previous works manually design relation paths of observed triplets in symbolic space (e.g. random walk) to learn multi-step relations during training. However, these approaches suffer some limitations as most paths are not informative, and it is prohibitively expensive to consider all possible paths. To address the limitations, we propose learning to traverse in vector space directly without the need of symbolic space guidance. To remember the connections between related observed triplets and be able to adaptively change relation paths in vector space, we propose Implicit ReasoNets (IRNs), that is composed of a global memory and a controller module to learn multi-step relation paths in vector space and infer missing facts jointly without any human-designed procedure. Without using any axillary information, our proposed model achieves state-of-the-art results on popular knowledge base completion benchmarks.", "text": "early work focuses exploring different objective functions model direct relationships entities several recent approaches demonstrate limitations prior approaches relying upon vector-space models alone example dealing multi-step relationships direct relationship-models suffer cascading errors recursively applied answer next input hence recent works propose different approaches injecting multi-step relation paths observed triplets training improve performance tasks. although using multi-step relation paths achieves better performance also introduces technical challenges. since number possible paths grows exponentially path length prohibitive consider possible paths training time knowledge bases existing approaches need humandesigned procedures sampling pruning paths observed triplets symbolic space. paths informative inferring missing relations approaches might suboptimal. paper rich neural knowledge completion models without need traversing knowledge symbolic space. instead using human-designed relation paths symbolic space training model separately propose learning relation paths vector space jointly model training without using auxiliary information. motivating example relation path symbolic space provides connection observed triplets path length entities obama usa. therefore traverse vector space without relation paths symbolic space model needs satisfy properrecent studies knowledge base completion task recovering missing facts based observed facts demonstrate importance learning embeddings multi-step relations. size knowledge bases previous works manually design relation paths observed triplets symbolic space learn multi-step relations training. however approaches suffer limitations paths informative prohibitively expensive consider possible paths. address limitations propose learning traverse vector space directly withneed symbolic space guidance. remember connections related observed triplets able adaptively change relation paths vector space propose implicit reasonets composed global memory controller module learn multi-step relation paths vector space infer missing facts jointly without human-designed procedure. without using axillary information proposed model achieves state-of-the-art results popular knowledge base completion benchmarks. knowledge bases wordnet freebase yago contain many real-world facts expressed triples e.g. knowledge bases useful many downstream applications question answering information extraction however despite formidable size knowledge bases many important facts still missing. example west showed frequent person entities recorded nationality recent version freebase. seek infer unknown facts based observed triplets. thus knowledge base completion task emerged important open research problem knowledge graph embedding-based methods popular tackling task. framework entities relations represented continuous representations. infer whether entities missing relationship predicted functions corresponding representations. words predict whether given edge capture desired properties design global memory controller. design global memory learn compact representation knowledge graph captures connections related triplets. design controller adaptively change relation paths determine path length relation paths shown fig. speciﬁcally adaptively adjust relation path length controller modeled uses termination module model whether irns stop many steps irns proceed given input. update vector representations step relation path controller updates next state using current state attention vector global memory. global memory external weight matrix shared across triplets. given limited size global memory global memory cannot store observed triplets explicitly. global memory forced learn store information reuse/relate previously saved relevant information. hence attention vector global memory provides context information knowledge graph current state controller. controller uses context information update next step representation relation path. name model implicit given exact information stored global memory implicit exact relation path controller also implicit. provide interpretations analysis implicit components experimental section. propose implicit reasonets learn trafigure overview tasks. global memory designed store compact representation knowledge graph connections related triplets. controller designed adaptively change relation paths vector space determine length relation paths. goal knowledge base completion tasks infer missing relationship entities formulated predicting head tail entity given relation type entity. early work focuses learning symbolic rules. schoenmackers learns inference rules sequence triplets e.g. implied however enumerating possible relations intractable knowledge base large since number distinct sequences triplets increases rapidly number relation types. also rules-based methods cannot generalized paraphrase alternations. recently several approaches achieve better generalization operating embedding representations vector similarity regarded semantic similarity. during training models learn scoring function optimizes score target entity given triplet. evaluation models given triplet missing entity input maps input vector space embeddings ﬁnally outputs prediction vector missing entity. compute similarity prediction vector candidate entities results ranked list entities. mean rank precision target entity used metrics evaluation. paper proposed model uses setup embedding type approaches global memory design goal global memory store knowledge graph information compact way. global memory stores connections related observed triplets provides context information controller updating relation paths. global memory denoted {mi}|m| consists list vectors shared across triplets randomly initialized. training global memory accessed attention mechanism note need deﬁne write operations since global memory updated w.r.t. training objectives back-propagation. inference learned global memory access global memory using attention mechanism. motivating example design global memory follows. suppose task input model required answer missing entity model learn utilize store information global memory controller. example order answer query correctly need collect three pieces relevant information triplet triplet knowing bornin nationality semantically correlated relations assume case controller global memory thus fails correct answer. then training global memory updated store information gradient update. e.g. vectors updated distance relations closer vector space. actually limited size effect similar regularizer model training forces irns make information stored reusable general possible. result compact representation knowledge graph semantically related/similar triples entities relations condensed similar vectors vector space illustrate using examples table similarly model encounters ﬁrst time model learns correlate input global memory minimizes distance attended vectors global memory input triplet. case triplet information stored global memory indirectly gradient update. hence input received model achieve accessing global memory. model learn compactly store knowledge graph correlate observed triplets global memory. controller design goal controller adaptively change relation paths vector space determinate length relation paths. controller iteratively reformulate representation incorporating context information retrieved global memory. without explicitly providing human-designed relation paths iterative progress controller needs explore multistep relation paths own. suppose given input triplet able resolved step. controller needs utilize reformulation capability explore different representations make prediction correctly order lower figure running example architecture. given input model iteratively reformulates input vector current input vector attention vector global memory determines stop based termination module. decoder module takes generated continuous representation outputs predicted vector used nearest entity embedding. basically encoder decoder modules convert representations symbolic space vector space. model learns remember connections relevant observed triplets implicitly global memory adaptively change relation paths depending complexity input guided controller. global memory shared across triplets used store compact representation knowledge graph captures connections triplets. using termination module given current representation controller determines whether model encodes enough information produce output prediction not. model decides terminate controller learns update representation previous state attention vector global memory. otherwise model uses current representation make prediction output. whole process performed repeatedly controller stops process. note number steps varies according complexity example. introduce component model detailed algorithm training objectives motivating examples following subsections. encoder/decoder given input encoder module retrieves entity relation embeddings embedding matrix concatenates vectors intermediate representation decoder module outputs prediction vector tanh based intermediate representation nonlinear projection controller hidden state weight matrix bias vector respectively. k-by-n matrix number possible entities dimension hidden vector distance output target entity possible entities. experiments choose hyperparameters based development sample negative examples speed training. assume ground truth target entity embedding expected reward time deﬁned section evaluate performance model benchmark fbk- datasets datasets contain multi-relations head tail entities. given head entity relation model produces ranked list entities according score entity tail entity triple. evaluate ranking report mean rank mean rank correct entity across test examples hits proportion correct entities ranked top- predictions. lower higher hits indicates better prediction performance. follow evaluation protocol bordes report ﬁltered results negative examples removed dataset. case avoid negative examples valid ranked target triplet. achieve properties controller roles model. first needs judge process stop. output generated. otherwise needs generate representation based previous representation context vector global memory. controller modeled recurrent neural network controls process keeping internal state sequences track current progress history. controller uses attention mechanism fetch information relevant memory vectors decides model output prediction continue update input vector next step. judge process continued controller uses termination module estimate logistical regression module sigmoid weight matrix bias vector learned training. probability process stopped decoder called generate output. probability controller needs generate next representation rnn. attention vector t-th step generated based current internal state global memory speciﬁcally attention score memory vector given state computed overall process inference inference process formally described algorithm given input encoder module converts vector concatenating entity/relation embedding lookup. second step probability model outputs prediction vector probability state updated based previous state vector generated performing attention global memory. iterate process till predeﬁned maximum step tmax. note overall framework generic different applications tailoring encoder/decoder target application. example shortest path synthesis task shown appendix. training objectives section introduce training objectives model. controller model needs determine relation path length. since number steps model proceed example unknown training data optimize expected reward directly motivated reinforce algorithm expected reward step obtained follows. t-step given representation vector model generates output vector convert output vector probability following steps. probability selecting prediction approximated yk∈d exp) input output modules) relation embedding -dimensions. encoder module decoder module encode input entities relations output entities respectively. memory vectors dimensions each initialized random vectors unit lnorm. single-layer cells search controller. maximum inference step tmax randomly initialize model parameters training algorithm mini-batch size learning rate constant number prevent model learning trivial solution increasing entity embeddings norms follow bordes enforce l-norm entity embeddings hits validation metric irn. following work reverse relations training triplet increase training data. models directly optimize scoring function triples knowledge base without using auxiliary information. second group models make uses auxiliary information multi-step relations. example rtranse ptranse models extensions transe model explicitly exploring multi-step relations knowledge base regularize trained embeddings. nlfeat model log-linear model makes simple node link features. table presents experimental results. according table model achieves state-of-the-art results without using auxiliary information. speciﬁcally model surpasses previous results outperforms others better understand behavior irns report results irns different memory sizes different tmax table performance irns increases signiﬁcantly number inference step increases. note tmax case without global memory. interestingly given tmax irns sensitive memory sizes. particular larger memory always improves score best obtained memory vectors. possible reason best memory size determined complexity tasks. order show inference procedure determined irns representation back humaninterpretable entity relation names table show randomly sampled example top- closest observed inputs terms l-distance top- answer predictions along termination probability step. throughout observation inference procedure quite different relation paths people designed symbolic space potential reason irns operate vector space. table interpret state step ﬁnding closest tuple corresponding top- predictions termination probability. rank stands rank target entity term. prob. stands termination probability. instead connecting triplets share exactly entity symbolic space irns update representations connect triplets vector space instead. observe examples table model reformulates representation step gradually increases ranking score correct tail entity higher termination probability inference process. last step table closest tuple actually within training tail entity forward-center target entity. hence whole inference process thought model iteratively reformulates representation order minimize distance target entity vector space. understand model learned global memory tasks table visualize global memory trained fbk. compute average attention scores relation type memory cell. table show relations ranked average attention scores randomly selected memory cells. memory cells activated certain semantic patterns within knowledge graph. suggests global memory capture connections triplets. still noisy relations clustered memory cell e.g. bridge-player-teammates/teammate relation memory cell olympic-medalhonor/medalist disease memory cell. provide prediction examples step shown appendix. addition tasks construct synthetic task shortest path synthesis evaluate inference capability without humanlink prediction knowledge base completion given relation head entity tail entity embedding models link prediction focus ﬁnding scoring function represents implausibility triple capturing direct relationship triplet recently different studies demonstrate importance models incorporate multi-step relation paths training. learning multi-step relations injects structured relationships between triples model. however also poses technical challenge considering exponential numbers multi-step relationships. prior approaches address issue designing path-mining algorithms considering possible paths using dynamic programming algorithm restriction using linear bi-linear models neelakantan roth mccallum model multi-step relationships random walk paths observed triplets. toutanova chen shows effectiveness using simple node link features encode structured information work outperforms prior results shows multistep relation paths captured jointly trained model without explicitly designing relation paths table global memory visualization trained show relations ranked average attention scores randomly selected memory cells. ﬁrst column represents interpreted relation. family lived-with/participant breakup/participant marriage/spouse vacation-choice/vacationer support/supported-organization marriage/location-of-ceremony canoodled/participant dated/participant disease disease-cause/diseases crime-victim/crime-type notable-person-with-medical-condition/condition cause-of-death/parent-cause-of-death disease/notable-people-with-this-condition olympic-medal-honor/medalist disease/includes-diseases disease/symptoms person person/gender person/nationality military-service/military-person government-position-held/ofﬁce-holder leadership/role person/ethnicity person/parents person/place-of-birth sports sports-team-roster/team basketball-roster-position/player basketball-roster-position/player baseball-player/position-s appointment/appointed-by batting-statistics/team basketball-player-stats/team person/profession award ﬁlm-genre/ﬁlms-in-this-genre ﬁlm/cinematography cinematographer/ﬁlm award-honor/honored-for netﬂix-title/netﬂix-genres director/ﬁlm award-honor/honored-for bridge-player-teammates/teammate program tv-producer-term/program tv-producer-term/producer-type tv-guest-role/episodes-appeared-in tv-program/languages tv-guest-role/actor tv-program/spin-offs award-honor/honored-for tv-program/country-of-origin studies show incorporating textual information improve tasks. would interesting incorporate information outside knowledge bases model future. neural frameworks sequence-to-sequence models shown successful many applications machine translation conversation modeling sequence-to-sequence models powerful recent work shown necessity incorporating external memory perform inference simple algorithmic tasks compared irns memory networks neural turing machines biggest difference model existing frameworks controller global memory. follow shen using controller module dynamically perform multi-step inference depending complexity input. memnn explicitly store inputs memory. contrast irns explicitly store observed inputs global memory. instead directly operate global memory stores structured relationships knowledge graph implicitly. training randomly initialize memory update memory jointly controller respect task-speciﬁc objectives back-propagation instead explicitly deﬁning memory write operations ntm. paper propose implicit reasonets learns traverse knowledge graph vector space design global memory controller. without using auxiliary information global memory stores large-scale structured relationships knowledge graph implicitly controller updates representations relation paths adaptively changes relation path length depending complexity input. demonstrate analyze multi-step inference capability irns knowledge base completion tasks. model without using auxiliary knowledge base information achieves state-of-the-art results fbk- benchmarks. future work extend irns ways. first inspired ribeiro singh guestrin would like develop techniques exploit ways generate human understandable reasoning interpretation global memory. second plan apply irns infer relationships unstructured data natural language. example given natural language query rabbits animals? model infer natural language answer implicitly global memory without performing inference directly huge amounts observed sentences mammals animals rabbits animals. believe ability perform inference implicitly crucial modeling large-scale structured relationships.", "year": 2016}