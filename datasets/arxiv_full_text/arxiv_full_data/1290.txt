{"title": "CNN-RNN: A Unified Framework for Multi-label Image Classification", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "While deep convolutional neural networks (CNNs) have shown a great success in single-label image classification, it is important to note that real world images generally contain multiple labels, which could correspond to different objects, scenes, actions and attributes in an image. Traditional approaches to multi-label image classification learn independent classifiers for each category and employ ranking or thresholding on the classification results. These techniques, although working well, fail to explicitly exploit the label dependencies in an image. In this paper, we utilize recurrent neural networks (RNNs) to address this problem. Combined with CNNs, the proposed CNN-RNN framework learns a joint image-label embedding to characterize the semantic label dependency as well as the image-label relevance, and it can be trained end-to-end from scratch to integrate both information in a unified framework. Experimental results on public benchmark datasets demonstrate that the proposed architecture achieves better performance than the state-of-the-art multi-label classification model", "text": "deep convolutional neural networks shown great success single-label image classiﬁcation important note real world images generally contain multiple labels could correspond different objects scenes actions attributes image. traditional approaches multi-label image classiﬁcation learn independent classiﬁers category employ ranking thresholding classiﬁcation results. techniques although working well fail explicitly exploit label dependencies image. paper utilize recurrent neural networks address problem. combined cnns proposed cnn-rnn framework learns joint image-label embedding characterize semantic label dependency well image-label relevance trained end-to-end scratch integrate information uniﬁed framework. experimental results public benchmark datasets demonstrate proposed architecture achieves better performance state-of-the-art multi-label classiﬁcation models. every real-world image annotated multiple labels image normally abounds rich semantic information objects parts scenes actions interactions attributes. modeling rich semantic information dependencies essential image understanding. result multi-label classiﬁcation task receiving increasing attention inspired great success deep convolutional neural networks single-label image classiﬁcation past years demonstrates effectiveness end-to-end frameworks explore learn uniﬁed framework multi-label image classiﬁcation. common approach extends cnns multi-label classiﬁcation transform multiple single-label classiﬁcation problems trained ranking loss cross-entropy loss however treating labels independently methods fail model figure show three images randomly selected imagenet classiﬁcation dataset. second shows corresponding label annotations. image label annotated imagenet dataset. however every image actually contains multiple labels suggested third row. dependency multiple labels. previous works shown multi-label classiﬁcation problems exhibit strong label co-occurrence dependencies instance cloud usually appear together water cars almost never co-occur. model label dependency existing works based graphical models among common approach model co-occurrence dependencies pairwise compatibility probabilities co-occurrence probabilities markov random ﬁelds infer ﬁnal joint label probability. however dealing large labels parameters pairwise probabilities prohibitively large lots parameters redundant labels highly overlapping meanings. moreover methods either model higher-order correlations sacriﬁce computational complexity model complicated label relationships paper explicitly model label dependencies recurrent neural networks capture higher-order label relationships keeping computational complexity tractable. signiﬁcantly improves classiﬁcation accuracy. part avoid problems like overﬁtting previous methods normally assume classiﬁers share image features however using image features predict multiple labels objects small images easily ignored hard recognize independently. work design rnns framework adapt image features based previous prediction results encoding attention models implicitly cnn-rnn structure. idea behind implicitly adapt attentional area images cnns focus attention different regions images predicting different labels. example predicting multiple labels images figure model shift attention smaller ones recognizing dominant object small objects hard recognize itself easily inferred given enough contexts. finally many image labels overlapping meanings. example kitten almost meanings often interchangeable. exploiting semantic redundancies reduce computational cost also improves generalization ability labels duplicate semantics training data. label semantic redundancy exploited joint image/label embedding learned canonical correlation analysis metric learning learning rank methods joint image/label embedding maps label image embedding vector joint low-dimensional euclidean space embeddings semantically similar labels close other embedding image close associated labels space. joint embedding model exploit label semantic redundancy essentially shares classiﬁcation parameters semantically similar labels. however label co-occurrence dependency largely ignored models. paper propose uniﬁed cnn-rnn framework multi-label image classiﬁcation effectively learns semantic redundancy co-occurrence dependency end-to-end way. framework proposed model shown figure multi-label model learns joint low-dimensional image-label embedding model semantic relevance images labels. image embedding vectors generated deep label label embedding vector. high-order label co-occurrence dependency low-dimensional space modeled long short term memory recurrent neurons maintains information label context internal memory states. framework computes probability multilabel prediction sequentially ordered prediction path priori probability label time step computed based image embedding output recurrent neurons. prediction multilabel prediction highest probability approximately found beam search algorithm. proposed cnn-rnn framework uniﬁed framework combines advantages joint image/label embedding figure illustration cnn-rnn framework multilabel image classiﬁcation. framework learns joint embedding space characterize image-label relationship well label dependency. blue dots label image embeddings respectively black dots image recurrent neuron output embeddings. recurrent neurons model label co-occurrence dependencies joint embedding space sequentially linking label embeddings joint embedding space. time step probability label computed based image embedding output recurrent neurons. evaluate proposed cnn-rnn framework exhaustive experiments public multi-label benchmark datasets inlcuding nus-wide microsoft coco pascal experimental results demonstrate proposed method achieves signiﬁcantly better performance compared current state-of-the-art multi-label classiﬁcation methods. also visualize attentional regions framework deconvolutional networks interestingly visualization shows framework focus corresponding image regions predicting different labels similar humans’ multi-label classiﬁcation process. progress image classiﬁcation partly creation large-scale hand-labeled datasets imagenet development deep convolutional neural networks recent work extends deep convolutional neural networks multi-label classiﬁcation achieves good results. deep convolutional ranking optimizes top-k ranking objective assigns smaller weights loss positive label. hypotheses-cnn-pooling employs pooling aggregate predictions multiple hypothesis region proposals. methods largely treat label independently ignore correlations labels. multi-label classiﬁcation also achieved learning joint image/label embedding. multiview canonical correlation analysis three-way canonical analysis maps image label semantics latent space. wasabi devise learn joint embedding using learning rank framework warp loss. metric learning learns discriminative metric measure image/label similarity. matrix completion bloom ﬁlter also employed label encodings. methods effectively exploit label semantic redundancy fall short modeling label co-occurrence dependency. various approaches proposed exploit label co-occurrence dependency multi-label image classiﬁcation. learns chain binary classiﬁers classiﬁer predicts whether current label exists given input feature already predicted labels. label co-occurrence dependency also modeled graphical models conditional random field dependency network co-occurrence matrix. label augment model augments label common label combinations. models capture pairwise label correlations high computation cost number labels large. low-dimensional recurrent neurons proposed model computationally efﬁcient representations high-order label correlation. lstm effectively model long-term temporal dependency sequence. successfully applied image captioning machine translation speech recognition language modeling word embedding learning demonstrate lstm also effective model label dependency. since characterize high-order label correlation employ long short term memory neurons recurrent neurons demonstrated powerful model long-term dependency. long short term memory networks class neural network maintains internal hidden states model dynamic temporal behaviour sequences arbitrary lengths directed cyclic connections units. considered hidden markov model extension employs nonlinear transition function capable modeling long term temporal dependencies. lstm extends adding three gates neuron forget gate control whether forget current state; input gate indicate read input; output gate control whether output state. gates enable lstm learn long-term dependency sequence make easier optimize gates help input signal effectively propagate recurrent hidden states without affecting output. lstm also effectively deals gradient vanishing/exploding issues commonly appear training figure architecture proposed model multilabel classiﬁcation. convolutional neural network employed image representation recurrent layer captures information previously predicted labels. output label probability computed according image representation output recurrent layer. number columns label embedding matrix convolutional neural network image representation. show learned joint embedding effectively characterizes relevance images labels. propose novel cnn-rnn framework multilabel classiﬁcation problem. illustration cnnrnn framework shown fig. contains parts part extracts semantic representations images; part models image/label relationship label dependency. decompose multi-label prediction ordered prediction path. example labels zebra elephant decomposed either probability prediction path computed network. image label recurrent representations projected lowdimensional space model image-text relationship well label redundancy. model employed compact powerful representation label cooccurrence dependency space. takes embedding predicted label time step maintains hidden state model label co-occurrence information. priori probability label given previously predicted labels computed according products image recurrent embeddings. probability prediction path obtained product a-prior probability label given previous labels prediction path. label represented one-hot vector k-th location elsewhere. label embedding obtained multiplying one-hot vector label embedding matrix k-th label embedding label recurrent layer takes label embedding previously predicted label models co-occurrence dependencies hidden recurrent states learning nonlinear functions hidden states outputs recurrent layer time step respectively label embedding t-th label prediction path non-linear functions described details sec. labels probability label computed information image previously predicted labels l··· lt−. model predicts multiple labels ﬁnding prediction path maximizes priori probability. since probability markov property optimal polynomial algorithm optimal prediction path. employ greedy approximation predicts label maxlt time step label prediction later predictions. however greedy algorithm problematic ﬁrst predicted label wrong likely whole sequence cannot figure example beam search algorithm beam size beam search algorithm ﬁnds best paths highest probability keeping intermediate paths time step iteratively adding labels intermediate paths. example beam search algorithm found figure instead greedily predicting probable label beam search algorithm ﬁnds top-n probable prediction paths intermediate paths time step time step probable labels intermediate path total paths. prediction paths highest probability among paths constitute intermediate paths time step prediction paths ending sign added candidate path termination condition beam search probability current intermediate paths smaller candidate paths. indicates cannot candidate paths greater probability. learning cnn-rnn models achieved using cross-entropy loss softmax normalization score softmax) employing back-propagation time algorithm. order avoid gradient vanishing/exploding issues apply rmsprop optimization algorithm although possible ﬁne-tune convolutional neural network architecture keep convolutional neural network unchanged implementation simplicity. determined according occurrence frequencies training data. frequent labels appear earlier less frequent ones corresponds intuition easier objects predicted ﬁrst help predict difﬁcult objects. explored learning label orders iteratively ﬁnding easiest prediction ordering order ensembles proposed simply using ﬁxed random order notable effects performance. also attempted randomly permute label orders mini-batch makes training difﬁcult converge. experiments module uses layers network pretrained imagenet classiﬁcation challenge dataset using caffe deep learning framework dimensions label embedding lstm layer respectively. employ weight decay rate momentum rate dropout rate projection layers. evaluate proposed method three benchmark multi-label classiﬁcation datasets nus-wide microsoft coco pascal datasets. evaluation demonstrates proposed method achieves superior performance state-of-the-art methods. also qualitatively show proposed method learns joint label/image embedding focuses attention different image regions sequential prediction. precision recall generated labels employed evaluation metrics. image generate highest ranked labels compare generated labels ground truth labels. precision number correctly annotated labels divided number generated labels; recall number correctly annotated labels divided number ground-truth labels. also compute per-class overall precision recall scores average taken classes testing examples respectively. score geometrical average precision recall scores. also compute mean average precision measure work shown fig. proposed framework performs well large objects person zebra stop sign objects high dependencies objects scene sports baseball glove. performs poorly small objects little dependencies objects toaster hair drier global convolutional neural network image features limited discriminative ability recognize small objects. zero prediction toaster hair drier resulting zero precision recall scores labels. relationship recall bounding area microsoft coco dataset shown fig. observe recall generally higher object larger unless object large almost whole image important information might lose image cropping process cnn. pascal visual object classes challenge datasets widely used benchmark multilabel classiﬁcation. dataset contains images divided train test subsets. conduct experiments trainval/test splits evaluation average precision mean comparison state-of-the-art methods shown table inria based transitional feature extraction-coding-pooling pipeline. cnn-svm directly applies overfeat features pre-trained imagenet. i-ft employs squared loss function shared features pascal multi-label classiﬁcation. hcp-c employs region figure attentional visualization multi-label framework. image ground-truth labels elephant zebra. bottom-left image shows framework’s attention beginning bottom-right image shows attention predicting elephant. interesting investigate cnn-rnn framework’s attention changes predicting different labels. visualize attention multi-label model deconvolutional networks fig. given input image attention multilabel model time step average synthesized image label nodes softmax layer using deconvolutional network. ground truth labels image elephant zebra. beginning attention visualization shows model looks whole image predicts elephant. after predicting elephant model shifts attention regions zebra predicts zebra. visualization shows although framework learn explicit attentional model manages steer attention different image regions classifying different objects. propose uniﬁed cnn-rnn framework multilabel image classiﬁcation. proposed framework combines advantages joint image/label embedding label co-occurrence models employing model label co-occurrence dependency joint image/label embedding space. experimental results several benchmark datasets demonstrate proposed approach achieves superior performance state-of-theart methods. proposed model steer attention different image regions predicting different labels. however predicting small objects still challenging limited discriminativeness global visual features. interesting direction predict labels also predict segmentation objects constructing explicit attention model. investigate future work. proposal information ﬁne-tune features pretrained imagenet dataset hives better performance methods exploit region information. proposed cnn-rnn method outperforms i-ft method large margin also performs better hcp-c method although method take region proposal information account. addition able generate multiple labels cnn-rnn model also effectively learns joint label/image embedding. nearest neighbors labels embedding space nus-wide ms-coco shown table label highly semantically related nearest-neighbor labels. fig. shows nearest neighbor labels images nus-wide -tag dataset computed according label embedding image embedding joint embedding space image nearest neighbor labels semantically relevant. moreover compared top-ranked labels predicted classiﬁcation model nearest neighbor labels usually ﬁne-grained. example nearest neighbor labels hawk glacier ﬁne-grained bird landscape. references cabral torre costeira bernardino. matrix completion multi-label image classiﬁcation. advances neural information processing systems pages t.-s. chua tang hong zheng. nus-wide real-world image database national university singapore. proceedings international conference image video retrieval page cisse usunier artieres gallinari. robust bloom ﬁlters large multilabel classiﬁcation tasks. advances neural information processing systems pages improving deep neural networks lvcsr using rectiﬁed linear units dropout. acoustics speech signal processing ieee international conference pages ieee deng dong socher l.-j. feifei. imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference pages ieee graves a.-r. mohamed hinton. speech recognition deep recurrent neural networks. acoustics speech signal processing ieee international conference pages ieee guillaumin mensink verbeek schmid. tagprop discriminative metric learning nearest neighbor models image auto-annotation. computer vision ieee international conference pages ieee multi-label classiﬁcation using conditional dependency networks. ijcai proceedingsinternational joint conference artiﬁcial intelligence volume page ding image completion dual-view linear sparse reconstructions. computer vision image understanding h.-j. zhang. uniﬁed analysis multi-edge graph. proceedings international conference multimedia pages razavian azizpour sullivan carlsson. features off-the-shelf astounding baseline recognition. computer vision pattern recognition workshops ieee conference pages ieee turpin scholer. user performance versus precision measures simple search tasks. proceedings annual international sigir conference research development information retrieval pages", "year": 2016}