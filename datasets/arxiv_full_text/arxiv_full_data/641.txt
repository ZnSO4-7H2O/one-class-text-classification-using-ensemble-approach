{"title": "Metaheuristic Algorithms for Convolution Neural Network", "tag": ["cs.CV", "cs.AI", "cs.NE", "68Txx", "I.2.10"], "abstract": "A typical modern optimization technique is usually either heuristic or metaheuristic. This technique has managed to solve some optimization problems in the research area of science, engineering, and industry. However, implementation strategy of metaheuristic for accuracy improvement on convolution neural networks (CNN), a famous deep learning method, is still rarely investigated. Deep learning relates to a type of machine learning technique, where its aim is to move closer to the goal of artificial intelligence of creating a machine that could successfully perform any intellectual tasks that can be carried out by a human. In this paper, we propose the implementation strategy of three popular metaheuristic approaches, that is, simulated annealing, differential evolution, and harmony search, to optimize CNN. The performances of these metaheuristic methods in optimizing CNN on classifying MNIST and CIFAR dataset were evaluated and compared. Furthermore, the proposed methods are also compared with the original CNN. Although the proposed methods show an increase in the computation time, their accuracy has also been improved (up to 7.14 percent).", "text": "typical modern optimization technique usually either heuristic metaheuristic. technique managed solve optimization problems research area science engineering industry. however implementation strategy metaheuristic accuracy improvement convolution neural networks famous deep learning method still rarely investigated. deep learning relates type machine learning technique move closer goal artificial intelligence creating machine could successfully perform intellectual tasks carried human. paper propose implementation strategy three popular metaheuristic approaches i.e. simulated annealing differential evolution harmony search optimize cnn. performance metaheuristic methods optimizing classifying mnist cifar dataset evaluated compared. furthermore proposed methods also compared original cnn. although proposed methods show increase computation time accuracy also improved deep learning mainly motivated research artificial intelligent general goal imitate ability human brain observe analyze learn make decision especially complex problem technique intersection amongst research area signal processing neural network graphical modeling optimization pattern recognition. current reputation implicitly drastically improve abilities chip processing significantly decrease cost computing hardware advance research machine learning signal processing generative models hybrid model discriminative models instance deep neural networks recurrent neural network. examples generative models deep belief networks restricted boltzmann machine regularized autoencoders deep boltzmann machines. hand hybrid model refers deep architecture combination discriminative generative model. example model pre-train deep improve difficult examples successful methods training stochastic gradient descent conjugate gradient hessian-free optimization krylov subspace descent. stochastic gradient descent easy implement also fast process case many training samples. however method needs several manual tuning make parameters optimal also process principally sequential result hard parallelize gpus. conjugate gradient side easier check convergence well stable train. nevertheless slow needs multicore cpus availability vast number rams hessian-free optimization applied train deep auto-encoders proficient handling fitting problem efficient pre-training fine tuning proposed hinton salakhutdinov side krylov subspace descent robust simpler well look like work better classification performance optimization speed. however needs memory optimization techniques applied solve optimization problems research area science engineering even industry however research metaheuristic optimize deep learning method rarely conducted. paper combining genetic algorithm proposed zhining yunming .their model select characteristic process recombination mutation model exists individual algorithm besides recombination process layers weights threshold value changed model. simulated annealing differential evolution harmony search optimizing cnn.. strategies looking best value fitness function last layer using metaheuristic algorithm results used calculate weights biases previous layer. case testing performance proposed methods mnist dataset. dataset images digital handwritten digits contains training data testing data. images centered standardized size pixels. pixel image represented black white different shade gray used metaheuristic algorithms section describe convolution neural networks section gives description proposed methods section present result simulation section conclusion. metaheuristic well-known efficient method hard optimization problems i.e. problems cannot solved optimally using deterministic approach within reasonable time limit. metaheuristic methods work three main purposes fast solving problem solving large problems making robust algorithm. methods also simple design well flexible easy implement algorithm instance evolution strategy phenomena ethology examples particle swarm optimization colony optimization bacterial foraging optimization algorithms colony optimization phenomena physic microcanonical annealing threshold accepting method another form metaheuristic inspired music phenomena algorithm based population-based. examples single-solution based metaheuristic noising method tabu search guided local search. case metaheuristic based population classified swarm intelligent evolutionary computation. general term swarm intelligent inspired collective behavior social insect colonies animal societies. examples algorithms side algorithm evolutionary computation takes inspiration principles darwinian developing adaptation environment. examples algorithms bfoa among metaheuristic algorithms used paper. technique random search problem global optimization. mimics process annealing material processing technique firstly proposed kirkpatrick gelatt vecchi principle idea using random search allows changes improve fitness function also maintaining changes ideal. example minimum optimization problem better changes decrease fitness function value accepted changes increase also accepted transition probability follow energy level changes boltzmann’s constant temperature controlling process annealing. equation based boltzmann distribution physics following standard procedure optimization problems differential evolution firstly proposed price storn solve chebyshev polynomial problem algorithm created individual’s difference exploiting random search space solution finally operate procedure mutation crossover well selection obtain suitable individual system indicates process mutation target vector randomly selected single different vector applied. acronym shows crossover process organized rule binomial decision. procedure algorithm shown following steps determining parameter setting population size number individuals. mutation factor control magnification individual differences avoid search stagnation. crossover rate decides many consecutive genes mutated vector copied offspring. harmony search algorithm proposed geem algorithm inspired musical process searching perfect state harmony. like harmony music solution vector optimization improvisation musician analogous structures local global search optimization techniques. improvisation music players sound pitch possible range together create vector harmony. case pitches create real harmony; experience stored memory player opportunity create better harmony next time three possible alternatives pitch improvised musician pitch played her/his memory nearby pitch played her/his memory entirely random pitch played range possible sound. options used optimization three equivalent components; harmony memory pitch adjusting randomization. algorithm rules correlated relevant parameters i.e. harmony consideration rate pitch adjusting rate procedure algorithm summarized five steps follows based hmcr randomization. selection value based hmcr parameter range vector harmony observed decide whether pitch-adjusted using parameter. process pitch adjusting executed value selected case meeting termination criterion computation ended. alternatively process reiterated. vector best nominated reflected best solution problem. convolution neural network variant standard multilayer perceptron substantial advantage method especially pattern recognition compared conventional approaches capability reducing dimension data extracting feature sequentially classifying structure network basic architecture model inspired visual cortex proposed hubel wiesel. fukushima’s neocognitron created first computation model following idea fukushima lecun found state-of-the-art performance number tasks pattern recognition using error gradient method three ideas local receive fields weights sharing spatial/temporal sub-sampling. ideas organized types layers convolution layers subsampling layers. showed fig. processing layers contain three convolution layers combined sub-sampling layers output layer convolution sub-sampling layers structured planes called features maps. principally convolution layer correlated feature maps size kernel connections previous layer. feature maps results convolution maps previous layer corresponding kernel linear filter. adding bias term applying non-linear function. k-th feature follow several convolution sub-sampling last structure classification layer. layer works input series fully connected layers execute classification task. output neuron every class label case mnist dataset layer contains neurons corresponds classes. architecture proposed method refers simple structure complex structure like alexnet variations design structure. first i-c-s-c-s number second i-c-s-c-s number kernel size convolution layer scale sub-sampling .these architecture designed recognizing handwritten digits mnist dataset. find condition best accuracy also minimize estimated error indicator network complexity. objective realized computing lost function vector solution standard error training set. following lost function used paper expected output real output training samples. case termination criterion situations used method. first maximum iteration reached second loss function principally algorithm computes values weight bias last layer used calculate lost function. values weight bias last layer used solution vector denoted optimized algorithm adding randomly. result accuracy time initialization set-up i-c-s-c-s calculation process weights biases lost function solution vector last layer; termination criteria satisfied first time method computes values weight bias. values weight bias last layer used calculate lost function adding randomly values used initialize individuals population. result accuracy time initialization set-up i-c-s-c-s calculation process weights biases lost function individual termination criteria satisfied furthermore individual population updated based algorithm. termination criterion satisfied weights biases updated layers system. following cnnde algorithm proposed method. first time like cnnsa cnnde method computes values weight bias. values weight bias last layer used calculate lost function adding randomly values used initialize harmony memory. furthermore harmony memory updated based algorithm. termination criterion satisfied weights biases updated layers system. following cnnhs algorithm proposed method. result accuracy time initialization set-up i-c-s-c-s calculation process weights biases lost function harmony memory termination criterion satisfied paper primary goal improve accuracy original using algorithm. performed minimizing classification task error tested mnist dataset. examples image mnist dataset shown fig.. iteration cnnde population size maxit cnnhs harmony memory size maxit since difficult make sure control parameter experiment values well hmcr also parameter i.e. learning rate batch size epoch parameter number epoch every experiment. experiment implemented matlab-ra personal computer processor intel core running memory window five separate runtimes. original program simulation deeplearn toolbox palm experiment result original cnn. results design i-c-s-c-s summarized table accuracy table computational time fig. error standard deviation well fig. computational time standard deviation. results design i-c-s-c-s summarized table accuracy table computational time fig. error standard deviation well fig. computational time standard deviation. value accuracy change experiment repeated condition. general tests conducted showed higher epoch value better accuracy. example epoch compared accuracy increased cnnsa cnnde cnnhs epoch compared increase accuracy cnnsa cnnde cnnhs case epoch shown fig. increase accuracy compared cnnsa cnnde cnnhs experiment results show cnnsa presents best accuracy epoch. accuracy improvement cnnsa compared original varies epoch range values computation time proposed method compared original range times times addition also test proposed method cifar data-set. dataset consists color images size every image five batches training composed images batch test images consist images. cifar dataset divided classes class images. example images dataset showed fig. follow. number epoch experiment. original program matconvnet paper program modified algorithm. results seen fig. objective fig. top- error fig. top- error. general results show cnnsa works better original cifar data-set. paper shows algorithms improve accuracy cnn. although increase computation time nevertheless error proposed method smaller original variation epoch.", "year": 2016}