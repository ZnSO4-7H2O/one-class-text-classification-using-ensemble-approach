{"title": "On Training Deep Boltzmann Machines", "tag": ["cs.NE", "cs.AI", "cs.LG"], "abstract": "The deep Boltzmann machine (DBM) has been an important development in the quest for powerful \"deep\" probabilistic models. To date, simultaneous or joint training of all layers of the DBM has been largely unsuccessful with existing training methods. We introduce a simple regularization scheme that encourages the weight vectors associated with each hidden unit to have similar norms. We demonstrate that this regularization can be easily combined with standard stochastic maximum likelihood to yield an effective training strategy for the simultaneous training of all layers of the deep Boltzmann machine.", "text": "deep boltzmann machine important development quest powerful deep probabilistic models. date simultaneous joint training layers largely unsuccessful existing training methods. introduce simple regularization scheme encourages weight vectors associated hidden unit similar norms. demonstrate regularization easily combined standard stochastic maximum likelihood yield effective training strategy simultaneous training layers deep boltzmann machine. since introduction salakhutdinov hinton deep boltzmann machine ambitious attempts build probabilistic model many layers latent hidden variables. shares characteristics earlier deep belief network models viewed multi-layer deep extensions popular restricted boltzmann machine however unlike commonly used approximate inference schemes implement true feedback mechanisms inferred activations high-level units inﬂuence activations lower-level units. thus alternative interpretations input able compete levels model simultaneously. sort sophistication inference potential lead robust globally coherent inferences likely translate better performance model employed tasks classiﬁcation. despite success entirely fulﬁlled potential remains popular model paradigm deep probabilistic models. potential explanation could tied difﬁculties encounters attempting estimate model parameters data. straightforward applications gradient-based methods stochastic maximum likelihood appear fall poor local minima fail adequately explore space model parameters. solution presented based greedy layer-wise pretraining strategy appears overcome poor local minima layer trained latent activations layer input. layers recombined rescaling weights account doubling inputs intermediate layer. procedure seems work well involves many steps complicated would ideal. also importantly necessity layer-wise pretraining makes difﬁcult organization upper layers inﬂuence topological organization lower layers. example cases would like learn fully connected layer would potentially desirable global connectivity pattern inﬂuence pattern learned ﬁlters layers. layer-wise pretraining precludes possibility. hand could jointly train layers deep boltzmann machine simultaneously pattern activation upper layer units would opportunity inﬂuence weights trained lower layers effect lower-layer units activations. paper describe simple scheme joint training layers deep boltzmann machine. strategy based observation poor local minima falls characterized high variance norms weight vectors particularly data ﬁrst layer hidden units. solution simply regularization term standard maximum likelihood training criterion penalizes large differences norms weight vectors within layer across neighbouring layers. regularization based intuition successful models units contribute roughly equally representation data. common principle previously applied form sparsity penalties training hidden units encouraged active equal proportion data. application principal somewhat different explicitly control activation hidden units instead encourage equal inﬂuence active hidden unit. demonstrate experiments failure standard training dbms effectiveness regularized-sml training strategy. boltzmann machine deﬁned network symmetrically-coupled binary stochastic units stochastic units divided groups visible units represent data hidden units mediate dependencies visible units mutual interactions. pattern interaction speciﬁed energy function model parameters respectively encode visible-to-visible interactions hidden-to-hidden interactions visible-to-hidden interactions visible selfconnections hidden self-connections avoid over-parametrization diagonals zero. general inference boltzmann machine intractable. example computing conditional probability given visibles requires marginalizing rest hiddens implies evaluating terms restricted boltzmann machine likely popular subset boltzmann machines. deﬁned restricting interactions boltzmann energy function i.e. erbm such said form bipartite graph visibles hiddens forming layers vertices graph. restriction possesses useful property conditional distribution hidden units factorizes given visibles conditional factorization property immediately implies inferences would like make readily tractable. example conditional independence hiddens implies posterior marginals form immediately available. importantly tractability extend partition function still involves sums exponential number terms. imply however limit number terms min{d usually still unmanageable number terms therefore must resort approximate methods deal computation. learning also rendered much tractable comparison general boltzmann machine. typically learning involves ﬁnding model parameters approximately maximizes likelihood training dataset expectations respect clamped condition full joint unclamped condition. training follow stochastic maximum likelihood algorithm i.e. performing updates mcmc chain parameter update. deep boltzmann machine another particular subset boltzmann machine family models units arranged layers. however unlike possesses multiple layers hidden units illustrated figure respect boltzmann energy function corresponds setting sparse connectivity structure make structure explicit specifying energy function. -layer model given point departure posterior distribution hidden units longer tractable interactions hidden units. salakhutdinov hinton resort mean-ﬁeld approximation posterior. speciﬁcally despite intractability inference training theory much complicated rbm. major difference instead maximizing likelihood directly instead choose parameters maximize lower-bound likelihood given sml-based algorithm maximizing lower-bound follows iterate convergence. generate negative phase samples sml. compute using values obtained steps finally update model parameters step gradient ascent. procedure appears simple extention highly effective scheme training rbms demonstrate sec. procedure seems vulnerable falling poor local minima leave many ﬁlters effectively dead failure joint training strategy noted salakhutdinov hinton successful alternative proposed greedy layer-wise training strategy. procedure consists pre-training layers much deep belief network i.e. stacking rbms training layer independently model output previous layer. ﬁnal joint ﬁne-tuning done following sml-based procedure. greedy layer-wise training shown reasonably successful means joint training would highly desireable. would simpler would open door local receptive ﬁeld learning general architectures would like toppattern connectivity inﬂuence learning lower-level features. section detail simple proposal means jointly train layers dbm. strategy based observation standard sml-based joint training tends produce high variance weight vectors associated hidden unit particularly across ﬁrst-layer weight vectors connect visible units ﬁrst hidden layer units. proposal thus regularize maximum likelihood objective order encourage units similar norms within given layer lesser extent across neighbouring layers. thus introduce additional parameter layer representing average norm weight vectors l-th layer. regularization term objective penalizes deviations mean-value using squared-error penalty. belonging adjacent layers constrained close gives rise following regularized objective think regularization term spring ensures system evolves jointly initial random weight conﬁguration small norm) good model input distribution undoubtedly requires weight vectors larger norms. evaluate effect regularization term trained deep boltzmann machines pervasive mnist dataset models trained updates using minibatches size varying learning rate many ﬁrst-layer ﬁlters signiﬁcantly different random initialization small norm. based difference results successful training speculate top-down interactions prevent ﬁrst layer learning useful ﬁlters. conﬁrmed high-frequency noise ﬁlters actually ones figure random subset ﬁlters obtained updates learning rate batch size higher-level weights visualized performing linear combination lower-level weights picking active connections. lowest norm. seems though early top-down input inﬂuencing activations hidden units perhaps directly form suppression indirectly reinforcing activation subset ﬁlters become useful early plot ﬁlters -layer networks results hold networks depth two. joint training using regularized objective train -layer hidden units layer. hyper-parameters follows furthermore dampen learning rate parameters factor thousand order layer norms evolve slowly. random subset ﬁlters shown figure effect regularization term drastic vast majority ﬁrst layer ﬁlters seem train successfully resemble pen-stroke detectors characteristic features learned mnist. difﬁcult interpret second layer ﬁlters also clearly superior ones figure combining lower-level features global digit-like objects. introduced simple regularization scheme appears prevent falling poor local minimum objective function. regularizer encourages units learn ﬁlters similar norms within across adjacent layers. empirically demonstrated failure joint training layers deep boltzmann machine leaves many ﬁlters lower layer -layer small norm consequently little contribution modeling data. also empirically demonstrated success regularization scheme simultaneously training layers -layer dbm. shown here scheme also appears work well dbms layers. future work would like determine many layers learnt simultaneously using similar basic norm-regularizaiton schemes. authors would like acknowledge support following agencies research funding computing support nserc calcul qu´ebec cifar. would also like thank developers theano", "year": 2012}