{"title": "Distributed Regression in Sensor Networks: Training Distributively with  Alternating Projections", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "cs.IT", "math.IT"], "abstract": "Wireless sensor networks (WSNs) have attracted considerable attention in recent years and motivate a host of new challenges for distributed signal processing. The problem of distributed or decentralized estimation has often been considered in the context of parametric models. However, the success of parametric methods is limited by the appropriateness of the strong statistical assumptions made by the models. In this paper, a more flexible nonparametric model for distributed regression is considered that is applicable in a variety of WSN applications including field estimation. Here, starting with the standard regularized kernel least-squares estimator, a message-passing algorithm for distributed estimation in WSNs is derived. The algorithm can be viewed as an instantiation of the successive orthogonal projection (SOP) algorithm. Various practical aspects of the algorithm are discussed and several numerical simulations validate the potential of the approach.", "text": "wireless sensor networks attracted considerable attention recent years motivate host challenges distributed signal processing. problem distributed decentralized estimation often considered context parametric models. however success parametric methods limited appropriateness strong statistical assumptions made models. paper ﬂexible nonparametric model distributed regression considered applicable variety applications including ﬁeld estimation. here starting standard regularized kernel least-squares estimator messagepassing algorithm distributed estimation wsns derived. algorithm viewed instantiation successive orthogonal projection algorithm. various practical aspects algorithm discussed several numerical simulations validate potential approach. keywords wireless sensor networks distributed estimation regression alternating projections kernel methods distributed learning nonparametric wireless sensor networks attracted considerable attention recent years research area focused separate aspects networks networking issues capacity delay routing strategies; applications issues. paper concerned second aspects wireless sensor networks particular problem distributed inference. wireless sensor networks fortiori designed purpose making inferences environments sensing typically characterized limited communications capabilities tight energy bandwidth limitations. thus distributed inference major issue study networks. problem distributed decentralized estimation often considered context parametric models strong assumptions made statistical description environment. example observations made sensor network modeled gaussian random ﬁeld whose dependency structure described graphical model; setting iterative message passing algorithms belief propagation loopy embedded trees embedded polygons gaussian elimination used eﬃciently estimate ﬁeld distributively. investigates distributed algorithm estimating ﬁeld modeled mixture gaussians. case sensors observe phenomenon independent noisy channels studies distributed maximum likelihood estimator using information sharing algorithm based fisher scoring. derive distributed algorithms based incremental subgradient methods useful parameter estimation sensor networks. finally focuses case sensor observations quantized one-bit. standard centralized case success distributed parametric methods limited appropriateness statistical assumptions made model. certain applications strong modeling assumptions warranted systems designed models show promise. however scenarios research supported part army research oﬃce grant daad--- part draper laboratory grant ir&d part national science foundation grants ccr- part oﬃce naval research grant n---. prior knowledge best vague translating knowledge statistical model undesirable. applications pave nonparametric study distributed estimation. several nonparametric analyses considered distributed estimation problem. example weakening assumptions made focuses universal decentralized estimation case sensor observations quantized bit. study existence consistent estimators several models distributed learning. paper motivated part success kernel methods machine learning consider nonparametric approach distributed estimation based regularized kernel least-squares regression. particular assume network sensors distributed plane sensor makes local observation ﬁeld. every sensor’s position ﬁeld measurement available central fusion center regularized least-squares estimator standard nonparametric estimate ﬁeld precisely assuming prior-knowledge spatial correlations ﬁeld encoded positive deﬁnite kernel standard centralized approach ﬁeld estimation minimize regularized empirical loss functional functions corresponding reproducing kernel hilbert space course limitations energy bandwidth sensors’ measurements positions available fusion center therefore centralized estimation schemes feasible wireless sensor networks. paper however suggest distributed algorithm computing approximation. arrive approximation ﬁrst interpret classic estimator projection onto hilbert space consider natural relaxation derived topology sensor network. relaxation suggests local message-passing algorithm based algorithm distributively estimating ﬁeld. several numerical experiments observed algorithm converges reasonable number iterations accuracy distributed estimate improved local message-passing algorithm estimate comparable centralized kernel least-squares estimator. work presented similar research presented general framework distributed kernel linear regression explored. network model essentially identical considered thus useful contrast methods. first whereas current work cast formally within context reproducing kernel hilbert spaces takes weight-space view attempts estimate ﬁeld linear combination local basis functions. secondly algorithm based distributed implementation gaussian elimination using junction trees. current approach message-passing algorithm derived algorithm network topology dependent relaxation centralized least-squares estimator. finally paper regularization incorporated model. alternating projection algorithms often applied areas statistical signal processing example alternating projection restricted least-squares regression computerized tomography approach taken problem localization sensor networks. however knowledge techniques applied generally problem distributed communication-constrained estimation. alternating projection methods often used algorithms parallel optimization algorithms similar presented paper useful help circumvent complexity induced massive data sets machine learning possibly parallelizing kernel methods. knowledge applied related context. given connection kernel methods gaussian processes familiar messagepassing algorithms belief-propagation algorithm presented familiar. formalizing connection would likely require interpret relaxation context dependency structures gaussian processes connect alternating projection algorithms generalized distributive remainder paper organized follows. section introduce notation review algorithm regularized least-squares regression reproducing kernel hilbert spaces. section formalize sensor network model derive algorithm distributed estimation sensor networks. section summarize several experiments simulated data algorithms evaluated. finally conclusions extensions future work discussed section algorithm often studied context convex feasibility problem wide-range signal processing applications generalized various ways often takes names below comment generalizations useful practical settings. sake space forego thorough examination alternating projection algorithms applications refer reader standard references topic reproducing kernel hilbert spaces often considered statistical signal processing popularized within machine learning community success kernel methods. section brieﬂy review regularized kernel least-squares regression order anchor notation. thorough introduction refer reader various references topic; example references therein. y-valued random variables respectively. known feature input observation space; known label output target space. sequel take model positions sensors plane take model real-valued sensor ﬁeld measurements. least-squares estimation problem seek decision rule mapping inputs outputs minimizes expected squared error. particular seek function minimizes well-known loss minimizing rule. however without prior knowledge joint distribution regression function cannot computed. supervised learning problem instead provided sequence input/output training examples learning task estimate regularized kernel least-squares methods oﬀer approach problem. particular denote reproducing kernel hilbert space induced positive semi-deﬁnite kernel precisely associate positive deﬁnite function unique collection functions identity reproducing property name derived. finally denote norm associated example canonical rkhs associated linear kernel example second canonical rkhs associated gaussian kernel exp−kx−x returning discussion least-squares estimation apply rkhs formalism regression problem. given positive semi-deﬁnite kernel often designed similarity measure inputs regularized kernel least-squares estimate deﬁned solution following optimization problem ﬁrst term objective function measurement well rule data; second term acts complexity control. constant parameter governs trade-oﬀ terms. learning context statistical behavior estimator well-understood various assumptions stochastic process generates examples moreover highly successful technique veriﬁed empirically applications ranging bioinformatics hand-written digit recognition. unfamiliar kernel methods note passing estimator useful variational interpretation context gaussian process estimation example posit statistical model paper focus algorithmic aspects computing solution distributed communication-constrained environments like sensor networks. consider following representer theorem proved originally result signiﬁcant states objective function deﬁned potentially inﬁnite dimensional hilbert space minimizer must ﬁnite dimensional subspace. kernel matrix practice matrix inversion often avoided using algorithms solving linear systems; often sparsity structure exploited. learning literature process computing given often called training whereas testing generally refers consider network sensors distributed plane {xi}n denote coordinates sensors’ positions. assume sensor accurately localize plane i.e. assume sensor knows assumption justiﬁed existence various localization algorithms wireless sensor networks follows interchangeably refer sensor network index position suppose sensors form wireless ad-hoc network topology described graph; example consider topology depicted figure node graph represents sensor network; edge nodes posits existence point-to-point communication link corresponding sensors. denote neighbors sensor interpret sensors sensor communicate directly. consistent interpretation abuse notation slightly assume every words sensor communicate itself. make additional assumptions structure graph however performance algorithms indeed eﬀected properties comment relationships below. note model capacity communication links network modeled explicitly. instead assume link support simple messages passed algorithm; assumption consistent applications message-passing algorithms sensor networks. network localized assume sensor makes noisy estimate ﬁeld position; denote measurement sensor posit relationship sensor’s measurement position. example could assume small. here expectation could taken respect probability distribution deﬁned instance distribution model mechanism sensors randomly positioned plane. fusion center access every sensor’s position temperature measurements regularized least-squares estimator natural choice. using formalism section take model positions sensors plane take model sensors’ real-valued ﬁeld measurements. choosing kernel reﬂect designer’s prior knowledge spatial correlation ﬁeld fusion center estimate ﬁeld solution here section denotes rkhs generated kernel course constraints energy bandwidth fusion center access data therefore cannot solved using centralized methods; general classical kernel methods infeasible sensor network. moreover even algorithm exists solve distributively sensors able share estimated model fusion center also complexity communicating. short assumption tight communication constraints preclude centralized training centralized testing model distributed regression. follows show optimization problem relaxed problem distributed regression. relaxation derived light topology sensor network suggest algorithm distributively estimating ﬁeld. algorithm sensor communicate neighboring sensors sensor locally determine global estimate ﬁeld. addition suggest several algorithms aggregating sensors’ estimates fusion center. here optimization variables {fi}n program data. coupling constraints dictate feasible solution every sensor’s associated function equivalent evaluated {xi}n result think equivalent form following sense. form regularized least-squares regression problem suggests natural relaxation allows incorporate communication model sensor network estimator. particular relax coupling constraints follows comparison coupling constraints require sensors’ functions equivalent evaluated shared neighbors’ locations. precisely feasible solution since assumed interpretation equivalent constraints stated formally fact straightforward derive using manipulation applied section signiﬁcance observation lies fact relaxed form regularized kernel least-squares estimator expressed projection onto intersection collection convex sets; particular note subspace. thus algorithm used solve relaxed problem moreover computing requires sensor gather information neighbors network. precisely note emphasize computing leaves unchanged leaves unchanged function associated sensor computed using {zj}j∈ni thus tying observations together left algorithm distributively estimating ﬁeld solving relaxed form regularized least-squares estimator algorithm sn-train summarized psuedo-code table depicted pictorially figure proceeding discussion algorithm brieﬂy mention important properties. first note asymptotic behavior sn-train implied analysis algorithm. particular following. lemma follows lemma fact convergence norm implies point-wise convergence rkhs. given structure rkhs general analysis sn-train expected converge linearly many kernels. forego discussion important technical point sake space. observe lemma characterizes output sn-train relative characterization useful insofar sheds light relationship output sn-train centralized regularized least-squares estimator. following straightforward generalization theorem step toward understanding important relationship. proof lemma follows original representer theorem fact closed. signiﬁcance result lies fact connectivity fundamentally limits accuracy sensor’s global estimate ﬁeld. particular sensor connected sensors otherwise dense network limited estimates span functions determined neighbors; thus connectivity inﬂuences bias network. intuitively however local message-passing sn-train optimizing estimator within limited span long network connected; intuition borne experiments section communication training phase sn-train algorithm communication occurs neighboring sensors query update stages iterative process. query stage message sensor sensor estimate ﬁeld sensor update stage message sensor sensor sensor estimate ﬁeld sensor emphasize though sensor retains locally estimated function messages passed network functions real numbers represent network’s estimate ﬁeld sensor locations. moreover though local information exchanged global information conveyed. particular examples rkhss paired sparse network topologies output sn-train equivalent centralized estimator. computation training phase sn-train computation occurs sensor updates fst. mild cstjk vector ir|ns|. substituting representer update equation sn-train update rule derived terms follows zst− ir|ns| appropriately deﬁned vector measurements sensor receives query stage. note principle sensor compute eﬃciently number neighbors reasonably small anticipated sensor networks. also note assuming ﬁeld slowly varying sensor network topology local kernel matrix likely sparse. parallelism described table inner loop sn-train iterates sensors network serially. note ordering non-essential parallelism introduced. fact sensors train simultaneously long share neighbors network practical settings various multiple-access algorithms adapted negotiate ordering distributed fashion. since algorithm lemma generalized general class control orderings lemma extended many cases. localization initialization phase sn-train sensors assumed store positions neighboring sensors. actually crucial sensor stores evaluation local kernel matrix ij∈ns ir|ns|×|ns| needed update however since many popular kernels depend distance sample points localization overkill. suﬃcient sensor estimate relative distances neighboring sensors. storage mentioned comments localization crucial sensor stores evaluation local kernel matrix moreover sensor must store representation fst. suﬃcient store additional |ns| coeﬃcients. thus storage requirement sensor inﬂuenced entirely many neighbors network. short sensor must maintain ﬂoating point numbers. centralized special case note useful think model distributed regression strict generalization centralized kernel linear regression framework. particular standard kernel linear regression model corresponds fully connected network sense described lemma robustness sn-train algorithm made robust changing network topology example sensor failures etc. sake space elaborate point here note passing sn-train adapted allow neighborhood sensor function time. resulting algorithm essentially identical sn-train moreover algorithm converges solution implied largest stationary neighborhood occurs inﬁnitely often well-deﬁned sense progress made iteration analysis robust approach requires several generalizations discussed aggregation discussed above unreasonable assume sensors share output distributed training algorithm {fst complexity communication wireless sensor networks. sake subsequent experimental studies assume fusion center wishes temperature employ following three strategies aggregate response network. single sensor fusion center simply choose sensor arbitrarily sensor future ﬁeld estimates. note rule feasible since sn-train sensor locally derives global estimate ﬁeld. connectivity-averaged fusion center take weighted average estimates provided sensors. weights average determined connected sensors network. particular fusion center consider following aggregation strategy section study sn-train numerically several simulations. indeed issues discussed section requires examination. however limited space focus attention. first study convergence rate sn-train. particular study number outer iterations needed expected error fusion center’s estimate converge. since even local communication burden power delay number necessary iterations excessive hope quantify aspect iterative approach. second study examine quality estimate derived sn-train. since algorithm distributed estimation ultimately validated accuracy estimate examine eﬀect local communication estimation quality. model description section assumed sensors distributed physically plane. however algorithm results hold euclidean∗ space thus simplify study take assume sensors distributed uniformly along interval {ni}n i.i.d. sequence zero-mean gaussian random variables variance consider cases diﬀer primarily choose keep problems interesting also modify according selection particular consider following cases. convenience case case contrasted pictorially figure random sampling sensor positions depicted along lower horizontal border plot; corresponding sampling sensor observations plotted well. case regression function linear hence local information useful constructing global ﬁeld estimates. case regression function constructed local information comparatively less useful constructing global ﬁeld estimates. discussed below diﬀerence inﬂuence performance sn-train various ways. follows examine convergence estimation quality experiments using case case parameter used deﬁne network topology random sensor network. particular sensors neighbors network distance less finally somewhat arbitrarily select regularization parameters investigate convergence rate sn-train vary regression function randomly sampled times; test error single sensor nearest neighbor connectivity-averaged fusion rule measured held test network trained distributively sn-train. results case case plotted figures respectively. left randomize random sensor positions noise sequences plot average test error right plot typical output estimate fusion rules. perspective error rate standard centralized kernel least-squares estimator plotted well. note error rate plots logarithmic dependent axis. observe case case convergence reasonably fast three fusion rules. particular note nearest-neighbor rule converges three outer iterations. further note nearest-neighbor fusion rule ultimately out-performs aggregation strategies competitive centralized estimator. moreover single sensor fusion rule quite poor; despite fact sensor retains global ﬁeld estimate bias estimate constrained lemma interesting note single sensor fusion competitive case case since linear rules global information useful local estimates individual sensors beneﬁt receiving local messages derived distant sensors. note passing observations fundamentally tied simulated relationship particular experiments neighboring sensors network topology kernels chosen. communicate expect correlated ﬁeld measurements. instance convergence rate fast nearest neighbor fusion rule general superior random network topology paired gaussian kernel. interest space explore important point paper. since sn-train adds complexity estimation algorithm important verify expense buys improved estimation accuracy. contrast training distributively sn-train local only approach. local approach make iteration network avoiding update step sn-train. sensor computes global estimate ﬁeld using samples observed neighborhood; however information estimates exchanged message variables. since diﬀerence using sn-train local only update step value communication isolated. perspective also construct standard centralized least-squares estimator estimator experiments select ensure sn-train converged. estimation accuracy measured expected squared error; particular randomly sample regression function times measure test error empirically held test set. study test error various levels connectivity network. case vary increments case vary increments randomize random sensor positions noise sequences plot average test error versus sn-train local only centralized approach. focus single sensor fusion rule allows compare local message passing inﬂuences sensor’s global estimate. intuitively grows expect error local approach decrease since sensor greater number neighbors. bias estimate decrease since lemma sensors’ estimates span larger basis functions; variance decrease since greater number samples incorporated average noise. priori trends expected sn-train. results cases plotted figure note clarity plot test error case logarithmically†. case expectations conﬁrmed local approach; test error decreases increased connectivity. moreover estimate derived sn-train also improves connectivity increases. case test error decreases connectivity sn-train local approach. attribute behavior fact studying single sensor fusion rule. levels connectivity individual sensors little information useful estimating ﬁeld outside neighborhood. thus sinusoidal regression function local approach incurs large test error performance assessed globally. interestingly local communication sn-train signiﬁcantly improves naive approach; techniques fundamentally constrained lemma local message passing sntrain leads better estimate ﬁxed level connectivity. further note certain degree connectivity estimate derived sn-train competitive centralized approach. level connectivity required local approach similarly competitive larger case compared case case network nearly fully connected local approach competitive centralized estimator. several contributions current paper literature distribution estimation. first though kernel methods advocated studied context distributed detection knowledge regularized reproducing kernel methods previously explored context distributed estimation. second alternating projection algorithms added list iterative algorithms usefully deployed problems distributed inference sensor networks. finally host statistical questions arise estimator formalized future work. algorithm generalized wide variety non-orthogonal projections including bregman projections thus using generalization similar algorithms developed distributed estimation sensor networks loss functions general regularizers. particular algorithms current paper generalized handle loss functions regularizers speciﬁed bregman divergences moreover discussed section sn-train generalized various ways allow parallel robust sensor update rules. future work consider extensions mentioned analyze statistical behavior sn-train. particular focus understanding design regularization parameters yield stable consistent estimates. work rely extensive literature statistics centralized kernel regression well various analyses alternating projection methods. delouille neelamani baraniuk robust distributed estimation sensor networks using embedded polygons algorithm ipsn’ proceedings third international symposium information processing sensor networks press pearl probabilistic reasoning intelligent systems networks plausible inference morgan kaufmann weiss freeman correctness belief propagation gaussian graphical models arbitrary topology rabbat nowak distributed optimization sensor networks ipsn’ proceedings third international symposium information processing sensor networks press s.-h. chiang kulkarni schwartz value clustering distributed estimation sensor networks proceedings ieee international conference wireless networks communications mobile computing june sch¨olkopf smola learning kernels press cambridge censor zenios parallel optimization theory algorithms applications oxford york guestrin bodi thibau paskin madde distributed regression eﬃcient framework modeling sensor network data ipsn’ proceedings third international symposium information processing sensor networks press stark image recovery theory application academic press orlando hero blatt sensor network source localization projection onto convex sets proceedings ieee international conference acoustics speech signal processing march simic learning theory approach sensor networks ieee pervasive computing nguyen wainwright jordan decentralized detection classiﬁcation using kernel methods", "year": 2005}