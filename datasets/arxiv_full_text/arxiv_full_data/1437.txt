{"title": "CNNs are Globally Optimal Given Multi-Layer Support", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Stochastic Gradient Descent (SGD) is the central workhorse for training modern CNNs. Although giving impressive empirical performance it can be slow to converge. In this paper we explore a novel strategy for training a CNN using an alternation strategy that offers substantial speedups during training. We make the following contributions: (i) replace the ReLU non-linearity within a CNN with positive hard-thresholding, (ii) reinterpret this non-linearity as a binary state vector making the entire CNN linear if the multi-layer support is known, and (iii) demonstrate that under certain conditions a global optima to the CNN can be found through local descent. We then employ a novel alternation strategy (between weights and support) for CNN training that leads to substantially faster convergence rates, nice theoretical properties, and achieving state of the art results across large scale datasets (e.g. ImageNet) as well as other standard benchmarks.", "text": "table image classiﬁcation results alternating training algorithm existing local descent algorithm applied different network architectures. algorithm achieves state-of-the-art results faster convergence rate nice theoretical properties many applications deep learning. central workhorse training modern cnns. although giving impressive empirical performance slow converge. paper explore novel strategy training using alternation strategy offers substantial speedups training. make following contributions replace relu non-linearity within positive hard-thresholding re-interpret nonlinearity binary state vector making entire linear multi-layer support known demonstrate certain conditions global optima found local descent. employ novel alternation strategy training leads substantially faster convergence rates nice theoretical properties achieving state results across large scale datasets well standard benchmarks. modern convolutional neural networks expressed simply terms composition afﬁne transform followed non-linear function relu max. output layer network expressed recursively afﬁne parameters previous layer’s output input signal. well understood applies relu vector resulting output sparse represents hadamard product operator. shall refer herein support individual elements constrained binary values further shall refer figure comparison training curves alternation strategy mnist. alternation iteration identiﬁed corresponding epoch number. alternating training converges faster similar performance. relu non-linearity layer instead replaced binary support. reinterpretation becomes completely linear knows multi-layer support input signal priori. obviously practice cannot determine multi-layer support without multi-layer afﬁne weights seems nothing gained modiﬁed network remains non-linear problematic optimize before. case alternation modern cnns typically optimized stochastic gradient descent efﬁcient optimization strategies possible quasinetwon methods however problematic high computational cost poor positive curvature objective functions general. alternation intepreted kind block coordinate descent global solution factor found keeping factors ﬁxed although alternation guaranteed reaching local minima objective reduced every iteration. argued convergence alternation iterations initially good typically computational overhead making attractive iteration scheme many large scale problems vision learning. discussed however alternation typically dismissed viable optimization strategy modern cnns. strategies like alternation make sense factors objective separated groups play relatively isolated roles. separation parameters problematic applied cnns nearly every parameter network heavily inﬂuences other. paper argue re-interpreting relu non-linearity binary support parameter orchestrate parameter separation makes alternation strategy attractive conventional sgd. speciﬁcally advocate strategy hold multi-layer support ﬁxed solve multi-layer afﬁne weights convergence. estimate support using updated weights iterating whole alternation process good solution found. necessary component proposed alternation strategy feasible approximate global solution cnn’s weights determined given known multi-layer support training example. seem ﬁrst glance tall order well documented modern cnns minor exceptions guarantees converging global solution. brings central insight paper well inspiring paper’s title. speciﬁcally demonstrate global solution weights proposed architecture assuming known multi-layer support training examples. result isolation little practical interest never knows multi-layer support input signal priori. however result considerable interest viewed applicability training cnns within efﬁcient alternation framework. demonstrate linearly separate weights support within modern using modiﬁed relu non-linearity. based separation demonstrate problem multi-layer afﬁne weight estimation represented determining rank tensor. further even though problem clearly non-convex characterize conditions global optima found using local descent assuming multi-layer support training examples known present empirical results show utility proposed alternation strategy optimization. speciﬁcally demonstrate substantially faster convergence results number benchmarks achieving state actual recognition performance finally discuss computational issues approach attempting learn large scale data naively would expected estimate multi-layer support every training example. speciﬁcally advocate novel modiﬁcation alternation strategy ﬁxed multi-layer support assumed across mini-batch training examples. demonstrate strategy obtain state performance still enjoying rapid convergence properties original naive approach deep neural networks revolutionary many ﬁelds machine learning artiﬁcial intelligence including computer vision speed recognition natural language processing. performance continues improve ever deepening network architectures optimization modern cnns still remain difﬁcult train nonconvexity optimization problem. typically optimized often cannot return global minima. efﬁcient optimization strategies exist quasi-netwon methods however incur high computational cost. recent work views forward pass thresholding pursuit serving multi-layered convolutional sparse coding model. authors argued improved results attainable employing sophisticated pursuit algorithm. promising results attained however computational complexity proposed pursuit strategy results limited small image datasets. optimality despite empirical success cnns theoretical underpinnings success optimality remains elusive. particular note regard drawback relu output mixes input threshold/bias complicating separation weights support. facilitate separation advocate instead positive hard thresholding operator replace relu within cnn. positive hard-thresholding deﬁned positive hard-thresholding facilitates decoupling weights support also show experiments section discernable empirical drawbacks compared relu even improves performance circumstances. also theoretical motivation change non-linearity. papyan recently demonstrated non-linearity relu viewed closed form solution following non-non-negative minimization η{q; argmin s.t. recent work kawaguchi studied optimality simplistic deep linear neural networks. kawaguchi proved local minimum point also global minimum critical point saddle point deep linear networks. further provided conditions critical point empirical risk function global minimum linear nonlinear networks. recently haeffele vidal derived sufﬁcient conditions guarantee local minima proposed network globally optimal requiring network output regularization positively homogeneous regularization designed control network size. however results apply networks hidden layer multiple subnetworks connected parallel. work restrictive assumptions networks. modern convolutional neural networks built collection feature layers followed small number fully connected layers. throughout theoretical treatment paper follow replace max-pooling convolutional layer increased strides. makes convolutional pooling layer linear leaving activation layers sole nonlinearity network. express network simply terms concatenation afﬁne transform followed non-linear function η{wx; relu. example three layer network expressed wη{wη{wx; β}−β. three layer network generalized layers expressing network output recursively deﬁnes number training examples n-th output label vector input vector respectively. numerous loss functions employed depending nature task ranging cross-entropy least-squares error. given bilinear form objective function characterize optimality recovering weights assuming masks given oracle. illustrate basic idea ﬁrst restrict two-layer neural network papyan proposed potentially closed form solution problem alternative non-linearity relu within cnn. closed form solution positive hard-thresholding non-linearity deﬁned equation armed positive hard-thresholding nonlinearity reinterpret manner. speciﬁcally layer network form indicator binary support vector mathematical induction implied multilayer always separated bilinear multiplication depends input examples binary support depends weights formally setting diag layer network function equation rewritten start evaluating alternation strategy four standard image classiﬁcation benchmarks cifar- cifar- mnist svhn network architectures employed convolutional network relu nonlinearity network network nonlinearities beyond relu derive method estimate multi-layer support every training example considered small datasets. resulting alternating training algorithm found often converge better local minima even relu sole nonlinearity. better results also come independent network architectures capacities well different datasets. importantly demonstrate much faster convergence properties algorithm. lastly propose batchbased modiﬁcation alternation algorithm scale large imagenet dataset show sharing ﬁxed multi-layer support across batch examples alternation algorithm computationally friendly naive per-example approach. experimented several popular deep highly nonlinear networks achieving state-of-the-art performance networks. performed experiments using caffe framework. image classiﬁcation small benchmarks small datasets simple implement alternation algorithm estimate support mask every training instance alternation. call approach i-alternation instance-wise mask stored even cached online small data. closedform solution mask obtained solving reconstruction problem note even though show condition global optimality occurs getting local optimality cannot guarantee polynomial time. also pointed vidal generally theorem shall generalized multi-layer cnns analogously using multi-dimensional tensors omitted sake brevity. recover high-quality weights supports ﬁxed estimate supports using updated weights iterating whole alternation process good solution found. consists convolutional layers relu nonlinearities softmax layer. linearized architecture provides perfect testbed specially evaluate capability relu-induced alternation algorithm. follow detailed training settings similarly models increasing network capacities. testing common practice apply relu testing example. predict image classes follow produce outputs different positions convolutional layer simply average whole image computing softmax probabilities. table conducts in-depth study various baselines without data augmentation. classiﬁcation error well number network parameters compared. following observations made table simple i-alternation approach already shows successful convergence competitive performance respect baseline empirical results consistent various network capacities. hand approach converges much faster baseline i-alternation+maskt baseline applies binary mask rather relu activations testing. surprisingly performance drops dramatically unacceptable. suggests learned mask preserves discrimination ability classiﬁcation also conﬁrming value standard testing procedure. cifar- cifar- table table compare i-alternation approach using all-cnn-c network state-of-the-art methods cifar cifar-. cifar- cases withdata augmentation method performs surpasses prior arts. cifar- approach achieves competitive performance well. note much smaller all-cnn-c network performing fractional pooling network thus train much faster. mnist svhn datasets adopt architecture nonlinearities relu pooling. therefore experiments regarded generalization test i-alternation algorithm different network architectures nonlinearities beyond relu. follow training hyper-parameters data preprocessing splitting datasets. table table compare results previous methods augment data. method performs comparable state arts again even though either complicated training schemes netfigure training curves alternation strategy cifar- mnist datasets. alternation iteration identiﬁed corresponding epoch number. observed alternating training reaches similar performance fewer iterations. ﬁnally discuss computational issues ialternation approach scaling large-scale data speciﬁcally claim estimating instance-wise support storage computationally demanding many large scale problems vision learning. propose batch-based modiﬁcation i-alternation algorithm called b-alternation ﬁxed multi-layer support shared across mini-batch training examples. closed-form solution batchwise mask obtained solving problem generate apply batch-wise masks on-the-ﬂy training. table ﬁrst examines balternation algorithm small datasets ﬁnds accuracy loss compared original i-alternation algorithm. b-alternation algorithm computationally friendly well plays important role large datasets. worth noting that also tried work architectures. shows efﬁcacy alternation algorithm consistent across networks datasets. claim algorithm applied complex networks compare recent methods even stronger performance datasets. experiments mainly validation test alternation algorithm’s efﬁcacy. analysis convergence speed good property alternation strategy strong performance comes consistent boost convergence speed. fig. compares training curves traditional i-alternation algorithm example datasets cifar- mnist note alternation iteration identiﬁed corresponding epoch number whose ranges epochs. approximately determined convergence speed keeping mask ﬁxed loss longer decreases considered dataset passed epochs turn estimate mask updated weights. seeing whole epoch training examples store individual masks used following training. networks randomly initialized weights generated masks meaningless beginning alternation. regular epochs obtain reasonable masks start whole alternation process. bias also initialized found good performance generally achieved. observed fig. i-alternation algorithm converges much faster conventional sgd. training loss sometimes increases abruptly time alternation mismatch outdated mask updated weights. overall i-alternation algorithm reaches similar performance fewer iterations. note masks known training examples proved able global optima certain conditions still choose majority vote among individual masks batch produce batch-wise mask. although theoretically grounded still achieve comparable results b-alternation algorithm. report numbers avoid cluttering experiments. next apply novel algorithm large imagenet dataset. choose deep googlenet resnet- higher levels nonlinearities relu practical values wide use. followed training ﬂow. ﬁrst train dense network learn full weight connections. sparse step network regularized pruning connections small weights ﬁnetuning rest. ﬁnal dense network obtained retraining connections previously pruned weights re-initialized zero. balternation algorithm integrated sparse step sparse nature generated masks. running alternation algorithm step simply prune connections zero mask eliminating need sparsity degree train number epochs change training settings hyper-parameters. table summarizes top- error rates base network training variants using alternation algorithms. surprisingly method able achieve best results types network architectures imagenet. suggests found support offers better guidance reducing weights redundancy. compared original i-alternation algorithm b-alternation variant found achieve stronger results. hypothesis batch-wise regularization help avoid overﬁtting escape noisy local minima instance-wise optimization. also batch-wise operation reﬂects current good practices training thus enables parallelization future. furthermore b-alternation algorithm typically computational overhead still enjoying rapid convergence properties i-alternation algorithm motivates propose novel alternation strategy weights support training leads substantially faster convergence rates nice theoretical properties. prove that certain condition global optimal solution weights obtained multi-layer support known. empirical results support utility success proposed alternation strategy achieves state results across large scale imagenet standard benchmarks. future work plan explore efﬁcient alternatives solve weights alternation process. motivated fact even though provide conditions local minima globally optimal local descent still canguarantee polynomial time room further speedups. also remark theoretical results actually open elegant connect deep learning sparse dictionary learning well. furthermore show proposed alternation strategy offer guidance applications network compression binarization facilitate efﬁcient network training storage.", "year": 2017}