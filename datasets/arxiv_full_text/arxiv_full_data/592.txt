{"title": "Learning Multiscale Features Directly From Waveforms", "tag": ["cs.CL", "cs.LG", "cs.NE", "cs.SD"], "abstract": "Deep learning has dramatically improved the performance of speech recognition systems through learning hierarchies of features optimized for the task at hand. However, true end-to-end learning, where features are learned directly from waveforms, has only recently reached the performance of hand-tailored representations based on the Fourier transform. In this paper, we detail an approach to use convolutional filters to push past the inherent tradeoff of temporal and frequency resolution that exists for spectral representations. At increased computational cost, we show that increasing temporal resolution via reduced stride and increasing frequency resolution via additional filters delivers significant performance improvements. Further, we find more efficient representations by simultaneously learning at multiple scales, leading to an overall decrease in word error rate on a difficult internal speech test set by 20.7% relative to networks with the same number of parameters trained on spectrograms.", "text": "figure diagram multiscale convolutions learning directly waveforms. sampled waveform originally frame. three convolutions different window sizes strides applied leading feature maps high temporal resolution. next maxpooling concatenation ensure consistent sampling frequency rest network. diagram shows real feature maps extracted applying learned features recording authors saying like cats mandarin. forms. multiscale convolutions already successfully applied address tasks computer vision ﬁeld image classiﬁcation scene labeling gesture detection successful applications exploit structure different scales encourages explore multiscale representations waveforms well. further multiscale convolutions enable split spectrum different ﬁlter banks independent choice stride window size number ﬁlters. learn high frequency features ﬁlters short window applied small stride waveforms. low-frequency features contrary employ long window applied larger stride. finally based needs speech recognition system independently tune number ﬁlters different frequency bands. deep learning dramatically improved performance speech recognition systems learning hierarchies features optimized task hand. however true end-toend learning features learned directly waveforms recently reached performance handtailored representations based fourier transform. paper detail approach convolutional ﬁlters push past inherent tradeoff temporal frequency resolution exists spectral representations. increased computational cost show increasing temporal resolution reduced stride increasing frequency resolution additional ﬁlters delivers signiﬁcant performance improvements. further efﬁcient representations simultaneously learning multiple scales leading overall decrease word error rate difﬁcult internal speech test relative networks number parameters trained spectrograms. index terms speech recognition multiscale learning convolutional neural networks waveforms speech waveforms densely sampled time thus require downsampling make many analysis techniques computationally tractable. speech recognition presents challenge reduce number timesteps signal withthrowing away relevant information. representations based fourier transform proven effective task transform forms complete basis signal reconstruction. deep learning’s recent success speech recognition based learning feature hierarchies atop representations increasing focus extending end-toend learning approach level waveform. popular approach pass waveform strided convolutions networks connected local temporal frames often followed pooling step create invariance phase shifts downsample signal studies inferior performance convolutional ﬁlters learned deeper networks recently matched performance hand-engineered features large vocabulary speech recognition tasks features based fourier transform computationally efﬁcient exhibit intrinsic tradeoff temporal frequency resolution. convolutional ﬁlters decouple time frequency resolution number ﬁlters stride chosen independently. despite this ﬁlter bank constrained window size single scale. herein explore jointly learning ﬁlter banks multiple different scales wavetable neural network architectures explored study. architecture held constant except waveform convolution layer. baseline compared different single multiscale convolutions. added signal-to-noise ratios ranging input data sampled normalized input feature zero mean unit variance. models learn directly waveforms preprocessing applied network inputs feature frame. models learn spectrograms linear features extracted size window size network inputs thus spectral magnitude maps ranging -khz features frame. train using stochastic gradient descent nesterov momentum batch size hyperparameters tuned model optimizing hold-out set. typical values learning rate momentum training converges epochs. following sort ﬁrst epoch utterance length promote stability training long utterances. ctc-trained acoustic model learns rudimentary language model training data testing supplement kneser-ney smoothed -gram model trained using kenlm toolkit cleaned text common crawl repository. decoding done beam search weighted combination acoustic model language model added word insert penalty used value function. test word error rates reported difﬁcult in-house test utterances diversely composed noisy conversational voice-command accented speech. test collected internally industry partners represented training data. previously observed deep neural networks trained sufﬁcient data perform better model size grows. order make fair comparisons number parameters models used experiments held constant aware results directly comparable literature proprietary datasets. however attempt tightly control experiments rather focus overall figure stride matters. single scale convolution reduces decreasing stride passing spectrogram baseline stride. smaller strides convolution larger strides maxpooling keeping ms/frame context constant. computationally efﬁcient larger strides correspond sparsely subsampling data lose information compared pooling. perform in-depth analysis scaling strides large numbers ﬁlters discover convolutional front signiﬁcantly outperform fourier features independently tuning temporal frequency resolution cost additional computation memory. propose multiscale convolutional front composed concatenated ﬁlters different window sizes requires less computation outperforms features learned single scale multiscale features naturally learn frequencies efﬁciently represent large small windows learning high frequencies respectively. contrasts single scale features cover entire frequency spectrum regardless window size. experimental design study modelled previous work end-to-end speech recognition however decrease experimental latency train reduced version model subset training data. basic architecture shown table vary front processing backend remains same convolutional layer followed three bidirectional simple recurrent layers fully connected layer. batch normalization employed layer individual timesteps rectiﬁed linear unit activation functions used layers including timesteps. connectionist temporal classiﬁcation cost function integrate possible alignments network outputs characters english alphabet training conducted hours audio randomly sampled hours data. training data drawn diverse collection sources including read conversational accented noisy speech epoch utterances randomly selected background noise figure multiscale features naturally separate according frequency. plots spectral centroid learned ﬁlters scales learned separately versus jointly quantitative comparison mean ﬁlter frequencies bank also shown. separate case ﬁlter bank tries span entire frequency range larger windows emphasis frequencies. learned jointly ﬁlter banks split responsibilities based efﬁciently represent smaller windows putting emphasis high frequencies larger windows dedicating ﬁlters frequencies. performance conclusions generalize architectures datasets well. optimizing performance worth noting drops training hours data bidirectional layers backend. many studies compared convolution waveforms features spectrograms mfccs melscale ﬁlterbanks often compare similar strides window sizes spectrograms employ high stride unique analytic structure basis functions. integrating twice real cosine imaginary sine counterpart identiﬁes phase magnitude response. many ways similar performing convolution every timestep max-pooling phase represented index occurs much computationally efﬁcient perform fft. decided test hypothesis convolution stride pooling least good basis spectrogram. figure table show effects replacing spectrogram single convolution pooling layer number ﬁlters window size. decrease stride convolution stride pooling increased give consistent total stride stride spectrogram pooling. comparable strides spectrogram convolution unable perform well likely needing represent phase shifts well frequency variation. however stride dips convolution asymptotically reaches superior level performance spectrogram. fair improved performance comes increased computational cost memory usage. representing high frequency information large ﬁlter difﬁcult many places information occur ﬁlter window. similarly representing frequency information small ﬁlter challenging separate ﬁlters required separate parts wave. hypothetable multiscale single scale features. holding number ﬁlters constant single scale convolution high frequency frequency frequency outperformed combination three sized applying convolution simultaneously several scales could allow scale learn ﬁlters selective frequencies efﬁciently represent. test hypothesis compare performance convolutional front ends constant number ﬁlters three different scales front employing three scales figure displays spectral centroids ﬁlter bank sorted frequency mean value printed alongside. clear high ﬁlter banks live names learning ﬁlters capture different frequency bands. jointly learn several scales ﬁlters exhibit heightened preference representing different bands. relaxing requirement bank cover entire spectrum causes high banks increase mean frequency banks decrease. further lowest multiscale features despite fact fewer parameters three times less large ﬁlters. figure representative learned multiscale ﬁlters. clear preference fourier-like wavelet representations varying degrees high frequency noise. ﬁlters also show combination frequencies localized transient structure. phase shifted ﬁlter pairs also found suggesting importance phase information speech recognition tasks. table decoupling time frequency resolution. window size shown parentheses. adding additional ﬁlters stride signiﬁcantly improves performance. models additional bottleneck layer inserted pooling maintain number features previous experiments. ∗increasing bottleneck layer leads small improvement. quencies present window size smears knowledge occur. decreasing stride cannot increase temporal resolution provide samples smeared signal. convolutions suffer tradeoff temporal resolution limited number ﬁlters temporal resolution stride independent. advantage comes added cost increased computation memory increasing number ﬁlters decreasing stride. explore value increased resolution performing multiscale experiments previous section increasing number ﬁlters. table demonstrates increasing number ﬁlters even factor leads signiﬁcant improvement wer. fullyconnected layer output dimension added concatenated feature maps order produce number features previous experiments. since using ﬁlters short strides costly terms computation memory speciﬁcally increase number ﬁlters long strides. increasing number ﬁlters expanding size bottle neck leads smaller gains. showing impressive cumulative improvement relative spectrogram baseline. paper consistently demonstrated learning features directly waveforms outperform spectrograms especially applied multiple scales. however several interesting research questions remain answered techniques likely widespread adoption. time/frequency tradeoff fourier considerable computational memory cost. many modern state systems train clusters gpus memory precious requiring memory transfer prohibitively slow training. especially problematic training long utterances amount memory required save activations increases number ﬁlters reduction stride. remains seen whether power learned input features combined efﬁciency analytic signal transformations fourier transform. approach could learn basis functions real imaginary domain performing backpropagation hilbert transformation enabling larger strides. alternatively learned features ﬁxed used augment ﬁxed features train time. sainath found noticeable improvements supplementing log-mel ﬁlterbanks manner. learned features outperformed spectrograms feeding temporal convolution study many state systems apply two-dimensional convolutions inputs learned features underperform context understandable spectrally ordered lack spatial structure. regularization techniques could perhaps learning ordered ﬁlter maps useful structure. experiments made sure downsample scale equally appropriate stride signals concatenated later recurrent layers. temporal pooling takes account local structure explicit knowledge information preserve based longrange dependencies. recently proposed architectures operate simultaneously different timescales clockwork could provide elegant combining multiscale signals. beyond incorporating recurrence frequency features require fewer temporal samples could also require less recurrent computation facilitate modeling long-range structure. finally observing representative ﬁlters learned scale figure redundancy representation. ﬁlter shapes appear similar multiple scales. interesting future direction could investigate learning features scale-free basis similar wavelets reduced basis could applied across range scales. ranzato huang boureau lecun unsupervised learning invariant feature hierarchies applications object recognition computer vision pattern recognition cvpr ieee conference june hinton deng dahl rahman mohamed jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition signal processing magazine amodei anubhai battenberg case casper catanzaro chen chrzanowski coates diamos elsen engel fougner hannun legresley narang ozair prenger raiman satheesh seetapun sengupta wang wang wang xiao yogatama zhan deep speech end-to-end speech recognition english mandarin corr vol. abs/. available http//arxiv.org/abs/. sainath weiss senior wilson vinyals learning speech front-end waveform cldnns sixteenth annual conference international speech communication association golik t¨uske schl¨uter convolutional neural networks acoustic modeling time signal lvcsr sixteenth annual conference international speech communication association t¨uske golik schl¨uter acoustic modeling deep neural networks using time signal lvcsr proceedings annual conference international speech communication association hannun case casper catanzaro diamos elsen prenger satheesh sengupta coates deepspeech scaling end-to-end speech recognition arxiv preprint arxiv. ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift corr vol. abs/. available http //arxiv.org/abs/. graves fern´andez gomez schmidhuber connectionist labelling unsegmented sequence data recurrent neural networks proceedings international conference machine learning. heaﬁeld pouzyrevsky clark koehn scalable modiﬁed kneser-ney language model estimation proceedings annual meeting association computational linguistics soﬁa bulgaria available http//kheaﬁeld.com/professional/edinburgh/estimate paper.pdf", "year": 2016}