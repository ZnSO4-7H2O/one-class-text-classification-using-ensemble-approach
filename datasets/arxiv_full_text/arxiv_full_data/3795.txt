{"title": "HoME: a Household Multimodal Environment", "tag": ["cs.AI", "cs.CL", "cs.CV", "cs.RO", "cs.SD", "eess.AS"], "abstract": "We introduce HoME: a Household Multimodal Environment for artificial agents to learn from vision, audio, semantics, physics, and interaction with objects and other agents, all within a realistic context. HoME integrates over 45,000 diverse 3D house layouts based on the SUNCG dataset, a scale which may facilitate learning, generalization, and transfer. HoME is an open-source, OpenAI Gym-compatible platform extensible to tasks in reinforcement learning, language grounding, sound-based navigation, robotics, multi-agent learning, and more. We hope HoME better enables artificial agents to learn as humans do: in an interactive, multimodal, and richly contextualized setting.", "text": "introduce home household multimodal environment artiﬁcial agents learn vision audio semantics physics interaction objects agents within realistic context. home integrates diverse house layouts based suncg dataset scale facilitate learning generalization transfer. home open-source openai gym-compatible platform extensible tasks reinforcement learning language grounding soundbased navigation robotics multi-agent learning more. hope home better enables artiﬁcial agents learn humans interactive multimodal richly contextualized setting. human learning occurs interaction multimodal experience prior work argued machine learning also beneﬁt interactive multimodal learning termed virtual embodiment driven breakthroughs static unimodal tasks image classiﬁcation language processing machine learning moved direction. recent tasks visual question answering image captioning audio-video classiﬁcation make steps towards learning multiple modalities lack dynamic responsive signal exploratory learning. modern challenging tasks incorporating interaction atari push agents learn complex strategies trial-and-error miss information-rich connections across vision language sounds actions. remedy shortcomings subsequent work introduces tasks multimodal interactive successfully training virtually embodied agents that example ground language actions visual percepts worlds virtual embodiment reach full potential though agents immersed rich lifelike context humans are. agents learn ground concepts various modalities also relationships concepts i.e. forks often kitchens near living rooms contain sofas etc. humans learn concept-to-concept association shown child learning psychology cognitive science neuroscience linguistics even machine learning contextual information given rise effective word representations improvements recommendation systems increased reward quality robotics importantly scale data proven algorithms learning context general present home household multimodal environment home large-scale platform agents navigate interact within hand-designed houses suncg dataset speciﬁcally home provides visual renderings based pandad. acoustic renderings based evert using ray-tracing high ﬁdelity audio. semantic image segmentations language descriptions objects. physics simulation based bullet handling collisions gravity agent-object interaction more. multi-agent support. python framework integrated openai home general platform extensible many speciﬁc tasks reinforcement learning language grounding blind navigation real-world context. home also ﬁrst major interactive platform support high ﬁdelity audio allowing researchers better experiment across modalities develop tasks. home ﬁrst platform provide realistic context show following sections home provides large-scale multimodal testbed existing environments making conducive virtually embodied learning many scenarios. community built numerous platforms drive algorithmic advances arcade learning environment openai universe minecraft-based malmo maze-based deepmind doom-based vizdoom ai-thor matterportd simulator housed several environments created powerful sandboxes developing learning algorithms home additionally aims provide uniﬁed platform multimodal learning realistic context table compares environments home. closely related environments home housed ai-thor matterportd simulator three household environments. housed concurrently developed environment also based suncg housed lacks sound true physics simulation capability interact objects aspects multimodal interactive learning. ai-thor matterportd simulator environments focused speciﬁcally visual navigation using photorealistic houses respectively. home instead aims provide extensive number houses easy integration multiple modalities tasks. house datasets could also turned interactive platforms datasets large-scale suncg consists house layouts. datasets include stanford scenes matterportd scenenn scenenet scenenet rgb-d used suncg scale diversity data proven critical machine learning algorithms generalize transfer simulation real suncg’s simpler graphics also allow faster rendering. environment atari openai universe malmo deepmind vizdoom ai-thor matterportd simulator housed home table comparison modern environments home. supports settings. context provides realistic context. large-scale thousands readily available environments. fast renders quickly. customizable adaptable towards various speciﬁc tasks. physics supports rigid body dynamics external forces agents objects. acoustics renders audio. photorealistic lifelike visual rendering. overviewed figure home interactive extension suncg dataset suncg provides hand-designed house layouts containing hand-designed rooms sometimes multiple ﬂoors. within rooms kinds objects among categories average objects room. shown figure home consists several distinct components built suncg utilized individually. platform runs faster real-time single-core enables acceleration allows users multiple environment instances parallel. features facilitate faster algorithmic development learning data. home provides openai gym-compatible environment loads agents randomly selected houses lets explore actions moving looking interacting objects home also enables multiple agents spawned once. following sections detail home’s core components. rendering engine implemented using pandad open-source game engine ships complete python bindings. suncg house home renders rgb+depth scenes based house object textures multi-source lighting shadows. rendering engine enables tasks vision-based navigation imitation learning planning. module provides image depth image. acoustic engine implemented using evert handles real-time acoustic ray-tracing based house object geometry. evert also supports multiple microphones sound sources distance-dependent sound attenuation frequency-dependent material absorption reﬂection air-absorption based atmospheric conditions sounds instantiated artiﬁcially based environment module provides stereo sound frames agents w.r.t. environmental sound sources. home provides short text description object well following semantic information color calculated object textures discretized basic colors intermediate semantics home extended generate language instructions scene descriptions questions home also provide agents dense ground-truth semanticallyannotated images based suncg’s ﬁne-grained categories module provides image segmentations object semantic attributes text descriptions. physics engine implemented using bullet engine. objects home provides rigid body representations fast minimal bounding approximation exact mesh-based body. objects subject external forces gravity based volume-based weight approximations. physics engine also allows agents interact objects picking dropping pushing etc. features useful applications robotics language grounding instance. module provides agent object positions velocities physical interaction collision. dialogue agent converses oracle full scene knowledge solve difﬁcult task. colors based large-scale survey randall munroe including relevant shades macaroni pied piper agent must follow another speciﬁc agent several making speciﬁc sounds. home’s advanced acoustics allow agents multichannel microphones perform sound source localization disentanglement task. multi-agent communication multiple agents communicate solve task maximize shared reward. example agent might know reward locations must guide agents. household multimodal environment provides platform agents learn within world context hand-designed houses high ﬁdelity sound simulated physics comprehensive semantic information object multi-agent interaction. rich setting many speciﬁc tasks designed relevant robotics reinforcement learning language grounding audio-based learning. home’s scale also facilitate better learning generalization transfer. hope research community uses home stepping stone towards virtually embodied general-purpose grateful collaborative research environment provided mila. also acknowledge following agencies research funding computing support cifar chistera iglu cper nord-pas calais/feder data advanced data science technologies calcul québec compute canada google. thank nvidia donating dgx- tesla used work. lastly thank acronymcreator.net acronym home. karl moritz hermann felix hill simon green fumin wang ryan faulkner hubert soyer david szepesvari wojtek czarnecki jaderberg denis teplyashin grounded language learning simulated world. arxiv preprint arxiv. abhinav dhall ramana murthy roland goecke jyoti joshi gedeon. video image based emotion recognition challenges wild emotiw international conference multimodal interaction david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature devendra singh chaplot kanthashree mysore sathyendra rama kumar pasumarthi dheeraj rajagopal ruslan salakhutdinov. gated-attention architectures task-oriented language grounding. arxiv preprint arxiv. lawrence barsalou. grounded cognition. annual review psychology kazu nakazawa michael quirk raymond chitwood masahiko watanabe mark yeckel linus akira kato candice carr daniel johnston matthew wilson requirement hippocampal nmda receptors associative memory recall. science jaderberg volodymyr mnih wojciech marian czarnecki schaul joel leibo david silver koray kavukcuoglu. reinforcement learning unsupervised auxiliary tasks. arxiv preprint arxiv. olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei-fei. imagenet large scale visual recognition challenge. ijcv ondˇrej bojar rajen chatterjee christian federmann barry haddow matthias huck chris hokamp philipp koehn varvara logacheva christof monz matteo negri matt post carolina scarton lucia specia marco turchi. findings workshop statistical machine translation. workshop statistical machine translation charles beattie joel leibo denis teplyashin ward marcus wainwright heinrich küttler andrew lefrancq simon green víctor valdés amir sadik deepmind lab. arxiv preprint arxiv. michał kempka marek wydmuch grzegorz runc jakub toczek wojciech ja´skowski. vizdoom doom-based research platform visual reinforcement learning. computational intelligence games peter anderson damien teney jake bruce mark johnson niko sünderhauf reid stephen gould anton hengel. vision-and-language navigation interpreting visually-grounded navigation instructions real environments. arxiv preprint arxiv. angel chang angela thomas funkhouser maciej halber matthias niessner manolis savva shuran song andy zeng yinda zhang. matterportd learning rgb-d data indoor environments.", "year": 2017}