{"title": "Good Semi-supervised Learning that Requires a Bad GAN", "tag": ["cs.LG", "cs.AI"], "abstract": "Semi-supervised learning methods based on generative adversarial networks (GANs) obtained strong empirical results, but it is not clear 1) how the discriminator benefits from joint training with a generator, and 2) why good semi-supervised classification performance and a good generator cannot be obtained at the same time. Theoretically, we show that given the discriminator objective, good semisupervised learning indeed requires a bad generator, and propose the definition of a preferred generator. Empirically, we derive a novel formulation based on our analysis that substantially improves over feature matching GANs, obtaining state-of-the-art results on multiple benchmark datasets.", "text": "semi-supervised learning methods based generative adversarial networks obtained strong empirical results clear discriminator beneﬁts joint training generator good semi-supervised classiﬁcation performance good generator cannot obtained time. theoretically show given discriminator objective good semisupervised learning indeed requires generator propose deﬁnition preferred generator. empirically derive novel formulation based analysis substantially improves feature matching gans obtaining state-of-the-art results multiple benchmark datasets. deep neural networks usually trained large amount labeled data challenge apply deep models datasets limited labels. semi-supervised learning aims leverage large amount unlabeled data boost model performance particularly focusing setting amount available labeled data limited. traditional graph-based methods extended deep neural networks involves applying convolutional neural networks feature learning techniques graphs underlying manifold structure exploited. employs ladder network minimize layerwise reconstruction loss addition standard classiﬁcation loss. variational auto-encoders also used semi-supervised learning maximizing variational lower bound unlabeled data log-likelihood. recently generative adversarial networks demonstrated able generate visually realistic images. gans adversarial game discriminator generator. goal discriminator tell whether sample drawn true data generated generator generator optimized generate samples distinguishable discriminator. feature matching gans apply gans semi-supervised learning kclass classiﬁcation. objective generator match ﬁrst-order feature statistics generator distribution true distribution. instead binary classiﬁcation discriminator employs -class objective true samples classiﬁed ﬁrst classes generated samples classiﬁed class. -class discriminator objective leads strong empirical results later widely used evaluate effectiveness generative models though empirically feature matching improves semi-supervised classiﬁcation performance following questions still remain open. first clear formulation discriminator improve performance combined generator. second seems good semisupervised learning good generator cannot obtained time. example observed mini-batch discrimination generates better images feature matching feature matching obtains much better semi-supervised learning performance. phenomenon also observed model generated better images failed improve performance semi-supervised learning. work take step towards addressing questions. first show given current -class discriminator formulation gan-based good semi-supervised learning requires generator. mean generator distribution match true data distribution. then give deﬁnition preferred generator generate complement samples feature space. theoretically mild assumptions show properly optimized discriminator obtains correct decision boundaries high-density areas feature space generator complement generator. based theoretical insights analyze feature matching works -dimensional datasets. turns practical observations align well theory. however also feature matching objective several drawbacks. therefore develop novel formulation discriminator generator objectives address drawbacks. approach generator minimizes divergence generator distribution target distribution assigns high densities data points densities true distribution corresponds idea complement generator. furthermore enforce assumptions theoretical analysis conditional entropy term discriminator objective. empirically approach substantially improves vanilla feature matching gans obtains state-of-the-art results mnist svhn cifar- methods compared discriminator architecture. results mnist svhn also represent state-of-the-art amongst single-model results. besides adversarial feature matching approach several previous works incorporated idea adversarial training semi-supervised learning. notably proposes categorical generative adversarial networks substitutes binary discriminator standard multi-class classiﬁer trains generator discriminator using information theoretical criteria unlabeled data. perspective regularization propose virtual adversarial training effectively smooths output distribution classiﬁer seeking virtually adversarial samples. worth noting bears similar merit approach learn auxiliary non-realistic samples rather realistic data samples. despite similarity principles approach orthogonal aims enforce smooth function leverage generator better detect low-density boundaries. different aforementioned approaches proposes train conditional generators adversarial training obtain complete sample pairs directly used additional training cases. recently triple also employs idea conditional generator uses adversarial cost match model-deﬁned factorizations joint distribution deﬁned paired data. apart adversarial training efforts semi-supervised learning using deep generative models recently. early work adapts original variational auto-encoder semi-supervised learning setting treating classiﬁcation label additional latent variable directed generative model. adds auxiliary variables deep structure make variational distribution expressive. boosted model expressiveness auxiliary deep generative models improve semi-supervised learning performance upon semi-supervised vae. different explicit usage deep generative models ladder networks take advantage local denoising auto-encoding criterion create informative unsupervised signal lateral connection. true data distribution. probability distribution classes ﬁrst classes true classes class fake class. objective function consists three terms. ﬁrst term maximize conditional probability labeled data standard cost supervised learning setting. second term maximize probability ﬁrst classes unlabeled data. third term maximize probability class generated data. note objective function bears similar merit original formulation treat probability fake samples difference split probability true samples sub-classes. nonlinear vector-valued function weight vector class standard setting previous work discriminator deﬁned since form over-parameterization ﬁxed zero vector next discuss choices different possible g’s. here perfect generator mean generator distribution exactly matches true data distribution i.e. show generator perfect improve generalization supervised learning setting. proposition inﬁnite capacity optimal solution following supervised objective proof provided supplementary material. proposition states optimal solution supervised objective exists optimal solution -class objective share generalization error. words using -class objective prevent model experiencing arbitrarily high generalization error could suffer supervised objective. moreover since optimal solutions equivalent w.r.t. -class objective optimization algorithm really decides speciﬁc solution model reach thus generalization performance achieve. implies generator perfect -class objective able improve generalization performance. fact many applications almost inﬁnite amount unlabeled data available learning perfect generator purely sampling purposes useful. case theory suggests generator help also unlabeled data effectively utilized generator perfect. function maps data points input space feature space. density data points class feature space. given threshold subset data support i.e. \u0001k}. assume given {\u0001k}k fk’s disjoint margin. formally assume exists real number long probability densities different classes share mode i.e. argmaxf argmaxf assumption always satisﬁed tuning thresholds \u0001k’s. assumption held show model performance would better thresholds could smaller values also assume contains least labeled data point. suppose k=fk bounded convex support generator feature space relative complement i.e. k=fk call complement generator. reason utilize bounded deﬁne complement presented supplementary material. note deﬁnition complement generator implies function treating function theoretically optimize original objective function present assumption convergence conditions discriminator. sets unlabeled data generated data. assumption convergence conditions. converges ﬁnite training {lug} learns correct decision boundary training data points. speciﬁcally class maxk assumption conditions assume classiﬁcation correctness labeled data true-fake correctness generated data respectively directly induced objective function. likewise also reasonable assume true-fake correctness unlabeled data i.e. however condition goes beyond assumes maxk discuss issue detail supplementary material argue assumptions reasonable. moreover section approach addresses issue explicitly adding conditional entropy term discriminator objective enforce condition lemma suppose l-norms weights bounded suppose exists exists conditions assumption corollary unlimited generated data samples available conditions lemma lim|g|→∞ proof. without loss generality suppose maxj=k prove contradiction. since fk’s disjoint margin convex suppose ∪kfk exists thus feature labeled data point corollary follows leading contradiction. follows proposition guarantees complement generator mild assumptions nearoptimal learns correct decision boundaries high-density subset data support feature space. intuitively generator generates complement samples logits true classes forced complement. result discriminator obtains class boundaries low-density areas. builds connection approach manifold-based methods also leverage low-density boundary assumption. theoretical analysis answer questions raised section first class formulation effective generated complement samples encourage discriminator place class boundaries low-density areas second good semi-supervised learning indeed requires generator perfect generator able improve generalization performance previous section established fact complement generator instead perfect generator makes good semi-supervised learning algorithm. intuitive understanding conduct case study based synthetic datasets easily verify theoretical analysis visualizing model behaviors. addition analyzing feature matching works space identify potential problems motivates approach introduced next section. speciﬁcally synthetic datasets four spins circles shown fig. soundness complement generator firstly verify complement generator preferred choice construct complement generator uniformly sampling bounded contains unlabeled data removing manifold. based complement generator result four spins visualized fig. expected classiﬁcation true-fake decision boundaries almost perfect. importantly classiﬁcation decision boundary always lies fake data area well matches theoretical analysis. visualization feature space next verify analysis feature space choose feature dimension apply simpler dataset circles visualize feature space fig. generated features resides features classes although exists overlap. result discriminator almost perfectly distinguish true generated samples indicated black decision boundary satisfying required assumption meanwhile model obtains perfect classiﬁcation boundary analysis suggests. pros cons feature matching finally understand strength weakness analyze solution reaches four spins shown fig. left panel many generated samples actually fall data manifold rest scatters around nearby surroundings data manifold. suggests matching ﬁrst-order moment performing kind distribution matching though rather weak manner. loosely speaking effect generating samples close manifold. weak power distribution matching inevitably generate samples outside manifold especially data complexity increases. consequently generator density usually lower true data density within manifold higher outside. hence optimal discriminator could still distinguish true generated samples many cases. however types mistakes discriminator still make higher density mistake inside manifold since generator still assigns signiﬁcant amount probability mass inside support wherever optimal discriminator incorrectly predict samples region fake. actually problem already shown examine feature space collapsing missing coverage outside manifold feature matching objective generator requires matching ﬁrst-order statistics exists many trivial solutions generator with. example simply collapse mean unlabeled features surrounding modes along feature mean matches. actually collapsing phenomenon high-dimensional experiments used result collapsed generator fail cover areas manifolds. since discriminator well-deﬁned union data supports prediction result missing area under-determined fully relies smoothness parametric model. case signiﬁcant mistakes also occur. discussed previous sections feature matching gans suffer following drawbacks ﬁrst-order moment matching objective prevent generator collapsing feature matching generate high-density samples inside manifold; discriminator objective encourage realization condition assumption discussed section approach aims explicitly address drawbacks. following prior work employ gan-like implicit generator. ﬁrst sample latent variable uniform distribution dimension apply deep convolutional network transform sample fundamentally ﬁrst drawback concerns entropy distribution generated features connection rather intuitive collapsing issue clear sign entropy. therefore avoid collapsing increase coverage consider explicitly increasing entropy. although idea sounds simple straightforward practical challenges. firstly implicit generative models gans provide samples rather analytic density form. result cannot evaluate entropy exactly rules possibility naive optimization. problematically entropy deﬁned high-dimensional feature space changing dynamically throughout training process. consequently difﬁcult estimate optimize generator entropy feature space stable reliable way. faced difﬁculties consider practical solutions. ﬁrst method inspired fact input space essentially static estimating optimizing counterpart quantities would much feasible. hence instead increase generator entropy input space i.e. using technique derived information theoretical perspective relies variational inference specially latent variable space input space. introduce additional encoder deﬁne variational upper bound negative entropy −exz∼pg lvi. hence minimizing upper bound effectively increases generator entropy. implementation formulate diagonal gaussian bounded variance i.e. neural networks threshold prevent arbitrarily large variance. alternatively second method aims increasing generator entropy feature space optimizing auxiliary objective. concretely adapt pull-away term auxiliary cost size mini-batch samples. intuitively pull-away term tries orthogonalize features mini-batch minimizing squared cosine similarity. hence effect increasing diversity generated features thus generator entropy. second drawback feature matching gans high-density samples generated feature space desirable according analysis. similar argument section infeasible directly minimize density generated features. instead enforce generation samples density input space. speciﬁcally given threshold minimize following term part objective indicator function. using threshold ensure high-density samples penalized low-density samples unaffected. intuitively objective pushes generated samples move towards low-density regions deﬁned model probability distribution images simply adapt state-of-the-art density estimation model natural images namely pixelcnn++ model. pixelcnn++ model used estimate density model pretrained training ﬁxed semi-supervised training. objective closely related idea complement generator discussed section that let’s ﬁrst deﬁne target complement distribution input space follows normalizer constant deﬁned mapping feature space input space. deﬁnition divergence klp∗) −h+ex∼pg \u0001]+ex∼pg form immediately reveals aforementioned connection. firstly shares exactly terms generator objective secondly deﬁned hard constraint however feature matching term seen softly enforcing constraint bringing generated samples close true data moreover identity function zero gradient almost everywhere last term would contribute informative gradient generator. summary optimizing proposed objective understood minimizing divergence generator distribution desired complement distribution connects practical solution theoretical analysis. order complement generator work according condition assumption discriminator needs strong true-fake belief unlabeled data i.e. maxk however objective function discriminator enforce dominant class. obtain correct decision boundary probabilities possibly uniformly distributed. guarantee strong true-fake belief optimal conditions conditional entropy term discriminator objective becomes optimizing discriminator encouraged satisfy condition assumption note conditional entropy term used semi-supervised learning methods well motivate minimization conditional entropy based theoretical analysis gan-based semi-supervised learning. train networks alternatively update generator discriminator optimize based mini-batches. encoder used maximize encoder generator updated time. table comparison state-of-the-art methods three benchmark datasets. methods without data augmentation included. indicates using discriminator architecture indicates using larger discriminator architecture means self-ensembling. cifar- respectively training standard data split testing. -quantile probability deﬁne threshold instance noise input discriminator spatial dropout obtain faster convergence. except modiﬁcations neural network architecture fair comparison also report performance implementation aforementioned differences. compare results best model state-of-the-art methods benchmarks table proposed methods consistently improve performance upon feature matching. achieve state-of-the-art results datasets small discriminator architecture considered. results also state-of-the-art mnist svhn among single-model results even compared methods using self-ensembling large discriminator architectures. finally note method actually orthogonal combining presented approach yield performance improvement practice. report results ablation study table following analyze effects several components model subject intrinsic features different datasets. first generator entropy terms improve performance svhn cifar points terms error rate. moreover shown model signiﬁcantly reduces collapsing effects present samples generated also indicates maximizing generator entropy beneﬁcial. mnist probably simplicity collapsing phenomenon observed vanilla training setting. circumstances maximizing generator entropy seems unnecessary estimation bias introduced approximation techniques even hurt performance. table ablation study. feature matching. low-density enforcement term entropy maximization methods described section means conditional entropy term log-p maximum probability generated samples evaluated pixelcnn++ model. -quant shows -quantile true image probability. error means number misclassiﬁed examples mnist error rate others. second low-density term useful indeed generates samples high-density areas. mnist typical example case. trained generated hand written digits highly realistic high probabilities according density model hence applied mnist improves performance clear margin. contrast generated svhn images realistic quantitatively svhn samples assigned probabilities expected negligible effect performance svhn. moreover log-p column table shows reduce maximum probability generated mnist samples large margin yield noticeable difference svhn. justiﬁes analysis. based conclusion conjecture would help cifar sample quality even lower. thus train density model cifar limit computational resources. third adding conditional entropy term mixed effects different datasets. conditional entropy important factor achieving best performance svhn hurts performance mnist cifar. possible explanation relates classic exploitationexploration tradeoff minimizing favors exploitation minimizing classiﬁcation loss favors exploration. initial phase training discriminator relatively uncertain thus gradient term might dominate. result discriminator learns conﬁdent even incorrect predictions thus gets trapped local minima. lastly vary values hyper-parameter shown bottom table reducing clearly leads better performance justiﬁes analysis sections off-manifold samples favorable. compare generated samples approach fig. images fig. extracted previous work collapsing widely observed samples model generates diverse images consistent analysis. work present semi-supervised learning framework uses generated data boost task performance. framework characterize properties various generators theoretically prove complementary generator improves generalization. empirically proposed method improves performance image classiﬁcation several benchmark datasets. references martin arjovsky léon bottou. towards principled methods training generative adversarial networks. nips workshop adversarial training. review iclr volume mikhail belkin partha niyogi vikas sindhwani. manifold regularization geometric framework learning labeled unlabeled examples. journal machine learning research goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems pages diederik kingma shakir mohamed danilo jimenez rezende welling. semisupervised learning deep generative models. advances neural information processing systems pages takeru miyato shin-ichi maeda masanori koyama shin ishii. virtual adversarial training regularization method supervised semi-supervised learning. arxiv preprint arxiv. antti rasmus mathias berglund mikko honkala harri valpola tapani raiko. semisupervised learning ladder networks. advances neural information processing systems pages salimans andrej karpathy chen diederik kingma. pixelcnn++ improving pixelcnn discretized logistic mixture likelihood modiﬁcations. arxiv preprint arxiv. jonathan tompson ross goroshin arjun jain yann lecun christoph bregler. efﬁcient object localization using convolutional networks. proceedings ieee conference computer vision pattern recognition pages jason weston frédéric ratle hossein mobahi ronan collobert. deep learning semisupervised embedding. neural networks tricks trade pages springer xiaojin zoubin ghahramani john lafferty. semi-supervised learning using gaussian ﬁelds harmonic functions. proceedings international conference machine learning pages feature space bound assumption obtain theoretical results assume deﬁnition complement generator requires introduction bounded introduced ensure assumption realizable. ﬁrst show assumption hold must convex set. deﬁne maxk lemma convex set. therefore maxk conclude convex set. feature space unbounded deﬁned k=fk feature space dimension assumption since complement k=fk fk’s disjoint non-convex however lemma convex leading contradiction. therefore deﬁne complement generator using bound here justify proposed assumption classiﬁcation correctness assumes correctness classiﬁcation labeled data requires transformation high enough capacity limited amount labeled data points linearly separable feature space. setting semi-supervised learning quite limited assumption usually reasonable. true-fake correctness assumes generated data classiﬁer correctly distinguish true generated data. seen noticing assumption thus reduces part hold essentially require transformation high enough capacity distinguish true fake data standard assumption made literature. note small interval logit whose possible range expands entire real line thus region violation happens limited size making assumption reasonable practice. empirically easy model satisfy correctness assumption labeled data perfectly. verify assumptions keep track percentage test samples assumptions hold best models. speciﬁcally verify true-fake correctness calculate ratio epoch", "year": 2017}