{"title": "Genetic Algorithms for Evolving Deep Neural Networks", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "In recent years, deep learning methods applying unsupervised learning to train deep layers of neural networks have achieved remarkable results in numerous fields. In the past, many genetic algorithms based methods have been successfully applied to training neural networks. In this paper, we extend previous work and propose a GA-assisted method for deep learning. Our experimental results indicate that this GA-assisted approach improves the performance of a deep autoencoder, producing a sparser neural network.", "text": "autoencoder unsupervised neural network sets target values equal inputs i.e. number neurons input output layers equal optimization goal output neuron value input neuron hidden layer neurons used input output layers number neurons hidden layer usually fewer input output layers thus creating bottleneck intention forcing network learn higher level representation input. weights encoder layer weights decoder layer tied autoencoders typically trained using backpropagation. autoencoder’s training completed discard decoder layer values encoder layer treat outputs hidden layer inputs autoencoder added previous autoencoder. autoencoder trained similarly. using layer-wise unsupervised training deep stacks autoencoders assembled create deep neural networks consisting several hidden layers given input passed deep network resulting high level outputs. typical implementation outputs used supervised classiﬁcation required serving compact higher level representation data. genetic algorithms successfully employed training neural networks speciﬁcally used substitute backpropagation algorithm used conjunction backpropagation improve overall performance. training autoencoder tied weights store multiple sets weights layer. population chromosome weights autoencoder. chromosome root mean squared error calculated training samples ﬁtness chromosome deﬁned sei. calculating ﬁtness score chromosomes sorted ﬁttest least weights high rankrecent years deep learning methods applying unsupervised learning train deep layers neural networks achieved remarkable results numerous ﬁelds. past many genetic algorithms based methods successfully applied training neural networks. paper extend previous work propose ga-assisted method deep learning. experimental results indicate ga-assisted approach improves performance deep autoencoder producing sparser neural network. motivation creating deep neural networks consisting several hidden layers present many years supported growing body knowledge deep architecture brain advocated solid theoretical grounds recently diﬃcult train neural networks hidden layers. recently deep learning methods facilitate training neural networks several hidden layers subject increased interest owing discovery several novel methods. common approaches employ either autoencoders restricted boltzmann machines train layer time unsupervised manner. past genetic algorithms applied successfully training neural networks shallow depths paper demonstrate genetic algorithms applied improve training deep autoencoders. chromosomes updated using backpropagation lower ranking chromosomes removed population. removed chromosomes replaced oﬀsprings high ranking chromosomes. selection performed uniformly remaining chromosomes equal probability selection given parents oﬀspring created follows crossover performed randomly selecting weights parents mutation performed replacing small number weights zero. gradient descent methods backpropagation susceptible trapping local minima. method assists backpropagation respect reducing probability trapping local minima. additionally mutating weights zero encourages sparsity network sparse representations appealing information disentangling eﬃcient variable-size representation linear separability distributed sparsity note training autoencoder complete values best chromosome selected autoencoder. values ﬁxed shared amongst chromosomes autoencoder layer added previously trained layer. thus chromosome contains values layer currently trained. experiments used popular mnist handwritten digit recognition database mnist dataset sample contains pixels grayscale value sample also contains target classiﬁcation label used subsequent supervised classiﬁcation phase deep neural network uses stack layers. ﬁrst layer neurons followed four higher level layers consisting neurons. layer trained separately next layer added training complete ﬁrst train layer output neurons inputs layer similarly layers. implementation uses population chromosomes. generation worst chromosomes removed replaced oﬀsprings best chromosomes. used crossover mutation rates accordingly. compare performance ga-assisted method traditional backpropagation methods under similar conditions. first traditional backpropagation version times selected result least reconstruction error next gaassisted method once allowing total runtime previous method. comparing reconstruction errors approaches ga-assisted method consistently yielded smaller reconstruction error well sparser network. order compare classiﬁcation accuracy methods test samples trained networks recorded output values sample. recall test phase weights network already ﬁxed hence input sample values passed layers neurons without modifying weights. representation quality networks compared applying supervised classiﬁcation higher level values produced neurons output layer. used classiﬁcation radial basis function kernel. using traditional autoencoder achieved classiﬁcation error ga-assisted method’s classiﬁcation error paper presented simple ga-assisted approach according initial results improves performance deep autoencoder. implementation used autoencoder method applicable forms deep learning restricted boltzmann machines recent years several improvements upon traditional autoencoders proposed improve generalization. improvements include dropout randomly disables neurons training dropconnect randomly disables weights training denoising autoencoders randomly noise removing portion training data. improved performance ga-assisted autoencoder could arise similar principle since mutation randomly disables weights training. important compare ga-assisted approach mentioned alternative improvements future research. g.e. hinton salakhutdinov. reducing dimensionality data neural networks. science ekanadham sparse deep belief model visual area advances neural information processing systems pages press ranzato c.s. poultney chopra lecun. eﬃcient learning sparse representations energy-based model. advances neural information processing systems pages press vincent larochelle lajoie bengio manzagol. stacked denoising autoencoders learning useful representations deep network local denoising criterion. journal machine learning research", "year": 2017}