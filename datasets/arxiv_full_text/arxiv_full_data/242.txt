{"title": "Adapting Resilient Propagation for Deep Learning", "tag": ["cs.NE", "cs.CV", "cs.LG", "stat.ML"], "abstract": "The Resilient Propagation (Rprop) algorithm has been very popular for backpropagation training of multilayer feed-forward neural networks in various applications. The standard Rprop however encounters difficulties in the context of deep neural networks as typically happens with gradient-based learning algorithms. In this paper, we propose a modification of the Rprop that combines standard Rprop steps with a special drop out technique. We apply the method for training Deep Neural Networks as standalone components and in ensemble formulations. Results on the MNIST dataset show that the proposed modification alleviates standard Rprop's problems demonstrating improved learning speed and accuracy.", "text": "dropout regularisation method random selection nodes network updated during training iteration ﬁnal evaluation stage whole network used. selection performed sampling dropout mask bernoulli distribution probability node muted weight update step backpropagation dropout rate usually middle layers input layers output layer. convenience dropout mask represented weight binary matrix covering weights network used multiply weight-space network obtain called thinned network current training iteration weight zeroed based probability parent node muted. abstract—the resilient propagation algorithm popular backpropagation training multilayer feed-forward neural networks various applications. standard rprop however encounters difﬁculties context deep neural networks typically happens gradient-based learning algorithms. paper propose modiﬁcation rprop combines standard rprop steps special drop technique. apply method training deep neural networks standalone components ensemble formulations. results mnist dataset show proposed modiﬁcation alleviates standard rprop’s problems demonstrating improved learning speed accuracy. deep learning techniques generated many stateof-the-art models reached impressive results benchmark datasets like mnist models usually trained variations standard backpropagation method stochastic gradient descent ﬁeld shallow neural networks several developments training algorithms sped convergence paper aims bridge ﬁeld deep learning advanced training methods combining resilient propagation dropout deep neural networks ensembles. resilient propagation weight update rule initially introduced possible solution vanishing gradients problem depth complexity artiﬁcial neural network increase gradient propagated backwards standard backpropagation becomes increasingly smaller leading negligible weight updates slow training considerably. rprop solves problem using ﬁxed update value increased decreased multiplicatively iteration asymmetric factor respectively depending whether gradient respect changed sign iterations not. backtracking allows rprop still converge local minima acceleration provided multiplicative factor helps skip regions much quickly. avoid double punishment backtracking phase rprop artiﬁcially forces gradient product following iteration skipped. illustration rprop found algorithm section show experimental results using mnist dataset ﬁrst highlight rprop able converge much quickly initial epochs speed training stacked ensemble. order avoid double punishment change sign gradient rprop artiﬁcially sets gradient product associated weight next iteration condition checked ∂wij following iteration true updates weights learning rate performed. using zero-valued gradient product indication skip iteration acceptable normal gradient descent occurrence would learning terminated. dropout introduced additional number events produce zero values making rprop aware dropout mask able distinguish whether zero-gradient event occurs signal skip next weight update whether occurs different reason therefore updates allowed. version rprop update rule weight shown algorithm indicate current training example previous training example next training example value appears intended initial value. notation used original rprop error function current update value weight index ∆wij current weight update value index line line particular providing necessary protection additional zerogradients implementing correctly recipe prescribed dropout completely skipping every weight dmij jrprop section describe initial evaluation performance mnist dataset. experiments deep neural network middle layers neurons respectively dropout rate drmid middle layers dropout inputs. dropout rate shown optimal choice mnist dataset similar architecture used produce state-of-the-art results however authors used entire training validation graphical transformations said training. added transformations virtually inﬁnite training size whereby every epoch training generated much larger validation original images. test remains original image test set. explanation transformations provided also conﬁrms that therefore attribute improvements transformations applied found primary goal replicate additional transformations obtain state-of-theart results instead focused utilising untransformed stays consistently reaches minimum. also unmodiﬁed version reach ﬁnal error modiﬁed version starts overtraining much sooner reach better error sgd. table shows detail performance methods compares ﬁrst epochs. increase speed convergence make practical produce ensembles deep neural networks time train member considerably reduced without undertraining network. able train ensembles less hours total singlegpu single-cpu desktop system trained different ensemble types report ﬁnal results table methods used bagging stacking member dnns. member trained maximum epochs. bagging ensemble method several different training sets created random resampling original training used train classiﬁer. entire trained classiﬁers usually aggregated taking average majority vote reach single classiﬁcation decision. stacking ensemble method different classiﬁers aggregated using additional learning algorithm uses inputs ﬁrst-space classiﬁers learn information reach better classiﬁcation result. additional learning algorithm called second-space classiﬁer. dataset using images training validation testing. subsequently performed search using validation indicator optimal hyperparameters modiﬁed version rprop. found best results reached ∆max ∆min trained models maximum allowed epochs measured error validation every epoch could used select model applied test set. also measured time took reach best validation error report approximate magnitude comparison orders magnitude. results presented average repeated runs limited maximum training epochs. results table modiﬁed version rprop able start-up much quicker reaches error value close minimum much quickly. reaches higher error value much longer time. although overall error improvement signiﬁcant speed gain using rprop appealing allows save large number iterations could used improving model different ways. rprop obtains best validation error epochs whilst reached minimum illustration ﬁrst epochs seen figure riedmiller braun direct adaptive method faster backpropagation learning rprop algorithm proceeding ieee international conference neural networks. ieee srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overﬁtting journal machine learning research vol. modiﬁed rprop. used original train validation test sets this collected average repeated runs. results still comparable presented consistent observations importance dataset transformations however note able improve error less time took train single network sgd. wilcoxon signed ranks test shows increase performance obtained using ensembles size compared ensemble size signiﬁcant conﬁdence level. highlighted many training methods used shallow learning adapted deep learning. looked rprop appearance zero-gradients training side effect dropout poses challenge learning proposed solution allows rprop train dnns better error still much faster standard backpropagation. showed increase training speed used train effectively ensemble dnns commodity desktop system reap added beneﬁts ensemble methods less time would take train deep neural network sgd. remains assessed work whether improved methodology would lead state-of-the-art error applying pre-training dataset enhancements used methods improvements rprop ported numerous variants. ciresan meier schmidhuber multi-column deep neural networks image classiﬁcation proceedings ieee conference computer vision pattern recognition ieee press", "year": 2015}