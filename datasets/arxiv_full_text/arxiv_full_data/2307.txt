{"title": "Vector Quantization as Sparse Least Square Optimization", "tag": ["cs.LG", "cs.AI", "cs.NA", "stat.ML"], "abstract": "Vector quantization aims to form new vectors/matrices with shared values close to the original. It could compress data with acceptable information loss, and could be of great usefulness in areas like Image Processing, Pattern Recognition and Machine Learning. In recent years, the importance of quantization has been soaring as it has been discovered huge potentials in deploying practical neural networks, which is among one of the most popular research topics. Conventional vector quantization methods usually suffer from their own flaws: hand-coding domain rules quantization could produce poor results when encountering complex data, and clustering-based algorithms have the problem of inexact solution and high time consumption. In this paper, we explored vector quantization problem from a new perspective of sparse least square optimization and designed multiple algorithms with their program implementations. Specifically, deriving from a sparse form of coefficient matrix, three types of sparse least squares, with $l_0$, $l_1$, and generalized $l_1 + l_2$ penalizations, are designed and implemented respectively. In addition, to produce quantization results with given amount of quantized values(instead of penalization coefficient $\\lambda$), this paper proposed a cluster-based least square quantization method, which could also be regarded as an improvement of information preservation of conventional clustering algorithm. The algorithms were tested on various data and tasks and their computational properties were analyzed. The paper offers a new perspective to probe the area of vector quantization, while the algorithms proposed could provide more appropriate options for quantization tasks under different circumstances.", "text": "vector quantization aims form vectors/matrices shared values close original. could compress data acceptable information loss could great usefulness areas like image processing pattern recognition machine learning. recent years importance quantization soaring discovered huge potentials deploying practical neural networks among popular research topics. conventional vector quantization methods usually suﬀer ﬂaws hand-coding domain rules quantization could produce poor results encountering complex data clustering-based algorithms problem inexact solution high time consumption. paper explored vector quantization problem perspective sparse least square optimization designed multiple algorithms program implementations. speciﬁcally deriving sparse form coeﬃcient matrix three types sparse least squares generalized penalizations designed implemented respectively. addition produce quantization results given amount quantized values paper proposed cluster-based least square quantization method could also regarded improvement information preservation conventional clustering algorithm. algorithms tested various data tasks computational properties analyzed. paper oﬀers perspective probe area vector quantization algorithms proposed could provide appropriate options quantization tasks diﬀerent circumstances. vector quantization takes vectors/matrices produce ones shared value acceptable diﬀerence originals. reducing number values vector/matrix quantization methods could compress information therefore reduce storage cost. quantization found great usefulness areas like image processing speech recognition machine learning techniques. recently growing research interests deploying neural networks storage-scarce edge devices vector quantization techniques grasped considerable attention ability reducing network size research also originated explorations quantization embedded neural networks thus aims explore vector quantization algorithms. conventional vector quantization methods usually utilize hand-coding domain rules and/or clusteringbased methods quantize values. common approaches include uniform quantization logarithm quantization k-means clustering quantization. approaches straightforward convenient frequently suﬀer several problems inadequately-reliable performance. domain quantization usually produce low-quality outcomes lead signiﬁcant information loss; k-means quantization could usually produce highquality results holistic results could include empty clusters irrational values number quantized values large; inexact results. results domain quantization depend choice domain k-means clustering heuristic method cannot guarantee optimal solution results subject random initialization; high time-consumption. k-means clustering method could usually lead solution insigniﬁcant information loss time consumption method signiﬁcant especially amount quantized values large. paper intend research quantization algorithm another perspective sparse least square optimization. consider sparse-inducing properties norm-regularizations designed algorithms minimize diﬀerence original sparsely-constructed vectors. optimize performance based computational properties alternative version norm-regularization also explored implemented. furthermore design least square quantization method could produce results certain amounts quantized values combined k-means clustering least square optimization produced another novel algorithm closed-form solution. interestingly approach could also interpreted improvement conventional k-means clustering quantization method. notice algorithms similar sparse compression signal processing diﬀerence problem constructed vector shared values sparse signal processing demands sparse vector able produce vector close original signal. rest paper arranged follows section introducing related work ﬁeld including research outcomes quantization algorithms sparse coding processing; section introducing designed algorithms mathematically analyze optimization schemes computational properties; experimental results algorithms shown compared analyzed section properties algorithms examined; ﬁnally conclusion drawn section research related paper discussed. goal vector quantization relatively straightforward achieve usual situations thus many complicated methods address task. provides brief survey basic methods vector quantization include domain-based uniform logarithm quantizations clustering-based techniques like k-means quantization. idea quantizing vectors clustering methods provides open skeleton could plug novel clustering techniques produce quantization algorithms. oﬀers alternative technique mixture gaussian method perform quantization speciﬁcally neural networks recent paper re-examined idea formally designed used neural network compression provided mathematical justiﬁcation techniques perform vector quantization include utilized divergence instead distance metric measurement derived algorithms based designed neural network perform vector quantization; considered pairwise dis-similarity metric quantization. best knowledge hitherto publications discussing quantization algorithms sparse least square optimization thus paper pioneering work area. plenty academic publication discussing applications vector quantization recently academic projects lying area growingly connected neural networks ability quantization compressing model size exploited implementing edge-device neural networks. proposed general pipeline reduce model storage important part quantization. similar mentioned above speciﬁcally designed quantization method neural networks. directly utilized existed vector quantization techniques network compressing illustrated technique could ideal modifying neural network precisions. mentioned introductory section origin algorithms also task solve neural network weight-sharing problem nn-based parameters test datasets experimental section. algorithm proposed work signiﬁcant similarity compressive sensing terms regularization idea optimization target functions typical approaches induce sparsity compressive sensing algorithms introduce norm norm and/or norm target optimization functions. similarly algorithms also utilize techniques perform sparsity. meanwhile since norm everywhere diﬀerentiable norm even convex also exist plenties algorithms devoted eﬃciently solve optimization problems paper distinct optimization schemes diﬀerent types target functions including newly-proposed algorithm program optimization vector quantization task could described follows suppose vector distinct values. intend vector distinct values alternatively strict constraint given value require. denoting diﬀerence original vector constructed norm original target function could formed means distinct values vector. notice consider vector form. data coded matrix like neural network parameters images could simply ’ﬂatten’ matrix vector perform quantization. original algorithm norm regularization begin with ﬁrstly change directly operate distinct values recover full vector indexing later. operation needing construct vector length distinct values. could assume exist ’base’ vector shape number given vector could generated randomly picking original vector transformation matrix form constructed vector matrices control constructed vector. intuitively and/or norm regularization target function possible produce vector shared values. consider ﬁrst place continuous convex. optimization target would become practice computational convenience could ﬁrst sort original vector form sorted scaling hyper-parameter original summation value compuˆ tational stability. ﬁnal optimization target regularization follows equation similar optimization target compressive sensing. nevertheless signiﬁcant diﬀerences ﬁrstly root target function derivations diﬀerent compressive sensing; secondly produced vector quantized vector instead simply sparse vector close original compressive sensing. optimization formula hard target function typical lasso problem solved coordinate descent lasso solvers sk-learn program problem algorithm equation optimization tends concentrate values around center make small diﬀerences values thus performance poorly encountering dispersedly-distributed data. avoid problem paper consider negative norm penalization encourage large dispersed non-zero values. revised form could denoted following formula equation like generalized elastic since conventional e-net permits positive co-eﬃcients parameters. rare integrated lasso optimization packages permits parameter setting like equation thus algorithm implemented tensorﬂow proximal adagrad optimizer program. another variation algorithm based could replace norm norm. algorithm instead directly penalization term explicitly limitations number distinct values number manually indicate upper bound amount distinct values. optimization norm np-hard thus could solve heuristic-based algorithms. paper utilize recent-proposed llearn package support number however notice optimization method universal means could reach arbitrary required number values settings. drawback revised could explicitly indicate number demanded clusters/distinct values. algorithm equation could upper bound amount distinct values guarantees many values ﬁnally produced. tackle indeﬁnite-amount problem discuss general target could produce deﬁnite amount values least square form design basic method based combination k-means clustering least square optimization. suppose want construct vector distinct values directly parameter vector vector entries shape). need transformation matrix transform p-value vector vector maintaining values constructed. possible scheme could transformation matrix one-hot encoded row. scheme optimization target following means group value belongs optimization would diﬃcult perform optimization variables changing simultaneously. here propose simple method deal problem ﬁrstly clustering methods obtain could matrix target function could transferred following expression interesting point perspective clustering methods equation could viewed improvement k-means clustering quantization. conventional clustering-based quantization algorithm simply center points quantization results. here alternatively point produce least distance original. notice exist multiple schemes solve optimization problem proposed equation method proposed basic solution. exploration solving task could future research concentrations. verify rationality eﬀectiveness proposed methods make comparison existed methods test algorithms three kind data weight matrix mnist image artiﬁcial data sampled diﬀerent distributions. experiments focus information loss time-consumption characteristic results analyze reason behind outcomes conditions method preferable. information loss denoted diﬀerence original vector quantized vector. notice addition loss value comparisons also relative loss comparison compare trend loss. relative loss comparison computed loss vector collects diﬀerences w.r.t. clusters/λ values. loss denotes ﬁrst loss loss vector lossn means last component vector. notice high loss might seems. example data distribution large amounts quantized values away original produce high loss performance might depended quantized values away ground truth clustering-based least square clustering methods analysis computational properties optimization comparison original revised regularization methods cluster number-diﬀerence analysis cluster number-time consumption analysis; results produced l-optimization method experimental results shows general clustering-based least square method enjoys best performance terms information loss additional time consumption comparing k-means method signiﬁcant; k-means method remains better terms information preservation l-based method tends concentrate weights central. however quantization large number clusters information loss acceptable amount values original vector/matrix relatively small magnitude l-based method could considerably save computation time; revised method optimization could outperform solely revised methods perform better gaussian-distributed data less competitive uniformly-distributed ones; ﬁnally l-based quantization method could provide good performance within acceptable running time could universally produce quantization results optimization could fail circumstances stated previous sections research originated task neural network weights quantization problem. test eﬀectiveness methods neural network weights pick weight matrix layer fully-connected network performed quantization methods described above. results loss time-consumption comparisons method k-means clustering clustering-based least square method shown ﬁgure ﬁgure could case method doesn’t show advantages clustering method number parameters optimization time increase number data raise. however could loss sparse least square method still acceptable. time consumption ﬁgure could clustering-based least square method doesn’t spend signiﬁcantly much time k-means method. considering variation time k-means algorithm itself sometime l-based least square method even cost less time k-means algorithm does. take method consideration normalize loss equation could result ﬁgure left side ﬁgure shows trend losses diﬀerent algorithms right hand ﬁgure shows comparison number clusters notice produce similar number clusters beginning rescaling factor compute λl−l equation λl−l. future insight characteristics result could observed ﬁgure ﬁgure could possible reason method enduring larger loss could tends concentrate values central thus lose valuable information. contrary results demonstrate strong trend focusing values beginning places. preferable characteristic phenomenon could attributed properties optimizer since also happen show rationality using solely performed quantization diﬀerent setting values. ﬁgure ﬁgure could values larger value could induce less cluster numbers simultaneously less loss. inspect ﬁgure illustration values could larger ﬁnally cluster-loss curve l-based optimization could shown ﬁgure ﬁgure shows nice property terms loss noticeable method universal could support number clusters case. vector quantization could used image processing quantize values image reduce storage cost. choose mnist digits example show performance image quantization proposed methods. compare performance time used ordinary k-means clustering method. result could shown ﬁgure ﬁgure could k-means clustering-based least square optimization could provide best performances general signiﬁcant diﬀerences execution time. could provide less norm diﬀerence loss using solely quality image might subject individual’s opinion. terms running time optimization approach could provide signiﬁcant time-cost advantages methods. another remark mnist quantization k-means methods sometime provide out-of-range values number clusters large. however least-square optimization methods problem happen least circumstance. optimization result method shown separately ﬁgure information among images could method could exactly reach every number amounts require. however quality images could generally high although strange behavior loss guaranteed smaller number clusters increases. test performance algorithm data diﬀerent distributions generated hundred data mixture gaussian uniform single gaussian respectively. distribution data used could shown ﬁgure according experimental results ﬁgure could naturally think based algorithms would perform better gaussian mixture gaussian data. indeed experimental result ﬁgure could verify assumption indicate based algorithms would better used distribution data gaussian-like. meanwhile setting artiﬁcial data sparse least square quantization method appears signiﬁcant time-saving property. mixture gaussiangaussiandistributed data absolute value loss still acceptable number distinct values large thus could used corresponding circumstances. behaviors diﬀerent values artiﬁcial data also examined experiments. result could shown ﬁgure again could obtain lower diﬀerence loss smaller amount clusters/distinct values would favorable characteristic quantization. paper discussed vector quantization problem proposed several least square optimizationbased algorithms better accomplish task. characteristics computational properties proposed algorithms examined advantages drawbacks analyzed. algorithms implemented diﬀerent optimization packages tested neural network weights mnist image artiﬁcially-generated data respectively results demonstrated analyzed. paper made following major contributions firstly proposed several novel quantization methods diﬀerent levels precision time-consumption could provide options quantization tasks diﬀerent situations; secondly paper innovated pioneering work using least square optimization solve could bring huge research potentials area; thirdly paper demonstrated experimental results implementations algorithms proposed paper tested diﬀerent kinds data various requirements goals. future authors intend continue explore quantization algorithms idea demonstrated paper. intend research ﬁnding better optimization methods revisions target functions extend quantization method high-dimensional situations. authors would like thank feng chen shixiong wang northwestern polytechnical university china. chen wang provided authors many useful comments algorithms programs research.", "year": 2018}