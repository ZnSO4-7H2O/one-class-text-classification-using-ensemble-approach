{"title": "Bandit Structured Prediction for Neural Sequence-to-Sequence Learning", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "Bandit structured prediction describes a stochastic optimization framework where learning is performed from partial feedback. This feedback is received in the form of a task loss evaluation to a predicted output structure, without having access to gold standard structures. We advance this framework by lifting linear bandit learning to neural sequence-to-sequence learning problems using attention-based recurrent neural networks. Furthermore, we show how to incorporate control variates into our learning algorithms for variance reduction and improved generalization. We present an evaluation on a neural machine translation task that shows improvements of up to 5.89 BLEU points for domain adaptation from simulated bandit feedback.", "text": "bandit structured prediction describes stochastic optimization framework learning performed partial feedback. feedback received form task loss evaluation predicted output structure without access gold standard structures. advance framework lifting linear bandit learning neural sequence-to-sequence learning problems using attention-based recurrent neural networks. furthermore show incorporate control variates learning algorithms variance reduction improved generalization. present evaluation neural machine translation task shows improvements bleu points domain adaptation simulated bandit feedback. many tasks involve learning predict structured output sequence tree graph. sequence-to-sequence learning neural networks recently become popular approach allows tackling structured prediction mapping problem variable-length sequences e.g. foreign language sentences target-language sentences natural language input sentences linearized versions syntactic semantic parses known bottleneck structured prediction requirement large amounts gold-standard structures supervised learning model parameters especially data-hungry neural network models. sokolov presented framework stochastic structured prediction bandit feedback alleviates need labeled output structures learning following online learning protocol iteration learner receives input predicts output structure receives partial feedback form task loss evaluation predicted structure. banditize several objective functions linear structured predictions evaluate resulting algorithms simulated bandit feedback various tasks. show lift linear structured prediction bandit feedback non-linear models sequence-to-sequence learning attentionbased recurrent neural networks framework applicable sequenceto-sequence learning various types weak feedback. example extracting learning signals execution structured outputs databases established communities semantic parsing grounded language learning since decade work build basis neural semantic parsing weak feedback. paper focus application machine translation neural sequence-to-sequence learning. standard procedure training neural machine translation models compare output human-generated translations infer model updates comparison. however creation reference translations post-edits requires professional expertise users. framework allows models learn feedback weaker human references post-edits. could imagine scenario personalized machine translation translations adapted user’s speciﬁc purpose domain. feedback required methods provided laymen users even starting work sokolov lift objectives neural sequence-to-sequence learning. evaluate resulting algorithms task french-toenglish translation domain adaptation seed model trained europarl data adapted newscommentary talks domain simulated weak feedback. learning feedback bleu points improvements newscommentary bleu points improvement ted. furthermore show control variates integrated algorithms yielding faster learning improved generalization experiments. models commonly trained under word-level maximum likelihood objective. even though objective successfully applied many sequence-to-sequence learning tasks resulting models suffer exposure bias since learn generate output words based history given reference words predictions. ranzato apply techniques reinforcement learning imitation learning learn feedback model’s predictions. furthermore address mismatch word-level loss sequence-level evaluation metric using mixture reinforce algorithm standard maximum likelihood training directly optimize sequence-level loss. similarly shen lift minimum risk training linear models machine translation nmt. works closely related technique score function gradient estimators stochastic learning. however learning environment shen different approximate true gradient risk objective full information setting sampling subset translations computing expectation rewards. bandit setting feedback single sample sentence available making learning problem much harder. approach ranzato approximates expectation single samples still requires reference translations unavailable bandit setting. knowledge work training weak feedback work propose dual-learning mechanism translation models jointly trained monolingual data. feedback case reward signal language models reconstruction error. attractive feedback automatically generated monolingual data require human references. however interested using estimates human feedback translation quality directly adapt model users’ needs. approach follows closely work sokolov introduce bandit learning objectives structured prediction apply various tasks including machine translation linear models. approach seen instantiation reinforcement learning one-state markov decision processes linear policy models. paper transfer algorithms nonlinear sequence-to-sequence learning. sokolov showed applications linear bandit learning tasks multiclass-classiﬁcation chunking learning done scratch. focus lifting linear machine translation experiments complex requires warm start training. done training seed model domain adapting domain based bandit feedback only. task build work freitag al-onaizan investigate strategies best worlds models adapt well domain without deteriorating domain. contrast previous approaches domain adaptation require in-domain parallel data consult direct feedback translations generated domain. neural models machine translation based sequence-to-sequence learning architecture consisting encoder decoder encoder recurrent neural network reads source sentence decoder adaptive learning rates momentum techniques required good local maxima speed convergence another trick trade ensemble several models different random initializations improve single models test time face search problem sequence target words highest probability. beam search reduces search error comparison greedy search also exponentially increases decoding time. algorithm adaptation bandit structured prediction algorithm sokolov neural models rounds model parameters receives input samples output structure receives user feedback. based feedback stochastic gradient computed model parameters updated. post-optimization step solution selected iterates. done onlineto-batch conversion choosing model optimal performance held-out data. core algorithm sampling model distribution peaked model exploits i.e. presents probable outputs user. distribution close uniform model explores i.e. presents random outputs user. balance exploitation exploration crucial learning process beginning model rather uninformed needs explore order outputs high reward ideally converges towards peaked distribution exactly user’s needs. pre-training model i.e. setting wisely ensures reasonable exploitationexploration trade-off. input encoder sequence vectors representing sequence source words length approach sutskever encoded single vector hidden state time several choices possible non-linear functions using gated recurrent unit attention mechanism deﬁnes context vector weighted encoder hidden states decoder predicts next target word time given context vector previous target words yt−} probability distribution target vocabulary distribution result softmax transformation decoder outputs oty} since encoder-decoder architecture fully differentiable trained gradient descent methods. given parallel training source sentences reference translations y)}s deﬁne wordlevel maximum likelihood estimation objective aims parameters objective provided stochastic gradients unbiased estimators true gradient objective i.e. require following present objectives sokolov transferred neural models explain enhanced control variates. case full-information learning reference outputs available could evaluate possible outputs reference obtain exact estimation loss function. however feasible setting since receive partial feedback single output structure input. instead stochastic approximation optimize loss. stochastic gradient objective computed follows denotes score function. give algorithm sample structures encoder-decoder model algorithm corresponds algorithm presented shen difference samples single structures assume reference structure additionally returns sample probabilities. similar objective also used reinforce algorithm adapted ranzato pairwise preference ranking previous objective requires numerical feedback estimate translation quality. alternatively learn pairwise preference judgments formalized preference ranking objectives. yj|yi denote output pairs input denote task loss function speciﬁes dispreference experimental simulations types pairwise feedback. firstly continuous pairwise feedback computed effect negation within softmax distributions rank next candidate target words opposite order. globally normalized models linear case lstm-crfs non-linear case would allow sampling full structures ranking full structures reversed. case locally normalized rnns retrieve locally reversed-rank samples. since want model learn rank would sample word-by˜ word however sampling words leads translations neither ﬂuent source-related propose randomly choose position next word sampled sample remaining words found method produces suitable negative samples slightly perturbed still relatively ﬂuent source-related. detailed algorithm given algorithm sought another random variable highly correlated. called control variate. furthermore denote expectation. following quantity x−ˆc unbiased estimator case random variable interest noisy gradient equation variance reduction effect control variates seen computing variance quantity note word-based sampling scheme described above sequence also includes words sampled pairwise preference ranking objective expresses expectation losses pairs training procedure resembles well-known approaches noise contrastive estimation negative sampling commonly used neural language modeling approaches negative samples drawn non-parametric noise distribution whereas draw perturbed model distribution. control variates stochastic gradients deﬁned equations used stochastic gradient descent optimization full gradient approximated using minibatch single example update. stochastic choice case inputs outputs introduces noise leads slower convergence degrades performance. following explain antithetic additive control variate techniques monte carlo simulation used remedy problems. machine translation domain adaptation. compare presented neural bandit learning objectives perform comparison linear models discuss handling unknown words eventually investigate impact techniques variance reduction. setup data. perform domain adaptation europarl news commentary talks translations french english. table provides details datasets. data pre-processing follow procedure sokolov using cdec tools ﬁltering lowercasing tokenization. challenge bandit learner adapt domain weak feedback only. models. choose standard encoderdecoder architecture single-layer rnns hidden units word embedding size tanh activations. encoder consists bidirectional hidden states backward forward concatenated. decoder uses attention mechanism proposed bahdanau source target vocabularies contain frequent words respective parts training corpus. limit maximum sentence length dropout probability applied network several places embedded inputs output layer initial state decoder rnn. gradient clipped norms exceeds prevent exploding gradients stabilize learning models implemented trained sequence learning framework neural monkey expectation zero simplifying implementation. however optimal scalar estimated every entry gradient separately score function control variate. explore types control variates stochastic gradient experiments. effect control variates reduce magnitude gradient stochastic gradient control variate covary. l-lipschitz continuous functions reduced gradient norm directly leads bound appears algorithmic stability bounds hardt effect improved generalization control variates empirically validated experiments. choosing variates negative reduce variance gradient estimate. certain assumptions stochastic gradient pairwise preference objective interpreted antithetic estimator score function antithetic variates case would antithetic dependence achieved construction loosely related approach). similar control variates antithetic variates effect shrinking gradient norm variates antithetically correlated leading possible improvements algorithmic stability baselines. out-of-domain baseline trained training standard mle. domains train fullinformation in-domain baselines ﬁrst indomain baseline trained relatively small in-domain training data. second in-domain baseline starts out-of-domain model trained in-domain data. baselines trained adam performance stops increasing respective held-out validation sets. performance out-ofdomain model in-domain models deﬁnes range possible improvements bandit learning. models evaluated neural monkey’s mteval. statistical signiﬁcance tests used approximate randomization testing bandit learning. bandit learning starts parameters out-of-domain baseline. bandit models expected improve out-of-domain baseline receiving feedback domain reach indomain baseline since feedback weak. models trained adam in-domain data epochs. adam’s step-size parameter tuned validation found perform best non-pairwise pairwise objectives pairwise objectives ted. best model parameters selected early stopping in-domain validation evaluated held-out in-domain test set. spirit freitag al-onaizan additionally evaluated out-of-domain test investigate much knowledge domain models lose adapting domain. bandit learning experiments repeated times different random seeds mean bleu scores standard deviation reported. feedback simulation. weak feedback simulated target side parallel corpus references never revealed learner. sokolov used smoothed version per-sentence bleu simulating weak feedback generated translations comparison reference translations. here ggleu instead recently introduced learning sentence-level reward signals correlating well corpus bleu. metric closely related bleu brevity penalty considers recall matching n-grams. deﬁned minimum recall precision total n-grams certain hence experiments −ggleu sample translation reference translation. unknown words. drawback models limitation ﬁxed sourcetarget vocabulary. domain adaptation setting limitation critical impact translation quality. larger distance domain words domain unknown models trained domain consider strategies problem experiments unk-replace jean luong replace generated tokens aligned source words lexical translations post-processing step. freitag al-onaizan hashimoto demonstrated technique beneﬁcial domain adaptation. sennrich introduce byte pair encoding word segmentation build translation models sub-word units. rare words decomposed subword units frequent words remain single vocabulary items. unk-replace fast align generate lexical translations training data. token generated look attention weights source token receives attention step. possible replace token lexical translation. included lexical translations replaced source token. main beneﬁt technique deals well unknown named entities passed source target. however since non-differentiable post-processing step model cannot directly trained behavior. therefore also train sub-word level bpe. apply merge operations obtain vocabulary sub-words. procedure training models exactly word-based models. advantage method model principle able generate word composing sub-word units. however training sequences become longer candidate translations sampled sub-word level introduces risk sampling nonsense words. control variates. implement average baseline control variate deﬁned equation results keeping running average previous losses. intuitively absolute ggleu feedback turned relative feedback reﬂects current state model. sign update switched ggleu current sample worse average ggleu model makes step away case absolute feedback would still make small step towards addition implement score function control variate running estimate results following discuss results experimental evaluation models described above. out-of-domain baseline results given table in-domain baselines results bandit learning reported table bandit learning give mean improvements respective out-of-domain baselines diff.-columns. in-domain ep→nc baselines outperform linear baseline bleu points. continuing training pre-trained out-of-domain model small amount domain data hence effective whilst performance models solely trained small indomain data highly dependent size training data set. in-domain dataset almost four times training in-domain baselines perform better. effect previously observed luong manning freitag al-onaizan bandit learning. bandit models optimize objective yield generally much higher improvement out-of-domain models corresponding linear models listed table improvements bleu points domain between bleu points domain. contrast linear models sparse features hypergraph re-decoding achieved maximum improvement bleu points table bandit results test sets. unk* models involve replacement testing unk** include replacement already training. either binary continuous feedback used. control variates average reward baseline score function results averaged independent runs standard deviation given subscripts. improvements respective out-of-domain models given diff.-columns. beneﬁcial side-effect learning weak feedback knowledge out-domain training simply overwritten. happens full-information in-domain tuning bleu points lost evaluation out-domain data. contrary bandit learning models still achieve high results original domain. useful conservative domain adaptation performance models domain still relevant. unknown words. handling unknown words unk-replace bpes consistent improvements plain word-based models baselines bandit learning models. observe models replacement essentially beneﬁt passing source tokens marginally lexical translations. bandit learning models take particular advantage replacement included already training. sub-word models achieve overall highest improvement baselines although sometimes generating nonsense words. paper showed lift structured prediction bandit feedback linear models non-linear sequence-to-sequence learning using recurrent neural networks attention. introduced algorithms train models under numerical feedback single output structures preference rankings pairs structures. experimental evaluation task neural machine translation domain adaptation found relative improvements bleu points out-of-domain seed models outperforming also linear bandit models. furthermore argued pairwise ranking bandit feedback interpreted antithetic variates showed include average reward score function baselines control variates improved training speed generalization. future work would like apply presented non-linear bandit learners structured prediction tasks. control variates. applying score function control variate optimization largely change learning speed bleu results. however average reward control variate leads kyunghyun bart merri¨enboer alar g¨ulc¸ehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder–decoder emnlp. statistical machine translation. doha qatar. junyoung chung caglar gulcehre kyunghyun yoshua bengio. empirical evaluation gated recurrent neural networks sequence modeling. eprint arxiv. kazuma hashimoto akiko eriguchi yoshimasa tsuruoka. domain adaptation attentionbased unknown word replacement chinese-tocoling japanese neural machine translation. workshop asian translation. osaka japan. s´ebastien jean orhan firat kyunghyun roland memisevic yoshua bengio. montreal neural machine translation systems wmt’. wmt. lisbon portugal. jindˇrich libovick`y jindˇrich helcl marek tlust`y pavel pecina ondˇrej bojar. cuni system automatic post-editing multimodal translation tasks. wmt. berlin germany. richard sutton david mcallester satinder singh yishay mansour. policy gradient methods reinforcement learning function approximation. nips. vancouver canada. yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey google’s neural machine translation system bridging human machine translation. eprint arxiv. luke zettlemoyer michael collins. learning sentences logical form structured classiﬁcation probabilistic categorial grammars. uai. edinburgh scotland. st´ephane ross geoffrey gordon drew bagnell. reduction imitation learning structured prediction no-regret online learning. aistats. lauderdale artem sokolov julia kreutzer christopher stefan riezler. learning structured predictors bandit feedback interactive nlp. acl. berlin germany. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. jmlr", "year": 2017}