{"title": "Sparse Coding and Dictionary Learning for Symmetric Positive Definite  Matrices: A Kernel Approach", "tag": ["cs.LG", "cs.CV", "stat.ML", "I.2.6; I.5.1; I.5.4; I.2.10"], "abstract": "Recent advances suggest that a wide range of computer vision problems can be addressed more appropriately by considering non-Euclidean geometry. This paper tackles the problem of sparse coding and dictionary learning in the space of symmetric positive definite matrices, which form a Riemannian manifold. With the aid of the recently introduced Stein kernel (related to a symmetric version of Bregman matrix divergence), we propose to perform sparse coding by embedding Riemannian manifolds into reproducing kernel Hilbert spaces. This leads to a convex and kernel version of the Lasso problem, which can be solved efficiently. We furthermore propose an algorithm for learning a Riemannian dictionary (used for sparse coding), closely tied to the Stein kernel. Experiments on several classification tasks (face recognition, texture classification, person re-identification) show that the proposed sparse coding approach achieves notable improvements in discrimination accuracy, in comparison to state-of-the-art methods such as tensor sparse coding, Riemannian locality preserving projection, and symmetry-driven accumulation of local features.", "text": "abstract. recent advances suggest wide range computer vision problems addressed appropriately considering non-euclidean geometry. paper tackles problem sparse coding dictionary learning space symmetric positive deﬁnite matrices form riemannian manifold. recently introduced stein kernel propose perform sparse coding embedding riemannian manifolds reproducing kernel hilbert spaces. leads convex kernel version lasso problem solved efﬁciently. furthermore propose algorithm learning riemannian dictionary closely tied stein kernel. experiments several classiﬁcation tasks show proposed sparse coding approach achieves notable improvements discrimination accuracy comparison state-of-the-art methods tensor sparse coding riemannian locality preserving projection symmetry-driven accumulation local features. sparse representation linear decomposition signal using atoms dictionary notable results various image processing computer vision tasks signiﬁcant steps taken towards expanding theory representations non-euclidean spaces received comparatively little attention. paper tackles problem sparse coding within space symmetric positive deﬁnite matrices. matrices fundamental building blocks computer vision machine learning. notable example covariance descriptor offer compact describing regions/cuboids images/videos fusion multiple features. covariance descriptors exploited several applications diffusion tensor imaging action recognition pedestrian detection face recognition texture classiﬁcation tracking negative curvature suitable analysing matrices speciﬁcally pennec introduced afﬁne invariant riemannian metric showed induced riemannian structure invariant inversion similarity transforms. airm perhaps widely used similarity measure matrices. nevertheless efﬁciently accurately handling riemannian structure non-trivial basic computations riemannian manifolds involve non-linear operators. hinders development optimisation algorithms also incurs signiﬁcant numerical burden. address drawbacks paper propose perform sparse coding matrices embedding riemannian manifolds reproducing kernel hilbert spaces contrast directly embedding euclidean spaces related work. used cone matrices frobenius norm measure similarity matrices. results regularised non-negative least-squares approach consider riemannian geometry induced airm. yuan separately proposed solve sparse representation log-euclidean approach riemannian problem converted euclidean embedding manifolds tangent spaces. log-euclidean approaches beneﬁt simplicity true geometry manifold taken account. speciﬁcally tangent space distances pole space true geodesic distances. such pairwise distances arbitrary points tangent space represent structure manifold. sivalingam used burg divergence metric reformulated riemannian problem determinant maximisation problem. advantage avoiding explicit manifold embedding well resulting convex maxdet problem solved interior point methods. however downsides solution computationally expensive relations burg divergence geometry riemannian manifolds well established. contributions. recently introduced stein kernel related airm tight bound propose riemannian sparse solver embedding riemannian manifolds rkhs. show embedding leads convex kernelised version lasso problem solved efﬁciently. furthermore propose sparsity-maximising algorithm dictionary learning within space matrices closely tied stein kernel. lastly show proposed sparse coding approach obtains superior performance several visual classiﬁcation tasks comparison several state-of-the-art methods tensor sparse coding log-euclidean sparse representation gabor feature based sparse representation riemannian locality preserving projection continue paper follows. section begins overview bregman divergence stein kernel. section describes proposed kernel solution riemannian sparse coding followed section covers problem dictionary learning riemannian manifolds. section compare performance proposed method previous approaches several visual classiﬁcation tasks. main ﬁndings possible future directions summarised section section ﬁrst overview properties bregman matrix divergences including special case known symmetric stein divergence. leads stein kernel used embed riemannian manifolds rkhs. matrices. bregman divergences non-negative deﬁnite general asymmetric. among several ways symmetrise them jensen-shannon symmetrisation often used ﬁrst property establishes bound geodesic distance stein divergence providing motivation addressing riemannian problems divergence. second property reinforces motivation explaining behaviour stein divergences along geodesic curves similar true riemannian geometry. shall refer kernel stein kernel following theorem states condition stein kernel positive deﬁnite. theorem symd riemannian points. matrix deﬁned positive deﬁnite interested readers follow proof values outside possible convert pseudo kernel true kernel discussed example determinant matrix efﬁciently computed sparse coding riemannian manifolds general means given query point manifold expressed sparse combination dictionary elements. idea embed manifold rkhs replace idea combination manifolds general concept linear combination hilbert spaces. speciﬁcally given riemannian dictionary d··· dn}; symd riemannian point seek embedding function symd sparse vector admits sparse representation φ··· words interested solving following kernelised version lasso problem reveals optimisation problem convex similar counterpart euclidean space except deﬁnition consequently greedy relaxation solutions adapted obtain sparse codes solve problem used package specifying solving convex programs. main approaches classiﬁcation based obtained sparse codes given query sample directly indirectly euclidean-based classiﬁer. elucidate approaches below. atoms sparse dictionary associated class labels sparse codes directly used classiﬁcation. approach applicable closed-set identiﬁcation tasks. viδ− i)]t class-speciﬁc sparse codes class label atom discrete dirac function efﬁcient using class-speciﬁc sparse codes computing residual errors case residual error query sample class deﬁned computed riemannian kernel similar manner class minimum residual error deemed represent query. alternatively similarity query sample class deﬁned si=hi. even non-linear like atoms sparse dictionary labelled generated sparse codes training query data euclidean-based classiﬁers support vector machines sparse code hence interpreted feature vector essence means classiﬁcation problem riemannian manifold converted euclidean classiﬁcation problem. approach applicable closed-set open-set classiﬁcation tasks. dictionary sparse codes v··· vm}; mindv among various solutions problem dictionary learning euclidean spaces iterative methods like k-svd received much attention borrowing idea euclidean spaces propose minimise energy iteratively. initialising dictionary example riemannian clustering using karcher mean iterate sparse coding step dictionary update step. sparse coding step ﬁxed computed. dictionary update step ﬁxed updated dictionary atom updated independently. since contains linear non-linear terms closed-form solution computing root cannot sought. such propose alternative solution exploiting previous values updating step. speciﬁcally rearranging estimating well without dictionary learning. atom dictionary training sample. contrast previous state-of-the-art methods several popular closed-set classiﬁcation tasks. residual error approach classiﬁcation described eqn. second performance method evaluated conjunction dictionaries learned three methods random riemannian k-means proposed dictionary learning technique synthetic data. ﬁrst consider multi-class classiﬁcation problem using synthetic data. compared proposed tensor sparse coding log-euclidean sparse representation data used experiments constitutes random samples classes. half samples used training rest used test data. create riemannian manifold samples generated particular tangent space mapped back manifold using exponential positions tangent spaces chosen randomly samples class obeyed table average recognition accuracy wall-clock time synthetic classiﬁcation tasks using log-euclidean sparse representation tensor sparse coding proposed approach. time represented combining times easy hard tasks. normal distribution. ﬁxing mean class increasing class variance created classiﬁcation problems ‘easy’ ‘hard’. draw useful statistics data creation process repeated times. table shows average recognition accuracy total running time algorithms implemented matlab executed intel cpu. terms recognition accuracy obtains superior performance compared previous state-of-the-art approaches. note increasing class variance samples four classes intertwined leading decrease recognition accuracy. performance loge-sr higher might fact generated data modelled gaussian distribution tangent space hence favouring tangent-based solution. focusing time table suggests loge-sr lowest complexity highest. proposed method substantially faster delivering highest recognition accuracy. performed four tests various pose angles. training data composed images marked ‘ba’ ‘bj’ ‘bk’ images ‘bd’ ‘be’ ‘bf’ ‘bg’ labels used test data. riemannian-based methods covariance descriptor described face image using following features table recognition accuracy face recognition task using pca-src gabor log-euclidean sparse representation tensor sparse coding proposed approach. table shows comparison loge-sr purely euclidean sparse representations pca-src gabor cases proposed method obtains highest accuracy. furthermore proposed approach signiﬁcantly outperforms state-of-the-art euclidean solutions especially test images label ‘bg’. texture classiﬁcation. performed classiﬁcation task using brodatz texture dataset examples shown fig. followed test protocol devised generated nine test scenarios various number classes. includes -texture -texture -texture mosaics. create riemannian manifold image ﬁrst downsampled split regions size feature vector pixel region described covariance descriptor features. test scenario covariance matrices class randomly selected training data rest used testing. random selection training/testing data repeated times. fig. compares proposed method loge-sr proposed approach obtains highest recognition accuracy test scenarios except test slightly worse performance tsc. person re-identiﬁcation. used modiﬁed ethz dataset original ethz dataset captured using moving camera providing range variations appearance people. dataset structured sequences. sequence config. performance bordatz texture dataset using log-euclidean sparse representation tensor sparse coding proposed approach. black bars indicate standard deviations. fig. performance sequences ethz dataset terms cumulative matching characteristic curves. proposed method compared histogram plus epitome symmetry-driven accumulation local features riemannian locality preserving projection downsampled images pixels. subject randomly selected images training used rest testing. random selection training testing data repeated times obtain reliable statistics. describe image covariance descriptor computed using following features compared proposed method several techniques previously used pedestrian detection histogram plus epitome symmetry-driven accumulation local features riemannian locality preserving projection performance loge-sr method shown. results could generated timely manner heavy computational load algorithm. results sequence shown fig. terms cumulative matching characteristic curves. curve represents expectation ﬁnding correct match matches. proposed method obtains highest accuracy. sequence similar performance obtained sdalf rlpp proposed lowest performance. compare performance proposed riemannian dictionary learning technique performances dictionaries obtained random sampling riemannian k-means. ﬁrst synthetic data show proposed method obtains lower representation error rkhs followed classiﬁcation experiments texture data. synthetic data. synthesised riemannian samples source points source points considered form ground-truth. synthesised samples used dictionary creation riemannian k-means proposed algorithm. generate source point matrix created computing covariance random samples dimensional normal distribution. mean variance distribution different source point. synthesise riemannian samples uniformly selected source points combined random positive weights weights obeyed normal distribution zero mean unit variance. performance measured terms representation error rkhs eqn. fig. shows representation error algorithms iterate proposed algorithm obtaining lower error riemannian k-means. fig. representation error learned dictionaries rkhs eqn. synthetic data. proposed method compared riemannian k-means source points interpreted form groundtruth. table recognition accuracy texture classiﬁcation task dictionary learning. cases proposed approach used coupled dictionary generated three separate methods random dictionary generation riemannian k-means proposed learning algorithm texture classiﬁcation. consider multi-class classiﬁcation problem using texture images brodatz texture dataset image randomly extracted blocks size train dictionary blocks image randomly selected resulting dictionary learning problem samples. remaining blocks image used probe data gallery samples. process random block creation dictionary generation repeated twenty times. average recognition accuracies probe data reported here. manner section used feature vector create covariance ﬁrst mension grayscale intensity remaining dimensions capture ﬁrst second order gradients. used proposed approach obtain sparse codes coupled dictionary generated three separate methods random dictionary generation riemannian k-means algorithm proposed learning algorithm sparse codes classiﬁed using nearest-neighbour classiﬁer. randomly generated dictionary case classiﬁcation rates averaged runs using different random dictionary. methods dictionaries size trained. best results approach reported table random dictionary k-means dictionary proposed dictionary learning algorithm results show proposed algorithm leads considerable gain accuracy. addressing sparse representation riemannian manifolds proposed seek solution embedding manifolds rkhs recently introduced stein kernel. relaxed extended version lasso problem riemannian manifolds. experiments several classiﬁcation tasks show proposed approach achieves notable improvements discrimination accuracy comparison state-of-the-art methods tensor sparse coding riemannian locality preserving projection symmetry-driven accumulation local features. conjuncture stems better exploitation riemannian geometry stein kernel related geodesic distances tight bound. proposed sparse coding method also considerably faster state-of-the-art maxdet reformulation used tensor sparse coding furthermore proposed algorithm learning riemannian dictionary closely tied stein kernel. comparison riemannian k-means proposed algorithm obtains lower representation error rkhs leads improved classiﬁcation accuracies. future directions include using stein kernel solving large margin classiﬁcation problems riemannian manifolds. translates designing machinery maximises margin matrices based stein divergence considered extension support vector machines tensor spaces. acknowledgements. nicta funded australian government represented department broadband communications digital economy well australian research council centre excellence program.", "year": 2013}