{"title": "Multi-Layer Convolutional Sparse Modeling: Pursuit and Dictionary  Learning", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "The recently proposed Multi-Layer Convolutional Sparse Coding (ML-CSC) model, consisting of a cascade of convolutional sparse layers, provides a new interpretation of Convolutional Neural Networks (CNNs). Under this framework, the computation of the forward pass in a CNN is equivalent to a pursuit algorithm aiming to estimate the nested sparse representation vectors -- or feature maps -- from a given input signal. Despite having served as a pivotal connection between CNNs and sparse modeling, a deeper understanding of the ML-CSC is still lacking: there are no pursuit algorithms that can serve this model exactly, nor are there conditions to guarantee a non-empty model. While one can easily obtain signals that approximately satisfy the ML-CSC constraints, it remains unclear how to simply sample from the model and, more importantly, how one can train the convolutional filters from real data.  In this work, we propose a sound pursuit algorithm for the ML-CSC model by adopting a projection approach. We provide new and improved bounds on the stability of the solution of such pursuit and we analyze different practical alternatives to implement this in practice. We show that the training of the filters is essential to allow for non-trivial signals in the model, and we derive an online algorithm to learn the dictionaries from real data, effectively resulting in cascaded sparse convolutional layers. Last, but not least, we demonstrate the applicability of the ML-CSC model for several applications in an unsupervised setting, providing competitive results. Our work represents a bridge between matrix factorization, sparse dictionary learning and sparse auto-encoders, and we analyze these connections in detail.", "text": "recently proposed multi-layer convolutional sparse coding model consisting cascade convolutional sparse layers provides interpretation convolutional neural networks framework computation forward pass equivalent pursuit algorithm aiming estimate nested sparse representation vectors feature maps given input signal. despite served pivotal connection cnns sparse modeling deeper understanding ml-csc still lacking pursuit algorithms serve model exactly conditions guarantee non-empty model. easily obtain signals approximately satisfy ml-csc constraints remains unclear simply sample model importantly train convolutional ﬁlters real data. work propose sound pursuit algorithm ml-csc model adopting projection approach. provide improved bounds stability solution pursuit analyze diﬀerent practical alternatives implement practice. show training ﬁlters essential allow non-trivial signals model derive online algorithm learn dictionaries real data eﬀectively resulting cascaded sparse convolutional layers. last least demonstrate applicability ml-csc model several applications unsupervised setting providing competitive results. work represents bridge matrix factorization sparse dictionary learning sparse auto-encoders analyze connections detail. signal models broad sense always central development algorithms. ways understanding real world signals proposing ways model intrinsic properties improvements signal image restoration detection classiﬁcation among problems. little decade sparse representation modeling brought idea natural signals described linear combination building blocks components commonly known atoms backed elegant theoretical results model series works dealing either problem pursuit decompositions design learning better atoms real data latter problem termed dictionary learning empowered sparse enforcing methods achieve remarkable results many diﬀerent ﬁelds signal image processing machine learning neural networks hand introduced around forty years shown provide powerful classiﬁcation algorithms series function compositions last half-decade however series incremental modiﬁcations methods boosted become state-of-the-art machine learning tools wide range problems across many diﬀerent ﬁelds part development variants withal research groups begun providing theoretical justiﬁcations analysis strategies cnns diﬀerent perspectives. instance employing wavelet ﬁlters instead adaptive ones work bruna mallat demonstrated scattering networks represent shift invariant analysis operators robust deformations work showed deep neural networks preserve metric structure data under gaussian weights assumption. authors proposed hierarchical tensor factorization analysis model analyze deep cnns. fascinating connections sparse modeling also proposed. neural network architecture shown able learn iterative shrinkage operators essentially unrolling iterations sparse pursuit. building interpretation work showed cnns fact improve performance sparse recovery algorithms. precise connection sparse modeling cnns recently presented contribution centered deﬁning multi-layer convolutional sparse coding model. deploying model real signals compromises made layer approximately explained following one. relaxation pursuit convolutional representations main observation work inference stage cnns nothing forward-pass interpreted crude pursuit algorithm seeking unique sparse representations. useful perspective provides precise optimization objective which turns cnns attempt minimize. work proposed improved pursuits approximating sparse representations network feature maps layered basis pursuit algorithm. nonetheless show later neither forward pass serve ml-csc model exactly provide signals comply model assumptions. addition theoretical guarantees accompanying layered approaches suﬀer bounds become looser network’s depth. lack suitable pursuit turn obscures properly sample ml-csc model train model’s dictionaries real data. proceeding worth noting model analyze work related several recent contributions realm sparse representations deep-learning. hand ml-csc model tightly connected dictionary learning approaches particular leveraging diﬀerent structures constraints construction dictionary. partial list works include chasing butterﬂies approach fast transform learning trainlets among several others. hand unsupervised ﬂavor learning algorithm work shares connections sparse auto-encoders particular k-sparse winner-take-all versions figure model ml-csc extension imposing similar model local perspective patch signal corresponding sparse stripe given sjγ. analogous decomposition stated patch signal represented pjγ. section analyze stability projection problem provide theoretical guarantees practical algorithms. propose learning formulation section allow ﬁrst time obtain trained ml-csc model real data perfectly faithful model assumptions. work restrict study learning model unsupervised setting. approach demonstrated signal approximation unsupervised learning applications section concluding section convolutional sparse coding convolutional sparse coding model assumes signal admits decomposition sparse rn×n convolutional structure. precisely immediate consequence model assumption fact patch global sparse vector sake simplicity drop ﬁrst index stripe patch extraction operators simply denoting stripe sjγ. context sparsity representation better captured pseudo-norm measure opposed traditional provides notion local sparsity deﬁned maximal number non-zeros stripe formally layers terms nested convolutional ﬁlters. suppose convolutional dictionary rn×n ∞-sparse representation cascade model imposing dictionary local ﬁlters ∞-sparse depicted figure case also convolutional dictionary local ﬁlters skipping entries time channels representation multi-layer structure vector viewed sparse representation signal thus refer stripes patches analyzing ml-csc model employ norm deﬁned above also leverage patch counterpart maximum taken patches sparse vector means patch extractor operator order make diﬀerence explicit denote stripes patches respectively. addition employ norm version naturally deﬁned addition referring signal often denote emphasize decomposition terms nested representations {γi}l note also expressed dlγl. purpose following derivations deﬁne eﬀective dictionary level i.e. concisely write interestingly ml-csc interpreted special case model enforces speciﬁc structure intermediate representations. make statement precise following lemma lemma given ml-csc model described convolutional dictionaries {di}l ﬁlters spatial dimensions channels dictionary words convolutional dictionary local atoms dimension ml-csc model structured global convolutional model. figure atoms molecules illustration ml-csc model number local convolutional atoms combined create slightly complex structures molecules second level combined create global atom representing case digit. note even though atoms local convolutional depict respective locations within global structure. refer main body detailed description decomposition. proof lemma rather straight forward include appendix note denoted eﬀective dimension layer nothing else known deep learning community receptive ﬁeld ﬁlter layer here made concept precise context ml-csc model. presented convolutional model assumes every n-dimensional atom located every possible location implies ﬁlter shifted strides alternative eﬀectively reduces redundancy resulting dictionary consider stride greater eﬀective size ﬁlters rather decreases length stripe factor dimension. limit eﬀectively considers non-overlapping blocks stripe length number local ﬁlters. naturally also employ multiple layers ml-csc model. consider layers derivations simplicity. ml-csc imposes unique structure global dictionary provides multilayer linear composition simpler structures. words contains local ndimensional atoms. product contains columns linear combination atoms merging create molecules. layers continue create complex constructions simpler convolutional building blocks. depict example decomposition figure rd-layer convolutional atom digit question obtain dictionaries addressed later make illustration concrete consider atom given sparse producing upper-most words eﬀective atom composed elements eﬀective dictionary building blocks depicted middle figure likewise focus fourth atoms elements express real signals might contain noise deviations idealistic model assumption preventing enforcing model exactly. consider scenario acquiring signal setting objective estimate representations explain measurements error nuisance vector bounded energy pursuit problem searching sparse convolutional features ml-csc model layer. general form pursuit represented deep coding problem -distance estimated representations true ones results depend characterization dictionaries mutual coherence measures maximal normalized correlation atoms dictionary. formally assuming atoms normalnote problem diﬀers dcpe counterpart seek signal close whose representations give rise demanding diﬀerently problem considered special case formulation dcpe dcpe model deviations allowed outer-most level. perspective instance dcpe recall theoretical analysis dcpe problem indicated error thresholds increase layers. here problem suggests completely diﬀerent approach. stability projection given seek underlying representations either dcpe problem. light discussion known stability result dcpe problem close solution problem true representations? obtained bound every layer depends sparsity representation mutual coherence eﬀective dictionary layer allows provide bound cumulative across layers grow depth network. problem assumptions sparse vectors given terms mutual coherence eﬀective dictionaries interestingly enough experimental section fact practice; i.e. eﬀective dictionary becomes incoherent becomes deeper. hand deeper layers correspond higher abstractions levels corresponding representations indeed expected sparser. conditions imposed sparse vectors might seem prohibitive remember follows worst case analysis. moreover eﬀectively construct analytic nested convolutional dictionaries small coherence measures shown interestingly also formulate bounds stability solution i.e. tightest inner-most layer increase moves shallower layers problem. result however provides bounds generally looser presented theorem defer appendix layer-wise manner solving sparse representations progressively surprisingly forward pass algorithm provides approximate solution problem. better alternatives also proposed layered algorithm representation sparse coded given previous representation ˆγi− dictionary solutions dcpe problem naturally algorithms inherit layer-wise relaxation referred above causes theoretical bounds increase function layers network depth. consider algorithm approach circumvents problem sparse coding intermediate features guaranteeing exact expression terms following layer. done ﬁrst running pursuit deepest representation algorithm provides approximate solution following problem setting signal corrupted noise known energy could reformulate problem analogously minimizing norm subject constraint employ formulation however preserves like dull strategy next section that measurements close enough signal model algorithm indeed provides stable estimates ˆγi. fact resulting stability bounds shown generally tighter existing layer-wise pursuit alternative. moreover later results section approach eﬀectively harnessed practice real-data scenario. given signal respective solution ml-csc pursuit algorithm close estimated original representations bounds clearly depend speciﬁc pursuit algorithm employed obtain ˆγl. follows present stability guarantees arise solving sparse coding problem diﬀerent strategies greedy convex relaxation approach. presenting results however shall need state elements become necessary derivations. layers. given support sparse vector supp consider dictionary matrix ridt extracts matrix right-hand side. words simply counts number non-zero rows deﬁne following property intuitively property implies entries cause atoms combined supports cancel other. notice natural assumption make. derivations follow worse-case deterministic analysis however need property formulate recovery guarantees pursuit algorithms. alternatively could assume non-zero entries gaussian distributed case n.v.s. property holds almost surely. second element concerns local stability stripe-rip convolutional version restricted isometric property deﬁned convolutional dictionary satisﬁes s-rip bounds maximal change energy ∞-sparse vector multiplied convolutional dictionary. would like establish equivalent property local sense. pix. recall analogously consider result worthy right shows ﬁrst time model globally stable ∞-sparse signals also bound change energy local sense norm). hand property states nothing unexpected model fully described shift-invariant local model properties able characterized local manner well. lastly lemma refers conjecture analogous lower bound shown hold bounds provided layer-wise pursuit algorithm recovery guarantees tightest inner-most layer bound increases slightly towards shallower representations. relaxation norm case formulation provides local error bounds guarantees greedy version implementation yield global bound representation error. results show ﬂavor theoretical claims obtained proposed ml-csc pursuit algorithm. employing similar derivations detailed respective proofs could principle provide recovery claims versions method employing sparse coding strategies. model algorithm might solve consider given general signal model ml-csc pursuit obtaining representations {ˆγj}. clearly ˆγls words order solve must guarantee sparsity constraints satisﬁed. algorithm progressively recovers sparse representations provide projection general signal solution initialized zero vector algorithm applied progressively larger constraint deepest representation modiﬁcation required setting check every iteration value ˆγls stop accordingly. step given estimated intermediate features norms computed. sparsity constraints satisﬁed algorithm proceeds. hand constraints violated previously computed reported solution. algorithm shown optimal certain assumptions provide sketch proof claim. consider ﬁrst iteration method succeeds providing closest subject respective constraint i.e. providing solution estimates satisfy n.v.s. property respective dictionaries sparsity non-decreasing iterations ˆγk− reason estimate obtained corresponding constraints violated necessarily constraint violated next iteration. therefore closest signal model corresponding iteration constraints violated. ﬁnal comment subject algorithms presented separately indeed related could envision combining single method. distinction motivated making derivations theoretical analysis easier grasp. nevertheless stating theoretical claims without assumption signal close shown measurements close enough signal model i.e. bounded noise ml-csc pursuit algorithm manages obtain approximate solutions representations deploying either algorithms. particular support estimated sparse vectors guaranteed subset correct support satisfy model constraints. introduced n.v.s. property proven ml-csc models locally stable. lastly prior information known signal proposed ompinspired algorithm ﬁnds closest signal measurements gradually increasing support representations guaranteeing model constraints satisﬁed. entire analysis presented relies assumption existence proper dictionaries allowing corresponding nested sparse features clearly ability obtain representations greatly depends design properties dictionaries. traditional sparse modeling scenario certain analytically-deﬁned dictionaries often perform well practice ml-csc case hard propose oﬀ-the-shelf construction would allow meaningful decompositions. clearly consider obtaining algorithm removing assumptions case nothing prevent ˆγl− dense. dictionaries could argue artifact presented algorithm nothing guarantees collection dictionaries would allow signal nested sparse components words know illustrate important point consider case random popular construction sparsity-related applications. case every atom dictionary random variable every entry dlγl random variable discussion conclude components ml-csc model sparse dictionaries dlγl sparse atoms must indeed contain non-zeros. make observation concrete following lemma. simple proof lemma included appendix notably claim tell certain model empty guarantee dictionaries satisfy given sparsity constraint simply sample model drawing inner-most representations real signals without also addressing learning dictionaries would allow respective representations. considering scenario given collection training signals {yk}k included constraint every dictionary atom every level unit norm prevent arbitrarily small coeﬃcients representations formulation complete diﬃcult address directly. begin with constraints representations coupled pursuit problem discussed previous section. addition sparse representations also depend variables follows provide relaxation cost function result simple learning algorithm. problem also understood perspective minimizing number non-zeros representations every layer subject error threshold typical reformulation sparse coding problems. main observation arise fact that since function minimizing sparsity representation done implicitly minimizing sparsity last layer number non-zeros dictionaries layer diﬀerently sparsity intermediate convolutional dictionaries serve proxies sparsity respective representation vectors. following observation recast problem equation following multi-layer convolutional dictionary learning problem example formulation problem seeks sparse representations forcing intermediate convolutional dictionaries sparse. reconstructed signal expected sparse reason enforce property note sparse coding process involved intermediate representations never computed explicitly. recalling theoretical results previous section fact convenient estimate representation recovery bound tightest. following theoretical guarantees presented section alternatively replace constraint deepest representation convex alternative. resulting formulation resembles lasso formulation problem presented theoretical guarproblem equation highly non-convex terms product factors. follows present online alternating minimization algorithm based stochastic gradient descent seeks deepest representation progressively updates layer-wise convolutional dictionaries. representation analyzed detail previous sections solution approximated iterative shrinkage algorithms. particular employ eﬃcient implementation fista algorithm also keep mind representing dictionary convenient terms notation matrices never computed explicitly would prohibitive. instead dictionaries applied eﬀectively convolution operators common deep learning community. addition operators expected eﬃcient apply high sparsity could principle beneﬁt speciﬁc libraries boost performance case seek update respective dictionaries. posed global norm dictionary nothing generalized pursuit well. therefore dictionary minimize function problem applying iterations projected gradient descent. done computing gradient terms problem respect dictionary making gradient step applying hard-thresholding operation depending parameter simply instance iterative hard thresholding algorithm addition computation involves multiplications convolutional dictionaries diﬀerent layers. overall algorithm depicted algorithm expand implementation details results section. parameters models involve penalty deepest representation i.e. parameter dictionary ﬁrst parameter manually determined obtain given given representation error. hand dictionary-wise parameters less intuitive establish question values given learning scenario remains subject current research. nevertheless show experimental section setting manually results eﬀective constructions. ﬁnal comment note approach also employed minimize problem introducing minor modiﬁcations sparse coding stage lasso replaced pursuit tackled greedy alternative iterative hard thresholding alternative addition could consider employing norm surrogate penalty imposed dictionaries. case update still performed projected gradient descent approach though replacing hard thresholding soft counterpart. naturally proposed algorithm tight connections several recent dictionary learning approaches. instance learning formulation closely related chasing butterﬂies work resulting algorithm similar palm method employed authors initially proposed however approach designed general multilevel dictionaries algorithm particularly targeted lower semicontinuous functions. inspiring work hand proposed learning approach dictionary expressed cascade convolutional ﬁlters sparse kernels eﬀectively showed approach used approximate large-dimensional analytic atoms wavelets curvelets. finally proposed approach eﬀectively learns sparse dictionary share similarities double-sparsity work particular trainlets version authors proposed learn dictionary sparse combination cropped wavelets atoms. previous comment work could also potentially expressed product sparse convolutional atoms. connection learning formulation deep convolutional networks? recalling analysis presented forward pass nothing layered nonnegative thresholding algorithm simplest form pursuit ml-csc model layerwise deviations. therefore pursuit setting solved algorithm problem implements convolutional neural network relu operator last layer sparse-enforcing penalties ﬁlters. moreover data-ﬁdelity term formulation proposed optimization problem provides nothing convolutional sparse autoencoder. such work related extensive literature topic. instance sparsity enforced hidden activation layer employing penalty term proportional divergence hidden unit marginals target sparsity probability. related works include k-sparse autoencoders hidden layer constrained activation neuron weights updated gradient descent. respect work thought generalization work pursuit algorithm sophisticated simple thresholding operation ﬁlters composed cascade sparse convolutional ﬁlters. recently work proposed winner-take-all autoencoders. nutshell non-symmetric autoencoders convolutional layers encoder simple linear decoder. sparsity enforced authors refer spatial lifetime sparsity. finally fact formulation eﬀectively provides convolutional network sparse kernels approach reminiscent works attempting sparsify ﬁlters deep learning models. instance work showed weights learned deep convolutional networks sparsiﬁed without considerable degradation classiﬁcation accuracy. nevertheless perpend fact works motivated merely cheaper faster implementations whereas model intrinsically built theoretically justiﬁed sparse kernels. attempt compare approach sparsifying methods stage defer future work. provide experimental results demonstrate several aspects ml-csc model. case-study consider mnist dataset deﬁne model consisting convolutional layers ﬁrst contains local ﬁlters size second consists ﬁlters dimensions last contains ﬁlters dimensions third layer eﬀective size atoms representing figure ml-csc model trained mnist dataset. local ﬁlters dictionary local ﬁlters eﬀective dictionary local atoms eﬀective dictionary which dimensions ﬁlters strides global atoms size training performed algorithm using mini-batch samples iteration. sparse coding stage leverage eﬃcient implementation fista algorithm adjust penalty parameter obtain roughly non-zeros deepest representation parameters penalty parameters dictionaries sparsity levels manually simplicity. addition commonly done various gradient descent methods employ momentum term update dictionaries within projected gradient descent step algorithm memory parameter step size update dictionary iterations algorithm epochs takes approximately minutes. implementation uses matconvnet library leverages eﬃcient functions gpu. depict evolution loss function training figure well sparsity second third dictionaries average residual norm. resulting model depicted figure ﬁrst layer composed simple small-dimensional edges blobs. second dictionary eﬀectively sparse non-zeros combine atoms order create slightly complex edges ones eﬀective dictionary lastly sparse combines atoms order provide atoms resemble diﬀerent kinds digits. ﬁnal global atoms nothing linear combination local small edges means convolutional sparse kernels. interestingly observed mutual coherence eﬀective dictionaries necessarily increase layers often decrease depth. measure relates worst-case analysis conditions mean much context practical performance eﬀective dictionary indeed becomes less correlated depth increases. intuitive simple edges every location expected show large inner products larger correlation complex number-like structures. eﬀect partially explained dictionary redundancy local ﬁlters implies -fold redundancy eﬀective dictionary level. redundancy decreases depth third layer merely atoms also multi-layer representation real images essentially solving projection figure depict multi-layer features obtained algorithm approximate image note representations notably sparse thanks high sparsity dictionaries decompositions provide sparse decomposition number diﬀerent ﬁrst experiment explore recovering sparse vectors corrupted measurements compare presented ml-csc pursuit layered approach sake completion understanding ﬁrst carry experiment synthetic setting projected real digits leveraging dictionaries obtained beginning section. begin constructing layers non-convolutional model signals length dictionaries atoms respectively. ﬁrst dictionary constructed random matrix whereas remaining ones composed sparse atoms random supports sparsity finally representations sampled drawing sparse vectors target sample sparsity normally distributed coeﬃcients. generate signals corrupt gaussian noise obtaining measurements order evaluate projection approach algorithm employing subspace pursuit algorithm sparse coding step oracle target cardinality recall deepest representations obtained inner ones simply computed ˆγi− ˆγi. layered approach hand pursuit representations progresses sequentially ﬁrst running pursuit employing estimate another pursuit etc. spirit employ subspace pursuit layer layer employing oracle cardinality representation stage. results presented figure depict relative error recovered representations γi/γi) bottom non-convolutional case still ml-csc model signal dimension length atoms stride magnitude choose setting synthetic experiment somewhat favor results layered pursuit approach. figure recovery representations noisy synthetic signals. normalized error estimated true representations. bottom normalized intersection estimated true support representations. figure recovery representations noisy mnist digits. normalized error estimated true representations. bottom normalized intersection estimated true support representations. projection algorithm manages retrieve representations accurately layered pursuit evidenced error support recovery. main reason behind diﬃculty layer-by-layer approach entire process relies correct recovery ﬁrst layer representations properly estimated little hope recovery deeper ones. addition representations least sparse ones expected challenging ones recover. projection alternative hand relies estimation deepest sparse. estimated remaining ones simply computed propagating shallower layers. following analysis section support estimated correctly support remaining representations ˆγi. turn deploy layer convolutional dictionaries real digits obtained previously. take digits mnist dataset project trained model essentially running algorithm obtaining representations create noisy measurements gaussian noise providing nothing noisy digits. repeat pursuit approaches seeking estimate underlying representations obtaining results reported figure clearly represents signiﬁcantly challenging scenario layered approach recovers small fraction correct support sparse vectors. projection algorithm hand provides accurate estimations negligible mistakes estimated supports error. note error little signiﬁcance layered approach algorithm manage true supports. reason signiﬁcant deterioration performance layered algorithm method actually ﬁnds alternative representations sparsity providing lower ﬁdelity term projection counterpart ﬁrst layer. however estimates necessarily provide signal model causes errors estimating straight forward application unsupervised learned model approximation well approximate reconstruct signal given non-zero values representation? subsection study performance ml-csc model task comparing related methods present results figure model trained training examples m-term approximation measured remaining testing samples. models designed hidden units given close connection ml-csc model sparse auto-encoders present results obtained approximating signals sparse autoencoders k-sparse autoencoders particular work trains sparse auto-encoders penalizing divergence activation distribution hidden neurons binomial distribution certain target activation rate. such resulting activations never truly sparse. reason since m-term approximation computed picking highest entries hidden neurons setting remaining ones zero method exhibits considerable representation error. k-sparse auto-encoders perform signiﬁcantly better though sensitive number non-zeros used training. indeed model trained non-zeros sample model performs well similar range cardinalities. despite sensitivity training performance remarkable considering simplicity pursuit involved reconstruction done computing wˆγk k-sparse activation obtained hard thresholding convolutional multi-layer version family autoencoders proposed constructions trained stacked manner i.e. training ﬁrst layer independently training second represent features ﬁrst layer introducing pooling operations forth. manner layer trained represent features previous layer entire architecture cannot trivially employed comparison problem. figure m-term approximation function non-zero coeﬃcients mnist digits comparing sparse autoencoders k-sparse autoencoders trainlets proposed ml-csc models diﬀerent ﬁlter sparsity levels. relative number parameters depicted navyblue. regarding ml-csc trained diﬀerent models enforcing diﬀerent levels sparsity convolutional ﬁlters ﬁxed target sparsity non-zeros. sparse coding inner-most done iterative hard thresholding algorithm order guarantee exact number non-zeros. numbers pointing diﬀerent models indicate relative amount parameters model number parameters sparse-autoencoders k-sparse autoencoders without counting biases). larger number parameters lower representation error model able provide. particular ml-csc yields slightly better representation error k-sparse autoencoders wide range non-zero values orders magnitude less parameters. since training ml-csc model also understood dictionary learning algorithm compare state-of-the-art method case trained trainlet atoms osdl algorithm. note comparison interesting osdl also provides sparse atoms reduced number parameters. sake comparison employed atom-sparsity results parameters relative total model size sparse coding done also algorithm. notably performance relatively sophisticated dictionary learning method leverages representation power cropped wavelets base dictionary slightly superior proposed ml-csc. unsupervised trained models usually employed feature extractors popular assess quality features train linear classiﬁer certain classiﬁcation task. non-negativity constraints ﬁtting simple linear classiﬁer obtained features. employing elastic-net formulation regularization parameter addition norm) results slightly denser representations improved classiﬁcation performance. similarly non-negativity constraint signiﬁcantly facilitates classiﬁcation linear classiﬁers. compare results similar methods experimental setup depict results table reporting classiﬁcation error testing samples. recall within ml-csc model features clear meaning provide sparse representation diﬀerent layer. leverage multi-layer decomposition natural within unsupervised classiﬁcation framework. detail classiﬁcation performance achieved model diﬀerent scenarios ﬁrst employ kdimensional features corresponding second layer ml-csc model obtaining better performance equivalent k-sparse autoencoder. second case previous features k-dimensional features third layer resulting classiﬁcation error comparable stacked winner take autoencoder lastly worth mentioning stacked version convolutional autoencoder achieve classiﬁcation error provide signiﬁcantly better results. however note model trained -stage process involving signiﬁcant pooling operations features diﬀerent layers. importantly features computed model -dimensional thus cannot directly compared results reporter method. principle similar stacked-constructions employ pooling could built model well remains part ongoing work. carefully revisited ml-csc model explored problem projecting signal onto provided theoretical bounds solution problem well stability results practical algorithms greedy convex. search signals within model propose simple eﬀective learning formulation adapting dictionaries across diﬀerent layers represent natural images. particular employed dictionary sparsity proxi sparsity inner representations eﬀectively yields model consisting cascade sparse convolutional ﬁlters. demonstrated proposed approach learning model mnist dataset studied several practical applications. experimental results show ml-csc indeed provide signiﬁcant expressiveness small number model parameters. several question remain open model modiﬁed incorporate pooling operations layers? consequences theoretical practical would have? recast learning problem order address supervised semi-supervised learning scenarios? lastly envisage analysis provided work empower development better practical theoretical tools structured dictionary learning approaches ﬁeld deep learning machine learning general.", "year": 2017}