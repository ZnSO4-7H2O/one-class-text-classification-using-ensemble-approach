{"title": "On- and Off-Policy Monotonic Policy Improvement", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Monotonic policy improvement and off-policy learning are two main desirable properties for reinforcement learning algorithms. In this paper, by lower bounding the performance difference of two policies, we show that the monotonic policy improvement is guaranteed from on- and off-policy mixture samples. An optimization procedure which applies the proposed bound can be regarded as an off-policy natural policy gradient method. In order to support the theoretical result, we provide a trust region policy optimization method using experience replay as a naive application of our bound, and evaluate its performance in two classical benchmark problems.", "text": "monotonic policy improvement oﬀ-policy learning main desirable properties reinforcement learning algorithms. paper lower bounding performance diﬀerence policies show monotonic policy improvement guaranteed onoﬀ-policy mixture samples. optimization procedure applies proposed bound regarded oﬀ-policy natural policy gradient method. order support theoretical result provide trust region policy optimization method using experience replay naive application bound evaluate performance classical benchmark problems. reinforcement learning aims optimize behavior agent interacts sequentially unknown environment order maximize long term future reward. main desirable properties algorithms monotonic policy improvement oﬀ-policy learning. model environment available state fully observable sequence greedy policies generated policy iteration scheme guaranteed improve monotonically. however approximate policy iteration including generated policy could perform worse lead policy oscillation policy degradation order avoid phenomena eﬀorts guarantee monotonic policy improvement hand oﬀ-policy data also crucial real world applications. oﬀ-policy setting policy generate data diﬀerent policy optimized. many theoretical eﬀorts eﬃciently oﬀ-policy data oﬀ-policy learning methods enable agent example optimize huge function approximators eﬀectively derive general performance bound onoﬀ-policy mixture samples. optimization procedure applies proposed bound regarded oﬀ-policy natural policy gradient method. order support theoretical result provide trust region policy optimization method using experience replay naive application bound evaluate performance classical benchmark problems. speciﬁed tuple ﬁnite possible states environment ﬁnite possible actions agent choose. markovian state transition probability distribution bounded reward function initial state distribution discount factor. interested model-free thus suppose unknown. state action spaces policy stochastic denotes distribution state-action pair policy exists unnormalized γ-discounted future state distribution initial state distribution γtpr deﬁne state value function action value vectors size scalar vectors size |s||a| stochastic matrix size |s||a| contains state transition probability distribution stochastic matrix size |s|×|s||a| contains policy stochastic matrix size |s|×|s| represents state transition matrix policy matrix whose entries maxj |mij| stationary policies lemma stationary policies inﬁnite-horizon state transition probability mixture coeﬃcient onoﬀ-policy samples. l-norm diﬀerence γ-discounted state distributions upper bounded follows corollary stationary policies. mixture coeﬃcient onoﬀ-policy samples. l-norm diﬀerence γ-discounted state distributions upper bounded follows right hand side positive. however discussed schulman evaluating dmax calculate divergence every point state space. following expected divergence identical used literature natural policy gradient analogously optimization procedure uses metric applies bound approximately regarded variant oﬀ-policy natural policy gradient method. dtvπ) order support theoretical result evaluate naive application bound classic benchmark problems. note method presented possible implementation perform monotonic policy improvement approximately onoﬀ-policy mixture samples. experiment conducted open gym. tasks acrobot-v pendulumnv. agent implemented based trpo baselines modiﬁed deal oﬀ-policy samples. policy state value approximated feedforward neural network hidden layers consist tanh units. surrogate objective approximated generalized advantage estimation state value function updated using λ-return target on-policy trajectory. decay rate discount factor trust region single on-policy trajectory consists transitions. previous trajectories stored replay buﬀer trajectries drawn oﬀ-policy samples. learning results various value mixture coeﬃcient shown figure learning result average independent runs diﬀerent seeds. acrobot-v onoﬀ-policy mixture learning outperformed on-policy trpo furthermore monotonic policy improvement established oﬀ-policy samples only resulted faster learning trpo however smaller becomes slower learning progresses. empirical result emphasizes importance deal mixture metric constraint. theoretical result proposed method extension works pirotta schulman onoﬀ-policy mixture case. thomas proposed algorithm monotonic improvement oﬀ-policy policy evaluation techique. however computational complexity high. proposed interpolate trpo deep deterministic policy gradient showed performance bound onoﬀ-policy mixture update well. notation penalty terms performance bound dmax constant respect policy update contrast corollary multiplied penalty terms dmax furthermore bound general sense dmax specify policy updatd. thus penalty terms controlled paper showed monotonic policy improvement guaranteed onoﬀ-policy mixture samples lower bounding performance diﬀerence policies. optimization scheme applies derived bound regarded oﬀ-policy natural policy gradient method. order support theoretical result provided trpo method using experience replay naive application bound evaluated performance various values mixture coeﬃcient. important direction practical algorithm uses metric constraint. determining depending", "year": 2017}