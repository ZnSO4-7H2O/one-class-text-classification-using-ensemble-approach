{"title": "Learning the Probabilistic Structure of Cumulative Phenomena with  Suppes-Bayes Causal Networks", "tag": ["cs.LG", "cs.AI"], "abstract": "One of the critical issues when adopting Bayesian networks (BNs) to model dependencies among random variables is to \"learn\" their structure, given the huge search space of possible solutions, i.e., all the possible direct acyclic graphs. This is a well-known NP-hard problem, which is also complicated by known pitfalls such as the issue of I-equivalence among different structures. In this work we restrict the investigations on BN structure learning to a specific class of networks, i.e., those representing the dynamics of phenomena characterized by the monotonic accumulation of events. Such phenomena allow to set specific structural constraints based on Suppes' theory of probabilistic causation and, accordingly, to define constrained BNs, named Suppes-Bayes Causal Networks (SBCNs). We here investigate the structure learning of SBCNs via extensive simulations with various state-of-the-art search strategies, such as canonical local search techniques and Genetic Algorithms. Among the main results we show that Suppes' constraints deeply simplify the learning task, by reducing the solution search space and providing a temporal ordering on the variables.", "text": "marco nobile department informatics university milano-bicocca milan italy marco antoniotti department informatics university milano-bicocca milan italy abstract—one critical issues adopting bayesian networks model dependencies among random variables learn structure given huge search space possible solutions i.e. possible direct acyclic graphs. wellknown -hard problem also complicated known pitfalls issue i-equivalence among different structures. work restrict investigations structure learning speciﬁc class networks representing dynamics phenomena characterized monotonic accumulation events. phenomena allow speciﬁc structural constraints based suppes’ theory probabilistic causation accordingly deﬁne constrained named suppes-bayes causal networks investigate structure learning sbcns extensive simulations various state-of-the-art search strategies canonical local search techniques genetic algorithms. among main results show suppes’ constraints deeply simplify learning task reducing solution search space providing temporal ordering variables. graphical models representing relations conditional dependence among random variables encoded directed acyclic graphs last decades effectively applied several different ﬁelds disciplines constraint-based techniques mainly works judea pearl approaches discovering relations conditional independence data using constraints learn network; score-based techniques case problem learning structure deﬁned optimization problem search space valid solutions evaluated score based likelihood function regardless approach main difﬁculty learning problem huge number valid solutions search space namely possible dags makes task known hard problem even constraining node parents therefore state-of-the-art techniques solve task means meta-heuristics inference complicated wellknown issue i-equivalence even different structures encode conditional independence properties thus algorithm structural learning converge equivalent structures rather correct given inference performed learning statistical relations among variables emerging induced distributions rather structure paper investigate application characterization speciﬁc class dynamical phenomena i.e. driven monotonic accumulation events. particular process modeled/observed must imply temporal ordering among events monotonic accumulation time probabilistically entails occurrence earlier event must positively correlated subsequent occurrence successors leading signiﬁcant temporal pattern example found dynamics cascading failures failure system interconnected parts failure part trigger failure successive parts. phenomenon happen different contexts power transmission computer networking ﬁnance biological systems.in scenarios different conﬁgurations lead failure likely others hence modeled probabilistically particular conditions mentioned represent basis notion probabilistic causation patrick suppes allow deﬁne structural constraints inferred which accordingly dubbed suppes-bayes causal networks previous works sbcns already applied number different ﬁelds ranging cancer progression inference social discrimination discovery speciﬁcally position work within aforementioned optimization-based framework structure learning. goal paper investigate structure learning inﬂuenced different algorithmic choices representing cumulative dynamical phenomena. particular known given temporal ordering variables ﬁnding optimal solution consistent ordering accomplished time number variables bounded in-degree node thus search space orderings performed efﬁciently search space structures search space much smaller branching factor lower acyclicity checks necessary determination right ordering complex dynamical phenomena generally difﬁcult task often requires considerable domain knowledge. however representation cumulative phenomena sbcns allows soften hurdle suppes’ constraints dramatically reduce search space valid solutions also providing temporal ordering variables. represents serious theoretical advancement structure learning modeling cumulative phenomena investigate work series synthetic experiments. paper structured follows. section ﬁrst provide background task structure learning details classical ideas foundations framework. then section describe features cumulative phenomena representing formally deﬁne sbcns machinery modeling. also introduce approximate algorithm efﬁcient inference sbcns. section make synthetic data simulate cumulative phenomena compare performance various search heuristics strategies especially focussection provide introduction bayesian networks together review state-of-the-art methods tackle problem learning structures observation variables described network. bayesian network statistical graphical model succinctly represents joint distribution random variables encodes direct acyclic graph nodes referring variables relations given structure full joint distribution variables written product conditional distributions variable. fact edge pair nodes e.g. denotes statistical dependence i.e. regardless variables condition variables holds variables connected toward node determines parent nodes recall that graph acyclic node cannot ancestor descendant another node would obviously cause directed cycle. moreover joint distribution variables written where node incoming edges product marginal probability thus compute probability combination values variables conditional probabilities variable given parents must parameterized. however even simplest case binary variables number parameters conditional probability table locally exponential size namely thus total number parameters needed compute full joint anyway considerably less sparse networks. useful property graphical structure deﬁne variable nodes called markov blanket that conditioned variable independent variables network. proven that markov blanket consists node’s parents children parents children also point usage symmetrical notion conditional dependence introduces important limitations task learning structure matter fact note edges denote equivalent dependence hence graphs different structure model identical independence conditional independence relations yields notion markov equivalence class partially directed acyclic graph edges take either orientation left undirected. also known markov equivalent skeleton v-structures former edges ignoring direction latter edge structures variable least parents share edge interesting relation canonical boolean logical operators formulas variables fact formulas deterministic principle naturally softened probabilistic relations allow degree uncertainty noise. probabilistic approach modeling logic allows representation qualitative relationships among variables inherently robust small perturbations noise. instance phrase order hear music listening necessary sufﬁcient power headphones plugged represented probabilistic conjunctive formulation relates power headphones music probability music audible depends whether power headphones present. hand small probability conditional pairwise independence tests algorithm would perform test possible relations. number indeed exponential requiring condition power testing conditional independence variables. inherent intractability class algorithms requires introduction approximations. score-based approaches approach structural learning aims maximization likelihood observed data. since assume data independent identically distributed likelihood data simply product probability observation. however likelihood never used practice quantity rapidly becomes small impossible represent computer. instead logarithm likelihood function usually adopted three reasons function monotonic; log-likelihood mitigates numerical issues caused normal likelihood; easy compute logarithm product equal logs likelihood bayesian network product simple terms practically however problem learning network structure maximizing loglikelihood alone. namely arbitrary data likely graph always fully connected since adding edge increase likelihood data. overcome limitation log-likelihood almost always supplemented regularization term penalizes complexity model. plethora regularization terms based information theory others bayesian literature initial families methods aimed learning structure data. methods belonging ﬁrst family explicitly capture conditional independence relations encoded edges referred constraint based approaches second family score based approaches aims selection model maximizes likelihood data given model. since approaches lead intractability computing verifying exact solution impractical. reason heuristic methods like hill climbing tabu search simulated annealing genetic algorithms generally employed. algorithms characterized polynomial complexity although provide asymptotic guarantees converging optimal solutions. recently third class learning algorithms takes advantage specialized logical relations introduced rest section describe detail approaches leaving speciﬁc readings detailed discussions constraint-based approaches brieﬂy present intuitive explanation several common algorithms used structure discovery explicitly considering conditional independence relations between variables. detailed explanations analyses complexity correctness stability refer reader related references basic idea behind class algorithms build graph structure reﬂecting independence relations observed data thus matching closely possible empirical distribution. difﬁculty approach lies number statistics references therein) serve promote sparsity learned graph structure though different regularization terms better suited particular applications learning logically constrained networks section ii-a noted important class captures common binary logical operators although learning algorithms mentioned used infer structure networks algorithms employ knowledge logical constraints learning process. widespread approach learning monotonic progression networks directed acyclic graph structure conjunctive events conjunctive bayesian networks approach originally adopted model cancer progression terms accumulation drivers genes closely related model discuss work. standard bernoulli random variables constraint probability node taking value zero least parents value deﬁnes conjunctive relationship parents must possibly thus model alone cannot represent noise essential part real data. response this hidden cbns developed augmenting variables correspondence variable represents observed state assigned variable captures true state. thus variable takes value corresponding variable high probability opposite value probability order model noise observations. model variables latent i.e. present observed data inferred observed values variables. section present foundations framework speciﬁcally deﬁne main characteristics suppes-bayes causal networks heuristic strategies likelihood without losing generality consider simpliﬁed formulation problem learning structure variables depicted graph bernoulli random variables i.e. support conclusions derived settings also directily applied general case nodes describe geneal random variables precisely consider input learning task dataset bernoulli variables cross-sectional samples. assume value indicate given variable observed sample variable observed. notion prima facie causality known limitations context general theories causality however characterizations seems appropriate model dynamics phenomena driven monotonic accumulation events temporal order among implied thus occurrence early event positively correlates subsequent occurrence time later one. recall systems cascading failure occur conﬁgurations events speciﬁc order likely cause failure others. condition leads emergence observable temporal pattern among events captured suppes’ deﬁnition causality terms statistial relevance i.e. statistical dependency. consider graphical representation aforementioned dynamics terms furthermore consider given node name nodes pointing then joint probability distribution variables induced written building model need constrain characteristics considered relations depicted network order account cumulative process mentioned which turns needs reﬂected induced probability distribution extent deﬁne class bernoulli random variables named monotonic progression networks intuitively mpns represent progression events monotonically accumulating time conditions event happen described probabilistic version canonical boolean operators i.e. conjunction inclusive disjunction exclusive disjunction moreover discussed mpns model accumulative phenomena probabilistic fashion i.e. also modeling irregularities data small probability observing later events given predecessors. given premises authors describe efﬁcient algorithm learn structure constrained bayesian networks account suppes’ criteria later dubbed suppes-bayes causal networks sbcns well suited model cumulative phenomena encode irregularities similar mpns efﬁcient inference schema algorithm rely observation circumventing intrinsic computational complexity task learning structure bayesian network postulate pre-determined ordering among nodes. intuitively capri exploits suppes’ theory ﬁrst mine ordering among nodes reducing complexity problem network means likelihood maximization. also shown sbcn learned using algorithm also embed notion accumulation time deﬁned speciﬁcally conjunctive parent sets; nevertheless sbcns easily generalized represent canonical boolean operetors notwithstanding increase algorithmic complexity refer reader details following formally deﬁne sbcn. deﬁnition given input cross-sectional dataset bernoulli variables samples suppesbayes causal network sbcn subsumed directed acyclic graph following requirements hold involving selective advantage relation nodes mild assumptions arcs satisfying suppes’ constraints before; among subsets arcs whose corresponding graph maximizes likelihood data adopted regularization function moving notice efﬁcient implementation suppes’ constraints algorithm general guarantee converge monotonic progression networks depicted before. fact probabilistic relations mpns deﬁned algorithm considers pair nodes rather pair overcome limitation could extend algorithm order learn addition network structure also logical relations involving parent increasing overall computational complexity. again refer interested reader discussions provided without losing generality purpose work consider efﬁcient implementation algorithm conclude section reporting pseudocode efﬁcient learning algorithm infer sbcns presented adopt assessment performance simulations next section. reported line algorithm problem inference sbcns re-stated optimization problem goal maximization likelihood score. regardless strategy used inference process huge size search space valid solutions makes problem hard solve. moreover stated above problem learning structure -hard that state-of-theart techniques largely rely heuristics often based stochastic population-based global optimization algorithms. instance methods based genetic algorithms colony optimization proposed literature. genetic algorithms introduced holland global search methodology inspired mechanisms natural selection. population candidate solutions iteratively evolves converging towards global optimum given ﬁtness function that context corresponds score maximized. characterized well-known convergence theorem named schema theorem proves presence schema population i.e. positively affecting ﬁtness value increases exponentially generation generation. shown effective learning case available available priori knowledge nodes’ ordering population composed randomly created individuals usually represented ﬁxedlength strings ﬁnite alphabet. strings encode putative solutions problem investigation; case learning individuals represent linearized adjacency matrices candidate nodes encoded string binary values whose length individuals undergo iterative process whereby three genetic operators i.e. selection crossover mutation applied sequence simulate evolution process results population possibly improved solutions. selection process individuals chosen using ﬁtness-dependent sampling procedure crossover operator used recombine structures promising parents taken improved offsprings. crossover generally applied userdeﬁned probability finally mutation operator used introduce genetic materials population allowing exploration search space. mutation operator replaces arbitrary symbol individual probability random symbol taken alphabet. case mutation consists ﬂipping single individual certain probability. genetic operators applied proceed replacing whole population offspring identifying best individuals among parents offspring used create population. worth noting case ordered nodes crossover mutation closed operators resulting offsprings always encode valid dags. ensuring consistent population individuals throughout generations case unordered nodes operators followed correction procedure candidate analyzed identify presence invalid cycles. information implementation inference including correction phase refer interested reader discuss results large number experiments simulated data assessing performance state-of-the-art score-based techniques structure inference comparing performance methods learning scheme deﬁned algorithm main objective investigate performance effected different algorithmic choices different steps learning process. constrain induced distribution generative structure implying cumulative model either conjunctions disjunctions i.e. child node cannot occur parent activated described method section iii. conﬁgurations generate random structures. furthermore simulate model noise terms random observations included generated datasets different rates described next section. data generation conﬁgurations chosen reﬂect different structural complexities models terms number parameters i.e. arcs learned different types induced distributions suitable model cumulative phenomena deﬁned mpns i.e. conjunction inclusive disjunction situations reduced sample sizes noisy data. provide example data generation. number nodes want include network pmin pmax minimum maximum probabilities node. directed acyclic graph without disconnected components topologies) maximum depth node parents generated shown algorithm algorithm data generation single source directed acyclic graphs input number nodes graph pmin pmax minimum maximum probabilities node maximum incoming edges node. following adopted likelihood functions experiments implemented using bnlearn package written language algorithm implemented using python language exploiting inspyred networkx numpy packages. algorithm settings test performance classical search strategies hill climbing tabu search sophisticated algorithms genetic algorithms generate data described networks nodes generated independent datasets combination sample levels noise levels total independent datasets. experiments repeated either including including suppes’ constraints described algorithm independently using distinct optimization scores regularizators namely standard log-likelihood leading ﬁnal number different conﬁgurations. detailed description regularizators beyond scope paper critically discuss different performances granted strategy inference bns. respect used restricted data generation settings using networks nodes datasets samples noise levels total independent datasets. tested either without suppes’ constraints using further experiments multi-objective optimization techniques non-dominated sorting genetic algorithm performed shown worse overall performance higher computational cost respect canonical looking figure ﬁrst appreciate variation accuracy respect speciﬁc search strategy i.e. taken example typical behavior. brief overall performance worsens respect larger number nodes network complex generative structures smaller samples sizes higher noise rates. although trend intuitively expected given larger number parameters learned complex models underline role statistical complications presence spurious correlations occurrence simpson’s paradox interesting observe typical decrease accuracy compare topologies properties different number roots former case expect fact lower number arcs learned hence attribute decrease performance emergence spurious correlations among independent nodes children different sources dag. fact that sample sizes inﬁnite unlikely observe perfect independence accordingly likelihood scores lead overﬁtting. trends displayed figure shared analyzed search strategies. role regularization factor looking figure ﬁrst notice accuracy regularization dramatically lower cases consequence expected overﬁtting leading unintuitive behaviors fact methods performance decreases higher level noise applied accuracy seems improve higher noise rates. result might explained observing given topological structure structural spurious correlations arise given node undirected predecessors higher error rates accordingly random samples datasets correlations reduced hence leading lower impact regularization term. given considerations hypothesize overall trend scarce penalization likelihood favoring dense networks rather sparse ones. search strategies signiﬁcant differences performance accuracy observed. however observe consistent improvement sensitivity using suggests different inherent properties search schemes regularization terms rather search strategy account inference performance capable returning denser networks better rates. probably ga’s random mutations allow jumps areas search space characterized excellent ﬁtness could reached means greedy approaches like important result observed across overall performance considered search strategies dramatically enhanced introduction suppes’ structural constraints. particular e.g. figure constant improvement inference fig. performance terms accuracy considered structures nodes noise levels step sample sizes samples. regularization scheme consider search stategy classical case suppes’ priors applied. fig. performance terms accuracy directed acyclic graphs multiple sources disjunctive parents nodes noise levels step sample sizes samples. consider search strategy classical case suppes’ priors applied show results considered regularizators i.e. standard log-likelihood problem learning structure bayesian network known untractable constraints allow prune search space possible solutions leading tremendous reduction number valid networks considered hence taming complexity problem remarkable way. discussed theoretical implications inference process different steps also comparing various state-of-the-art algorithmic approaches regularization methods. ﬁnally provided in-depth study realistically simulated data effect inference choice thus providing sound guidelines design efﬁcient algorithms inference models cumulative phenomena. according results genetic algorithms outperform hill climbing tabu search search strategy respect sensitivity speciﬁcity. could prove suppes’ constraints consistently improve inference accuracy considererd scenarios inference schemes hence positioning sbcns benchmark efﬁcient inference representation cumulative phenomena. larranaga poza yurramendi murga kuijpers structure learning bayesian networks genetic algorithms ieee transactions pattern analysis machine intelligence vol. fig. performance terms accuracy directed acyclic graphs multiple sources disjunctive parents nodes noise levels step sample sizes samples. regularization scheme consider search strategies classical case suppes’ priors applied. suppes’ priors used. even though accuracy inference affected noise observations fact results suppes’ priors consistently better inference constraints respect considered inference settings performance measures. extremely important result proves introduction structural constraints based suppes’ probabilistic causation indeed simplify optimization task reducing huge search space dealing describing cumulative phenomena. paper investigated structure learning bayesian networks aimed modeling phenomena driven monotonic accumulation events time. made subclass constrained bayesian networks named suppes-bayes causal networks infig. performance terms sensitivity speciﬁcity forests directed acyclic graphs multiple sources conjunctive parents directed acyclic graphs multiple sources disjunctive parents nodes noise levels step sample sizes samples. regularization scheme algorithms either consider consider suppes’ constraints ramazzotti caravagna loohuis graudenzi korsunsky mauri antoniotti mishra capri efﬁcient inference cancer progression models cross-sectional data bioinformatics vol. caravagna graudenzi ramazzotti sanzpamplona sano mauri moreno antoniotti mishra algorithmic methods infer evolutionary trajectories cancer progression pnas press buntine theory reﬁnement bayesian networks proceedings seventh conference uncertainty artiﬁcial intelligence. morgan kaufmann publishers inc. ramazzotti nobile cazzaniga mauri antoniotti parallel implementation efﬁcient search schemes inference cancer progression models ieee international conference computational intelligence bioinformatics computational biology. ieee campos g´amez mart´ın puerta castell´on learning bayesian networks colony optimisation searching different spaces mathware soft computing. vol. n´um. holland adaptation natural artiﬁcial systems introductory analysis applications biology control artiﬁcial intelligence. michigan press back selective pressure evolutionary algorithms characterization selection mechanisms evolutionary computation ieee world congress computational intelligence. proceedings first ieee conference schwarz estimating dimension model annals statistics vol. heckerman geiger chickering learning bayesian networks combination knowledge statistical data machine learning vol. cooper herskovits bayesian method constructing bayesian belief networks databases proceedings seventh conference uncertainty artiﬁcial intelligence. morgan kaufmann publishers inc. pearson mathematical contributions theory evolution.–on form spurious correlation arise indices used measurement organs proceedings royal society london vol. daniele ramazzotti daniele ramazzotti received ph.d. computer science university milanobicocca italy february currently postdoctoral research fellow department pathology stanford university usa. current research interests involves bioinformatics speciﬁc focus cancer evolution statistics bayesian learning machine learning algorithmarco nobile marco nobile received ph.d. computer science university milanobicocca italy currently research fellow department informatics systems communication university milanobicocca italy. also member sysbio centre systems biology italy. research interests include evolutionary computation swarm intelligence bio-inspired algorithms computational biology high-performance computing. marco antoniotti prof. marco antoniotti obtained m.s. ph.d. computer science courant institute mathematical sciences york university current research interests bioinformatics systems biology; simulation veriﬁcation language design issues embedded hybrid systems. prof. antoniotti author several journal conference papers author director several software projects. p.i. bimib group dipartimento informatica sistemistica comunicazione dell’universit`a degli studi milano bicocca. prof. antoniotti member ieee. alex graudenzi alex graudenzi assistant professor computer science dept. informatics systems communication university milan-bicocca. received ph.d. multiscale modeling computational simulation unviersity modena reggio emilia. works boundaries informatics complex systems statistics systems biology investigate properties biological systems especially emergence development cancer. author numerous articles international journals conference proceedings.", "year": 2017}