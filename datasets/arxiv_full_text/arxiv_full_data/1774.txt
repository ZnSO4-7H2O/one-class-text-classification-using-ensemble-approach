{"title": "A Probabilistic Generative Grammar for Semantic Parsing", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "We present a framework that couples the syntax and semantics of natural language sentences in a generative model, in order to develop a semantic parser that jointly infers the syntactic, morphological, and semantic representations of a given sentence under the guidance of background knowledge. To generate a sentence in our framework, a semantic statement is first sampled from a prior, such as from a set of beliefs in a knowledge base. Given this semantic statement, a grammar probabilistically generates the output sentence. A joint semantic-syntactic parser is derived that returns the $k$-best semantic and syntactic parses for a given sentence. The semantic prior is flexible, and can be used to incorporate background knowledge during parsing, in ways unlike previous semantic parsing approaches. For example, semantic statements corresponding to beliefs in a knowledge base can be given higher prior probability, type-correct statements can be given somewhat lower probability, and beliefs outside the knowledge base can be given lower probability. The construction of our grammar invokes a novel application of hierarchical Dirichlet processes (HDPs), which in turn, requires a novel and efficient inference approach. We present experimental results showing, for a simple grammar, that our parser outperforms a state-of-the-art CCG semantic parser and scales to knowledge bases with millions of beliefs.", "text": "present framework couples syntax semantics natural language sentences generative model order develop semantic parser jointly infers syntactic morphological semantic representations given sentence guidance background knowledge. generate sentence framework semantic statement ﬁrst sampled prior beliefs knowledge base. given semantic statement grammar probabilistically generates output sentence. joint semantic-syntactic parser derived returns k-best semantic syntactic parses given sentence. semantic prior ﬂexible used incorporate background knowledge parsing ways unlike previous semantic parsing approaches. example semantic statements corresponding beliefs knowledge base given higher prior probability type-correct statements given somewhat lower probability beliefs outside knowledge base given lower probability. construction grammar invokes novel application hierarchical dirichlet processes turn requires novel efﬁcient inference approach. present experimental results showing simple grammar parser outperforms state-of-theart semantic parser scales knowledge bases millions beliefs. accurate efﬁcient semantic parsing long-standing goal natural language processing. countless applications methods provide deep semantic analyses sentences. leveraging semantic information text provide improved algorithms many problems named entity recognition word sense disambiguation semantic role labeling coreference resolution etc. sufﬁciently expressive semantic parser directly provide solutions many problems. lowerlevel language processing tasks mentioned even beneﬁt incorporating semantic information especially task solved jointly semantic parsing. knowledge plays critical role natural language understanding. formalisms used semantic parsing approaches require ontology entities predicates semantic content sentences represented. moreover even seemingly trivial sentences large number ambiguous interpretations. consider sentence started machine example. without additional knowledge fact machine refer computing devices figure high-level illustration setting grammar applied. parsing input observed sentence knowledge base want probable semantic-syntactic parses given input training data. thesis underlying research natural language understanding requires belief system; large pre-existing beliefs related domain discourse. clearly young children many beliefs world learn language fact process learning language largely learning ground meanings words sentences non-linguistically acquired beliefs. ways idea language understanding requires belief system natural language researchers saying years background knowledge essential reducing ambiguity sentence meanings despite general acknowledgement importance background knowledge natural language understanding systems actually employ large belief system basis comprehending sentence meanings determining whether meaning sentence contradicts extends already present belief system. present step direction probabilistic semantic parser uses large knowledge base form prior probability distribution meanings sentences parses \"understands\" sentence either identifying existing beliefs correspond sentence’s meaning creating beliefs. precisely semantic parser corresponds probabilistic generative model assigns high probability sentence semantic parses resulting beliefs already holds lower prior probability parses resulting beliefs hold consistent abstract knowledge semantic types arguments different relations still lower prior probability parses contradict beliefs entity types participate relations. work ﬁrst step. limited currently parse sentences simple noun-verb-noun syntax considers factual assertions declarative sentences. importance introduces novel approach semantic parser prefers sentence semantic parses yield assertions already believes still allowing lower prior probability sentence interpretations yield beliefs involving novel words even allowing beliefs inconsistent background knowledge semantic typing different relations. introduce algorithms training probabilistic grammar producing parses high posterior probability given prior beliefs sentence. present experimental evidence success tractability approach sentences simple syntax evidence showing incorporated belief system containing millions beliefs allows outperform state-of-the-art semantic parsers hold beliefs. thus provide principled probabilistic approach using current belief system guide semantic interpretation sentences which turn used augment extend belief system. also argue approach extended document-level context sentence additional source background beliefs. reasons including limited performance complexity modern parsers operate tokens words. worked sufﬁciently well many applications approach assumes tokenization preprocessing step produces correct output. nontrivial many languages chinese thai japanese tibetic languages. addition large portion english vocabulary created combination simpler morphemes words build-er indescrib-able anti-modern-ist. moreover language noisy. text messages communication social media real-world speech examples noise obfuscating language. standard algorithms tokenization lemmatization preprocessing oblivious underlying semantics much less background knowledge. incorporating components joint parsing framework enable semantics background knowledge jointly inform lower-level processing language. method couples semantics syntax lower-level aspects language guided background knowledge semantic prior. demonstrate leveraged framework model morphology individual verbs temporally-scoped relation extraction task. semantic statements logical expressions represent meaning sentences. example semantic statement turn_on_device used express meaning sentence example given earlier. many languages semantic formalisms used encode logical forms ﬁrstorder logic lambda calculus frame semantics abstract meaning representation dependency-based compositional semantics vector-space semantics example. approach ﬂexible require speciﬁc semantic formalism. section review hdps describe setting require deﬁne grammar. present approach section perform inference setting. section present main generative process framework detail application hdp. although present model generative perspective show description framework discriminative techniques integrated. inference model described section there present chart-driven agenda parser leverage semantic prior guide search. finally section evaluate parser relation-extraction tasks ﬁrst task extract simple predicate-argument representations sentences second temporally-scoped relation extraction task demonstrates parser’s ability model morphology individual words leading improved generalization performance words. moreover demonstrate inclusion background knowledge knowledge base improves parsing performance tasks. contributions article framework deﬁne grammars coupled semantics syntax morphology etc. prior semantic statement incorporate prior knowledge efﬁcient exact k-best parsing algorithm guided belief system. model extension context-free grammars couples syntax semantics. generate sentence framework semantic statement ﬁrst drawn prior. grammar recursively constructs syntax tree top-down randomly selecting production rules distributions depend semantic statement. present particular incarnation grammar framework hierarchical dirichlet processes used select production rules randomly. application hdps setting novel requiring inference technique. term generative refer chomskian tradition generative grammar although approach fall broadly within framework. rather refers fact model posits probabilistic mechanism sentences generated performing probabilistic inference model yields parsing algorithm generative approach modeling grammar underscores duality language generation language understanding. grammar related synchronous cfgs extended perform semantic parsing however established scfgs describe generation syntactic semantic components sentences simultaneously makes assumption induced probability distributions semantic syntactic components factorize parallel manner. model instead describes generation semantic component step occurs prior syntactic component. captured scfgs prior semantic start symbol making factorization assumptions prior. particularly useful employing richer prior distributions semantics model context knowledge base. adaptor grammars provide framework jointly model syntactic structure sentences addition morphologies individual words unlike previous work adaptor grammars method couples syntax semantics guided background knowledge semantic prior. demonstrate leveraged framework model morphology individual verbs temporally-scoped relation extraction task. cohen blei smith show perform dependency grammar induction using adaptor grammars. grammar induction framework constitutes interesting research problem address work. parsing approaches equivalence drawn parsing problem problem ﬁnding shortest paths hypergraphs algorithm understood application search k-best paths large hypergraph. parser incorporates prior knowledge guide search ontology beliefs knowledge base. using kind approach parser biased context-appropriate interpretations otherwise ambiguous terse utterances. systems durrett klein nakashole mitchell moldovan salloum background knowledge semantic types different noun phrases improve ability perform entity linking co-reference resolution prepositional phrase attachment information extraction question answering systems ratinov roth durrett klein prokofyev link noun phrases wikipedia entries improve ability resolve co-references uses background knowledge remain fragmentary. krishnamurthy mitchell developed parser incorporates background knowledge knowledge base training distant supervision method able parsing. parser trained once applied variety settings different context semantic prior. core component statistical model dirichlet process understood distribution probability distributions. distribution drawn write characterized parameters concentration parameter base distribution useful property concentration parameter describes closeness base distribution typical number parameters drawn discrete distribution drawn dirichlet process. observations drawn using parameters another distribution written application deﬁne ﬁnite dirichlet distribution categorical distribution. marginalized model above resulting chinese restaurant process representation jnew max{z zi−} indicator table quantity number observations assigned table analogy imagine restaurant customers enter time. customer chooses table probability proportional number people currently sitting table table jnew probability proportional customer’s choice represented shown later sections representation amenable inference using markov chain monte carlo methods hierarchical dirichlet process extension dirichlet process hierarchical modeling advantage approach statistical strength shared across nodes belong subtree. every node ﬁxed tree associated distribution marginalizing dirichlet process yields chinese restaurant process marginalizing yields chinese restaurant franchise every node tree chinese restaurant consisting inﬁnite number tables. every table restaurant node assigned table parent restaurant. assignment variable index parent table table node assigned. parent node current number customers node sitting table mildly abusing notation here since refer number customers time drawn). draw observation start leaf node path section describe method performing posterior inference hdp. table assignment variables hdp. distributions conditionally conjugate application variables integrated closed form posterior intractable compute exactly approximate sampling. obtain samples performing collapsed gibbs sampling described section repeatedly sample conditional distribution integrated descendant observations table node yn−i observations zn−i table assignment variables. computing straightforward since follow chain table assignments root. root cluster assignment table node fact found advantageous performance keep track root cluster assignments every table hierarchy. thus must case number occupied tables node root node probability prior observe probabilities linear functions likelihoods various root cluster assignments implemented naively generating single sample equation take time linear number clusters root would result quadratic-time algorithm single gibbs iteration however exploit sparsity root cluster assignment likelihoods improve performance. dirichlet distribution categorical collapsed root cluster here rising factorial number elements value notice denominator depends sizes contents caching denominator values common sizes allow sampler avoid needless recomputation. especially useful application since many tables root tend small. similarly observe numerator factor values thus time required compute probability linear number unique elements improve scalability sampler. perform computations space avoid numerical overﬂow. computing probabilities paths. previous uses paths assumed ﬁxed. instance document modeling paths correspond documents predeﬁned categories documents. application however paths random. fact later show parser heavily relies posterior predictive distribution paths paths correspond semantic parses. precisely given collection training observations paths want compute probability path xnew given observation ynew here mxnew number occupied tables node xnew nxnew number customers sitting table node xnew nxnew total number customers node xnew. ﬁrst term computed since table exists assigned table parent node turn assigned table parent node follow chain table assignments root. second term observation assigned table whose assignment unknown marginalize assignment parent node parent node xnew. again probability ﬁrst term computed before probability second term depends assignment table unknown. thus since possible table created every level hierarchy root apply formula recursively. root probability prior probability ynew. tree small straightforward compute quantity equation every path xnew tree using method described above. application however size depends size ontology easily become large. case naïve approach becomes computationally infeasible. such develop algorithm incrementally best paths maximize quantity equation sparse distributions probability mass concentrated small number paths xnew algorithm effectively characterize predictive distribution equation algorithm essentially search nodes tree starting root descending nodes tree guided paths high probability. search state consists following ﬁelds current position search tree. array probability scores length nsamples. element array represents probability drawing observation ynew current node thus identical probability assigning ynew table child node s.n. useful compute quantity equation using recursive method described above. search outlined algorithm observe quantity equation independent functions linear combination terms tables available node table thus upper bound equation paths pass node result algorithm completed items guaranteed search found best paths. thus iterator data structure efﬁciently implemented using algorithm returns paths xnew order decreasing predictive probability ﬁrst item optimal. search algorithm modiﬁed representations extended case conjugate. also incorporated larger inference procedure jointly infer paths latent variables hdp. also straightforward compute predictive probabilities path xnew restricted subset paths important concern performing inference large trees feasible explicitly store every node memory. fortunately collapsed gibbs sampling require storing nodes whose descendants zero observations. addition algorithm augmented avoid storing nodes well. make observation node tree whose descendants observations zero occupied tables. therefore probability identical path xnew passes thus search reaches node simultaneously complete paths xnew pass avoid expanding nodes zero observations among descendants. result need explicitly store number nodes linear size training data enables practical inference large hierarchies. xnew) factorize since joint likelihood workaround representation joint likelihood factorizes direct assignment representation another approach approximate present generative model text sentences. model semantic statements generated probabilistically higher-order process. given semantic statement formal grammar selects text phrases concatenated form output sentence. present model remains ﬂexible regard semantic formalism. even though grammar viewed extension contextfree grammars important note model grammar conditionally context-free given semantic statement. otherwise semantic information marginalized grammar sensitive context. deﬁnition nonterminals terminals. production rules written form tuple context-free grammar couple syntax semantics augmenting production rules every production rule assign every right-hand side symbol surjective operation transforms semantic statements semantic statements associated symbol semantic statements associated symbol intuitively operation describes semantic statement passed child nonterminals generative process. parsing operations describe simpler semantic statements combine form larger statements enabling semantic compositionality. example suppose semantic statement has_color production rule pair semantic operation right-hand side reptilefrog selects subject argument. similarly pair semantic operation right-hand side identity operation. augmented production rule augmented rules parsing require computation inverse semantic operations preimage given semantic statement continuing example above returns contains statement has_color addition statements like eats_insect. complete deﬁnition grammar need specify method that given nonterminal semantic statement selects production rule rules left-hand side nonterminal accomplish this deﬁne selectax distribution rules left-hand side dependent later provide number example deﬁnitions selectax distribution. thus grammar framework fully speciﬁed tuple note semantic grammar formalisms framework. example categorical grammars lexicon describes mapping elementary components language syntactic category semantic meaning. rules inference available combine lexical items derivations eventually resulting syntactic semantic interpretation full sentence framework imagine process reverse. derivable semantic statements syntactic category generative process begins selecting statement next consider applications rules inference would yield unique application inference rule equivalent production rule framework. select production rules according generative process continue recursively. items lexicon equivalent preterminal production rules framework. thus generative process describes endow parses categorical grammar probability measure. used example extend earlier work generative models different choices select distribution induce different probability distributions parses. straightforward linear log-linear models full parses framework vector features computed full parse assumption that given semantic statement probability parse factorizes production rules used construct parse. however select distribution deﬁned using linear log-linear models describe section process generating sentences framework begins drawing semantic statement root nonterminal. thus prior distribution next syntax generated top-down starting draw production rule left-hand side selectsx. semantic transformation operations applied process repeated right-hand side nonterminals. concretely deﬁne following operation expand takes arguments symbol semantic statement yield operation concatenates strings single output string. then output sentence generated simply expand. depending application require generative process capitalizes ﬁrst letter output sentence and/or appends terminating punctuation end. noise model also appended generative process. algorithm easily extended also return full syntax tree. many possible choices select distribution. straightforward deﬁne categorical distribution available production rules simply draw selected rule distribution. result would simple extension probabilistic context-free grammars couples semantics syntax. however would remove dependence semantic statement production rule selection. illustrate importance dependence consider generating sentence semantic statement athlete_plays_sport using grammar ﬁgure start root nonterminal step select ﬁrst production rule apply semantic operation select_arg semantic statement obtain athleteroger_federer right-hand side nonterminal apply semantic operation delete_arg obtain athlete_plays_sport figure example grammar framework. grammar operates semantic statements form predicate. semantic operation select_arg returns ﬁrst argument semantic statement. likewise operation select_arg returns second argument. operation delete_arg removes ﬁrst argument identity returns semantic statement change. step finally expand nonterminal drawing selectn statement sporttennis. suppose tennis returned. concatenate returned strings form sentence andre agassi plays tennis. however consider generating another sentence grammar statement athlete_plays_sport. step process select distribution would necessarily depend semantic statement. english probability observing sentence form versus depends underlying semantic statement. capture dependence hdps deﬁne select distribution. every nonterminal associated order fully specify grammar need specify structure tree. tree associated nonterminal model ﬂexible trees deﬁned construct trees following method. first select discrete features integers. features operate semantic statements. example suppose restrict space semantic statements single predicate instances relations ontology assigned unique integer indices deﬁne semantic feature function simply returns index predicate given semantic statement. construct tree starting root child node every possible output repeat process recursively constructing complete tree depth example construct tree nonterminal example grammar ﬁgure suppose ontology predicates athlete_plays_sport musician_plays_instrument labeled respectively. ontology also contains concepts athleteroger_federer sporttennis sportswimming also labeled respectively. deﬁne ﬁrst feature return predicate index. second feature returns index concept second argument semantic statement. tree constructed starting root child node predicate ontology athlete_plays_sport musician_plays_instrument. next child node grandchild node every concept ontology athleteroger_federer sporttennis sportswimming. resulting tree depth root node child nodes child node grandchild nodes. construction enables select distribution nonterminal depend predicate second argument semantic statement. fully-speciﬁed hdps corresponding trees fully speciﬁed select. sampling selectax nonterminal semantic statement compute semantic features given semantic statement sequence indices speciﬁes path root tree many alternatives deﬁning select distribution. instance log-linear model used learn dependence features. provides statistical advantages smoothing learned distributions resulting model robust data sparsity issues. order describe inference framework must deﬁne additional concepts notation. nonterminal observe paths root leaves tree induce partition semantic statements precisely semantic statements belong equivalence class correspond path tree. figure example decomposition parse tree left outer parse inner parse right outer parse. example decomposition. instance similarly produce decomposition prepositional phrase inner parse verb inner parse. terminals omitted syntactic portion parse displayed consiseness. every parse consists semantic statement syntax tree syntax tree rooted tree containing interior vertex every nonterminal leaf every terminal. every vertex associated start position position sentence. interior vertex along immediate children corresponds particular production rule grammar interior vertex associated nonterminal children respectively correspond symbols left-to-right. thus every edge tree labeled semantic transformation operation. subgraph called inner syntax tree. corresponding outer syntax tree syntax tree deleted. draw distinction left right components outer syntax tree. deﬁne left outer syntax tree minimal subgraph containing subtrees positioned left containing ancestor vertices right outer syntax tree forms remainder outer parse decomposed three distinct trees ﬁgure illustration. note possible consists multiple disconnected trees. description parser frequently notation refer joint probability inference collection training sentences along corresponding syntax trees semantic statement labels xn}. given sentence ynew goal parsing compute probability semantic statement xnew syntax snew expression latent variables grammar. different applications rely probability different ways. example interested semantic parse maximizes probability. integral intractable compute exactly markov chain monte carlo approximate ﬁrst product nonterminals grammar. note probability equivalent probability drawing rule selectaxnew nonterminal semantic statement xnew. plugging selected minimize divergence posterior experimented number variational families found sufﬁciently expressive accurately approximate posterior purposes. second product iterates production rules constitute syntax snew. note applied approximation described equation semantic prior plays critically important role framework. prior dependence background knowledge parsing. although present setting training supervised syntax trees semantic labels straightforward apply model setting semantic labels syntax information missing. setting gibbs step added parser input sentence ﬁxed semantic statement returning distribution syntax trees sentence. inference paths given semantic statements incrementally within equation observe quantity depends associated nonterminal note exactly setting described section directly apply algorithm implement component. likely semantic syntactic parses {xnew snew} maximize given sentence ynew. describe component greater detail next section. component utilizes previous component. develop top-down parsing algorithm computes k-best semantic/syntactic parses maximize given sentence ynew. emphasize parser largely independent choice distribution select. algorithm searches space items called rule states rule state represents parser’s position within speciﬁc production rule grammar. complete rule states represent parser’s position completing parsing rule grammar. algorithm also works nonterminal structures represent completed parse nonterminal within grammar. parser keeps priority queue unvisited rule states called agenda. data structure called chart keeps intermediate results contiguous portions sentence. predeﬁned operations available algorithm. every iteration main loop algorithm pops rule state highest weight agenda adds chart applying available operation state using intermediate structures chart. operations additional rule states agenda priority given upper bound overall structure parser reminiscent earley parsing algorithm classical example current position production rule. dotted rule notation convenient represent variables rule example parser currently examining rule rule position write denotes current position parser. syntax partially completed syntax tree. example parser currently examining rule position tree root node labeled child subtrees labeled respectively. every complete rule state contains ﬁelds addition iterator ﬁeld keeps intermediate state inference method described section every nonterminal structure contains ﬁelds expansion takes incomplete rule state input. notational convenience r.rule written operation examines next right-hand symbol possible cases nonterminal every production rule grammar whose left-hand symbol every {r.i r.end} create rule state previously expanded given start position r∗.rule r∗.end r∗.i r∗.log_probability semantic statement ﬁeld state semantic statements expanded nonterminal r∗.semantics xbk. syntax tree rule state r∗.syntax initialized single root node. rule state added agenda operation analogous prediction step earley parsing. terminal read terminal sentence starting position create rule state where r∗.rule r.rule r∗.i |bk| r∗.log_probability r.log_probability r∗.semantics r.semantics. syntax tree identical syntax tree added child node corresponding terminal symbol rule state added agenda. operation analogous scanning step earley parsing. completion takes input incomplete rule state nonterminal structure n.nonterminal matches next right-hand nonterminal r.rule starting position nonterminal structure n.start matches current sentence position rule state r.i. notational convenience r.rule written bnfn. operation constructs rule state r∗.rule r.rule r∗.i n.end compute semantic statements rule state ﬁrst invert semantic statements nonterminal structure semantic transformation operation intersect resulting semantic statements incomplete rule state r∗.semantics r.semantics n.semantics}. syntax tree rule state r∗.syntax syntax tree incomplete rule state r.syntax added subtree nonterminal structure n.syntax. probability rule state input states r∗.log_probability r.log_probability n.log_probability. rule state added agenda. operation analogous completion step earley parsing. iteration takes input complete rule state completed parsing proa) equation determine paths order highest lowest posterior predictive probability using inference approach described section store current position list r.iterator. operation increments iterator adds rule state back agenda next operation creates nonterminal structure where n∗.nonterminal n∗.end r.end recall paths induce partition semantic statements path returned iterator corresponds subset semantic statements semantic statements nonterminal structure computed intersection subset semantic statements rule state n∗.semantics r.semantics. probability nonterminal structure n∗.log_probability probability path returned iterator r.log_probability. nonterminal structure added chart. algorithm started executing expansion operation production rules form root nonterminal starting position sentence semantics initialized possible semantic statements describe prioritization agenda items recall complete syntax tree decomposed inner left outer right outer portions observe probability full parse written product four terms semantic prior left outer probability right outer probability inner probability items agenda sorted upper bound probability entire parse. order compute this rely upper bound inner probability considers syntactic structure terminal ibkmkmk+ |bk| correct length terminal; otherwise ibkmkmk+ term maxx computed exactly using algorithm tight upper bound computed quickly terminating algorithm early using priority value given equation value computed efﬁciently using existing syntactic parsers time also compute upper bound probability outer portion syntax tree semantic prior. precise parses syntax inner syntax tree root begins sentence position ends then bound outer probability currently-considered rule r.rule r.end. note ﬁrst terms constitute upper bound inner probability nonterminal third term upper bound outer probability semantic prior. second term computed efﬁciently using dynamic programming. tighten adding term bounds probability rule items agenda prioritized quantity. long log_probability ﬁeld remains exact approach overall search yield exact outputs. syntactic parser naive computation equation highly infeasible would require enumerating possible outer parses. however rely fact search algorithm monotonic highest score agenda never increases algorithm progresses. prove monotonicity induction number iterations. given iteration inductive hypothesis parser visited reachable rule states priority strictly larger priority current rule state. show rule states added priority queue iteration must priority equal priority current rule state. consider operation expansion operation r.k. next right-hand side symbol terminal agenda item score agenda item since r∗.log_probability r.log_probability inner probability bounds equation cannot increase. nonterminal claim rule state created operation must priority priority agenda item. suppose contrary exists r.end} rule production rule syntax tree containing sibling subtree rooted parse implies existence rule state r∗.rule r∗.start r∗.end start positions vertex corresponding i|x∗ search which turn strictly greater priority thus priority strictly larger would imply nonterminal previously expanded start position position contradiction. rule state r∗.log_probability inner probability rule state r.log_probability bound maxj ibkij. thus priority rule state bounded priority rule state. therefore parser monotonic. consequence whenever algorithm ﬁrst expands nonterminal rule start position position sentence found left outer parse maximizes equation thereby computing obr.kij additional cost. similarly parser ﬁrst constructs nonterminal structure symbol start position position monotonicity guarantees nonterminal structure higher probability. exploit updating value iaij algorithm progresses incorporating semantic information values figure step-by-step example parser running sentence chopin plays using grammar similar shown ﬁgure top-left table lists semantic statements sorted probability drawing observation chopin associated nonterminal top-center top-right tables deﬁned similarly. figure example labeled data instance experiments. brevity omit semantic transformation operations syntax elements word boundaries irregular verb forms etc. experiments section evaluate parser’s ability parse semantic statements short sentences consisting subject noun simple verb phrase object noun. also evaluate ability incorporate background knowledge parsing semantic prior. used ontology knowledge base never-ending language learning system snapshot nell iteration containing concepts relation predicates beliefs relations nell typed domain range relation category ontology. compare parser state-of-the-art parser trained tested data. ﬁrst evaluate parser relation extraction task dataset subject-verbobject sentences. created dataset ﬁltering labeling sentences corpus triples extracted dependency parses clueweb dataset nell provides can_refer_to relation mapping noun phrases concepts nell ontology. created mapping verbs relations nell ontology. using mappings identify whether triple refer belief nell knowledge base. accepted sentences referred high-conﬁdence beliefs nell accepted sentences labeled referred beliefs. experiment restrict verbs present tense. yielded ﬁnal dataset three-word sentences along corresponding semantic statement nell spanning relations concepts. randomly split data training sentences test sentences. task parser makes predictions every test sentence mark correct output semantic statement exactly matches label. main difﬁculty task learn mapping relations sentence text. example dataset contains verbs ‘makes’ refer least nell relations including companyeconomicsector directordirectedmovie musicartistgenre. semantic types subject object concepts informative resolving ambiguity prior knowledge form belief system parsing. precision-recall curves ﬁgure generated sorting outputs parser posterior probability computed using output parses test sentence call semantic statement type-correct subject object concepts agree domain range instantiated relation nell ontology. experimented three prior settings parser uniform prior prior type-correct semantic statements prior probability larger units type-incorrect statements prior semantic statements correspond true beliefs prior probability larger type-incorrect statements type-correct correct statements probability larger type-incorrect statements. simple relation extraction task performs comparably parser uniform type-correct prior. fact parsers make almost identical predictions test sentences. differences precision-recall curves arise differences scoring predictions. primary source incorrect predictions noun test refers concept ontology refer concept training set. example sentence wilson plays guitar parsers predict wilson refers politician greg wilson. similarity performance parser uniform prior typecorrect prior suggests parser learns type-correctness training data. fact that grammar distribution verb depends jointly types arguments. prior parser outperforms demonstrating parser effectively incorporates background knowledge semantic prior improve precision recall. second experiment demonstrate parser’s ability extract semantic information morphology individual verbs operating characters instead preprocessed tokens. generated labeled dataset using process similar ﬁrst experiment. experiment restrict verbs present tense. dataset contains sentences spanning relations concepts. data randomly split training sentences test sentences. added simple temporal model semantic formalism sentences past tense refer semantic statements true past; sentences present tense refer presently true statements; sentences future tense refer statements true future. thus task becomes temporally-scoped relation extraction. simple verb morphology model incorporated grammar. verb modeled concatenated root afﬁx. grammar random selection production rule captures selection verb tense. afﬁx selected deterministically according desired tense grammatical person subject. posterior probability parse estimated using parses test example. results shown ﬁgure figure precision-recall curves standard relation extraction task. compare approach three different settings prior uniform prior places equal probability mass semantic statements. type-correct prior places higher mass semantic statements subject object types agree domain range relation predicate. knowledge base prior similar type-correct prior except additionally places higher probability mass semantic statements correspond true beliefs nell knowledge base table sample randomly selected parses simple relation extraction task using uniform prior sample sentences parse outputs displayed along probabilities. recall parser operates sets semantic statements outputs contain wildcards. evident output right phrase kidneys appear training highest-ranked parse ambiguous. figure precision-recall curves temporally-scoped relation extraction task. compare approach three different settings prior solid lines denote parser’s performance using grammar models morphology verbs whereas dashed lines produced parser trained grammar model verb morphology. uniform prior places equal probability mass semantic statements. type-correct prior places higher mass semantic statements subject object types agree domain range relation predicate. knowledge base prior similar type-correct prior except additionally places higher probability mass semantic statements correspond true beliefs nell knowledge base temporally-scoped relation extraction task parser demonstrates better generalization verb forms. parser performs better trained grammar models verb morphology trained simpler grammar consider morphology verbs parser able accomplish ability model semantics morphology individual words. ﬁrst experiment performance parser improves knowledge base prior supporting observation parser effectively leverage background knowledge improve accuracy. again difference performance using uniform prior type-correct prior. recall parser operates sets semantic statements opposed individual statements. thus possible parsing completes output non-singleton semantic statements share highest probability parse. ﬁrst experiments counted outputs non-parse fairly compare ccg. however evaluate quality ambiguous outputs. ﬁgure perform simple relation extraction task modiﬁcation measure correctness parser’s output whether ground truth semantic statement contained within semantic statements share highest probability parse. produce output sentences contain tokens appear training data. sense evaluation measures out-of-vocabulary performance. although precision high in-vocabulary test recall much figure precision recall simple relation extraction task including unambiguous outputs subset sentences provide output. compare approach three different settings prior uniform prior places equal probability mass semantic statements. type-correct prior places higher mass semantic statements subject object types agree domain range relation predicate. knowledge base prior similar type-correct prior except additionally places higher probability mass semantic statements correspond true beliefs nell knowledge base parser search algorithm terminate found k-best semantic parses. useful evaluating conﬁdence parser output estimating posterior probability parse. examine behavior parser function ﬁgure timing results demonstrate parser scale large knowledge bases maintaining efﬁciency parsing. figure area precision-recall curve average parse time versus simple relation extraction task uniform prior. dark green curve measures area precision-recall curve without considering ambiguous outputs whereas light green curve measures area precision-recall curve taking account ambiguous outputs recall that order produce precision-recall curves parser’s output sort outputs conﬁdence scores ﬁgure area precision-recall curve converges quickly fairly small values indicating relative ordering parse outputs converges quickly. found behavior consistent choices prior distributions complex temporally-scoped relation extraction task. values smaller parser provides outputs quickly. fact that parser performs search quickly semantic statements size roughly wildcard denotes concept ontology). parser require additional time search beyond initial ambiguous output. note threshold value identical number concepts ontology second threshold likely related product number concepts number relations ontology earlier experiments used dataset contained sentences refer beliefs nell knowledge base. order inspect performance parser sentences refer nell beliefs create dataset. start sentence corpus modify ﬁltering process accept sentences contain noun phrases also exist ﬁrst dataset contain verb phrases exist hand-constructed verb-relation used create ﬁrst dataset cannot refer nell belief according can_refer_to instances verb-relation map. precisely every sentence can_refer_to relation table precision recall parsers evaluated out-of-knowledge base dataset. uniform prior places equal probability mass semantic statements. type-correct prior places higher mass semantic statements subject object types agree domain range relation predicate. knowledge base prior similar type-correct prior except additionally places higher probability mass semantic statements correspond true beliefs nell knowledge base maps noun phrase possible referent concepts; relation-verb provides possible referent relations; cartesian product provides possible referent semantic statements. discard sentences referent semantic statements contains nell belief. sorted resulting sentence list frequency labeled hand. frequent sentences ﬁltered labeled since sentences referred concepts outside ontology verbs referred unrecognized relations dataset referred out-of-knowledge base dataset since sentences refer beliefs outside knowledge base. selected sentences dataset training sentences. trained parsers sentences addition entirety ﬁrst dataset. tested parsers remaining sentences. table displays performance results parser out-ofknowledge base dataset simple relation extraction task well temporally-scoped task. parser trained simple grammar simple relation extraction task grammar models verb morphology temporally-scoped relation extraction task. expected informative priors uniformly improve parsing performance evaluation. interestingly parser behaves conservatively incorporating stronger priors outputting smaller conﬁdent responses results higher precision reduced recall. parser indeed capable extracting correct semantic statements sentences refer beliefs outside knowledge base informative priors obviously hurt performance. article presented generative model sentences semantic parsing extending formalism couple semantics syntax. generative process semantic statement ﬁrst generated example knowledge base. next tree constructed top-down using recursive procedure production rules selected randomly possibly depending features semantic statement. semantic transformation operations specify decompose semantic statement order continue recursion. presented particular construction production rules selected using hdp. applied mcmc perform inference model constructed chart-driven agenda parser. application distinct previous uses since construction path indicator observation assumed ﬁxed. evaluate parser dataset sentences labeled semantic statements nell. results demonstrate parser incorporate prior knowledge knowledge base semantic prior. informative prior parser outperforms state-of-the-art parser. addition demonstrate model used jointly model morphology individual verbs leading improved generalization verbs temporally-scoped relation extraction task. results indicate framework scale knowledge bases nell millions beliefs extended complex grammars richer semantic formalisms without sacriﬁcing exact inference principled nature model. interesting parallel drawn inference problem problem ﬁnding shortest paths hypergraphs. similar parallels drawn parsers since approach top-down speciﬁcation hypergraph involved. imagine hypergraph containing vertex every semantic statement vertex every intermediate rule state vertices every nonterminal hyperedge graph every allowable operation parser. hyperedge generalization edge head tail sets vertices. then problem parsing equivalently stated ﬁnding shortest path sets vertices source vertices representing incomplete nonterminal elements destination vertex complete nonterminal. gallo longo pallottino klein manning deﬁnitions details. algorithm understood application search k-best paths hypergraph. monotonicity property algorithm consequence dijkstra’s theorem generalized hypergraphs also suggests parser improved utilizing tighter heuristic. parser prior contribution rather loosely incorporated objective fact assumed nothing structure semantic prior. however algorithm could potentially made efﬁcient could factorize prior example nonterminals rules grammar. could provide additive term equation presented inference approach combination search algorithms inference component joint syntactic-semantic parser. however possible merge searches single search potentially improving overall efﬁciency. article showed utilize hdps dependence semantic features probabilistic selection production rules generative process. would interesting explore application dependent dirichlet processes random probability measures possibly means induct grammar framework. another highly promising avenue research explore complex prior structures. instance generative model knowledge base could composed framework present here. would result parser would learn beliefs reads text. another direction model generation sequence sentences complex relationships concepts bridge across multiple sentences. approach would likely contain model context shared across sentences. example generative process would generate representation document context using background knowledge base. then semantic statements sentence document generated intermediate document-level representation addition sources document-level information. finally grammar would generate sentences semantic statement. problem co-reference resolution also becomes apparent settings complex sentences. single-sentence multiplesentence settings co-reference resolution integrated parsing. incorporating extensions richer uniﬁed parsing framework would promising. thank emmanouil platanios jayant krishnamurthy insightful discussion helpful comments draft. also thank anonymous reviewers feedback shortened version manuscript. work supported part grant part darpa deft contract references adams beverly colwell laura bell charles perfetti. trading relationship reading skill domain knowledge children’s text comprehension. discourse processes anderson richard david pearson. schema-theoretic view basic processes reading comprehension. university illinois urbana-champaign champaign ill. bolt beranek newman inc. callan jamie mark changkuk zhao. clueweb dataset. chomsky editor. syntactic structures. mouton hague. chomsky noam. three models description language. transactions joint parsing named entity recognition non-jointly labeled data. hajic sandra carberry stephen clark editors pages association computer linguistics. combinatory categorial grammar. proceedings annual meeting association computational linguistics pages baltimore maryland june. association computational linguistics. enriched synchronous context-free grammar. lluís màrquez chris callison-burch jian daniele pighin yuval marton editors emnlp pages association computational linguistics. merlo paola gabriele musillo. semantic parsing high-precision semantic role labelling. alexander clark kristina toutanova editors conll pages acl. mitchell cohen hruscha talukdar betteridge carlson dalvi gardner kisiel krishnamurthy mazaitis mohammad nakashole platanios ritter samadi settles wang wijaya gupta chen saparov greaves welling. never-ending learning. aaai. never-ending learning aaai-. coreference resolution. proceedings main conference human language technology conference north american chapter association computational linguistics pages philippe cudrãl’-mauroux. sanaphor ontology-based coreference resolution. marcelo arenas ã¸sscar corcho elena simperl markus strohmaier mathieu d’aquin kavitha srinivas paul groth michel dumontier jeff heﬂin krishnaprasad thirunarayan steffen staab editors international semantic conference volume lecture notes computer science pages springer.", "year": 2016}