{"title": "Multi-source Transfer Learning with Convolutional Neural Networks for  Lung Pattern Analysis", "tag": ["cs.CV", "stat.ML"], "abstract": "Early diagnosis of interstitial lung diseases is crucial for their treatment, but even experienced physicians find it difficult, as their clinical manifestations are similar. In order to assist with the diagnosis, computer-aided diagnosis (CAD) systems have been developed. These commonly rely on a fixed scale classifier that scans CT images, recognizes textural lung patterns and generates a map of pathologies. In a previous study, we proposed a method for classifying lung tissue patterns using a deep convolutional neural network (CNN), with an architecture designed for the specific problem. In this study, we present an improved method for training the proposed network by transferring knowledge from the similar domain of general texture classification. Six publicly available texture databases are used to pretrain networks with the proposed architecture, which are then fine-tuned on the lung tissue data. The resulting CNNs are combined in an ensemble and their fused knowledge is compressed back to a network with the original architecture. The proposed approach resulted in an absolute increase of about 2% in the performance of the proposed CNN. The results demonstrate the potential of transfer learning in the field of medical image analysis, indicate the textural nature of the problem and show that the method used for training a network can be as important as designing its architecture.", "text": "capture carry oxygen bloodstream eventually causes permanent loss ability breathe. early diagnosis diseases crucial making treatment decisions misdiagnosis lead life-threatening complications although ilds histologically heterogeneous mostly similar clinical manifestations differential diagnosis challenging even experienced physicians. high resolution computed tomography considered appropriate protocol screening ilds speciﬁc radiation attenuation properties lung tissue. scans interpreted assessing extent distribution existing pathologies lung. however inherent difﬁculty problem large quantity radiological data radiologists scrutinize result diagnostic accuracy high interintra-observer variability great ambiguity radiological assessment often leads additional histological biopsies increase risk cost patients. order assist radiologist diagnosis avoid biopsies research done towards computer-aided diagnosis systems. basic module systems often ﬁxed scale texture classiﬁcation scheme detects various patterns scan outputs pathologies later used reach ﬁnal diagnosis. great variety image descriptors classiﬁers proposed recognizing lung patterns. deep learning techniques especially convolutional neural networks attracted much attention impressive results imagenet large scale visual recognition competition numerous studies followed transformed state many computer vision applications. even though cnns existed couple decades already breakthrough made possible thanks current processing capabilities large image databases available. potential deep learning medical image analysis already investigated initial results promising however adaptation existing deep learning tools domain natural color images medical images brings challenges. firstly medical imaging data much difﬁcult acquire compared general imagery freely available internet. that annotation performed multiple specialists ensure validity whereas natural image recognition anyone could serve annotator. lack data makes training medical abstract—early diagnosis interstitial lung diseases crucial treatment even experienced physicians difﬁcult clinical manifestations similar. order assist diagnosis computer-aided diagnosis systems developed. commonly rely ﬁxed scale classiﬁer scans images recognizes textural lung patterns generates pathologies. previous study proposed method classifying lung tissue patterns using deep convolutional neural network architecture designed speciﬁc problem. study present improved method training proposed network transferring knowledge similar domain general texture classiﬁcation. publicly available texture databases used pretrain networks proposed architecture ﬁne-tuned lung tissue data. resulting cnns combined ensemble fused knowledge compressed back network original architecture. proposed approach resulted absolute increase performance proposed cnn. results demonstrate potential transfer learning ﬁeld medical image analysis indicate textural nature problem show method used training network important designing architecture. index terms—interstitial lung diseases convolutional neural networks texture classiﬁcation model ensemble transfer learning knowledge distillation model compression research carried within framework intact research project supported bern university hospitalinselspital swiss national science foundation grant christodoulidis anthimopoulos contributed equally work. asterisk indicates corresponding author. anthimopoulos artorg center biomedical engineering research university bern bern switzerland department diagnostic interventional pediatric radiology bern university hospital inselspital bern switzerland also department emergency medicine bern university hospital inselspital bern switzerland ebner christe department diagnostic interventional pediatric radiology bern university hospital inselspital bern switzerland mougiakakou* department diagnostic interventional pediatric radiology bern university hospital inselspital bern switzerland also artorg center biomedical engineering research university bern bern switzerland images difﬁcult even impossible many huge networks proposed computer vision. common overcome problem pretrain networks large color image databases like imagenet ﬁne-tune medical imaging data method often referred transfer learning. approach yielded adequately good results many applications demonstrated effectiveness transfer learning rather different image classiﬁcation tasks secondly architecture popular cnns ﬁeld computer vision generally suboptimal problems encountered medical imaging texture analysis input size ﬁxed often suitable. proposed novel achieved signiﬁcant improvement respect state art. network’s architecture especially designed extract textural characteristics patterns much smaller size allowed successfully trained solely medical data without transfer learning. study propose novel training approach improves performance newly introduced additionally exploiting relevant knowledge transferred multiple general texture databases. typical pattern classiﬁcation scheme takes input local region interest volume interest depending available imaging modality mainly characterized chosen feature classiﬁcation method. ﬁrst proposed systems used handcrafted texture features ﬁrst order statistics gray level cooccurrence matrices run-length matrices fractal analysis systems utilized ﬁlter banks morphological operations wavelet transformations local binary patterns moreover ability multiple detector computed tomography scanners provide threedimensional data motivated researchers expand existing texture descriptors three dimensions recently researchers proposed feature sets learned data able adapt given problem. methods rely unsupervised techniques features sparse representation models restricted boltzmann machines also used learn multi-scale ﬁlters responses used features. feature vector calculated classiﬁer trained discriminate patterns. many different approaches proposed classiﬁcation including linear discriminant analysis bayesian classiﬁers k-nearest neighbors fully-connected artiﬁcial neural networks random forests support vector machines attempts recently made deep learning techniques especially cnns classiﬁcation lung tissue patterns. unlike aforementioned feature learning methods cnns learn features supervised manner train classiﬁer time minimizing cost function. although term deep learning refers multiple learning layers ﬁrst studies problem utilized rather shallow architectures. modiﬁed proposed resembles performs feature extraction classiﬁcation. hidden nodes share weights densely connected output nodes whole network trained supervised manner. convolutional layer three fully-connected layers used rather shallow architecture network unable capture complex non-linear image characteristics. previous work designed trained ﬁrst time deep problem lung tissue classiﬁcation outperformed shallower networks. proposed consists convolutional layers kernels leakyrelu activations followed global average pooling three fully-connected layers. studies used popular deep cnns exploit discriminative power gained pretraining huge natural image datasets although architecture networks optimal lung tissue classiﬁcation managed achieve relatively good results transferring knowledge tasks. transfer learning generally deﬁned ability system utilize knowledge learned task another task shares common characteristics. formal deﬁnitions survey transfer learning found study focus supervised transfer learning cnns. deep cnns shown remarkable abilities transferring knowledge apparently different image classiﬁcation tasks even imaging modalities task. cases done weight transferring. network pretrained source task weights layers transferred second network used another task. cases activations second network used off-the-shelf features classiﬁer cases nontransferred weights network randomly initialized second training phase follows time target task training transferred weights could kept frozen initial values trained together random weights process usually called ﬁne-tuning. target dataset capacity network ﬁne-tuning result overﬁtting features often left frozen. finding many layers transfer depends proximity tasks also proximity corresponding imaging modalities. shown last layers network task speciﬁc earlier layers network modality speciﬁc hand overﬁtting issues best strategy transfer ﬁnetune every layer discovered features adapted target task keeping useful common knowledge. another type transfer learning multitask learning approach trains multiple related tasks simultaneously using shared representation process increase performance tasks typically applied training data tasks limited. transfer learning extensively studied past years especially ﬁeld computer vision several interesting ﬁndings. pretrained cnns vgg-net alexnet used extract offthe-shelf features image search classiﬁcation. authors demonstrate fusing features extracted multiple layers improves performance different benchmark databases. factors inﬂuence transferability knowledge ﬁne-tuning framework investigated. factors include network’s architecture resemblance source target tasks training framework. similar study effects different ﬁne-tuning procedures transferability knowledge investigated procedure proposed quantify generality speciﬁcity particular layer. number studies also utilized transfer learning techniques order adapt well-known networks classify medical images. cases network used alexnet googlenet pretrained imagenet however networks designed ﬁxed input size usually images resized channels artiﬁcially extended three network. procedure inefﬁcient also impair descriptive ability network. section present method transferring knowledge multiple source databases ultimately used pattern classiﬁcation. prior this describe databases utilized purposes study well architecture newly proposed order provide better foundation description methodology. amsterdam library textures describable textures dataset flickr material database kylberg texture database kth-tipsb ponce research group’s texture database moreover concatenation aforementioned databases also used. target domain used databases scans swiss university hospitals multimedia database university hospital geneva bern university hospital inselspital database source domain databases publicly available texture classiﬁcation benchmarks. class corresponds speciﬁc texture represented pictures instances texture. databases alot kth-tips-b also contain multiple pictures instance different angles illumination scales. image size ﬁxed databases apart also provides texture masks. creation training-validation dataset color databases converted gray-scale non-overlapping patches extracted size equal input proposed namely provided partitioning training validation sets performed instance level except alot number instance equal number classes. testing created source domain databases since ultimate goal test system target domain. case training validation test sets provided test added training set. table summarizes characteristics original source databases corresponding patch datasets. target domain dataset database consists hrct scans different cases pixels slice average slices case. average pixel spacing slice thickness -mm. manual annotations different lung patterns also provided along clinical parameters patients histologically proven diagnoses ilds. insel database consists hrct scans cases resolution average slices case. average pixel spacing slice thickness -mm. number preprocessing steps applied scans creating ﬁnal patch dataset. axial slices rescaled match certain xy-spacing value rescaling applied z-axis. image intensity values cropped within window hounsﬁeld units mapped experienced radiologists bern university hospital annotated databases manually drawing polygons around seven different patterns including healthy tissue relevant patterns namely ground glass reticulation consolidation micronodules honeycombing combination ground glass reticulation. total ground truth polygons annotated non-overlapping image patches size extracted unequally distributed across classes. patches entirely included lung ﬁeld overlap corresponding ground truth polygons least patch dataset patches randomly chosen class validation test set. remaining patches used training artiﬁcially augmented increase amount training data prevent over-ﬁtting. label-preserving transformations applied rotation well combinations two. total transformations used duplicates also added classes samples. ﬁnal number training samples constrained rarest class condition equal class representation training patches class. total training consists patches order minimize parameters involved focus aspects transfer learning used architecture proposed throughout different steps method. input network image patch pixels. patch convolved subsequent convolutional layers kernels number kernels proportional receptive ﬁeld layer respect input. number kernels used layer parameter depends complexity input data chosen output ﬁnal convolutional layer globally pooled thus passing average value feature series three dense layers. rectiﬁed linear unit used activation function dense layers convolutional layers employ leaky relu activations finally dropout used dense layer dropping units. training network adam optimizer used default values hyperparameters. training ends none consecutive epochs improves network’s performance validation least task namely classiﬁcation. starting ﬁrst layer number consecutive layers transferred pretrained network initialize counterpart network. rest network randomly initialized last layer changes size match number classes target dataset transferred layers ﬁnetuned along training randomly initialized ones. decided ﬁne-tune layers instead freezing since proposed network relatively small previously trained target dataset without overﬁtting according weight freezing used avoid overﬁtting problems. order investigate effects transferring different number layers performed experiments source datasets. knowledge fusion ensemble ensembles systems multiple predictors statistically independent extent order attain aggregated prediction. using ensembles achieve better performance wellestablished technique successfully exploited many applications systems usually perform better predictors alone also gain stability. performance gain arises fact different prediction models form ensemble capture different characteristics function approximated. order build strong ensemble instead manually selecting models implemented ensemble selection approach similar presented employed algorithm forward selection procedure selects models pool iteratively adds ensemble following speciﬁc criterion. moreover additions prevent over-ﬁtting also implemented. pool algorithm selects models includes networks pretrained source datasets ﬁne-tuned dataset snapshots networks training well randomly initialized networks trained scratch target data. creating model pool subset randomly sampled half size. fig. multi-source transfer learning knowledge transferred source database different cnn. selection process combines cnns ensemble used teach single randomly initialized model. source datasets presented section iii-a demonstrate wide spectrum different characteristics shown fig. table hence expect also contribute range diverse complementary features. assumption holds parallel transfer learning datasets model improve performance individual dataset would. however standard transfer learning approach transferring weights utilize source dataset. tackle problem transfer knowledge source different fuse ensemble expected performance superior individual models also larger computational complexity. transfer fused knowledge back network original architecture order reduce complexity keeping desirable performance. simple weight transferring possible here since requires models architecture. therefore model compression technique transfers knowledge arbitrary models task. fig. depicts full pipeline proposed multi-source transfer learning method next paragraphs describe three basic components detail. experiments presented section trainvalidation-test scheme utilized. presented results calculated test selection hyperparameters best resulting models made validation set. rest section describe chosen evaluation protocol implementation details. evaluation principle evaluation metric used average f-score different classes increased sensitivity imbalances among classes. f-score calculated follows then models subset ranked performance best models chosen initialize ensemble. rest subset’s models increases ensemble performance most continue adding models gain achieved. model selection performed replacement meaning model included once. whole procedure repeated subsets generating ensembles aggregated averaging outputs. selection models based average fscore validation involved parameters tested grid position parameter grid selection repeated times ﬁnally best ensemble found model compression model compression used ﬁnal step compress knowledge huge ensemble created previous procedure single network original architecture. model compression also known knowledge distillation procedure training model using soft targets produced another usually complex model soft targets class probabilities produced complex model logits namely output model softmax activation. model produces soft targets often called teacher model knowledge distilled plays role student. soft targets carry additional knowledge discovered teacher regarding resemblance every sample classes. procedure considered another type knowledge transfer performed task different models. case ensemble employed teacher student single randomly initialized original architecture described section iii-b. trained soft targets student model approximate behavior ensemble model even learn make similar mistakes. however mistakes student would probably made training hard targets considering relatively inferior capacity. another fuse knowledge multiple sources multiple models. study used baseline method. method simultaneously trains models tasks weights shared among models. implementation train seven networks source datasets target dataset. cnns share weights apart last layer size depends number classes particular task. parallel training achieved alternating every epoch task target source tasks. words epochs train target task even epochs train source tasks sequential manner. although fuses knowledge involved tasks tasks exclusively source target like standard transfer learning approach. since ﬁnal goal improve performance target task implementation proposed method implemented python using keras framework theano back-end. experiments performed linux machine intel core .ghz nvidia geforce titan ram. section present results performed experiments grouped according three basic components system presented section iii-c. finally analyze performance proposed network compare methods. single-source transfer learning ﬁrst series experiments investigate performance gain transferring knowledge individual source datasets target task i.e. classiﬁcation patterns. model pretrained source datasets ﬁnetuned data. seventh source dataset added consists datasets merged one. described section iii-b proposed network convolutional three dense layers. starting ﬁrst transfer seven layers pretrained networks. rest layers randomly initialized entire ﬁnetuned task. different random initializations result deviations results minimize effect repeated experiment three times reported mean values. results experiment depicted fig. region light gray background denotes convolutional layers rest denote ﬁrst dense layers. horizontal dashed line represents performance network trained scratch best results achieved layers transferred pretrained dataset. however optimal weight transferring strategy inferred every pretrained network relative different behavior. additional line average performance source datasets also shown. according line contribution weight transferring increases average transferring least four layers. weight transferring seems help even transferring layers. probably ability ﬁne-tuning adapt even task-speciﬁc features target task observation inline conclusions runtime experiments could expect faster training pretrained network since initial state closer good solution randomly initialized network. indeed average number epochs pretrained instead random epoch taking seconds. however difference small statistically non-signiﬁcant probably fact loss drops lower rate approaching training starting point signiﬁcantly affect number required epochs. random initializations pretraining ﬁne-tuning well different source datasets introduce significant variance network’s results. unstable behavior single-source transfer learning combined assumption reduced correlation among resulting models motivated build ensemble model fuse extracted knowledge reduce aforementioned variance. knowledge fusion ensemble fig. also illustrates performance ensemble built described section iii-c. ensemble clearly outperforms rest models reducing variance transferring multi-source knowledge time. order investigate contribution ensemble averaging alone also built ensemble pool fig. f-score produced transferring knowledge single source domains different number transferred layers averaged three experiments. horizontal lines correspond without knowledge transfer different ensembles cnns. randomly initialized models. output ensemble reached performance better single randomly initialized still inferior multisource ensemble. addition used bootstrap aggregating boost performance even reducing correlation models. trained ensemble samples randomly sampled training replacement. performance slightly improved reaching however still inferior proposed ensemble. results showed although ensemble increase accuracy stochastic models transferred knowledge also contributes ﬁnal result. model compression last part ensemble employed teacher producing soft targets training dataset used train cnns. experimented number different choices student networks choosing pretrained ﬁnetuned networks previous steps well randomly initialized ones. different students reached similar levels performance ﬁnally chose student random initialization simplicity. achieved performance teaching chosen student test set. result lies ensemble’s performance previously presented results. performance comparison previous work baseline method comparison multi-source transfer learning used approach described section iii-d. performance task training along tasks reached value ﬁnetuning step performance reached value much better network trained scratch similar number single source pretrained networks. results could limited capacity network attempts solve multiple problems time. modiﬁcations scheme weighting contributions different tasks sharing different parts network could yield better results however would increase complexity scheme would require large number experiments different strategies. table provides comparison methods literature. ﬁrst three rows correspond methods hand crafted features range different classiﬁers. rest correspond methods utilize cnns. results reproduced authors implementing different methods using data framework test them. proposed multi-source transfer learning technique improved performance proposed network absolute compared previous performance finally fig. shows confusion matrix proposed approach. shown confusion basically ﬁbrotic classes expected. also notice matrix balanced presented paper presented training method improves accuracy stability task lung tissue pattern classiﬁcation. performance gain achieved multiple transfer knowledge general texture databases. network pretrained source databases ﬁne-tuned target database transferring different numbers layers. networks obtained combined ensemble using model selection process employed teach network original size. resulting achieved gain performance compared network trained hard targets. result proves potential transfer learning natural medical images could beneﬁcial many applications limited available medical data and/or annotations. believe challenging datasets additional classes and/or higher diversity beneﬁt even similar approaches. considering even experienced radiologists would achieve perfect classiﬁcation especially patch level reported performances could reached peak. finally reported increase accuracy comes expense increased training time since multiple models trained. however inference time still exactly additional training time required considered fair compromise improving performance cases data shortage. future research plans topic include ensemble teacher labeling unlabeled samples augment training student model. approach could partially assist common problem limited annotated data ﬁeld medical image analysis. greenspan ginneken summers guest editorial deep learning medical imaging overview future promise exciting technique ieee transactions medical imaging vol. anthimopoulos christodoulidis ebner christe mougiakakou lung pattern classiﬁcation interstitial lung diseases using deep convolutional neural network ieee transactions medical imaging vol. analysis using isotropic wavelet frames information technology biomedicine ieee transactions vol. tulder bruijne combining generative discriminative representation learning lung analysis convolutional restricted boltzmann machines ieee transactions medical imaging vol. h.-c. shin roth nogues mollura summers deep convolutional neural networks computer-aided detection architectures dataset characteristics transfer learning ieee transactions medical imaging vol. razavian azizpour sullivan carlsson features off-the-shelf astounding baseline recognition proceedings ieee conference computer vision pattern recognition workshops ser. cvprw washington ieee computer society castrejon aytar vondrick pirsiavash torralba learning aligned cross-modal representations weakly aligned data ieee conference computer vision pattern recognition june azizpour razavian sullivan maki carlsson from generic speciﬁc deep representations visual recognition ieee conference computer vision pattern recognition workshops june tajbakhsh shin gurudu hurst kendall gotway liang convolutional neural networks medical image analysis full training tuning? ieee transactions medical imaging vol. depeursinge vargas platon geissbuhler p.-a. poletti m¨uller building reference multimedia database interstitial lung diseases computerized medical imaging graphics vol. chollet keras https//github.com/fchollet/keras theano development team theano python framework fast computation mathematical expressions arxiv e-prints vol. abs/. kylberg kylberg texture dataset centre image analysis swedish university agricultural sciences uppsala university uppsala sweden external report september uppaluri hoffman sonka hartley hunninghake mclennan computer recognition regional lung disease patterns american journal respiratory critical care medicine vol. anthimopoulos christodoulidis christe mougiakakou classiﬁcation interstitial lung disease patterns using local features random forest engineering medicine biology society annual international conference ieee. ieee uchiyama katsuragawa shiraishi c.-t. zhang suzuki quantitative computerized analysis diffuse lung disease high-resolution computed tomography medical physics vol. sowmya multiple kernel learning classiﬁcation diffuse lung disease using hrct lung images engineering medicine biology society annual international conference ieee. korﬁatis karahaliou kazantzi kalogeropoulou costaridou texture-based identiﬁcation characterization interstitial pneumonia patterns lung multidetector information technology biomedicine ieee transactions vol. depeursinge chin leung rubin muller unser optimized steerable wavelets texture analysis lung tissue classiﬁcation usual interstitial pneumonia biomedical imaging ieee international symposium gangeh sørensen shaker kamel bruijne loog texton-based approach classiﬁcation lung parenchyma images medical image computing computerassisted intervention–miccai springer foncubierta-rodr´ıguez depeursinge m¨uller using multiscale visual words lung texture classiﬁcation retrieval medical content-based retrieval clinical decision support. springer zhao hirano tachibana kido classiﬁcation diffuse lung diseases patterns sparse representation based method hrct images engineering medicine biology society annual international conference ieee. ieee sowmya multiscale sparse representation highresolution computed tomography lung images diffuse lung disease classiﬁcation image processing ieee international conference", "year": 2016}