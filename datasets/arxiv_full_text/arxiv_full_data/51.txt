{"title": "Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for  Visual Question Answering", "tag": ["cs.CV", "cs.AI", "cs.CL", "cs.NE"], "abstract": "We address the problem of Visual Question Answering (VQA), which requires joint image and language understanding to answer a question about a given photograph. Recent approaches have applied deep image captioning methods based on convolutional-recurrent networks to this problem, but have failed to model spatial inference. To remedy this, we propose a model we call the Spatial Memory Network and apply it to the VQA task. Memory networks are recurrent neural networks with an explicit attention mechanism that selects certain parts of the information stored in memory. Our Spatial Memory Network stores neuron activations from different spatial regions of the image in its memory, and uses the question to choose relevant regions for computing the answer, a process of which constitutes a single \"hop\" in the network. We propose a novel spatial attention architecture that aligns words with image patches in the first hop, and obtain improved results by adding a second attention hop which considers the whole question to choose visual evidence based on the results of the first hop. To better understand the inference process learned by the network, we design synthetic questions that specifically require spatial inference and visualize the attention weights. We evaluate our model on two published visual question answering datasets, DAQUAR [1] and VQA [2], and obtain improved results compared to a strong deep baseline model (iBOWIMG) which concatenates image and question features to predict the answer [3].", "text": "abstract. address problem visual question answering requires joint image language understanding answer question given photograph. recent approaches applied deep image captioning methods based convolutional-recurrent networks problem failed model spatial inference. remedy this propose model call spatial memory network apply task. memory networks recurrent neural networks explicit attention mechanism selects certain parts information stored memory. spatial memory network stores neuron activations diﬀerent spatial regions image memory uses question choose relevant regions computing answer process constitutes single network. propose novel spatial attention architecture aligns words image patches ﬁrst obtain improved results adding second attention considers whole question choose visual evidence based results ﬁrst hop. better understand inference process learned network design synthetic questions specifically require spatial inference visualize attention weights. evaluate model published visual question answering datasets daquar obtain improved results compared strong deep baseline model concatenates image question features predict answer visual question answering emerging interdisciplinary research problem intersection computer vision natural language processing artiﬁcial intelligence. many real-life applications automatic querying surveillance video assisting visually impaired compared recently popular image captioning task requires deeper understanding image considerably easier evaluate. also puts focus artiﬁcial intelligence namely inference process needed produce answer visual question. fig. propose spatial memory network answers questions images using spatial inference. ﬁgure shows inference process two-hop model examples dataset ﬁrst attention process captures correspondence individual words question image regions. high attention regions marked bounding boxes corresponding words highlighted using color. second ﬁne-grained evidence gathered ﬁrst well embedding entire question used collect exact evidence predict answer. early works seen turing test proxy. authors propose approach based handcrafted features using semantic parse question scene analysis image combined latent-world bayesian framework. recently several end-to-end deep neural networks learn features directly data applied problem directly adapted captioning models utilize recurrent lstm network takes question convolutional neural image features input outputs answer. though deep learning methods shown great improvement compared handcrafted feature method drawbacks. models based lstm reading question image features show clear improvement compared lstm reading question furthermore rather complicated lstm models obtain similar worse accuracy baseline model concatenates features bag-of-words question embedding predict answer img+bow model ibowimg model major drawback existing models explicit notion object position support computation intermediate results based spatial attention. intuition answering visual questions often involves looking diﬀerent spatial regions comparing contents and/or locations. example answer questions fig. need look portion image child phone booth. similarly answer question basket? fig. ﬁrst basket objects compare locations. propose deep learning approach incorporates explicit spatial attention call spatial memory network approach based memory networks recently proposed text question answering memory networks combine learned text embeddings attention mechanism multi-step inference. text memory network stores textual knowledge memory form sentences selects relevant sentences infer answer. however knowledge form image thus memory question come diﬀerent modalities. adapt end-to-end memory network solve visual question answering storing convolutional network outputs obtained diﬀerent receptive ﬁelds memory explicitly allows spatial attention image. also propose repeat process gathering evidence attended regions enabling model update answer based several attention steps hops. entire model trained end-to-end evidence computed answer visualized using attention weights. propose novel multi-hop memory network spatial attention task allows visualize spatial inference process used deep network implementation made available) design attention architecture ﬁrst uses word embedding capture ﬁne-grained alignment image question create series synthetic questions explicitly require spatial inference analyze working principles network show learns logical inference rules visualizing attention weights sec. introduces relevant work memory networks attention models. sec. describes design multi-hop memory network architecture visual question answering sec. visualizes inference rules learned network synthetic spatial questions shows experimental results daquar datasets. sec. concludes paper. popularity visual question answering text question answering already established mature research problem area natural language processing. previous methods include searching words question search engine parsing question knowledge base query embedding question using similarity measurement evidence answer recently memory networks proposed solving problem. ﬁrst introduces memory network general model consists memory four components input feature generalization output feature response. model investigated context question answering long-term emory acts dynamic knowledge base output textual response. proposes competitive memory network model uses less supervision called end-to-end memory network recurrent attention model large external memory. neural turing machine couples neural network external memory interacts attentional processes infer simple algorithms copying sorting associative recall input output examples. paper solve problem using multimodal memory network architecture applies spatial attention mechanism input image guided input text question. neural attention mechanism widely used diﬀerent areas computer vision natural language processing example attention models image captioning video description generation machine translation machine reading systems methods soft attention mechanism ﬁrst proposed adds layer network predicts soft weights uses compute weighted combination items memory. main types soft attention mechanisms diﬀer function aligns input feature vector candidate feature vectors order compute soft attention weights. ﬁrst type uses alignment function based concatenation input candidate second type uses alignment function based product input candidate. concatenation alignment function adds input vector candidate feature vector embeds resulting vectors scalar values applies softmax function generate attention weight candidate. concatenation alignment function soft attention models gives literature review models applied diﬀerent tasks. hand product alignment function ﬁrst projects inputs common vector embedding space takes product input vectors applies softmax function resulting scalar value produce attention weight candidate. end-to-end memory network uses product alignment function. authors compare alignment functions attention model neural machine translation task implementation concatenation alignment function yield good performance task. motivated this paper product alignment function spatial memory network. related image captioning. several early papers directly adapt image captioning models solve problem generating answer using recurrent lstm network conditioned output. models’ performance still limited proposes dataset uses similar attention model image captioning give results common benchmark implementation model less accurate baseline models. summarizes several recent papers reporting results dataset arxiv.org gives simple strong baseline model dataset. simple baseline concatenates image features word embedding question representation feeds softmax classiﬁer predict answer. ibowimg model beats models considered paper. here compare proposed model models dppnet model comparable better results ibowimg model. model essentially lstm model except uses image attribute features generated image caption relevant external knowledge knowledge base input lstm’s ﬁrst time step. dppnet model tackles learning convolutional neural network parameters predicted separate parameter prediction network. parameter prediction network uses gate recurrent unit generate question representation based question input maps predicted weights hashing. neither models contain spatial attention mechanism external data addition dataset e.g. knowledge base large-scale text corpus used pre-train question representation paper explore complementary approach spatial attention improve performance visualize network’s inference process obtain improved results without using external data compared ibowimg model well model dppnet model external data. ﬁrst give overview proposed smem-vqa network illustrated fig. sec. details word-guided spatial attention process ﬁrst shown fig. sec. describes adding second smemvqa network. input network question comprised variable-length sequence words image ﬁxed size. word question ﬁrst represented one-hot vector size vocabulary value corresponding word position zeros positions. one-hot vector embedded real-valued word vector words questions used compute attention visual memory contains extracted image features. input image processed convolutional neural network extract high-level -dimensional viprojected semantic space question word vectors using attention visual embedding results used infer spatial attention weights watt using word-guided attention process shown word-guided attention. process predicts attention determined question word maximum correlation embedded visual features location e.g. choosing word basket attend location basket image resulting spatial attention weights watt used compute weighted visual features embedded separate evidence transformation e.g. selecting evidence concept basket location. finally weighted evidence vector satt combined full question embedding predict answer. additional repeat process gather evidence convolutional image feature vectors location embedded common semantic space word vectors. diﬀerent embeddings used attention embedding evidence embedding attention embedding projects visual feature vector combination embedded question words generates attention weight location. evidence embedding detects presence semantic concepts objects embedding results multiplied attention weights summed locations generate visual evidence vector satt. finally visual evidence vector combined question representation used predict answer given image question. next section describe one-hop spatial memory network model speciﬁc attention mechanism uses detail. rather using bag-of-words question representation guide attention attention architecture ﬁrst uses word vector separately extract correlated visual features memory. intuition representation coarse letting word select related region provide ﬁne-grained attention. correlation matrix spatial attention weights watt calculated taking maximum word dimension correlation matrix selecting highest correlation value spatial location applying softmax function locations weights equal instance example shown fig. question basket? produces high attention weights location basket high correlation word vector basket visual features location. evidence embedding projects visual features produce high activations certain semantic concepts. e.g. fig. high activations region containing cat. results evidence embedding multiplied generated attention weights watt summed produce finally evidence vector satt question embedding used predict answer given image question. question representation choose bag-of-words question representations lstm also used however fewer parameters shown good performance. noted simple model performs roughly well better sequence-based lstm task. speciﬁcally compute possible prediction answers. activation function relu here. running example step adds evidence gathered near basket location question since found predicts answer attention evidence computation steps optionally repeated another predicting ﬁnal answer detailed next section. repeat hops promote deeper inference gathering additional evidence hop. recall visual evidence vector satt added question representation ﬁrst produce updated question vector correlation matrix ﬁrst provides ﬁne-grained local evidence word vectors question correlation vector chop next considers global evidence whole question representation ﬁnal answer predicted combining whole question representation local visual evidence satt word vector ﬁrst global visual evidence satt whole question second entire network diﬀerentiable trained using stochastic gradient descent standard backpropagation allowing image feature extraction image embedding word embedding answer prediction jointly optimized training image/question/answer triples. fig. absolute position experiment image question pair show original image attention weights watt attention follows following rules. ﬁrst rule looks position speciﬁed question contains square answer yes; otherwise answer section conduct series experiments evaluate model. explore whether model learns perform spatial inference necessary answering visual questions explicitly require spatial reasoning design experiments using synthetic visual question/answer data sec. experimental results model standard datasets datasets) reported sec. questions public datasets quite varied diﬃcult often require common sense knowledge answer furthermore past work showed question text alone strong predictor answer. therefore evaluating standard datasets would ﬁrst like understand proposed model uses spatial attention answer simple visual questions answer cannot predicted question alone. visualization demonstrates attention mechanism learn attend objects gather evidence certain inference rules. absolute position recognition investigate whether model ability recognize absolute location object image. explore designing simple task object appears region white-background image question square fig. relative position experiment image question pair show original image evidence embedding convolutional layer attention weights watt evidence embedding high activations square. attention weights follow similar inference rules fig. diﬀerence attention position around cat. cannot infer answer obtains accuracy around prior probability answer training set. smem-vqa one-hop model equivalent ibowimg model attention weights onehop model equally location since ibowimg model uses mean pool convolutional feature googlenet smem-vqa model. check visualization attention weights relationship high attention position answer expressed logical expressions. show attention weights several typical examples fig. reﬂect logic rules look position ibowimg model mean-pooled googlenet visual features lose spatial information thus cannot distinguish images square diﬀerent positions. contrary smem-vqa model high attention diﬀerent regions according question generate answer based selected region using learned inference rules. experiment demonstrates attention mechanism model able make absolute spatial location inference based spatial attention. relative position recognition order check whether model ability infer position object relative another object collect images coco detection dataset square smem-vqa one-hop model achieves test accuracy synthetic task baseline model accuracy around also check another simple baseline predicts answer based absolute position square image gets around accuracy. visualize evidence embedding features attention weights watt several typical examples fig. evidence embedding high activations square attention weights high attention certain locations around cat. analyze attention correctly predicted examples using rules absolute position recognition experiment. rules still work position relative object check speciﬁed position relative ﬁnds square answer otherwise find square answer speciﬁed position answer positions around cat. also check images model makes mistakes mistakes mainly occur images cats. square appears near cats image model might make mistakes focusing cats. conclude smem-vqa model infer relative spatial position based spatial attention around speciﬁed object also represented logical inference rules. results daquar daquar dataset relatively small dataset builds depth dataset reduced daquar dataset evaluation metric dataset accuracy. embedding dimension models running daquar dataset. several reported models daquar baselines listed below fig. visualization spatial attention weights smem-vqa one-hop two-hop models daquar datasets. image question pair show original image attention weights watt one-hop model attention weights watt watt two-hop model order. results smem-vqa model daquar dataset baseline model results reported previous work shown tab. daquar result tab. models based deep features significantly outperform multi-world approach based hand-crafted features. modeling question either lstm model question model equally well comparison indicating question text contains important prior information predicting answer. also dataset vis+lstm model achieves better accuracy neural-image-qa model; former shows image ﬁrst timestep lstm latter timestep. comparison one-hop model two-hop spatial attention models outperform img+bow well baseline models. major advantage model ability visualize inference process deep network. illustrate this attention weights visualization examples smem-vqa one-hop two-hop models daquar dataset shown fig. results dataset recent large dataset based coco full release open-ended dataset contains train set. following standard practice choose answers train sets possible prediction answers keep examples whose answers belong answers training data. question vocabulary size word frequency least three. larger training size embedding dimension dataset. report test-dev test-standard results evaluation server. server evaluation uses evaluation metattention models mirror input image using extract convolutional features since might cause confusion spatial locations objects input image. optimization algorithm used stochastic gradient descent minibatch size momentum base learning rate halved every epoches. regularization dropout norm cross-validated used. dataset simple ibowimg model baseline model beats existing models currently arxiv.org. also compare models comparable better results ibowimg model. three baseline models well best model dataset paper listed following predict certain parameters classiﬁcation network. pre-train question representation large-scale text corpus improve generalization performance. overall accuracy per-answer category accuracy smem-vqa models four baseline models dataset shown tab. table smem-vqa one-hop model obtains slightly better results compared ibowimg model. however smem-vqa two-hop model achieves improvement test-dev test-standard compared ibowimg model demonstrating value spatial attention. smem-vqa two-hop model also shows best performance per-answer category accuracy. smem-vqa two-hop model slightly better result dppnet model. dppnet model uses large-scale text corpus pre-train gated recurrent unit network question representation. similar pre-training work extra data improve model accuracy fig. visualization original image spatial attention weights watt ﬁrst correlation vector correlation matrix location highest attention weight smem-vqa two-hop model dataset. higher values correlation vector indicate stronger correlation word chosen location’s image features. done considering fact model extra data pretrain word embeddings results competitive. also experiment adding third model dataset result improve further. attention weights visualization examples smem-vqa one-hop two-hop models dataset shown fig. visualization two-hop model collects supplementary evidence inferring answer necessary achieve improvement complicated real-world datasets. also visualize ﬁnegrained alignment ﬁrst smem-vqa two-hop model fig. correlation vector values measure correlation image regions word vector question. higher values indicate stronger correlation particular word speciﬁc location’s image features. observe ﬁne-grained visual evidence collected using local word vector together global visual evidence whole question complement infer correct answer given image question shown fig. paper proposed spatial memory network memory network architecture spatial attention mechanism adapted visual question answering task. proposed synthetic spatial questions demonstrated model learns inference rules based spatial attention attention weight visualization. evaluation challenging daquar datasets showed improved results previously published models. model used visualize inference steps learned deep network giving insight processing. future work include exploring inference ability smem-vqa model exploring attention models.", "year": 2015}