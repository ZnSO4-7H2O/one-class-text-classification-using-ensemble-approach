{"title": "Input Switched Affine Networks: An RNN Architecture Designed for  Interpretability", "tag": ["cs.AI", "cs.CL", "cs.LG", "cs.NE"], "abstract": "There exist many problem domains where the interpretability of neural network models is essential for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations - in other words an RNN without any explicit nonlinearities, but with input-dependent recurrent weights. This simple form allows the RNN to be analyzed via straightforward linear methods: we can exactly characterize the linear contribution of each input to the model predictions; we can use a change-of-basis to disentangle input, output, and computational hidden unit subspaces; we can fully reverse-engineer the architecture's solution to a simple task. Despite this ease of interpretation, the input switched affine network achieves reasonable performance on a text modeling tasks, and allows greater computational efficiency than networks with standard nonlinearities.", "text": "exist many problem domains interpretability neural network models essential deployment. introduce recurrent architecture composed input-switched afﬁne transformations words without explicit nonlinearities inputdependent recurrent weights. simple form allows analyzed straightforward linear methods exactly characterize linear contribution input model predictions; change-of-basis disentangle input output computational hidden unit subspaces; fully reverse-engineer architecture’s solution simple task. despite ease interpretation input switched afﬁne network achieves reasonable performance text modeling tasks allows greater computational efﬁciency networks standard nonlinearities. neural networks move applications outcomes human lives depend decisions increasingly crucial able interpret decisions make. indeed european union considering legislation clause asserts individuals ’rights explanation’ i.e. individuals able understand algorithms make decisions example problem domains re*equal contribution this work performed intern google brain work done member google brain residency program google brain mountain view work performed author visiting faculty google brain. correspondence jakob foerster <jakob.foerstercs.ox.ac.uk> david sussillo <sussillogoogle.com>. quiring interpretable include self-driving cars trafﬁc control power grid control hiring promotion decisions preventing bias automated sentencing decisions courts medical diagnosis many applications practitioners adopt models without fully understanding drives predictions including understanding models fail approach interpreting neural networks train network normal apply analysis techniques training. often approach yields systems perform extremely well interpretability challenging. example sussillo barak used linearization nonlinear dynamical systems theory understand rnns solving simple varied tasks. karpathy analyzed lstm trained character-based language modeling task. able break lstm language model errors classes e.g. rare word errors. concurrently submission murdoch szlam decomposed lstm outputs using telescoping sums statistics computed memory cells different steps. decomposition exact unique authors justify demonstrating good performance decision rules formed using computed cell statistics. community also interested post interpretation feed-forward networks. examples include linear probes alain bengio variety techniques assign credit activations speciﬁc inputs input patterns feed-forward networks second approach build neural network interpretability explicit design constraint. approach typical outcome system better understood cost reduced performance. model classes whose work follow second approach build interpretability network model maintaining good though always state-of-the-art performance tasks study. focus commonly studied task character based language modeling. develop analyze model trained one-step-ahead prediction task text dataset million characters wikipedia text billion word benchmark ﬁnally multiple parentheses counting task fully reverse engineer. model introduce input switched afﬁne network input determines switching behavior selecting transition matrix bias function input nonlinearity. linear timevarying systems standard material undergraduate electrical engineering text books closely related technique. although isan deterministic probabilistic versions switching linear models discrete latent variables history context probabilistic graphical models. recent example switched linear dynamical system focusing language modeling deﬁned probabilistic linear dynamical system generative language model creating context-dependent token embeddings used steady-state kalman ﬁltering inference token sequences. used singular value decomposition discovered right left singular vectors semantically syntactically related. critical difference isan isan weight matrices input token dependent multiplicative neural networks proposed precisely character based language modeling mrnn architecture similar dynamics matrix switches function input character. however mrnn relied tanh nonlinearity isan explicitly linear. property model makes amenable analysis computationally efﬁcient. performs probabilistic sequence modeling. unlike isan requires linear projection hidden state corresponds normalized sequence probability. imposes strong constraints model parameters model dynamics restricts choice training algorithms. contrast isan applies afﬁne readout hidden state obtain logits pushed softmax function obtain probabilities. therefore constraints need imposed isan’s parameters training easy using backprop. lastly isan formulated afﬁne rather linear model. doesn’t change class processes modeled stabilizes training greatly enhances interpretability facilitating analysis section follows deﬁne isan architecture demonstrate performance one-step-ahead prediction task analyze model multitude ways would currently difﬁcult impossible accomplish modern nonlinear recurrent architectures. trained rnns text wikipedia dataset billion word benchmark one-step-ahead character prediction. text dataset consists characters ‘a’-‘z’ dataset consist unicode text modelled sequence bytes formed utf-encoded data. given character sequence rnns trained minimize cross-entropy true next character output prediction. hidden state logit space afﬁne map. probabilities computed table isan similar performance architectures text dataset. performance architectures text one-step-ahead prediction measured cross-entropy loss held-out test bits character. loss shown function maximum number parameters model allowed. values reported architectures taken data train validation test respectively line network trained hyperparameter tuning infrastructure dataset used data splits evaluation setup identical long experiment running times manually tuned hyperparameters. results text shown table largest parameter count isan matches almost exactly performance nonlinear models number maximum parameters irnn lstm. however note small numbers parameters isan performs considerably worse architectures. analyses isan trained maximum parameters samples generated text model relatively coherent. show examples priming \"annual reve\" inverse temperature respectively annual revenue producer telecommunications former communist action saving state house replicas many practical persons annual revenue seven three million nine nine eight rest country united states south africa new. preliminary comparative analysis performed state sequence large sequences vanilla varying sizes isan. shown figure eigenvalue spectra variance explained signiﬁcantly ﬂatter isan architectures. figure isan makes fuller uniform latent space vanilla rnns grus. figure shows explained variance ratio ﬁrst signiﬁcant dimensions hidden states across several architectures text dataset. legend provides number latent units architecture. without input switched dynamics. achieves crossentropy bits char independent network size. perplexity slightly better naive bayes model task bits char. output probability fully linear network product contributions previous character naive bayes. factorial contributions learned however giving non-switched afﬁne network slight advantage. also trained fully linear network nonlinear readout. achieves bits char independent network size. comparisons illustrate importance input switched dynamics achieving good results. lastly also test extent isan deal large dictionaries running byte-pair encoding text task input dictionary consists different possible character combinations. setup lstm consistently outperforms isan number parameters. parameters lstm achieves cross entropy bits char-pair isan achieves explanation ﬁnding matrices isan times smaller matrices lstms. large numbers parameters performance architecture saturates number parameters point isan ‘catch-up’ parameter efﬁcient architectures like lstms. trained isan lstm models dataset. networks trained using asynchronous gradient descent using adagrad learning rule. best lstm model reached bits character matches published results lstm model layer lstm units whose outputs projected onto dimensions best isan models reached bits character used previous figure using linearity hidden state dynamics predictions step broken contributions steps. accordingly panel corresponds propagated contribution input character time prediction time penultimate contains output bias vector replicated every time step. last contains logits predicted next character rows above. bottom panel contains corresponding softmax probabilities time characters labeled character maximum predicted probability. time step boxed examined detail figure empty product convention learned initial hidden state. using decomposition fact matrix multiplication linear transformation also write unnormalized logit-vector terms linear biases contribution time step logits time step bxt. notational convenience sometimes replace subscript corresponding input character step referring example refers contribution character string. similarly discussing summed contributions word substring sometimes write ‘the’ refers hidden units reduced common input tokens output tokens increasing isan’s hidden layer size units yielded perplexity improvement bits/char. investigation generated samples shows isan learned distinction lowerupper-cased letters able generate text coherent short segments. demonstrate sample variability show continuations prompt \"the generated using isan pol|ish pilgrims angry holiday trip pol|ice department subsequently slipped toward pol|ice federation sought helix also investors pol|itico tight crowd ever moderated pol|itical scientist shirt romance cannot pol|icy balanchine formed pol|l conducted suspected among hispanic pol|itical frenzy sparked primary care programs isan analyze factors important past determining current character prediction. taking advantage linearity hidden state dynamics sequence inputs decompose current latent state contributions originating different figure detailed view prediction stack ﬁnal ‘_annual_revenue’. shown contributions logit logits shown orange respectively earlier character string. corresponds zoom view columns highlighted orange show contributions string ‘_annual’ ‘_annual’ pushes prediction ‘_annual_reve’ ‘n’. without contribution model decodes based ‘_reve’ leading prediction ‘reverse’. ‘_annual’ instead predicts ‘revenue’. contribution contribution ‘_annual’ logits linear exact. terms propagated transformed time. emphasize includes multiplicative contributions however independent prior inputs main difference analysis carry isan compared nonlinear rnn. general recurrent network contribution speciﬁc character sequence depend hidden state start sequence. linearity dynamics dependency exist isan. figure show example decomposition allows understand particular prediction made given point time previous characters inﬂuence decoding. example sequence ‘_annual_revenue_’ processed isan starting all-zero hidden state equation accumulate sequence .... used values understand prediction network time simple addition across index. provide detailed view past characters contribute logits predicting next character figure competing options next letter word stem ‘reve’ either ‘revenue’ ‘reverse’. show without contributions ‘_annual’ likely decoding character second contributions ‘_annual’ balance favor decoding ‘revenue’. using isan investigate information timescales network. example investigated quickly decay function average. contributions figure shows contribution decays different exponential timescales. hypothesize ﬁrst time scale corresponds decay within word source characters. norm appears decay exponentially rates faster rate ﬁrst characters slower rate long term contributions. median cross entropy function position word three different circumstances line uses green line sets zero blue line sets zero. results panel demonstrate disproportionately large importance decoding especially onset word. cross-entropy function history artiﬁcially limiting number characters available prediction. corresponds considering recent length history. next corresponds decay information across words sentences. also show relevance contributions decoding characters different positions word example observe makes important contributions prediction next character time show using model achieve cross entropy less char position character letters beginning word. finally link norm-decay importance past characters decoding quality artiﬁcially limiting number past available prediction show prediction quality improves rapidly extending history characters saturates. rapid improvement aligns range faster decay figure isan provides natural means moving character level representation word level. using linearity hidden state dynamics aggregate belonging given word visualize sinκt contribution prediction letters next word. allows understand preceding word impacts decoding letters later words. figure show words ‘was’ ‘higher’ make large contributions prediction characters ‘than’ measured norm ‘_higher’. figure transforming isan dynamics basis better understand action input-dependent biases. observe strong correlation norms input dependent biases log-probability unigram training data. begin understand correlation structure using basis transform ‘readout basis’. breaking norm components pro⊥ respectively shows correlation component orthogonal wro. implies connection information ‘surprise’ distance ’computational’ subspace state space. figure isan architecture used precisely characterize relationship words characters. panel shows exploiting linearity network’s operation word single contribution combine word measure magnitude effect previous word selection current character bottom panel shows probabilities assigned network next sequence character. lighter lines show predictions conditioned decreasing number preceding words. example predicting characters ‘than’ large contribution ‘higher’ shown pane. effect probabilities seen bottom panel model becomes less conﬁdent excluding system leverages contextual information across multiple words. free perform change basis hidden state afﬁne isan dynamics basis. note change basis possible architectures since action nonlinearity depends choice basis. particular construct ‘readout basis’ explicitly divides latent space subspace spanned rows readout matrix orthogonal complement pro⊥ representation explicitly divides hidden state dynamics -dimensional ‘readout’ subspace accessed readout matrix make predictions ‘computational’ subspace comprising remaining dimensions orthogonal readout matrix. apply change basis analyze intriguing observation hidden offsets shown figure norm strongly correlated log-probability unigram training data. reexpressing network parameters using ‘readout basis’ shows correlation related reading next-step prediction. norm projection pro⊥ remains strongly correlated character frequency projection shows little correlation. indicates information content ’surprise’ letter encoded norm figure transforming isan dynamics basis better interpret structure input-dependent biases. show cosine distance input dependent bias vectors split vowels consonants show correlation considering components subspace spanned rows readout matrix wro. shows correlation components orthogonal complement pro⊥ plots white corresponds black similarly figure illustrate structure correlations biases components correlation pro⊥ relatively uniform. clearly blocks high correlations vowels consonants respectively b‘_’ uncorrelated either. compared computation performed n-gram language models performed isan. n-gram model back-off weights expresses conditional probability smoothed count ratios n-grams different lengths contribution shorter n-grams down-weighted back-off weights. hand computations performed isan start contribution logits shown figure corresponds unigram log-probabilities. logits additively updated contributions longer n-grams represented additive contribution logits corresponds multiplicative modiﬁcation emission probabilities histories different length. long time lags additive correction logprobabilities becomes small corresponds multiplication uniform distribution. despite differences n-gram history incorporated nevertheless observe agreement empirical models estimated training model predictions unigrams bigrams. figure shows bias term gives unigram probabilities letters addition offset terms accurately predict bigram distribution shown panel example panel summary plot letters. explore n-gram comparison artiﬁcially limiting length character history available isan making predictions shown figure show possibility complete interpretability isan train model parenthesis counting task. bringing together ideas section re-express transition dynamics basis fully reveals performed computations. analyze task counting nesting levels multiple parentheses types simpliﬁed version task deﬁned brieﬂy -unit isan required keep track nesting level different types parentheses independently. inputs one-hot encoding different opening closing parentheses well noise character output one-hot encoding nesting level counts parenthesis type furthermore target output nesting level previous time step. artiﬁcial delay requires model develop memory. change exchange cross-entropy error error. leads slightly cleaner ﬁgures qualitatively change results. elucidate mechanism isan’s operation ﬁrst reexpress afﬁne transitions linear equivalents next used linear regression change basis augmented character matrices hidden states sparse. construct ‘readout’ ‘computational’ subspace decomposition discussed section choose basis pro⊥ makes projections hidden figure predictions isan characters well approximate predictions unigram bigram models. compare softmax empirical unigram distribution compare softmax empirical distribution show correlation softmax characters compare correlation empirical unigram probabilities plot shows readout bias vector better predictor conditional distribution unigram probability. denote readout computational porhr tions denote readout readout readout computation computation readout computation computation blocks character matrix character respectively. figure show hidden states rotated basis sequence column vectors. dimensional hidden states -hot. treat concatenation computation readout part. -dimensional readout corresponds network’s output time step encodes counts time step -hot vector computational dimensional encodes space current counts another -hot vector. note basis isan effectively uses dimensions remaining dimensions noticeable effect computation. figure show parentheses ﬁxed. matrices behave analogously. clear solution general retraining increased numbers parentheses types increased counting maximum would analogous solution. paper motivated input-switched afﬁne recurrent network purpose interpretability. showed switched afﬁne architecture achieves performance lstms text dataset number maximum parameters reasonable performance bwb. performed series analyses demonstrating ability understand inputs point input sequence affect outputs later output sequence. showed multiple parentheses counting task isan dynamics completely reverse engineered. summary work provides evidence isan able express complex dynamical systems operation principle fully understood prospect remains reach many popular recurrent architectures. switched afﬁne networks hold potential massively computationally memory efﬁcient text processing recurrent architectures. first input-dependent afﬁne transitions reduce number parameters used every step. possible inputs parameters furthermore isan unique ability precompute afﬁne transformations corresponding input strings. possible composition afﬁne transformations also afﬁne transformation. property used section evaluate linear contributions words rather characters. means hidden state update corresponding entire input sequence computed identical cost update single character isan therefore achieve large speedups input processing cost increased memory accumulating large look-up tables corresponding common input sequences. course practical implementations incorporate figure visualization dynamics isan parentheses counting task time nesting levels one-step readout delay). weight matrix shown transformed highlight delay-line dynamics. activations hidden units shown original basis rotated basis highlight delay-line dynamics intelligible way. white line delineates transition matrix elements hidden state dimensions directly contribute output. matrices parentheses types appear similarly closing parentheses e.g. changing direction delay line. obvious future directions work. currently deﬁne switching behavior using input ﬁnite manageable cardinality. studying word-level language models enormous vocabularies require additional logic scale. another idea build language model switches bigrams trigrams rather characters words targeting intermediate number afﬁne transformations. adapting model continuous-valued inputs another important direction. approach tensor factorization similar employed mrnn deﬁning weights additional networks hypernetworks finally expect automated methods changing bases enable sparse representations hidden state dynamics matrices particularly fruitful direction pursue. would like thank jasmine collins help advice quoc david mohammad norouzi helpful discussions. would also like thank herbert jaeger insightful discussions regarding observableoperator-model. berk richard heidari hoda jabbari shahin kearns michael roth aaron. fairness criminal justice risk assessments state art. arxiv preprint arxiv. bojarski mariusz testa davide dworakowski daniel firner bernhard flepp beat goyal prasoon jackel lawrence monfort mathew muller zhang jiakai learning self-driving cars. arxiv preprint arxiv. chelba mikolov schuster brants koehn robinson billion word benchmark measuring progress statistical language modeling. arxiv e-prints december ching travers himmelstein daniel beaulieu-jones brett kalinin alexandr brian gregory ferrero enrico agapow paul-michael rosen gail opportunities obstacles deep learning biology medicine. biorxiv gulshan varun peng lily coram marc stumpe martin derek narayanaswamy arunachalam venugopalan subhashini widner kasumi madams cuadros jorge development validation deep learning algorithm detection diabetic retinopathy retinal fundus photographs. jama quoc ranzato marc monga rajat devin matthieu chen corrado greg dean andrew building high-level features using large scale unsupervised learning. international conference machine learning linderman scott miller andrew adams ryan blei david paninski liam johnson matthew recurrent switching linear dynamical systems. arxiv preprint arxiv. martens james sutskever ilya. learning recurrent neural networks hessian-free optimization. proceedings international conference machine learning scarborough david somers mark john. neural networks organizational research applying pattern recognition analysis organizational behavior. ameripsychological association siano pierluigi cecati carlo kolbusz janusz. real time operation smart grids networks optimal power ﬂow. ieee transactions industrial informatics sutskever ilya martens james hinton geoffrey generating text recurrent neural networks. proceedings international conference machine learning", "year": 2016}