{"title": "Stochastic Neural Networks for Hierarchical Reinforcement Learning", "tag": ["cs.AI", "cs.LG", "cs.NE", "cs.RO"], "abstract": "Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.", "text": "carlos florensa† duan†‡ pieter abbeel†‡§ berkeley department electrical engineering computer science international computer science institute openai florensaberkeley.edu {rockypieter}openai.com deep reinforcement learning achieved many impressive results recent years. however tasks sparse rewards long horizons continue pose signiﬁcant challenges. tackle important problems propose general framework ﬁrst learns useful skills pre-training environment leverages acquired skills learning faster downstream tasks. approach brings together strengths intrinsic motivation hierarchical methods learning useful skill guided single proxy reward design requires minimal domain knowledge downstream tasks. high-level policy trained skills providing signiﬁcant improvement exploration allowing tackle sparse rewards downstream tasks. efﬁciently pre-train large span skills stochastic neural networks combined information-theoretic regularizer. experiments show combination effective learning wide span interpretable skills sample-efﬁcient signiﬁcantly boost learning performance uniformly across wide range downstream tasks. recent years deep reinforcement learning achieved many impressive results including playing atari games pixel inputs mastering game acquiring advanced manipulation locomotion skills sensory inputs despite success stories deep algorithms typically employ naive exploration strategies \u0001-greedy uniform gaussian exploration noise shown perform poorly tasks sparse rewards tasks sparse rewards common realistic scenarios. example navigation tasks agent rewarded ﬁnds target. challenge complicated long horizons naive exploration strategies lead exponentially large sample complexity tackle challenges main strategies pursued ﬁrst strategy design hierarchy actions composing low-level actions high-level primitives search space reduced exponentially. however approaches require domain-speciﬁc knowledge careful hand-engineering. second strategy uses intrinsic rewards guide exploration computation intrinsic rewards require domain-speciﬁc knowledge. however facing collection tasks methods provide direct answer knowledge solving task transfer tasks solving tasks scratch overall sample complexity still high. paper propose general framework training policies collection tasks sparse rewards. framework ﬁrst learns span skills pre-training environment employed nothing proxy reward signal whose design requires minimal domain knowledge downstream tasks. proxy reward understood form intrinsic motivation encourages agent explore capabilities without goal information sensor readings speciﬁc downstream task. skills used later wide collection different tasks training separate high-level policy task skills thus reducing sample complexity uniformly. learn span skills propose stochastic neural networks class neural networks stochastic units computation graph. class architectures easily represent multi-modal policies achieving weight sharing among different modes. parametrize stochasticity network feeding latent variables simple distributions extra input policy. experiments observed direct application snns always guarantee wide range skills learned. hence propose information-theoretic regularizer based mutual information pre-training phase encourage diversity behaviors policy. experiments hierarchical policy-learning framework learn wide range skills clearly interpretable latent code varied. furthermore show training high-level policies learned skills results strong performance challenging tasks long horizons sparse rewards. main appealing aspects hierarchical reinforcement learning skills reduce search complexity problem however specifying good hierarchy hand requires domain-speciﬁc knowledge careful engineering hence motivating need learning skills automatically. prior work automatic learning skills largely focused learning skills discrete domains popular approach statistics state transitions identify bottleneck states however clear techniques applied settings high-dimensional continuous spaces. recently mnih propose draw-like recurrent neural network architecture learn temporally extended macro actions. however learning needs guided external rewards supervisory signals hence algorithm cannot straightforwardly applied sparse reward settings. also work learning skills tasks continuous actions methods extract useful skills successful trajectories tasks hence require ﬁrst solving comparably challenging task demonstrations. guided policy search levine leverages access training simpler setting. trains ilqg todorov state space parallel trains neural receives sensory inputs agree ilqg controller. neural policy able generalize situations. another line work skill discovery particularly relevant approach hireps algorithm daniel where instead mutual information bonus like proposed approach introduce constraint equivalent metric. solution approach nevertheless different cannot policy gradients. furthermore although achieve multimodality like tried episodic case single option active rollout. hence hierarchical learned skills less clear. similarly option-critic architecture learn interpretable skills whether reuse across complex tasks still open question. recently heess independently proposed learn range skills pre-training environment useful downstream tasks similar framework. however pre-training setup requires goals speciﬁed. comparison proxy rewards signal agent during pre-training phase construction requires minimal domain knowledge instrumentation environment. deﬁne discrete-time ﬁnite-horizon discounted markov decision process tuple state action transition probability distribution bounded reward function initial state distribution discount factor horizon. necessary attach sufﬁx symbols resolve ambiguity policy search methods typically optimize stochastic policy parametrized objective maximize γtr] denotes whole trajectory main interest solving collection downstream tasks speciﬁed collection mdps tasks share common structure cannot expect acquire skills speed learning them. hand want structural assumptions minimal make problem statement generally applicable. assume state space factored components sagent rest weakly interact other. sagent mdps also assume mdps share action space. intuitively consider robot faces collection tasks dynamics robot shared across tasks covered sagent components task-speciﬁc state space denoted srest. instance grasping task rest include positions objects interest sensory input associated task. speciﬁc structural assumption studied past sharing agent-space given collection tasks satisfying structural assumption objective designing algorithm minimize total sample complexity required solve tasks. commonly studied past sequential settings agent exposed sequence tasks learn make experience gathered solving earlier tasks help solve later tasks however requires earlier tasks relatively easy solve directly applicable tasks sparse rewards much challenging setting. next section describe formulation takes advantage pre-training task constructed minimal domain knowledge applied challenging scenario. section describe formulation solve collection tasks exploiting structural assumption articulated above. sec. describe pre-training environment proxy rewards learn useful span skills. sec. motivate usage stochastic neural networks discuss architectural design choices made tailor skill learning sec. describe information-theoretic regularizer improves span skills learned snns. sec. describe architecture high-level policies learned skills training procedure downstream tasks sparse rewards. finally sec. describe policy optimization phases training. given collection tasks would like construct pre-training environment agent learn span skills useful enhancing exploration downstream tasks. achieve letting agent freely interact environment minimal setup. mobile robot spacious environment robot ﬁrst learn necessary locomotion skills; manipulator used object manipulation tasks environment many objects robot interact with. skills learned environment depend reward given agent. rather setting different rewards pre-training environment corresponding desired skills requires precise speciﬁcation skill entail generic proxy reward reward signal guide skill learning. design proxy reward encourage existence locally optimal solutions correspond different skills agent learn. words encodes prior knowledge high level behaviors might useful downstream tasks rewarding roughly equally. mobile robot reward simple proportional magnitude speed robot without constraining direction movement. manipulator reward successful grasping object. every time train usual uni-modal gaussian policy environment converge towards potentially different skill. seen sec. applying approach mobile robots gives different direction locomotion every time. inefﬁcient procedure training policy scratch sample efﬁcient nothing encourages individual skills distinct. ﬁrst issue addressed next subsection using stochastic neural networks policies second issue addressed sec. adding information-theoretic regularizer. learn several skills time propose stochastic neural networks general class neural networks stochastic units computation graph. large body prior work special classes snns restricted boltzmann machines deep belief networks sigmoid belief networks rich representation power fact approximate well-behaved probability distributions policies modeled snns hence represent complex action distributions especially multi-modal distributions. purpose simple class snns latent variables ﬁxed distributions integrated inputs neural network form joint embedding standard feed-forward neural network deterministic units computes distribution parameters uni-modal distribution simple categorical distributions uniform weights latent variables number classes hyperparameter upper bounds number skills would like learn. simplest joint embedding shown figure concatenate observations latent variables directly. however limits expressiveness power integration observation latent variable. richer forms integrations multiplicative integrations bilinear pooling shown greater representation power improve optimization landscape achieving better results complex interactions needed inspired work study using simple bilinear integration forming outer product observation latent variable note concatenation scenarios observation high-dimensional images form lowdimensional embedding observation alone using neural network jointly embedding latent variable. integration effectively corresponds changing bias term ﬁrst hidden layer depending latent code sampled bilinear integration changing ﬁrst hidden layer weights. shown experiments choice integration greatly affects quality span skills learned. bilinear integration already yields large span skills hence type snns studied work. obtain temporally extended consistent behaviors associated latent code pretraining environment sample latent code beginning every rollout keep constant throughout entire rollout. training latent codes categorical distribution correspond different interpretable skill used downstream tasks. compared training separate policies training single allows ﬂexible weight-sharing schemes among different policies. also takes comparable amount samples train regular uni-modal gaussian policy sample efﬁciency training skills effectively times better. encourage diversity skills learned introduce information theoretic regularizer detailed next section. although snns sufﬁcient expressiveness represent multi-modal policies nothing optimization prevents collapsing single mode. observed worst case scenario experiments sometimes different latent codes correspond similar skills. desirable direct control diversity skills learned. achieve this introduce information-theoretic regularizer inspired recent success similar objectives encouraging interpretable representation learning infogan chen concretely additional reward bonus proportional mutual information latent variable current state. measure respect relevant subset state. mobile robot choose coordinates center mass formally random variable denoting current coordinate agent latent variable. expressed denotes entropy function. case constant since probability distribution latent variable ﬁxed uniform stage training. hence maximizing equivalent minimizing conditional entropy entropy measure uncertainty another interpretation bonus that given robot easy infer skill robot currently performing. penalize −ezc modify reward received every step speciﬁed estimate posterior probability latent code sampled rollout given coordinates time rollout. estimate posterior apply following discretization partition coordinate space cells continuous-valued coordinates cell containing overloading notation discrete variable indicating cell time calculation empirical posterior requires maintaining visitation counts many times cell visited latent code sampled. given batch policy optimization method trajectories current batch compute estimate shown high dimensional posterior also estimated ﬁtting regressor maximum likelihood. given span skills learned pre-training task describe basic building blocks solving tasks sparse reward signals provided. instead learning scratch low-level controls leverage provided skills freezing training high-level policy operates selecting skill committing ﬁxed amount steps imposed temporal consistency quality skills yield enhanced exploration allowing solve downstream tasks sparse rewards. given task train manager common skills. given factored representation state space sagent rest high-level policy receives full state input outputs parametrization categorical distribution sample discrete action possible choices corresponding available skills. skills independently trained uni-modal policies dictates policy following time-steps. skills encapsulated used place latent variable. architecture depicted fig. weights level high level neural networks could also jointly optimized adapt skills task hand. end-to-end training policy discrete latent variables stochastic computation graph could done using straight-through estimators like proposed jang maddison nevertheless show experiments frozen low-level policies already sufﬁcient achieve good performance studied downstream tasks directions left future research. pre-training phase training high-level policies trust region policy optimization policy optimization algorithm choose trpo excellent empirical performance require excessive hyperparameter tuning. training downstream tasks require modiﬁcations except action space skills used. pre-training phase presence categorical latent variables marginal distribution mixture gaussians instead simple gaussian even become intractable compute complex latent variables. avoid issue consider latent code part observation. given still gaussian trpo applied without modiﬁcation. applied framework hierarchical tasks described benchmark duan locomotion maze locomotion food collection observation space tasks naturally decompose sagent robot rest task-speciﬁc attributes like walls goals sensor readings. report results using swimmer robot also described benchmark paper. fact swimmer locomotion task described therein corresponds exactly pretrain task also solely reward speed plain environment. report results complex robots appendix c-d. increase variety downstream tasks constructed four different mazes. maze described benchmark maze reﬂection robot backwards-right-right instead forward-left-left. correspond figs. robot shown starting position reach turn. mazes different instantiations environment shown fig. goal placed north-east south-west corner respectively. reward granted robot reaches goal position. gather task depicted fig. robot gets reward collecting green balls reward ones positioned randomly beginning episode. apart robot joints positions velocities tasks agent also receives lidar-like sensor readings distance walls goals balls within certain range. benchmark continuous control problems shown algorithms employ naive exploration strategies could solve them. advanced intrinsically motivated explorations achieve progress report stronger results exact setting appendix hyperparameters neural network architectures algorithms detailed appendix full code available. evaluate every step skill learning process showing relevance different pieces architecture impact exploration achieved using hierarchical fashion. report results sparse environments described above. seek answer following questions multimodality snns bonus consistently yield large span skills? pre-training experience improve exploration downstream environments? enhanced exploration help efﬁciently solve sparse complex tasks? evaluate diversity learned skills visitation plots showing position robot’s center mass rollouts time-steps each. beginning every rollout reset robot origin always orientation fig. show visitation plot different feed-forward policies trained scratch pre-training environment. better graphical interpretation comparison next plots policies fig. superposes batch rollout policies different color. given morphology swimmer natural preference forward backward motion. therefore extra incentive added visitation concentrates heavily direction always initialized with. note nevertheless proxy reward general enough independently trained policy yields different advancing hence granting potentially useful skills solve downstream tasks embedded described multi-policy hierarchical architecture. next show that using snns learn similar even larger span skills without training several independent policies. number samples used train single feed-forward policy fig. therefore method learning skills effectively reduces sample complexity factor equal numbers skills learned. adequate architecture bonus show span skills generated also richer. rows figs. present visitation plots snns policies obtained different design choices. colors indicate latent code sampled beginning rollout. observe latent code generates particular interpretable behavior. given initialization orientation always same different skills truly distinct ways moving forward backwards sideways. following analyze impact span skills integration latent variables bonus simple concatenation latent variables observations rarely yields distinctive behaviors latent code sampled. hand observed trained snns bilinear integration acquire least forward backward motion associated different latent codes. improved increasing also observe bonus yields less overlapped skills advancing/turning skills independently integration latent variables. hierarchical architectures propose direct impact areas covered random exploration. illustrate plots showing visitation position center mass single rollouts million steps generated different architecture. hand show fig. exploration obtained actions drawn gaussian similar would happen ﬁrst iteration training multi-layer perceptron normalized random initialization weights. noise relatively large swimmer robot actions clipped still yield good exploration point reached robot million steps rollout within around initial position. exploration enough reach ﬁrst reward downstream sparse reward environments never learn already reported duan hand using hierarchical structures pretrained policies introduced sec. yields considerable increase exploration reported fig. plots also show possible millions steps rollout randomly initialized manager network hence outputting uniformly distributed one-hot vectors every steps. color every point trajectory corresponds latent code ﬁxed time-step clearly showing skill changes. following describe speciﬁc architecture used plot. rollout fig. generated following policy multi-policy architecture. hierarchical architecture strong baseline given used times samples pretrain trained policies independently. explained sec. every steps uses sampled one-hot output manager network select pre-trained policies. observe exploration given multi-policy hierarchy heavily concentrates around upward downward motion expected individual pre-trained policies composing finally rollouts fig. hierarchical architecture bilinear integration respectively. described sec. placeholder training received categorical latent code receives one-hot vector output manager network instead. exploration obtained snns yields wider coverage space underlying policy usually larger span skills. note changes latent code yield corresponding change direction. simple pre-train setup interpretability obtained skills advantages respect previous hierarchical structures. fig. evaluate learning curves different hierarchical architectures proposed. sparsity tasks none properly solved standard reinforcement algorithms therefore compare methods better baseline adding downstream task center mass proxy reward granted robot pre-training task. baseline performs quite poorly mazes fig. long time-horizon needed reach goal associated credit assignment problem. furthermore proxy reward alone encourage diversity actions bonus does. proposed hierarchical architectures able learn much faster every effectively shrink time-horizon aggregating time-steps useful primitives. beneﬁt using snns bilinear integration hierarchy also clear mazes although pretraining bonus always boost performance. observing learned trajectories solve mazes realize turning sideway motions help maze critical tasks robot speciﬁc forward motion wall maze reorient itself. think extra skills shine tasks requiring precise navigation. indeed case gather task seen fig. average return increases also variance learning curve lower algorithm using pretrained bonus denoting consistent learning across different snns. details gather task refer appendix important mention curve multi-policy performance corresponds total runs random seeds different snns obtained random seeds pretrain task. cherry picking pre-trained policy sparse environment done previous work. also explains high variance proposed hierarchical methods tasks. particularly hard mazes pretrained particular motion allows reach goal reward. appendix details. propose framework ﬁrst learning diverse skills using stochastic neural networks trained minimum supervision utilizing skills hierarchical architecture solve challenging tasks sparse rewards. framework successfully combines parts ﬁrstly unsupervised procedure learn large span skills using proxy rewards secondly hierarchical structure encapsulates latter span skills allows re-use future tasks. span skills learning greatly improved using stochastic neural networks policies additional expressiveness multimodality. bilinear integration mutual information bonus consistently yield wide interpretable span skills. hierarchical structure experiments demonstrate signiﬁcantly boost exploration agent environment demonstrate relevance solving complex tasks mazes gathering. limitation current approach switching skills unstable agents reported appendix robot. multiple future directions make framework robust challenging robots like learning transition policy integrating switching pretrain task. limitations current approach ﬁxed sub-policies ﬁxed switch time training downstream tasks. ﬁrst issue alleviated introducing end-to-end training example using straight-through gradient estimators stochastic computations graphs discrete latent variables second issue critical static tasks like ones used here studied appendix case becoming bottleneck complex dynamic tasks termination policy could learned manager similar option framework. finally used feedforward architectures hence decision skill next depends observation moment switching using sensory information gathered previous skill active. limitation could eliminated introducing recurrent architecture manager level. work supported part darpa berkeley vision learning center berkeley artiﬁcial intelligence research laboratory berkeley deep drive pecase award. carlos florensa also supported caixa ph.d. fellowship duan bair fellowship huawei fellowship. marc bellemare sriram srinivasan georg ostrovski schaul david saxton remi munos. unifying count-based exploration intrinsic motivation. advances neural information processing systems infogan interpretable representation learning information maximizing generative adversarial nets. advances neural information processing systems coline devin abhishek gupta trevor darrell pieter abbeel sergey levine. learning modular neural network policies multi-task multi-robot transfer. international conference robotics automation duan chen rein houthooft john schulman pieter abbeel. benchmarking deep reinforcement learning continuous control. international conference machine learning xiaoxiao satinder singh honglak richard lewis xiaoshi wang. deep learning real-time atari game play using ofﬂine monte-carlo tree search planning. advances neural information processing systems nicolas heess greg wayne yuval tassa timothy lillicrap martin riedmiller david silver. learning transfer modulated locomotor controllers. arxiv preprint arxiv. rein houthooft chen duan john schulman filip turck pieter abbeel. variational information maximizing exploration. advances neural information processing systems timothy lillicrap jonathan hunt alexander pritzel nicolas heess erez yuval tassa david silver daan wierstra. continuous control deep reinforcement learning. arxiv preprint arxiv. chris maddison andriy mnih whye teh. concrete distribution continuous relaxation discrete random variables. international conference learning representations volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature volodymyr mnih john agapiou simon osindero alex graves oriol vinyals koray kavukcuoglu strategic attentive writer learning macro-actions. advances neural information processing systems pravesh ranchod benjamin rosman george konidaris. nonparametric bayesian reward segmentation skill discovery using inverse reinforcement learning. international conference intelligent robots systems ieee john schulman philipp moritz sergey levine michael jordan pieter abbeel. highdimensional continuous control using generalized advantage estimation. international conference learning representations david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature emanuel todorov weiwei generalized iterative method locally-optimal feedback control constrained nonlinear stochastic systems. american control conference proceedings ieee christopher vigorito andrew barto. intrinsically motivated hierarchical skill learning structured environments. ieee transactions autonomous mental development aaron wilson alan fern soumya prasad tadepalli. multi-task reinforcement learning hierarchical bayesian approach. proceedings international conference machine learning yuhuai saizheng zhang ying zhang yoshua bengio ruslan salakhutdinov. multiplicative integration recurrent neural networks. advances neural information processing systems policies trained trpo step size discount neural networks layers hidden units. training mesh density used grid space give bonus divisions/unit. number skills trained batch size maximum path length pre-train task also ones used benchmark respectively. downstream tasks tab. fairly compare methods previous work downstream gather environment report results exact settings used duan maximum path length batch-size hierarchical approach outperforms state-of-the-art intrinsic motivation results like vime baseline center mass speed intrinsic reward task happens stronger expected. factors. first task offers not-so-sparse reward easily reached agent intrinsically motivated move. second limit steps makes original task much simpler sense agent needs learn reach nearest green ball won’t time reach anything else. therefore actual need skills re-orient properly navigate environment making hierarchy useless. increasing time-horizon steps hierarchy shines much more also seen videos. section study method scales complex robot. repeat experiments section snake agent -link robot depicted fig. compared swimmer action dimension doubles state-space increases also perform analysis relevance switch time reporting performance variation among different pretrained snns hierarchical tasks. snake robot learn large span skills consistently swimmer. report figs. visitation plots obtained different random seeds pretraining algorithm secs. mutual information bonus impact different spans skills later hierarchical training discussed sec. evaluate performance hierarchical architecture solve snake sparse tasks maze gather sec. given larger size robot also increased size maze gather task respectively. maximum path length also increased proportion batch size switch time despite tasks harder approach still achieves good results figs. clearly shows outperforms baseline intrinsic motivation center mass hierarchical task. analysis switch time switch time critically affect performance static tasks robot required react fast precisely changes environment. performance different reported gather task sizes figs. smaller environment implies red/green balls closer other hence precise navigation expected achieve higher scores. framework lower switch time allows faster changes skills therefore explaining slightly better performance gather task size hand larger environments mean sparcity number balls uniformly initialized wider space. case longer commitment every skill expected improve exploration range. explains slower learning gather task size still surprising mild changes produced large variations switch time gather task. maze task figs. show difference different important critical. particular radical increases little effect performance task robot simply keep bumping wall switches latent code. behavior observed videos attached paper. large variances observed performance different pretrained snn. studied sec. previous section variance learning curves gather indicates pretrained preform equally good task. maze performance depends strongly pretrained snn. example observe obtained random seed visitation weak backward span happens critical solve maze explains lack learning fig. particular pretrained snn. note large variability different random seeds even worse baseline reward maze seen fig. seeds never reach goal ones unstable learning long time-horizon. section report progress failure modes approach challenging unstable robots like agent -dimensional space -dimensional action space. unstable sense might fall cannot recover. show despite able learn well differentiated skills snns switching skills hard task instability. pretraining step still able yield large span skills covering directions long bonus well tuned observed fig. videos comparing different gaits observed attached videos. switching skill robot ﬁnds region state-space might unknown skill. increases instability robot example case might leads falling terminating rollout. fig. depicted rollouts terminate falling over. details failure cases possible solutions please refer technical report http//bit.ly/snnhrl-antreport", "year": 2017}