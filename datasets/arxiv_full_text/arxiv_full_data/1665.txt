{"title": "Improved Neural Text Attribute Transfer with Non-parallel Data", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Text attribute transfer using non-parallel data requires methods that can perform disentanglement of content and linguistic attributes. In this work, we propose multiple improvements over the existing approaches that enable the encoder-decoder framework to cope with the text attribute transfer from non-parallel data. We perform experiments on the sentiment transfer task using two datasets. For both datasets, our proposed method outperforms a strong baseline in two of the three employed evaluation metrics.", "text": "text attribute transfer using non-parallel data requires methods perform disentanglement content linguistic attributes. work propose multiple improvements existing approaches enable encoder-decoder framework cope text attribute transfer non-parallel data. perform experiments sentiment transfer task using datasets. datasets proposed method outperforms strong baseline three employed evaluation metrics. goal text attribute transfer task change input text value particular linguistic attribute interest transferred different desired value task needs approaches disentangle content linguistic attributes text. success neural encoder-decoder methods perform text attribute transfer tasks machine translation text summarization rely large parallel datasets expensive produced. effective non-parallel data perform family problems still open problem. text attribute transfer non-parallel data given large sets non-parallel texts contain different attribute values respectively task consists using data train models rewrite text resulting text attribute value vice-versa. overall message contained rewritten text must relatively original chosen attribute value change. main challenges using non-parallel data perform task straightforward train encoder-decoder maximum likelihood estimation transferred text lack ground truth; difﬁcult preserve content transferring input style. recent work shen showed promising results style-transfer non-parallel text tackling challenging work propose method perform text attribute transfer tackles challenges cope using single collaborative classiﬁer alternative commonly used adversarial discriminators e.g. note potential extension problem multiple attributes transfer would still single classiﬁer require many discriminators number attributes. approach constraints including attention mechanism combined cyclical loss novel noun preservation loss ensure proper content transfer. compared algorithm shen sentiment transfer task datasets using three evaluation metrics outperforming baseline terms ﬁrst two. proposed method assume access text dataset consisting non-parallel corpora different attribute values total sentences denote randomly sampled sentence attribute natural approach perform text attribute transfer regular encoder-decoder network however training network requires parallel data. since work consider problem attribute transfer non-parallel data propose extend basic encoder-decoder introducing collaborative classiﬁer specialized loss functions enable training data. figure shows overview proposed attribute transfer approach. note clarity figure used multiple boxes show encoder decoder classiﬁer actual model contains single encoder decoder classiﬁer. together encoder ˆxi→j form rnn) general improve quality decoded sentence. original attribute decoded sentence ˆxi→i decoded/transferred sentence ˆxi→j different attribute denote classiﬁer transferred sentences {ˆxi→j takes input decoded sentences outputs probability distribution attribute details). using collaborative classiﬁer goal produce training signal indicates effectiveness current decoder transferring sentence given attribute value. note branch figure considered auto-encoder therefore enforce closeness ˆxi→i using standard cross-entropy loss below). however bottom branch lack parallel data cannot approach purpose proposed novel content preservation loss finally note transferred transfer back using bottom branch figure below). follows present details loss functions employed training model. utilize content preservation loss. enforce closeness attention mechanism. recall mechanism enables establish approximate correspondence words original transferred sentences. example denote words sentence utilizing attention mechanism establish correspondence |xi→j words. among different pairings words select ones noun enforce corresponding transferred word wi→j matches noun i.e. lcnt_rec k={...wj kr...}∼x indices noun pair established attention mechanism. note although always applicable heuristic effective attributes sentences share nouns classiﬁcation loss. loss formulated follows encoder-decoder loss gives feedback current generator’s effectiveness transferring sentences attribute. classiﬁer provides additional training signal generated data enabling classiﬁer trained semi-supervised regime. classiﬁcation loss original data. order enforce high classiﬁcation accuracy classiﬁer also uses supervised classiﬁcation loss measuring classiﬁer predictions original instances reconstruction loss. back-transfer loss motivated difﬁculty imposing constraints transferred sentences. back-transfer transforms transferred sentences xi→j also implicitly imposes constraints generated sentences improves content preservation loss formulated follows summary training components architecture consists optimizing following loss function using stochastic gradient descent back-propagation weights sample mini-batch original sentences sample mini-batch transferred sentences {ˆxi→j generator’s distribution ˆxi→j sample mini-batch back-transferred sentences {ˆxi→j→i generator’s distribution ˆxi→j→i compute lrec lcnt_rec lclass_td lclass_od lback_rec lclass_btd update θgθc} gradient descent loss related work attribute transfer studied extensively context images text domain several works studying style transfer task setting non-parallel data however style/attribute transfer text fundamentally different textual data sequential potentially varying lengths versus constant-sized images. image domain similar works cyclegan also employs cycle consistency loss ensures composition transfer reverse close identity map. however several differences cyclegan work single generator generating styles makes easier scale multiple style transfer collaborative classiﬁer measuring style instead adversarial discriminator imparts stability training additional syntactic regularizers better content preservation. controlled text generation style transfer without parallel data also received attention language community recently ficler goldberg consider problem attribute conditioned generation text conditioned language modeling setting using lstm. mueller allows modifying hidden representations generate sentences desired attributes measured classiﬁer however model explicitly encourage content preservation. proposed model similarities approach taken shen main differences instead adversarial discriminators simple encode-decoder framework collaborative classiﬁer augmented attention mechanism specially designed content preservation losses. section present experimental results applying proposed approach sentiment transfer example text attribute transfer. compared algorithm approach datasets. dataset based yelp restaurant reviews contains sentences based negative reviews similarly positive sentences. sentences maximum length words. second dataset based general customer reviews amazon selected positive number negative sentences tokens sentence. used three evaluation metrics sentiment accuracy computed based pre-trained classiﬁer measures percentage sentences table evaluation results yelp amazon datasets. yelp pre-trained classiﬁer default accuracy pre-trained language model default perplexity amazon values classiﬁcation perplexity. test correct sentiment label; content preservation accuracy evaluation metric proposed work computed percentage transferred sentences least nouns present original sentence; perplexity score computed based pre-trained language model measures quality generated text. results presented table compared algorithm shen proposed method although able better perplexity scores achieve accurate sentiment transfer better content preservation. possible explanation higher perplexity since algorithm explicitly enforce content similarity easier achieving high sentiment accuracy perplexity transferred sentences. algorithm hand penalized content changes forces sacriﬁce perplexity. achieving better results across metrics still remains challenge. table also show sentences generated algorithms yelp dataset. algorithm although able create well structured sentences correct sentiment labels many cases cannot accurately preserve content. hand approach generate text somewhat higher perplexity ensures better sentiment content transfer. work proposed novel algorithm text attribute transfer non-parallel corpora based encoder-decoder architecture attention augmented collaborative classiﬁer content preservation losses. although experimental evaluations showed promising results number challenges remain achieve better results across three metrics propose evaluation metrics better capture quality transfer; improve architecture enable transfer challenging text attributes text goes signiﬁcant transformation simpler sentiment transfer tasks; extend architecture work multi-attribute transfer challenging problem. luong pham manning. effective approaches attention-based neural machine translation. proceedings conference empirical methods natural language processing pages september", "year": 2017}