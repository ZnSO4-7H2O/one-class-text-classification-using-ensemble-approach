{"title": "Gaussian Attention Model and Its Application to Knowledge Base Embedding  and Question Answering", "tag": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "abstract": "We propose the Gaussian attention model for content-based neural memory access. With the proposed attention model, a neural network has the additional degree of freedom to control the focus of its attention from a laser sharp attention to a broad attention. It is applicable whenever we can assume that the distance in the latent space reflects some notion of semantics. We use the proposed attention model as a scoring function for the embedding of a knowledge base into a continuous vector space and then train a model that performs question answering about the entities in the knowledge base. The proposed attention model can handle both the propagation of uncertainty when following a series of relations and also the conjunction of conditions in a natural way. On a dataset of soccer players who participated in the FIFA World Cup 2014, we demonstrate that our model can handle both path queries and conjunctive queries well.", "text": "propose gaussian attention model content-based neural memory access. proposed attention model neural network additional degree freedom control focus attention laser sharp attention broad attention. applicable whenever assume distance latent space reﬂects notion semantics. proposed attention model scoring function embedding knowledge base continuous vector space train model performs question answering entities knowledge base. proposed attention model handle propagation uncertainty following series relations also conjunction conditions natural way. dataset soccer players participated fifa world demonstrate model handle path queries conjunctive queries well. growing interest incorporating external memory neural networks. example memory networks equipped static memory slots content location addressable. neural turing machines implement memory slots read written turing machines differentiable attention mechanism. memory slot models stores vector corresponding continuous representation memory content. order recall piece information stored memory attention typically employed. attention mechanism introduced bahdanau uses network outputs discrete probability mass memory items. memory read implemented weighted memory vectors weights given attention network. reading single item realized special case output attention network peaked desired item. attention network depend current context well memory item itself. attention model called location-based content-based depends location memory stored memory vector respectively. embed entities knowledge base continuous vector space capacity embedding model appropriately controlled expect semantically similar entities close other allow model generalize unseen facts. however notion proximity strongly depend type relation. example benjamin franklin engineer also politician. would need different metrics capture proximity engineers politicians time. figure comparison conventional content-based attention model using inner product proposed gaussian attention model mean different covariances. context denotes variables attention depends example american engineers century american politicians century would contexts include benjamin franklin attentions would different shapes. compared inner product used previous work content-based addressing gaussian model additional control spread attention items memory. show figure view conventional inner-product-based attention proposed gaussian attention addressing afﬁne energy function quadratic energy function respectively. making addressing mechanism complex represent many entities relatively dimensional embedding space. since knowledge bases typically extremely sparse likely afford complex attention model large embedding dimension. apply proposed gaussian attention model question answering based knowledge bases. high-level goal task learn mapping question objects knowledge base natural language probability distribution entities. scoring function embedding entities vectors extracting conditions mentioned question taking conjunction score candidate answer question. ability compactly represent objects makes gaussian attention model well suited representing uncertainty multiple-answer question moreover traversal knowledge graph naturally handled series gaussian convolutions generalizes addition vectors. fact model relation gaussian mean variance parameters. thus traversal relation corresponds translation mean addition variances. proposed question answering model able handle case answer question associated atomic fact called simple also questions require composition relations conjunction queries. example model deals question plays forward borussia dortmund? shown figure section paper structured follows. section describe gaussian scoring function used embed entities knowledge base continuous vector space. call model transgaussian similarity transe model proposed bordes section describe question answering model. section carry experiments worldcup dataset collected. dataset relatively small allows evaluate simple questions also path queries conjunction queries. proposed transgaussian embedding question answering model achieves signiﬁcantly higher accuracy vanilla transe embedding transe trained compositional relations combined question answering model. section describe proposed transgaussian model based gaussian attention model possible train network computes embedding single pass multiple passes efﬁcient ofﬂoad embedding separate step question answering based large static knowledge base. transgaussian model entities relations. knowledge base collection triplets call relation object triplet respectively. triplet encodes fact. example triplets given knowledge base assumed true. however generally speaking triplet true false. thus knowledge base embedding aims training model predict triplet true given parameterization entities relations paper associate vector entity associate relation parameters positive deﬁnite symmetric matrix rd×d given subject relation compute score object triplet using gaussian attention model µcontext σcontext note ﬁxed identity matrix modeling relation subject object translation equivalent transe model allow covariance depend relation handle one-to-many relations capture shape distribution objects triplet. call model transgaussian similarity transe parameterization computational efﬁciency restrict covariance matrix diagonal paper. furthermore order ensure strictly positive deﬁnite employ exponential linear unit parameterize follows ranking loss suppose triplets incorrect objects triplet objective function uses ranking loss measure margin scores true answers false answers written follows relation separate relation denote r∪r− relations including relations inverse relations; relation inverse relation implies vice versa. moreover et∼n denotes expectation respect uniform distribution incorrect objects approximate random samples experiments. finally last terms regularization terms embedding parameters. recently shown training transe compositional relations make competitive complex models although transe much simpler compared example neural tensor networks transh wang here compositional relation relation composed series relations example grand father composed ﬁrst applying parent relation father relation seen traversal path knowledge graph. transgaussian model naturally handle propagate uncertainty chain relations convolving gaussian distributions along path. score entity τ-step relation r/r/··· subject denote triplet given training compositional relations randomly sampled paths knowledge graph. relation path relation inverse relation scoring function generalized training objective compositional relations written identically except replacing replacing given question-answer pairs question phrased natural language answer entity knowledge base goal train model learns mapping question correct entity. question answering model consists three steps entity recognition relation composition conjunction. ﬁrst identify list entities mentioned question question plays forward borussia dortmund? list would next step predict path relations knowledgegraph starting entity list extracted ﬁrst step. example /forward/position played /borussia dortmund/has player/ predicted series gaussian convolutions. general multiple relations appearing path. finally take product gaussian attentions renormalize equivalent bayes’ rule independent observations noninformative prior. assume oracle provides list containing entities mentioned question domain speciﬁc entity recognizer developed efﬁciently generally entity recognition challenging task beyond scope paper show whether beneﬁt training question answering model jointly entity recognizer. assume number extracted entities different question. figure input system question natural language. entities forward borussia dortmund identiﬁed question associated point mass distributions centered corresponding entity vectors. lstm encodes input sequence output vectors length. take average output vectors weighted attention recognized entity predict weight relation associated entity form gaussian attention entities entity convolving corresponding point mass gaussian embeddings relations weighted according ﬁnal prediction produced taking product normalizing gaussian attentions. rectiﬁed linear unit used ensure positivity weights. note however weights normalized want relation path. making weights positive also effect making attention sparse interpretable cancellation. extracted entity view extracted entity answer question subject object triplet respectively path inferred question weights described above. accordingly score candidate answer expressed using conjunction entities recognized question ﬁnal step model take conjunction gaussian attentions derived previous step. step simply carried multiplying gaussian attentions follows tions. training time assume training supervised question-answer pairs here question formulated natural language knowledge base entities appears question answer question. example knowledge base soccer players valid training sample could marco reus). note answer question necessarily unique allow true answers knowledge base. test time model shown task denote answers train question-answering model minimize objective function et∼n expectation respect uniform distribution incorrect answers approximate random samples. assume number relations implied question small compared total number relations knowledge base. hence coefﬁcients computed question regularized norms. demonstration proposed framework perform question answering dataset soccer players. work consider types questions. path query question contains named entity knowledge base answer found knowledge graph walking path consisting relations. conjunctive query question contains entities answer given conjunction path queries starting entity. furthermore experimented knowledge base completion task transgaussian embeddings test capability generalization unseen fact. since knowledge base completion main focus work include results appendix. football players participated fifa world build knowledge base original dataset consists players’ information nationality positions ﬁeld ages etc. picked attributes constructed entities atomic relations. entities include players professional soccer clubs countries numbers positions. atomic relations plays position player position plays club player club wears number player number aged player number country club country plays country player country player club number denote type entities appear left right argument relation. relations share type right argument e.g. plays country country. given entities relations transformed dataset triplets. list sample triplets found appendix. based triplets created sets question answering tasks call path query conjunctive query respectively. answer every question always entity knowledge base question involve triplets. questions generated follows. path queries. among paths knowledge graph natural composition relations e.g. plays country decomposed composition plays club country addition atomic relations manually picked meaningful compositions relations formed query templates takes form true subject atomic relation path relations. formulate path-based question-answer pairs manually created question templates every query template then particular instantiation query template subject object entities randomly select question template generate question given subject; object entity becomes answer question. table list composed relations sample questions answers. note atomic relations dataset many-to-one composed relations one-to-many many-to-many well. conjunctive queries. generate question-and-answer pairs conjunctive queries ﬁrst picked three pairs relations used create query templates form find true. pair relations enumerated pairs entities subjects formulated corresponding query natural language using question templates path queries. table list sample questions answers. result created question-and-answer pairs path queries pairs conjunctive queries partitioned train validation test subsets. refer table statistics dataset. templates generating questions list table perform question answering proposed framework ﬁrst train transgaussian model worldcup dataset. addition atomic triplets randomly sampled paths length knowledge graph trained transgaussian model compositionally described inverse relation treated separate relation. following naming convention denote trained embedding transgaussian found learned embedding possess interesting properties. dimensions embedding space dedicate represent particular relation. players clustered attributes entities’ embeddings projected corresponding lower dimensional subspaces. elaborate illustrate properties appendix. baseline methods also trained transgaussian model atomic triplets denote model transgaussian since inverse relation involved transgaussian trained embedding question answering tasks represent inverse relations follows relation mean variance model inverse gaussian attention mean variance equal also trained transe models worldcup dataset using code released authors likewise transe denote model trained atomic triplets transe denote model trained union triplets paths. note transe considered special case transgaussian variance matrix identity hence scoring formula applicable transe well. training conﬁgurations models dimension entity embeddings hidden size lstm word embeddings trained jointly question answering model dimension word embedding employed adam optimizer. parameters tuned validation set. setting experimented cases ﬁrst trained models path queries conjunctive queries separately; furthermore trained single model addresses types queries. present results latter case next subsection results former included appendix. evaluation metrics test time model receives question natural language list knowledge base entities contained question. predicts mean variance gaussian attention formulated expected capture distribution positive answers. rank entities knowledge base scores gaussian attention. next entity correct answer check rank relative incorrect answers call rank ﬁltered rank. example correct entity ranked negative answers except ﬁltered rank two. compute rank true answers report mean ﬁltered rank percentage true answers ﬁltered rank present results joint learning table results show transgaussian works better transe general. fact transgaussian achieved best performance almost aspects. notably achieved highest rates challenging questions where club edin dzeko plays for? defenders german national team? table shows transgaussian beneﬁts remarkably compositional training. example compositional training improved transgaussian’s rate near queries players given countries queries players play particular position also boosted transgaussian’s performance conjunctive quries signiﬁcantly. understand transgaussian weak performance answering queries professional football club located given country queries professional football club players particular country tested capability modeling composed relation feeding correct relations subjects test time. turns relations modeled well transgaussian embedding limits performance question answering. limit found three embeddings well. note models compared table uses proposed gaussian attention model transe special case transgaussian variance ﬁxed one. thus main differences whether variance learned whether embedding trained compositionally. finally refer table appendix experimental results models trained path conjunctive queries separately. work vilnis mccallum similar gaussian attention model. discuss many advantages gaussian embedding; example arguably better handling asymmetric relations entailment. however work presented wordvec -style word embedding setting gaussian embedding used capture diversity meaning word. gaussian attention model extends work general setting memory item addressed concept represented gaussian distribution memory items. club alan pulido play for? position gonzalo higuain play? samuel etoo? jersey number mario balotelli? country thomas mueller country soccer team porto based plays professionally liverpool player iran? name player plays goalkeeper? soccer club based mexico? club edin dzeko plays name soccer club player australia bordes proposed question-answering model embeds questions answers common continuous vector space. method bordes combine multiple knowledge bases even generalize knowledge base used training. however method limited simple question answering setting answer question associated triplet knowledge base. contrast method handle composition relations conjunction conditions naturally enabled proposed gaussian attention model. neelakantan proposed method combines relations deal compositional relations knowledge base completion. technical contribution recurrent neural networks encode chain relations. restrict path queries question answering seen sequence transduction task input text output series relations. rnns decoder model would able handle non-commutative composition relations current weighted convolution cannot handle well. another interesting connection work take maximum inner-product scores computed along multiple paths connecting pair entities. representing collection vectors taking maximum inner-product scores natural represent memory items. gaussian attention model propose paper however advantage differentiability composability. paper proposed gaussian attention model used variety contexts assume distance memory items latent space compatible notion semantics. shown proposed gaussian scoring function used knowledge base embedding achieving competitive accuracy. also shown embedding model naturally propagate uncertainty compose relations together. embedding model also beneﬁts compositional training proposed furthermore demonstrated power gaussian attention model challenging question answering problem involves composition relations conjunction queries. future work includes experiments natural question answering datasets end-to-end training including entity extractor. antoine bordes jason weston ronan collobert yoshua bengio. learning structured embeddings knowledge bases. conference artiﬁcial intelligence number epfl-conf- antoine bordes nicolas usunier alberto garcia-duran jason weston oksana yakhnenko. translating embeddings modeling multi-relational data. advances neural information processing systems antoine bordes jason weston nicolas usunier. open question answering weakly supervised embedding models. joint european conference machine learning knowledge discovery databases springer maximilian nickel volker tresp hans-peter kriegel. three-way model collective learning multi-relational data. proceedings international conference machine learning richard socher danqi chen christopher manning andrew reasoning neural tensor networks knowledge base completion. advances neural information processing systems jason williams eslam kamal hani mokhtar ashour jessica miller geoff zweig. fast easy language understanding dialog systems microsoft language understanding intelligent service annual meeting special interest group discourse dialogue entity atomic relations atomic triplets relations path queries question answer pairs path queries types questions conjunctive queries question answer pairs conjunctive queries size vocabulary question template club play professional football team play football club play position play jersey number number wear nationality national team play country country soccer team based name player plays soccer club professional football team plays professionally player name player plays national football team name player plays plays soccer club based name soccer club country play professionally football club plays professional football team players play name soccer club player professional football team player plays name plays plays national team name footballer plays name players footballer plays name player player sample question club alan pulido play professional football team klaas huntelaar play position gonzalo higuain play samuel etoo luis suarez jersey number mario balotelli number shinji okazaki wear country thomas mueller nationality helder postiga country soccer team porto based plays professionally liverpool name player roma player iran name player italy name player plays goalkeeper plays forward soccer club based mexico name soccer club australia club edin dzeko plays country sime vrsaljko play professionally name soccer club player australia name soccer club player spain sample answer tigres uanl schalke napoli germany portugal portugal steven gerrard miralem pjanic masoud shojaei daniele rossi gianluiqi buffon raul jimenez cruz azul melbourne victory england italy crystal palace barcelona trained transgaussian model triplets paths worldcup dataset illustrated embeddings recall modeled every relation gaussian diagonal covariance matrix. shows learned variance parameters different relations. corresponds variances relation. columns permuted reveal block structure. ﬁgure every relation small variance dimensions. implies coordinates embedding space partitioned semantically coherent clusters represent particular attribute player verify further picked coordinates relation least variance projected embedding valid subjects objects relation dimensional subspace. fig. relation subjects objects simply translation projection corresponding subspace dimensional true relations requires larger dimension challenging visualize dimensions. relations large number unique objects plotted eight objects subjects clarity illustration. furthermore order elucidate whether limited capacity transgaussian embedding ability decode question expressed natural language evaluated test question-answer pairs using transgaussian embedding composed according ground-truth relations entities. results evaluated metrics sec. estimation conducted transe embeddings well. table results. compared table accuracy transgaussian higher atomic relations path queries lower conjunctive queries. natural query simple much room question-answering network improve upon combining relations according ground truth relations whereas query complex network could combine embedding creative overcome limitation. fact queries transgaussian perform well table pertain single relation country− composition relations plays country− plays club performance queries even ground truth table evaluation embeddings. evaluate embeddings feeding correct entities relations path conjunctive query embedding model using scoring function retrieve answers embedded knowledge base. knowledge base completion common task testing knowledge base models ability generalizing unseen facts. here apply transgaussian model knowledge completion task show competitive performance. tested subset wordnet released atomic triplets dataset originally created socher added path queries randomly sampled knowledge graph. build transgaussian model training triplets paths tested model link prediction task done socher done trained transgaussian atomic triplets trained transgaussian union atomic triplets paths. incorporate word embedding task entity assigned individual vector. without getting parameters tuned much transgaussian obtained accuracy comparable transe table plays club plays position aged wears number plays country country plays club− plays country− plays position− country− plays club country plays country− plays club", "year": 2016}