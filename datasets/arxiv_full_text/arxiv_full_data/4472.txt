{"title": "Nonlocal Low-Rank Tensor Factor Analysis for Image Restoration", "tag": ["cs.CV", "stat.ML"], "abstract": "Low-rank signal modeling has been widely leveraged to capture non-local correlation in image processing applications. We propose a new method that employs low-rank tensor factor analysis for tensors generated by grouped image patches. The low-rank tensors are fed into the alternative direction multiplier method (ADMM) to further improve image reconstruction. The motivating application is compressive sensing (CS), and a deep convolutional architecture is adopted to approximate the expensive matrix inversion in CS applications. An iterative algorithm based on this low-rank tensor factorization strategy, called NLR-TFA, is presented in detail. Experimental results on noiseless and noisy CS measurements demonstrate the superiority of the proposed approach, especially at low CS sampling rates.", "text": "low-rank signal modeling widely leveraged capture non-local correlation image processing applications. propose method employs low-rank tensor factor analysis tensors generated grouped image patches. low-rank tensors alternative direction multiplier method improve image reconstruction. motivating application compressive sensing deep convolutional architecture adopted approximate expensive matrix inversion applications. iterative algorithm based low-rank tensor factorization strategy called nlr-tfa presented detail. experimental results noiseless noisy measurements demonstrate superiority proposed approach especially sampling rates. image patches various image processing algorithms proposed investigate low-rank property image patch groups general methods ﬁrst select reference patch search similar patches across image form group. following this patches group vectorized stacked matrix. since patches similar constructed matrix low-rank property. performing low-rank model every patch image state-ofthe-art image restoration results achieved. common issue algorithms original two-dimensional patches vectorized construct group matrix loses spatial structure within image patch. propose tensor based algorithm retain structure still leveraging advantages low-rank patch models. tensor factorization †this work performed xinyuan zhang summer intern nokia bell labs ∗corresponding author. figure illustration nlr-tfa model. left image initial estimate using fast algorithm. tensor concatenated reference image patch similar patches. blue cubes represent full-rank tensors; green cubes represent low-rank tensors. right image reconstructed using nlr-tfa compressive sensing measurements rate notice proposed method restore rich semantic content structure image even sensing rate extremely low. methods offer useful learn latent structure complex multiway data used image processing tasks methods decompose tensor data factor matrices used latent feature representation objects tensor modes recently tensor approaches applied computer vision image denoising video denoising consider general image restoration problem speciﬁc applications compressive sensing using tensor factor analysis approach. particular instead vectorizing image patches impose low-rank image patch groups. practically tensor decomposition problems nphard. however real applications long tensor many components components adversarially chosen tensor decomposition computed polynomial time tensordecomposition problem seeks estimate coefﬁcients tensor adopting least-squares ﬁtness criterion problem compressive sensing signal acquisition technique enables sampling signal sub-nyquist rates reconstruction algorithm used recover original high-dimensional signal small number random linear measurements. taking compressive measurements viewed process linearly mapping n-dimensional signal vector m-dimensional measurement vector using measurement matrix cm×n i.e. since matrix rankdeﬁcient exists yields measurement paper compressive sensing rate deﬁned csr= m/n. recover searches vector possesses certain structure among vectors satisfy case sparse popular method solve optimization problem different identity matrix denoising problem diagonal matrix whose diagonal elements either keeping removing corresponding pixels becomes image inpainting cm×n compressive sensing problem. focus problem consider image reconstruction limited number compressive measurements framework proposed nlr-tfa method shown fig. recent interest using deep learning technique problems reconstruction network developed admm-net proposed former paper focused block-wise latter considered cs-mri contrast develop tensor-based framework general applications. further overcome large-scale matrix inversion propose pre-train deep convolutional neural network approximate expensive matrix inversion operator remainder paper organized follows. section presents background knowledge tensors section derives algorithm. extensive results provided section demonstrate superiority proposed algorithm relative leading approaches section concludes paper. tensors arrays indexed three indices. speciﬁcally matrices two-way arrays tensors threehigher-way arrays. sequel mainly focus -way tensors everything naturally generalizes higher-order tensors. rank- -way tensor ri×j×k outer product unit vectors zeros distance itself always found leading patch i.e. eventually tensors tensor corresponds reference patch. coordinates grouped patches also recorded later image aggregation. suppose size patch size generated tensor rm×n×k. shown grouped patches denoised low-rank approximation work low-rankness imposed tensor taking signiﬁcant tensor factors tensor decomposition selects signiﬁcant tensor factors therefore rank signiﬁcance tensor factors evaluated λr’s assumption grouped patches similar structure low-rank property ensures represented relatively low-rank tensor low-rank imposition shares spirit hard thresholding algorithm fig. impose rankness tensors size generated clean image using approach aggregate tensors back images. seen reconstructed images low-rank tensors accurately approximate original image. images aggregated rank- problem convex known formally basis pursuit denoising shown sufﬁciently sparse satisﬁes certain properties s-sparse signal accurately recovered random linear measurements equation equivalently translated following unconstrained optimization problem regularization parameter. various methods used solve minimization problem work adopt alternative direction multiplier method framework speciﬁcally consider application image beyond sparsity propose tensor factorization approach exploit high-order structure image patches seeking high reconstruction performance extremely e.g. csr<.. refer fig. example reconstructed image using proposed algorithm compared leading algorithms csr= propose model recovers compressively sensed images using low-rank tensor factor analysis admm. first generate tensor estimated image based patch grouping. impose rankness tensor tensor decomposition. low-rank tensor global objective function solved admm. steps iteratively performed satisfying criterion. complete algorithm shown algorithm patch-based low-rank tensor factorization given observation ﬁrst obtain estimated image using fast algorithm e.g. wavelet based algorithm denoising simply equal estimated image divided overlapping patches xp}. basic assumption underlying proposed approach sufﬁcient number similar patches found reference patch a.k.a nonlocal self-similarity prior reference patch perform k-nearest-neighbor search nonlocal similar patches across image form group xpk} number similar patches here euclidean distance pixel intensity used metric group patches. concatenating grouped patches third dimension ascending order euclidean distances generate tensor reference patch divide overlapping patches size form tensors size using patch block matching; decompose tensors using jenrich’s algorithm; impose rankness tensors generate rank- tensors; admm initialize update partial fourier transform matrix pretrained deep convolutional architectures term involves expensive matrix inversion makes direct computation impractical. inversion-free approach proposed addresses problem learning convolutional neural network approximate matrix inversion. note training neural network data-independent since dependent applying sherman-morrison-woodbury formula reduce matrix inversion smaller scale denotes results averaging similar patches corresponds image pixel location whose value number overlapping patches cover pixel location. matrix inverted large conjugate gradient descent usually applied adopt admm solve problem introducing auxiliary variable applying admm obtain global objective function lagrange multiplier penalty parameter. instead minimizing simultaneously admm decomposes problem subproblems minimizes w.r.t respectively. speciﬁcally optimization problem consists following iterations dimension approximate trainable deep convolutional neural network parameterized employed i.e. learned minimizing reconstruction losses auto-encoders shared weights computed directly sampled publicly available image datasets note pretrained different plugging learned obtain reusable term replacement cumbersome inversion matrix. hence updated equation solved transforming problem image space fourier space partial fourier transform matrix down-sampling matrix fourier transform matrix substituted conduct experiments noisy noiseless mearurements test images barbara boats cameraman foreman house lena monarch parrots. images resized since excellent results obtained rate large i.e. focus testing cases limited number measurements measurements generated pseudo-radial sampling test images fourier domain. unlike traditional random sampling schemes pseudo-radial sampling produces streaking artifacts difﬁcult remove. also perform experiments using random sensing matrix generated standard gaussian distribution. deep convolutional architecture pretrained sensing matrix used admm solve expensive matrix inversion. based recovery algorithm used initial image estimate. main parameters proposed method follows patch size number similar patches reference patch imposed rank regularization parameter admm parameter tuned separately sensing rate. practice found admm converges fast iterations performance gain mainly low-rank tfa. thus number outer-loop iterations number inner loop iterations experimental results noiseless measurements noisy measurements evaluated peak signal-to-noise ratio structural similarity compare results four competitive image restoration algorithms bmd-cs tval nlr-cs d-amp bmd-cs nonparametric method applies block matching ﬁltering algorithm compressive sensing. tval restores images combining classic augmented lagrangian multiplier method total-variation regularization. nlr-cs currently state-of-the-art method uses nonlocal low-rank regularization method along admm solve image problems. d-amp employs denoiser approximate message passing framework achieving state-ofthe-art performance noisy measurements especially high rates. source codes baseline methods downloaded respective author’s website structures reconstructed images. particular extremely nlr-tfa method effectively approximate original image methods nlr-cs reconstruct scratches next generate measurements sampling test images random gaussian sensing matrix. deep convolutional neural network pretrained using sensing matrix solve large-scale matrix inversion problem admm. compare results d-amp provides random gaussian sensing implementations sourcing code. reconstruction results shown table proposed method outperforms d-amp test images sensing rates. addition runtime algorithm using inversion-free approach also much faster using approaches inner-loop updates conjugate gradient demonstrates efﬁciency accuracy deep convolutional architecture approximating inversion matrix. patch size selection conducted experiments using different patch sizes. seen table smaller patch sizes lead better reconstruction results. adopt patch size foreman image psnr signiﬁcantly increased compared results larger patch sizes. similar scheme different rates i.e. table summarizes results proposed algorithm compared various inversion algorithms different values. bmd-cs state-of-the-art methods nlr-cs d-amp suffer extremely csr. increases nlr-cs yields signiﬁcant performance improvements leads results higher surpass methods. proposed method achieves best performance cases csr< average proposed nlr-tfa algorithm outperforms competing methods psnr gains nlr-tfa bmd-cs tval nlr-cs damp much respectively. furthermore observed average reconstruction psnr decreases decreases number nlr-cs d-amp respectively indicating proposed method stable i.e. limited number measurements. evaluate reconstruction visually examples restored foreman monarch images shown figs. evident method recovers best visual quality among competiting methods. large-scale sharp edges small-scale zero-mean gaussian noise noiseless measurements. test algorithms three levels noise standard deviation psnr comparison methods boats monarch images sensing rates respectively shown fig. proposed nlr-tfa outperforms competing methods levels noise. furthermore d-amp shows great robustness noise bmd-cs nlrcs relatively sensitive high-level noise. noise level increases reconstruction performance algorithm decreases slowly. shows proposed method robust noise. fig. show reconstructions boats images using various algorithms csr=. proposed nlr-tfa clearly yields best reconstruction robust noise. presented low-rank tensor-factor-analysisbased approach solve image-restoration problems. tensor generated concatenating similar patches estimated image exemplar patch. low-rank imposed tensors exploit non-local correlation high-order structure information tensor factorization. admm employed low-rank tensors either pretrained convolutional architecture fourier approaches solve matrix inversion. experimental results demonstrate proposed nlr-tfa method outperforms state-of-the-art algorithms image reconstruction sampling rates. demonstrated superiority model image three-way tensors also used depth polarization dynamic range future work includes extending model four-way tensors video hyperspectral image ﬁve-way tensors joint spectral-temporal conclusions found images well. addition running time model largely inﬂuenced tensor size. tensor size average running time csr=. minutes desktop .ghz ram.", "year": 2018}