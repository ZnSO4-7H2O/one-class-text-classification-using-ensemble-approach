{"title": "Kalman filter control in the reinforcement learning framework", "tag": ["cs.LG", "cs.AI", "I.2.6; I.2.8"], "abstract": "There is a growing interest in using Kalman-filter models in brain modelling. In turn, it is of considerable importance to make Kalman-filters amenable for reinforcement learning. In the usual formulation of optimal control it is computed off-line by solving a backward recursion. In this technical note we show that slight modification of the linear-quadratic-Gaussian Kalman-filter model allows the on-line estimation of optimal control and makes the bridge to reinforcement learning. Moreover, the learning rule for value estimation assumes a Hebbian form weighted by the error of the value estimation.", "text": "abstract. growing interest using kalman-ﬁlter models brain modelling. turn considerable importance make kalman-ﬁlters amenable reinforcement learning. usual formulation optimal control computed oﬀ-line solving backward recursion. technical note show slight modiﬁcation linear-quadratic-gaussian kalman-ﬁlter model allows on-line estimation optimal control makes bridge reinforcement learning. moreover learning rule value estimation assumes hebbian form weighted error value estimation. kalman ﬁlters various extensions well studied widely applied tools state estimation control. recently increasing interest kalman-ﬁlters kalman-ﬁlter like structures models neurobiological substrates. suggested kalman-ﬁltering occur sensory processing underlying computation hippocampus underlying principle control architectures detailed architectural similarities kalman-ﬁlter entorhinal-hippocampal loop well kalman-ﬁlters neocortical hierarchy described recently interplay dynamics kalman-ﬁlter-like architectures learning parameters neuronal networks promising aspects explaining known puzzling phenomena priming repetition suppression categorization well known kalman-ﬁlter provides on-line estimation state system. hand optimal control cannot computed on-line typically given backward recursion on-line parameter estimations without control aspects paper derive on-line control method kalman-ﬁlter achieve optimal performance asymptotically. slight modiﬁcation linearquadratic-gaussian kalman-ﬁlter model introduced treating model reinforcement learning problem. value iteration greedy action selection temporal diﬀerencing error. value iteration starts arbitrary initial cost-to-go function this control actions selected according current value function estimate value function updated according experience steps iterated. sake simplicity cost-to-go function updated using -step temporal diﬀerencing method. naturally substituted sophisticated methods like multi-step eligibility traces. error kalman-ﬁlter control problem slightly modiﬁed framework on-line control rule achieved. well-founded theory reinforcement learning ensures asymptotic optimality algorithm. described method highly extensible. straightforward generalizations cases e.g. extended kalman ﬁlters dynamics unknown parameters non-quadratic cost functions advanced algorithms e.g. eligibility traces. quadratic loss functions found learning hebbian weighted error value-estimation.", "year": 2003}