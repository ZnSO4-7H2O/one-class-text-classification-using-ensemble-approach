{"title": "LIDE: Language Identification from Text Documents", "tag": ["cs.CL", "cs.NE"], "abstract": "The increase in the use of microblogging came along with the rapid growth on short linguistic data. On the other hand deep learning is considered to be the new frontier to extract meaningful information out of large amount of raw data in an automated manner. In this study, we engaged these two emerging fields to come up with a robust language identifier on demand, namely Language Identification Engine (LIDE). As a result, we achieved 95.12% accuracy in Discriminating between Similar Languages (DSL) Shared Task 2015 dataset, which is comparable to the maximum reported accuracy of 95.54% achieved so far.", "text": "abstract—the increase microblogging came along rapid growth short linguistic data. hand deep learning considered frontier extract meaningful information large amount data automated manner. study engaged emerging ﬁelds come robust language identiﬁer demand namely language identiﬁcation engine result achieved accuracy discriminating similar languages shared task dataset comparable maximum reported accuracy achieved far. automatic language detection ﬁrst step toward achieving variety tasks like detecting source language machine translation improving search relevancy personalizing search results according query language providing uniform search multilingual dictionary tagging data stream twitter appropriate language etc. classifying languages belonging disjoint groups hard disambiguation languages originating source dialects still pose considerable challenge area natural language processing. regular classiﬁers based word frequency inadequate making correct prediction similar languages utilization state machine learning tools capture structure language become necessary boost classiﬁer performance. work took advantage recent advancement deep neural network based models showing stellar performance many natural language processing tasks build state language classiﬁer. past variety methods tried like naive bayes n-gram graph-based n-gram prediction partial matching linear interpolation post independent weight optimization majority voting combining multiple classiﬁers etc. best accuracy achieved still lower ninety percents. researchers worked various critical tasks challenging dimensions topic including limited supporting resource languages i.e. nepali urdu icelandic handling user-generated unstructured short texts i.e. microblogs building domain agnostic engine existing benchmarking solutions approach problem different ways logr adopts discriminative approaches regularized logistic regression textcat google recruits n-gram-based algorithm langid.py relies naive bayes classiﬁer multinomial event model. outstanding results time suggested cavnar trenkle became facto standard even today signiﬁcant ingredient method shown rank order statistic called place distance measure problem approach generated n-grams words requires tokenization. however many languages including japanese chinese word boundaries. considering japanese second frequent language used twitter need better approach scale solution languages. solution problem dunning came better approach incorporating byte level n-grams whole string instead char level n-grams words rigorous literature survey found prior study applied deep learning language identiﬁcation text. hand number studies applied deep learning identify language speech believe study ﬁrst literature published textual data means deep learning. data project work obtained discriminating similar language shared task instances language evaluation provided different world languages. dataset also consisted subset overall training data utilized hyper-parameter tuning. languages grouped shown table names groups frequently referred subsequent sections. entry dataset full sentence extracted journalistic corpora written languages tagged language group country origin. similar mixed language instance also provided applied t-sne algorithm visualize instances euclidean space feature extraction vectorized sentence -grams tokens delimited white space characters. fig. shows resulting plot. seen plot languages group overlap languages different groups linearly separable. dimensional visualization languages viewed www.youtube.com/watch?v=mhrdfcq. created baseline result training multinomial naive bayes model quick prototype runs fast known provide decent results ﬁeld text processing. done pre-processing text commonly done ﬁeld like stemming stop word removal believe could potentially remove important signatures particular language particularly language spoken geographically disconnected group people experimented word character n-grams. character n-grams turned particularly useful character level n-gram behaves quite differently word level n-grams shown fig. single characters carry little information therefore performance character n-gram improves quite sharply number characters increased saturating experimented character n-grams restricted word boundaries spanning across word boundaries. latter marginal performance boost cost longer training time memory pressure. word n-gram model peaks drops beyond that. higher order n-grams carry structure language become increasingly infrequent therefore models don’t always boost character level word level n-gram models show similar performance really excel certain languages poorly next tried regularized logistic regression character level n-gram performed little better word n-grams. fig. shows model able completely training performance validation plateaued close best performance obtained character -gram model includes n-grams n-grams truncated word boundaries words n-grams capture consecutive words. relaxing criterion signiﬁcantly increases size term frequency matrix pushes boundary computer memory improve performance fraction percent. approaches work really well distinguishing languages little common n-grams little overlap them. approach work well languages close share words them. therefore becomes necessary capture used subset overall training data hyper parameter selection. subset divided training data validation data. ﬁrst step process varied single parameter keeping constant. plots show performance resultant models validation dataset parameter changed. rnns special kind neural networks possess internal state virtue cycle hidden units. such rnns able record temporal dependencies among input sequence opposed machine learning algorithms inputs considered independent other. hence well suited natural language processing tasks successfully used applications like speech recognition hand writing recognition etc. recently rnns considered difﬁcult train problem exploding vanishing gradients makes difﬁcult learn long sequences input. methods like gradient clipping proposed remedy this. recent architectures like long short term memory gated recurrent unit also speciﬁcally designed around problem. experiments used single hidden layer recurrent neural networks used gated recurrent units. dropout deep neural networks large number parameters powerful machines extremely susceptible overﬁtting. dropout provides simple remedy problem randomly dropping hidden units example propagates plot fig. increasing number training epochs improves model performance certain stage plateaus. hence next stage tuning ﬁxed number training epochs using best values number hidden units dropout found above performed grid search combinations parameters. result grid search visualized fig. combinations gave best performance validation set. ﬁnal values chosen experimentation hidden units dropout avoid overﬁtting. training procedure ﬁnal model ensemble rnns built using different feature namely character -grams character -grams word unigrams. train models divided entire training data training validation set. trained measured performance model individually validation reported table ﬁnal classiﬁcation language group captured confusion matrix fig. quite evident results biggest challenge consistently posed classiﬁers distinguishing languages south western slavic group training revealed among words common since fig. clearly showed didn’t underﬁt training made sense augment training data three language categories. incorporated signiﬁcantly larger labeled data languages also downloaded newspaper articles classiﬁcation accuracy language group improve. looking closer external datasets revealed none words could uniquely associated three languages therefore additional data probably added noise signal. training rnns used python library called passage built theano. although library provides several tools text pre-processing including tokenization lacked ability generate character n-gram level features. therefore extend library custom character level feature generators. addition training neural networks cpus consumes time. hence experiments leveraged instances provided boost time required train model. table shows comparison models experimented with. surprising feature result individual models able beat performance models even though latter models minimal knowledge language structure. however created ensemble models turned best model crossed threshold ﬁrst time. noted particular n-gram model models m-grams however nature architecture combination n-grams cannot used lead overlapping sequence content network. since given n-gram captures limited information language natural ensemble n-gram models different values structure language captured multiple different levels. boost performance ensemble also attributed model combination aims achieve least good performance worst model ensemble. individual models make mistakes different examples therefore using ensemble able reduce variance. tried model combination strategies like median manual weighting building logistic regression classiﬁer rnns really helped optimal weight given individual model. could include models beyond character -gram ensemble memory limitation including model ensemble dialect language distinguished iberoromance language group lack support i.e. google always predicts portuguese sentences brazilian portuguese european portuguese. comparative test design queried test dataset shared task compared resulting predictions labels test-gold ﬁle. didn’t employ training session benchmarked solutions since solutions already trained claimed ready general purpose use. google provides language detection service languages. lide surpassed distinguishing south western slavic group. accuracy google translate particular group lower lide formed main difference overall accuracy lide google translate api. understand failure mechanism classiﬁer south western slavic language group classiﬁer best single models validation according table different fractions document failed classify correctly. example following document bosnian classiﬁer predicts language serbian usto osvrnuo ekonomsku situaciju kojoj veliki broj novinara potrazi poslom mizerne plae guranje etike strane profesije zapeak. classiﬁer usto noted prediction usto noted prediction full sentence fed. classiﬁer prediction different stages sentence scan plotted fig.. left panel fig. shows classiﬁer part thinks document actually last word sentence switched prediction think fact last word associated uniquely training corpus. bottom left panel fig. shows similar scenario case classiﬁer switched back forth couple times. ’confusion’ classiﬁer high right panel ﬁgure particular sentence made words phrases common three languages. believe correct classiﬁcation documents needs creation extra features based deeper understanding language group. another possible scenario classiﬁer struggle body text contains quotation different language. bottom right panel fig. shows scenario document serbian comment portuguese though cause eventual classiﬁcation failure. removing quotes document potential option also adverse effect quote language main document. presented deep neural network based language identiﬁcation scheme achieves near perfect accuracy classifying dissimilar languages accuracy highly similar languages. speciﬁcally languages western slavic slavic group posed highest challenge. expanding corpus languages using external sources help much mainly n-grams words unique certain languages ingested expanded part corpus. relied ensemble models discover structure unique speciﬁc language could engineer additional feature lack knowledge speciﬁc languages. point think improvement achieved designing rule based features talking language experts native speakers. would like thank david jurgens department computer science stanford university helping initial idea dataset previous research junjie department computational mathematical engineering stanford university mentoring insightful comments polished outcome study educate program providing credits computing resources microsoft azure research program providing azure credits full featured computing resources lastly google yandex basis tech providing free access language detection apis benchmarking analysis. rosette language supports languages excluding bosnian. since bosnian excluded language inventory rosette discarded bosnian sentences queried remaining languages. apart bosnian sentences rosette showed highly similar accuracy characteristics yandex translator api. langid.py off-the-shelf language identiﬁcation tool considered cornerstone literature langid.py shared similar accuracy characteristics yandex translator subtle difference langid.py came slightly higher accuracy astronesian south western slavic groups. noted langid.py software owned used competitors namely unimelb-nlp. merrienboer gulcehre bougares schwenk bengio learning phrase representations using encoderdecoder statistical machine translation arxiv available http//arxiv.org/abs/. stiller g¨ade petras ambiguity queries challenges query language detection clef labs workshops notebook papers available http//clef. org/resources/proceedings/cleflabs{ }submission{ }.pdf nguyen word level language identiﬁcation online multilingual communication vol. october baldwin langid. off-the-shelf language identiﬁcation tool proceedings system demonstrations july carter weerkamp tsagkias microblog language identiﬁcation overcoming limitations short unedited idiomatic text language resources evaluation vol. available http//www.springerlink.com/ index/./s---y bergsma mcnamee bagdouri fink wilson identiﬁcation creating language-speciﬁc twitter language collections proceedings workshop language social media naacl-hlt’ available http//www.aclweb.org/anthology-new/w/ w/w-.pdf half japanese lopez-moreno gonzalez-dominguez plchot martinez gonzalez-rodriguez moreno automatic language identiﬁcation using deep neural networks icassp- montavon deep learning spoken language identiﬁcation nips workshop deep learning speech recognition related applications pascanu mikolov bengio difﬁculty training recurrent neural networks international conference machine learning available http//jmlr.org/proceedings/papers/v/pascanu.pdf", "year": 2017}