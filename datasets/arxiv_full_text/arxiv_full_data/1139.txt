{"title": "Adversarial Training for Probabilistic Spiking Neural Networks", "tag": ["stat.ML", "cs.LG", "cs.NE", "eess.SP"], "abstract": "Classifiers trained using conventional empirical risk minimization or maximum likelihood methods are known to suffer dramatic performance degradations when tested over examples adversarially selected based on knowledge of the classifier's decision rule. Due to the prominence of Artificial Neural Networks (ANNs) as classifiers, their sensitivity to adversarial examples, as well as robust training schemes, have been recently the subject of intense investigation. In this paper, for the first time, the sensitivity of spiking neural networks (SNNs), or third-generation neural networks, to adversarial examples is studied. The study considers rate and time encoding, as well as rate and first-to-spike decoding. Furthermore, a robust training mechanism is proposed that is demonstrated to enhance the performance of SNNs under white-box attacks.", "text": "training methods snns typically assume deterministic non-linear dynamic models spiking neurons either motivated biological plausibility spiketiming-dependent plasticity rule attempt mimic operation anns associated learning rules references therein). deterministic models known limited expressive power especially pertains prior domain knowledge uncertainty deﬁnition generic queries tasks. training probabilistic models snns recently investigated e.g. using variational inference principles. paper ﬁrst time sensitivity snns trained studied white-box adversarial attacks robust training mechanism proposed demonstrated enhance performance snns adversarial examples. speciﬁcally focus two-layer consider rate time encoding well rate ﬁrst-to-spike decoding results illuminate sensitivity snns adversarial example different encoding decoding schemes effectiveness robust training methods. rest paper organized follows. sec. describe architecture two-layer generalized linear model neuron well information encoding decoding mechanisms. design adversarial perturbations covered sec. robust training presented sec. sec. presents numerical results closing remarks given sec. abstract—classiﬁers trained using conventional empirical risk minimization maximum likelihood methods known suffer dramatic performance degradations tested examples adversarially selected based knowledge classiﬁer’s decision rule. prominence artiﬁcial neural networks classiﬁers sensitivity adversarial examples well robust training schemes recently subject intense investigation. paper ﬁrst time sensitivity spiking neural networks third-generation neural networks adversarial examples studied. study considers rate time encoding well rate ﬁrst-to-spike decoding. furthermore robust training mechanism proposed demonstrated enhance performance snns white-box attacks. classiﬁcation accuracy artiﬁcial neural networks trained large data sets problem domain attained super-human levels many tasks including image identiﬁcation nevertheless performance classiﬁers trained using conventional empirical risk minimization maximum likelihood known decrease dramatically evaluated examples adversarially selected based knowledge classiﬁer’s decision rule mitigate problem robust training strategies aware presence adversarial perturbations shown improve accuracy classiﬁers including anns tested adversarial examples anns known energy-intensive hindering implementation energy-limited processors mobile devices. despite recent industrial efforts around production energy-efﬁcient chips anns energy efﬁciency human brain anns remains signiﬁcant promising alternative paradigm offered spiking neural networks synaptic input neuronal output signals sparse asynchronous binary spike trains unlike anns snns hybrid digital-analog machines make temporal dimension neutral substrate computing means encode process information denote respective number basis functions; basis vectors; {wjik} {vik} learnable weights kernels respectively. experiments discussed sec. adopt raised cosine basis functions introduced information decoding also consider alternative decoding methods namely rate decoding ﬁrst-to-spike decoding. rate decoding rate decoding decoding carried selecting output neuron largest number spikes. first-to-spike decoding ﬁrst-tospike decoding class corresponds neuron spikes ﬁrst selected. training conventional training performed differently rate ﬁrst-to-spike decoding methods brieﬂy reviewed next. postsynaptic neuron corresponding correct label assigned desired output spike train containing number spikes all-zero vector assigned postsynaptic neurons. using criterion hence maximizes log-probabilities desired output spikes given input spike trains xnx}. log-likelihood function given training example written network architecture consider problem classiﬁcation using two-layer illustrated fig. fully connected presynaptic neurons input sensory layer neurons output layer. output neuron associated class. order feed input example e.g. gray scale image encoded discrete-time spike trains samples. input spike trains postsynaptic neurons output discrete-time spike trains. decoder selects image class basis spike trains emitted output neurons. rate encoding conventional rate encoding method entry input signal converted discrete-time spike train generating independent identically distributed bernoulli vector. probability generating proportional value entry. experiments sec. gray scale images usps dataset pixel intensities normalized yield proportional spike probability time encoding time encoding method entry input signal converted spike train spike whose timing depends entry value. particular assuming intensity-to-latency encoding spike timing time interval depends linearly entry value maximum value yields spike ﬁrst time sample minimum value mapped spike last time sample neuron model relationship input spike trains presynaptic neurons output spike train postsynaptic neuron follows bernoulli canonical link function elaborate denote binary signal emitted j-th presynaptic i-th postsynaptic neurons respectively time also vector samples spiking process presynaptic neuron time interval vector contains samples spiking process neuron interval membrane potential postsynaptic neuron time given vector deﬁnes synaptic kernel applied synapse presynaptic neuron postsynaptic neuron feedback kernel bias parameter. note denote lengths respectively. vector variable parameters includes bias parameters deﬁne ﬁlters discussed below. denotes number non-zero elements perturbation controls adversary strength. particular adversary allowed remove spikes fraction input samples i.e. samples input neuron. constraint problem given binary perturbations i.e. attacks since spikes added; {−}nx remove attacks; {±}nx attacks. exact solution problem requires exhaustive search possible perturbations samples. worst case attacks resulting search space hence exponential therefore resort greedy search method. detailed algorithm steps method looks best spike remove depending attack type. reduce complexity searching among ﬁrst samples across input neurons. results complexity step algorithm order increase robustness trained adversarial examples section propose robust training procedure. accordingly manner similar sgd-based training phase training example first-to-spike decoding ﬁrst-to-spike decoding class corresponds neuron spikes ﬁrst selected. criterion hence maximizes probability ﬁrst spike output neuron corresponding correct label. logarithm probability given example written work consider white-box attacks based full knowledge model i.e. parameter vector well encoding decoding strategies. accordingly given example adversarial spike train xadv obtained perturbed version original input perturbation selected cause classiﬁer likely predict incorrect label sufﬁciently small. consider following types perturbations remove attack spikes removed input attack spikes added input flip attack spikes added removed. size disturbance measured attacks number spikes added and/or removed. mathematically expressed hamming distance order select adversarial perturbation input consider maximization likelihood given incorrect target class according effective choose target class class least likely given model mathematically given training example least likely class obtained solving problem substituted adversarial example xadv obtained algorithm current iterate training algorithm detailed algorithm note that robust training algorithm parameterized determine parameters assumed adversary training. section numerically study performance described probabilistic adversarial attacks. standard usps dataset input data. result input neuron pixel images. unless stated otherwise focus solely classes assume worst-case adversary test phase. rate decoding desired spike train spike every three zeros. applied training epochs early stopping used schemes. holdout validation training samples applied select constant learning rate model parameters randomly initialized uniform distribution ﬁrst evaluate sensitivity different encoding decoding schemes adversarial examples obtained explained sec. iii. reference consider also perturbations obtained randomly uniformly adding removing ﬂipping spikes. fig. illustrates test accuracy adversarial random perturbations performing standard training. accuracy plotted versus adversary’s power assuming rate encoding rate ﬁrst-to-spike decoding rules. results highlight notable difference performance degradation caused random perturbations adversarial attacks. particular adversarial changes cause signiﬁcant drop classiﬁcation accuracy even small attacks vulnerable rate decoding remove spike attacks. resilience ﬁrst-tospike decoding interpreted consequence fact log-likelihood unlike rate decoding associates multiple outputs correct class namely correct neuron spiking ﬁrst. nevertheless removing properly selected spikes deleterious ﬁrst-to-spike decoding prevent spiking correct neuron. comparison rate time encoding terms sensitivity adversarial examples considered fig. under assumption ﬁrst-to-spike decoding. time encoding seen signiﬁcantly less resilient rate encoding. fact time encoding form considered intensity-to-latency encoding associated single spike input neuron easily made ineffective removing selected spikes. evaluate impact robust adversarial training compared standard fig. plot test accuracy case remove attacks adversarial training also focus solely classes recall adversarial training scheme parametrized time support attacks considered training power observed robust training signiﬁcantly improve robustness classiﬁer even equal value used attacker test phase. furthermore increasing enhances robustness trained cost higher computational complexity. instance attacker test phase i.e. ﬂips conventional achieves accuracy adversarial training achieves accuracy finally results show classiﬁer remains resilient type attacks despite trained assuming attack. finally conditions fig. study effect limiting power adversary assumed training considering assume time encoding rate decoding. observed robust training still improve robustness classiﬁer even training. instance attacker test phase i.e. ﬂips conventional achieves accuracy adversarial training achieves accuracy levels respectively. paper studied ﬁrst time sensitivity probabilistic two-layer adversarial perturbations. considered rate time encoding well rate ﬁrst-to-spike decoding. proposed mechanisms build adversarial examples well robust training method increases resilience snn. additional work needed order generalize results multi-layer networks. ranjan sankaranarayanan bansal bodla j.-c. chen patel castillo chellappa deep learning understanding faces machines good better humans ieee signal process. mag. vol. stromatias soto serrano-gotarredona linaresbarranco event-driven classiﬁer spiking neural networks synthetic dynamic vision sensor data front neurosci vol.", "year": 2018}