{"title": "A Simple Exponential Family Framework for Zero-Shot Learning", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "We present a simple generative framework for learning to predict previously unseen classes, based on estimating class-attribute-gated class-conditional distributions. We model each class-conditional distribution as an exponential family distribution and the parameters of the distribution of each seen/unseen class are defined as functions of the respective observed class attributes. These functions can be learned using only the seen class data and can be used to predict the parameters of the class-conditional distribution of each unseen class. Unlike most existing methods for zero-shot learning that represent classes as fixed embeddings in some vector space, our generative model naturally represents each class as a probability distribution. It is simple to implement and also allows leveraging additional unlabeled data from unseen classes to improve the estimates of their class-conditional distributions using transductive/semi-supervised learning. Moreover, it extends seamlessly to few-shot learning by easily updating these distributions when provided with a small number of additional labelled examples from unseen classes. Through a comprehensive set of experiments on several benchmark data sets, we demonstrate the efficacy of our framework.", "text": "abstract. present simple generative framework learning predict previously unseen classes based estimating class-attribute-gated class-conditional distributions. model class-conditional distribution exponential family distribution parameters distribution seen/unseen class deﬁned functions respective observed class attributes. functions learned using seen class data used predict parameters class-conditional distribution unseen class. unlike existing methods zero-shot learning represent classes ﬁxed embeddings vector space generative model naturally represents class probability distribution. simple implement also allows leveraging additional unlabeled data unseen classes improve estimates class-conditional distributions using transductive/semi-supervised learning. moreover extends seamlessly few-shot learning easily updating distributions provided small number additional labelled examples unseen classes. comprehensive experiments several benchmark data sets demonstrate efﬁcacy framework. problem learning predict unseen classes also popularly known zeroshot learning important learning paradigm refers problem recognizing objects classes seen training time especially relevant learning in-the-wild scenarios concepts need discovered on-the-ﬂy without access labelled data novel classes/concepts. tremendous amount interest developing methods learn robust scalable manner even amount supervision classes interest relatively scarce. large body existing prior work based embedding data semantic vector space distance based methods applied likely class represented point semantic space however limitation methods class represented ﬁxed point embedding space adequately account intra-class variability provide detailed overview existing work related work section. another limitation existing methods usually lack proper generative model data. generative model several advantages example data different types modeled principled using appropriately chosen class-conditional distributions; unlabeled data seamlessly integrated parameter estimation leading transductive/semi-supervised estimation procedure useful amount labeled data seen classes small distributions seen unseen classes different rich body work frequentist bayesian learning generative models brought bear parameter estimation process. motivated desiderata present generative framework zero-shot learning. framework based modelling class-conditional distributions seen well unseen classes using exponential family distributions conditioning parameters distributions respective class-attribute vectors linear/nonlinear regression model one’s choice. regression model allows predict parameters class-conditional distributions unseen classes using class attributes enabling perform zero-shot learning. addition generality modelling ﬂexibility framework another appealing aspects simplicity. contrast various state-of-the-art methods framework simple implement easy extend. particular show parameter estimation framework simply reduces solving linear/nonlinear regression problem closed-form solution exists. moreover extending framework incorporate unlabeled data unseen classes small number labelled examples unseen classes i.e. performing few-shot learning also remarkably easy framework models classconditional distributions using exponential family distributions conjugate priors. zero-shot learning assume total seen classes unseen classes. labelled training examples available seen classes. test data usually assumed come unseen classes although experiments also evaluate model setting test data could come seen unseen classes setting known generalised zero-shot learning take generative modeling approach problem model classconditional distribution observation seen/unseen class using exponential family distribution natural parameters given test example class predicted ﬁnding class likely ﬁnding class largest posterior probability given however requires ﬁrst estimating parameters {θc}s+u given labelled training data class modelled exponential family distribution straightforward estimate model parameters using maximum likelihood estimation maximum-a-posteriori estimation using fully bayesian inference however since labelled training examples unseen classes cannot estimate parameters {θc}s+u c=s+ classconditional distributions unseen classes. address issue learn model allows predict parameters class using attribute vector class gating scheme basically deﬁned linear/nonlinear regression model attribute vector parameters. common practice attribute vector class derived human-provide description class obtained external source wikipedia form word-embedding class. assume class-attribute class vector size class-attribute classes denoted {ac}s+u assume regression model class-attribute vector parameters class particular assume class-attribute vector mapped function generate parameters class-conditional distribution class follows note function could consist multiple functions consists multiple parameters. concereteness also simplify rest exposition focus case class-conditional distribution dimensional gaussian deﬁned mean vector p.s.d. covariance matrix further assume diagonal matrix deﬁned diag. note also assume full covariance matrix signiﬁcantly increase number parameters estimated. model note equations deﬁne regression models. ﬁrst regression model deﬁned function input output. second input output. goal regression model deﬁned learn functions available training data. note form functions modelling choice chosen appropriately. consider linear well nonlinear functions. using available training data seen classes form empirical estimates parameters respective class-conditional distributions using mle/map estimation. note that since framework generative labeled well unlabeled data seen classes used form empirical estimates reliable even seen class small number labeled examples. given estimates seen classes nonlinear model nonlinear case assume inputs {ac}s mapped kernel induced space kernel function associated nonlinear mapping case using representer theorem solution regression models note mappings computed explicitly since learning nonlinear regression model requires products nonlinear mappings classes ther exponential family distributions although illustrated framework taking example gaussian class-conditional distributions framework readily generalizes case distributions modelled using exponential family distribution. estimation problems solved similar gaussian case basic recipe remaining same form empirical estimates parameters {ˆθc}s seen classes using available seen class data learn linear/nonlinear regression model class-attributes additional modeling ﬂexibility especially remarkable aspect generative framework easy implement since linear model well nonlinear model closed-form solutions given respectively block-diagram describing framework shown figure note another appealing aspect framework modular architecture blocks figure make suitable method one’s choice. fig. block-diagram framework. denotes seen class data denotes seen class attributes; denotes unseen class attributes; denotes estimated seen class parameters; denotes estimated unseen class parameters. last stage transductive/few-shot reﬁnement optional procedure described section relies seen class data gaussian case seen class data used form empirical estimates parameters class-conditional distributions seen classes estimates used learn linear/nonlinear regression functions functions ﬁnally used compute parameters c=s+ class-conditionals unseen classes. call setting inductive setting. note procedure make data unseen classes. sometimes access unlabeled data unseen classes. generative framework makes easy leverage unlabeled data unseen classes improve upon estimates c=s+ classconditional distributions. framework done settings transductive semi-supervised leverage unlabeled data unseen classes slightly different ways. unlabeled data unseen class test data itself call transductive setting. unlabeled data unseen classes different actual unseen class test data call semi-supervised setting. either setting expectation-maximization based procedure alternates inferring labels unlabeled examples unseen classes using inferred labels update estimates parameters c=s+ distributions unseen classes. case class-conditional distribution gaussian procedure equivalent estimating gaussian mixture model using unlabeled data {xn}nu unseen classes. initialized using estimates c}s+u c=s+ obtained inductive procedure section note mixture components corresonds unseen class. note procedure applied even class-conditional distribution exponential family distribution gaussian. steps resulting mixture model straightforward case well. step simply require gausian likelihood replaced corresponding exponential family distribution’s likelihood. step require exponential family distribution’s parameters closed-form solutions. few-shot learning assume small number labeled examples also available unseen classes generative aspect framework along fact data distribution exponential family distribution conjugate prior parameters makes convenient model extended setting. outputs c=s+ generative zero-shot learning model naturally serve hyper-parameters conjugate prior parameters class-conditional distributions unseen classes updated given small number labeled examples unseen classes. example gaussian case conjugacy able update estimates c=s+ particularly appealing aspect few-shot learning model outlined also updated online manner labelled examples become available unseen classes without re-train model scratch using data. earliest works based predicting attributes example followed related line work based models assume data class mapped class-attribute space seen/unseen class also represented point mapping learned using various ways linear models feed forward neural networks convolutional neural networks. predicting label novel unseen class example involves mapping space ﬁnding closest unseen class. work aimed improving semantic embeddings concepts/classes. example proposed model incorporate relational information concepts. another recent work proposed model improve semantic embeddings using metric learning formulation. complementary line work semantic embedding methods based reverse mapping i.e. mapping class-attribute observed feature space contrast semantic embedding methods assume classes collapsed onto single point framework offers considerably ﬂexibility modelling class using distribution. makes model suitable capturing intra-class variability simple point-based embedding models incapable handling. another popular approach based modelling unseen class linear/convex combination seen classes abstract basis classes latter class methods particular seen special case framework since linear model view columns regression weights representing basis classes. note however model regression weights parameter class-conditional distribution allowing considerably ﬂexible. moreover framework also signiﬁcantly different ways fully generative framework ability incorporate unlabeled data performing few-shot learning ability model different types data using appropriate exponential family distribution. important issue domain shift problem arise seen unseen class come different domains. situations standard models tend perform badly. somewhat alleviated using additional unlabeled data unseen classes. provide dictionary learning based approach learning unseen class classiﬁers dictionary adapted unseen class domain. dictionary adaptation facilitated using unlabeled data unseen classes. another related work leverage unlabeled data transductive framework handle domain shift problem. note framework robust domain shift problem ability incorporate unlabeled data unseen classes experimental results corroborate this. semi-supervised learning also used improve semantic embedding based methods. provide semi-supervised method leverages prior knowledge improving learned embeddings. another recent work present model incorporate unlabeled unseen class data setting unseen class represented linear combination seen classes. provide another approach motivated applications computer vision jointly facilitates domain adaptation attribute space visual space. another semi-supervised approach presented combines semisupervised classiﬁcation model observed classes unsupervised clustering model unseen classes together address zeroshot multi-class classiﬁcation. contrast models mechanism incorporating unlabeled data model-speciﬁc framework offers general approach this also simple implement. moreover large-scale problems also leverage efﬁcient solvers estimating regression coefﬁcients associated class-conditional distributions. evaluate generative framework zero-shot learning several benchmark data sets compare number state-ofthe-art baselines. conduct experiments various problem settings including standard inductive zero-shot learning transductive zero-shot learning few-shot learning report experimental results following benchmark data sets animal attribute data contains images seen classes unseen classes class humanprovided binary/continuous -dimensional class-attribute vector continuous class-attributes since prior works found discriminative power. caltech-ucsd birds-- cub- data contains images seen classes unseen class image binary -dimensional class-attribute vector specifying presence absence various attribute image attribute vectors images class averaged construct continuous class-attribute vector train/test split data used attribute data contains images seen classes unseen classes image described -dimensional binary class-attribute vector. like cub- data average attribute vectors images class continuous attribute vector train/test split data used image features considered googlenet features vgg- features found approach works better vgg-. state-of-the-art baselines compare experiments vgg- features googlenet features nonlinear variant model quadratic kernel. experiments include transductive setting also unlabeled test data learning unseen class parameters. note setting access information unseen class; however unlabeled data. generalized whereas standard assumes test data unseen classes generalized assumes test data unseen well seen classes. usually challenging setting existing methods known biased towards predicting seen classes. standard train/test split given data description section. selecting hyperparameters divide train train validation set. model hyper-parameter tune using validation dataset. seen classes random selection classes used training classes used validation set. cub- seen classes used training rest used validation set. similarly dataset seen classes used training rest used validation set. crossvalidation validation choose best hyperparameter data testing unseen classes. inductive table- shows results inductive setting. results various baselines taken corresponding papers. shown table- cub- models perform better state-of-the-art methods. model marginally lower test accuracy compared best performing baseline however also average improvement data sets compared overall best baseline among baselines using vgg- features model achieves relative improvement best baseline cub- data considered difﬁcult data many ﬁne-grained classes. table accuracy different type images features. deep features like alexnet googlenet etc. bottom deep vgg- features. indicates result reported. contrast models embed test examples semantic space similar class euclidean distance based nearest neighbor search models based computing similarity scores seen unseen classes models ﬁnding most probable class corresponds computing distance test example distribution. naturally takes account shape spread class-conditional distribution. explains favourable performance model compared methods. transductive setting transductive setting follow procedure described section estimate parameters class-conditional distribution unseen class. learning parameters probable class test example evaluating probability unseen class distribution assign class largest probability. table- compare results transductive setting state-of-the-art baselines designed transductive setting. addition accuracy also report precision recall results model baselines table- models outperform baselines data sets. also comparing inductive setting results presented table accuracy obtained transductive setting results reported using feature. average precision recall dataset standard daviation iteration. indicates result reported original paper. table precision recall scores obtained transductive setting results reported using vgg- features. average precision recall dataset standard daviation iteration. note precision recall scores available romera al.+ zhang next perform experiment few-shot learning setting provide model small number labelled examples unseen classes. experiment follow procedure described section learn parameters class-conditional distributions unseen classes. particular train inductive model reﬁne learned model using small number labelled examples unseen classes effect knowledge transfer seen classes multiclass baseline provided number labelled examples unseen class. experiment vary number labelled examples unseen classes figure- also compare standard access labelled examples unseen classes. results shown table- figure-. shown table- figure- classiﬁcation accuracy unseen classes shows signiﬁcant improvement standard inductive even additional labelled examples class. also observe few-shot learning method outperform multiclass relies labelled data unseen classes. demonstrates advantage knowledge transfer seen class data. ﬁnally perform experiment challenging generalized few-shot learning setting setting assumes test examples come seen well unseen classes. setting known notoriously hard setting although models tend well predicting test examples seen classes performance correctly predicting unseen class example poor since trained models heavily biased towards predicting seen classes. mitigate issue could labelled examples unseen classes therefore perform similar experiment section table- show results model classifying unseen class test examples setting. shown table- model’s accuracies generalized task improve gets labelled examples unseen classes. however still outperformed standard multiclass svm. better performance attributed fact biased towards seen classes since classiﬁer class learned independently. ﬁndings also corroborated recent work generalized suggest need ﬁnding robust ways handle setting. leave direction investigation possible future work. presented ﬂexible generative framework zero-shot learning based modelling seen/unseen class using exponential family class-conditional distribution. contrast semantic embedding based methods zero-shot learning model class point latent space approach models class distribution parameters class-conditional distribution functions respective class-attribute vectors. generative framework allows learning functions easily using seen class training data especially appealing aspect framework simplicity modular architecture allows using variety algorithms building blocks. showed generative framework admits natural extensions related problems transductive zero-shot learning few-shot learning. particularly easy implement scale large number classes using advances large-scale regression. generative framework also extended jointly learn class attributes external source data interesting direction future work. finally although considered point estimation parameters class-conditional distributions also possible take fully bayesian approach learning distributions. leave possibility direction future work. acknowledgements work supported grant tower research deep singh daljeet kaur fellowship research-i foundation kanpur. vinay verma acknowledges support visvesvaraya ph.d. fellowship.", "year": 2017}