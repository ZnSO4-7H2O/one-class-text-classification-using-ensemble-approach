{"title": "Training Input-Output Recurrent Neural Networks through Spectral Methods", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We consider the problem of training input-output recurrent neural networks (RNN) for sequence labeling tasks. We propose a novel spectral approach for learning the network parameters. It is based on decomposition of the cross-moment tensor between the output and a non-linear transformation of the input, based on score functions. We guarantee consistent learning with polynomial sample and computational complexity under transparent conditions such as non-degeneracy of model parameters, polynomial activations for the neurons, and a Markovian evolution of the input sequence. We also extend our results to Bidirectional RNN which uses both previous and future information to output the label at each time point, and is employed in many NLP tasks such as POS tagging.", "text": "consider problem training input-output recurrent neural networks sequence labeling tasks. propose novel spectral approach learning network parameters. based decomposition cross-moment tensor output non-linear transformation input based score functions. guarantee consistent learning polynomial sample computational complexity transparent conditions non-degeneracy model parameters polynomial activations neurons markovian evolution input sequence. also extend results bidirectional uses previous future information output label time point employed many tasks tagging. learning sequential data widely encountered domains natural language processing genomics speech recognition video processing ﬁnancial time series analysis recurrent neural networks ﬂexible class sequential models memorize past information selectively pass across sequence steps multiple scales. however training rnns challenging practice backpropagation suﬀers exploding vanishing gradients length training sequence grows. overcome this either rnns trained short sequences incorporate complex architectures long short-term memories detailed overview rnns figure contrasts feedforward neural network memory. theoretical front understanding rnns best rudimentary. current techniques tractable analyze highly non-linear state evolution rnns. analysis backpropagation also intractable non-convexity loss function general reaching global optimum hard. here take ﬁrst steps towards addressing challenging issues. design novel spectral methods training io-rnn brnn models. consider class input-output io-rnn models input sequence output label useful sequence labeling tasks many applications parts speech tagging named-entity recognition motif ﬁnding protein analysis action recognition videos addition also consider extension io-rnn viz. bi-directional brnn ﬁrst proposed schuster paliwal includes classes hidden neurons ﬁrst class receives recurrent connection previous states second class receives next steps. figure brnn useful tasks tagging previous next words sentence eﬀect labeling current word. paper develop novel spectral methods training io-rnn brnn models. spectral methods previously employed unsupervised learning range latent variable models hidden markov models topic models network community models idea decompose moment matrices tensors using computationally eﬃcient algorithms. recovered components tensor decomposition yield consistent estimates model parameters. however direct application techniques ruled non-linearity activations rnn. recently janzamin derived framework training input-output models supervised framework. based spectral decomposition moment tensors obtained certain non-linear transformations input. non-linear transformations take form score functions depend generative models input. provides approach transferring generative information obtained unsupervised learning discriminative training labeled samples. based aforementioned approach janzamin provided guaranteed risk bounds training two-layer feedforward neural network models polynomial sample computational bounds. conditions obtaining risk bounds mild small approximation error target function given class neural networks generative input model continuous distribution general sigmoidal activations neurons. propose spectral approaches training io-rnns classiﬁcation regression settings. previous score function approach training feedforward networks immediately extend non-trivial challenges non-linearity propagated along multiple steps sequence two-layer feedforward network non-linearity applied input. immediately clear untangle non-linearities obtain guaranteed estimates network weights. learning bidirectional rnns even challenging since recursive non-linearities applied directions assumption i.i.d. input output training samples longer applicable analyzing concentration bounds samples generated non-linear state evolution challenging. address challenges concretely paper. main contributions novel approaches training input-output bidirectional models using tensor decomposition methods guaranteed recovery network parameters polynomial computational sample complexity transparent conditions successful recovery based nondegeneracy model parameters bounded evolution hidden states. score function transformations training input-output neural networks arbitrary input computationally hard. hand show training becomes tractable spectral methods input generated probabilistic model continuous state space. paper considered study takes uncover nonlinear dynamics system. since learning arbitrary input extremely challenging seek discover functions/information input problem becomes tractable. although diﬀers usual approach training io-rnns promising ﬁrst step towards demystifying widely-used models. show knowledge input distribution solve extremely hard problem training nonlinear iornns. assume knowledge score function forms correspond normalized derivatives input probability density function instance input standard gaussian score functions given hermite polynomials. many unsupervised approaches estimating score function eﬃciently appendix estimate score function need estimate density distinction especially crucial models normalizing constant partition function intractable compute. guarantees derived estimating score functions many ﬂexible model classes inﬁnite dimensional exponential families addition many settings control designing input distribution method directly applicable. assume markovian model input sequence continuous state space. markovian model score function depends markov kernel compact representation seen section method readily leads higher order markov chains. main paper discuss ﬁrst order markov chain notation simplicity discuss extension appendix tensor decomposition form cross-moments output label score functions input. vector input ﬁrst order score function vector second order matrix higher orders corresponds tensors. hence empirical moments tensors perform tensor decomposition obtain rank- components. eﬃcient algorithms tensor decomposition proposed before based simple iterative updates tensor power method simple manipulations components provide estimates network parameters models. overall algorithm involves simple linear multilinear steps embarrassingly parallel practical implement recovery guarantees guarantee consistent recovery polynomial computational complexity. consider realizable setting samples generated io-rnn brnn following transparent conditions hidden layer neurons polynomial activation function markovian input sequence full rank weight matrices input hidden output layers spectral norm bounds weight matrices ensure bounded state evolution. currently question approximation bounds ﬁxed number neurons satisfactorily resolved valid ﬁrst consider realizeable setting complex problem. polynomial activations departure usual sigmoidal units also capture nonlinear signal evolution employed diﬀerent applications e.g. markovian assumption input limits extent dependence allows derive concentration bounds empirical moments. full rank conditions weight matrices imply non-degeneracy neural representation weights neurons cannot linearly combine generate weight another neuron. conditions previously derived spectral learning hmms latent variable models moreover easily relaxed considering higher order moment tensors relevant want neurons input dimension network. rank assumption output weight matrix implies vector output suﬃcient dimensions i.e. suﬃcient number output classes. relaxed scalar output details given appendix spectral norm condition weight matrices arises analysis concentration bounds empirical moments. since assume polynomial state evolution important ensure bounded values hidden states entails bound spectral norm weight matrices. employ concentration bounds functions markovian input combine matrix azuma’s inequality obtain concentration empirical moment tensors. implies learning rnns polynomial sample complexity. related work following works directly relevant paper. spectral approaches sequence learning previous guaranteed approaches sequence learning mostly focus class hidden markov models anandkumar provide tensor decomposition method learning parameters non-degeneracy conditions similar ours. framework extended general hmms relationship hidden observed variables modeled linear non-linear. however io-rnn inputs outputs helpful handling non-linearities. input-output sequence models rich models based rnns employed practice wide range applications. lipton provides nice overview various models. balduzzi ghifari recently apply physics based principles design rnns stabilizing gradients getting better training error. however rigorous analysis techniques lacking. denote inner product vectors sequence vectors notation denote whole sequence. vector refers elementwise power matrix rd×k j-th column referred referred throughout paper denotes order derivative operator w.r.t. variable tensor reshaping reshape means tensor order made reshaping tensor ﬁrst mode includes modes shown second mode includes modes shown example tensor order reshape third order tensor ﬁrst mode made concatenation modes polyl denotes element-wise polynomial order input sequence consists vectors rdx.ht hence rdh×dx rdh×dh rdh×dy learn parameters model using method. algorithm called gloree shown algorithm throughout paper assume p.d.f. input sequence vanishes boundary also assumption consider case input geometrically ergodic markov chain. order mixing assure ergodicity output need impose additional constraints model. mentioned introduction method builds method introduced janzamin called feast goal feast extract discriminative directions using cross-moment label score function input. score function normalized derivative p.d.f. input. importance score function provides derivative operator. janzamin proved cross-moment label score function input yields information regarding derivative label w.r.t. input. extend notion score function handle sequence data i.i.d. samples. denote score function time step sequence deﬁned below. theorem readily modiﬁed lemma vector sequence respectively denote joint density function corresponding order score function. then mild regularity conditions continuously diﬀerentiable functions paper functions input sequence output sequence assuming structured form function mapping terms io-rnn hope recover function parameters eﬃciently. exploit score function forms derived compute partial derivatives output sequence.we ﬁrst start simple intuitions. generalized linear model considering consider simple generalized linear model i.i.d. samples weight matrix element-wise activation. here partial derivative w.r.t. linear relationship weight matrices i.e. recover scaling rows. second order derivative form tensor decomposition uniquely recovers scaling rows full rank assumptions. recovering input-output weight matrices io-rnn intuition readily carries forms depend given ht−. thus weight matrices easily recovered forming given compact form markovian input note intuition holds non-linear element-wise activation function require polynomial stage. recovering hidden state transition matrix io-rnn recovering transition matrix much challenging access hidden state sequence thus cannot readily form partial derivatives form also non-linearities recursively propagated along chain. here provide algorithm works polynomial activation function ﬁxed degree i.e. previous input aﬀects current output ﬁrst glance appears complicated indeed various terms highly coupled nice tensor decomposition form. however prove derivative order suﬃciently large nice tensor form emerges depends degree polynomial activation. simplicity provide intuitions quadratic activation function degree polynomial function since activation function applied twice. considering fourth order derivative many coupled terms vanish since correspond polynomial functions degree less surviving term nice tensor form recover eﬃciently note fourth order partial derivative computed eﬃciently using fourth order score function forming cross-moment precise algorithm given algorithm provide algorithm training io-rnns. paper consider vector output polynomial activation functions order simplicity notation ﬁrst discuss case quadratic activation function. algorithm called gloree shown algorithm quadratic activation function. general algorithm analysis shown appendix completeness handle linear case appendix also cover case output scalar appendix compute i)]. empirical average single sequence. tensor decomposition; appendix compute tensor decomposition; appendix ˆai† row-wise kronecker product deﬁnition hence recover tensor decomposition. since previously recovered using exists full rank assumption recover thus algorithm consistently recovers parameters io-rnn quadratic activations. proof appendix bidirectional recurrent neural network ﬁrst proposed schuster paliwal groups hidden neurons.the ﬁrst group receives recurrent connections previous time steps next time steps. following equations describe brnn denote neurons receive forward backward connections respectively. note brnns cannot used online settings require knowledge future steps. however various natural language processing applications part speech brnns eﬀective models since consider past future words sentence. learn parameters bidirectional modifying earlier algorithm. notation simplicity algorithm shows case quadratic activation functions general polynomial function considered algorithm appendix provide intuitions. forward backward connections would directly apply previous method gloree. backward connections diﬀerence would derivatives w.r.t. learn transition matrix hidden neurons mixing yield output vector cross-moment tensor decomposition compute i)]. empirical average single sequence. appendix compute deﬁnition multilinear form section compute tensor decomposition; appendix ˆai† row-wise kronecker product deﬁnition corresponds tensor incorporating columns incorporates columns hence full rank assumption before recover next invert recover decompose recover steps recovering remains gloree. diﬀerence derivatives w.r.t. learn order analyze sample complexity ﬁrst need prove concentration bound cross-moment tensor analysis tensor decomposition show sample complexity order polynomial corresponding parameters. proof sketch proof main parts. first need prove concentration bound moment tensor. second readily analysis tensor decomposition earlier works compute sample complexity moment tensor. since ﬁrst part contribution paper focus that. order prove concentration bound moment tensor note input sequence geometrically ergodic markov chain. think empirical moment functions samples markov chain. note assumes deterministic functions analysis extended additional randomness. kontorovich weiss provide result scalar functions extension result matrix-valued functions. assumptions ensure bounded hidden variable. next leveraging assumptions prove cross-moment tensor satisﬁes lipschitz property paves proving concentration bound. details appendix computational complexity method related complexity tensor decomposition methods. detailed discussion. popular perform tensor decomposition stochastic manner splitting data mini-batches reducing computational complexity. starting ﬁrst mini-batch perform small number tensor power iterations result initialization next mini-batch assume score function given eﬃcient form. note write cross-moment tensor terms summation rank- components need form whole tensor explicitly. example input follows gaussian distribution score function simple polynomial form computational complexity tensor decomposition number samples number initializations tensor decomposition. similar argument follows input mixture gaussian distributions. work ﬁrst step towards answering challenging questions sequence modeling. propose ﬁrst method recover parameters io-rnn well brnn guarantees. many assumptions relaxed e.g. assumed io-rnns aligned inputs outputs. relax assumption obtain general rnns. paper opens horizon future research extending framework hmms general settings analysis non-stationary inputswe assumed realizable setting samples generated rnn. question approximation bounds ﬁxed number neurons interesting problem. authors thank majid janzamin discussions sample complexity constructive comments draft. thank ashish sabharwal editorial comments draft. work done time sedghi visiting researcher university california irvine supported career award anandkumar supported part microsoft faculty fellowship career award ccf- award n--- award wnf--- afosr award fa---.", "year": 2016}