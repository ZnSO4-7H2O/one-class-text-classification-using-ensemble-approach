{"title": "Unsupervised learning of depth and motion", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We present a model for the joint estimation of disparity and motion. The model is based on learning about the interrelations between images from multiple cameras, multiple frames in a video, or the combination of both. We show that learning depth and motion cues, as well as their combinations, from data is possible within a single type of architecture and a single type of learning algorithm, by using biologically inspired \"complex cell\" like units, which encode correlations between the pixels across image pairs. Our experimental results show that the learning of depth and motion makes it possible to achieve state-of-the-art performance in 3-D activity analysis, and to outperform existing hand-engineered 3-D motion features by a very large margin.", "text": "present model joint estimation disparity motion. model based learning interrelations images multiple cameras multiple frames video combination both. show learning depth motion cues well combinations data possible within single type architecture single type learning algorithm using biologically inspired complex cell like units encode correlations pixels across image pairs. experimental results show learning depth motion makes possible achieve state-of-the-art performance activity analysis outperform existing hand-engineered motion features large margin. common property inference motion estimation rely establishing correspondences pixels images. depth estimation correspondences multiple views scene motion estimation multiple frames video. despite superﬁcial differences tasks typical size average displacement across images whether geometry constant variable across pairs much stronger commonalities tasks fact rely ﬁnding positions image match another image. suggests tasks learnable using essentially type architecture type learning algorithm hardly work trying exploit practice. besides obvious advantage allowing develop maintain single piece code achieve tasks makes trivial fuse information sources thereby design architectures learn representations multi-camera video streams application example activity analysis. neuroscience literature so-called complex cell energy model assumed main underlying mechanism behind depth motion estimation provides elegant explanation brain learn using type neural hardware. progress recently learning motion energy models data learning based methods among state-of-the-art activity analysis videos. however hardly work learning energy models depth inference learning depth motion information time. work show fact possible learn depth entirely unsupervised data similar learning motion done using complex cell type models. experiments show makes possible achieve stateof-the-art performance activity analysis multicamera video without making hand-crafted features. ﬁrst step infer depth views correspondences points represent location standard ways approach task position image nearby matching point image using measure similarity local image patches position images extract features describe phase frequency content region around point read phase difference across images ﬁlter responses ﬁrst approach common practice although second biologically plausible require loops local patches. importantly second approach amenable data-driven learning shall show. well-known account phasebased disparity estimation binocular energy crosscorrelation model basic form model states local disparities encoded squared responses neurons binocular receptive ﬁeld. binocular receptive ﬁeld turn shows position-shift across views. receptive ﬁelds show quadrature relationship shown position-shift across views allows energy model encode local disparity quadrature relationship within view allows independent fourier phase local stimulus analogous models also based energy crosscorrelation proposed independently motion encoding surprising considers motion deﬁned transformation given input time disparity transformation input across multiple views stereo pair. given input frames time sequence model encodes motion input stereo pair encodes disparity. proposed that similarity models motion disparity encoding possible integrate date practical exploration idea learning depth data. paper present approach learning depth motion combination data using feature learning architecture based energy model. approach based view energy model proposed shows energy model viewed independent contributions motion encoding detection spatio-temporal synchrony encoding invariance. present autoencoder model using multiplicative interactions detection synchrony show pooling layer independently trained hidden responses used achieve content invariance. adopt approach estimation depth motion cues gives rise efﬁcient single-layer learning algorithm. variety learning based energy models could instead description synchrony condition used implicit encoding depth presented next section. since depth encoded implicitly feature responses model show possible calibrate energy model learned stereo data using available ground truth data compute explicit depth encoding. since applications representation depth means goal explore variety ways utilize implicit encoding depth well motion using approach learning features. evaluate compare several variations approach hollywoodd activity recognition dataset demonstrate improves signiﬁcantly upon state-of-the-art using minimum hand engineering. classic energy model states obtain estimate transformation images computing weighted products ﬁlter responses images. particular ﬁlters differ transformation that product ﬁlter responses large input images holds too. makes possible extract motion denote adjacent frames video disparity denote patches cropped position stereo pair. practical situations dominant transformation images local translation case optimal ﬁlters gabor features small phase shift. early biologically motivated approaches estimating displacements ﬁlters hand-coded context motion estimation various approaches proposed recently learning ﬁlters data learning inefﬁcient vast amounts image patch pairs required learning good ﬁlters recently presented synchrony autoencoder learns motion representations efﬁciently using single-layer autoencoder multiplicative interactions. similar approach deﬁning models learn encode depth. shall review model well show depth motion estimation following section. based description deﬁne model based synchrony autoencoder learning depth representation stereo pair images follows. assume given stereo image pairs rq×n denote matrices containing feature vectors standard train autoencoder minimizing reconstruction error. since vector multiplicative interactions factors represents transformation deﬁne reconstruction obtain ﬁlters represent depth minimize image pairs cropped identical positions multiple views scene. important patchsize large enough cover maximal disparity data otherwise model able encode corresponding depth. contrast traditional approaches estimating depth however need rectiﬁcation since model learn transformation frames horizontal shift. previous section described model encoding depth across stereo image pairs. propose several extensions approach learn representations stereo sequences still images. makes possible extract representation informed motion depth sequence. defer detailed quantitative evaluation approaches section encoding depth concatenation vectorized frames deﬁned stereo image pairs. rq×n note matrices containing feature vector pairs stacked row-wise. feature composed indiqt spans vidual frame features frame input sequence. accordingly features well. words motion encoded indirectly model computing products responses different times across cameras. shall refer model saedepth encoding synchrony autoencoder following. here assume autoencoder tied weights similar allows deﬁne reconstruction error symmetric squared difference inputs corresponding reconstructions analyzing effect encoding motion depth classiﬁcation sequences deﬁne hidden representation employs single stereo sequence follows. represent single camera channel stereo sequence. weight matrices therefore take large individual frames. indvidual value ﬁlters match frames implies jointly satisfy observation basis well-known equivalence energy model cross-correlation model thus case synchrony detected time encoding motion present input sequence. analogy previous section encoding motion weakly related depth scene well depth motion tend correlated. camera motion example viewed providing multiple views single scene thereby implicitly containing information depth however absence camera motion consistent across dataset well presence multitude object motions depth information weakly present encoding motion. shall call model representing motion sae-m following. model equivalent deﬁned encoding motion singlechannel video. obtain explicit encoding depth motion require detection synchrony across time across stereo-pairs. obtain representation practice combine representations deﬁned previous sections example using average concatenation. third alternative propose deﬁning joint representation including products frame responses across time stereo-pairs. recall square frame-wise ﬁlter responses contains within-channel motion information. suggest obtaining estimate across-channel disparity information deﬁning hidden unit response product theses squares. allows extract information disparity relation temporal evolutions complete video sequence rather feature positions across single frames. deﬁne hidden representation models described decoder reconstruction cost derived similar stereopair model section particular reconstruction error contraction cost models sae-d sae-m derived replacing corresponding parameters equations sae-d model amounts replacing frames sequences sae-m model substituting sae-md model found contraction cost unstable presence higher exponents hidden representation. this trained weights sae-d model inference representation equation alternatively possible train model using denoising criterion instead contraction regularization. hand-crafted image motion descriptors typically accompanied corresponding interest point detectors. since reduce number positions extract representations from shown improve efﬁciency performance example bag-of-features based recognition pipelines. learned representation based linear projection image patches possible deﬁne default interest point operator using norm-thresholding feature activations motivated observation norms relevant features higher edge motion locations homogeneous static locations norm thresholding interest point detection amounts simply discarding features norm value chosen based mean norm features training set. well-known energy cross-correlation models hand-crafted gabor features able extract depth information random-dot stereograms order test whether depth information extracted realistic settings using features learned data proposed section ﬁrst conducted experiment depth estimated given stereo image pair. experiment stereo images kitti stereo/ﬂow benchmark dataset consists training image pairs test image pairs. training image pairs corresponding ground truth depth provided. since ground truth captured means velodyne sensor calibrated stereo pair provided approximately image pixels. down-sampled images resolution pixels pixels local shift image pairs falls within local patch size crucial requirement models using local phase matching disparity computation discussed section trained stereo-pair model described section patch pairs cropped training set. patch size pixels total number training samples patches used learning ﬁlters cropped regions images corresponding depth information available. learned ﬁlters shown figure ﬁgure shows ﬁlters localized gabor-like span wide range frequencies positions. since cameras parallel ﬁlters learned predominantly horizontal shifts. test extract depth information learned hidden representation trained logistic regression classiﬁer using available ground truth output data. generate labels taking mean non-zero pixel intensities corresponding patches ground truth quantize bins. training classiﬁer estimation depth given stereo pair involves dense sampling patch pairs followed feature computation prediction classiﬁer. sample stereo image pair learned depth shown figure ﬁgure predicted depth label pixel estimated depth map. artifact depth estimation procedure object boundaries expanded actual size patch size used model. also observed depth featureless regions like plane surfaces less accurate feature rich regions model cannot detect shift cases. goal explicit depth markov random field similar approach cleaning obtained depth map. event interested exact depth rather depth cues help make predictions merely depend depth possible alternative interest point detector explained section figure shows example estimated depth interest points shows norm thresholding masks regions predominantly homogeneous regions image. general thus observe possible infer depth information ﬁlter responses deﬁned section even information comes form noisy cues similar common estimates motion rather form clean depth map. shall discuss approach exploiting information bag-of-features evaluate effect implicit depth encoding task activity recognition using hollywoodd dataset introduced dataset consists stereo video sequences along computed depth videos. videos different categories videos training testing. different categories ’run’ ’punch’ ’kick’ ’shoot’ ’eat’ ’drive’ ’usephone’ ’kiss’ ’hug’ ’standup’ ’sitdown’ ’swim’ ’dance’ ’noaction’. videos downsampled spatially size models trained whitened spatio-temporal block pairs block size samples used training number hidden units ﬁxed models sample feature pair learned sae-d model shown figure ﬁlter pair spans frames. ﬁlters gabor-like show continuous phase shift time another phase shift across camera views. quantitative evaluation framework presented performing feature extraction perform k-means vector quantization followed multiclass kernel classiﬁcation. diagram pipeline visualized figure feature extracted using convolutional architecture similar presented super block pairs size pixels cropped densely stride time space respectively stereo video pairs. super blocks sub-blocks size training block size cropped stride resulting sub-blocks super block. ﬁrst compute feature vector stereo subblock pair. concatenate feature vectors corresponding sub-blocks super block reduce dimensionality using pca. procedure using sae-d model example visualized figure number words k-means vector quantization main goal experiments evaluate impact implicit depth encoding task activity recognition. compare variety settings end. experiment sae-d used feature extraction. discussed section sae-d primarily encodes depth. experiment uses sae-m feature extraction stereo channels input. experiment employs sae-md features extraction thus based representation integrates across-frame across-channel correlations. experiment test alternative ways integrating depth motion information combining representations separately trained sae-d sae-m models. ﬁrst call sae-md amounts concatenating representations sae-d sae-m features. other sae-md amounts computing average precision using mean conﬁdences experiments thus amounts averaging classiﬁcation decisions separate classiﬁcation pipelines conﬁguration evaluated computing average precision correct-classiﬁcation rates. results reported tables repeated experiments using norm-thresholding interest point detector described section results observed combination motion depth cues performs better using individual cues. results including motion-only depth-only combination models outperform existing models based hand-crafted representations observed past learning based features tend outperform traditional features like sift object recognition tasks recently larger margin motion analysis tasks compared spatio-temporal variations sift observation conﬁrmed dataset seems even pronounced. also observe models using interest points provide additional consistent improvement not. furthermore depth information provides edge motionmodels. interestingly overall effect various variations model differ heavily across action class seen table example classes kick shoot highest using primarily depth features classiﬁcation; noaction kiss best using motion features; classes highest combining depth motion features. multiple reasons likely related average depth variation within activity class. detailed analysis type information useful type activity class interesting direction future work. well-known popular recipe improve performance learning tasks base classiﬁcation decisions combination multiple different models utilizes different type feature. recipe often works well practice main challenge make work develop models sufﬁciently different another yield sufﬁciently large reduction variance. utilizing combination depth motion cues viewed context also extract cues video data different since represent different properties environment. biology take different route might depth deep learning makes possible exact learning algorithm depth inference also used recognize objects motion; simple depth given feature vector often entirely sufﬁcient take swift vital decisions dodge approaching object; learning depth inference data allows feed-forward depth perception thus avoid need complicated brittle pipeline involves rectiﬁcation hypothesis generation robustiﬁcation using ransac paper showed unsupervised feature learning used mimic extracting depth cues image pairs learning joint representations motion depth within single type architecture single type learning rule achieve state-of-the-art performance activity recognition task. work best knowledge ﬁrst published work shows deep learning approaches hitherto shown work well object motion recognition tasks also applicable domain depth inference generally vision. table average precision class hollywood action dataset. words descriptor using d-ha d-ha .d-ha interest points reported values bold best class across methods.", "year": 2013}