{"title": "Strategies for Conceptual Change in Convolutional Neural Networks", "tag": ["cs.LG", "cs.AI"], "abstract": "A remarkable feature of human beings is their capacity for creative behaviour, referring to their ability to react to problems in ways that are novel, surprising, and useful. Transformational creativity is a form of creativity where the creative behaviour is induced by a transformation of the actor's conceptual space, that is, the representational system with which the actor interprets its environment. In this report, we focus on ways of adapting systems of learned representations as they switch from performing one task to performing another. We describe an experimental comparison of multiple strategies for adaptation of learned features, and evaluate how effectively each of these strategies realizes the adaptation, in terms of the amount of training, and in terms of their ability to cope with restricted availability of training data. We show, among other things, that across handwritten digits, natural images, and classical music, adaptive strategies are systematically more effective than a baseline method that starts learning from scratch.", "text": "remarkable feature human beings capacity creative behaviour referring ability react problems ways novel surprising useful. transformational creativity form creativity creative behaviour induced transformation actor’s conceptual space representational system actor interprets environment. report focus ways adapting systems learned representations switch performing task performing another. describe experimental comparison multiple strategies adaptation learned features evaluate eﬀectively strategies realizes adaptation terms amount training terms ability cope restricted availability training data. show among things across handwritten digits natural images classical music adaptive strategies systematically eﬀective baseline method starts learning scratch. problem description evaluation criteria convolutional neural networks convolutional autoencoders multi-task learning transfer model task another strategies conceptual change cnns baselines prior regularisation note random initialization order survive living organism adapt environment. static environment organisms subject constant change. arguably complex plastic information processing structures mammal brain evolutionary answer requirement adapt unforeseen circumstances lifespan organism aspects adaptive behaviour notable higher mammals humans called creative. although term notoriously evasive precise deﬁnition common agreement creative behaviour involves elements novelty surprise value example creative behaviour found domain names world wide web. although level domains intended means structuring world wide nowadays used linguistic meaning rather original denotation thematical geographical structure. examples uses youtu.be friend.ly podca.st. arguably creative driven increasing communication urls therefore growing need shorter urls easy memorize. seminal attempts formalizing notion creativity boden links creative behaviour agent conceptual space. space represents agent interprets perception actions. according boden creativity amounts mapping exploration transformation conceptual space agent. example transformation given boden transformation conceptual space music arnold schoenberg invented radically music dropping constraint home existing conceptual space music. literature creativity focuses high level cognitive phenomena involves taking granted conceptual space dealing environment already present. contrast viewing creativity discrete transformation static conceptual space another promising alternative perspective regard conceptual spaces inherently dynamic. view creativity inherent property dynamics conceptual spaces brings existence ﬁrst place. view also suggests gradual distinction perception cognition. hofstadter goes ability reperceive critical element creativity. focus dynamic aspects conceptual spaces naturally leads representation learning active ﬁeld research machine learning. area computational models predominantly form neural networks trained data form hierarchically structured representations data. often representations capture semantically relevant characteristics data several levels form robust basis subsequent tasks classiﬁcation organization data. typical methods representation learning deal classical machine learning scenario model trained data possibly labels drawn distribution evaluated data also assumed distribution implies training methods designed converge single representations optimal given stated beginning section however creativity often driven need adapt changing environment. terms machine learning paradigm violates assumption static time. thus representation learning model stay eﬀective face changing environment requires methods beyond standard representation learning algorithms. shift focus learning representations static environment adapting learned representations dynamic environment brings multiple non-trivial problems addressed. example need measure well given representation suits environmental requirements. furthermore light environmental change sometimes gradual change learned representations beneﬁcial whereas occasions radical change eﬀective. scope current report address problems. document restrict scenario radical change environment given investigate several alternative strategies dealing change eﬀective neural network models representation learning. ultimately interested ﬁnding generally useful strategies allow computational system adapt eﬃciently changing environmental requirements. start precise description problem model architecture learning. deﬁne number possible adaptation strategies report experiment strategies compared evaluating several data sets. that discuss work presented relates creativity broader sense. document focus strategies conceptual change adapt learned representations task another task. tasks consider classiﬁcation autoencoding. change task either mean switching classiﬁcation auto-encoding vice versa case classiﬁcation change target classes. note however change task inducing conceptual change cases extrinsic factor. perspective agent interacting outside world changes reﬂect changes environment scenario corresponds type creativity described above form adaptive problem solving behaviour. diﬀerent related interpretation emphasizes generative aesthetic aspects creative behaviour lead novel artefacts. case conceptual change manifest creative behaviour thought driven intrinsic factors rather response changes environment. methods conceptual change described here evaluated context classiﬁcation autoencoding also realization second kind creativity? current computational theories creative behaviour suggest since posit basis creative behaviour mechanisms encode incoming information adapt dynamic environment provide optimal encodings line evidence learning principles living organisms minimum entropy coding wiggins forth propose theory creativity based global workspace theory criterion determines representations develop model based information-theoretic principles demanding representations facilitate accurate prediction perceptual future leading eﬃcient information processing. model multiple competing generators match perceptual input learned representations memory predict future input. representations eﬀective generators surface global workspace reﬂecting conscious awareness. awareness turn updates associative memory stores adaptive representations used generators. changes semantics result adapting conceptual space regarded restricted form creative behaviour. addition wiggins forth theorize instantiation theory form computational model exposed stimuli imprinted memories able generate novel artefacts absence external stimuli system perceptual buﬀers memory thus continuing cycle generation memory selection updating memory. schmidhuber also argues fundamental aspect cognition agent learn compress outside world aﬀordances. plausible drivers goal optimal compression/prediction extrinsic rewards sparing physiological resources selective advantage schmidhuber argues important explanatory factor human behaviour intrinsic motivation improve compression/prediction capabilities. means agent desirable discover ways better compressing predicting experience reward proportional degree improvement. schmidhuber claims intrinsic motivation manifests curiosity human beings subjective experience improving compression/prediction aesthetic also leads notion surprise diﬀerent usual information-theoretic notion surprise sense rather information content event itself degree information content event triggers improvement agent’s compression/predicting capabilities. thus sequence repeated events sequence random events surprising soon become boring. theories creative behaviour described involve adaptive component generates predicts compresses perceptual input words maps incoming data internal representation adapts accommodate novel patterns data. precise nature accommodation process part theories methods evaluated hint possible implementations process especially situations patterns data change drastically. furthermore fact methods proposed concern neural networks suits reinforcement learning paradigm schmidhubers theory neural networks combination q-learning shown successful variety game-playing tasks better solve another task domain rather general deﬁnition leaves room number variations transfer learning problems depending conditions transfer learning problems characterized help probabilistic formalization david yang another sub-problem transfer learning class imbalance particular class substantially underrepresented source domain respect target domain vice torrey shavlik describe three ways transfer source task/domain help model perform target task illustrated figure studies concerning transfer loss asymptotic performance criterion successful transfer. common measure transfer-loss measures loss transfer method minus loss in-domain baseline method target domain informed source domain. transfer-loss thus proportional diﬀerence asymptotic performance figure furthermore unsupervised pre-training deep architectures regarded forms transfer learning since unsupervised learning task model trained ﬁrst phase diﬀerent ﬁnal supervised task. previously stated domain adaptation form transfer learning out-of-domain data used improve performance model solving task target domain. framework tical independent previous work domain adaptation source data prior knowledge estimate model parameters target domain using maximum posteriori approach tasks involving language modelling parsing chelba acero propose model using maximum entropy model estimation model parameters target domain capitalization text. strategy later revisited parameters model trained source domain used regularise parameters model trained target domain. multi-task learning related transfer learning sense model used address diﬀerent tasks transfer learning problems involve phase model trained task turning another task multi-task learning involves simultaneous training model diﬀerent tasks. motivation training signals related tasks serve regularize model leading generalize better. example task steering direction vehicle predicted based camera images road front vehicle caruana found beneﬁcial train model simultaneously number additional tasks predicting left right borders road whether road tracks. simultaneous training multi-task learning related transfer learning sense model used address diﬀerent tasks transfer learning problems involve phase model trained task turning another task multi-task learning involves simultaneous training model diﬀerent tasks. motivation training signals related tasks serve regularize model leading generalize better. example task steering direction vehicle predicted based camera images road front vehicle caruana found beneﬁcial train model simultaneously number additional tasks predicting left right borders road whether road tracks. simultaneous training case realized output unit task eﬀectively upper layer model aﬀected training multiple tasks. speciﬁc problem focus current experiment form transfer learning. interested question model trained perform particular task adapt novel task eﬀectively. illustrated figure several aspects notion eﬀective adaptation importantly asymptotic accuracy novel task amount training takes perform task accurately. deﬁne criteria evaluation adaptation strategies section that give precise description problem addressing. given labelled data divide data subsets subset contains data pertaining half labels subset contains data pertaining half labels. call subset source domain target domain. implies label follows. instead label space deﬁne multi-task formalization optimal task model optimal example model trained classiﬁcation task domain might adapted perform classiﬁcation task domain also adapted radically perform autoencoding domain since architecture model determined task performs obvious mean adapting model task another. section describe procedure detail. another aspect clariﬁed measure success adaptation strategies. address issue section described above goal adaptation strategies representation learning models allow model perform well possible task little training necessary. terms schematic plot figure primary interest slope learning curve obviously model adapts quickly substantially lower asymptotic performance possible baseline method undesirable. therefore slope asymptotic performance respect baseline methods taken account evaluating methods. ultimately interested adaptation strategies work well independent source target tasks involved. suggests deﬁne single quality measure aggregates slopes asymptotic performances strategy diﬀerent combinations source target tasks. chosen following reasons. firstly since different quantities obvious combine slope asymptotic performance adaptation strategy principled manner. weighting quantities necessarily reﬂects personal judgement relative importance. secondly experiments reported regarded explorative. although interested establishing superiority adaptation strategy another independent tasks domains involved priori indication ranking established across tasks domains. reasons refrain deﬁning single evaluation criterion adaptation strategies. believe insight gained qualitative analysis evolution task performance target domain adaptation strategies function training using plots similar diagram figure convolutional neural networks special kind feed forward neural networks build invariance properties structure neural network cnns successfully used several machine learning applications including natural language processing image classiﬁcation cnns number advantages fully connect ffnns. firstly convolutional nature architecture using small convolutional ﬁlters enforces extraction local features secondly typically shared weights greatly reduces number parameters compared similar sized ffnns lastly typically perform spatial sub-sampling adds robustness noise local distortions. typical three building blocks convolutional subsampling dense layers. illustrated figure pooling understood form non-linear downsampling. maxpooling layers partition input feature non-overlapping rectangles pool outputs maximum value. common activation functions cnns include sigmoid tanh rectiﬁer softmax particularly useful output multi-class classiﬁer appendix explicit deﬁnitions activation functions. practice convolutional pooling layers used learn feature hierarchy dense layers used classiﬁcation purposes based computed features following refer stack convolutional pooling layers convolutional stage stack dense layers fully connected stage classiﬁcation stage primary objective classiﬁcation. figure example architecture convolutional neural network classiﬁer including convolutional-pooling substages classiﬁcation stage consisting fully connected layer methods unsupervised learning based encoder-decoder paradigm input ﬁrst transformed lower-dimensional space expanded reproduce initial data examples paradigm include lowcomplexity coding decoding machines predictability minimization layers restricted boltzmann machines autoencoders decoding parameters. usual constraint i.e. decoding parameters weights take form weights encoding input decoding latent representation. convolutional autoencoders autoencoders cnns encoding stage previously stated section cnns allow discovery localized features appear input. reconstruction input data combination basic image patches based latent code. downsampling layers encoding stage corresponding decoding layer corresponds upsampling layer. possible upsampling strategy max-pooling layers remember position input maximum value every max-pooling forward propagation input data network. refer layer implements strategy unpooling layer. solving problem constrained multiple objective functions object study muticriterion optimization vector optimization problem formalized follows. given vector loss function whose components interpreted diﬀerent scalar objectives minimized. order apply standard scalar optimization methods gradient descent scalarise multi-criterion problem forming weighted objective i.e. represents weighting coeﬃcient i-th component using framework possible express multi-task objective minimizing joint loss function classiﬁcation autoencoding follows order reduce degrees freedom transfer model task another biases decoding layers built autoencoder. guarantees weights responsible learning task. subsection propose number strategies adapting learned representations response tasks. clarity label choice keyword refer ﬁnal strategies combination keywords listed section. obvious baseline approaches adaptation model task. make changes model upon change task. case change training data task replaced training data task. practical issue task implies interpretation outputs possibly diﬀerent number outputs. reusing parameters output layer task raises question output variables task mapped task mapping necessarily arbitrary. reason reuse parameters prior model replace output layer layer initialized random parameter values. ignoring detail refer baseline strategy reuse all. another contrary approach altogether ignore representations learned prior task start randomly initialized model learn task. regarded case inﬁnite plasticity task forms representations without trace representations learned prior task. strategy referred reset. considering dynamics learning reuse ideal since starts learning task model specialized task take eﬀort unlearn aspects task irrelevant task learn task scratch. however data task task similar nature likely data least common structure. example case natural images depicting diﬀerent classes objects likely certain level representations local edges natural images useful different tasks recognition diﬀerent object classes. diﬀerent classes objects involve diﬀerent constellations similar forms. example current experiment observation inspires adaptation strategy convolutional ﬁlters representing level representations preserved across tasks whereas rest network re-initialized random state. option referred reuse strategy domain adaptation proposed chelba acero take maximum a-posteriori approach estimation model parameters task model learned ﬁrst task provides prior estimate parameters. prior serves regulariser parameters. gaussian distribution around parameters θold learned ﬁrst task. assumption leads alternative loss function including prior regularisation term idea investigating variance layer outputs improve weight initialization deep learning introduced result motivated search careful initialization rather unsupervised pretraining networks methods rbms thus representing considerable speedup training neural networks glorot bengio suggest keeping layer-to-layer transformations singular values jacobian matrices associated layer approximately equivalent keeping ratio average activation variance going layer layer result implies random initialization model parameters layer depends nonlinear activation function size layer. main intuition behind initialization strategy network parameters initialized small output shrinks passing layer eventually small. hand network parameters large output layer keeps growing output network saturated. prior model initialize convolutional ﬁlter parameters prior model prior model trained classiﬁer prior model trained autoencoder multi-task prior model trained simultaneously classiﬁer autoencoder table labels denoting adaptation strategies meaning used results. labels parentheses represent prior models used conjunction adaptation strategies model evaluated target test training order monitor adaptation. adaptation strategy results training runs averaged order reduce impact random eﬀects schematic overview process given figure since process produces multitude models trained diﬀerent conditions refer using combinations labels denoting adaptation strategy used labels denoting prior model used adaptation strategy. labels listed table since interested adaptation methods allow quick adaptation model source task/domain target task/domain intentionally limit size training target domain. scarcity training samples makes harder model adequately generalize target domain thus increases potential beneﬁt adapting model another domain figure schematic overview experiment single data set; grey rounded boxes represent training methods circles represent models document shapes represent data instances white rounded represents evaluation method mnist mixed national institute standards technology database consists handwritten digits collected american high school students employees united states census bureau database constitutes used data sets benchmarking machine learning algorithms mnist consists gray-scaled images rescaled pixel centred box. experiments mnist divided subsets namely data consisting digits data consisting digits training divided examples computing parameter updates examples validation examples testing. data contains examples training validating testing. data samples class randomly selected training validating making total examples training validating testing. cifar- cifar- labelled subset million tiny images data collected krizhevsky nair hinton data used evaluate performance algorithms machine learning paper divided classes comprising vehicles animals. dataset chosen include instances classes airplane automobile bird deer data consists classes frog horse ship truck. training split training validation sets consisting data respectively test contains examples. data consists examples respectively training validating testing data contains randomly selected samples class training randomly selected samples class testing make make total training examples validating examples examples testing. composers application proposed methods musical domain data consisting excerpts musical pieces baroque classical periods used. excerpts represented piano-rolls i.e. images pixel axis corresponds musical note pixel axis represents unit time. scores taken muse data database electronic library classical music scores created center computer assisted research humanities stanford university. excerpt length quarter notes sample rate note size quarter notes contiguous windows. pieces centred midi range notes. signalize note last left blank. data consists selection excerpts telemann’s cantatas bach’s cantatas h¨andel’s concerti grossi trio sonatas data consists excepts string quartets haydn mozart. excerpts coming single piece appear either training validation test sets i.e. pieces appear data set. total training examples validation examples testing examples data containing examples training validating testing respectively data consisting randomly selected samples class training randomly selected samples class validation make total examples training examples validating examples testing. models trained using rmsprop derivative traditional backpropagation algorithm. method mini batch variant stochastic gradient descent adaptively adjusts learning rate dividing gradient average recent magnitude. order accelerate gradient descent nesterov’s method accelerating gradient descent order avoid overﬁtting several strategies used including l-norm weight regularisation enforcing sparseness layer activations early stopping dropout. regularisation norm enforces sparse parameters sparsity layer activations enforced using hoyer’s sparseness measure dropout prevents overﬁtting provides approximately combining diﬀerent neural networks eﬃciently randomly removing units network along incoming outgoing connections network architectures hyper-parameters empirically selected optimizing models respective validation sets. classiﬁcation stage consisting fully connected layer sigmoid units fully connected layer softmax units. dropout used convolutional stage i.e. learning rate rmsprop nesterov’s momentum probability dropout regularisation coeﬃcient sparsity coeﬃcient target sparseness batch size network trained maximum epochs maximum epochs best result early stopping. multi-criterion weighting coeﬃcient multi-task models. learning rate rmsprop nesterov’s momentum probability dropout regularisation coeﬃcient sparsity coeﬃcient target sparseness batch size network trained maximum epochs maximum epochs best result early stopping. multi-task models multi-criterion weighting coeﬃcient kernels size rectiﬁed linear activations followed maxpooling layer pool size second convolutional layer kernels size rectiﬁed linear activations followed maxpooling layer pool size convolutional stage followed fully connected layer rectiﬁed linear units fully connected layer softmax units conforming classiﬁcation stage. similar fashion previous cases dropout used convolutional stage i.e. learning rate rmsprop nesterov’s momentum probability dropout regularisation coeﬃcient sparsity coeﬃcient target sparseness batch size networks trained maximum epochs maximum epochs best result early stopping. models solving joint classiﬁer autoencoding task multi-criterion weighting coeﬃcient fact accuracy curves rather noisy classiﬁcation accuracy training objective rather categorical cross-entropy model output one-hot representation class labels furthermore occasional discontinuities curves averaging multiple runs runs converge fewer epochs others. figure consistent trend reuse baseline strategy slow adapt independent source task. interestingly among reuse conditions adaptation autoencoder model source domain—reuse beneﬁcial learning target classiﬁcation task adaptation classiﬁer—reuse source domain. indication autoencoder learns representations useful encoding data general whereas classiﬁer learns specialised features useful primarily recognising speciﬁc classes happen source domain. degree ﬁnding underlines rationale unsupervised pre-training learning encode data independent classiﬁcation objective provides robust representations driving representations classiﬁcation objective start. fact unsupervised-pretraining—reuse —does surpass reset strategy suggests distribution data autoencoder seen suﬃciently representative target domain reuse strategy retains ﬁrst layer convolutional ﬁlters learnt source domain resets parameters model considerably speeds adaptation target task respect reset baseline. strategy adaptation classiﬁcation task source domain generally successful even adapting models source tasks adaptation mostly quicker reset. plausible explanation level structure data usually general whereas higher level structure speciﬁc particular data categories. start target domain opposed adaptation strategies training model using strategy hardly improve autoencoding loss beyond initial value. reuse strategy hand adapts much worse reset baseline cifar mnist reuse autoencoder task target domain composers data shows diﬀerent pattern here adaptation strategies outperform reset baseline discards knowledge source domain. indication source target domains composers adaptation strategies advantage reset baseline. explanation structure musical data common across music diﬀerent composers prevalence common structure would also explain rather overall accuracy composer classiﬁcation task simultaneous learning autoencoding classiﬁcation source domain appears provide useful knowledge transferred autoencoding target task classiﬁcation target task latter case adaptation successful source task also classiﬁcation. prior regularization strategy ﬁrst layer convolutional ﬁlters randomly initialised network biased towards ﬁlters learned source domain task. regularization improve upon reset baseline sometimes consistently gains usually moderate. issue discussed whether asymptotic performance strategies substantially diﬀerent others. figures clear broadly speaking adaptation strategies converge certain range even individual diﬀerences. make better judgements asymptotic performance adaptation strategies however necessary extensive experimentation longer training phases data sets averaging results runs condition. moreover although hyperparameters models selected exploration prior experiment exhaustive study hyperparameters provide better view value adaptation strategies. computational point view considerable undertaking given results presented require several weeks continuous computation multiple gpgpu-enabled machines. figure classiﬁcation accuracy target test-set diﬀerent data sets diﬀerent adaptation strategies. curves averaged runs. table explanation legend interacting dynamical unpredictable environment ability agent easily adapt conceptual space interprets environment strong advantage. argued section adaptability also underlying mechanisms creative behaviour. document described evaluated several adaptation strategies transform learned representations optimized speciﬁc task speciﬁc domain another task another domain evaluated strategies using convolutional neural network multiple data sets comprising natural images handwritten digits classical music. results show that across domains tasks adaptation strategies transform existing representations allow quicker adaptation task domain starting representation learning scratch. although single adapatation strategy universally superior others clear patterns emerge results. firstly target task classiﬁcation successful adaptation strategy keep ﬁrst level convolutional ﬁlters source task/domain reset rest parameters. strategy even beneﬁcial source task autoencoding rather classiﬁcation. secondly autencoding target task reuse full model successful strategy sense even start task performance strategy comparable asymptotic performance strategies. contrast strategies strategy substantially improve model beyond initial performance. experiments provide evidence strong advantage prior regularization convolutional ﬁlters condition model trained scratch standard regularization. elaborate experiments required provide ﬁrmer conclusions merits adaptation strategies. particular longer training phase necessary provide better insight asymptotic behaviour strategies. project lrncre acknowledges ﬁnancial support future emerging technologies programme within seventh framework programme research european commission grant number", "year": 2017}