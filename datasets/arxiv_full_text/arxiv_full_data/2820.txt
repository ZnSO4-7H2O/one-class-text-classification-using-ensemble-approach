{"title": "Learning Robust Options", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Robust reinforcement learning aims to produce policies that have strong guarantees even in the face of environments/transition models whose parameters have strong uncertainty. Existing work uses value-based methods and the usual primitive action setting. In this paper, we propose robust methods for learning temporally abstract actions, in the framework of options. We present a Robust Options Policy Iteration (ROPI) algorithm with convergence guarantees, which learns options that are robust to model uncertainty. We utilize ROPI to learn robust options with the Robust Options Deep Q Network (RO-DQN) that solves multiple tasks and mitigates model misspecification due to model uncertainty. We present experimental results which suggest that policy iteration with linear features may have an inherent form of robustness when using coarse feature representations. In addition, we present experimental results which demonstrate that robustness helps policy iteration implemented on top of deep neural networks to generalize over a much broader range of dynamics than non-robust policy iteration.", "text": "length modelled dynamical system pdynamics also referred state-transition function transition model. terms used interchangeably throughout paper. transition model governs dynamics arm. different models pdynamics pdynamics generated arms lengths respectively. arms attempting perform task. agent trained using model pdynamics adequately perform task using pdynamics pdynamics. however ideally agent agnostic uncertainty model parameters still able solve task practical applications rely following twostep blueprint step build model models attained three ways ﬁnite noisy batch data acquired model built based data; simpliﬁed approximate model environment provided directly model environment derived step learn policy methods applied good policy based model. cases parameters model uncertain noisy ﬁnite data simpliﬁed model respectively. case model uncertainty occurs parameters physical agent uncertain discussed above-mentioned example. especially important industrial robots periodically replaced robots might share exact physical speciﬁcations learning policy agnostic parameters model crucial robust model uncertainty. focus learning policies dynamical systems using robust framework robust model uncertainty robust reinforcement learning aims produce policies strong guarantees even face environments/transition models whose parameters strong uncertainty. existing work uses value-based methods usual primitive action setting. paper propose robust methods learning temporally abstract actions framework options. present robust options policy iteration algorithm convergence guarantees learns options robust model uncertainty. utilize ropi learn robust options robust options deep network solves multiple tasks mitigates model misspeciﬁcation model uncertainty. present experimental results suggest policy iteration linear features inherent form robustness using coarse feature representations. addition present experimental results demonstrate robustness helps policy iteration implemented deep neural networks generalize much broader range dynamics non-robust policy iteration. paper focus developing methods learning temporally extended actions robust model uncertainty. temporally extended actions also known options skills macro-actions shown theoretically experimentally result faster convergence rates planning algorithms. refer temporally extended action option much research dedicated automatically learning options e.g. work best knowledge focused learning options robust model uncertainty. model misspeciﬁcation cartpole; experiments suggest linear approximate dynamic programming algorithms robustness ‘for free’ utilizing coarse feature representation. ro-dqn solves multiple tasks learning robust options using ropi mitigate model misspeciﬁcation. robust markov decision process robust markov decision process represented ﬁnite states; ﬁnite actions immediate reward bounded deterministic discount factor. transition function mapping given state action measure next states. given nature allowed choose transition state family transition probability functions family called uncertainty uncertainty state transitions therefore represented robust learn robust policy function mapping states actions maximizes worst case performance robust value function γinfp∈pep)|x uncertainty state transitions; bounded immediate reward infp∈pep)|x worst-case expected return state order solve value function using policy evaluation deﬁne robust operator r|x| given state inf{pt action r|x| also deﬁned operator r|x| r|x| ﬁxed policy σp)v. using operator robust value function given following matrix equation γσπv previously shown robust bellman operator ﬁxed policy γσπv contraction sup-norm robust bellman operator supπt also contraction timal value function policy ﬁxed point robust projected value iteration robust literature focused small mediumsized mdps. provide approach capable solving larger continuous mdps using function approximation. suppose value function represented using linear function approximation feature-based model misspeciﬁcation linear setting occurs learning agent provided limited policy feature representation rich enough solve task. non-linear setting occurs deep network learns sub-optimal features resulting sub-optimal performance. show work options indeed necessary mitigate fmm. however discussed example model uncertainty also results sub-optimal performance. show experiments especially problematic deep networks. therefore learn robust options mitigate model uncertainty collectively term model misspeciﬁcation policy iteration powerful technique present different variations many algorithms. deep network example powerful non-linear function approximator employs form actor-critic policy gradient algorithms perform online form result decided perform option learning policy iteration framework. introduce robust options policy iteration algorithm learns robust options mitigate model misspeciﬁcation convergence guarantees. novel ropi algorithm consists steps illustrated figure include policy evaluation step policy improvement step. utilize rpvi perform policy evaluation learn value function parameters perform using robust policy gradient process repeated convergence. ropi learns robust options robust inter-option policy theoretical convergence guarantees. showcase algorithm linear non-linear feature settings. linear setting show non-robust version linear option learning algorithm called asap learns inherently robust options. this claim coarseness chosen feature representation. provides evidence that cases linear approximate dynamic programming algorithms robustness ‘for free’. however non-linear setting explicitly incorporating robustness learning algorithm crucial robust model uncertainty. incorporate ropi deep network form robust options deep network using ro-dqn agent learns robust options solve multiple tasks mitigates model misspeciﬁcation. learning robust options using novel ropi algorithm convergence guarantees; includes developing robust policy gradient framework includes robust compatibility condition; linear version ropi able mitigate robust literature policy often deterministic notational convenience. stochastic policy trivially derived incorporating uncertainty transitions figure parameter uncertainty dynamical systems also referred model uncertainty. agent needs able solve given task different parameter settings. high-level overview ropi framework. option hyperplanes option partitions asap option learning algorithm. options reinforcement learning option consists three-tuple initiation states option executed; intra-option policy parameterized mapping states probability distribution actions; indicates probability option terminating state option learning recent approaches option learning approach focus adaptive skills adaptive partitions framework enables agent automatically learn hierarchical policy corresponding option set. hierarchical policy learns execute options based learning intersections hyperplane half-spaces divide state space. figure contains example option hyperplanes whose intersection divides state space four option partitions. option partition contains option options option partitions learned using policy gradient approach represented within asap policy. asap policy function mapping state probability distribution actions deﬁned follows deﬁnition given option hyperplanes options {ζi|i space possible mdps asap policy dei= pβξχi probability executing option given agent state represents probability that given option executing option choose action aζix. deep networks algorithm powerful deep algorithm able solve numerous tasks atari video games minecraft stores experiences experience replay buffer remove data correlation issues. learns minimizing temporal difference loss. typically separate trained solve task. works combined learning multiple tasks single network require pre-trained experts train agent supervised manner. recently robustness incorporated d-dimensional feature vector parameters. robust projected value iteration involves solving equation φwk+ projection operator onto subspace spanned tamar show contraction respect sup-norm. results update equation sampled data solved efﬁciently parameterized uncertainty sets convex parameters here r|x|×d matrix linearly independent feature vectors rows diag state distribution policy rpvi utilizes deterministic policy notational convenience assume stochastic policy remainder paper. note equation written robust critic update actor critic policy gradient algorithms robust error robust error deﬁned equation projection omitted since viewed dynamic learning rate policy gradient policy gradient standard technique reinforcement learning used estimate parameters maximize performance objective stochastic gradient descent typical performance objective γt−rt|x reward time discount factor given start state. gradient previously shown robust performance objective r-pg optimizes discounted expected reward obt= γt−rt|x disgiven uncertainty set; count time next deﬁne robust action value function γt−rt|x denote robust state value function πqπ. robust policy parameterized parameters wish maximize obtain optimal policy parameters maxθ robust policy gradient given robust performance objective respect robust discounted state distribution derive robust policy gradient respect robust policy parameterized unless otherwise stated. derive gradient using robust formulation discounted scenario. discounted expected reward case presented lemma main differences lemma incorporate robust state distribution emit transition distribution ˆpmin leading agent areas lowest value timestep. lemma suppose maximizing robust pert= γt−rt|x respect given start policy parameterized robust action value function deﬁned γt−rt|xt gradient respect performance objective robust compatibility conditions robust gradient update possess ability incorporate function approximation. however deriving robust compatibility conditions replace linear function approximator ψxa. represents approximator’s parameters represent compatibility features. robust compatibility features presented lemma note compatibility condition respect robust state distribution ˆdπ. lemma approximation minimizes mean squared error throughout paper make following assumptions standard policy gradient literature assumption policy markov chain resulting robust irreducible aperiodic. assumption policy pair continuously differentiable respect parameter addition make assumption optimal value function found within hypothesis space function approximators utilized. assumptions deﬁne robust transition probability function robust steady-state distribution discounted setting. deﬁnitions derive robust policy gradient version equation robust transition probability distribution robust value function deﬁned policy ep)|x robust action value function given ep)|x pmin|x ep)|x here pmin|x transition probability distribution minimizes expected return ep)|x given state action belongs pre-deﬁned uncertainty since ˆpmin|x selected independently state construct stochastic matrix pmin deﬁned ˆpmin|x·). robust state distribution matrix ˆpmin interpreted adversarial distribution zero-sum game adversary ﬁxes worst case strategy deﬁnition tion robust discounted state distribution state distribution used discounted setting. however transition kernel selected robustly rather assumed transition kernel target mdp. robust discounted state distribution intuitively represents executing transition probability model leads agent worst areas state space. using deﬁnitions derive robust policy gradient discounted setting used policy improvement ropi. derive r-pg deﬁne robust performance objective derive corresponding robust compatibility conditions enables incorporate function approximator policy gradient. performed experiments well-known continuous domains called cartpole acrobot transition dynamics cartpole acrobot modelled dynamical systems. experiment agent faced model misspeciﬁcation. feature-based model misspeciﬁcation model uncertainty. experiment agent mitigates utilizing options model uncertainty learning robust options using ropi. analyze performance ropi linear non-linear feature settings. linear setting apply ropi adaptive skills adaptive partitions option learning framework. non-linear setting apply ropi robust options network. experiments divided parts. section show ropi necessary learned linear ‘non-robust’ options solving cartpole provide natural form robustness mitigate model misspeciﬁcation. provides evidence linear approximate dynamic programming algorithms coarse feature representations cases robustness ‘for free’. question whether natural form robustness present deep setting? show case experiments section here robust options learned using ropi necessary mitigate model misspeciﬁcation. experiment compare misspeciﬁed agent ‘non-robust’ option learning algorithm mitigates robust option learning ropi algorithm mitigates model uncertainty domains acrobot acrobot planar two-link robotic vertical plane robotic contains actuator elbow actuator shoulder shown figure focus swing-up task whereby agent needs actuate elbow actuator generate motion causes swing reach goal height shown ﬁgure. state space algorithm ropi require approximate value function parameters features arbitrary parameterized option policy parameters discount factor uncertainty nominal model given robust policy gradient present ropi deﬁned algorithm parameterized uncertainty parameters nominal model without uncertainty provided input ropi. practice uncerˆ tainty example conﬁdence intervals specifying plausible values mean normal distribution. nominal model normal distribution without conﬁdence intervals. iteration trajectories generated using nominal model current option policy trajectories utilized learn critic parameters line using rpvi stated background section rpvi converges ﬁxed point. converged ﬁxed point critic learn option policy parameters using r-pg update. policy improvement step. process repeated convergence. convergence theorem presented theorem theorem differentiable function approximators option policy value function respectively satisfy compatibility condition derived option policy differentiable second derivative respect deﬁne {αk}∞ maxθxaij step-size sequence satisfying limk→∞ deﬁned -tuple consists shoulder angle shoulder angular velocity elbow angle elbow angular velocity respectively. action space consists torques applied elbow rewards received agent reached goal received upon reaching goal. episode length timesteps. cartpole cartpole system involves balancing pole cart vertical position shown figure domain modelled continuous state mdp. continuous state space consists -tuple represent cart location cart horizontal speed pole angle respect vertical pole speed respectively. available actions constant forces applied cart either left right direction. agent receives reward timestep cart balances pole within goal region shown ﬁgure. agent terminates early reward received. length episode timesteps therefore maximum reward agent receives course episode. uncertainty sets domain generated uncertainty cartpole uncertainty pcartpole generated ﬁxing normal distribution length pole lpole sampling lengths distribution range meters prior training. sampled length substituted cartpole dynamics equations generating different transition functions. robust update performed choosing transition function uncertainty generates worst case value. acrobot uncertainty pacrobot generated ﬁxing normal distribution mass link marm shoulder elbow. five masses sampled distribution generated corresponding transition functions. nominal model training cartpole acrobot agent transitions according nominal transition model. cartpole nominal model corresponds pole length meters. acrobot nominal model corresponds mass evaluation agent executes learned policy transition models different parameter settings linear ropi asap ﬁrst tested online variation ropi cartpole domain using linear features. implemented robust version actor critic policy gradient critic updated using robust error shown equation used constant learning rate worked well practice. critic utilizes coarse binary features contain bins dimension respectively. provide actor limited policy representatrained agent nominal pole length meters. evaluation averaged performance learned policy episodes parameter setting parameters pole-lengths range meters. seen figure agent cannot solve task using limited policy representation pole length resulting fmm. mitigate misspeciﬁcation learn non-robust options using asap algorithm using single option hyperplane asap learns options option’s intra-option policy contains limited policy representation before. expected asap options mitigate solve pole lengths around nominal pole length meters trained. however struggle solve task signiﬁcantly different pole lengths surprise asap option learning algorithm able solve pole lengths meters shown figure even though trained nominal pole length meters. even grid searches learning parameters agent still solved task across pole lengths. compared robust version asap mitigated misspeciﬁcation solved task across multiple pole lengths expected. decided analyze learned ‘non-robust’ options asap algorithm. figure shows learned option hyperplane separates domain different options. axis represents axis region indicates learned option always executes force right direction. blue region indicates learned option executes force left direction. learned option execution regions cover approximately half state space option. therefore agent point figure pole length varies transition dynamics propagate agent slightly different regions state space case. however transitions generally keep agent correct option execution region coarseness option partitions providing natural form robustness. interesting observation since provides evidence linear approximate dynamic programming algorithms figure learned options option hyperplanes non-robust version asap. average reward performance robust options learns options solve cartpole acrobot. compared option misspeciﬁed agent training heads performed alternating optimization fashion online manner executing episode cartpole acrobot last hidden layer corresponding cartpole acrobot activated respectively backpropagation occurs respect relevant option head. network able learn options solve tasks seen figures cartpole acrobot respectively. however parameters tasks change option performance o-dqn domains degrades. especially severe cartpole seen ﬁgure. here robustness crucial mitigating model misspeciﬁcation uncertainty transition dynamics. incorporated robustness o-dqn form robust option network. network performs online version ropi able learn robust options solve multiple tasks online manner. main difference loss function incorporates robust update discussed section speciﬁcally robust error calculated presented ropi framework able learn options robust uncertainty transition model dynamics. ropi convergence guarantees requires deriving robust policy gradient corresponding robust compatibility conditions. ﬁrst work kind attempted learn robust options. experiments shown linear options learned using ‘non-robust’ asap algorithm natural form non-linear ropi ro-dqn non-linear setting train agent learn robust options mitigate model misspeciﬁcation multitask scenario. here learning agent needs learn option solve cartpole option solve acrobot using common shared representation single network experiment variant consisting fully-connected hidden layers weights layer relu activations. hyper-parameter values found appendix. optimize loss function using adam optimizer maximum episodes evaluation learned network averaged episodes parameter setting setting network struggles learn good features solve tasks simultaneously using common shared representation. typically oscillates suboptimally solving task resulting model misspeciﬁcation. average performance trained cartpole acrobot across different parameter settings shown figures respectively. setup multi-task learning better illustrates use-case robust options mitigating model misspeciﬁcation compared single task setup use-case less clear. while different modiﬁcations potentially added improve performance goal work show withmodiﬁcations options used mitigate model misspeciﬁcation. robustness solving cartpole coarseness option execution regions. however translate deep setting. here robust options crucial mitigating model uncertainty therefore model misspeciﬁcation. utilized ropi learn robust options ro-dqn learned robust options solve acrobot cartpole different parameter settings respectively. robust options used bridge sim-to-real robotic learning applications robotic policies learned simulations performance policy applied real robot. framework also provides building blocks incorporating robustness continual learning applications include robotics autonomous driving.", "year": 2018}