{"title": "Distributed Autonomous Online Learning: Regrets and Intrinsic  Privacy-Preserving Properties", "tag": ["cs.LG", "cs.AI"], "abstract": "Online learning has become increasingly popular on handling massive data. The sequential nature of online learning, however, requires a centralized learner to store data and update parameters. In this paper, we consider online learning with {\\em distributed} data sources. The autonomous learners update local parameters based on local data sources and periodically exchange information with a small subset of neighbors in a communication network. We derive the regret bound for strongly convex functions that generalizes the work by Ram et al. (2010) for convex functions. Most importantly, we show that our algorithm has \\emph{intrinsic} privacy-preserving properties, and we prove the sufficient and necessary conditions for privacy preservation in the network. These conditions imply that for networks with greater-than-one connectivity, a malicious learner cannot reconstruct the subgradients (and sensitive raw data) of other learners, which makes our algorithm appealing in privacy sensitive applications.", "text": "online learning become increasingly popular handling massive data. sequential nature online learning however requires centralized learner store data update parameters. paper consider online learning distributed data sources. autonomous learners update local parameters based local data sources periodically exchange information small subset neighbors communication network. derive regret bound strongly convex functions generalizes work convex functions. importantly show algorithm intrinsic privacy-preserving properties prove suﬃcient necessary conditions privacy preservation network. conditions imply networks greater-than-one connectivity malicious learner cannot reconstruct subgradients learners makes algorithm appealing privacy sensitive applications. online learning emerged attractive paradigm machine learning given ever-increasing amounts data collected everyday. eﬃciently reduces training time processing data once assuming training data available central location. many applications however assumption problematic. instance sensor networks deployed rain forests collect data autonomously. cost transmitting data central server prohibitively high. also sharing sensitive data might lead information leakage raise privacy concerns. example banks collect credit information customers might share data ﬁnancial institutions privacy concerns. similarly privacy concerns might prevent sharing patient records across hospitals. therefore desirable conduct distributed learning fully decentralized setting. speciﬁcally treat individual computational units network autonomous learner. learn model parameters independently local data sources pass estimation information neighbors communication network. distributed learning avoids sharing original sensitive data others storing data central location. paper consider general distributed autonomous online learning algorithm learn fully decentralized data sources. address important questions associated general algorithm. ﬁrst question distributed online learners perform compared optimal learner chosen hindsight. derive regret bound strongly convex functions. work closely related recent work nedic ozdaglar main diﬀerence lies analysis strongly convex functions naturally extends results second question topology computational network affects privacy preservation. answer question draw ideas modern control theory model distributed online learning algorithm structured linear time-invariant system establish theorems necessary suﬃcient conditions malicious learner reconstruct subgradients learners locations. based conditions conclude communication topologies namely connectivity greater algorithm inherently prevents reconstruction subgradients locations therefore avoiding information leakage. unlike previous works privacy-preserving learning mostly alter original learning algorithms patching cryptographical tools secure multiparty computation randomization data aggregation privacy-preserving properties intrinsic sense require modiﬁcations algorithm solely determined communication network topology distributed learners. reconstructability local subgradients topology communication network implies privacy preservation local data well-chosen communication networks. notation lower case letters denote vectors upper case letters denote matrices. denote element i-th column subscripts used indexing parameter vector respect time superscripts used indexing respect processor. instance denotes parameter vector i-th processor time denote i-th basis vector denote vector ones. unless speciﬁed otherwise refers denotes euclidean product revealed learner incurs convex loss learner adjusts parameter vector. succinctly denote online learning equivalent solving following optimization problem stochastic fashion communication doubly stochastic matrix shall autonomous learners exchange information neighbors. communication pattern deﬁned weighted directed graph m-by-m adjacency matrix doubly stochastic. recall matrix said doubly stochastic elements non-negative rows columns one. depend size topology example spectral famous spectral geometric bound duchi examined impact diﬀerent choices network topologies convergence rate dual averaging algorithm distributed optimization. since relationship network topology convergence rate focus paper bound given chapter paper simplicity related minimum non-zero values easy show regret bounds modiﬁed accordingly general markov mixing bound. distributed autonomous online learning assume local online learners using data stored local sites. trial data points given i-th learner updates model parameters based i-th point. learner produces parameter vector used learners exchange information selected neighbors updating communication pattern amongst processors assumed form strongly connected graph. particular assume directed weighted graph whose adjacency matrix doubly stochastic. interpret entry importance learner places parameter vector communicated learner course learners send data learner present general online learning algorithm solving here. speciﬁcally local learner propagates parameter learners. receiving parameters learners learner updates local parameter linear combination received parameter. local learner updates local model parameter based data collected local subgradient. cooperation learners learn model distributed data sequentially. algorithm summarized algorithm analysis make following standard assumptions assumed hold proofs theorems presented below. strongly convex modulus learner communicates learner. assume irreducible aperiodic exists deﬁned closed convex subset non-empty interior. subgradient computed every diameter diam supxx∈ω bounded optimal solutions denoted non-empty. norm subgradients identically initialized. algorithm reduces classical sequential online learning. accordingly bounds become classical square root regret logarithmic regret recall every time processors simultaneously process data points. therefore steps learners process data points. bounds rewritten respectively. must borne mind algorithm aﬀected limiting factors. first limited information sharing diﬀerent learners. second deﬁnition regret algorithm forced predict data points shot single parameter vector contrast sequential online learner access full data diﬀerent parameter vectors data points. treat distributed parameters across learners single aggregated parameter apply results sequential online learning obtain generalization bounds distributed online learning terms regret bounds. space limitation present generalization bounds appendix. common form dient w.r.t. thus algorithms transmit subgradients disclose sensitive information data undesirable privacy-sensitive applications mentioned mining patient information across hospitals. decentralized algorithm transmits local model parameters neighbors network reducing possibility information leakage. figure illustrating impact network topology privacy preservation. three-node networks malicious node wants gather subgradients easily reconstruct subgradients diﬀerentiating successive parameters received cannot reconstruct subgradients intuitively receive information parameters mixed parameters subgradients. intuitively topology communication graph aﬀect privacy-preserving capability. consider examples ﬁgure gain intuition. assume nodes know matrix representing communication graph convex suppose malicious node wants gain information input data recovering subgradients. based communication graph figure receives parameters received parameters compute linear combination subgradient. contrast intuitively diﬃcult recover subgradients based communication graph figure parameters mixed parameters linear combination local subgradient step sent directly receive information ambiguity parameters prevents malicious node correctly reconstructing local subgradients inspired examples formally examine conditions malicious node cannot reconstruct subgradients nodes based parameter vectors adjacent nodes. refer problem full reconstruction subgradient contrast partial reconstruction subgradients discussed later. assume moment i.e. projection step algorithm projection handled diﬀerently later. throughout section shall following deﬁnitions notations. also assume every learner knows whole communication matrix initial parameter values learners. without loss generality also assume dimension since reconstructed row-by-row. theorem nodes connected almost choice nonzero entries output sequence malicious node gives rise unique sequence subgradients ˜gt. hand nodes connected regardless choice nonzero entries output sequence uniquely specify ˜gt. nodes connected malicious node reconstruct duplicating linear combination steps nodes diﬀerentiating successive parameter vectors. exactly happens ﬁgure proof latter part theorem relies analysis generic rank structured systems standard control notation treat state system column vector systems written state vectors paper written vectors order maintain consistency rest paper. reconstructing subgradients nodes severely constrained topology communication graph malicious node turn reconstruct subgradients nodes. logical step forward full reconstruction problem partial reconstruction. given nodes topological requirements communication graph allows malicious node reconstruct subgradients nodes. suitable matrices align input corresponding columns. instead considering invertibility consider partial invertibility output next theorem relates partial invertibility topological properties communication satisfy conditions theorem nodes form reconstruct subgradients nodes duplicating linear combination local subgradient steps node similar full whose recovery depends knowledge initial parameters proof necessity signiﬁcantly harder full reconstruction long proof given appendix. theorem conﬁrms intuition saying theory developed guide examine design communication networks privacy-preserving properties. deﬁne privacypreserving communication network following. nodes called vertex directed graph removal nodes renders graph disconnected. connectivity graph size smallest vertex cut. suppose communication network privacyshown many interesting networks including studied duchi privacy-preserving. example grid nodes aligned -dimension grid connected nearest neighbors; k-dimension hyper-cube nodes placed vertices imaginary k-dimension hyper-cube connected neighbor vertices; expander graphs construct expander graphs large connectivity. graphs good mixing properties. separate inputs node node simply propagates summation certain types convex sets hyper-balls polytopes easy diﬀerent data vectors projection value. formally following theorem proof theorem follows similar line theorem except diﬀerent topological arguments. theorem exercised caution. possible gain information subgradients presence priori knowledge. example ball ηtgi co-linear summation −ηtgi figure convergence distributed learning synthetic real datasets. datasets distributed online learning algorithm uses nodes linked hypercubes. converges test error rate sequential online learning. convergence distributed learning diﬀerent communication graphs consisting nodes synthetic data. communication graphs grids hypercubes algorithm converges slightly slower communication graphs cliques. unlike cliques grids cliques prevent malicious nodes reconstructing subgradients nodes. recently research eﬀort devoted devising distributed online learning. instance zinkevich shows distribute data slave nodes. slaves periodically poll centralized master node receive latest parameter vector. used compute stochastic gradients back master node expense using delayed subgradients. bounds form similar ours. decentralized learning paradigm pioneered distributed optimization. example duchi proposed dual averaging algorithm distributed convex optimization. provided sharp bounds convergence rates function network size topology careful mixing time arguments. zinkevich proposed perform local stochastic gradient descent individually give output average local parameters ﬁnal step. however ﬁxed step size assumption guarantee algorithm converge true optimum. terms algorithmic structures underlying mathematical foundations algorithm natural extension works nedic ozdaglar distributed convex optimization online learning analysis handles strongly convex function yields regret. regret bounds converted convergence rates obtain rates convex functions also rates strongly convex functions covered nedic ozdaglar except work zinkevich obviously privacy-preserving lack communication none works considered privacy-preserving aspect algorithms. privacy-preserving active research area machine learning data mining. privacy-preserving machine learning algorithms modify original algorithms cryptographic tools achieve privacy preservation. popular techniques secure multi-party computation randomization. example privacy-preserving versions linear regression belief propagation/gibbs sampling online prediction discrete values securely compute function values distributed data without disclosing unwanted identities; privacy-preserving logistic regression uses randomized perturbation modify cost function preserve data privacy. many algorithms association rule mining decision tree either randomization achieve privacy preservation compared algorithms using randomization analysis privacy require modiﬁcation original algorithm. privacy-preserving properties intrinsic sense relies component algorithm communication graph prevent disclosure local subgradients nodes. dients approach privacy preservation closely related aggregation-based methods conceptual level. example r¨uping trains support vector machines using group probability subsets data. avidan butman proposed boosting based privacy-preserving face detection algorithm restricting learner limited features provided data feeder. drawback algorithms sacriﬁce algorithm performance data privacy revealing aggregated limited information. contrast algorithm achieves asymptotic convergence rate sequential algorithm ﬁxed number learners. conduct simulations illustrate quickly generalization error distributed learning algorithm converges given certain number nodes examine impact topology communication graphs convergence rate. implementations form {±}} training data available hinge loss function max{ robustness learning rate first investigate number nodes aﬀects predictive performance algorithm synthetic datasets. synthetic data generated uniformly -dimension unit ball. classiﬁer randomly sampled less labels based true classiﬁer ﬂipped wrong labels. total generate training test examples. second dataset actually subset dataset. subset contains training examples test examples features many zero entries sample. figures summarize results. line theoretically guarantee regret distributed algorithm converges test error algorithm even nodes indeed converges sequential learner datasets. second experiment construct three types communication graphs consisting nodes grid nodes laid connected mesh grid; hypercube nodes laid connected -dimensional hypercube; clique nodes form clique. shown ﬁgure clique topology leads slightly faster convergence grid hypercube discloses subgradients presence malicious nodes according theorem analyzed case communication matrix ﬁxed evolve time. proofs extended settings asynchronous update random communication studied nedic ozdaglar resulting linear systems time-invariant much harder analyze. however conjecture privacy-preserving properties still hold transient network connectivity greater upon update step. equipped probability measure. random variables denoted capital letters e.g. assume functions according unknown distribution risk deﬁned common form cost function functions number functions theorem bounds risk regret rda. theorem random. inequality gives bound risk best aggregated parameter strongly convex functions translates convergence rate proof theorem generalization bound sequential online learning cesa-bianchi gentile based bernstein’s martingale inequality. proof interpret hypotheses training examples. proposition gives next relate rank condition topological property communication graph. construct directed graph adding input nodes node communication graph edges input nodes corresponding ηtgi respectively. deﬁnition suggests following consistent deﬁnition vertex disjoint path starting placing also forms vertex disjoint path. conclude rank) rank). therefore sequence cannot determine unique sequence subgradients", "year": 2010}