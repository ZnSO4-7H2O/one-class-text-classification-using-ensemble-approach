{"title": "A Deterministic and Generalized Framework for Unsupervised Learning with  Restricted Boltzmann Machines", "tag": ["cs.LG", "cond-mat.dis-nn", "cs.NE", "stat.ML"], "abstract": "Restricted Boltzmann machines (RBMs) are energy-based neural-networks which are commonly used as the building blocks for deep architectures neural architectures. In this work, we derive a deterministic framework for the training, evaluation, and use of RBMs based upon the Thouless-Anderson-Palmer (TAP) mean-field approximation of widely-connected systems with weak interactions coming from spin-glass theory. While the TAP approach has been extensively studied for fully-visible binary spin systems, our construction is generalized to latent-variable models, as well as to arbitrarily distributed real-valued spin systems with bounded support. In our numerical experiments, we demonstrate the effective deterministic training of our proposed models and are able to show interesting features of unsupervised learning which could not be directly observed with sampling. Additionally, we demonstrate how to utilize our TAP-based framework for leveraging trained RBMs as joint priors in denoising problems.", "text": "restricted boltzmann machines energy-based neural-networks commonly used building blocks deep architectures neural architectures. work derive deterministic framework training evaluation rbms based upon thoulessanderson-palmer mean-ﬁeld approximation widely-connected systems weak interactions coming spin-glass theory. approach extensively studied fullyvisible binary spin systems construction generalized latent-variable models well arbitrarily distributed real-valued spin systems bounded support. numerical experiments demonstrate eﬀective deterministic training proposed models able show interesting features unsupervised learning could directly observed sampling. additionally demonstrate utilize tap-based framework leveraging trained rbms joint priors denoising problems. past decade witnessed groundswell research machine learning bolstered deep learning revolution resurgence neural networks since inception researchers identiﬁed deep connection neural networks statistical mechanics. perhaps well-known unsupervised neural models studied lens statistical physics hopﬁeld model boltzmann machine models proposed connectionist perspective cognitive science studied context emergent representation unsupervised machine learning. look hopﬁeld model directly observe contributions physics machine learning cognitive sciences. example applying techniques study spin-glasses amit famously able derive memory capacity hopﬁeld model provide concrete understanding dynamics model study phase transitions. fundamental understanding behavior hopﬁeld model provided insight complexities associative memory. f.k. laboratoire physique statistique ecole normale supérieure lhomond paris france université pierre marie curie sorbonne universités paris france. interested inverse problem learning couplings spins order generate particular conﬁgurations equilibrium. process learning couplings training often referred inverse ising problem physics literature however couplings exist pairs spins fully-visible ising spin-glass models limited practical application cannot successfully capture higher-order correlations might exist training conﬁgurations. reason general boltzmann machine introduces unobserved latent spins. eﬀect latent spins abstract high-order correlations within observed spins. optimal training couplings would potentially lead eﬀective general model high-dimensional joint distributions intractability joint latent model confounds practical application general boltzmann machines. restricted boltzmann machine special case general boltzmann machine couplings exist latent observed spins. bipartite structure eﬃcient eﬀective training rbms rbms found many applications machine learning problems diverse dimensionality reduction classiﬁcation collaborative ﬁltering feature learning topic modeling additionally rbms stacked multi-layer neural networks played historically fundamental role pre-training deep network architectures constructions known deep belief networks ﬁrst truly deep neural architectures leading current explosion activity deep learning access vast training datasets made pre-training dispensable certain tasks rbms remain fundamental tool theory unsupervised learning. such better understanding rbms future developments emergent machine intelligence. paper organized follows. sec. formally describe classical binary review literature training analysis. subsequently sec. describe proposed modiﬁcation binary model arbitrary real-valued distributions bounded support. next sec. brieﬂy describe apply belief-propagation perform inference setting real-valued spins. details approach pedagogically described appendices sec. derive approximation real-valued high-temperature expansion two-moment gibbs free energy. then sec. detail convert approximation practical training algorithm. sec. conduct series experiments real datasets demonstrating properties machine interpretation provide insight unsupervised learning process. additionally show trained model bit-ﬂip correction simple example leveraging machine inference tasks. lastly appendix detail derivations necessary distribution-speciﬁc functions. restricted boltzmann machines latentvariable generative models often used context unsupervised learning. weights biases model parameters correspond couplings local ﬁelds present system constructs energy function data points follows gibbs-boltzmann probability density function. well-known binary visible latent variables distribution refer sums entire space possible conﬁgurations visible latent variables respectively. taken respect parameters model known partition function. give factor-graph representation distribution fig. evidenced exact computation normalizing partition function thus probability given high-dimensional data point inaccessible practice. sophisticated monte carlo schemes relying importance sampling produce estimates short-chain monte carlo sampling cover detail sequel. techniques yield trained rbms produce sampled conﬁgurations similar target dataset used number applications detailed previously bridge understanding learned. furthermore understanding modes internal representations sampling-based frameworks mostly consisted subjective comparisons sampled conﬁgurations well subjective analysis couplings themselves often referred receptive ﬁelds machine learning literature. additionally comparing trained models even monitoring training model becomes problematic using sampling-based investigative tools. example annealed techniques provide estimates log-likelihood model large computational cost much lower computational cost pseudo-likelihoods used monitor training estimates produced manner inaccurate compared annealed importance sampling even fail detect model divergence practice present work seek address concerns developing deterministic framework train compare analyze rbms well leverage modeling power inference tasks. accomplish statistical physics techniques thouless-anderson-palmer formalism spin-glass theory manner produce model longer refers stochastic model possessing intractable gibbs measure machine entirely self-consistent mean-ﬁeld model operates classical admits deeper introspection deterministic inference. machines also naturally handle non-binary variables well deep architectures. deep boltzmann machines’ state-of-the-art training algorithms monte carlo sampling naïve mean-ﬁeld approximation deep machine relies entirely mean-ﬁeld approximation. interpretation machine generative probabilistic model deterministic model deﬁning representational magnetizations given training dataset. advantageously learning output computed exactly ﬁnite time converging ﬁxed-point iteration contrast indeterminate stopping criterion markov-chain monte carlo sampling. major distinction machine classical true probability density function intractable. core machine training consists arranging minima solutions proposed tap-approximated free energy maximize correlation solutions dataset. experiments demonstrate track growth geometry solutions novel investigate progress unsupervised learning. gradient ascent parameters commonly calculate gradients data-point training instead calculates gradients average across data-points often referred mini-batch. mini-batch gradients given ·sampled refers averages particles sampled model refers so-called clamped expectations values ﬁxed training data samples mini-batch. case expectations involving hidden units unobserved therefore training data originally proposed conﬁgurations sampled however could also exact conditional expectations directly calculate clamped averages; especially cases sampling conditionals problematic. since number proposed modiﬁcations core sampling-based training scheme described above. persistent trick takes neatly advantage iterative gradient ascent mini-batches quickly obtain thermalized markov chains gibbs sampling extra computational cost step nevertheless probability density function trained typically highly multimodal thus making sampling inexact. indeed glassy landscapes mixing becomes slow markov chains become stuck metastable states leading overunder-represented well missed modes highdimensional distribution. turn produces high variance estimates means correlations. accurate sampling achieved using parallel tempering particles swapped multiple markov chains running diﬀering temperatures. approach however requires additional computational burden running chains also requires tuning hyper-parameters number chains temperatures them. accurate sampling-based inference rbms costly would seem usefulness limited. learning gradient ascent dependent upon inference diﬃculty training generative model high-degree accuracy compounded. however rbms proven useful many applications sampling full-ﬂedged generative model unneeded. instance rbms used unsupervised feature extraction pre-training feedforward networks rbms also used data-imputation tasks e.g. image in-painting label recovery collaborative ﬁltering reconstructing missing data single visible-hidden-visible step. truth cd-k training algorithm popularized rbms ﬁrst binary units gaussian units fig. factor graph representation distribution. variables indicated circles latent variables denoted shaded circles. shaded rectangles indicated layer partitions within structure. factors represented squares right hand side factors representing pairwise relationships variables left hand side factors representing inﬂuence localized prior distributions variables. thankfully precise estimate normalization unnecessary many applications. additionally bipartite structure admits couplings hidden visible variables leveraged construct eﬃcient sampling schemes. approach demonstrated contrastive divergence short-chain block-gibbs sampling shown suﬃcient adequate training. approach consists sampling chain alternating samples drawn conditional probabilities layer dependent conditional expectations previously sampled layer. speciﬁcally conditional probabilities hidden visible units factorize indicates visible hidden units model parameters deﬁning respectively varilocal distributions ables variables case {±}nv {±}nh distribution reduces bipartite spin glass model representing local ﬁelds acting system spins; ﬁelds binary spins described speciﬁc case simply binary described previous section already considered within extended mean-ﬁeld framework important distinction model evaluate assume binary discrete distribution variables instead allow formulation variables system possess distributions discretereal-valued bounded support. considering general class models include wide range diﬀerent models including hopﬁeld model spike-and-slab rbms data sets images genomic data varying distributions hidden visible units. distribution visible variables obtained marginalizing latent variables ﬁnally arbitrary units thermalized samples evaluate means correlations. instead focuses region conﬁguration space nearest training dataset using short block-gibbs markov chains starting training data points fast variance estimates moments. however cd-k prone learn spurious minima conﬁguration space data explore region training also systematically increase true likelihood training data however training strategy found eﬃcient applications mentioned above consistently remain close dataset conﬁguration space. ﬁnds falls short applications require long-chain mcmc sampling trained represents fundamental mismatch training application rbm. order address theses shortcomings sampling-based approaches turn attention deterministic mean-ﬁeld approximations rbm. approximation disordered systems relies deterministic inference approximated magnetizations obtain estimators kinds observables starting log-partition free energy. derived small weight expansion variational approach considered extension naïve mean-ﬁeld method previous works attempted make approximation shown negative results approximation ﬁrst considered boltzmann machines context small random models without hidden units recent work approximation extended practical training algorithm full-scale binary rbms shown competitive persistent contrastive divergence applied real-world datasets. parallel works used related bethe approximation perform inference binary boltzmann machines next sections detail re-write model non-binary case generalized distributions visible hidden units similar spirit however unlike earlier techniques approach problem estimating normalization model tools statistical mechanics resulting fully-deterministic framework inference training application. computes conditional expectation knowing value visible units. important note data-dependent term tractable thanks bipartite structure one-hidden layer rbm. contrast data-dependent terms second terms eqs. require knowledge partials normalization w.r.t. parameter interest. however term cannot written exactly explicit calculation normalization intractable. rather resorting sampling attempt method might estimate partition belief-propagation review appendix pairwise models rbm. essentially given factor graph joint statistical model fig. algorithm attempts estimate marginal distributions variable. case tree-like graphs provides exact calculation marginals. application factor graphs containing cycles loopy guaranteed provide accurate estimates marginals. however often estimated marginals signiﬁcant overlap true ones additionally known solutions loopy ﬁxed-points bethe free energy allows construction approximation applying inverse learning problem compute gradients bethe free energy terms parameters model allowing gradient ascent bethe-approximated log-likelihood training data. signiﬁcant hurdle application loopy learning real-valued variables messages propagated edges factor graph continuous pdfs. case discrete variables ising potts spins messages written using magnetizations full discrete respectively. binary variables mean-ﬁeld approximations fully-connected boltzmann machines considered context inference ﬁxed parameters. similar study binary rbms conducted loopy important note studies investigated properties boltzmann machines i.i.d. random weights. studies permit many analytical tools studying behavior cannot directly observations inference practice trained weights exhibit order construct algorithm pdfs realvalued support requires ﬁnite memory description messages. examples descriptions given non-parametric moment matching relaxed appendix following example r-bp show arrive two-moment approximation continuous messages smallweight expansion coupling parameters there also show r-bp approximated free energy pairwise models well demonstrating need distributions bounded support order preserve bounded messages. could utilize r-bp approach order estimate free energy generalized real-valued spin model detailed earlier section approach might desirable practice. speciﬁcally wishes solve inverse learning problem estimating model parameters given dataset necessary estimate gradients model parameters w.r.t. model likelihood parameter update. using steepest-ascent approach detailed sec. requires estimate gradients many thousands times. systems large size r-bp scales quite poorly. estimating gradient requires iteration r-bp equations messages. additionally must distinguish cavity terms marginal terms ﬁnal gradients desire estimated using marginal terms alone requiring iteration messages extremely costly operation. instead turn mean-ﬁeld approach writing free energy stationary points terms marginals alone. done including certain correction terms speciﬁed degree weights. context rbms approaches proposed order naive mean-ﬁeld order using so-called thoulessanderson-palmer equations introducing additional correction term case grbm arbitrary distributions unit however must re-derive approximation terms parameters distributions well approximate marginalized distribution site ﬁrst moments. task turns closely related approach low-rank matrix factorization average augmented system given auxiliary ﬁelds. since partial derivatives ﬁeld-augmented helmholtz free energy generate cumulants boltzmann distribution shown deﬁne solution auxiliary ﬁelds deﬁned make explicit dependence auxiliary ﬁeld solutions values conjugate variables. looking stationary points auxiliary ﬁelds taylor expansion rather focus free energy directly provide gradients require training grbm parameters high-temperature expansion present below. lastly point free energy secondorder term depends statistical properties weight distribution. derivation presented below assumes independent identically distributed weights scaling assumption simpliﬁcation practice weight distribution cannot known priori. distribution depends training data changes throughout learning process according training hyper-parameters. adaptive formalism attempts correct assumption allowing directly compute correct second-order correction term realization matrix without hypothesis entries distributed. although algorithm principled approach computational complexity almost rules implementation. moreover practical learning experiments indicate training using adatap diﬀer signiﬁcantly assuming i.i.d. weights. detailed discussion computational complexity learning performance described appendix limit assume entries scale sites widely connected order size system apply approximation high temperature expansion gibbs free energy second-order case boltzmann distribution global minima gibbs free energy value equilibrium matches helmholtz free energy derive twovariable parameterization gibbs free energy derived legendre transform additionally show two-variable gibbs free energy variational attains helmholtz free energy minima. clarity notation make derivation terms pairwise interacting hamiltonian without enforcing speciﬁc structure couplings; bipartite structure reintroduced section wish derive two-variable gibbs free energy system terms ﬁrst moments marginal distributions site. accomplish this proceed ﬁrst deﬁning augmented conversely deriving stationarity conditions auxiliary ﬁelds obtain self-consistency equations show free energy valid following self-consistencies hold substituting values closes free energy marginal distribution moments completes derivation free energy approximation deﬁned elements versus values required r-bp. true event solutions auxiliary ﬁelds truly looking inverse legendre transform gibbs free energy implies minimum gibbs free energy equivalent helmholtz free energy. holds since −βgβ convex lagrange multipliers given functions temperature order make clear order apply later. exact form gibbs free energy intractable original free energy apply taylor expansion order generate approximate gibbs free energy make expansion limit inﬁnite temperature interactions sites vanish system described terms individual sites relationship system average local potentials allowing fig. cartoon description estimating helmholtz free energy gibbs free energies. example convex gibbs free energy exists unique minimum moments gibbs free energy matches range free energies gives boundary location averaging free energies provides estimate solutions computed iterating selfconsistency equations easily probe region initializing iteration according training data. subsequently encounter band high-energy solutions must weight against. instead obtain solutions small region support free energy. uniformity solutions un-weighted averaging across solutions seems best approach terms eﬃciency. subsequent section explore properties numerically trained rbms. utilize inference sec. need write free energy terms variables rbm. clarify bipartite structure grbm rewrite free energy terms hidden visible variables ﬁxed temperature given free energy valid self-consistency equations stationary points. thus certain physical meaning. additionally know minima exact gibbs free energy correspondence original exact helmholtz free energy. exact gibbs free energy terms moments convex exponential family free energy possess multiple stationary points whose number increases rapidly grows later sec. show grbm training progresses number identiﬁed solutions. explained variance weights growing training. ﬁxed practical grbm implementation variance weights serves eﬀective inverse temperature increasing magnitude identical eﬀect system cooling increases. additionally gibbs free energy correspondence helmholtz free energy minimum necessarily true free energy. approximate nature second-order expansion removes correspondence. thus possible ascertain accurate estimate helmholtz free energy single inferred shown fig. case naïve mean-ﬁeld estimate implies order accurate estimate foundational principle variational approaches. however extra expansion term free energy improve accuracy modeling provide lower bound estimate free energy could underover-estimate. instead might attempt obtain estimate helmholtz free energy utilizing either subset equilibrium solutions free energy. since manner might distinguish equilibrium moments proximity unknown averaging free energy across solutions denoted serve simple estimator weighting introduced average correcting helmholtz free energy estimate temperature removing over-inﬂuence exponential number high-energy solutions. weights approach proportional exponents solution’s free energy placing much stronger emphasis low-energy solutions. however approach well-justiﬁed general setting expect large deviations expectations derived model. additionally weighting scheme shown across entire solutions particular random model case interested solution space centered particular dataset wish model. since presented gradients make point data samples solutions diﬀerent cardinality. example might employ mini-batch strategy training data samples used gradient calculation might order however depending application grbm might desire probe large number solutions order accurate picture representations learned grbm. case might start large number initializations resulting large number unique solutions. contrary might start number initializations equal number unique solutions might especially early training number hidden units small. using gradients simple gradient ascent ﬁxed monotonically decreasing step-size used update grbm parameters. present ﬁnal grbm training algorithm alg. besides considering non-binary units another natural extension traditional rbms consider additional hidden layers deep boltzmann machines possible deﬁne train deep machines well. probabilistic dbms substantially harder train rbms data-dependent terms gradient updates become intractable depth. interestingly state-of-the-art training algorithms retain monte carlo evaluation intractable terms introducing naıve mean-ﬁeld approximation data-dependent terms. deep machines consistently utilize equations. explicit deﬁnition training algorithm fully described appendix bears much resemblance iteration derived context compressed sensing matrix factorization note rather updating entire system time step ﬁxing side time eﬀect stabilizing ﬁxedpoint iteration. clarity alg. written single initialization visible marginals. however noted sec. exist large number initializationdependent solutions free energy. thus order capture plurality modes present free energy landscape inference independently many diﬀerent initializations. mnist mnist handwritten digit dataset consists training testing samples respectively. data samples real-valued pixel -bit grayscale images normalize dynamic range images centered crops digits roughly balanced proportion. construct separate versions mnist dataset. ﬁrst refer binary-mnist applies thresholding pixel values others second real-mnist simply refers normalized dataset introduced above. cbcl cbcl face database consists face non-face -bit grayscale pixel images. experiments utilize face images. database contains training testing samples face images. experiments normalize samples dynamic range fig. training performance epochs tested datasets varying numbers hidden units performance measured terms normalized log-likelihood estimate computed training data samples. free energy estimated using unique solutions thermalized initial conditions drawn data samples thermalization determined convergence magnetizations diﬀerence iterations. solid lines indicate average normalized log-likelihood tested training samples shaded regions indicate standard error. investigate behavior grbm course learning procedure looking metrics interest tap-approximated log-likelihood training dataset free energy number discovered solutions. note metrics unique tap-based model grbm. fig. subsets ﬁnal receptive ﬁelds i.e. columns obtained training grbm models varying numbers hidden units receptive ﬁelds dark blue yellow mapped respectively green indicates value receptive ﬁelds ranked according criteria. first spread conversely localization measured p-norm receptive ﬁeld second activation level measured mean activation receptive ﬁeld’s corresponding hidden unit averaged across training dataset. shown indeed maximize log-likelihood case binary rbms speciﬁc construction entirely independent model grbm. thus hard tap-trained grbm better general case. present present comparisons grbms varying complexity trained ﬁxed hyper-parameters settings indicated table fig. comparison log-likelihood function training epochs binary-mnist binary rbms consisting diﬀering numbers hidden units. gradient-ascent log-likelihood performed batch-by-batch training data deﬁne epoch single pass training data every example presented gradient ascent once. speciﬁcs particular experiment given caption. note equal comparison across varying model complexity log-likelihood normalized number visible hidden units present model. observe per-unit log-likelihood gives measure concentration representational power encapsulated unit model. increasing values normalized log-likelihood indicate evaluated training samples becoming likely given state grbm model parameters. observed level complexity log-likelihood data rapidly increases values quickly adjust random initializations receptive ﬁelds correlated training data. however across tested models epoch rate increase log-likelihood tapers constant rate improvement. reference also show subset trained receptive ﬁelds i.e. rows tested experiments. since full receptive ﬁelds would large display attempt show representative samples fig. looking extreme samples terms spatial spread/localization activity training set. observe trained grbms case binary-mnist real-mnist able learn localized stroke features commonly observed literature binary rbms trained mnist dataset interesting note even case real-mnist using novel implementation truncated gauss-bernoulli visible units able observe similar learned features case binary-mnist. take empirical indication proposed framework grbm learning truly learning correlations present dataset intended. finally feature localization increase number hidden units. date understanding what learns unlabelled data mostly purely subjective exercise studying receptive ﬁelds shown fig. however interpretation grbm machine provide novel insight nature dynamics grbm learning stationary points free energy detail next section. given deterministic nature framework possible investigate structure modes given grbm parameters produces free energy landscape. understanding nature concentration modes gives intuition representational power grbm. fig. distribution free energy estimates solutions function training epochs three diﬀerent datasets. case mnist experiments number hidden units samples drawn training data used initial conditions. cbcl training samples used. free energy unique solutions helmholtz free energy estimate uniform averaging number unique solutions also given bottom detail free energy distributions slices training. histograms given bars kernel density estimates free energy distribution given curves. date observing modes given grbm model could approached long-chain sampling. given enough sampling chains diverse initial conditions thermalizing chains produces samples could attempt derive statistics concentrations samples high-dimensional space attempt pinpoint likely modes model. however number required chains resolve features increases dimensionality space number potential modes might exist space. this numerical evaluation carry would impractical sampling techniques. r-bp mean-ﬁeld models allow directly obtain modes model running inference solve direct problem. given diverse initial conditions given training dataset running r-bp provides deterministic mapping initial conditions drawn data nearest solution free energy. initial point drawn dataset solution interpreted rbm’s bestmatching internal representation data point. large number structurally diverse data points single solution indicator grbm parameters suﬃcient model diverse nature data perhaps changes model parameters hyper-parameters required. conversely number solutions explodes roughly equivalent number initial data points indicates potential spin-glass phase speciﬁc over-trained perhaps memorizing original data samples training. additionally phase large solutions replete spurious solutions convey little structural information dataset. case hyper-parameters model need tuned order ensure model possess meaningful generalization data space. observe eﬀects obtain subset solutions initializing iteration initial conditions drawn data running iteration convergence counting unique solutions. present measures solutions fig. here count number unique solutions well distribution free energy solutions across training epochs. common features across tested datasets. first early phase training shows marked increase free energy gradually declines training continues. comparing point inﬂection free energy normalized loglikelihood shown fig. shows early phase grbm training dominated reinforcement empirical moments training data grbm model correlations playing small role gradient makes sense random initialization ing. positive data-term dominant grbm parameters appear minimize free energy would expect. however solutions appear data model terms gradient become balanced free energy minimized. point inﬂection leveling normalized log-likelihood. second observe free-energy bands solutions. feature especially pronounced case binary-mnist experiment. here training epochs exist signiﬁcant modes free energy distribution solutions. eﬀect clearly training-slice histograms shown bottom fig. case real-mnist experiment free energy distributions exhibit tight banding show presence highlow-energy solutions persist across training. main feature across experiments multi-modal structure free energy distribution. finally note real-mnist binary-mnist case don’t empirically observe explosion solutions potential indicator spin-glass phase since proportion unique solutions initial data points remains less order investigate whether modes free energy distributions randomly assigned conﬁguration space exist separate continuous partitions conﬁguration space need look proximity solutions conﬁguration space. space cannot observed ambient dimensionality project conﬁguration space two-dimensional embedding fig. here utilize well known isomap algorithm calculating two-dimensional manifold approximately preserves local neighborhoods present original space. using visualization observe training progresses assignment high free energy solutions appear random nature seems inherent structure solutions themselves location conﬁguration space. additionally progression solutions many spread across conﬁguration space. interesting note solutions start highly correlated state proceed diversify. also observe solutions respect initializations produced them shown fig. charts similar approach fig. mapping high-dimensional data points well magnetizations embedding using isomap. allows approximate solutions distribute data space. also show number solutions grows many training maintain spread distribution data space. demonstrates training procedure altering parameters model place solutions within dense regions data space. sake clarity included lines indicating attribution initial data point resultant solution. however training progresses sees solutions attractors data space clustering together data points machine recognizes similar. serving prior inference particular case machine interpretation grbm. simple demonstration turn common signal processing task denoising. speciﬁcally given planted signal observes noisy observations measures true signal corrupted stochastic process. denoising tasks ubiquitous signal processing analog level level digital communications goal task produce accurate estimate unknown signal. analog case measure mean-square-error estimate true signal. binary case measure accuracy counting number incorrect estimates function binary confusion matrix f-score matthews correlation coeﬃcient ﬁxed observations channel parameters assume original signal drawn unknown intractable generating distribution construct accurate tractable approximate priors accurately construct estimate original signal. words know structure content unknown signal priori closer estimate often case wavelet-based image denoising statistics gathered transform coeﬃcients particular images classes heuristic denoising approaches designed by-hand accordingly by-hand derivation denoising algorithms works well practice owing generality. speciﬁc priori information original signal required beyond signal class however meaningful features must assumed investigated practitioners successful inference take place. fig. isomap visualization solutions binary-mnist training epochs solutions mapped two-dimensional embedding isomap transform ﬁtted solutions epoch embedding performed hidden visible inferred expectations color mapping corresponds free energy values solution range colors normalized minimum maximum free energies solution training epoch. fig. comparison initial conditions equilibration compared converged solutions tested datasets diﬀerent stages training. dataset two-dimensional isomap embedding calculated initialization data. subsequently magnetizations solutions embedded space. case initial variances also random selection solution magnetizations chosen provide context representations learning. binary-mnist here digits corresponding classes drawn ﬁrst training samples binarized mnist dataset initializations. reduced labels used readability. real-mnist initializations used however initializations binarized. cbcl available training face images used initializations. binary denoising problems assume binary symmetric channel deﬁned following manner. given binary signal observe signal independent ﬂips occurring probability gives following might per-site empirical averages obtained available training data posterior factorizes construct bayes-optimal pointwise estimator average binary problem. thus site given given dataset gives best-case performance using pointwise statistics dataset namely empirical estimates magnetizations either returns observations case prior magnetizations case case complete information loss worst case performance bounded according deviation dataset mean. present performance fig. binary-mnist dataset. makes valuable baseline comparison sanity check grbm approximation grbm model takes account pointwise pairwise relationships data properly trained grbm provide estimates least good ope. k-nearest-neighbor algorithm represents diﬀerent heuristic approach problem case noisy measurements compared exemplars training dataset. then according distance metric correlation ﬁnds exemplars minimal distance noisy observations serve basis recovering original binary signal. arbitrary approach fusing exemplars together ﬁnal estimate simplest case would simple average. case performance using averaging bounded empirical magnetizations. limit estimate simply nearest exemplar. hard show limiting performance approach dependent distances chosen metric exemplars observations well interplay noise channel distance metric. fig. average denoising performance reconstruction bit-ﬂip errors binary-mnist probability ﬂipped. denoising inference binary-binary denoised varying numbers hidden units also shown baseline comparisons given empirical factorized magnetizations site matching training experiments data samples drawn held-out test compared using mcc. binary estimates obtained inferred estimates rounding resulting magnetizations. signal unless true signal contained within training data. show performance fig. advantage approach successfully regularizes noise nearest exemplar always noise-free least marginally correlated original signal distance metric. additionally performs better regime explained since think k-nn approach implicitly though indirectly taking account higherorder correlations dataset naïvely returning data exemplars; estimates trivially posses arbitrarily complex structure unknown signal. using grbm hope capture best points approaches. first hope perfectly estimate original signal case second hope leverage pairwise correlations present dataset returning estimates retain structure data even grbm denoising longer factorized posterior. instead grbm likelihood given summed hidden units. using deﬁnition binary chanexponential finding averages model simply consists running tap-based inference alg. modiﬁed visible binary prior heuristic caveat approach must take account multi-modal nature free energy. since must initialize somewhere resulting inference estimate dependent upon initialization initialize inference result. highest probability conﬁguration becomes observations. limit able obtain true signal especially since initialize within well potential. shown binary rbms trained varying numbers hidden units fig. every case true signal recovered. case inference binary always outperforms ope. additionally cases performance closely mirrors limit result inference essentially uncorrelated original signal case extra potential present bias inference resulting estimate simply arbitrary solution free energy. closely mirrors exemplar selection curves approaches similar. case over-training eﬀect occurs. essentially values inference binary able accurately identify original signal. however certain point owing increased number solutions free energy exist many undesirable minima around noisy solutions leading poor denoising estimates. observe subjectively fig. case inference results either nearly zero-modes localized ones. would seem indicate landscape free energy around initializations becoming unstable density solutions increases around additionally since free energy landscape probed using data points training clustering solutions around noisy samples remains ambiguous. augmenting initializations used calculating solutions gradient estimate noisy data samples could help alleviate problem regularize free energy landscape space noisy data samples. subjective comparison denoising estimates fig. single digit image approaches inferred posterior averages shown rather ﬁnal conﬁgurations white black represent values respectively. tested value noise realization used method. increases inference provides estimates still possess digit structure. case inference gets caught spurious undesirable minima increases. paper proposed novel interpretation within fully tractable deterministic framework learning inference approximation. deterministic construction allows novel tools scoring unsupervised models investigation memory trained models well allowing eﬃcient structured joint priors inverse problems. deterministic methods based training shown inferior cd-k level approximation accuracy aﬀorded ﬁnally makes deterministic approach rbms eﬀective shown case binary rbms additionally construction generalized distribution hidden visible units. unique work works propose unique training methods models changing distribution visible units. example seen modiﬁed hamiltonians used real-valued data construction allows consider binary real-valued sparse real-valued datasets within framework. additionally also consider architectures changing distributions imposed hidden unit. here present experiments using binary hidden units could also proposed framework gaussian-distributed hidden units thus mimicking hopﬁeld network also sparse gaussbernoulli distributed hidden units could mimic functionality proposed spike-and-slab left investigations works topic. message-passing constructed factor graph writing messages variables factors also factors variables. since factors degree write messages system variable variable messages here notation represents message variable index variable index ∂i/j refers neighbors variable index except variable index denote neighboring variables share pairwise factor. finally super-scripts messages refer time-index iteration implies successive application convergence {νi→j messages pairs neighboring variables. also note inclusion message normalization term zi→j ensures messages valid pdfs. additionally possible write marginal beliefs variable collecting messages neighbors ever general case formulation cannot make assumption. instead turn relaxed described next section assumes two-moment parameterization messages. proposed framework also oﬀers possibility explore statistical mechanics latent variable models level approximation. speciﬁcally given statistical model weights cavity method replica begin make predictions unsupervised models. analytical understanding complexity free energy landscape transitions function model hyper-parameters allow richer understanding statistically optimal network construction learning tasks. case random networks already progress area shown however similar comprehensive studies conducted learning realistic setting still realized. finally framework applied deep boltzmann machines minimal alteration also potentially lead richer understanding deep networks role hierarchy regularizing learning problem high dimensionality. research funded european research council european union’s framework programme m.g. acknowledges funding \"chaire recherche modèles sciences données\" fondation pour recherche-ens. order estimate derivatives must ﬁrst construct algorithm factor graph representation given fig. note graph terms variables make explicit distinction latent visible variables. instead treat graph full generality clarify derivation notation. graph corresponds following unfortunately message-passing cannot written computable algorithm continuous nature pdfs. instead must manner parameterize messages. case binary variables message exactly parameterized expectation. howhere closed equations dependence moments vice versa. values moments written function dependent upon form local potentials i.e. prior distribution assign variables themselves wants obtain estimate free energy given parameters possible iterate until ideally convergence. important note however potentially loopy nature network well smallweight expansion iteration guaranteed converge additionally retain time-indices derivation clear whether attempt iterate message fully sequential parallel fashion clustering partitioning variables applied determine update order dynamically. consider possible parametric approximation message r-bp approach also gone number diﬀerent names parallel re-discoveries approach e.g. moment matching non-parametric essence assuming messages well-approximated mean variance gaussian assumption. approximation arises second-order expansion assuming small weights wij. making assumption ultimately able close approximation considering marginalization taking place perform second order expansion assuming start taking taylor series incoming message marginal negligible weights approximate series dropping terms less approximation justiﬁed event weight values satisfy |wil| identifying integrals expansion moments following approximation however would like write approximation terms central second moment. second approximation neglects terms arrive desired parameterization incoming message marginalization terms message’s ﬁrst central moments additionally write speciﬁc form bethe free energy r-bp two-moment parameterization messages. case simply apply small weight expansion bethe free energy smaller prior messages become unbounded fail meaningful probability distributions expansion fails. implication r-bp message passing utilized contexts exists some preferably strong evidence site weights suﬃciently small. stronger local potential smaller weights favorable model r-bp inference. observation mirrors made however authors make observation setting based small-weight expansion binary variables fails converge. case without taking form regularization inference fails entirely. thus large magnitude couplings must backed high degree evidence site property could utilized inverse learning problem must learn couplings given dataset order constrain learning parameters amenable r-bp inference. direct manner create probability distributions otherwise unbounded continuous functions truncation. speciﬁcally enforce non-inﬁnite normalization factor restricting support distribution subset case slightly violating bounded condition induce uniform message distribution distribution support strong violation cause distribution concentrate boundaries support. another approach might simply hard boundary constraint ai→j thus never permitting unbounded messages occur. mean variance original gaussian prior truncation range deﬁnes lower upper bounds truncation normal distribution. make things easier later deﬁne prior little diﬀerent manner making following deﬁnitions write r-bp messages though gaussian distributions slight since implication expansion that general messages fact unbounded. unboundedness direct result form conventional pairwise factor exiwij avenues available address unbounded messages produce meaningful messagepassing generalized rbms. consider cases messages unbounded given speciﬁc variable distribution. assume site assigned next write variance function earlier since speciﬁc form truncated gaussian distribution utilize well-known variance formula distribution. case modify function special case speciﬁcally determine gradients log-likelihood w.r.t. model parameters necessary calculate gradients terms distribution parameters assume boundary terms remain ﬁxed. since distributions truncated gaussians treat terms derivatives normalization general-case truncated gaussian detail computation partition ﬁrst moments gaussian-product distribution ax+bx. calculation moments function provide deﬁnitions calculation normalization provide terms necessary computation free energy well gradients necessary learning training. normalization terms free parameters. this consider truncated normalization following product gaussians since simply truncated gaussian updated parameters ﬁrst moment given according well known truncated gaussian expectation. expectation usually written terms mean variance un-truncated gaussian distribution case positive mean instead write expectation terms exponential polynomial n-term power series representation error function. experiments similar approximation used variances situation. approximation could potentially computationally costly large note used updates variables small value d+/− detected. function everywhere else. using construction truncation done gaussian mode alone across entire distribution easily write necessary functions distribution terms values already calculated appendix long additionally useful calculations deﬁne probability non-zero according continue previous appendices write normalization ﬁrst moments gaussian product distribution ax+bx. first normalization written simply function truncated gaussian normalization modiﬁed form erf] erf] rewritten erf−erf multiple diﬀerence boundaries truncated gaussian distribution. since wish consider case take taylor expansion centered since value dominates. here following approximation works well practice finally ready write gradients loglikelihood used hidden visible updates truncated gaussian distribution. given mini-batch data indexed number solutions indexed gradients visible variable given averages mini-batch solutions respectively. updates hidden side variables using truncated gaussian distribution following gradients updating parameters complicates matters observing term d+/− occurs ﬁrst second moment computations. magnitude limits become vanishingly small comparison scaled joint mean term d+/− however numerators also zero implies able method approximation estimates moments without numerical precision dividing zero. next turn attention log-likelihood gradients necessary updating parameters training. first look derivatives required updates visible units. order calculate derivatives split probability cases compactly deﬁned blocks visible hidden units. deﬁned diagonal matrix gives estimate correlation diﬀerent units must computed step algorithm. quantities incorporated training algorithm alg. computational burden alg. lies matrix inversion needed evaluate needs performed iteration. fig. compare time needed perform iteration algorithms identical experimental conditions. larger cost per-iteration compensated principle accurate inference procedure. however seem translate improvements training performance. fig. presents minimal test mnist training samples performances reported terms pseudo-likelihood. evaluate algorithms diﬀerent numbers iterations. results suggest strategies roughly equivalent except running adatap small number iterations always leads poorer result. possible deﬁne well deep models boltzmann machines considering several stacked hidden layers. deep boltzmann machines consist straightforward extension rbms. distribution corresponding hidden layers indexed performing inference could employ instead variant known adaptive gives general accurate results albeit slower iterate. brieﬂy investigate performance method binary case. adatap algorithm generally presented without distinction visible hidden variables. thus write algorithm generic weight matrix bias vector practice deﬁned blocks proposed implementation alg. uses recently introduced vector approximate message-passing adatap ﬁxed points. convergence quantities subscripts equal identify outputs inference algorithm alg. major diﬀerence rbms dbms lies complexity evaluating expression. whereas rbms log-partition features problematic multidimensional integral logpartition last term intractable. additional complication carries computation gradients necessary training since datadependent term deriving last term longer tractable. intractability follows fact hidden units neighboring layers connected other thus longer conditionally independent. interestingly ﬁrst proposal deal datadependent terms dbms consisted using naive mean-ﬁeld approximation keeping monte carlo based strategy compute gradients deriving log-partition. work propose instead approximation them hence improving approximation avoiding sampling rather complicated rbms. thus implementing grbm inference algorithm alg. proper weights outputs solutions vector components corresponding diﬀerent units dbm. last term recognize log-partition model closely related considered visible units anymore variable ﬁxed clamped values original interaction between visible ﬁrst hidden layer units replaced additional local ﬁeld finally simple modiﬁcation hamiltonian equations follow general derivation sec. resultant solutions depending data points said clamped denoted ¯a}. gradients log-likelihood respect model parameters similar ones given however ﬁrst data-dependent term cannot analytically computed anymore clamped solutions approximate second term evaluated using data-independent solutions similarly strategy rbms. corresponding expressions gradients expressions plugged gradient ascent algorithm training algorithm alg. nevertheless simple strategy simultaneous training parameters model usually fails magnitude weights deep layers typically remains small model eventually resembles mere rbm. several regularizations proposed tackle well-known problem training experiments used greedy layerwise pre-training consists computing meaningful initialization weights training rbms layer-by-layer performing joint training. complete algorithm described alg. fig. left evolution pseudo-likelihood along training binary visible units binary hidden units. training performed using ﬁrst images binarized mnist learning rate batches size diﬀerent curves correspond diﬀerent strategies estimation likelihood gradients either adatap. algorithms iterated ﬁxed number times cases damping used. methods yield comparable results terms training performance except adatap iterations shows poorer performance. right computation time iteration inference algorithm function batch size. time reported seconds identical experimental settings. need matrix inversion batch element makes vamp orders magnitude slower tap. fig. training performances training epochs -hidden layer -hidden layer deep boltzmann machines binarized mnist datasets. models pretrained epochs learning rate training performance measured normalized log-likelihood test images train images ricci-tersenghi bethe approximation solving inverse ising problem comparison inference methods stat. mech. ekeberg lövkvist weigt aurell improved contact prediction proteins using pseudolikelihoods infer potts models phys. rev. goodfellow bengio courville deep learning http//www.deeplearningbook.org. tieleman training restricted boltzmann machines using approximations likelihood gradient proc. int. conf. machine learning schulz müller behnke investigating convergence restricted boltzmann machine learning deep learning unsupervised feature learning nips workshop smolensky information processing dynamical systems foundations harmony theory desjardins courville bengio vincent delalleau parallel tempering training restricted boltzmann machines ricci-tersenghi bethe approximation solving inverse ising problem comparison inference methods stat. mech. mézard mean-ﬁeld message-passing equations arxiv lesieur krzakala zdeborová mmse probabilistic low-rank matrix estimation universality respect output channel proc. allerton conf. communication control computing krzakala mézard sausset zdeborová probabilistic reconstruction compressed sensing algorithms phase diagrams threshold achieving matrices stat. mech. şendur selesnick bivariate shrinkage functions wavelet-based denoising exploiting interscale dependency ieee trans. sig. processing ginneken mendrik image denoising k-nearest neighbor support vector regression proc. int. conf. pattern recognition", "year": 2017}