{"title": "What makes ImageNet good for transfer learning?", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "The tremendous success of ImageNet-trained deep features on a wide range of transfer tasks begs the question: what are the properties of the ImageNet dataset that are critical for learning good, general-purpose features? This work provides an empirical investigation of various facets of this question: Is more pre-training data always better? How does feature quality depend on the number of training examples per class? Does adding more object classes improve performance? For the same data budget, how should the data be split into classes? Is fine-grained recognition necessary for learning good features? Given the same number of training classes, is it better to have coarse classes or fine-grained classes? Which is better: more classes or more examples per class? To answer these and related questions, we pre-trained CNN features on various subsets of the ImageNet dataset and evaluated transfer performance on PASCAL detection, PASCAL action classification, and SUN scene classification tasks. Our overall findings suggest that most changes in the choice of pre-training data long thought to be critical do not significantly affect transfer performance.? Given the same number of training classes, is it better to have coarse classes or fine-grained classes? Which is better: more classes or more examples per class?", "text": "tremendous success imagenet-trained deep features wide range transfer tasks raises question imagenet dataset makes learnt features good are? work provides empirical investigation various facets question looking importance amount examples number classes balance images-per-class classes role coarse grained recognition. pre-train features various subsets imagenet dataset evaluate transfer performance variety standard vision tasks. overall ﬁndings suggest changes choice pre-training data long thought critical signiﬁcantly affect transfer performance. become increasingly common within computer vision community treat image classiﬁcation imagenet itself rather pretext task training deep convolutional neural networks learn good general-purpose features. practice ﬁrst training perform image classiﬁcation imagenet adapting features target task become facto standard solving wide range computer vision problems. using imagenet pre-trained features impressive results obtained several image classiﬁcation datasets well object detection action recognition human pose estimation image segmentation optical image captioning others dataset forces representation general. others argue large number distinct object classes forces network learn hierarchy generalizable features. others believe secret sauce large number classes fact many classes visually similar turning ﬁne-grained recognition task pushing representation work harder. almost everyone computer vision seems opinion topic little empirical evidence produced far. work systematically investigate aspects imagenet task critical learning good general-purpose features. evaluate features ﬁne-tuning three tasks object detection pascalvoc dataset action classiﬁcation pascal-voc dataset scene classiﬁcation dataset section details. paper organized experiments answering list questions feature learning imagenet. following summary main ﬁndings many pre-training imagenet examples sufﬁcient transfer learning? pre-training half imagenet data results small drop transfer learning performance drop much smaller drop imagenet classiﬁcation task itself. section figure details. many pre-training imagenet classes sufﬁcient transfer learning? pre-training order magnitude fewer classes results small drop transfer learning performance curiously also found transfer tasks pre-training fewer classes leads better performance. section figure details. figure change transfer task performance pre-trained varying number images imagenet class. left y-axis mean class accuracy used imagenet cls. right y-axis measures pascal action-cls. number examples class reduced random sampling. accuracy imagenet classiﬁcation task increases faster compared performance transfer tasks. figure change transfer task performance varying number pre-training imagenet classes. number imagenet classes varied using technique described section pre-training classes transfer performances unaffected small drop observed classes used pretraining. imagenet classiﬁcation performance measured ﬁntetuning last layer original -way classiﬁcation. important ﬁne-grained recognition learning good features transfer learning? features pre-trained subset imagenet classes require ﬁnegrained discrimination still demonstrate good transfer performance. section figure details. pre-training coarse classes produce features capable ﬁne-grained recognition imagenet itself? found trained classify coarse imagenet classes produces features capable telling apart ﬁne-grained imagenet classes whose labels never seen training likewise trained classify imagenet classes able distinguish unseen coarse-level classes higher wordnet hierarchy given budget pre-training images classes images class? training fewer classes images class performs slightly better transfer tasks training classes fewer images class. section table details. data always helpful? found training imagenet classes exclude pascal classes achieves nearly performance pascal-det training complete imagenet. further experiments conﬁrm blindly adding training data always lead better performance sometimes hurt performance. section table details. number papers studied transfer learning cnns including various factors affect pre-training ﬁne-tuning. example question whether pretraining terminated early prevent over-ﬁtting layers used transfer learning studied thorough investigation good architectural choices transfer learning conducted propose approach ﬁne-tuning tasks without forgetting ones. contrast works ﬁxed ﬁne-tuning central downside supervised pre-training large quantity expensive manually-supervised training data required. possibility using large amounts unlabelled data feature learning therefore attractive. numerous methods learning features optimizing auxiliary criterion data proposed. well-known criteria image reconstruction comprehensive overview) feature slowness unfortunately features learned using methods turned competitive obtained supervised imagenet pre-training force better feature generalization recent self-supervised methods difﬁcult data prediction auxiliary tasks effort make cnns work harder. attempted selfsupervised tasks include predictions ego-motion spatial context temporal context even color sound features learned using methods often come close imagenet performance date none able beat table transfer performance network pre-trained using classes obtained top-down clustering wordnet tree comparable transfer performance ﬁnetuning imagenet classes. indicates ﬁnegrained recognition necessary learning good transferable features. code provided faster-rcnn report performance test set. finetuning pascal-det performed adapting pre-trained convolution layers alexnet. model trained iterations using stochastic gradient descent initial learning rate reduction factor iteration. pascal-act-cls used pascal train/val ﬁnetuning testing using experimental setup code provided r*cnn ﬁnetuning process pascal-act-cls mimics procedure described pascal-det. sun-cls used train/val/test splits used finetuning performed ﬁrst replacing layer alexnet model randomly initialized fully connected layer output units. finetuning performed iterations using initial learning rate reduced factor every iterations. faster-rcnn r*cnn known variance across training runs; therefore three times report mean standard deviation. hand reports little variance runs sun-cls report result using single run. experiments pre-train imagenet using different number images class. model images/class uses original imagenet ilsvrc training set. models images/class trained drawing random sample images images class made available part imagenet training set. answering question trained different alexnet models scratch using images imagenet classes using procedure described section variation performance amount pre-training data models ﬁnetuned pascal-det pascal-act-cls figure illustration bottom procedure used construct different label sets using wordnet tree. node tree represents class leaf nodes shown red. different label sets iteratively constructed clustering together leaf nodes common parent. iteration leaf nodes clustered. procedure results sequence label sets images consequent contains labels coarser previous one. wordnet tree imbalanced even multiple iterations label sets contain classes present imagenet challenge. reasonable middle ground expensive fully-supervised pre-training free unsupervised pretraining weak supervision. example yfccm dataset million flickr images labeled noisy user tags pre-training instead imagenet. again even though yfccm almost orders magnitude larger imagenet somewhat surprisingly resulting features appear give substantial boost pre-trained imagenet. overall despite keen interest problem alternative methods learning general-purpose deep features managed outperform imagenet-supervised pretraining transfer tasks. process using supervised learning initialize parameters using task imagenet classiﬁcation referred pre-training. process adapting pretrained continuously train target dataset referred ﬁnetuning. experiments caffe implementation single network architecture proposed krizhevsky refer architecture alexnet. closely follow experimental setup agrawal evaluating generalization pre-trained features three transfer tasks pascal object detection pascal action recognition scene classiﬁcation dataset pascal-det used pascal train/val ﬁnetuning using experimental setup figure trained discriminating coarse classes learns feature embedding capable distinguishing classes? quantiﬁed measuring induction accuracy deﬁned following training feature embedding particular classes induction accuracy nearest neighbor classiﬁcation accuracy measured feature space subset imagenet classes present syntax x-axis classes indicates network trained classes induction accuracy measured classes. baseline accuracy accuracy classes trained classes. margin baseline induction accuracy indicates drop network’s ability distinguish classes trained coarse classes. results show features learnt pre-training classes still lead fairly good induction. sun-cls shown figure pascal-det mean average precision cnns images/class found similar trend observed pascal-act-cls suncls. results indicate using half amount pre-training data leads marginal reduction performance transfer tasks. important note performance imagenet classiﬁcation task steadily increases amount training data whereas transfer tasks performance increase respect additional pre-training data signiﬁcantly slower. suggests adding additional examples imagenet classes improve imagenet performance diminishing return transfer task performance. previous section investigated varying number pre-training images class effects performance transfer tasks. investigate side keeping amount data constant changing nomenclature training labels. classes imagenet challenge derived leaves wordnet tree using tree possible generate different class taxonomies keeping total number images constant. generate taxonomies ways bottom clustering wherein leaf nodes belonging common parent iteratively clustered together ﬁxing distance nodes root node using bottom clustering possible taxonomies generated. among these chose sets labels constituting classes respectively. using top-down clustering label sets generated used classes. studying effect number pretraining classes transfer performance trained separate alexnet cnns scratch using label sets. figure shows effect number pre-training classes obtained using bottom clustering wordnet tree transfer performance. also include performance different networks imagenet classiﬁcation task ﬁnetuning last layer distinguish classes. results show increase performance transfer tasks signiﬁcantly slower increase number classes compared performance imagenet itself. using classes results performance drop pascal-det accuracy sun-cls boost pascalact-cls. table shows transfer performance pre-training classes obtained clustering. results table ﬁgure indicate diminishing returns transfer performance observed classes used. results also indicate making imagenet classes ﬁner help improve transfer performance. argued pascal task requires discrimination classes therefore pre-training classes lead substantial reduction performance. however trend also holds true sun-cls requires discrimination classes. results taken together suggest although training large number classes beneﬁcial diminishing returns observed beyond using distinct classes figure feature embeddings obtained training coarse classes able distinguish classes never trained e.g. training monkeys network pick macaques? look nearest neighbors randomly sampled images macaque giant schnauzer showing feature embeddings trained different number classes dotted line indicate image class training classes whereas rows image class present training set. images green indicate image belongs correct class orange indicates correct coarse class incorrect class; indicated incorrect coarse class. green images dotted line indicate instances correct ﬁne-grain nearest neighbor retrieval features never trained class. furthermore pascal-act-cls sun-cls ﬁnetuning cnns pre-trained class sizes actually results better performance using classes. indicate many classes pre-training works learning good generalizable features. hence generating dataset attentive nomenclature classes. imagenet challenge requires classiﬁer distinguish classes ﬁne-grained different breeds dogs cats. indeed humans perform well imagenet unless speciﬁcally trained easily able perform everyday visual tasks. raises question ﬁne-grained recognition necessary models learn good feature representations coarse-grained object recognition sufﬁcient? note label classes previous experiment contains classes present original classes remainder inner nodes wordnet tree. however classes represent coarse semantic concepts. discussed earlier pre-training classes results small drop transfer performance suggests performing ﬁne-grained recognition marginally helpful appear critical learning good transferable features. earlier shown features learned coarse classes perform almost well transfer tasks full imagenet classes. probe asking different question feature embedding induced coarse class classiﬁcation task capable separating labels imagenet investigate this used top- top- nearest neighbors feature space measure accuracy identifying ﬁne-grained imagenet classes training coarse classes. call measure induction accuracy. qualitative example figure shows nearest neighbors macaque schnauzer feature embeddings trained imagenet different number classes. greenborder images dotted line indicate instances correct ﬁne-grain nearest neighbor retrieval features never trained class. quantitative results shown figure results show classes used ﬁne-grained recognition k-nn performance lower compared training directly ﬁne-grained classes rather surprising suggests cnns implicitly discover features capable distinguishing ﬁner classes attempting distinguish relatively coarse classes. table ﬁxed budget pre-training data better examples class fewer classes vice-versa? ‘more examples/class‘ pretrained subsets imagenet containing classes examples each. ‘more classes‘ pretrained classes examples each. interestingly transfer performance pascal appears broadly similar scenarios. table pascal-det results pre-training entire imagenet pascal-removed-imagenet places data sets. removing pascal classes imagenet leads insigniﬁcant reduction performance. investigate this split amount pretraining data ways classes fewer images class fewer classes images class. datasets size images experiment. images considered ways constructing training classes images/class classes images/class. similar splits made data budgets images. classes experiments drawn uniform distribution among imagenet classes. similarly image subsets containing images drawn uniform distribution among images belong class. results presented table show images class fewer number classes results features perform slightly better pascaldet whereas sun-cls performance comparable across settings. natural expect higher correlation pretraining transfer tasks leads better performance transfer task. indeed shown true possible source correlation pre-training figure network learn discriminate coarse semantic concepts training ﬁner sub-classes? degree concept coarse class learnt quantiﬁed measuring difference accuracy classifying coarse class average accuracy individually classifying sub-classes coarse class. here bottom classes sorted metric shown using label size classes least subclasses. observe classes whose subclasses visually consistent better represented visually dissimilar investigating whether network learns features relevant ﬁne-grained recognition training coarse classes raises reverse question training ﬁnegrained classes induce features relevant coarse recognition? indeed case would expect makes error likely confuse sub-class sub-classes coarse class. effect measured computing difference accuracy classifying coarse class average accuracy individually classifying sub-classes coarse class figure shows results. coarse semancontain classes mammal fruit bird etc. visually similar sub-classes show hypothesized effect whereas classes tool home appliance contain visually dissimilar subclasses exhibit effect. results indicate subclasses share common visual structure allow learn features generalizable. might suggest improve feature generalization making class labels respect visual commonality rather simply wordnet semantics. classes examples class? results previous sections show possible achieve good performance transfer tasks using signiﬁcantly less pre-training data fewer pre-training classes. however unclear important number classes number examples class. exfigure illustration procedure used split imagenet dataset. splits constructed different ways. random split selects classes random imagenet classes. minimal split made manner ensures classes split common ancestor depth four wordnet tree. collage figure visualizes random minimal splits. transfer tasks classes common tasks. order investigate strong inﬂuence common classes experiment removed classes imagenet contained pascal challenge. pascal classes imagenet class thus applying exclusion criterion left imagenet classes. table compares results pascal-det pascal-removed-imagenet used pre-training original imagenet baseline pretraining places dataset. pascal-removedimagenet achieves indicating training imagenet classes present pascal sufﬁcient learn features also good pascal classes. analysis using pascal-removed imagenet indicates pre-training non-pascal classes aids performance pascal. raises question always better pre-training data additional classes part target task? investigate test hypothesis chose different methods splitting imagenet classes. ﬁrst random split imagenet classes split randomly; second minimal split classes deliberately split ensure similar classes split order determine additional data helps performance classes split pre-trained cnns classifying classes split classifying classes split ﬁnetuned last layer network trained full dataset split only. case addifigure visualization random minimal splits used testing adding pre-training data always useful? minimal sets contain disparate sets objects. minimal split consists mostly inanimate objects living things respectively. hand random splits contain semantically similar objects. using random split figure shows results experiment conﬁrms intuition additional data indeed useful splits. however random class split within imagenet almost certain extremely similar classes ending different sides split. shown improve performance husky classiﬁcation also training poodles. hence motivation minimal split adding arbitrary unrelated classes trucks help classiﬁcation? classes minimal split share common ancestor minimal split nodes depth wordnet hierarchy ensures class split sufﬁciently disjoint split split classes split classes order intuitively understand difference splits visualized random sample images splits figure split consists mostly static images split consists living objects. contrary earlier observation figure shows split performs better full dataset ﬁnetune last layer. result quite surprising shows ﬁnetuning last layer network pre-trained full dataset possible experiments investigated architecture alexnet. imagenet-trained alexnet features currently popular starting point ﬁne-tuning transfer tasks exist deeper architectures resnet googlenet would interesting ﬁndings hold deeper networks. might suggest alexnet capacity less previously thought. results might indicate researchers overestimating amount data required learning good general features. case might suggest training data-hungry previously thought. would also suggest beating imagenet-trained features models trained much bigger data corpus much harder thought. finally might currently popular target tasks pascal similar original imagenet task really test generalization learned features. alternatively perhaps appropriate approach test generalization much less ﬁne-tuning ﬁne-tuning conclusion answer titular question what makes imagenet good transfer learning? still lacks deﬁnitive answer results shown folk wisdom imagenet works well accurate. hope paper pique colleagues’ curiosity facilitate research fascinating topic. work supported part muri n---. gratefully acknowledge nvidia corporation donation gpus access nvidia cluster research. would like acknowledge support berkeley vision learning center berkeley deepdrive minyoung partially supported rose hill foundation. figure adding arbitrary classes pre-training data always improve transfer performance? question tested training cnns classifying classes split classifying classes split both. ﬁnetuned trained splits split case adding pre-training data helps performance pre-trained splits higher pre-trained single split random splits indeed case whereas minimal splits adding pre-training data hurts performance. suggests additional pre-training data useful correlated target task. match performance network trained split. observed training layers extensive amount time accuracy split beneﬁt pre-training split split explanation could images split contained images split vice versa. might possible recover performance clever adjustments learning rates current results suggest training data unrelated classes push network local minimum might hard better optima obtained training network scratch. work analyzed factors affect quality imagenet pre-trained features transfer learning. goal consider alternative neural network architectures rather establish facts aspects training data important feature learning. quote inﬂuential r-cnn paper ..success resulted training large million labeled images... publication r-cnn researchers assumed full imagenet necessary pre-train good general-purpose features. work quantitatively questions assumption yields quite surprising results. example found sig-", "year": 2016}