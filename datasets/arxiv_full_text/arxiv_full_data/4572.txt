{"title": "Learning Symbolic Models of Stochastic Domains", "tag": ["cs.LG", "cs.AI"], "abstract": "In this article, we work towards the goal of developing agents that can learn to act in complex worlds. We develop a probabilistic, relational planning rule representation that compactly models noisy, nondeterministic action effects, and show how such rules can be effectively learned. Through experiments in simple planning domains and a 3D simulated blocks world with realistic physics, we demonstrate that this learning algorithm allows agents to effectively model world dynamics.", "text": "article work towards goal developing agents learn complex worlds. develop probabilistic relational planning rule representation compactly models noisy nondeterministic action eﬀects show rules eﬀectively learned. experiments simple planning domains simulated blocks world realistic physics demonstrate learning algorithm allows agents eﬀectively model world dynamics. goals artiﬁcial intelligence build systems complex environments eﬀectively humans perform everyday human tasks like making breakfast unpacking putting away contents oﬃce. many tasks involve manipulating objects. pile things objects boxes drawers arrange shelves. requires understanding world works depending objects pile arranged made pile sometimes slips falls over; pulling drawer usually opens sometimes drawer sticks; moving typically break items inside building agents perform common tasks challenging problem. work approach problem developing rule-based representation agents model learn eﬀects acting environment. learning allows agents adapt environments without requiring humans hand-craft models something humans notoriously especially numeric parametrization required. representation probabilistic relational includes additional logical concepts. present supervised learning algorithm uses representation language build model action eﬀects given example action executions. optimizing tradeoﬀ maximizing likelihood examples minimizing complexity current hypothesis algorithm eﬀectively selects relational model structure model parameters language relational concepts together provide compact highly accurate description action eﬀects. agent hopes real world must integrated system perceives environment understands commands motors eﬀect changes unfortunately current state reasoning planning learning perception locomotion manipulation removed human-level abilities cannot contemplate figure three-dimensional blocks world simulation. world consists table several cubes roughly uniform density varying size robotic gripper moved simulated motors. popular proxy used since beginning work planning world stacking blocks. typically formalized version logic using predicates clear describe relationships blocks another. blocks always neatly stacked; don’t fall jumbles. article present work context slightly less ridiculous version blocks world constructed using three-dimensional rigid-body dynamics simulator example world conﬁguration shown figure simulated blocks world blocks vary size colour; piles always tidy sometimes fall over; gripper works medium-sized blocks unreliable even there. approach capable enabling eﬀective behavior domain must handle noisy nondeterministic nature nontrivial dynamics able handle domains similar characteristics. strategy formulating approach learn models world’s dynamics planning diﬀerent courses action based goals change time. another strategy assume ﬁxed goal reward function learn policy optimizes reward function. worlds complexity imagining would impossible establish advance appropriate reaction every possible situation; addition expect agent overall control architecture hierarchical individual level hierarchy changing goals. reasons learn model world dynamics make plans achieve goals hand. reasonable alternative approach advocated brooks working real world natural complexity solving problems almost ridiculously simpliﬁed proxies problems interest. learning models language. validate models introduce simple planning algorithm provide empirical results demonstrating utility learned models showing plan them. finally survey relevant previous work draw conclusions. agent introduced novel world must best possible explanation world’s dynamics within space possible models represent deﬁned agent’s representation language. ideal language would able compactly model every action eﬀect agent might encounter others. extra modeling capacity wasted complicate learning since agent consider larger space possible models likely overﬁt experience. choosing good representation language provides strong bias algorithm learn models language. languages used describe deterministic planning models least surface ﬁrst order; abstract particular identities objects describing eﬀects actions terms properties relations among objects. representational capacity crucial reasons compactness generalization usually grossly ineﬃcient describe behavior individual objects. much original work probabilistic planning uses formalism markov decision processes represents states world individually atomically recently propositional representations dynamics employed ﬁrst-order representations developed including probabilistic rules equivalence classes situation calculus approach boutilier reiter price representations also make easy articulate take direct advantage useful assumptions world dynamics frame assumption states that agent takes action world anything explicitly changed stays same outcome assumption states action aﬀects world small number distinct ways possible eﬀect causes changes world happen together single outcome. take point departure probabilistic ﬁrst-order representations world dynamics. representations traditionally applied domains logistics planning traditional abstract blocks world idealized symbolic abstractions underlying domain. goal learn models realistic worlds requires adapt modeling language accommodate additional uncertainty complexity. action parameterization traditional representations action dynamics objects whose properties changed result action must named argument list action. instead deﬁne actions parameters describe objects free parameters action example block picked object currently held block placed. however actions change properties objects ones parameter list models determining objects aﬀected. paper introduce deictic references identify objects. deictic references identify objects relative agent action performed. example refer objects thing block picked currently held object table block accidentally falls onto. deictic references mechanism adding logical variables models much benson modeling noise complex domains actions aﬀect world variety ways. must learn model circumstances reasonable eﬀects also behavior unusual situations. complicates dynamics makes learning diﬃcult. also actions executed physical world guaranteed small number simple eﬀects result violate outcomes assumption. blocks world happen example stack knocked over. develop simple noise mechanism allows partially model action eﬀects ignoring ones rare complicated model explicitly. language extension traditional symbolic domains rules constructed using predeﬁned observable predicates. however sometimes useful deﬁne additional predicates whose truth values computed based predeﬁned ones. found essential modeling certain advanced planning domains traditional blocks worlds example usual predicates contains clear inhand. working realistic noisy blocks world found predicates would suﬃcient allow agent learn accurate model. example would diﬃcult state putting block tall stack likely cause stack topple without concept stack height state attempting pick block clear usually picks block stack without describing block stack. could simply additional predicates seem useful perceptual language hand-engineering appropriate language every time tackle problem diﬃcult time consuming error prone. state-of-the-art planning representations pddl concept language deﬁne predicates concepts terms previous simpler ones. paper show concepts learned much like predicates invented traditional blocks world predicates including inhand clear well useful concepts height easily deﬁned terms given simple concept language goal learn model state transition dynamics world. need able represent possible states world possible actions agent take. represent components using subset relatively standard ﬁrst-order logic equality. representation states actions ground inference learning planning. begin deﬁning primitive language includes constants predicates functions three types functions traditional functions range objects; discrete-valued functions range predeﬁned discrete values; integer-valued functions range ﬁnite subset integers. primitives observed directly world. constants assumed intrinsic meaning viewed meaningless markers assigned perceptual system described detail below. states describe possible diﬀerent conﬁgurations properties relations objects. state describes particular conﬁguration values objects world individual objects denoted using constants. limit number objects world conﬁguration though current formalism mechanism creation deletion objects result world dynamics. arity predicate function constants length lists elements indicates predicates optionally negated indicates functions assigned value range. manner states list truth values possible groundings predicates functions terms. sentence gives complete speciﬁcation vocabulary properties interrelations objects present world. ﬁrst approach state descriptions refers objects using intrinsic constants. intrinsic constant associated particular object consistently used denote object. constants useful perceptual system unique identify perceived objects independent attributes relations another. example internet software agent might access universal identiﬁers distinguish objects perceives. example consider representing states simple blocks world using language contains predicates clear inhand inhand-nil block table integer-valued function height. objects world include blocks block-a block-b table table gripper. blocks blocks table. block nothing clear. gripper hold block empty. sentence alternatively states also denote objects using skolem constants. skolem constants arbitrary identiﬁers associated objects world inherent meaning beyond used state description. constants useful perceptual system assigning meaningful identiﬁers objects observes. example consider robot might build state description room ﬁnds assume robot observe objects present properties relationships other. however naming objects reason choose particular name speciﬁc object. instead creates arbitrary identiﬁers skolem constants uses build state. perspective states world isomorphic interpretations logical language since might many interpretations satisfy particular statespeciﬁcation sentence; interpretations permutation objects constants refer occurs objects distinguishable based properties relations objects. techniques develop paper generally applicable representing learning dynamics worlds intrinsic constants skolem constants. highlight cases true presented. also skolem constants perceptually plausible also forces create learning algorithms abstract object identity aggressively previous work improve quality learned models. example simulated blocks world contains pickup/ action picking blocks puton/ action putting blocks. action literal pickup could represent action gripper attempts pickup block block-a state represented sentence learning probabilistic transition dynamics world viewed conditional probability distribution represent dynamics rules constructed basic logic described section using logical variables abstract identities particular objects world. section begin describing traditional representation deterministic world dynamics. next present probabilistic case. finally extend ways mentioned section permitting rules refer objects mentioned action description adding noise extending language allow construction concepts. meaning that vector terms context holds current time step taking action cause formula hold terms next step. action must contain every constrain conjunctions literals constructed primitive predicates terms functions applied terms equal value range. addition allowed contain literals constructed integer-valued functions term related integer range greater-than less-than predicates. stands literal predicate function argument list literals negations ignored funct ground functions extracted equality assignments. every literal would needed make complete description state included retrieved associated truth value equality assignment general rules action require contexts mutually exclusive given state-action pair covered rule; covered none assume nothing changes. example consider small rules picking blocks line rule shows action followed context; next line describes eﬀects outcome. according rules executing pickup changes world hand empty exact changes depends whether table block height nine less. deterministic dynamics rules described allow generalization objects exploitation frame assumption well suited highly stochastic domains. order apply domains extend describe probability distribution resulting states probabilistic strips operators model agent’s actions aﬀect world around describing actions alter properties relationships objects world. rule speciﬁes small number simple action outcomes—sets changes occur tandem. without restriction would need deﬁne method choosing possibly conﬂicting predictions diﬀerent covering rules. simplest would involve picking rules perhaps speciﬁc conﬁdent given state action compute coverage deterministic case. however given covering substitution probabilistic rules longer predict unique successor state. instead used construct state single deterministic case. possible subsequent states general possible representation subsequent state covered rule’s outcomes. case probability occurring probabilities relevant outcomes. consider rule painting blocks rule used model transition caused action paint initial state contains painted possible successor state change occurs painted remain true. outcomes describe successor state must probabilities recover state’s total probability. rules speciﬁes complete conditional probability distribution following current state action covered exactly rule distribution subsequent states prescribed rule. predicted probability standard relational representations action dynamics variable denoting object whose properties changed result action must named argument list action. result awkwardness even deterministic situations. example abstract action picking block must take arguments. pickup block picked block picked relationship encoded added condition rule’s context. condition restrict applicability rule; exists guarantee bound appropriate object. restriction adopted means that given grounding action variables rule bound necessary search substitutions would allow rule cover state. however complicate planning because many cases ground instances operator considered even though eventually rejected violations preconditions. example would reject instances violating relation context. complex domains requirement even awkward depending circumstances taking action aﬀect diﬀerent varied sets objects. blocks worlds block several others pickup action aﬀect properties blocks. model without additional mechanism referring objects might increase even vary number arguments pickup takes. handle gracefully extend rule formalism include deictic references objects. rule augmented list deictic references. deictic reference consists variable restriction literals deﬁne respect variables action restrictions supposed pick single unique object not—if pick several none—the rule fails apply. handle pickup action described above action would single argument pickup rule would contain deictic variable constraint rules deictic references must extend procedure computing rule coverage ensure deictic references resolved. deictic variables bound simply starting bindings working sequentially deictic variables using restrictions determine unique bindings. point formulation means extra variables need included action speciﬁcation reduces number operator instances requirement unique designation substitution still quickly discovered testing coverage. example denote block table again noise outcome rule allows noise occur situations speciﬁc rule applies; probability assigned noise outcome default rule speciﬁes kind background noise level. since explicitly modeling eﬀects noise longer calculate transition probability using equation lack required distribution noise outcome. instead substitute worst case constant bound pmin could modeled using well-deﬁned probability distribution describing noise world would give full distribution next states. premise might diﬃcult specify distribution–in domain would ensure distribution assign probability worlds impossible worlds blocks ﬂoating midair. long events unlikely enough would want consider planning reasonable model directly. format rules before section except rule includes explicit noise outcome. ﬁrst three rules similar versions diﬀerence model noise. ﬁnal rule default rule states that rule applies probability observing change together rules provide complete example type rule learn section however written ﬁxed modeling language functions predicates. next section describes concepts used extend language. addition observed primitive predicates often useful background knowledge deﬁnes additional predicates whose truth values computed based observations. found essential modeling certain planning domains background knowledge consists deﬁnitions additional concept predicates functions. work express concept deﬁnitions using concept language includes conjunction existential quantiﬁcation universal quantiﬁcation transitive closure counting. quantiﬁcation used deﬁning concepts inhand block ¬∃y.on. transitive closure included language kleene star operator deﬁnes concepts above on∗. finally counting included using special quantiﬁer returns number objects formula true. useful deﬁning integer-valued functions height y.above. deﬁned concepts enable simplify context deictic variable deﬁnitions well restrict ways cannot described using simple conjunctions. note however need track concept values outcomes since always computed primitives. therefore rule contexts language enriched concepts; outcomes contain primitives. rule complicated example rules given thus deals situation block picked middle stack. deictic variable identiﬁes block stack deictic variable z—the object deictic variable t—the table. might expected gripper succeeds lifting high probability. concept deﬁnitions include clear deﬁned there exists object inhand deﬁned block object; inhand-nil deﬁned there exists object hand; above deﬁned transitive closure topstack deﬁned clear; height deﬁned number objects using chain ons. explained above concepts used context deictic variable deﬁnitions outcomes track primitive predicates; fact appears outcomes since value table predicates never changes. combine concept deﬁnitions rules deﬁne action model. best action models represent rule using ndrs comparison purposes experiments involve rule sets simpler representations without noise deictic references. moreover rule sets diﬀer whether allowed contain constants. rules presented contained none neither context outcomes. reasonable setup states contain skolem constants constants inherent meaning names assigned general repeated. however states intrinsic constants perfectly acceptable include constants action models. constants used uniquely identify objects world. algorithm ensure learned models contain any. also show section learning action models restricted free constants provides useful bias improve generalization training small data sets. deﬁned rule action models describe constructed using learning algorithm attempts return action model best explains example actions results. formally algorithm takes training example triple searches action model maximizes likelihood action eﬀects seen subject penalty complexity. finding involves distinct problems deﬁning concept predicates constructing rule using language contains predicates together directly observable primitive predicates. section ﬁrst discuss second problem rule learning assuming ﬁxed predicates provided learner. then present simple algorithm discovers useful concept predicates. problem learning rule sets general np-hard here address problem using greedy search. structure search hierarchically identifying self-contained subproblems outcome learning subproblem general rule search parameter estimation subproblem outcome learning. thus overall algorithm involves three levels greedy search outermost level learnrules searches space rule sets often constructing rules altering existing ones; middle level induceoutcomes which given incomplete rule consisting context action deictic references ﬁlls rest rule; innermost level learnparameters takes slightly complete rule lacking distribution outcomes ﬁnds distribution optimizes likelihood examples covered rule. present three levels starting inside subroutine described depends since three subroutines attempt maximize scoring metric begin introducing metric. rule governing transition occurring performed scaling parameter complexity penalty applied rule thus favors rule sets maximize likelihood bound data penalizes rule sets overly complex. tion instead. deﬁned simply total number literals chose penalty simplicity also performed worse penalty term tested informal experiments. scaling parameter experiments could also using cross-validation hold-out dataset principled technique. metric puts pressure model explain examples using non-noise outcomes increases also opposing pressure complexity ﬁrst algorithms described section learnparameters takes incomplete rule consisting action deictic references context outcomes learns distribution maximizes score examples covered since procedure allowed alter number literals rule therefore cannot aﬀect complexity penalty term optimal distribution simply maximizes likelihood case rules noise outcomes every example covered unique outcome maximum expressed closed form. examples covered outcome eψ′. lagrange multiplier enforce constraint distributions must however seen section possible example covered outcome; indeed noise outcome covers examples always case. situation examples cannot rewritten simple terms representing diﬀerent outcomes containing single relevant probability probabilities overlapping outcomes remain tied together general closed-form solution exists estimating maximum-likelihood parameters nonlinear programming problem. fortunately instance well-studied problem maximizing concave function probability simplex. several gradient ascent algorithms known problem since function concave guaranteed converge global maximum. learnparameters uses conditional gradient method works iteration moving along parameter axis maximal partial derivative. step-sizes chosen using armijo rule search converges improvement small less chose algorithm easy implement converged quickly experiments tried. however problems found method converges slowly many nonlinear optimization methods constrained newton’s method could directly applied. given learnparameters algorithm learning distribution outcomes consider problem taking incomplete rule consisting context action perhaps deictic references ﬁnding optimal rest rule—that outcomes associated distribution maximize score examples covered total number literals outcomes factor scoring metric equation rule without aspects ﬁxed purposes subroutine number literals context.) general outcome induction np-hard induceoutcomes uses greedy search restricted subset possible outcome sets proper training examples outcome proper every outcome covers least training example. operators described below move space immediate moves improve rule score. outcomes considers induceoutcomes calls learnparameters supply best can. initial outcomes created example writing atoms changed truth values result action creating outcome describe every changes observed way. example consider coins domain. coins world contains coins showing either heads tails. action ﬂip-coupled takes arguments ﬂips coins half time heads otherwise tails. training data learning outcomes coins might look like part figure stands heads stands ¬heads part example ﬂip-coupled. suppose suggested rule ﬂip-coupled context deictic references. given data initial outcomes four entries part figure rule contained variables either abstract action arguments deictic references induceoutcomes would introduce variables appropriate places outcome set. variable introduction achieved applying inverse action substitution example’s changes computing initial outcomes. given deictic reference always found refer coin example outcomes would contain wherever currently contains finally disallow constants rules variables become outcomes refer objects whose properties changed. then changes containing constant referred variable cannot expressed corresponding example covered noise outcome. induceoutcomes uses search operators. ﬁrst operator picks pair non-contradictory outcomes creates outcome conjunction. example might pick combine them adding outcome set. second remove operator drops outcome set. outcomes dropped overlapping outcomes every example cover otherwise outcome would remain proper. whenever operator adds removes outcome learnparameters called optimal distribution thus induceoutcomes introduces variables aggressively wherever possible based intuition corresponding objects would better described constant become apparent training example. sometimes learnparameters return zero probabilities outcomes. outcomes removed outcome since contribute nothing likelihood complexity. optimization improves eﬃciency search. notice outcome always equal union sets literals change training examples covers. fact ensures every proper outcome made merging outcomes initial outcome set. induceoutcomes theory outcomes. know incomplete rules describe learnrules outermost level learning algorithm takes examples ﬁxed language primitive derived predicates performs greedy search space rule sets. precisely searches space proper rule sets rule deﬁned proper respect data includes rule applicable every example change occurs include rules applicable examples. search proceeds described pseudocode figure starts rule contains default rule. every step takes current rule applies search operators obtain rule sets. selects rule maximizes scoring metric deﬁned equation ties broken randomly. learnrules initialized proper rule set. paper always initialize noisy default rule. treats action eﬀects training noise; search progresses search operators introduce rules explain action eﬀects explicitly. chose initial starting point simplicity worked well informal experiments. another strategy would start speciﬁc rule describing detail examples. bottom-up methods advantage data-driven help search reach good parts search space easily. however show several search operators used algorithm presented guided training examples algorithm already desirable property. moreover bottom-up method complexity properties figure learnruleset pseudocode. algorithm performs greedy search space rule sets. step search operators propose rule sets. highest scoring rule selected used next iteration. search operators work creating rule rules integrating rules rule ensures rule remains proper. rule creation involves picking action deictic references context calling induceoutcomes learning algorithm complete rule ﬁlling pis. integration rule involves adding rules also removing rules cover examples. increase number examples covered default rule. search operator takes input rule training examples creates rule sets evaluated greedy search loop. eleven search operators. ﬁrst describe complex operator explainexamples followed simple droprules. then present remaining nine operators share common computational framework outlined figure together operators provide many diﬀerent ways moving space possible rule sets. algorithm adapted learn diﬀerent types rule sets restricting search operators used. figure operatortemplate pseudocode. algorithm basic framework used diﬀerent search operators. operator repeatedly selects rule uses make rules integrates rules original rule create rule set. explainexamples takes input training rule creates alternative rule sets contain additional rules modeling training examples covered default rule figure shows pseudocode algorithm considers training example covered default rule executes three-step procedure. ﬁrst step builds large speciﬁc rule describes example; second step attempts trim rule generalize maximize score still ensuring covers third step creates rule copying integrating rule rule set. illustration consider steps explainexamples might applied training example pickup {on}) background knowledge deﬁned rule section constants allowed. step builds rule creates variable represent object action; then action substitution becomes action pickup. context conjunction inhand-nil ¬inhand clear height ¬above ¬topstack. then step explainexamples attempts create deictic references name constants whose properties changed example already action substitution. case changed literal substitution {a}; deictic variable created restricted extended step explainexamples trims rule remove literals always true training examples like ¬tables redundant ones like ¬inhand ¬clear perhaps heights give rule’s context describes starting example concisely. explain examples consider dropping remaining literals thereby generalizing rule applies examples diﬀerent starting states. however generalizations necessarily improve score. smaller contexts might creating outcomes describe examples penalty term guaranteed improve. change likelihood term depend whether examples higher likelihood rule default rule whether examples higher likelihood distribution one. quite frequently need cover examples give rule distribution closer random before usually lead decrease likelihood large overcome improvement penalty given likelihood-penalty trade-oﬀ. remaining operators create rule sets input rule repeatedly choosing rule making changes create rules. rules integrated explainexamples create rule figure shows general pseudocode done. operators vary select rules changes make them. variations described droplits selects every rule times number literals context words selects literal context. creates rule removing literal context; figure simply containing example pickup rule created explainexamples would selected three times inhand-nil clear height would create three rules three singleton sets three candidate rule sets since newly-created generalizations certain cover examples removed r′s. changes suggested droplits therefore exactly suggested trimming search explainexamples crucial diﬀerence droplits attempts integrate rule full rule instead making quick comparison default rule step explainexamples. explainexamples used trimming search relatively cheap local heuristic allowing decide rule size droplits uses search globally space rule sets comparing contributions various conﬂicting rules. generalizeequality selects rule twice equality literal context create rules equality replaced replaced rule integrated rule resulting returned. again generalized rules certain cover examples contain context pickup rule contains equality literal height generalizeequality attempt replace literal height height domain containing blocks would likely yield interesting generalizations. changeranges selects rule times equality inequality literal context total number values range literal. time selects creates rule replacing numeric value chosen equality another possible value range. note quite possible rules cover examples abandoned. remaining rules integrated copies rule usual. pickup rule contains equality literal height two-block domain example drawn height take values rule will again selected thrice rules created containing equalities. since rule constrains something rule containing height never cover examples certainly abandoned. splitonlits selects rule times number literals absent rule’s context deictic references. constructs rules. case predicate inequality literals creates rule positive version literal inserted context negative version. case equality literals constructs rule every possible value equality could take. either case rules cover examples dropped. remaining rules corresponding literal placed integrated rule simultaneously. list literals added pickup rule consists inhand inhand table table clear height possible applications topstack. literals make interesting examples adding context create rules either cover examples abandoned cover examples original rule rejected likelihood worse penalty. however illustrate process attempting height predicate result creation three rules height context rules would added rule once. addlits selects rule times number predicate-based literals absent rule’s context deictic references reﬂects fact literal considered positive negative form. constructs rule literal inserting literal earliest place rule variables well-deﬁned literal contains deictic variables context otherwise restriction last deictic variable mentioned literal. would inserted restriction resulting rule integrated rule set. example reason. illustration inhand would chosen twice ¬inhand added context case. since context already contains inhand-nil adding ¬inhand redundant adding inhand produce contradiction neither rule seriously considered. selects rule times number literals constructed using available predicates variables variable case creates deictic reference using current literal deﬁne restriction adds deictic reference antecendent construct rule integrated rule set. supposing variable list literals would constructed pickup rule consists inhand clear table possible applications topstack used create deictic references like table. raiseconstants operator used constants permitted. selects rule times number constants among arguments action. constant constructs rule creating variable replacing every occurrence integrates rule rule set. splitvariables operator used constants permitted. selects rule times number variables among arguments action. variable goes examples covered rule collects constants binds then creates rule constants replacing every occurrence constant. rules corresponding variable combined integrated rule together. found operators consistently used learning. operators heuristic complete sense every rule constructed initial rule set—although course guarantee scoring metric lead greedy search global maximum. learnrules’s search strategy large drawback; learned rules guaranteed proper training testing data. test examples could covered rule. happens employ alternative rule-selection semantics return default rule model situation. essentially saying don’t know happen. however signiﬁcant problem; problematic test examples always added future training used learn better models. given suﬃciently large training failures rare. figure three training examples three blocks world. example paired initial rule explainexamples might create model example agent trying block onto block example consider learnruleset might learn rules model three training examples figure given settings complexity penalty noise bound later used experiments pmin pmin three-block domain since diﬀerent states consistency. initialization rule contains default rule; changes occur examples modeled noise. since examples include change default rule noise probability describe path greedy search takes. ﬁrst round search explainexamples operator suggests adding rules describe examples. general explainexamples tries construct rules compact cover many examples assign relatively high probability covered example. reasonable rules suggested shown right-hand side figure notice deterministic high-probability relatively compact unique initial state explainexamples take advantage this. meanwhile consider adding rules. guarantee constitute improvement since high complexity penalty would make rule look high pmin would make default rule look good. determine best move algorithm compares scores rule sets containing proposed rules score initial rule containing default rule. calculate scores example starting rule consisting rule covers default rule covers remaining example therefore noise probability equation rule’s complexity number literals body case three. rule containing score similar calculations show rule sets containing scores respectively. since initial rule score rule sets improvements containing best picked greedy search. rule next step search decide altering existing rule introducing another rule describe example currently covered default rule. since default rule covers examples altering single rule rule option. operators likely score highly noise outcome rule means referring block appropriate operator therefore addrefs introduce deictic reference describing block. course increases size rule complexity addition means rule longer applies leaving example handled default rule. however rule raises probabilities examples enough compensate increase complexity ends score clear improvement highest score obtainable step algorithm alters rule note rule could added earlier also covered ﬁrst rule added specialized. thus adding rule containing would knocked caused examples explained noise default rule would reduced overall score. learning continues search. attempts apply rule-altering operators current rules either make bigger without changing likelihood lead creation noise outcomes. dropping either rule noise probability default rule lower score. since extra examples explained operator improve score search stops rule set. seems like reasonable rule domain rule covers happens puton clear block describes puton block another block ideally would like ﬁrst rule generalize blocks something them instead notice would need examples containing higher stacks. explore eﬀects constants rules evaluate three diﬀerent versions rule learning propositional relational deictic. propositional rule learning explainexamples creates initial trimmed rules constants never introduces variables. none search operators introduce variables used. thus learned rules guaranteed propositional—they cannot generalize across identities speciﬁc objects. relational rule learning variables allowed rule action arguments search operators allowed introduce deictic references. explainexamples creates rules constants name objects long constants already variable action argument list mapped them. finally deictic rule learning constants allowed. deictic learning provides strong bias improve generalization. demonstrate addition noise deictic references result better rules learn action models enhancements. again done changing algorithm minor ways. disallow noise rule noise probability zero means must constrain outcome sets contain outcome every example change observed; rules cannot express changes abandoned. disallow deictic references disable operators introduce them explainexamples create empty deictic reference set. contexts deictic references ndrs make concept predicates functions well primitive ones. concepts speciﬁed hand learned using rather simple algorithm learnconcepts uses learnruleset subprocedure testing concept usefulness. algorithm works constructing increasingly complex concepts running learnruleset checking concepts appear learned rules. ﬁrst created applying operators figure literals built original language. subsequent sets concepts constructed using literals proved useful latest run; concepts tried before always true always false across examples discarded. search ends none concepts prove useful. example consider predicate topstack simple blocks world could discovered follows. ﬁrst round learning literal used deﬁne predicate true stacked assuming predicate appears learned rules used second round learning deﬁne among others clear. ensuring clear predicate true highest block stack containing notion topstack used determining happen gripper tries pick descends above likely grasp block stack instead. figure operators used invent predicate operator takes input literals listed left. represent predicates; represents function; refer numerical constant. operator takes literal returns concept deﬁnition. operators applied literals used rules rule create predicates. number distinct concepts used rule scaling parameter. metric used learnruleset; avoids overﬁtting favoring rule sets fewer derived predicates. rule learning challenge addressed section complicated need learn structure rules numeric parameters associated outcome distributions deﬁnitions derived predicates modeling language. learnconcepts large number possible search operators might cause concern overall computational complexity learnruleset algorithm. although algorithm expensive search operators designed control complexity attempting keep number rules current small possible. step search number rule sets considered depends current rules. explainexamples operator creates rule sets number examples covered default rule. since search starts rule containing default rule initially equal number training examples. however explainexamples designed introduce rules cover many examples practice grows small quickly. operators create rule sets number rules current depends speciﬁc operator. example could number literals dropped context rule droplits operator. although large stays small practice search starts default rule complexity penalty favors small rule sets. ensure score increases search step algorithm guaranteed converge optimum. however guarantee quickly there. practice found algorithm converged quickly test domains. learnruleset algorithm never took steps learnconcepts outer loop never cycled times. entire algorithm never took hours single processor although signiﬁcant eﬀort made cache intermediate computations ﬁnal implementation. spite this realize that scale complex domains approach eventually become prohibitively expensive. plan handle problem developing algorithms learn concepts rules rule parameters online manner directed search operators. however leave complex approach future work. experiments section involve learning models complex actions true models dynamics level relational rules available evaluation. instead learned models evaluated planning executing actions. many possible ways plan. work explore planning. -tuple possible states possible actions distribution encodes transition dynamics world finally reward signal maps every state real value. policy plan mapping states actions. expected amount reward achieved executing starting called value γir|π] states reached time discount factor favors immediate rewards. goal planning policy achieve reward time. planning large domains diﬃcult solve bellman equations exactly. approximation implemented simple planner based sparse sampling algorithm given state creates tree states sampling forward using transition model computes value node using bellman equation selects action highest value. adapt algorithm handle noisy outcomes predict next state estimating value unknown next state fraction value staying state i.e. sample forward stayed state scale value obtain. scaling factor depth branching factor four. scaling method guess value unknown next state might noisy rules partial models compute value explicitly. future would like explore methods learn associate values noise outcomes. example value outcome tower blocks falls diﬀerent goal build tall stack blocks goal blocks table. algorithm solve hard combinatorial planning problems allow choose actions maximize relatively simple reward functions. next section enough distinguish good models poor ones. moreover development ﬁrst-order planning techniques active ﬁeld research section demonstrate rule learning algorithm robust variety lownoise domains show works intrinsically noisy simulated blocks world domain. begin describing test domains report series experiments. slippery gripper domain inspired work draper abstract symbolic blocks world simulated robotic used move blocks around table nozzle used paint blocks. painting block might cause gripper become makes likely fail manipulate blocks successfully; fortunately gripper dried. figure shows rules model domain. individual states represent world objects intrinsic constants experimental data generated sampling rules. section explore learning algorithms section compare number training examples scaled single complex world. trucks drivers logistics domain adapted aips international planning competition four types constants trucks drivers locations objects. trucks drivers objects locations. locations connected paths links. drivers board trucks exit trucks drive trucks locations linked. drivers also walk without truck locations connected paths. finally objects loaded unloaded trucks. rules shown figure actions simple rules succeed fail change world. however walk action interesting twist. drivers walk location another succeed time representation presented cannot encode action eﬃciently. best rule rule origin location outcomes every location origin linked extending representation allow actions like walk represented single rule interesting area future work. like slippery gripper domain individual states represent world objects intrinsic constants experimental data generated sampling rules. trucks drivers dynamics diﬃcult learn section learned enough training data. validate rule extensions paper section presents experiments rigid body simulated physics blocks world. section describes logical interface simulated world. description extra complexities inherent learning dynamics world presented section deﬁne interface symbolic representation describe action dynamics physical domain simulated blocks world. perceptual system produces states contain skolem constants. logical language includes binary predicate deﬁned exerts downward force obtained querying internal state simulator unary typing predicates table block. actuation system translates actions sequences motor commands simulator. actions always execute regardless state world. deﬁne actions; parameters allow agent specify objects intends manipulate. pickup action centers gripper lowers hits something grasps raises gripper. analogously puton action centers gripper lowers encounters pressure opens raises using simulator sidestepping diﬃcult pixels-to-predicates problem occurs whenever agent domain observations internal representation. primitive predicates deﬁned terms internal state simulation simpler cleaner observations real world would also make domain completely observable prerequisite learning planning algorithms. choosing predicates observe important. make rule learning problem easy hard diﬃculty making choice magniﬁed richer settings. limited language described balances extremes providing would diﬃcult derive means providing predicates inhand clear learned. section describes sets experiments. first compare learning deictic relational propositional rules slippery gripper trucks drivers data. domains modeled planning rules contain intrinsic constants noisy thus allow explore eﬀect deictic references constants rules directly. then describe experiments learns rules model data simulated blocks world. data inherently noisy contains skolem constants. result focus evaluating full algorithm performing ablation studies demonstrate deictic references noise outcomes concepts required eﬀective learning. experiments examples generated randomly constructing state randomly picking arguments action executing action state generate distribution used construct biased guarantee that approximately half examples chance change state. method data generation designed ensure learning algorithms always data representative entire model learn. thus experiments ignore problems agent would face generate data exploring world. know model used generate data evaluate model respect similarly generated test examples calculating average variational distance true model estimate comparisons performed four actions. ﬁrst paint pickup slippery gripper domain second drive walk trucks drivers domain. action presents diﬀerent challenges learning. paint simple action outcome lead successor state pickup complex action must represented figure variational distance function number training examples propositional relational deictic rules. results averaged trials experiment. test size examples. planning rule. drive simple action four arguments. finally walk complicated action uses path connectivity world noise model lost pedestrians. slippery gripper actions performed world four blocks. trucks driver actions performed world trucks drivers objects four locations. compare three versions algorithm deictic includes full rules language allow constants; relational allows variables constants deictic references; propositional constants variables. figure shows results. relational learning consistently outperforms propositional learning; implies variable abstractions useful. cases except walk action deictic learner outperforms relational learner. result implies forcing rules contain variables preventing overﬁtting learning better models. results walk action interesting. here deictic learner cannot actually represent optimal rule; requires noise model complex. deictic learner quickly learns best rule relational propositional learners eventually figure performance various action model variants function number training examples. data points averaged planning trials three rule sets learned diﬀerent training data sets. comparison average reward performing actions reward obtained human directed gripper averaged experiments variable abstraction helps learn less data deictic rules abstract aggressively perform best long represent model learned. next section consider deictic rules since working domain simulated perception access objects’ identities names using skolem constants. true model known evaluate learned model using plan estimating average reward gets. reward function used simulated blocks world average height blocks world breadth depth search sampling planner four. learning pmin tested four action model variants varying training size; results shown figure curve labeled ‘learned concepts’ represents full algorithm presented paper. performance approaches obtained human expert comparable algorithm labeled ‘hand-engineered concepts’ concept learning instead provided hand-coded versions concepts clear inhand inhand-nil above topstack height. concept learner discovered these well useful predicates e.g. clear call onclear. could action models outperformed hand-engineered ones slightly small training sets. domains less well-studied blocks world might less obvious useful concepts are; concept-discovery technique presented prove helpful. remaining model variants obtained rewards comparable reward nothing all. variant used full predeﬁned concepts rules could noise outcomes. requirement explain every action eﬀect signiﬁcant overﬁtting decrease performance. rule given traditional blocks world language include above topstack height allowed learn rules noise outcomes. also tried full-language variant noise outcomes allowed deictic references resulting rule sets contained noisy rules planner attempt all. poor performance ablated versions representation shows three extensions essential modeling simulated blocks world domain. human agent commanding gripper solve problem received average total reward theoretical maximum unexpected action outcomes. thus rules performing near-human levels suggesting representation reasonable problem. also suggests planning approximations learning bounds limiting performance. traditional rules face challenge modeling transitions seen data much larger hypothesis space consider learning; surprising generalize poorly consistently out-performed ndrs. informally also report algorithms execute signiﬁcantly faster traditional ones. standard desktop learning ndrs takes minutes learning traditional rules take hours. noisy deictic action models generally compact traditional ones planning much faster well. rule applies gripper asked contents block inside stack topped small block placing things small block chancy reasonable probability fall table small probability follow. paper developed probabilistic action model representation rich enough used learn models planning physically simulated blocks world. ﬁrst step towards deﬁning representations algorithms enable learning complex worlds. problem learning deterministic action models well studied. work area focused incrementally learning planning operators interacting simulated worlds. however work assumes learned models completely deterministic. oates cohen earliest work learning probabilistic planning operators. rules factored apply parallel. however representation strictly propositional allows rule contain single outcome. previous work developed algorithms learning probabilistic relational planning operators unfortunately neither probabilistic algorithms robust enough learn complex noisy environments like simulated blocks world. previous system comes close goal trail learner trail learns extended version horn clauses noisy environments applying inductive logic programming learning techniques robust noise. trail introduced deictic references name objects based functional relationships arguments actions. deictic references exists-unique quantiﬁcation semantics generalization benson’s original work. moreover trail models continuous actions real-valued ﬂuents allows represent complex models date including knowledge required pilot realistic ﬂight simulator. however rules trail learns limited probabilistic representation represent possible transition distributions. trail also include mechanisms learning predicates. work action model learning used diﬀerent versions greedy search rule structure learning closely related inspired learning version spaces mitchell later work paper also explore ﬁrst time moving space rule sets using noise rule initial rule set. found approach works well practice avoiding need hand-selected initial rule allowing algorithm learn signiﬁcantly complex environments. know work learning action models explored learning concepts. literature recent work shown adding concept learning decision tree learning algorithms improves classiﬁcation performance. outside action learning exists much related research learning probabilistic models relational logical structure. complete discussion beyond scope paper present highlights. work learns representations relational extension bayesian networks. comprehensive example work getoor work extends research incorporating probabilistic dependencies. example wide range techniques presented kersting additionally recent work learning markov logic networks log-linear models features deﬁned ﬁrst-order logical formulae. action models action model learning algorithms paper designed represent action eﬀects special case general approaches listed above. discussed section tailoring representation match model learnt simplify learning. finally consider work related action model representation. relevant approach ppddl representation language probabilistic planning operators problem domains representation partially inspired ppddl operators includes restrictions make easier learn extensions noise outcomes required eﬀectively model simulated blocks world. future algorithms paper could extended learn full ppddl rules. also ppddl planning algorithms could adapted improve simple planning presented section general sense ndrs related probabilistic relational representations designed model dependencies across time. examples work relational dynamic bayesian networks specialization prms logical hidden markov models come research tradition. approaches make diﬀerent modeling assumptions closely tied planning representations models extend. remains much done context learning probabilistic planning rules. first likely work applied additional domains representation need adapted search operators adjusted accordingly. possible changes mentioned article include allowing rules apply parallel diﬀerent rules could apply diﬀerent aspects state extending outcomes include quantiﬁers actions like walk trucks drivers domain section could described using single rule. signiﬁcant change intend pursue expanding approach handle partial observability possibly incorporating techniques work deterministic learning also hope make changes make using rules easier associating values noise outcomes help planner decide whether avoided. second research direction involves development algorithms learn probabilistic operators incremental online manner similar learning setup deterministic case potential scale approach larger domains make applicable even situations diﬃcult obtain training examples contains reasonable sampling worlds likely relevant agent. line work require development techniques eﬀectively exploring world learning model much done reinforcement learning. longer term would like online algorithms learn operators concept predicates also useful primitive predicates motor actions. material based upon work supported part defense advanced research projects agency department interior acquisition services division contract nbchd; part darpa grant hr---", "year": 2011}