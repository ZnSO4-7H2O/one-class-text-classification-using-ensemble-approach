{"title": "Visual Question Answering: Datasets, Algorithms, and Future Challenges", "tag": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "Visual Question Answering (VQA) is a recent problem in computer vision and natural language processing that has garnered a large amount of interest from the deep learning, computer vision, and natural language processing communities. In VQA, an algorithm needs to answer text-based questions about images. Since the release of the first VQA dataset in 2014, additional datasets have been released and many algorithms have been proposed. In this review, we critically examine the current state of VQA in terms of problem formulation, existing datasets, evaluation metrics, and algorithms. In particular, we discuss the limitations of current datasets with regard to their ability to properly train and assess VQA algorithms. We then exhaustively review existing algorithms for VQA. Finally, we discuss possible future directions for VQA and image understanding research.", "text": "visual question answering recent problem computer vision natural language processing garnered large amount interest deep learning computer vision natural language processing communities. algorithm needs answer text-based questions images. since release ﬁrst dataset additional datasets released many algorithms proposed. review critically examine current state terms problem formulation existing datasets evaluation metrics algorithms. particular discuss limitations current datasets regard ability properly train assess algorithms. exhaustively review existing algorithms vqa. finally discuss possible future directions image understanding research. recent advancements computer vision deep learning research enabled enormous progress many computer vision tasks image classiﬁcation object detection activity recognition given enough data deep convolutional neural networks rival abilities humans image classiﬁcation annotated datasets rapidly increasing size thanks crowd-sourcing similar outcomes anticipated focused computer vision problems. however problems narrow scope require holistic understanding images. humans identify objects image understand spatial positions objects infer attributes relationships other also reason purpose object given surrounding context. arbitrary questions images also communicate information gleaned them. recently developing computer vision system answer arbitrary natural language questions images thought ambitious intractable goal. however since enormous progress developing systems abilities. visual question answering computer vision task system object recognition image? object detection cats image? attribute classiﬁcation color cat? scene classiﬁcation sunny? counting many cats image? beyond these many complex questions asked questions spatial relationships among objects common sense reasoning questions robust system must capable solving wide range classical computer vision tasks well needing ability reason images. many potential applications vqa. immediate blind visually impaired individuals enabling information images real world. example blind user scrolls social media feed captioning system describe image user could query image insight scene. generally could used improve human-computer interaction natural query visual content. system also used image retrieval without using image meta-data tags. example images taken rainy setting simply raining?’ images dataset. beyond applications important basic research problem. good system must able solve many computer vision problems considered component turing test image understanding visual turing test rigorously evaluates computer vision system assess whether capable human-level semantic analysis images passing test requires system capable many diﬀerent visual tasks. considered kind visual turing test also requires ability understand questions necessarily sophisticated natural language processing. algorithm performs well better humans arbitrary questions images arguably much computer vision would solved. true benchmarks evaluation tools suﬃcient make bold claims. review discuss existing datasets methods vqa. place particular emphasis exploring whether current benchmarks suitable evaluating whether system capable robust image understanding. section compare computer vision tasks also require integration vision language then section describe currently available datasets emphasis strengths weaknesses. discuss biases datasets severely limit ability assess algorithms. section discuss evaluation metrics used vqa. then review existing algorithms analyze eﬃcacy section finally discuss possible future developments open questions. figure object detection semantic segmentation image captioning compared vqa. middle ﬁgure shows ideal output typical object detection system right ﬁgure shows semantic segmentation coco dataset tasks lack ability provide contextual information objects. captions coco image range generic descriptions scene e.g. busy town sidewalk next street parking intersections. focused discussion single activity without qualifying overall scene e.g. woman jogging leash. acceptable captions signiﬁcantly information extracted vqa. coco-vqa dataset questions asked image kind shoes skater wearing? urban suburban? animal there? attributes whole image based question. many computer vision problems involve extracting information images limited scope generality compared vqa. object recognition activity recognition scene classiﬁcation posed image classiﬁcation tasks today’s best methods using cnns trained classify images particular semantic categories. successful object recognition algorithms rival humans accuracy object recognition requires classifying dominant object image without knowledge spatial position role within larger scene. object detection involves localization speciﬁc semantic concepts placing bounding around instance object image. best object detection methods deep cnns semantic segmentation takes task localization step classifying pixel belonging particular semantic class instance segmentation builds upon localization diﬀerentiating separate instances semantic class semantic instance segmentation important computer vision problems generalize object detection recognition suﬃcient holistic scene understanding. major problems face label ambiguity. example figure assigned semantic label position yellow cross ‘bag’ ‘black’ ‘person.’ label depends task. moreover approaches alone understanding role object within larger context. example labeling pixel ‘bag’ inform whether carried person labeling pixel ‘person’ tell person sitting running skateboarding. contrast system required answer arbitrary questions images require reasoning relationships objects overall scene. appropriate label speciﬁed question. besides signiﬁcant amount recent work combines vision language. studied image captioning algorithm’s goal produce natural language description given image. image captioning broad task potentially involves describing complex attributes object relationships provide detailed description image. however several problems visual captioning task evaluation captions particular challenge. ideal method evaluation human judges slow expensive. reason multiple automatic evaluation schemes proposed. widely used caption evaluation schemes bleu rouge meteor cider exception cider developed speciﬁcally scoring image descriptions caption evaluation metrics originally developed machine translation evaluation. metrics limitations. bleu widely used metric known score large variations sentence structure largely varying semantic content captions generated bleu scores ranked machine generated captions human captions. however human judges used judge captions judges ranked captions equal better quality human captions. evaluation metrics especially cider meteor show robustness terms agreement human judges still often rank automatically generated captions higher human captions reason evaluating captions challenging given image many valid captions speciﬁc others generic nature however captioning systems produce generic captions superﬁcially describe image’s content often ranked high evaluation metrics. generic captions person walking street’ ‘several cars parked side road’ applicable large number images often ranked highly evaluation schemes human judges. fact simple system returns caption training image similar visual features using nearest neighbor yields relatively high scores using automatic evaluation metrics dense image captioning avoids generic caption problem annotating image densely short visual descriptions pertaining small salient image regions example densecap system output wearing black shirt’ ’large green trees’ ‘roof building’ description accompanied bounding box. system generate large number descriptions rich scenes. although many descriptions short still diﬃcult automatically assess quality. densecap also omit important relationships objects scene producing isolated descriptions regions. captioning densecap also task agnostic system required perform exhaustive image understanding. conclusion captioning system liberty arbitrarily choose level granularity image analysis contrast level granularity speciﬁed nature question asked. example ‘what season this?’ require understanding entire scene ‘what color standing behind girl white dress?’ would require attention speciﬁc details scene. moreover many kinds questions speciﬁc unambiguous answers making amenable automated evaluation metric captioning. ambiguity still exist question types many questions answer produced algorithm evaluated one-to-one matching ground truth answer. beginning major datasets publicly released. datasets enable systems trained evaluated. article main datasets daquar coco-qa dataset fm-iqa visualw visual genome exception daquar datasets include images microsoft common objects context dataset consists images common object categories million labeled instances average captions image. visual genome visualw images flickrm addition coco images. portion dataset contains synthetic cartoon imagery refer synth-vqa. consistent papers rest dataset referred coco-vqa since contains images coco image dataset. table contains statistics datasets. ideal dataset needs suﬃciently large capture variability within questions images concepts occur real world scenarios. also fair evaluation scheme diﬃcult ‘game’ well indicates algorithm answer large variety question types images deﬁnitive answers. dataset contains easily exploitable biases distribution questions answers possible algorithm perform well dataset without really solving problem. dataset question answering real-world images ﬁrst major dataset released. smallest datasets. consists training testing pairs based images nyu-depthv dataset dataset also available even smaller conﬁguration consisting object categories known daquar-. daquar- consists training pairs testing pairs. additional ground truth answers collected daquar create alternative evaluation metric. variant daquar called daquar-consensus named evaluation metric. daquar pioneering dataset small successfully train evaluate complex models. apart small size daquar contains exclusively indoor scenes constrains variety questions available. images tend signiﬁcant clutter cases extreme lighting conditions makes many questions diﬃcult answer even humans able achieve accuracy full dataset. coco-qa pairs created images using natural language processing algorithm derives coco image captions. example using image caption playing frisbee possible create question playing? frisbee answer. coco-qa contains training testing pairs. questions object image questions color counting location questiontypes evaluationtype annotation imagesource avg.answerlength longestanswer longestquestion humanaccuracy %coveredbytop- %coveredbytop- distinctanswers qapairs totalimages figure sample images daquar coco-qa datasets corresponding pairs. signiﬁcant number coco-qa questions grammatical errors nonsensical whereas daquar images often marred clutter resolution images. biggest shortcoming coco-qa ﬂaws algorithm used generate pairs. longer sentences broken smaller chunks ease processing many cases algorithm cope well presence clauses grammatical variations sentence formation. results awkwardly phrased questions many containing grammatical errors others completely unintelligible major shortcoming four kinds questions limited kinds things described coco’s captions. dataset consists real images coco abstract cartoon images. work dataset focused solely portion containing real world imagery coco refer coco-vqa. refer synthetic portion dataset synth-vqa. coco-vqa consists three questions image answers question. amazon mechanical turk workers employed generate questions image asked ‘stump smart robot’ separate pool workers hired generate answers questions. compared datasets coco-vqa consists relatively large number questions questions answered independent annotators. multiple answers question used consensus-based evaluation metric dataset discussed section synth-vqa consists synthetic scenes depict cartoon images diﬀerent simulated scenarios. scenes made diﬀerent objects diﬀerent animal models human cartoon models. human models used contain deformable limbs eight diﬀerent facial expressions. models would like that? dataset contains subjective questions prone cause disagreement annotators also clearly lack single objectively correct answer. color trees? green. total questions dataset asking question. questions majority answer green. questions often answered without information image. would woman strong? lift arms headstand handstand stand head standing upside stool. questions seeking descriptive explanatory answers pose signiﬁcant diﬃculty evaluation. also span diﬀerent gender races provide variation appearance. synth-vqa pairs questions scene ground truth answers question. using synthetic images becomes possible create varied balanced dataset. natural image datasets tend consistent context biases e.g. street scene likely picture zebra. using synthetic images biases reduced. yang dataset built synth-vqa tried eliminate biases answers people questions. discuss yang section correct answer frequent answer given annotators. plausible answers three answers collected annotators without look popular answers popular answers dataset. random answers randomly selected correct answers questions. diversity size dataset coco-vqa widely used evaluate algorithms. however multiple problems dataset. coco-vqa large variety questions many accurately answered without using image language biases. relatively simple image-blind algorithms achieved accuracy coco-vqa using question alone dataset also contains many subjective opinion-seeking questions single objective answer similarly many questions seek explanations verbose descriptions. example given figure also shows unreliability human annotators popular answer ‘yes’ completely wrong given question. complications reﬂected inter-human agreement dataset several practical issues also arise dataset’s biases. example ‘yes/no’ answers span questions almost answered ‘yes.’ combined evaluation metric used coco-vqa biases make diﬃcult assess whether algorithm truly solving problem using solely dataset. discuss section freestyle multilingual image question answering dataset another dataset based coco contains human generated answers questions. dataset originally collected chinese english translations made available. unlike coco-qa daquar dataset also allowed answers full sentences. makes automatic evaluation common metrics intractable. reason authors suggested using human judges evaluation judges tasked deciding whether answer provided human well assessing quality answer scale approach impractical research groups makes developing algorithms diﬃcult. discuss importance automatic evaluation metrics section visual genome consists images occur yfccm coco images. contains million pairs images average pairs image. article visual genome largest dataset. recently introduced methods evaluated beyond baselines established authors. visual genome consists types questions what where when why. distinct modes data collection used make dataset. free-form method annotators free question image. however asking free-form questions human annotators tend similar questions image’s holistic content e.g. asking ‘how many horses there?’ sunny?’ promote bias kinds questions asked. creators visual genome combated also prompting workers questions speciﬁc image regions. using regionspeciﬁc method worker might prompted question region image containing hydrant. region-speciﬁc question prompting made possible using visual genome’s descriptive bounding-box annotations. example region bounding boxes pairs visual genome shown figure visual genome much greater answer diversity compared datasets shown figure answers occur frequently visual genome cover answers dataset whereas cover coco-vqa daquar coco-qa. visual genome’s long-tailed distribution also observed length answers. answers single words compared answers coco-vqa answers coco-qa answers daquar. diversity answers makes open-ended evaluation signiﬁcantly challenging. moreover since categories required strictly belong types diversity answer times artiﬁcially stem simply variations phrasing could eliminated prompting annotators choose concise answers. example cars parked? answered street’ concisely ‘street.’ visual genome binary questions. dataset creators argue encourage using complex questions. contrast dataset ‘yes’ ‘no’ frequent answers dataset. discuss issue section figure visualw subset visual genome. apart pointing task questions visualw sourced visual genome data. visual genome however includes pairs region annotations. visualw dataset subset visual genome. visualw contains images visual genome also present coco. visualw named seven categories questions contains what where when which. dataset consists distinct types questions. ‘telling’ questions identical visual genome questions answer text-based. ‘pointing’ questions ones begin ‘which’ questions algorithm select correct bounding among alternatives. example pointing question shown figure visualw uses multiple-choice answer framework standard evaluation four possible answers made available algorithm evaluation. make task challenging multiple-choices consist answers plausible given question. plausible answers collected prompting annotators answer question without seeing image. pointing questions multiple-choice options four plausible bounding boxes surrounding likely answer. like visual genome dataset contain binary questions. datasets contain either real synthetic scenes shapes dataset consists shapes varying arrangements types colors. questions attributes relationships positions shapes. approach enables creation vast amount data free many biases plague datasets varying degrees. shapes consists unique questions every question asked images dataset. unlike datasets means completely balanced free bias. questions binary yes/no answers. many questions require positional reasoning layout properties shapes. while shapes cannot substitute using real-world imagery idea behind extremely valuable. figure graph shows long-tailed nature answer distributions newer datasets. example choosing repeated answers training would cover possible answers coco-qa less visual genome dataset. classiﬁcation based frameworks translates training model output classes. posed either open-ended task algorithm generates string answer question multiple-choice question picks among choices. multiple-choice simple accuracy often used evaluate algorithm getting answer right makes correct choice. open-ended simple accuracy also used. case algorithm’s predicted answer string must exactly match ground truth answer. however accuracy stringent errors much worse others. example question ‘what animals photo?’ system outputs ‘dog’ instead correct label ‘dogs’ penalized strongly would output ‘zebra.’ questions also multiple correct answers e.g. ‘what tree?’ might ‘bald eagle’ listed correct ground truth answer system outputs ‘eagle’ ‘bird’ would penalized much output wu-palmer similarity proposed alternative accuracy tries measure much predicted answer diﬀers ground truth based diﬀerence semantic meaning. given ground truth answer predicted answer question wups assign value based similarity other. ﬁnding least common subsumer semantic senses assigning scores based back semantic tree needs traversed common subsumer. using wups semantically similar non-identical words penalized relatively less. following earlier example ‘bald eagle’ ‘eagle’ similarity whereas ‘bald eagle’ ‘bird’ similarity however wups tends assign relatively high scores even distant concepts e.g. ‘raven’ ‘writing desk’ wups score remedy this proposed threshold wups scores score threshold scaled factor. threshold scaling factor suggested modiﬁed wups metric standard measure used evaluating performance daquar coco-qa addition simple accuracy. major shortcomings wups make diﬃcult use. first despite using thresholded version wups certain pairs words lexically similar carry vastly diﬀerent meaning. particularly problematic questions object attributes color questions. example correct answer ‘white’ predicted answer ‘black’ answer would still receive wups score seems excessively high. another major problem wups works figure simple questions also evoke diverse answers annotators coco-vqa. dog? eating bowl; ﬂoor; feeding station; food; inside; ﬂoor eating dish; ﬂoor; front gray bowl right trash can; near food bowl; ﬂoor alternative relying semantic similarity measures multiple independently collected ground truth answers question done dataset daquar-consensus daquar-consensus average human annotated ground truth answers question collected. dataset’s creators proposed ways answers called average consensus consensus. average consensus ﬁnal score weighted toward preferring popular answer provided annotators. consensus answer needs agree least annotator. total number annotators answer algorithm. using metric algorithm agrees three annotators awarded full score question. although metric helps greatly ambiguity problem substantial problems remain especially coco-vqa portion dataset study next paragraphs. using accuracyv inter-human agreement coco-vqa impossible algorithm achieve accuracy. inter-human agreement especially poor ‘why’ questions questions less three annotators giving exactly answer. makes impossible full score questions. lack inter-human agreement also seen simpler straightforward questions example system predicts answers awarded score least several cases answers provided annotators consist complete antonyms many cases accuracyv leads multiple correct answers question direct opposition other. example coco-vqa ‘yes/no’ answers ‘yes’ ‘no’ repeated three annotators. either answering ‘yes’ ‘no’ would receive highest possible score. even eight annotators answered ‘yes’ answered ‘no’ algorithm would still receive score question. weight majority play role evaluation. problems result scores inﬂated. example answering ‘yes’ yes/no questions ideally score around questions. however using accuracyv score partially dataset biased majority answer questions ‘yes’ time score excessively inﬂated. consist word answers. occurs coco-vqa questions cocoqa questions daqaur questions. possibility multiple correct answers increases greatly answers need multiple words. occurs frequently fmiqa visualw visual genome e.g. visualw answers three words. scenario metrics accuracyv unlikely help score predicted answers ground truth answers open-ended vqa. creators fm-iqa suggested using human judges assess multi-word answers presents number problems. first using human judges extremely demanding process terms time resources expenses. would make diﬃcult iteratively improve system measuring changing algorithm altered performance. second human judges need given criteria judging quality answer. creators fm-iqa proposed metrics human judges. ﬁrst determine whether answer produced human regardless answer’s correctness. metric alone poor indicator system’s abilities could potentially manipulated. second metric rate answer -point scale totally wrong partially correct perfectly correct alternative using judges handling multi-word answers multiple-choice paradigm used part dataset visualw visual genome. instead generating answer system needs predict given choices correct. greatly simpliﬁes evaluation believe unless used carefully multiple-choice ill-suited undermines eﬀort allowing system peek correct answer. discuss issue section best evaluate system still open question. evaluation method strengths weaknesses method depends dataset constructed level bias within available resources. considerable work needs done develop better tools measuring semantic similarity answers handling multi-word answers. large number algorithms proposed past three years. existing methods consist extracting image features extracting question features algorithm combines features produce answer. image features algorithms cnns pre-trained imagenet common examples vggnet resnet googlenet wider variety question featurizations explored including bag-of-words long short term memory encoders gated recurrent units skipthought vectors generate answer common approach treat classiﬁcation problem. framework image question features input classiﬁcation system unique answer treated distinct category. illustrated figure featurization scheme classiﬁcation system take widely varied forms. systems diﬀer signiﬁcantly integrate question image features. examples include figure simpliﬁed illustration classiﬁcation based framework vqa. framework image question features extracted combined classiﬁer predict answer. variety feature extraction methods algorithms combining features proposed common approaches listed respective blocks ﬁgure. full details presented section classiﬁcation framework used open-ended algorithms approach generate answers seen training prompting explore alternatives. lstm used produce multi-word answer word time. however answer produced still limited words seen training. multiple-choice proposed treating ranking problem system trained produced score possible multiple-choice answer question image trio selects highest scoring answer choice. following subsections group algorithms based common themes. results daquar coco-qa coco-vqa methods given table increasing order performance. table report plain accuracy daquar coco-qa report accuracyv coco-vqa. table breaks results coco-vqa based techniques used paper. baseline methods help determine diﬃculty dataset establish minimal level performance sophisticated algorithms exceed. simplest baselines random guessing guessing repeated answers. widely used baseline classiﬁcation system apply linear non-linear e.g. multi-layer perceptron classiﬁer image question features combined table results across datasets open-ended multiple-choice evaluation schemes. simple models trained image data question data well human performance also shown. imgques-only models evaluated ‘test-dev’ section coco-vqa. mcb-ensemble presented separately additional data training. single vector common methods combine features include concatenation elementwise product elementwise sum. combining schemes also explored lead improved results variety featurization approaches used baseline classiﬁcation frameworks. authors used bag-of-words represent question features googlenet visual features. concatenation features multi-class logistic regression classiﬁer. approach worked well surpassing previous baseline coco-vqa used theoretically powerful model lstm represent question similarly used skip-thought vectors question features resnet- extract image features. found model hidden layers trained oﬀ-the-shelf features worked well datasets. however work linear classiﬁer outperformed model smaller datasets likely model overﬁtting. table overview diﬀerent methods evaluated open-ended coco-vqa design choices. results report ‘test-dev’ split ‘test-standard’ results available lstm ibowimg dppnet smem d-nmn hybrid dmn+ hiecoatten-vgg* hiecoatten-resnet vgg* resnet mcb* mcb-att* dan-vgg* dan-resnet mlb+vg* mcb-ensemble vggnet googlenet vggnet googlenet googlenet vggnet vggnet vggnet resnet resnet resnet resnet vggnet resnet vggnet resnet resnet resnet vggnet resnet resnet resnet resnet lstm encoder acting one-hot encoding sentence used represent question features googlenet used image features. dimensionality features reduced match dimensionality lstm encoding hadamard product vectors used fuse together. fused vector used input hidden layers. lstm model embedding word sequentially features concatenated continued question reached. subsequent time-steps used generate list answers. related approach used lstm features ﬁrst last time-steps word features between. image features acted ﬁrst last words sentence. lstm network followed softmax classiﬁer predict answer. similar approach used image features lstm question instead classiﬁer another lstm used generate answer word time. requires drawing inferences modeling relationships question image. questions images featurized modeling co-occurrence statistics question image features helpful drawing inferences correct answers. major bayesian frameworks explored modeling relationships. ﬁrst bayesian framework proposed. authors used semantic segmentation identify objects image positions. then bayesian algorithm trained model spatial relationships objects used compute answer’s probability. earliest known algorithm eﬃcacy surpassed simple baseline models. partially dependent results semantic segmentation imperfect. diﬀerent bayesian model proposed model exploited fact type answer predicted using solely question. example ‘what color ﬂower?’ would assigned color question model essentially turning open-ended problem multiple-choice one. this model used variant quadratic discriminant analysis modeled probability image features given question features answer type. resnet- used image features skip-thought vectors used represent question. using global features alone obscure task-relevant regions input space. attentive models attempt overcome limitation. models learn ‘attend’ relevant regions input space. attention models shown great successes vision tasks object recognition captioning machine translation numerous models used spatial attention create region-speciﬁc features rather using global features entire image. fewer models also explored incorporating attention text representation. basic idea behind models certain visual regions image certain words question informative others answering given question. example system answering ‘what color umbrella?’ image region containing umbrella informative image regions. similarly ‘color’ ‘umbrella’ textual inputs need addressed directly others. global image features e.g. last hidden layer global text features e.g. bag-of-words skip-thoughts etc. granular enough address region speciﬁc questions. using spatially attentive mechanisms algorithm must represent visual features spatial regions instead solely global level. then local features relevant regions given higher prominence based question asked. common ways achieve local feature encoding. shown figure impose uniform grid image locations local image features present grid location. often done operating last layer prior ﬁnal spatial pooling ﬂattens features. relevance grid location determined question. alternative implement spatial attention generate region proposals image encode boxes using determine relevance box’s features using question. multiple papers focused using spatial visual attention signiﬁcant diﬀerences among methods. corresponding feature maps. apply attention representation suppressing enhancing features diﬀerent spatial locations. using question features local image features weighting factor grid location computed determines spatial location’s relevance question used compute attention-weighted image features. focus regions focused dynamic attention models used edge boxes generate bounding region proposals images. used extract features boxes. input system consisted features question features multiple choice answers. system trained produced score multiple-choice answer highest scoring answer selected. score calculated using weighted average scores regions weights simply learned passing product regional feature question embedding fully connected layer. authors proposed region proposals objects mentioned question. algorithm requires input list bounding boxes corresponding object label. training object labels bounding boxes obtained coco annotations. test labels obtained classifying bounding using resnet subsequently wordvec used compute similarity words question object labels assigned bounding boxes. score greater successively lstm network. last time-step global features entire image also network giving access global local features. separate lstm also used question representation. output lstms fully connected layer softmax classiﬁer produce answer predictions. attention layer speciﬁed single layer weights uses question feature softmax activation function compute attention distribution across image locations. distribution applied feature pool across spatial feature locations using weighted generates global image representation emphasizes certain spatial regions others. feature vector combined vector question features create representation used softmax layer predict answer. generalized approach handle multiple attention layers enabling system model complex relationships among multiple objects image. similar attentive mechanism used spatial memory network model spatial attention produced estimating correlation image patches individual words question. word-guided attention used predict attention distribution used compute weighted visual features embedding across image regions. diﬀerent models explored. one-hop model features encoding entire question combined weighted visual features predict answer. two-hop model combination visual question features looped back attentive mechanism reﬁning attention distribution. another approach incorporated spatial attention using feature maps presented this used modiﬁed dynamic memory network consists input module episodic memory module answering module. dmns used text based word sentence recurrent neural network output network used extract ‘facts.’ then episodic memory module makes multiple passes subset facts. pass internal memory representation network updated. answering module uses ﬁnal state memory representation input question predict answer. used visual facts addition text. generate visual facts features spatial grid location treated words sentence sequentially recurrent neural network. episodic memory module makes passes text visual facts update memory. answering module remains unchanged. hierarchical co-attention model applies attention image question jointly reason diﬀerent streams information. model’s approach visual attention similar method used spatial memory network addition visual attention method uses hierarchical encoding question encoding occurs word level phrase level question level using hierarchical question representation authors proposed diﬀerent attentive mechanisms. parallel co-attention approach simultaneously attended question image. alternative co-attention approach alternated attending question image. approach allowed relevance words question relevance speciﬁc image regions determined other. answer prediction made recursively combining co-attended features three levels question hierarchy. idea allow image question attention guide other directing attention relevant words visual regions simultaneously. achieve this visual question input jointly represented memory vector used simultaneously predict attention question image features. attentive mechanism computes updated image question representations used recursively update memory vector. recursive memory update mechanism repeated times reﬁne attention multiple steps. authors’ found value worked best coco-vqa. relies jointly analyzing image question. early models combining respective features using simple methods e.g. concatenation using element-wise product question image features complex interactions would possible outer-product streams information. similar ideas shown work well improving ﬁne-grained image recognition below describe prominent methods used bilinear pooling multimodal compact bilinear pooling proposed novel method combining image text features vqa. idea approximate outerproduct image text features allowing deeper interaction modalities compared mechanisms e.g. concatenation element-wise multiplication. rather outer-product explicitly would high dimensional outer-product lower dimensional space. used predict spatial features relevant question. variation model soft-attention mechanism similar method also used major change combining text question features instead element-wise multiplication combination yielded good results coco-vqa winner challenge workshop. authors’ argued computationally expensive despite using approximate outer-product. instead proposed multi-modal low-rank bilinear pooling scheme uses hadamard product linear mapping achieve approximate bilinear pooling. used spatial visual attention mechanism rivaled reduced computational complexity using neural network fewer parameters. questions often require multiple steps reasoning answer properly. example questions like ‘what left horse?’ involve ﬁrst ﬁnding horse naming object left compositional frameworks proposed attempt tackle solving series sub-steps neural module network framework uses external question parsers subtask question whereas recurrent answering units trained end-to-end sub-tasks implicitly learned. especially interesting approach framework treats sequence sub-tasks carried separate neural sub-networks. sub-network performs single well-deﬁned task e.g. find module produces heat presence certain object. modules include describe measure transform. modules must assembled meaningful layout. methods explored inferring required layout. natural language parser used input question sub-tasks question infer required layout sub-tasks executed sequence would produce answer given question example answering ‘what color tie?’ would involve executing find module followed describe module generates answer. group explored using algorithms dynamically select best layout given question automatically generated layout candidates. model implicitly perform compositional reasoning without depending external language parser. model used multiple self-contained answering units solve sub-tasks. answering units arranged recurrent manner. answering unit chain equipped attentive mechanism derived classiﬁer. authors’ claimed inclusion multiple recurrent answering units allows inferring answer series sub-tasks solved answering unit. however perform visualization ablation studies show answer might reﬁned time-step. makes diﬃcult assess whether progressive reﬁnement reasoning occurring especially considering complete image question information available answering units every time step output ﬁrst answering unit used test stage. answering questions images often require information beyond directly inferred analyzing image. knowledge uses typical context objects present image helpful vqa. example system access knowledge bank could answer questions particular animals habitats colors sizes feeding habits. idea explored demonstrated knowledge bank improved performance. external knowledge bases tailored general information obtained dbpedia possible using source tailored could yield greater improvement. authors’ incorporated dynamic parameter prediction layer fully connected layers cnn. parameters layer predicted question using recurrent neural network. allows visual features model uses speciﬁc question ﬁnal classiﬁcation step. approach seen kind implicit attentive mechanism modiﬁes visual input based question. multimodal residual networks proposed motivated success resnet architecture image classiﬁcation. system modiﬁcation resnet visual question features residual mapping. visual question embedding allowed residual blocks skip connections. however residual block visual data inter-weaved question embedding. authors explored several alternate arrangement constructing residual architecture multi-modal input chose network based performance. although many methods proposed diﬃcult determine general techniques superior. table provides breakdown diﬀerent algorithms evaluated coco-vqa based techniques design choices utilize. table also includes ablation models respective algorithms whenever possible. ablation models help identify individual contributions design choices made authors. ﬁrst observation make resnet produces superior performance vggnet googlenet across multiple algorithms. evident models identical setup change image representation. increase observed using resnet- instead vgg- image features. found increase making change model. similarly changing vgg- resnet- increased performance cases rest architecture kept unchanged. general believe spatial attention used increase performance model. shown experiments models evaluated without attention attentive version performed better cases. however attention alone appear suﬃcient. discuss section bayesian compositional architectures signiﬁcantly improve comparable models despite interesting approaches. model performed competitively combined model. unclear whether increase model averaging proposed bayesian method. similarly models outperform comparable non-compositional models e.g. possible methods perform well speciﬁc sub-tasks e.g. shown specially helpful positional reasoning questions shapes dataset. however since major datasets provide detailed breakdown question types possible quantify systems perform speciﬁc question types. moreover improvements rare question types negligible impact overall performance score making diﬃcult properly evaluate beneﬁts methods. discuss issues section shown figure rapid improvement performance algorithms still signiﬁcant best methods humans. remains unclear whether improvements performance come mechanisms incorporated later systems e.g. attention factors. moreover diﬃcult decouple contributions text image data isolation. also numerous challenges comparing algorithms variations evaluated. section discuss issues. consists distinct data streams need correctly used ensure robust performance images questions. current systems adequately vision language? ablation studies routinely shown question models perform drastically better image models especially open-ended coco-vqa. coco-qa simple image-blind models question achieve figure current state-of-the-art results across datasets compared earliest baseline human performance. earliest baseline refers numbers reported creators datasets current state-of-the-art models chosen highest performing methods table daquar daquar- coco-qa report plain accuracy coco-vqa reports accuracyv accuracy gain using image comparatively modest also shown daquar- using better language embedding image-blind model produced results superior earlier works employing images questions. primarily factors. first question severely constrains kinds answers expected many cases essentially turning open-ended question multiple-choice e.g. questions color object color answer. second datasets tend strong bias. factors make language much stronger prior image features alone. predictive power language images corroborated ablation studies. authors studied model trained using image question features. studied predictions model diﬀered given image question compared given both. found image-only model’s predictions diﬀered combined model often question model. also showed question phrased strongly biases answer. training neural network regularities incorporated model. produces increased performance dataset potentially detrimental creating general system. figure slight variations question phased causes current algorithms produce diﬀerent answers. left example uses system right example uses system bias studied using synthetic cartoon images. created dataset solely binary questions question could asked images mostly identical except minor change caused correct answer diﬀerent. found model trained unbalanced version dataset performed worse balanced test dataset compared model trained balanced version dataset. conducted experiments assess eﬀect language bias vqa. first used model model trained coco-vqa allows contribution question image features assessed independently splitting weights softmax output layer image question components. asked simple binary questions relatively equal prior choices image must analyzed answer question. examples shown figure system performs poorly especially considering baseline accuracy yes/no questions coco-vqa next studied language bias aﬀected complex mcb-ensemble model trained coco-vqa. model winner challenge workshop. this created small dataset consisting yes/no questions. create dataset used annotations validation split coco dataset determine whether image contained person asked equal number ‘yes’ ‘no’ questions whether people present. used questions ‘are people photo?’ person picture?’ person photo?’ variation yes/no questions accuracy mcb-ensemble dataset worse chance starkly contrasts results coco-vqa likely severe bias training dataset inability learn task. shown figure systems sensitive question phrased. observed similar results using system quantify issue created another dataset validation split coco dataset used evaluate mcb-ensemble model trained coco-vqa. dataset task identify sport played. asked three variations question ‘what doing?’ ‘what playing?’ ‘what sport playing?’ variation contains questions seven common sports mcb-ensemble achieved variation variation variation dramatic increase performance variation caused inclusion keyword ‘playing’ instead generic verb ‘doing.’ increment variation caused explicitly including keyword ‘sport’ question. suggests systems over-dependent language ‘clues’ annotators often include. taken together experiments show language bias issue critically aﬀects performance current systems. conclusion current systems dependent question image content. language bias datasets critically aﬀects performance current systems limits deployment. datasets must endeavor compensate issue either questions force analysis image content and/or making datasets less biased. diﬃcult determine much attention helps algorithms. ablation studies attentive mechanisms removed models impairs performance currently best model coco-vqa employ spatial visual attention simple models attention shown exceed earlier models used complex attentive mechanisms. example attention-free model used multiple global image feature representations instead single performed well compared attentive models. combined image question features using element-wise multiplication addition instead solely concatenating them. combined ensembling yielded results signiﬁcantly higher complex attention-based models used similar results obtained systems employ spatial attention attention alone ensure good performance incorporating attention model appears improve performance model attention used. authors showed methods commonly used incorporate spatial attention speciﬁc image features cause models attend regions humans tasked vqa. made observation using attentive mechanisms used regions model learns attend discriminative biases dataset algorithm attend. example asked question whether drapes image algorithm instead look bottom image rather windows questions figure using system answer score question ‘are people picture?’ roughly ‘yes’ ‘no’ answering question correctly requires examining image model fails appropriately image information. dataset bias signiﬁcantly impairs ability evaluate algorithms. questions require image content often relatively easy answer. many presence objects scene attributes. questions tend handled well cnns also strong language biases. harder questions beginning ‘why’ comparatively rare. serious implications evaluating performance. coco-vqa system improves accuracy questions beginning ‘is’ ’are’ increase overall accuracy however increase ‘why’ ‘where’ questions increase accuracy fact even ‘why’ ‘where’ questions answered correctly overall increase accuracy hand answering ‘yes’ questions beginning there’ yield accuracy questions. problems could overcome type question evaluated isolation mean accuracy across question types used instead overall accuracy benchmarking algorithms. approach similar mean per-class accuracy metric used evaluating object classiﬁcation algorithms adopted bias amount test data available diﬀerent object categories. using binary questions evaluate algorithms attracted signiﬁcant discussion community. main argument using binary questions lack complex questions relative ease answering questions typically generated human annotators. visual genome visualw exclude binary questions altogether. authors argued choice would encourage complex questions annotators. hand binary questions easy evaluate questions theory encompass enormous variety tasks. shapes dataset uses binary questions exclusively contains complex questions involving spatial reasoning counting drawing inferences using cartoon images also showed questions especially diﬃcult algorithms dataset balanced. however challenges creating balanced binary questions real world imagery. coco-vqa ‘yes’ much common answer ‘no’ simply people tend questions biased toward ‘yes’ answer. long bias controlled yes/no questions play important role future benchmarks system capable solely binary questions abilities fully assessed. real-world applications enabling blind questions visual content require output system open-ended. system solely handle binary questions limited real-world utility. challenging evaluate open-ended multi-word answers multiple-choice proposed evaluate algorithms. long alternatives suﬃciently diﬃcult system could evaluated manner deployed answer open-ended questions. reasons multiple choice used evaluate visualw visual genome variant dataset. framework algorithm access number possible answers along question image. must select among possible choices. major problem multiple-choice evaluation problem reduced determining answers correct instead actually answering question. example formulated answer scoring task system trained produce score based image question potential answers. answers system features. achieved state-of-the-art results visualw rivals best methods coco-vqa method performing better many complex systems attention. large extent believe system performed well learned better exploit biases answers instead reasoning images. visualw showed variant system used solely answers imagequestion-blind rivaled baselines using question image. argue system able operate without given answers inputs. multiple-choice important ingredient evaluating multi-word answers alone suﬃcient. multiple-choice used choices must selected carefully ensure question hard deducible provided answers alone. system solely capable operating answers provided really solving available system deployed. existing benchmarks suﬃcient evaluate whether algorithm ‘solved’ vqa. section discuss future developments datasets make better benchmarks problem. future datasets need larger. datasets growing size diversity algorithms enough data training evaluation. small experiment trained simple baseline model using resnet- image features skip-thought features questions assessed performance function amount training data available coco-vqa. results shown figure clear curve started approach asymptote. suggests even datasets biased increasing size dataset could signiﬁcantly improve accuracy. however mean increasing size dataset suﬃcient turn good benchmark humans tend create questions strong biases. cause truly evaluating algorithm. real-world open-ended diﬃcult achieve without carefully instructing humans generate questions. bias long problem images used computer vision datasets problem compounded bias questions well. addition larger less biased future datasets need nuanced analysis benchmarking. publicly released datasets evaluation metrics treat every question equal weight kinds questions easier either bias existing algorithms excel answering kind question e.g. object recognition questions. datasets coco-qa divided questions distinct categories e.g. coco-qa color counting location object. believe mean per-question type performance replace standard accuracy question would equal weight evaluating performance. would long towards making algorithm perform well wide variety question types perform well overall otherwise system excelled answering ‘why’ questions slightly worse another model common questions would fairly evaluated. this question would need assigned category. believe eﬀort would make benchmark results signiﬁcantly meaningful. scores question type could also used compare algorithms kind questions excel important basic research problem computer vision natural language processing requires system much task speciﬁc algorithms object recognition object detection. algorithm answer arbitrary questions images would milestone artiﬁcial intelligence. believe necessary part visual turing test. paper critically reviewed existing datasets algorithms vqa. discussed challenges evaluating answers generated algorithms especially multiword answers. described biases problems plague existing datasets. major problem ﬁeld needs dataset evaluates important characteristics algorithm algorithm performs well dataset means well general. future work involves creation larger varied datasets. bias datasets diﬃcult overcome evaluating diﬀerent kinds questions individually nuanced manner rather using naive accuracy alone help signiﬁcantly. work needed develop algorithms reason image content algorithms lead signiﬁcant areas research.", "year": 2016}