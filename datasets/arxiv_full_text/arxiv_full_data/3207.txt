{"title": "A Bi-clustering Framework for Consensus Problems", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We consider grouping as a general characterization for problems such as clustering, community detection in networks, and multiple parametric model estimation. We are interested in merging solutions from different grouping algorithms, distilling all their good qualities into a consensus solution. In this paper, we propose a bi-clustering framework and perspective for reaching consensus in such grouping problems. In particular, this is the first time that the task of finding/fitting multiple parametric models to a dataset is formally posed as a consensus problem. We highlight the equivalence of these tasks and establish the connection with the computational Gestalt program, that seeks to provide a psychologically-inspired detection theory for visual events. We also present a simple but powerful bi-clustering algorithm, specially tuned to the nature of the problem we address, though general enough to handle many different instances inscribed within our characterization. The presentation is accompanied with diverse and extensive experimental results in clustering, community detection, and multiple parametric model estimation in image processing applications.", "text": "abstract. consider grouping general characterization problems clustering community detection networks multiple parametric model estimation. interested merging solutions diﬀerent grouping algorithms distilling good qualities consensus solution. paper propose bi-clustering framework perspective reaching consensus grouping problems. particular ﬁrst time task ﬁnding/ﬁtting multiple parametric models dataset formally posed consensus problem. highlight equivalence tasks establish connection computational gestalt program seeks provide psychologically-inspired detection theory visual events. also present simple powerful bi-clustering algorithm specially tuned nature problem address though general enough handle many diﬀerent instances inscribed within characterization. presentation accompanied diverse extensive experimental results clustering community detection multiple parametric model estimation image processing applications. group contains outliers. depending application context might empty. generically consider grouping algorithm provides candidate groups power groups need form partition cover general characterization subsumes many diﬀerent grouping partition considering clustering algorithms. traditionally algorithms assume broad perspective clus dataset network i.e. vertices commupartition comprehensive survey contains single group obtained ﬁtting parametric model running diﬀerent grouping algorithms; running algorithm diﬀerent parameters; running grouping algorithm diﬀerent modalities data; simultaneously. speciﬁc discard others. proven rather diﬃcult task even standard measures modularity normalized cuts known critical shortcomings settle selecting solution pool? argue better option combine information diﬀerent results pool result consistent them. sistent groups higher quality. posed maximum-weight clique problem. type formulation simultaneously introduced image segmentation extended clustering community detection candidate sets form solution share common issue need sound general quality measure clustering ﬁeld plethora methods assess classify clustering algorithms developed interesting results e.g. unfortunately lack general deﬁnition makes diﬃcult unifying clustering theory and/or measure. community detection example best establish community structure network also elusive task still disputed e.g. alternative approach exploit consistencies diﬀerk=. consensus/ensemble clustering well known family techniques used data analysis solve type problems. typically goal search so-called mean partition i.e. partition similar average input partitions. many algorithms analyzing simple techniques applying clustering algorithm complex techniques. thorough survey area. another techniques binary low-rank decomposition context community detection works explicitly addressed consensus within standard framework described. matrix simply thresholded connected components give ﬁnal result. considered adjacency matrix weighted network. following steps mentioned aggregation process used build involves loosing information contained individual matrices particular pairwise relations conserved relations involving larger groups nodes might lost. addition using average several partitions might robust poor contributions. propose novel framework perspective reaching consensus grouping problems posing bi-clustering problem. proposed approach main advantages relations conserved contribute consensus search much smaller matrix rendering problem tractable large datasets. also propose parameterless bi-clustering algorithm type matrices analyze. stress goal ﬁnding better optimum objective function given grouping method obtaining overall good solution consensus search. addition ﬁrst time task ﬁnding/ﬁtting multiple parametric models dataset formally posed consensus/bi-clustering problem. equivalence tasks highlighted proposed framework devote special attention explain rationale behind characterization. finally make formal connection computational gestalt program seeks provide psychologically-inspired detection theory visual events. proposed framework allows consider theory perspective statistical algorithmic viewpoints. provide insights show suitability approach research direction ﬁeld. organization. remainder paper organized follows. section present proposed approach general grouping framework. sections show framework applies problems clustering community detection multiple model parametric estimation respectively. sections accompanied diverse extensive experimental results. section discuss links computational gestalt theory. finally provide closing remarks section reaching consensus solving bi-clustering problem. input consensus algorithm pool {ck}c candidates deﬁnes universe also assign weight group candidate data deﬁne matrix whose rows columns represent data elements candidates respectively; element i-th element belongs j-th group otherwise. call preference matrix. fig. presents simpliﬁed examples. fig. examples preference matrix. data consist points segments forming star. groups potential parametric models. data consist points four clusters. cases preference matrix reordered group improved visualization. although clustering elements/objects using features sampled models belong might lead good results certain applications fully address problem hand. relationship objects sampled models actual focus interest pattern-discovery algorithm needed relationship space. main contribution work therefore address problem grouping bi-clustering preference matrix provides intuitive rationale since bi-cluster jointly selecting subset elements subset groups former belongs latter. directly analyzing keep information contained classical consensus algorithms analyze matrix deﬁnition contrast work much smaller matrix since common grouping problems multiple parametric model estimation commonly show prune back general scenario. another important feature analyzing large datasets base algorithm need complete dataset. split network several chunks algorithms chunk bi-clustering formulation perform stitching consensus. notice course base algorithms consistently make mistakes translated consensus solution characteristic common consensus algorithms since natural detect consistent mistake. sparse singular value decomposition shown great promise mainly conceptual algorithmic simplicity. looking interpretable row-column associations within algorithms iterate steps correctly setting parameters crucial since determine size bi-clusters sets cross-validation ssvd uses bayesian information criterion. minimum description length criterion used number iterations ssvd. experiments ﬁnding correct values parameters proven extremely challenging since experiment needs speciﬁcally tuned values. motivates part development algorithm described next. application sparse non-negative matrix. notice positivity constraints sparsifying eﬀect them. intuition behind approximating sparse non-negative matrix non-negative factors create sparse approximation sparse. thus obtain sparse factors ssvd without introducing parameters. another consequence sparsity frobenius norm entirely well suited analyzing appropriate instead ﬁtting term challenge easy parameter set. avoid cumbersome decision process propose inscribe l-nmf approach iterative loop ssvd. rank-one factorization thus approximate subset correctly detecting single bi-cluster. note related common problem clustering known masking conceptually similar iterative procedure address issue algorithmic decisions parameters. algorithm summarizes proposed non-negative bi-clustering approach. notice instead subtracting product ssvd corresponding rows columns zero enforcing disjoint active sets successive hence orthogonality. also ensures non-negativity maintained throughout iterations. bi-clusters allowed share elements change rows proposed algorithm eﬃcient simple code demonstrated work well experimental results present later. iterations stop empty contains noise second case controlled parameters parameters determine minimum bi-cluster size rows columns necessary considered structured pattern. note contrast parameters discussed before intuitive related physics problem easier encodes minimum number elements bi-cluster contain encodes minimum number candidate groups need agreement form bi-cluster. theory depend probability non-zero entry preference matrix would interesting explore dependencies theoretical point view. experiments paper clustering community detection experiments. number diversity experiments values work show that practice need carefully tuned case. part fact clustering community detection experiments bi-clusters allowed intersect enabling proposed algorithm quickly eliminate spurious entries preference matrix. case multiple parametric model estimation bi-clusters intersect enforce posteriori strict value using computational gestalt theory. adjustment done bi-clustering process decided posteriori improve clarity homogeneity diﬀerent applications. notice that alternative could consider proposed bi-clustering algorithm lossy compression encoder; select ﬁrst bi-clusters yield maximum compression minimal loss note. seemingly related approach involves using symmetric non-negative matrix factorization cluster co-occurrence matrix deﬁned equation clusters. achieved approximating problem hdht provides non-negative low-rank approximation matrix encodes sizes obtained clusters. matrix decomposition methods number clusters critical parameter fact parameters interested discovering constraints equation proposed formulation presents beneﬁts equation increased robustness. double averaging present original problem thus problem first equation acts like pooling operator loosing critical information. second frobenius norm known non-resilient outliers. hand formulation computes robust median approximation preference matrix carries needed information. input pool group candidates. ingredient every consensus problem quality input group candidates. performing consensus many extremely poor groups yield good consensus solution. among candidates consensus algorithm needs certain number reasonably good groups time many groups. otherwise number groups overwhelms number good groups masking phenomenon occur facing extremely hard patterndiscovery problem. thus assume that general case reasonably good candidate groups contaminated groups particular case parametric model estimation disposal information nature groups thus employ simple powerful testing procedure eliminate vast majority candidate groups. currently investigating perform similar tests non-parametric scenarios. possible algorithms ideally data well mistakes caused diﬀerent factors data and/or diﬀerent algorithmic decisions/artifacts hence systematically appearing over. observation trivially enforced practice; often enough pool candidates exhibit consistency results keeping variety. following presentation general consensus grouping framework procedure proceed show diﬀerent application contexts. focus clustering community detection networks multiple parametric model estimation. consensus clustering. clustering seeks group observations subsets that sense intra-cluster observations similar inter-cluster ones. components exploratory data mining common technique statistical data analysis used diverse ﬁelds machine learning pattern recognition image analysis information retrieval bioinformatics. broad overview ﬁeld. consensus clustering straightforward application grouping result instance clustering algorithm preference matrix therefore constructed straightforward fashion. experimental results. experiments uniform weights assessing quality solution ground truth available standard normalized mutual information f-measure evaluate ﬁgures observe synthetic examples datasets diﬀerent mixtures gaussians. figure several instances gaussian mixture model clustering algorithm k-means instance diﬀerent number pre-speciﬁed clusters. figure several instances mean-shift algorithm diﬀerent kernel size. show bi-clusters obtained proposed consensus algorithm. cases without tuning parameters quality solution approximates quite well ground truth qualitatively quantitatively. figure consensus solution ranks second best comparing base algorithms terms f-measure; figure ranks ﬁrst. interesting visualize diﬀerences classical problem l-nmf problem figure compare active-sets rank-one factors obtained formulations ﬁrst iteration algorithm example figure approximates average preference fig. synthetic example proposed bi-clustering consensus computed different instances k-means varying number classes. bi-cluster corresponds jointly selected point cluster assignments thus show color points covariance matrices constituting clusters represented ellipses. stand normalized mutual information f-measure respectively. matrix whole; thus active-set analyzed bi-cluster merges information almost candidate groups trying approximate single group contrarily proposed l-nmf formulation robustly subset preference matrix entries; thus selects sparse number candidate groups closely single ground-truth group. fig. synthetic example proposed bi-clustering consensus computed diﬀerent mean-shift instances varying kernel size. bi-clusters correspond jointly selected point cluster assignments thus show color points corresponding covariance matrix represented ellipse. stand normalized mutual information f-measure respectively. methods standard synthetic dataset. ﬁrst observation spectral clustering obtain good results consensus algorithm long correct number classes used. notice sort self-defeating argument introducing parameters getting parameters. consideration valid snmf although case obtain poor solution even correct number classes used anclustering algorithm j-linkage popular parametric model estimation also yields poor solution. algorithm without tuning parameters yields good solution misclassiﬁes single point also present experiments standard real datasets repository figure compute consensus solution several instances k-means spectral clustering. four examples obtained results competitive best solutions pool iris dataset consensus solution better base solutions. dataset contains classes e.g. breast dataset becomes much harder optimize number classes since pool solutions balance over-clustered solutions either right number clusters over-clustered. case obtained good results employing algorithms yield three clusters. fig. bi-cluster extraction using versus l-nmf algorithm show active ﬁrst extracted bi-cluster example figure sparsely ﬁtting preference matrix using l-norm helps detect bi-cluster corresponds single ground-truth group without tuning active-set thresholds. subspace clustering. many problems involving high-dimensional data class category spans low-dimensional subspace high-dimensional ambient approximately union low-dimensional subspaces. sparse subspace clustering aims clustering data points ﬁnding correct subspaces. data subspace clustering motion segmentation videos given feature points multiple rigidly moving objects tracked multiple frames video goal separate feature trajectories according object’s motions treat case subspace clustering another grouping instance therefore addressed proposed bi-clustering consensus approach. several instances algorithm diﬀerent values regularization parameter compute consensus solution them. results experiment shown figure fig. solving consensus problem standard synthetic dataset results obtained base algorithms. clustering co-occurrence matrix might yield good results introduces parameters depending employed clustering algorithm. snmf yields poor result even correct number classes speciﬁed. j-linkage yields poor results. single point misclassiﬁed proposed bi-clustering approach. stand normalized mutual information f-measure respectively. following methodology misclassiﬁcation error analyze results. ﬁrst observation range values give correct results thus enabling consensus algorithms. next observation misclassiﬁcation errors obtained consensus solution sometimes even outperforming best individual result. normalized mutual information f-measure values individual algorithm proposed consensus solution. stands spectral clustering kernel clusters. k-means value parenthesis indicates actual value indicates ground truth number classes. fig. consensus experiments datasets repository proposed bi-clustering solution provides normalized mutual information values universally best algorithms pool without information nature algorithms parameters. graph representation table considering instances dataset. value marked vertical dotted line. competitive misclassiﬁcation errors obtained consensus solution without information nature algorithms parameters. fig. subspace clustering example hopkins dataset consists video sequences motions corresponding low-dimensional subspace video. original trajectories data projected lower-dimensional space using pca. base algorithms diﬀerent subspace clustering instances varying regularization parameter show mean misclassiﬁcation error consensus solution. table bi-cluster sizes several clustering experiments. algorithm returns bi-clusters. compare sizes bi-clusters i.e. last returned ﬁrst discarded respectively. cases missing numbers columns. mentioned above every clustering experiment table compares size bi-clusters bi-cluster) several clustering experiments. size drop cases dramatic become extremely easy set. consensus community detection networks. networks frequently used describe many real-life scenarios units interact references therein). seemingly common property many networks community structure networks divided groups intra-group connections denser inter-group ones. finding analyzing communities sheds light important characteristics networks data represent. however best establish community structure still disputed. addressing topic section. experimental results. experiments following base algorithms community detection louvain infomap spectral clustering number detected clusters/communities. assessing quality solution ground truth available normalized mutual information unless speciﬁed uniform weights small size makes easy visualize preference matrix network models interactions middle bronze aegean archaeological sites. algorithm recovers correct number bi-clusters critical parameter classical approaches. figure observe detail bi-clustering algorithm selects entries create solution preferring regularities matrix disregarding peculiarities individual solutions important feature proposed algorithm blindly select best solution composes consensual solution provided candidates. experiment compute modularity solution provided k-th base algorithm smooth increasing nonlinear function weight community general community detection using seeds. happens enough information network recover correct structure? college football network presents interesting example. network represents matches played teams season. teams organized divisions teams play matches teams division diﬀerent ones. hence divisions table results synthetic networks produced standard benchmark generator single algorithm produces best solution every network; however consensus solution always competitive best base solution help spectral clustering number ground truth communities. mod. stand normalized mutual information modularity respectively. fig. running diﬀerent standard algorithms mis´erables network nodes show three individual results consensus solution. biclustering result carbon-copy individual solutions creates one. also notice colored preference matrix algorithm corrects individual solutions algorithm proposed consensus algorithm considered ground truth communities network. diﬀerent community detection algorithms network observe divisions well recovered them teams assigned diﬀerent community blue arrows figure fact observe team’s matches play teams division tiny priori information manually adding seeds i.e. forcing nodes community. this modify corresponding rows replacing disjunction simple seeding mechanism able correct original mistake. algorithm diﬀerent networks. proposed approach also allows combine results community structure algorithms analyze diﬀerent aspects given network facebook dataset presents example. edges represent facebook friendship also observe several node attributes particular example focus duke graduates. build modalities network assigning diﬀerent weights edges. ﬁrst gender information assigning weight edge links students diﬀerent otherwise. second fig. college football network nodes representing matches between diﬀerent teams. base methods separate node division. looking edges indeed correct given network. using little extra information i.e. forcing green nodes graph community corrects eﬀect. before mod. stand normalized mutual information modularity respectively. study ﬁeld information assigning weight students share major minor share major minor share major minor. louvain algorithm independently networks obviously obtain diﬀerent community structures figure running consensus algorithm results produce solution aggregates information modalities. proposed approach allows perform coherent cross-modality analysis something possible traditional community detection algorithms. therefore expand analysis multimodal networks using standard algorithms without need develop algorithms. another interesting example occurs network connectivity changes time diﬀerent modalities exhibit diﬀerent edge sets. important example graph obtained inference diﬀerences and/or errors inference process might yield diﬀerent connectivities. simulate example taking network building perturbed copies randomly reassigning subset edges. infomap copy compare result terms infomap result original network small portion edges perturbed best individual solution still good non-negligible chance perturbations alter community structure original network. longer true number perturbed edges increases. algorithm able balance peculiarities perturbed solutions obtaining solution much closer original hence resilient perturbations. fig. network facebook links duke graduates nodes build diﬀerent modalities assigning weights edges according diﬀerent node features ﬁeld study gender. consensus diﬀerent results louvain algorithm. charts represent distribution nodes modality’s communities respect consensus. dom. provide average values across trials using perturbed networks trial. result infomap original network considered ground truth. consensus solution outperforms individual ones performance increasing edges perturbed. multiple parametric model estimation. section addresses problem ﬁtting multiple instances parametric model data corrupted noise outliers formally connecting bi-clustering consensus. context outliers detailed next. objects subset might intersect measurements described parametric model parameter vector. following objects inliers model also generally refer describes them. objects cannot described models refer outliers. important diﬃcult problem standard robust estimators like ransac designed extract single model. begin formally explaining ransac machinery work illustrate value perspective proposed multi-model formulation. denote minimum number elements necessary uniquely characterize given parametric model e.g. lines circles. example want discover alignments point cloud elements points models lines since line deﬁned points. objects called minimal sample ransac randomly samples msss generating model hypothesis. number overestimation number trials needed obtain certain number good models ransac computes model hypothesis using equation algorithm describes standard ransac procedure. applying ransac sequentially removing inliers dataset model instance detected proposed solution multi-model estimation e.g. however approach known suboptimal multiransac algorithm provides eﬀective alternative although number models must known priori imposing limiting constraint many applications. alternative approach consists ﬁnding modes parameter space. overall idea data parameter space random sampling seek modes distribution discretizing distribution i.e. using randomized hough transform using non-parametric density estimation techniques like mean-shift clustering algorithm these however intrinsically robust techniques even robustiﬁed outliers rejection heuristics moreover choice parametrization critical among important shortcomings computational cost techniques high well. techniques share common high-level conceptual approach model estimation order solve problem objects clustered. work propose alternative formulation involves bi-clustering objects arise ransac execution explicitly avoiding spurious inconsistencies. formulation conceptually changes data produced popular ransac related model-candidate generation techniques analyzed. feature vectors obtaining powerful state-of-the-art clustering-based technique multiple model estimation called j-linkage. this tailored agglomerative clustering algorithm. agglomerative clustering algorithms j-linkage proceeds bottom-up manner starting singletons iteration algorithm merges clusters smallest distance. j-linkage uses jaccard distance features updated merging process. cluster’s feature computed intersection features objects although clustering objects using features sampled models belong might lead good results certain applications fully address problem hand. relationship objects sampled models actual focus interest pattern-discovery algorithm needed relationship space. thus interthus propose address problem model estimation bi-clustering matrix using algorithm presented section conceptual advantage object allowed belong multiple bi-clusters frequently observed figure lines intersect share points translated elements outside block-diagonal structure ransac-related techniques arbitrarily assign shared objects given model. cleansing preference matrix. standard random sampling approach multiple model estimation generates many good model instances also generates many models general number models exceeds number good ones. worth devoting computational eﬀort analysis columns pattern-discovery technique bi-clustering approach presented previous section would beneﬁt simple eﬃcient statistically meaningful method discarding models. models typically contain handful objects. question determine minimum size good consensus set? important computational contribution addressed next following contrario testing mechanism presented depth computing probability model associated consensus least objects. simplifying assumption objects i.i.d. probability event binomial tail built noise level later provide details compute model associated consensus obtained minimum number elements necessary uniquely characterize given parametric model. easy prove linearity expectation expected number ε-meaningful models ﬁnite random models smaller alternatively ntests empirically analyzing training dataset providing tighter bound expectation. equation provides formal probabilistic method testing model likely happen random not. statistical viewpoint method goes back multiple hypothesis testing. following contrario reasoning decide whether event interest occurred probability occurring chance deﬁned random model. words model notice sense using contrario validation procedure backwards instead using detect good models eliminate models. need sharp event-detection procedure order keep good models; need statistical test eliminate vast majority clearly poor models. hence value critical model inherently robust poor models shown section statistical test controls false positives contrario tests provide good control false negatives result statistical validation procedure preference matrix considerably shrunk. many unuseful columns eliminated elimination rows might also become zero-valued also eliminated. shrunk preference matrix bi-clustering algorithm gaining stability results well speed. algorithm multiple parametric model estimation. presented algorithm claimed tighter bounds could computed case parametric models. tests used cleansing preference matrix readily available task. bi-cluster compute number elements group number rows bi-cluster. finally keep bi-clusters ε-meaningful. resulting complete algorithm follows bi-cluster cleansed version using algorithm discard bi-clusters ε-meaningful section tests also bi-cluster meaningful consider elements belong bi-clusters. last step ensures bi-cluster contains points belong mild version exclusion principle provide several standard examples lines circles figure line examples circles examples proposed bi-clustering approach much better recovering data structure j-linkage popular techniques figure j-linkage considered state-of-the-art algorithm general tendency clusters fewer points expected emphasized comparing recall methods table examples bi-clustering algorithm automatically ﬁnds number clusters. j-linkage uses size obtained clusters decide whether keep discard them. figure easily seen method stable since decay size indicate proper cut-point. j-linkage perform accurate point-model assignments notice missing points detected lines circles. size j-linkage clusters robust criterion selecting ﬁnal clusters. proposed approach correctly retrieves lines circles. fig. comparisons several multiple model estimation algorithms example figure contrarily proposed approach techniques miss models detect false ones. also observe figure proposed approach correctly recover overlapping models. intrinsic limitation j-linkage multiple model estimation techniques generally based partitioning objects figure presents example real application planes pozzovegianni dataset. points obtained diﬀerent images building sparse multi-view reconstruction algorithm. algorithm recovers planes scene properly reconstruct building structure additional example developed simple method detecting cells microscopy images figure achieved using ellipses models instead using edge points base elements line segments detected providing robust detections making process estimating ellipse stable. example instead using zero-centered distance distribution equation mean distance pixels. method retrieves ellipses image without tuning generic algorithm. notice employ generic contrario test completely adapted scenario lengths segments taken account deﬁnition results would fig. example planes pozzoveggiani dataset. points obtained images ones using multi-view reconstruction. correctly recover building structure detecting planes. automatically reﬁned improved. noticeable small ellipses contain segments. goal work present general detection framework leave speciﬁc reﬁnement future work. finally present another application proposed framework vanishing point detection uncalibrated images. cells application line segments base objects/elements method. groups formed detecting subset lines intersect given point image plane case since need segments compute helps show cases sampling procedure artiﬁcially boosts performance helps speed algorithm. figure show results york urban database robustly reliably detect vanishing points. ellipses results could improved considering segment fig. ellipse detection example. line segments base elements method. able reliably detect ellipses without speciﬁc highly tuned method. notice ﬁnal cleansing process eliminates detected ellipses left corner bottom image removals make sense perceptual point view. j-linkage yield good results returning wrong clusters exhibiting arbitrary cut-oﬀ point experiments observe bi-clustering approach provides good description detected model instance terms objects; compact overall description considering reduced number detected bifig. vanishing point detection example. simple approach correctly recovers vanishing points images. show line segments bi-cluster vanishing points might away image image plane. detection parallelism images ill-posed problem information inherently lost projection. handful segments determining observed projections true orientation. despite indeterminacy overall problem still solvable. boundaries multiple parametric model estimation. proposed approach detecting parametric models assumes observing unusual concentration elements around parametric model instance enough produce robust candidate. situations assumption valid works well practice seen numerous previous examples. however situations element-model distances enough fully characterize given conﬁguration elements. illustrate simple example figure using classical deﬁnition consensus every line passes central cluster large consensus set. artiﬁcially ﬁres many candidate detections creating bi-cluster. notice result completely consistent theoretical formulation bi-product using lines since similar eﬀect occur circles example. neither ransac hough transform discern points lying along line points concentrated small cluster. fact setup missing dispersion constraint along model. addressed particular case many formulations could employed achieve desired eﬀect ripley’s functions additionally improved validation would greatly simplify validation step proposed method. fig. aberrant example containing uniformly distributed points small cluster center. line detection algorithm recover central cluster. result consistent proposed protocol although might claim line detection scheme yield detections case. stricter deﬁnition contrario pruning rule needed correct eﬀect before need concentration points orientation orthogonal line additionally dispersion points along line. values general important obtain good results. notice dependency introduced bi-clustering formulation common parametric model estimation approaches. example methods based ransac randomized hough transform share dependency. general case theoretical formulation clear practical guideline properly setting values. practice many cases present reasonably intuitive range experience setting value critical time concern simply select large value. time-critical applications learn training examples. pmss probability drawing cardinality composed inliers details. formulation shortcomings. first pmss trivial quantity and/or estimate without deep knowledge structure dataset second equation relates single model; establishing sound relation multiple models requires knowing advance number models. research much needed theoretically practically important aspect state-of-the-art approaches would beneﬁt side note problem similar nature chinese restaurant indian buﬀet processes ﬁxed unknown number tables/dishes driving principle computational gestalt theory helmholtz principle simplest form states perceive structure uniform random image computational gestalt theory makes extensive stronger form namely whenever large deviation randomness occurs structure perceived. realization uniform process. ﬁnally realization probable not? proves contrario grouping process play. helmholtz principle states roughly mental experiments object features assumed uniformly distributed independent theory interpreted multiple hypothesis testing framework visual events works controlling number false alarms proxy expectation number occurrences visual event stated contrario model. section exploited detection theory twice cleanse preference matrix adjust minimum number elements necessary accept bi-cluster. contrario tests allow assess improbability given event conﬁguration. however framework also computes information regarding repeatability conﬁguration probing data. sense interpret framework repeatedly querying data random looking conﬁgurations arise over. contrario tests exploit additional dimension nonetheless plays central role assessing randomness given conﬁguration arises over chances realization random process? simple discussion brings forward need develop statistical tests also consider repeatability conﬁguration random sampling framework. pseudo-norm counting number non-zeros. simple observation might lead develop simpler general extensible geometric probes employing reduced assumptions computing ones used appendix common contrario literature. presented framework also connections so-called pr¨agnanz principle gestalt psychology ...of several geometrically possible organizations actually occur possesses best simplest stable shape quoted koﬀka’s book analyzing possible conﬁgurations proposed framework assigns formal mathematical meaning terms best simplest stable. analyze characteristics best. resulting conﬁgurations obtained solution non-parametric minimization problem intuitive interpretation problem point view framework might provide mathematically sound formulate implement pr¨agnanz principle integrating helmholtz principle seen stopping criterion bi-clustering process. psychophysical experiments course needed fully validate intuitive conceptual vinculation. conclusion. paper proposed framework perspective reaching consensus grouping problems. general characterization grouping problems subsumes many diﬀerent areas e.g. clustering community detection networks multiple parametric model estimation. pose consensus grouping bi-clustering problem obtaining conceptually simple descriptively rich modeling. presented simple powerful bi-clustering algorithm speciﬁcally tuned nature problem address though general enough handle many diﬀerent instances inscribed within framework. particular ﬁrst time task ﬁnding/ﬁtting multiple parametric models dataset formally posed consensus bi-clustering problem. equivalence tasks highlighted proposed framework devoted special attention explain rationale behind characterization. future line research currently investigating whether using hard thresholding scheme actually necessary. instead working binary data could work real-valued object-model distance matrix eliminating critical parameter haunting ransac framework years. also discussed connection computational gestalt program seeks provide quantitative psychologically-inspired detection theory visual events. provided cues show suitability approach research direction ﬁeld. working exploiting connections fully develop perspective fundamental problem. also exploring contrario statistical tests non-parametric scenarios. perspective point need sharp testing mechanism coarse procedure avoid ﬁlling preference matrix huge number poor groups could clutter bi-clustering algorithm. would allow freedom selection pool input algorithms. alternative framework could extended individual weights preference matrix. weights would thus model conﬁdence measure i-th element belonging j-th group. would interesting explore possibility depth. finally would like stress proposed framework limited presented applications. ﬂexible enough handle type grouping problem plan address applications formulated way. clear examples image segmentation supervised classiﬁcation fusing output diﬀerent classiﬁers obtain robustiﬁed results. alternating direction method multipliers solving algorithm works coordinate descent fashion successively minimizing respect time ﬁxing others recent values i.e. appendix describe compute examples presented section simplicity assume objects background model independent identically distributed following uniform law. presented tests cases rather crude. much tighter bounds found carefully tuning probabilistic models speciﬁc application. however simpler forms already useful demonstrating capabilities proposed framework suﬃcient lead state-of-the-art results. area bounding enclosing points longest line segment bounding length diagonal length bounding box. given line accept points compute probability random point lying band length width given δd/a value paatero tapper positive matrix factorization non-negative factor model optimal utilization error estimates data values environmetrics papalexakis sidiropoulos k-means higher-way coclustering multilinear decomposition sparse latent factors ieee trans. signal process. planti´e crampes survey social community detection social media retrieval ramzan zwol j.-s. clver x.-s. eds. computer communications networks springer london witten tibshirani hastie penalized matrix decomposition applications sparse principal components canonical correlation analysis biostatistics", "year": 2014}