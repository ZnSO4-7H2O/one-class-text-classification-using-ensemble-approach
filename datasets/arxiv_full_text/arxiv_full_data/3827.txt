{"title": "Query-Reduction Networks for Question Answering", "tag": ["cs.CL", "cs.NE"], "abstract": "In this paper, we study the problem of question answering when reasoning over multiple facts is required. We propose Query-Reduction Network (QRN), a variant of Recurrent Neural Network (RNN) that effectively handles both short-term (local) and long-term (global) sequential dependencies to reason over multiple facts. QRN considers the context sentences as a sequence of state-changing triggers, and reduces the original query to a more informed query as it observes each trigger (context sentence) through time. Our experiments show that QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and in a real goal-oriented dialog dataset. In addition, QRN formulation allows parallelization on RNN's time axis, saving an order of magnitude in time complexity for training and inference.", "text": "minjoon university washington seoul national university allen institute artiﬁcial intelligence {minjoon hannaneh}cs.washington.edu shmswsnu.ac.kr paper study problem question answering reasoning multiple facts required. propose query-reduction network variant recurrent neural network effectively handles short-term long-term sequential dependencies reason multiple facts. considers context sentences sequence state-changing triggers reduces original query informed query observes trigger time. experiments show produces state-of-the-art results babi dialog tasks real goal-oriented dialog dataset. addition formulation allows parallelization rnn’s time axis saving order magnitude time complexity training inference. paper address problem question answering reasoning multiple facts required. example consider know frogs insects flies insects. answering frogs flies? requires reasoning facts. question answering speciﬁcally context-based extensively studied machine comprehension tasks however datasets primarily focused lexical syntactic understanding hardly concentrate inference multiple facts. recently several datasets aimed testing multi-hop reasoning emerged; among story-based dialog task recurrent neural network variants long short-term memory gated recurrent unit popular choices modeling natural language. however used multi-hop reasoning question answering purely rnn-based models shown perform poorly largely fact rnn’s internal memory inherently unstable long term. reason recent approaches literature mainly relied global attention mechanism shared external memory attention mechanism allows models focus single sentence layer. sequentially read multiple relevant sentences memory multiple layers perform multi-hop reasoning. however major drawback standard attention mechanisms insensitive time step sentences accessing them. proposed model query-reduction network single recurrent unit addresses long-term dependency problem rnn-based models simplifying recurrent update taking advantage rnn’s capability model sequential data considers context sentences sequence state-changing triggers transforms original query informed query observes trigger time. instance figure original question apple? cannot directly answered single sentence story. observing ﬁrst sentence sandra apple there transforms original question reduced query sandra? presumably figure unit -layer -sentence story entire system story question predicted answer natural language respectively. corresponding vector representations update gate reduce functions respectively. assigned local query last time step last layer. also red-colored text inferred meanings vectors easier answer original question given context provided ﬁrst sentence. unlike rnn-based models qrn’s candidate state depend previous hidden state compared memory-based approaches better encodes locality information global memory access controller query updates performed locally. short main contribution threefold. first simple variant reduces query given context sentences differentiable manner. second situated attention mechanism effectively handling time dependency long-term dependency problems technique respectively. hence well-suited sequential data local global interactions third unlike rnn-based models parallelized time computing candidate reduced queries directly local input queries context sentence vectors fact parallelizability implies suffer vanishing gradient problem hence effectively addressing long-term dependency. experimentally demonstrate contributions achieving state-of-the-art results story-based interactive dialog datasets. story-based input context sequence sentences question natural language output predicted answer question natural language supervision provided training answer question. paper particularly focus end-to-end solutions i.e. supervision comes questions answers restrain using manually deﬁned rules external language resources lexicon dependency parser. denote sequence sentences number sentences story denote question. denote predicted answer denote true answer. proposed system end-to-end task divided three modules input module layers output module. layers. layers sentence vectors question vector input module obtain predicted answer vector space layer refers recurrent application unit considered variant inputs outputs hidden state operate vector space. details module explained throughout section ﬁrst formally deﬁne base model unit explain connect input output modules also present extensions network improve qrn’s performance finally show parallelized time giving computational advantage rnn-based models order magnitude rnn-based model single recurrent unit updates hidden state time layers. figure depicts schematic structure unit figure demonstrates layers stacked. unit accepts inputs outputs local query vector necessarily identical original query vector order compute outputs update gate function reduce function intuitively update gate function measures relevance sentence local query used update hidden state. reduce function transforms local query input candidate state reduced query given sentence. outputs calculated following equations scalar update gate candidate reduced query ﬁnal reduced query time step sigmoid activation tanh hyperboolic tangent activation rd×d weight matrices bias terms element-wise vector multiplication vector concatenation along row. base case explicitly deﬁned reasonable differentiable functions. update gate similar global attention mechanism measures similarity sentence query. however signiﬁcant difference update gate computed using sigmoid function current memory slot whereas global attention computed using softmax function entire memory update gate rather considered local sigmoid attention. stacking layers showed single-layer case multiple layers able perform reasoning multiple facts effectively shown example figure order stack several layers outputs current layer used inputs next layer. using superscript denote current layer’s index note passed next layer without modiﬁcation layer index bi-direction. assumed needs look past sentences whereas often times query answers depend future sentences. instance consider sentence john dropped football. time then even mention football past implied john football current time order incorporate future dependency obtain forward backward directions connecting input output modules. figure depicts connected input output modules. ﬁrst layer obtained input module processing natural language question input also obtained input module. output last time step last layer passed output module. represent number layers network. output module gives predicted answer natural language. reset gate. inspired found useful allow unit reset candidate reduced query necessary. reset gate function deﬁned similarly update gate function vector gates. lstm update reset gates vectors instead scalar values ﬁne-controlled gating. vector gates modify dimension weights biases equation obtain element-wise multiplied instead broadcasted equation important advantage recurrent updates equation computed parallel across time. contrast rnn-based models cannot parallelized computing candidate hidden state time explicitly requires previous hidden state. ﬁnal reduced queries decomposed computing candidate reduced queries without looking previous reduced query. primarily show query update equation parallelized rewriting equation matrix operations. extension equation straightforward. proof vector gates shown appendix recursive deﬁnition equation explicitly written figure schematics state-of-the-art models end-to-end memory networks improved dynamic memory networks simpliﬁed emphasize differences among models. agru variant update gate replaced soft attention proposed kumar dmn+ forward direction arrows shown. rt×t lower strictly lower triangular matrices respectively elementwise multiplication matrix tiled across column i.e. rt×t similarly rt×d. implicit operations matrix multiplications. reasonable matrix operations equation comfortably computed modern gpus. inspired rnn-based models gating mechanism lstm lstm previous hidden state current input obtain candidate hidden state uses current inputs obtain candidate reduced query conjecture gives computational advantage parallelization also makes training easier i.e. avoiding vanishing gradient overﬁtting converging local minima. idea structurally simplifying rnns learning longer-term patterns explored recent previous work structurally constrained recurrent network strongly-typed recurrent neural network similar strnn architectures gating mechanism gates candidate hidden states depend previous hidden states simpliﬁes recurrent relation. however distinguished strnn three ways. first qrn’s update gate simulates attention mechanism measuring relevance input sentence query. hand gates strnn considered simpliﬁcation lstm/gru removing dependency previous hidden state. second natively compatible context-based tasks unit accepts inputs i.e. context sentence query. distinct strnn input. third show timewise-parallelizable gpus. parallelization algorithm also applicable strnn. end-to-end memory network uses external memory multi-layer attention mechanism focus sentences relevant question. differences qrn. first summarizes entire memory layer control attention next layer instead controller node able focus relevant sentences update gate internally embodied within unit. second adds time-dependent trainable weights sentence representations model time dependency sentences need additional weights inherent architecture allows improved dynamic memory network uses hybrid attention mechanism architecture model sequence sentences. consists distinct grus time axis layer axis note update gate time axis replaced external softmax attention weights. dmn+ uses time-axis summarizes entire memory layer layer-axis controls attention weights layer. contrast simply single recurrent unit without controller node. babi story-based dataset babi story-based dataset composed different tasks synthetically-generated story-question pair. story short sentences long sentences. system evaluated accuracy getting correct answers questions. answers single words lists answering questions task requires selecting relevant sentences applying different kinds logical reasoning them. dataset also includes training data allows training complex models. note dmn+ reports dataset. babi dialog dataset babi dialog dataset consists different tasks synthetically-generated goal-oriented dialogs user system domain restaurant reservation. dialog long utterances comes external knowledge base providing information restaurant. authors also provide out-of-vocabulary version dataset many words keywords test data seen training. system evaluated accuracy response utterance user choosing possible candidate responses. system required understand user’s request also refer previous conversations order obtain context information current conversation. dstc dialog dataset bordes weston transformed second dialog state tracking challenge dataset format babi dialog dataset measurement performance real dataset. dialog long utterances system needs choose possible candidate responses utterance user. note evaluation metric original dstc different transformed dstc previous work original dstc directly compared work. refer transformed dstc dataset task dialog dataset. input module. input module given sentences question want obtain vector representations trainable embedding matrix rd×v encode one-hot vector word sentence d-dimensional vector sentence representation obtained position encoder encoder embedding matrix also used obtain question vector output module story-based output module given vector representation predicted answer want obtain natural language form answer -way single-layer softmax classiﬁer -dimensional sparse vector output module dialog. ﬁxed number single-layer softmax classiﬁers similar sotry-based model sequentially output word system’s response. similar spirit decoder output module recurrent hidden state gating mechanism. instead solely uses ﬁnal ouptut current word output inﬂuence prediction next word among possible candidates. training. withhold training development. hidden state size deafult. batch sizes babi story-based babi dialog dstc dialog babi used. weights input output modules initialized zero mean standard deviation weights unit initialized using techniques glorot bengio tied across layers. forget bias used update gates weight decay used weights. loss function cross entropy one-hot vector true answer. loss minimized stochastic gradient descent maximally epochs training early stopped loss development data decrease epochs. learning rate controlled adagrad initial learning rate since model sensitive weight initialization repeat training procedure times random initialization weights report result test data lowest loss development data. compare model baselines previous state-of-the-art models story-based dialog tasks include lstm end-to-end memory networks dynamic memory networks gated end-to-end memory networks differentiable neural computer story-based table reports summary results model previous work babi data qrn’s outperforms models large margin dataset average accuracy qrn’s model outperforms previous models large margin achieving nearly perfect score dialog. table reports summary results model previous work babi dialog task dialog done previous work also report results ‘match’ dialogs. ‘match’ extension model additionally takes input whether answer candidate matches context outperforms previous work large margin every comparison. ablations. test four types ablations number layers reset gate gate vectorization dimension hidden vector show subset combinations ablations babi table table combinations performed poorly and/or give interesting observations. according ablation results infer that number layers model lacks reasoning capability. case dataset many layers seems correctly training model becomes increasingly difﬁcult. case dataset many layers hidden dimensions helps reasoning notably difﬁcult task task adding reset gate helps. including vector gates hurts datasets model either overﬁts training data converges local minima. hand vector gates babi story-based dataset sometimes help. increasing dimension hidden state dialog’s task helps much improvement dialog’s task hypothesized larger hidden state required real data. table babi dataset number failed tasks average error rates obtained github.com/therne/dmn-tensorflow. babi dialog dstc dialog dataset average error rates previous work ﬁrst number indicates number layers means reset gate used last number exists indicates dimension hidden state default value indicates ‘match’ used. task-wise results shown appendices table table section details. task supporting facts sandra picked apple there. sandra dropped apple. daniel grabbed apple there. sandra travelled bathroom. daniel went hallway. apple? figure babi dataset visualization update reset gates model babi dialog dstc dialog dataset visualization update reset gates model. note stories many sentences; show part here. visualizations shown figure figure parallelization. implement without parallelization tensorflow single titan qunaitify computational gain parallelization. without parallelization library provided tensorflow. parallelization gives times faster training inference without parallelization average. expect speedup even higher datasets larger context. interpretations. advantage intermediate query updates interpretable. figure shows intermediate local queries interpreted natural language where sandra?. order obtain these place decoder input question embedding loss recovering question classiﬁcation loss decoder decode intermediate queries. helps understand information networks. figure question apple? transformed sandra? sandra dropped apple apple relevant sandra. obtain daniel? time propagated observe sentence used answer query. visualization. figure shows vizualization magnitudes update reset gates story sentences dialog utterances. visualizations shown appendices figure figure figure observe high values facts provide information answer question task example observe high update gate values ﬁrst layer facts state apple second layer high update gate values inform person went also observe signifying apple belongs sandra. dialog task model able infer three restaurants already recommended recommend another one. dialog task model focuses sentences containing spanish concentrate much facts don’t care. conclusion paper introduce query-reduction network answer context-based questions carry conversations users require multi-hop reasoning. show state-of-theart results three datasets story-based dialog. model story dialog sequence state-changing triggers compute ﬁnal answer question system’s next utterance recurrently updating query. situated attention mechanism effectively handling time dependency long-term dependency problems technique respectively. addresses long-term dependency problem rnns simplifying recurrent update candidate hidden state depend previous state. moreover parallelized address well-known problem rnn’s vanishing gradients. research supported allen institute allen distinguished investigator award google research faculty award samsung award. thank anonymous reviewers helpful comments. martın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. kyunghyun bart merriënboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. emnlp alex graves greg wayne malcolm reynolds harley danihelka agnieszka grabskabarwi´nska sergio gómez colmenarejo edward grefenstette tiago ramalho john agapiou hybrid computing using neural network dynamic external memory. nature ankit kumar ozan irsoy jonathan james bradbury robert english brian pierce peter ondruska ishaan gulrajani richard socher. anything dynamic memory networks natural language processing. icml single supporting fact supporting facts three supporting facts relations three relations yes/no questions counting lists/sets simple negation indeﬁnite knowledge basic coreference conjunction compound coreference time reasoning basic deduction basic induction positional reasoning size reasoning path ﬁnding agents motivations failed average error rates table babi dataset error rates previous work lstm end-to-end memory networks dynamic memory networks gated end-to-end memory networks results within task differentiable neural computer provided paper graves number front indicates number layers. number back indicates dimension hidden vector default value indicates reset gate used indicates gates vectorized. indicates joint training. issuing calls updating calls displaying options providing extra information conducting full dialogs average error rates issuing calls updating calls displaying options providing extra information conducting full dialogs average error rates dstc dialog table babi dialog dstc dialog dataset average error rates previous work end-to-end memory networks) gated end-to-end memory networks). number front indicates number layers number back indicates dimension hidden vector default value indicates reset gate used indicates gates vectorized indicates ‘match’ used. match. similar spirit ‘match’ model slightly different previous work answer candidate embedding matrix dimension matrix expresses whether answer candidate matches word paragraph question. words softmax computed task model focuses facts contain ’football’ ﬁrst layer found mary journeyed bathroom second layer. task model focuses facts provide information location sandra. figure visualization update reset gates model several tasks babi reset gate last layer. note show recent sentences here though stories many sentences. figure visualization update reset gates model several tasks babi dialog dstc dialog reset gate last layer. note show recent sentences here even dialog sentences. visualization dialog. figure shows visualization models dialog tasks. ﬁrst dialog task model focuses user utterance mentions user’s desired cuisine location current query informs system number people system able learn needs user desired price range. second dialog task model focuses facts provide information requests user. task model focuses restaurant user talking information restaurant.", "year": 2016}