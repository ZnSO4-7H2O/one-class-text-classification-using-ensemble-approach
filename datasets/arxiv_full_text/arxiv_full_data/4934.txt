{"title": "Particle Value Functions", "tag": ["cs.LG", "cs.AI"], "abstract": "The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.", "text": "chris maddison dieterich lawson george tucker nicolas heess arnaud doucet andriy mnih whye university oxford deepmind google brain cmaddisstats.ox.ac.uk policy gradients expected return objective react slowly rare rewards. cases agents wish emphasize high returns regardless probability. borrowing economics control literature review risk-sensitive value function arises exponential utility illustrate effects example. risk-sensitive value function always applicable reinforcement learning problems introduce particle value function deﬁned particle ﬁlter distributions agent’s experience bounds risk-sensitive one. illustrate beneﬁt policy gradients objective cliffworld. expected return objective dominates ﬁeld reinforcement learning makes difﬁcult express tolerance unlikely rewards. kind risk sensitivity desirable e.g. real-world settings ﬁnancial trading safety-critical applications risk required achieve speciﬁc return matters greatly. even ultimately care expected return beneﬁcial training tolerate high variance order discover high reward strategies. paper introduce risk-sensitive value function based system interacting trajectories called particle value function value function amenable large-scale reinforcement learning problems nonlinear function approximation. idea inspired recent advances variational inference bound marginal likelihood importance sampling estimators takes orthogonal approach reward modiﬁcations e.g. section review risk sensitivity simple decision problem risk consideration. section introduce particle value function. section highlight beneﬁts cliffworld trained policy gradients. look ﬁnite horizon markov decision process setting instantaneous reward generated agent following non-stationary policy appendix utility function invertible non-decreasing function speciﬁes ranking possible rt)|s speciﬁes ranking policies agent following natural deﬁnition value state real number note identity recover expected return. consider exponential utilities choice well-studied implied assumption additive deterministic translations reward function corresponding value function recover expected return. appendix details. interpret value following thought experiment. agent given choice single represents interaction environment immediate deterministic return minimum return agent would take exchange forgoing interaction. meaning agent willing take loss relative expected return exchange certainty. risk-avoiding attitude emphasizes returns. agent would forgo interaction expect receive. risk-seeking behavior emphasizes high returns. illustrate effect risk consider state shown figure agent begins state acts time steps choosing leaving remaining. suppose agent’s policy deﬁned single parameter describes probability remaining. local maxima expected return solution global maximum. policy gradient trajectory initialized converge suboptimal solution risk appetite grows basin attraction global maximum expands entire unit interval. sort state aliasing happens often reinforcement learning non-linear function approximation. cases modifying risk appetite favorably modify convergence policy gradient algorithms even ultimate objective expected return. risk-seeking variant helpful deterministic environments agent exactly reproduce previously experienced trajectory. rare rewards rare current policy better pursue high yield trajectories aggressively. note however non-decreasing general risk-seeking guaranteed improve expected return. note also literature regularized control gives different perspective risk sensitive control mirrors relationship variational inference maximum likelihood. appendix related work. suffer numerical issues high variance apalgorithms optimizing pendix instead deﬁne value function bounds approaches inﬁnite sample limit. call particle value function assigns value bootstrap particle ﬁlter particles representing state-action trajectories. distinct related kantas investigates particle ﬁlter algorithms inﬁnite horizon risk-sensitive control. brieﬂy bootstrap particle ﬁlter used estimate normalizing constants hidden markov model states transitions emissions given sample probability computed forward algorithm. bootstrap particle ﬁlter stochastic procedure forward algorithm avoids integrating state space latent variables. propagating particles resampling step proportion potentials desired probability insight treat state-action pairs latents emission potentials exp) bootstrap particle ﬁlter returns note sophisticated sampling schemes doucet johansen result distinct pvfs. consider value initialize particles jensen’s inequality unbiasedness estimator bound opposite direction. informative consider behaviour trajectories different values algorithm greedily prefers trajectories encounter large rewards aggregate return time step soft-max. algorithm prefers trajectories encounter large negative rewards aggregate return time step soft-min. appendix bellman equation policy gradient pvf. highlight beneﬁts using pvfs apply variant gridworld task called cliffworld appendix comparison methods details. trained time dependent tabular policies using policy gradients distinct pvfs {−−. tried learning rates case independent non-interacting trajectories averaged policy gradient estimated baselines. figure shows density ﬁnal state trained varying treatments notice higher risk parameter broader policy agent eventually solving task. corresponding standard reinforce runs solved task even increasing number agents introduced particle value function approximates risk-sensitive value function given mdp. seek address theoretical questions whether increasing monotonic number particles. also efﬁcient tabular representation understanding effect efﬁcient approximations would valuable. experimentally hope explore ideas complex sequential tasks non-linear function approximators. obvious example tasks variational inference sequential model. references francesca albertini wolfgang runggaldier. logarithmic transformations discrete-time ﬁnite-horizon stochastic control problems. applied mathematics optimization matt hoffman arnaud doucet nando freitas ajay jasra. solving general state-space sequential decision problems using inference algorithms. technical report technical report university british columbia computer science nikolas kantas arnaud doucet sumeetpal singh maciejowski nicolas chopin particle methods parameter estimation state-space models. statistical science sven koenig reid simmons. risk-sensitive planning probabilistic decision graphs. proceedings international conference principles knowledge representation reasoning ralph neuneier oliver mihatsch. risk sensitive reinforcement learning. proceedings international conference neural information processing systems press michael pitt ralph santos silva paolo giordani robert kohn. properties markov chain monte carlo simulation methods based particle ﬁlter. journal econometrics konrad rawlik marc toussaint sethu vijayakumar. approximate inference approach temporal optimization optimal control. advances neural information processing systems consider decision problems agent selects actions receives rewards stochastic environment. sake exposition consider ﬁnite horizon consists ﬁnite state space ﬁnite action space stationary environmental transition kernel satisfying markov property reward functions rt−t time step agent chooses actions according policy πt−t given current state. πt−t action distribution rt−t reward function steps remaining. together proceeds stochastically producing sequence random variables according following dynamics time steps. utility theory gives language describing relative importance high returns. utility function invertible non-decreasing function speciﬁes ranking rt)|s speciﬁes ranking policies expected utility necessarily interpretable scale afﬁne transformation utility function results relative ordering policies return outcomes. therefore deﬁne value associated utility returning scale rewards deﬁned mdp. agent following value state real number note identity recover expected return. course non-decreasing invertible utilities value gives ranking policies. interpret value following thought experiment. agent given choice single interaction environment immediate deterministic return represents minimum return agent would take exchange forgoing interaction. convex. case linear risk-neutral case. reasons known certain equivalent economics focus exponential utilities form broadly studied choice implied assumption value function additive deterministic translations return assumption nice preserves markov nature decision process agent given choice every time step continuing interaction terminating taking value deterministic return additivity value function means decision made regardless return accumulated value function corresponding exponential utility practical point view value function behaves like soft-max soft-min depending sign emphasizing avoidance returns pursuit high returns value approaches supremum returns trajectories positive probability best-case penalty. approaches inﬁmum worst-case value thus large positive value tolerant high variance lead high returns. large negative intolerant rare returns. despite attractive properties risk-sensitive value function always applicable reinforcement learning tasks value function satisﬁes multiplicative bellman equation operating log-space breaks ability exploit recurrence monte carlo returns generated single trajectory expectations exchange log. operating exp-space possible learning algorithms must minimum/maximum possible return exp) underﬂow/overﬂow. issue rewards represent probabilities often case variational inference. policy gradient even ignoring underﬂow/overﬂow issues reinforce style algorithms would difﬁculties deriving unbiased estimators ratio exp) single trajectories hard. lastly policy gradient risk sensitivity originates study utility choice economics extensively studied control mdps reinforcement learning risk sensitivity studied although none consider direct policy gradient approach considered work. methods considered variants learning approach policy iteration. well idea treating rewards emissions idea idea treating reinforcement learning inference problem idea broadly speaking without works still optimize expected reward objective regularization penalties policy. ones share closest connection risk sensitive objective ruiz kappen observation fully controllable transition dynamics optimizing policy completely speciﬁes transition dynamics achieves risk sensitive value note interesting connection bayesian inference. here plays role prior role variational posterior role variational lower bound role marginal likelihood. effect regularized control like variational inference risk sensitive control like maximum likelihood. finally environmental dynamics stochastic necessarily hold therefore risk sensitive value distinct case. certain special cases risk sensitive objectives also cast solutions path integral control problems knowledge work considered using particle ﬁlters risk sensitive control treating particle ﬁlter’s estimator partition function return whose expectation bounds risk sensitive value whose policy gradients cheap compute. also think value function expected return agent whose actions space product space environment state space whose transition kernel includes resampling dynamic. satisﬁes bellman equation point interacting trajectories generate monte carlo return ensures indeed consider particle value particle value function deﬁnes bound function corresponds initializing trajectories state deﬁne have jensen’s inequality figure left plot probability solving task standard deviation deﬁned achieving positive average return. right plot average reward training standard deviation. vimco trained learning rate averages runs. particles runs began solving cliffworld vimco ones did. agent state ends timesteps passed. actions available agent moving north east south west moving grid prohibited. environmental transitions deterministic. ‘cliff’ occupies states start goal along northern edge world. cliff states absorbing agent enters cliff state initially receives reward receives reward timestep thereafter. goal state also absorbing agent receives reward upon entering reward after. agent receives reward every action transition cliff goal state. optimal policy cliffworld cliff proceed start goal speedily possible could incur high variance reward agent falls cliff. uniform random policy trajectories result large negative rewards occasionally high positive reward. means initially independent trajectories venturing east high variance reward. trained non-stationary tabular policies parameterized parameters size policies trained using policy gradients distinct pvfs {−−. tried learning rates case independent non-interacting trajectories averaged policy gradient estimated baselines. used instead reinforce estimator simply estimated monte carlo returns. control variates used distinct baselines depending whether not. used baseline exponential moving average smoothing factor baselines also non-stationary dimensionality used baseline except vimco’s control variate immediate reward. vimco control variate applicable whole return future time steps correlated action interaction trajectories. temporal bellman equation. case though vimco policy gradients able solve cliffworld conditions policy gradients able solve. occasionally solved cliffworld vimco not. figure however regime vimco could solve task reliability variant. note case reinforce expected return solve variant.", "year": 2017}