{"title": "Markov random fields factorization with context-specific independences", "tag": ["cs.AI", "cs.LG"], "abstract": "Markov random fields provide a compact representation of joint probability distributions by representing its independence properties in an undirected graph. The well-known Hammersley-Clifford theorem uses these conditional independences to factorize a Gibbs distribution into a set of factors. However, an important issue of using a graph to represent independences is that it cannot encode some types of independence relations, such as the context-specific independences (CSIs). They are a particular case of conditional independences that is true only for a certain assignment of its conditioning set; in contrast to conditional independences that must hold for all its assignments. This work presents a method for factorizing a Markov random field according to CSIs present in a distribution, and formally guarantees that this factorization is correct. This is presented in our main contribution, the context-specific Hammersley-Clifford theorem, a generalization to CSIs of the Hammersley-Clifford theorem that applies for conditional independences.", "text": "markov random ﬁelds provide compact representation joint probability distributions representing independence properties undirected graph. well-known hammersley-cliﬀord theorem uses conditional independences factorize gibbs distribution factors. however important issue using graph represent independences cannot encode types independence relations context-speciﬁc independences particular case conditional independences true certain assignment conditioning set; contrast conditional independences must hold assignments. work presents method factorizing markov random ﬁeld according csis present distribution formally guarantees factorization correct. presented main contribution context-speciﬁc hammersley-cliﬀord theorem generalization csis hammersley-cliﬀord theorem applies conditional independences. markov random ﬁelds also known undirected graphical models markov networks belong family probabilistic graphical models well-known computational framework compact representation joint probability distributions. models composed independence structure numerical parameters. independence structure undirected graph encodes compactly conditional independences among variables domain. given structure numerical parameters quantify relationships structure. probability distributions present practice important complexity deﬁciencies exponential space complexity representation time complexity inference sample complexity learning data. based structure independences possible represent eﬃciently joint probability distribution factorizing smaller functions subset domain variables resulting times exponential reductions complexities. factorization done using well-known hammersley-cliﬀord theorem important issue using graph represent independences cannot encode types independence relations context-speciﬁc independences independences similiar conditional independences except true certain assignments conditioning set. csis applied wide range scenarios achieving signiﬁcant improvements time space sample complexities comparison approaches uses conditional independences encoded graph. contributions csis encoded alternative data structures carried assuming factors distribution conditional probability distributions. sense csis used factorize distribution used representing eﬃciently factors. main contribution work contextspeciﬁc hammersley-cliﬀord theorem. importance theoretical result lies allows factorize distribution using csis obtain sparse representation obtained independences providing theoreticonditional guarantees. this log-linear model used ﬁne-grained representation mrfs using models possible extend advantages hammersley-cliﬀord theorem improvements time space sample complexities. remainder work organized follows. next section provides summary related work literature. section presents overview factorize distribution exploiting independences. section formally describes contextspeciﬁc hammersley-cliﬀord theorem factorizes log-linear model according csis. paper concludes summary section literature learn log-linear models directly presenting diﬀerent procedures selecting features data. neither works discuss csis present guarantee log-linear model generated related underlying distribution. csis ﬁrst introduced coding locally within conditional probability tables bayesian networks decision trees. approach hybrid encoding conditional independencies directed graph csis decision trees variables conditional probability table. also work presents theoretical results sound graphical representation. work instead proposes uniﬁed representation csis conditional independencies log-linear model. such requires ﬁrst theoretical guarantees distribution factorizes according model remains future investigation eﬃcient graphical representation work closests work presenting algorithm factorizing loglinear model according csis. introduces statistical independence test eliciting independencies data. work assumes underlying distribution thin junction tree. although theoretical results presented guarantee eﬃcient computational performance results presented guarantee factorization proposed sound. section provides background mrfs explaining factorize distribution exploiting independences. start introducing necessary notation. capital letters sets indexes reserving letter domain distribution nodes graph. represent vector random variables. function returns values domain returns possible values variables complete assignment values denoted val|. finally denote value taken variables complete assignment conditional independences regularities distributions extensively studied ﬁeld statistics demonstrating eﬀectively soundly used reducing dimensionality distribution formally conditional independence deﬁned follows remark. alternative viewpoint deﬁnition obtained considering every triplet domain implicitly conditioned constant assignment external variable domain sense triplets dependency model become conditioned assignment sense probability distribution dependency model conditional independence assertion possible test truth value using equation work particularly interested dependency models graph-isomorph independences dependences represented undirected graph. formally undirected graph deﬁned nodes edges node associated random variable edge represents direct probabilistic inﬂuence necessary suﬃcient condition dependency models graph-isomorph independence assertions satisfy following independence axioms commonly called pearl axioms independence deﬁnition random variables pairwise disjoint sets variables contain assignment variables contextually independent given context denoted important property need later reconstruct graphs dependency models pairwise markov property asserts undirected graph built dependency model graph-isomorph follows every independence assertion contained dependency model holds said i-map similar fashion also i-map pairwise property necessary cases graph encode subset independences present distribution. distribution present additional type independences. work focus ﬁner-grained type independences context-speciﬁc independences independences similar conditional interestingly conditional independence assertion seen conjunction csis csis contexts conditioning conditional since csis deﬁned speciﬁc context cannot represented together single undirected graph instead captured dependency model extended csis using equation test validity every assertion call model context-speciﬁc dependency model every independence assertion contained holds said csi-map deﬁne formally context-speciﬁc dependency model follows uses undirected graph numerical parameters represent distribution. completely connected sub-graphs used factorize distribution potential functions cliques)} lower dimension parameterized following theorem shows factorize distribution distribution factorized theorem called gibbs distribution. n¨aive form contains potentials represented tables entry corresponds assignment associated numerical parameter. proof. assmuptions graph-isomorph i-map deﬁnition former implies exists undirected graph exactly encodes therefore must also imap assumptions hammersleycliﬀord theorem hold factorized potential functions cliques also since graph-isomorph conditional independences satisfy pearl axiom particular strong union axiom. therefore conditional independence conditional independence also using fact pairwise markov property imply no-edge words cannot belong clique. since hammersleycliﬀord holds last fact implies factor contain variables xci. corollary shows dependency model factorize distribution follows present theoretical results show context-speciﬁc dependency model used factorize general rationale decompose subsets csis contextualized certain context dependency models sub-domains decompose conditional distributions using hammersley-cliﬀord. deﬁnition distribution context subset context-speciﬁc dependency model deﬁne reduced dependency model domain rule pair disjoint subsets assignments assigns truth value triplet independence assertions follows despite clear beneﬁt factorization described hammersley cliﬀord theorem representation factor potential allow encode csis. patterns easily encoded convenient representation called log-linear. log-linear model represents gibbs distribution using features represent potentials. feature assignment subset variables domain. denote features make clear distinction features log-linear input assignment thus potential loglinear represented linear combination features follows kronecker delta function otheris wise. joining linear combinations potentials merging indexes unique index represent equation using following log-linear model next section present context-speciﬁc hammersley-cliﬀord theorem generalization hammersley cliﬀord theorem shows factorize distribution using context-speciﬁc dependency model captures csis. section presents main contribution work generalization hammersley-cliﬀord theorem factorizing distribution represented log-linear based context-speciﬁc dependency model csi-map this begin deﬁning following corollary hammersleycliﬀord theorem proof. start arguing csi-map extend proof show i-map conditional csi-map follows fact csi-map implies csis holds obtained conjoining csis values variables particular conjunction equation i-map follows fact equivalent conditional independence conditional auxiliary lemma positive distribution dependency model graph-isomorph dependency model i-map conditional conditional factorized potential functions {φi}i true factor contains proof. proof consists using corollary conditional distribution dependency model. that show satisfy requirements corollary positive graphisomorph dependency model domain i-map conditional. i-map conditional graph-isomorph follows assumptions. remains prove positivity conditional. that conditional expanded follows expansion denominator follows total probability. conditional expressed operation joints positive follows numerator denominator therefore whole quotient positive. lemma present main theoretical result theorem generalizes theorem factorize features log-linear according given context-speciﬁc dependency model this need deﬁne precisely mean factorization features steps deﬁnes factorization according dependency models contextualized case context-speciﬁc dependency models. deﬁnition features domain context-speciﬁc dependency model. features said factorize according factorize according reduced dependency model val. theorem positive distribution features log-linear context-speciﬁc dependency model reduced dependency models graph-isomorph. csi-map factorizes according proof. deﬁnition context-speciﬁc feature factorization conclusion theorem holds factorizes according reduced dependency model arbitrary reduced dependency model context prove factorizes according deﬁnition requires true s.t. holds either proceed then ﬁrst apply auxiliary lemma context reduced dependency model requirements satisﬁed positive graph-isomorph i-map conditional conclude consequent lemma i.e. conditional factorized potencial functions {φi}i s.t. true factor {φi}i holds either auxiliary lemma equivalent requirements factorization. clearly conclusions matches requirement factorization. show equivalence factor conditional equivalent factor joint composed features fci∪w whose values matches i.e. fci∪w theorem requires possible reduced dependency model graph-isomorph. implication requirement? deﬁnition graph-isomorphism implies possible context reduced dependency model encoded undirected graph subdomain provides mean construct graphically i.e. constructing undirected graph possible sub-domain assignment complement. practice done experts provide list csis hold domain running structure learning algorithm context. sound overly complex cleary exponential number contexts. doubt future works explore aspect ﬁnding alternatives simplifying complexity diﬀerent special cases. presented theoretical method factorizing markov random ﬁeld according csis present distribution formally guaranteed correct. presented context-speciﬁc hammersley-cliﬀord theorem generalization csis hammersley-cliﬀord theorem applies conditional independences. according theoretical result believe worth guiding future work implementing algorithms learning data structure mrfs possible context factorizing distribution using learned structures. intuitively seems likely achieve improvements time space sample complexities comparison approaches uses conditional independences encoded graph.", "year": 2013}