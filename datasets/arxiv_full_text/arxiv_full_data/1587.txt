{"title": "The Latent Relation Mapping Engine: Algorithm and Experiments", "tag": ["cs.CL", "cs.AI", "cs.LG", "H.3.1, I.2.6, I.2.7"], "abstract": "Many AI researchers and cognitive scientists have argued that analogy is the core of cognition. The most influential work on computational modeling of analogy-making is Structure Mapping Theory (SMT) and its implementation in the Structure Mapping Engine (SME). A limitation of SME is the requirement for complex hand-coded representations. We introduce the Latent Relation Mapping Engine (LRME), which combines ideas from SME and Latent Relational Analysis (LRA) in order to remove the requirement for hand-coded representations. LRME builds analogical mappings between lists of words, using a large corpus of raw text to automatically discover the semantic relations among the words. We evaluate LRME on a set of twenty analogical mapping problems, ten based on scientific analogies and ten based on common metaphors. LRME achieves human-level performance on the twenty problems. We compare LRME with a variety of alternative approaches and find that they are not able to reach the same level of performance.", "text": "many researchers cognitive scientists argued analogy core cognition. inﬂuential work computational modeling analogy-making structure mapping theory implementation structure mapping engine limitation requirement complex hand-coded representations. introduce latent relation mapping engine combines ideas latent relational analysis order remove requirement handcoded representations. lrme builds analogical mappings lists words using large corpus text automatically discover semantic relations among words. evaluate lrme twenty analogical mapping problems based scientiﬁc analogies based common metaphors. lrme achieves human-level performance twenty problems. compare lrme variety alternative approaches able reach level performance. faced problem recall similar problems faced past transfer knowledge past experience current problem. make analogy past situation current situation analogy transfer knowledge survey computational modeling analogy-making french cites structure mapping theory implementation structure mapping engine inﬂuential work modeling analogy-making. analogical mapping source target source familiar known concrete whereas target relatively unfamiliar unknown abstract. analogical mapping used transfer knowledge source target. gentner argues kinds similarity attributional similarity relational similarity. distinction attributes relations understood terms predicate logic. attribute predicate argument large meaning large. relation predicate arguments collides with meaning collides structure mapping engine prefers mappings based relational similarity mappings based attributional similarity example able build mapping representation solar system representation rutherford-bohr model atom mapped nucleus planets mapped electrons mass mapped charge. note mapping emphasizes relational similarity. nucleus diﬀerent terms attributes large nucleus small. likewise planets electrons little attributional similarity. hand planets revolve around like electrons revolve around nucleus. mass attracts mass planets like charge nucleus attracts charge electrons. gentner provides evidence children rely primarily attributional similarity mapping gradually switching relational similarity mature. uses terms mere appearance refer mapping based mostly attributional similarity analogy refer mapping based mostly relational similarity literal similarity refer mixture attributional relational similarity. since analogical mappings solve problems make predictions focus structure especially causal relations look beyond surface attributes things analogy solar system rutherford-bohr model atom illustrates importance going beyond mere appearance underlying structures. figures show lisp representations used input analogy solar system atom chalmers french hofstadter criticize sme’s requirement complex hand-coded representations. argue hard work done human creates high-level hand-coded representations rather sme. name mass-planet) name >mass) name attracts-form) name revolve) name and) name cause-revolve) name temp-sun) name temp-planet) name >temp) name force-gravity) name why-attracts))) gentner forbus colleagues attempted avoid hand-coding recent work sme. cogsketch system generate lisp representations simple sketches gizmo system generate lisp representations qualitative physics models learning reader system generate lisp representations natural language text systems require lisp input. however cogsketch user interface requires person draws sketch identify basic components sketch hand-label terms knowledge base derived opencyc. forbus note opencyc contains hand-coded concepts added hand-coded concepts opencyc order support cogsketch. gizmo system requires user hand-code physical model using methods qualitative physics learning reader uses phrasal patterns derived researchcyc evident still requires substantial hand-coded knowledge. work present paper eﬀort avoid complex hand-coded representations. approach combine ideas latent relational analysis call resulting algorithm latent relation mapping engine represent semantic relation terms using vector elements derived pattern frequencies large corpus text. semantic relations automatically derived corpus lrme require hand-coded representations relations. needs list terms source list terms target. given lists lrme uses corpus build representations relations among terms constructs mapping lists. tables show input output lrme analogy solar system rutherford-bohr model atom. although human eﬀort involved constructing input lists considerably less eﬀort requires input scientiﬁc analogies analogy solar system rutherfordbohr model atom seem esoteric believe analogy-making ubiquitous daily lives. potential practical application work task identifying semantic roles since roles relations attributes appropriate treat semantic role labeling analogical mapping problem. example judgement semantic frame contains semantic roles judge evaluee reason statement frame contains roles speaker addressee message topic medium task identifying training labeled sentences testing unlabeled sentences view task labeling testing sentences problem creating analogical mappings training sentences testing sentences table shows blames government failing enough help. might mapped they blame company polluting environment. mapping found transfer knowledge form semantic role labels source target. evaluate lrme created twenty analogical mapping problems science analogy problems common metaphor problems table science analogy problems. intended solution given table validate intended solutions gave colleagues lists terms asked generate mappings lists. section presents results experiment. across twenty problems average agreement intended solutions lrme algorithm outlined section along evaluation twenty mapping problems. lrme achieves accuracy diﬀerence performance human average statistically signiﬁcant. section examines variety alternative approaches analogy mapping task. best approach achieves accuracy approach requires hand-coded partof-speech tags. performance signiﬁcantly lrme human performance. section discuss questions raised results preceding sections. related work described section future work limitations considered section conclude section section list assumptions guided design lrme. results present paper necessarily require assumptions might helpful reader understand reasoning behind approach. analogies semantic relations analogies based semantic relations example analogy solar system rutherford-bohr model atom based similarity semantic relations among concepts involved understanding solar system semantic relations among concepts involved rutherford-bohr model atom. co-occurrences semantic relations terms interesting significant semantic relation tend co-occur within relatively small window relatively large corpus interesting semantic relation causes co-occurrence co-occurrence reliable indicator interesting semantic relation meanings semantic relations meaning relations among words individual words. individual words tend ambiguous polysemous. putting words pair constrain possible meanings. putting words sentence multiple relations among words sentence constrain possible meanings further. focus word pairs instead individual words word sense disambiguation less problematic. perhaps word sense apart relations words pattern distributions semantic relations many-to-many mapping semantic relations patterns terms co-occur. example relation causeeﬀect expressed causes likewise pattern expression causeeﬀect originentity however given statistical distribution patterns co-occur reliable signature semantic relations paper examine algorithms generate analogical mappings. simplicity restrict task generating bijective mappings; mappings injective surjective assume entities mapped given input. formally input algorithms sets terms explore basic kinds algorithms generating analogical mappings algorithms based attributional similarity algorithms based relational similarity attributional similarity words sima depends degree correspondence properties correspondence greater attributional similarity. relational similarity pairs words simr depends degree correspondence relations correspondence greater relational similarity. example wolf relatively high degree attributional similarity whereas bark meow relatively high degree relational similarity. recall gentner’s terms discussed section mere appearance analogy literal similarity take abstract model mapping based analogy model mere appearance. literal similarity combine take care normalize scorer scorea combine them. designed evaluate proportional analogies. proportional analogies form means example mason stone carpenter wood means mason stone carpenter wood. mason artisan works stone carpenter artisan works wood. turney describes potential applications recognizing proportional analogies structure mapping theory modeling metaphor classifying semantic relations word sense disambiguation information extraction question answering automatic thesaurus generation information retrieval identifying semantic roles. applications experimentally evaluated state-of-the-art results. turney compares performance relational similarity attributional similarity task solving multiple-choice proportional analogy questions college entrance test. used measure relational similarity variety lexicon-based corpus-based algorithms used measure attributional similarity. achieves accuracy questions signiﬁcantly diﬀerent average human score hand best performance attributional similarity results show attributional similarity better random guessing good relational similarity. result consistent gentner’s theory maturation human similarity judgments. turney also applies task classifying semantic relations nounmodiﬁer expressions. noun-modiﬁer expression phrase laser printer head noun preceded modiﬁer task identify semantic relation noun modiﬁer. case relation instrument; laser instrument used printer. hand-labeled noun-modiﬁer pairs diﬀerent classes semantic relations attains accuracy. turney employs variation solving four diﬀerent language tests achieving accuracy analogy questions accuracy toefl synonym questions accuracy task distinguishing synonyms antonyms accuracy task distinguishing words similar words associated words similar associated. core algorithm used four tests tuning parameters particular test. section evaluate lrme science analogies common metaphors supports claim applications beneﬁt ability handle larger sets terms. section identifying semantic roles also involves terms believe lrme superior semantic role labeling. semantic relation classiﬁcation usually assumes relations binary; semantic relation connection terms yuret observed binary relations linked underlying n-ary relations. example nastase szpakowicz deﬁned taxonomy binary semantic relations. table shows binary relations nastase szpakowicz covered -ary relation agenttoolactionaﬀectedtheme. agent uses tool perform action. somebody something aﬀected action. whole event summarized theme. semeval task found easier manually datasets expanded binary relations underlying n-ary relations believe expansion would also facilitate automatic classiﬁcation semantic relations. results section suggest applications discussed section might beneﬁt able handle bijective analogies evaluate algorithms analogical mapping created twenty mapping problems given appendix twenty problems consist science analogy problems based examples analogy science chapter holyoak thagard common metaphor problems derived lakoﬀ johnson tables appendix show intended mappings twenty problems. validate mappings invited colleagues institute information technology participate experiment. experiment hosted server people participated anonymously using browsers oﬃces. volunteers began experiment went end. analysis data participants completed mapping problems. instructions participants appendix sequence problems order terms within problem randomized separately participant remove eﬀects order. table shows agreement intended mapping mappings generated participants. across twenty problems average agreement higher agreement ﬁgures many linguistic annotation tasks. agreement impressive given participants minimal instructions training. solar system atom water heat transfer waves sounds combustion respiration sound light projectile planet artiﬁcial selection natural selection billiard balls molecules computer mind slot machine bacterial mutation argument buying item accepting belief grounds building reasons theory impediments travel diﬃculties money time seeds ideas machine mind object idea following understanding seeing understanding column labeled gives number terms source terms mapping problem average problem third column table gives mnemonic summarizes mapping note mnemonic used input algorithms mnemonic shown participants experiment. agreement ﬁgures table individual mapping problem averages mappings problem. appendix gives detailed view showing agreement individual mapping mappings. twenty problems contain total individual mappings appendix shows every select mapping chosen majority participants perfect score twenty problems. precisely mappings problem select mapping maximizes number participants agree individual mapping mappings score twenty problems. strong support intended mappings given appendix section applied genter’s categories mere appearance analogy literal similarity mappings best mapping according simr best mapping according sima. twenty mapping problems chosen analogy problems; intended mappings appendix meant relational mappings mappings maximize relational similarity simr. tried avoid mere appearance literal similarity. section twenty mapping problems evaluate relational mapping algorithm section evaluate several diﬀerent attributional mapping algorithms. hypothesis lrme perform signiﬁcantly better attributional mapping algorithms twenty mapping problems analogy problems expect relational attributional mapping algorithms would perform approximately equally well literal similarity problems expect mere appearance problems would favour attributional algorithms relational algorithms test latter hypotheses primary interest paper analogy-making. goal test hypothesis real practical eﬀective measurable diﬀerence output lrme output various attributional mapping algorithms. skeptic might claim relational similarity simr reduced attributional similarity sima sima; therefore relational mapping algorithm complicated solution illusory problem. slightly less skeptical claim relational similarity versus attributional similarity valid distinction cognitive psychology relational mapping algorithm capture distinction. test hypothesis refute skeptical claims created twenty analogical mapping problems show lrme handles problems signiﬁcantly better various attributional mapping algorithms. brieﬂy idea lrme build pair-pattern matrix rows correspond pairs terms columns correspond patterns. example might correspond pair terms solar system column might correspond pattern centered patterns wild card match single word. value element based frequency pattern instantiated terms pair example take pattern centered instantiate pair solar system pattern centered solar system thus value element based frequency centered solar system corpus. matrix smoothed truncated singular value decomposition relational similarity simr pairs terms given cosine angle corresponding vectors ﬁrst step make list contains pairs terms input mapping problem pairs members pairs members pairs pairs typical pair would solar system. allow duplicates list pair types pair tokens. twenty mapping problems list pairs. pair make list phrases corpus contain pair terms pair search corpus phrases following form here need pairs orders. want calculate simr order pairs always less however ensure simr symmetrical need make matrix symmetrical rows matrix orders every pair. next make list patterns based phrases found. pair found phrase replace replace remaining words either left replaced wild card symbol replace replace remaining words wild cards leave are. remaining words replaced generate patterns patterns patterns list pattern types pattern tokens; duplicates example pair solar system found phrase centered solar system illustrates. replace centered illustrates. three remaining words generate eight patterns illustrates centered illustrates patterns added replace yielding centered illustrates. gives another eight patterns centered thus phrase centered solar system illustrates generates total sixteen patterns revise make list pairs correspond rows frequency matrix remove pairs phrases found corpus terms either order. terms pair remove empty. remove rows would correspond zero vectors matrix reduces pairs pairs. number pairs next revise make list patterns correspond columns frequency matrix following experiments stage contains millions patterns many eﬃcient processing standard desktop computer. need reduce manageable size. select patterns shared pairs. pattern pair phrase pattern generated identical pairs generated sort patterns descending order number pairs generated pattern select patterns sorted list. following turney parameter hence reduced patterns number patterns rows columns deﬁned build frequency matrix i-th pair terms j-th pattern instantiate pattern terms element frequency instantiated pattern corpus. note need search corpus instantiated pattern order frequency. process creating pattern keep track many phrases generated pattern pair. frequency checking record patterns generated next step transform matrix frequencies form enhances similarity measurement. turney used entropy transformation suggested landauer dumais kind tf-idf transformation gives weight elements matrix statistically surprising. however bullinaria levy recently achieved good results transformation called ppmic therefore lrme uses ppmic. frequencies used calculate probabilities calculate pointwise mutual information element matrix. element negative zero. i-th pair terms j-th pattern estimated probability pattern instantiated pair estimated probability estimated probability statistically independent pi∗p∗j thus pmiij zero interesting semantic relation terms pattern captures aspect semantic relation expect larger would indepedent; hence pi∗p∗j thus pmiij positive. hand terms completely diﬀerent domains avoid other case pmiij negative. ppmic designed give high value pattern captures aspect semantic relation terms otherwise value zero indicating pattern tells nothing semantic relation terms smooth applying truncated singular value decomposition svdlibc calculate svdlibc designed sparse matrices. decomposes product three matrices uσvt column orthonormal form diagonal matrix singular values rank also rank diagonal matrix formed singular values matrices produced selecting corresponding columns matrix ukσkvt matrix rank best approximates original matrix sense minimizes approximation errors. ukσkvt minimizes matrices rank denotes frobenius norm think matrix ukσkvt smoothed compressed version original matrix following turney parameter relational similarity simr pairs inner product corresponding rows ukσkvt rows normalized unit length. simplify calculations dropping take matrix ukσk normalize unit length. resulting matrix. square matrix size ×nr. matrix contains cosines combinations pairs mapping problem pair terms pair terms suppose i-th j-th pairs simr element i-th j-th column either empty similarity zero. finally mapping problem output maximizes relational similarities. simpliﬁed form used calculate simr diﬀers used turney several ways. lrme synonyms generate alternate forms pairs terms. lrme morphological processing terms. lrme uses ppmic process frequencies instead entropy. following turney lrme uses slightly diﬀerent search template lrme sets number columns instead using constant. section evaluate impact changes tested changes mainly motivated desire increased eﬃciency simplicity. following experiments dual core opteron computer running linux. running time spent searching corpus phrases. took hours minutes wumpus fetch phrases. remaining steps took minutes took minutes. running time could half using raid speed disk access. table shows performance lrme baseline conﬁguration. comparison agreement volunteers intended mapping copied table diﬀerence performance lrme human participants statistically signiﬁcant solar system atom water heat transfer waves sounds combustion respiration sound light projectile planet artiﬁcial selection natural selection billiard balls molecules computer mind slot machine bacterial mutation argument buying item accepting belief grounds building reasons theory impediments travel diﬃculties money time seeds ideas machine mind object idea following understanding seeing understanding table column labeled humans average people whereas lrme column algorithm comparing average several scores individual score give misleading impression. results individual person typically several scores scores range. average mapping problem seven terms. possible exactly term mapped incorrectly; incorrect mappings must incorrect mappings. follows nature bijections. therefore score uncommon. table looks results another perspective. column labeled lrme wrong gives number incorrect mappings made lrme twenty problems. columns labeled number people wrong show various values people made incorrect mappings. average mapping problem participants perfect score remaining participants made mistakes table shows clearly table lrme’s performance signiﬁcantly diﬀerent human performance. table examine sensitivity lrme parameter settings. ﬁrst shows accuracy baseline conﬁguration table next eight rows show impact varying dimensionality truncated singular value decomposition eight rows show eﬀect varying column factor number columns matrix given number rows multiplied second last shows eﬀect eliminating singular value decomposition lrme. equivalent setting number rows matrix. ﬁnal gives result ppmic replaced entropy lrme sensitive manipulations none variations table perform signiﬁcantly diﬀerently baseline conﬁguration following experiments test lexicon-based attributional similarity measures wordnet implemented perl package wordnetsimilarity builds wordnetquerydata package. core idea behind treat wordnet graph measure semantic distance terms length shortest path graph. similarity increases distance decreases. wordnet developed team princeton available http//wordnet.princeton.edu/. pedersen’s wordnetsimilarity package http//www.d.umn.edu/∼tpederse/similarity.html. jason rennie’s wordnetquerydata package http//people.csail.mit.edu/jrennie/wordnet/. works nouns verbs adjectives adverbs work nouns verbs. used wordnetsimilarity possible parts speech possible senses input word. many adjectives true valuable also noun verb senses wordnet still able calculate similarity them. form word found wordnet wordnetsimilarity searches morphological variations word. multiple similarity scores multiple parts speech multiple senses select highest similarity score. similarity score word wordnet could alternative noun verb form adjective adverb score zero. also evaluate corpus-based attributional similarity measures pmi-ir core idea behind word characterized company keeps similarity terms measured similarity statistical distributions corpus. used corpus section along wumpus implement pmi-ir used online demonstration. selected matrix comparison option general reading year college topic space term-to-term comparison type. pmi-ir work parts speech. eighth similarity measure based observation intended mappings terms part speech partof-speech assigned term part-of-speech tags deﬁne measure attributional similarity simpos follows. hand-labeled terms mapping problems part-of-speech tags automatic taggers assume words tagged embedded sentence terms mapping problems sentences tags ambiguous. used knowledge intended mappings manually disambiguate part-of-speech tags terms thus guaranteeing corresponding terms intended mapping always tags. ﬁrst seven attributional similarity measures above created seven similarity measures combining simpos. example simhso hirst st-onge similarity measure. combine simpos simhso simply adding them. values returned simpos range whereas values returned simhso much smaller. chose large values getting tags match weight similarity measures. manual tags table presents accuracy various measures attributional similarity. best result without labels best result labels accuracy lrme signiﬁcantly higher accuracy lin+pos average human performance also signiﬁcantly higher accuracy lin+pos summary humans lrme perform signiﬁcantly better variations attributional mapping approaches tested. reference hirst st-onge jiang conrath leacock chodrow resnik turney landauer dumais santorini hirst st-onge jiang conrath leacock chodrow resnik turney landauer dumais section examine three questions suggested preceding results. diﬀerence science analogy problems common metaphor problems? advantage combining relational attributional mapping approaches? advantage relational mapping approach attributional mapping approach? table suggests science analogies diﬃcult common metaphors. supported table shows agreement participants intended mapping varies science problems metaphor problems. science problems lower average performance greater variation performance. diﬀerence science problems metaphor problems statistically signiﬁcant average science problem terms average metaphor problem might contribute diﬃculty science problems. however table shows clear relation number terms problem level agreement. believe people metaphor problems easier science problems common metaphors entrenched language whereas science analogies peripheral. table shows algorithms studied perform slightly worse science problems metaphor problems diﬀerence statistically signiﬁcant hypothesize attributional mapping approaches performing well enough sensitive subtle diﬀerences science analogies common metaphors. science metaphor problems. table marked bold font cases human scores greater lrme’s scores. problems cases; science problems cases; metaphor problems cases. evidence lrme’s performance signiﬁcantly diﬀerent human performance. lrme near middle range performance human participants. combine scores simply adding multiplying them scorer scorea quite diﬀerent scales distributions values; therefore ﬁrst normalize probabilities. probability estimates assume scorer scorea necessary constant value added scores ensure negative. combine scores adding multiplying probabilities. table shows accuracy lrme combined lin+pos adding multiplying probabilities. lrme accuracy combining lrme lin+pos increases accuracy improvement statistically signiﬁcant combining lrme results decrease accuracy. decrease signiﬁcant probabilities multiplied signiﬁcant probabilities added summary experiments show signiﬁcant advantage combining lrme attributional mapping. however possible larger sample problems would show signiﬁcant advantage. also combination methods explored elementary. sophisticated approach weighted combination perform better. hand relational similarity impact swap limited way. change part mapping aﬀects whole score. kind global coherence relational similarity lacking attributional similarity. testing hypothesis lrme beneﬁts coherence somewhat complicated need design experiment coherence eﬀect isolated eﬀects. this move terms outside accuracy calculation. suppose calculate accuracy methods based subproblem b′i. ﬁrst might seem advantage total coherence must explore larger space possible mappings internal coherence larger additional terms explores involved calculating accuracy. however hypothesize total coherence higher accuracy internal coherence additional external relations help select correct mapping. test hypothesis randomly generated reduced mapping problems twenty problems average accuracy internal coherence whereas average accuracy total coherence diﬀerence statistically signiﬁcant hand attributional mapping approaches cannot beneﬁt total coherence connection attributes attributes outside. decompose scorea independent parts. relational mapping cannot decomposed independent parts relations connect parts. gives relational mapping approaches inherent advantage attributional mapping approaches. conﬁrm analysis compared internal total coherence using lin+pos problems size average accuracy internal coherence whereas average accuracy total coherence diﬀerence statistically signiﬁcant beneﬁt coherence suggests make analogy mapping problems easier lrme adding terms. diﬃculty terms cannot randomly chosen; must logic analogy overlap existing terms. course important diﬀerence relational attributional mapping approaches. believe important diﬀerence relations reliable general attributes using past experiences make predictions future unfortunately hypothesis diﬃcult evaluate experimentally hypothesis coherence. french categorizes analogy-making systems symbolic connectionist symbolicconnectionist hybrids. g¨ardenfors proposes another category representational systems cognitive science calls conceptual spaces. spatial geometric systems common information retrieval machine learning inﬂuential example latent semantic analysis ﬁrst spatial approaches analogy-making began appear around time french’s survey. lrme takes spatial approach analogy-making. computational approaches analogy-making date back analogy argus systems designed solve proportional analogies analogy could solve proportional analogies simple geometric ﬁgures argus could solve simple word analogies. systems used hand-coded rules able solve limited range problems designers anticipated coded rules. unquestionably inﬂuential work date modeling analogy-making applied wide range contexts ranging child development folk physics. explicitly shifts emphasis analogymaking structural similarity source target domains. major principles underlie lrme follows principles. lrme uses relational similarity; attributional similarity involved coherent systems relations preferred mappings individual relations however spatial approach lrme quite diﬀerent symbolic approach sme. martin uses symbolic approach handle conventional metaphors. gentner bowdle wolﬀ boronat argue novel metaphors processed analogies conventional metaphors recalled memory without special processing. however line conventional novel metaphor unclear. dolan describes algorithm extract conventional metaphors dictionary. semantic parser used extract semantic relations longman dictionary contemporary english symbolic algorithm ﬁnds metaphorical relations words using extracted relations. veale developed symbolic approach analogy-making using wordnet lexical resource. using spreading activation algorithm achieved score multiple-choice lexical proportional analogy questions college entrance test lepage demonstrated symbolic approach proportional analogies used morphology processing. lepage denoual apply similar approach machine translation. connectionist approaches analogy-making include acme lisa like symbolic approaches systems handcoded knowledge representations search mappings takes connectionist approach nodes weights incrementally updated time system reaches stable state. third family examined french hybrid approaches containing elements symbolic connectionist approaches. examples include copycat tabletop much work fluid analogies research group concerns symbolic-connectionist hybrids marx dagan buhmann shamir present coupled clustering algorithm uses feature vector representation analogies collections text. example given documents buddhism christianity ﬁnds related terms {school mahayana zen} buddhism {tradition catholic protestant} christianity. mason describes cormet system extracting conventional metaphors text. cormet based clustering feature vectors represent selectional preferences verbs. given keywords source domain laboratory target domain ﬁnance able discover mappings liquid income container institution. turney littman bigham shnayder present system solving lexical proportional analogy questions college entrance test combines thirteen diﬀerent modules. twelve modules either attributional similarity symbolic approach relational similarity module uses spatial approach measuring relational similarity. module worked much better modules; therefore studied detail turney littman relation pair words represented vector elements pattern frequencies. similar lrme important diﬀerence turney littman used ﬁxed hand-coded patterns whereas lrme automatically generates variable number patterns given corpus turney introduced latent relational analysis examined thoroughly turney achieves human-level performance multiple-choice proportional analogy questions college entrance exam. lrme uses simpliﬁed form lra. similar simpliﬁcation used turney system processing analogies synonyms antonyms associations. contribution lrme beyond proportional analogies larger systems analogical mappings. lakoﬀ johnson provide extensive evidence metaphor ubiquitous language thought. believe system analogy-making able handle metaphorical language analogy problems derived lakoﬀ johnson agree claim metaphor merely involve superﬁcial relation couple words; rather involves systematic mappings domains. thus analogy problems involve larger sets words beyond proportional analogies. holyoak thagard argue analogy-making central daily thought especially ﬁnding creative solutions problems. scientiﬁc analogies derived examples analogy-making scientiﬁc creativity. section mentioned applications section claimed results experiments section suggest lrme perform better applications ability handle bijective analogies focus future work testing hypothesis. particular task semantic role labeling discussed section seems good candidate application lrme. input lrme simpler input still human eﬀort involved creating input. lrme immune criticism chalmers french hofstadter human generates input work computer makes mappings although trivial matter right mapping choices. future work would like relax requirement must bijection adding irrelevant words synonyms. mapping algorithm forced decide terms include mapping terms leave out. would also like develop algorithm take proportional analogy input automatically expand larger analogy would automatically search corpus terms analogy. next step would give computer topic source domain topic target domain work rest own. might possible combining ideas lrme ideas coupled clustering cormet seems analogy-making triggered people encounter problem problem deﬁnes target immediately start searching source. analogical mapping enables transfer knowledge source target hopefully leading solution problem. suggests input ideal analogical mapping algorithm would simply statement algorithms considered perform exhaustive search possible mappings acceptable sets small here problematic larger problems. future work necessary heuristic search algorithms instead exhaustive search. takes almost hours lrme process twenty mapping problems better hardware changes software time could signiﬁcantly reduced. even greater speed algorithm could continuously building large database vector representations term pairs ready create mappings soon user requests them. similar vision banko etzioni lrme like uses truncated singular value decomposition smooth matrix. many algorithms proposed smoothing matrices. past work experimented nonnegative matrix factorization probabilistic latent semantic analysis iterative scaling kernel principal components analysis interesting results small matrices none algorithms seemed substantially better truncated none scaled matrix sizes however believe unique future work likely discover smoothing algorithm eﬃcient eﬀective svd. results section show signiﬁcant beneﬁt svd. table hints ppmic important svd. section noted found average phrases pair. information phrases combined vector represent semantic relation pair. quite diﬀerent relation extraction automatic content extraction evaluation. task identify label semantic relation single sentence. semantic role labeling also involves labeling single sentence contrast lrme analogous distinction cognitive psychology semantic episodic memory. episodic memory memory speciﬁc event one’s personal past whereas semantic memory memory basic facts concepts unrelated speciﬁc event past. lrme extracts relational information independent speciﬁc sentence like semantic memory. concerned extracting relation speciﬁc sentence like episodic memory. cognition episodic memory semantic memory work together synergistically. experience event semantic memory interpret event form episodic memory semantic memory constructed past experiences accumulated episodic memories. suggests synergy combining lrme-like semantic information extraction algorithms ace-like episodic information extraction algorithms. analogy core cognition. understand present analogy past. predict future analogy past present. solve problems searching analogous situations daily language saturated metaphor metaphor based analogy understand human language solve human problems work humans computers must able make analogical mappings. best theory analogy-making structure mapping theory structure mapping engine puts much burden analogy-making human users lrme attempt shift burden onto computer remaining consistent general principles smt. shown lrme able solve bijective analogical mapping problems human-level performance. attributional mapping algorithms able reach level. supports claims relations important attributes making analogical mappings. still much research done. lrme takes load human user formulating input lrme easy. paper incremental step towards future computers make surprising useful analogies minimal human assistance. thanks colleagues institute information technology participating experiment section thanks charles clarke egidio terra corpus. thanks stefan b¨uttcher making wumpus available giving advice use. thanks doug rohde making svdlibc available. thanks wordnet team princeton university wordnet pedersen wordnetsimilarity perl package jason rennie wordnetquerydata perl package. thanks team university colorado boulder online demonstration lsa. thanks deniz yuret andr´e vellino dedre gentner vivi nastase yves lepage diarmuid s´eaghdha roxana girju chris drummond howard johnson stan szpakowicz anonymous reviewers jair helpful comments suggestions. appendix provide detailed information twenty mapping problems. figure shows instructions given participants experiment section instructions displayed browsers. tables show twenty mapping problems. ﬁrst column gives problem number mnemonic summarizes mapping second column gives source terms third column gives target terms. mapping expresses analogy horse car. horse’s legs like car’s wheels. horse eats consumes gasoline. horse’s brain controls movement horse like car’s driver controls movement car. horse generates dung waste product like generates exhaust waste product. instructions unclear please continue exercise. answers twenty problems used standard evaluating output computer algorithm; therefore proceed conﬁdent understand task. source solar system planet mass attracts revolves gravity average agreement water ﬂows pressure water tower bucket ﬁlling emptying hydrodynamics average agreement waves shore reﬂects water breakwater rough calm crashing average agreement combustion fuel burning intense oxygen carbon dioxide average agreement sound high echoes loud quiet horn average agreement source projectile trajectory earth parabolic gravity attracts average agreement breeds selection conformance artiﬁcial popularity breeding domesticated average agreement balls billiards speed table bouncing moving slow fast average agreement computer processing erasing write read memory outputs inputs average agreement slot machines reels spinning winning losing average agreement source soldier destroy ﬁghting defeat attacks weapon average agreement buyer merchandise buying selling returning valuable worthless average agreement foundations buildings supporting grounds building solid reasons theory weak crack average agreement obstructions destination route traveller travelling companion arriving average agreement money allocate budget eﬀective cheap expensive average agreement source seeds planted fruitful fruit grow wither blossom average agreement machine working turned turned broken power repair average agreement object hold weigh heavy light average agreement follow leader path follower lost wanders twisted straight average agreement seeing light illuminating darkness view hidden average agreement problem participants mapped gravity electromagnetism. ﬁnal column gives part-of-speech tags source target terms. used penn treebank tags assigned tags manually. intended mappings tags chosen mapped terms tags. example maps nucleus nucleus tagged tags used experiments section tags used lrme shown participants experiment section", "year": 2008}