{"title": "Sequence Transduction with Recurrent Neural Networks", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Many machine learning tasks can be expressed as the transformation---or \\emph{transduction}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since \\emph{finding} the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus.", "text": "example transforming audio signals sequences words requires ability identify speech sounds despite apparent distortions created diﬀerent voices variable speaking rates background noise etc. language model used inject prior knowledge output sequences must also robust missing words mispronunciations non-lexical utterances etc. recurrent neural networks promising architecture general-purpose sequence transduction. combination high-dimensional multivariate internal state nonlinear state-to-state dynamics oﬀers expressive power conventional sequential algorithms hidden markov models. particular rnns better storing accessing information long periods time. early years rnns dogged diﬃculties learning recent results shown capable delivering state-of-the-art results real-world tasks handwriting recognition text generation language modelling furthermore results demonstrate long-range memory perform actions closing parentheses many intervening characters using delayed strokes identify handwritten characters trajectories however rnns usually restricted problems alignment input output sequence known advance. example rnns used classify every frame speech signal every amino acid protein chain. network outputs probabilistic leads distribution output sequences length input sequence. general-purpose sequence transducer output length unknown advance would prefer distribution sequences lengths. furthermore since inputs outputs aligned distribution would many machine learning tasks expressed transformation—or transduction—of input sequences output sequences speech recognition machine translation protein secondary structure prediction text-to-speech name few. challenges sequence transduction learning represent input output sequences invariant sequential distortions shrinking stretching translating. recurrent neural networks powerful sequence learning architecture proven capable learning representations. however rnns traditionally require pre-deﬁned alignment input output sequences perform transduction. severe limitation since ﬁnding alignment diﬃcult aspect many sequence transduction problems. indeed even determining length output sequence often challenging. paper introduces end-to-end probabilistic sequence transduction system based entirely rnns principle able transform input sequence ﬁnite discrete output sequence. experimental results phoneme recognition provided timit speech corpus. ability transform manipulate sequences crucial part human intelligence everything know world reaches form sensory sequences everything interact world requires sequences actions thoughts. creation automatic sequence transducers therefore seems important step towards artiﬁcial intelligence. major problem faced systems represent sequential information invariant least robust sequential distorconnectionist temporal classiﬁcation output layer deﬁnes distribution alignments output sequences longer input sequence however well precluding tasks text-to-speech output sequence longer input sequence model interdependencies outputs. transducer described paper extends deﬁning distribution output sequences lengths jointly modelling input-output output-output dependencies. discriminative sequential model transducer similarities ‘chain-graph’ conditional random ﬁelds however transducer’s construction rnns ability extract features data potentially unbounded range dependency marked contrast pairwise output potentials handcrafted input features typically used crfs. closer spirit graph transformer network paradigm diﬀerentiable modules globally trained perform consecutive graph transformations detection segmentation recognition. section deﬁnes transducer showing trained applied test data section presents experimental results timit speech corpus concluding remarks directions future work given section length input sequence arbitrary length belonging sequences input space length output sequence belonging sequences output space inputs vectors output vectors represented ﬁxed-length real-valued vectors; example task phonetic speech recognition would typically vector coeﬃcients would one-hot vector encoding particular phoneme. paper assume output space discrete; however method readily extended continuous output spaces provided tractable diﬀerentiable model network referred transcription network scans input sequence outputs sequence transcription vectors. network referred prediction network scans output sewih input-hidden weight matrix hidden-hidden weight matrix hiddenoutput weight matrix bias terms hidden layer function. traditional rnns elementwise application tanh logistic sigmoid functions. howfor simplicity assume transcription sequence length input sequence; however true example transcription network uses pooling architecture reduce sequence length. respectively input gate forget gate output gate state vectors size hidden vector weight matrix subscripts obvious meaning example hidden-input gate matrix input-output gate matrix etc. weight matrices state gate vectors diagonal element gate vector receives input element state vector. bias terms omitted clarity. prediction network attempts model element given previous ones; therefore similar standard next-step-prediction added option making ‘null’ predictions. scans input sequence forwards backwards separate hidden layers feed forward single output layer. bidirectional rnns preferred because output vector depends whole input sequence however tested extent impacts performance. given length input sequence forward hidden bidirectional computes backward hidden sesequence transcription sequence ﬁrst iterating backward layer transducer evaluated test data seek mode output sequence distribution induced input sequence. unfortunately ﬁnding mode much harder determining probability single sequence. complication output ﬁrst elements output sequence point transcription sequence. horizontal arrow leaving node represents probability outputting nothing vertical arrow represents probability outputting element black nodes bottom represent null state outputs emitted. paths starting bottom left reaching terminal node right correspond possible alignments input output sequences. alignment starts probability ﬁnal probability product transition probabilities arrows pass initial condition deﬁnition forward backward variables follows product point output lattice equal probability emitting complete output sequence emitted transcription step fig. shows plot forward variables backward variables product speech recognition task. figure forward-backward variables speech recognition task. image bottom input sequence spectrogram utterance. three heat maps show logarithms forward variables backward variables product across output lattice. text left target sequence. outputs emitted model. method employed paper ﬁxed-width beam search tree output sequences. advantage beam search scales arbitrarily long sequences allows computational cost traded search accuracy. best elements instead single best element. length normalisation ﬁnal line appears important good performance otherwise shorter output sequences excessively favoured longer ones; similar techniques employed hidden markov models speech handwriting recognition observing prediction network outputs independent previous hidden vectors given current iteratively compute prediction vectors output sequence considered beam search storing hidden vectors running step input. prediction vectors combined transcription vectors compute probabilities. procedure greatly accelerates beam search cost increased memory use. note lstm networks hidden vectors state vectors stored. core training test sets timit contain respectively phonetically transcribed utterances. deﬁned validation randomly selecting sequences training set; slight disadvantage compared many timit evaluations validation drawn non-core test sequences used training. reduced phoneme targets used training testing. standard speech preprocessing applied transform audio ﬁles feature sequences. channel mel-frequency ﬁlter bank pre-emphasis coeﬃcient used compute mel-frequency cepstral coeﬃcients plus energy coeﬃcient hamming windows intervals. delta coeﬃcients added create input sequences length vectors coeﬃcient normalised mean zero standard deviation training set. standard performance measure timit phoneme error rate test summed edit distance output sequences target sequences divided total length target sequences. phoneme error rate customarily presented percentage recorded transcription network transducer. error recorded prediction network misclassiﬁcation rate next phoneme given previous ones. prediction network consisted size lstm hidden layer input units output units. transcription network consisted size lstm hidden layers inputs outputs. gave total weights transducer. standalone prediction networks weights respectively. networks trained online steepest descent using learning rate momentum gaussian weight noise standard deviation injected training reduce overﬁtting. prediction transduction networks stopped point lowest log-loss validation set; network stopped point lowest phoneme error rate validation set. network initialised uniformly distributed random weights range network preﬁx search decoding used transcribe test probability threshold transduction network beam search algorithm described algorithm used beam width nonetheless advantage transducer network relatively slight. timit transcriptions small training prediction network around labels opposed millions words typically used train language models. supported poor performance standalone prediction network misclassiﬁes almost three quarters targets per-phoneme loss much better entropy phoneme distribution would therefore hope greater improvement larger dataset. alternatively prediction network could pretrained large ‘target-only’ dataset jointly retrained smaller dataset part transducer. analogous procedure speech recognisers combine language models extracted large text corpora acoustic models trained smaller speech corpora. advantage diﬀerentiable system sensitivity component every component easily calculated. allows analyse dependency output probability lattice sources information input sequence previous outputs. fig. visualises relationships transducer applied ‘end-to-end’ speech recognition spectrogram images directly transcribed character sequences intermediate conversion phonemes. introduced generic sequence transducer composed recurrent neural networks demonstrated ability integrate acoustic linguistic information speech recognition task. currently training transducer large-scale speech handwriting recognition databases. illustrations paper drawn ongoing experiment end-to-end speech recognition. future would like look wider range sequence transduction problems particularly diﬃcult tackle conventional algorithms hmms. example would text-tospeech small number discrete input labels transformed long continuous output trajectories. another machine translation particularly challenging complex alignment between input output sequences. ilya sutskever chris maddison geoﬀrey hinton provided helpful discussions suggestions work. alex graves junior fellow canadian institute advanced research.", "year": 2012}