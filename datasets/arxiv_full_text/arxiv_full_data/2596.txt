{"title": "Neural Coarse-Graining: Extracting slowly-varying latent degrees of  freedom with neural networks", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We present a loss function for neural networks that encompasses an idea of trivial versus non-trivial predictions, such that the network jointly determines its own prediction goals and learns to satisfy them. This permits the network to choose sub-sets of a problem which are most amenable to its abilities to focus on solving, while discarding 'distracting' elements that interfere with its learning. To do this, the network first transforms the raw data into a higher-level categorical representation, and then trains a predictor from that new time series to its future. To prevent a trivial solution of mapping the signal to zero, we introduce a measure of non-triviality via a contrast between the prediction error of the learned model with a naive model of the overall signal statistics. The transform can learn to discard uninformative and unpredictable components of the signal in favor of the features which are both highly predictive and highly predictable. This creates a coarse-grained model of the time-series dynamics, focusing on predicting the slowly varying latent parameters which control the statistics of the time-series, rather than predicting the fast details directly. The result is a semi-supervised algorithm which is capable of extracting latent parameters, segmenting sections of time-series with differing statistics, and building a higher-level representation of the underlying dynamics from unlabeled data.", "text": "present loss function neural networks encompasses idea trivial versus nontrivial predictions network jointly determines prediction goals learns satisfy them. permits network choose sub-sets problem amenable abilities focus solving discarding ’distracting’ elements interfere learning. this network ﬁrst transforms data higher-level categorical representation trains predictor time series future. prevent trivial solution mapping signal zero introduce measure non-triviality contrast prediction error learned model naive model overall signal statistics. transform learn discard uninformative unpredictable components signal favor features highly predictive highly predictable. creates coarse-grained model time-series dynamics focusing predicting slowly varying latent parameters control statistics time-series rather predicting fast details directly. result semi-supervised algorithm capable extracting latent parameters segmenting sections time-series diﬀering statistics building higher-level representation underlying dynamics unlabeled data. physicists feature engineering? statistical physics corresponding concept ’golden feature’ order parameter single variable captures emergent large-scale dynamics physical system projecting internal ﬂuctuations microscopic structures. descriptions based entirely system’s order parameters tend much generalizable transferable detailed microscopic models capture behavior many disparate systems share underlying structure symmetry. process extracting large-scale dynamics system discarding microscopic details irrelevant overarching dynamics referred ’coarse-graining’. physical models often wants predict dependencies parameters time evolution variables working order-parameters means results become much general also means certain questions become unanswerable details question depends coarse-grained away. trade-oﬀ goes hand hand ability order parameters intuition zooms bigger bigger scales certain kinds mistakes mismatches microscopic details model reality erased ones remain relevant matter zoom out. order parameters things left discarded everything eﬃciently approximated make system bigger. perform self-consistent must things matter large-scale details anything could associated kind error compare many problems machine learning phrased novel element here. usually loss function designed speciﬁc problem mind errors performance problem de-facto important. wish construct unsupervised technique somehow decide inspired dependencies within data asymptotically important errors irrelevant. example recent advances image synthesis style transfer error functions constructed intermediate layer activations object classiﬁer network rather working pixel level result minimizing perceptually meaningful inconsistencies rather errors pixel values transfer algorithm eﬀective uses supervised component order determine meaningful object classiﬁcation task provided aesthetic sense evaluate artistic styles even though supervised task didn’t directly related goal style transfer details aren’t irrelevant using network trained identify source video frame autoencoder instead object classiﬁer would emphasize certain aspects data others. labels object type location tend emphasize edges whereas delocalized information source video clip frame tends emphasize distributions color intensity speciﬁc shape. recent work suggested possible combine generative adversarial networks autoencoders allow auto-encoder eﬀectively discover loss function here intrinsic predictability used drive network organize around data speciﬁcally ability predict whether something member distribution given data. want unsupervised fashion need sense intrinsic meaningfulness features others using data generator meaning. since considering approach permissible declare aspects data irrelevant becomes doubly tricky. thing still however require things retain self-predictive possible. brings back physics analogy degrees freedom taken point time best predict future data. kind approach used construct things like wordvec generate latent conceptual spaces words. however following analogy physics suggestion perhaps asking much trying predict future microscopic variables macroscopic measurements mean retain information solely purpose spanning microscopic basis inherently abstracts compresses underlying processes generate data. train wordvec database containing many diﬀerent distinct dialects would useful predicting ’micro’ future sentence know dialect sentence belongs wished model conceptual structure sentences dialect information would mostly irrelevant would force learn many parallel models relationships much dense neural network repeatedly learn relationships every pixel oﬀset whereas convolutional network kernels generalize translationally invariant fashion. features data describe speciﬁcally proposed features predict themselves everything data. allows learner sense choose problem solve ﬁnding things eﬃciently predicted discarding highly unpredictable information consideration. work schmidhuber explored idea asking networks make prediction given different views data requiring make ’correct’ prediction make prediction found would organize networks discover systematic features data. extend representation contain suﬃcient information predict relationships variations data representation without speciﬁc reference underlying data follows presentation speciﬁc algorithm loss function able perform task stable fashion refer ’neural coarse-graining’ ’ncg’. point analogy order parameters order-parameter self-predictive. microscopic model contains enough information predict future microscopic state macroscopic model retain enough information predict future macroscopic state think separate tasks task transform data variables second task variables point predict value variables diﬀerent point. novel element prediction evaluated respect matching data evaluated respect matching transformed data network helping deﬁne loss function introduces potential problem network projects data constant value? case predictor would perfect obviously wouldn’t capture anything underlying data. avoid this need loss function evaluate quality predictions also somehow evaluate hard task network itself. take inspiration information theory much information gained future making prediction contingent upon past globally optimal predictor quantity known predictive information deﬁned xfuture represents data signal observed future xpast represents data already observed past. shannon entropy. markov chain predictive information reduces case consider transformed signal rather original signal want optimize transform maximize predictive information transformed signal. since transform deterministic predictive information turns measure non-trivial informational closure proposed bertschinger amount information gained conditioning past bounded entropy signal predicted transformation maps simple distribution much additional information gained knowledge past even predictor happens accurate protects projection onto constant value. wanted construct coarse-grained process predictive future captures much information possible underlying process could optimize ntic maximized. however also want adapt coarse-graining capabilities speciﬁc predictor given predicts yt+. case beneﬁcial transform throw information general could used increase ntic. information extracts information predict well. measure ntic account adaptation ntic state space compression framework provide intuitions optimization function implementation employs certain practical tweaks. main diﬀerence previous discussion instead optimizing function maps random variables construct transforms directly interpreted probability distribution macroscopic random variable play explicit role itself. words treat probability distribution reason using entropy dataset average don’t want necessarily various data points maximum entropy distributions. well mapped delta distribution-like instead want capture much variation data possible time. cross-entropy hand small every point time time average taken instantaneous cross-entropies. crossentropy term takes role conditional entropy term ntic. instead minimizing rest-entropy given minimize diﬀerence actually predicted distribution observed distribution softmax activation generates probability distribution abstract classes functions discovered order parameter. network forks branch simply oﬀsetting transformed data time branch processing pattern classes another arbitrary hidden layers ﬁnally ending another softmax layer. order evaluate quality predictions output ﬁnal softmax layer compared oﬀset output intermediate softmax layer. straightforward application timeseries analysis predicting future given present provides needed locality sequence future-sequence means transform inputs predicted outputs using shared transform. such examine applications timeseries analysis feature engineering provide reference python implementation using theano lasagne https//github.com/ arayabrain/neural-coarse-graining/. many cases observable data generated indirectly sort complex process governed small slowly-varying control parameters. neural coarse-graining attempt discover latent control parameters automatically. example noise signal detailed statistics noise slowly varied background mean noise remained constant direct attempt auto-encode signal extract hidden feature would diﬃculty auto-encoder would capture high entropy noise signal being able accurately reproduce amplitude values. self-predictor would even worse noise unpredictable detail. however ﬁrst make feature described high-order statistics noise local window dynamical behavior feature might highly predictable. ﬁrst consider problem form order test ability neural coarse-graining extract latent control parameter unsupervised fashion. generate timeseries contains mixture uncorrelated gaussian noise autocorrelated noise controlled envelope function controls timescale experiments. independent samples autocorrelated noise chosen zero mean unit standard deviation. generate samples autocorrelated noise ﬁnite diﬀerence equation cosxt− sinηt gaussian noise zero-mean unit standard-deviation. noise sources linearly since kl-divergence measures diﬀerence entropy term might seem superﬂuous. however stops becoming uniform distribution. would every uniform distribution loss function omits term would minimized. entropy term forced entropy distributions then contrast uniform distributions contain information particular loss function also generalize partitions data past future sort partition could used long transformation applied sides partition. example rather predicting future timeseries predict far-away part image given local neighborhood. outcome optimizing loss function transform extract variable subcomponent data algorithm conﬁdent about throw rest information data. increasing number abstract classes dimensionality regression forces algorithm include data’s structure order maximize entropy transformed data. similar considerations govern choosing number would apply choosing size auto-encoder’s bottleneck layer. however ordering learning opposite autoencoder start noisy simplify representation bottleneck whereas tend start simple elaborate discovers things ’say’ data. care must taken large numbers classes softmax activations experience increasingly strong tendency stuck mean number classes increases. methods hierarchical softmax adjusting softmax temperature course training useful avoid problems. order construct concrete implementation neural coarse-graining components need parameterized transform data order parameters predictor uses part transformed data predict nearby parts. paper implement single end-to-end connected neural network. transform network takes data applies number hidden layers layer decrease correlation length performance drops diﬀerent types noise become less clearly distinguished. certain point outcome training becomes bistable either ﬁnding weakly correlated order-parameter falling local minimum network fails detect anything data. size bistable region inﬂuenced batch sized used training batch apparent bistable region shrinks terms hyperparameters prediction distance learning rate seem make much diﬀerence ability network discern types noise. increasing size hidden layers likewise little eﬀect. however larger ﬁlter sizes transformer network appear eﬀect improving pearson correlation large cases. even larger ﬁlter size however bistable region appears unaﬀected. perform similar test case distinguishing uncorrelated noise drawn structurally diﬀerent distributions. compose signals alternate gaussian noise various kinds discrete noise standard deviation zero mean. discrete noise taken generalization bernoulli distribution number discrete values selected uniformly. consider binary noise whereas linear autoregressive model distinguish noise types previous case diﬀerent noise distributions problem distinguished functions nonlinear signal variable poses test whether kind higher-order nonlinear order parameter learned network. fact type problem seem signiﬁcantly harder solve. batch size full training network appears easily become stuck local minimum. linear features distinguish noise types network must promising weakly nonlinear feature begin train predictor successfully. batch size large tendency simply decay towards safe uniform prediction. however batch size smaller ﬂuctuations larger greater chance following spurious linear feature enough discover relevant nonlinearity. result smaller batch size network able order parameter binary noise case. furthermore decreasing ﬁlter sizes used layers except output transformer predictor layers softmax. batch normalization used ﬁrst layers transformer ﬁrst layers predictor. transformed signal probability distribution classes predictor attempting predict distribution timesteps future weights optinetwork trained epochs. example convergence curves shown analyze performance respect discovering envelope function measure pearson correlation between transformed signal known envelope function fig. example signal correlated noise segmentation task. blue line signal line training curves loss function envelope function types noise. example diﬀerent values. problem becomes hard sometimes gets stuck local minimum around trivial prediction assigning transformed classes equal probability point time. trivial prediction loss exactly plateau around common feature training problem diﬃcult. discovered coarse-grained variable versus actual envelope function example. pearson correlations discovered coarse-grained variable true envelope function multiple runs diﬀerent values. apparent phase transition beyond network longer solve problem segment noise types. ploratory tool. this apply problem detecting diﬀerent types activity using accelerometer data. general class problem timeseries accelerometers worn individual goal categorize person using seconds data. human activity recognition dataset already contains number hand-designed features describing statistics accelerometer data -second long chunks data transformed dimensional representation taking account measures standard deviation kurtosis ﬂuctuations signal. using engineered features out-of-the-box application adaboost achieves accuracy. dataset transform neighborhood timesteps full -dimensional input classes temporal resolution. prediction network takes input size neighborhood transformed classes predicts class steps future. full receptive ﬁeld predictor fig. correlation matrix discovered coarsegrained variables diﬀerent activity classes. columns matrix sorted bring together coarsegrained variables strongly correlated activity turn. coarse-grained variables strongly associated single activity class exceptions columns resulting classes already show strong correlations diﬀerent behaviors however it’s clear samples discovered classes ambiguous uniquely identify behavior. features three adjacent timesteps adaboost observe accuracy test compared using original features. however combine original features classes accuracy test increases features seem expose structures data otherwise diﬃcult extract adaboost itself. confounding factor algorithm access multiple timesteps whereas temporal information available original score -second interval used produce hand-designed features original data. such increase performance availability wider time window inputs. test measuring performance using original features taken table iii. results adaboost classiﬁer dataset trained diﬀerent sets features original features three frames neural coarsegraining features three frames features produce worse classiﬁer result improvement original features alone combined. test set. however take time-extended original features combine discovered features performance worse using instantaneous original features classes seems suggest degree discovered order parameters eﬃciently summarize coherent aspects time-dependence data. also classes generated neural coarsegraining exploratory analyses data. since transformed data tends locally stable sharp transitions categories natural thing sample between-class transitions markov process. dataset there’s complication data taken according speciﬁc protocol ordering activities performed speciﬁc subject. length time spent activity quite short would hard make predictions cross activity boundary. turns algorithm ends predominantly discovering structure resulting markov process regularity protocol means discovering activity currently good predictor activity taking place future probably strongly encouraged order parameters correlated activity types. double-loop structure indicate discovery subject-speciﬁc details. introduced neural coarse-graining algorithm extracts coarse-grained features data readily determined local details also highly predictive themselves. coarse-graining preserve underlying relationships data instead tries subset relationships readily predict. provides form unsupervised labelling problem onto simpler sub-problem discards parts data confound premartin biehl would like thank nathaniel virgo elsi origins network earth-lifescience institute tokyo institute technology inviting short-term visitor. part research performed stay. show optimizing non-trivial informational closure reduces optimizing ipred transformed siggeneral processes nal. assume joint process markov chain non-trivial informational closure respect measured smaller value non-trivially closed ﬁrst term measure much information contained future second term measure much information contains future already contained present process called non-trivially closed respect shares information information contained itself. constructing things terms information measures easier discrete states rather continuous states. however continuous version coarse-graining loss function constructed extract continuous-valued variables. interpretation simpler doesn’t require treating transformed variable distribution value diﬀerent parts algorithm. order make construction must able compare entropy transformed signal naive point fig. left graph transitions discovered categories transformed data. links drawn transitions occur probability previous class. numbering nodes plot corresponds column order fig. node colors based corresponding highly-correlated activity. right graph transitions activities data. activities always performed ﬁxed order cyclic structure ends strongly determining behavior long-term temporal sequence predictions possibly artefact picking generating features problem. diction. advantage approach compared directly using self-prediction underlying data neural coarse-graining free predict parameters controlling distribution noise rather details individual random samples making method robust prediction tasks highly noisy data sets including ones structure noise important understand take account. although neural coarse-graining trains predictor unclear whether general predictor useful towards particular task. rather needing able construct predictor forces transformation preserve certain features data others. such sub-problem network decides solve used exploratory analysis characterize dominant features underlying processes behind data. examining transition matrix discovered classes possible extract coarse-grained picture dynamics detecting things underlying periodicities branching decision points time-series data. addition experiments seems extracted features capture clean details underlying data used augment performance machine learning algorithms form unsupervised feature engineering. view entropy transformed signal conditioned predictor. this generate signals signal corresponding coarse-grained variable ’residual’ signal corresponding prediction error future coarse-grained signal must assume something distributions assume signals correspond samples taken multi-dimensional gaussian distribution entropy signal corresponds logarithm determinant covariance matrix. lets construct regression loss function continuous coarse-grained variables. mostly include example completeness demonstration construct coarse-graining loss functions diﬀerent types variable. although form conceptually tidy discrete case found general discrete version algorithm seems perform better less prone overﬁtting least cases investigated. dieleman schlter raﬀel olson snderby nouri maturana thoma battenberg kelly fauw heilman almeida mcfee weideman takcs rivaz crall sanders rasul french degrave lasagne first release. bertschinger olbrich jost explorations complexity possible life abstracting synthesizing principles living systems proceedings german workshop artiﬁcial life reyes-ortiz esann song zhao pedregosa varoquaux gramfort michel thirion grisel blondel prettenhofer weiss dubourg vanderplas passos cournapeau brucher perrot duchesnay journal machine learning research", "year": 2016}