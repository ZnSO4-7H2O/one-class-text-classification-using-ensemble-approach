{"title": "CNN-based Patch Matching for Optical Flow with Thresholded Hinge  Embedding Loss", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Learning based approaches have not yet achieved their full potential in optical flow estimation, where their performance still trails heuristic approaches. In this paper, we present a CNN based patch matching approach for optical flow estimation. An important contribution of our approach is a novel thresholded loss for Siamese networks. We demonstrate that our loss performs clearly better than existing losses. It also allows to speed up training by a factor of 2 in our tests. Furthermore, we present a novel way for calculating CNN based features for different image scales, which performs better than existing methods. We also discuss new ways of evaluating the robustness of trained features for the application of patch matching for optical flow. An interesting discovery in our paper is that low-pass filtering of feature maps can increase the robustness of features created by CNNs. We proved the competitive performance of our approach by submitting it to the KITTI 2012, KITTI 2015 and MPI-Sintel evaluation portals where we obtained state-of-the-art results on all three datasets.", "text": "learning based approaches achieved full potential optical estimation performance still trails heuristic approaches. paper present based patch matching approach optical estimation. important contribution approach novel thresholded loss siamese networks. demonstrate loss performs clearly better existing losses. also allows speed training factor tests. furthermore present novel calculating based features different image scales performs better existing methods. also discuss ways evaluating robustness trained features application patch matching optical ﬂow. interesting discovery paper low-pass ﬁltering feature maps increase robustness features created cnns. proved competitive performance approach submitting kitti kitti mpi-sintel evaluation portals obtained state-of-the-art results three datasets. recent years variants patchmatch approach showed useful nearest neighbor ﬁeld estimation also challenging problem large displacement optical estimation. performing methods like deep matching flow fields strongly rely robust multi-scale matching strategies still engineered features like siftflow actual matching. hand works like demonstrated effectiveness features based convolutional neural network matching patches. however works validate performance features using actual patch matching approach like patchmatch flow fields matches pixels image pairs. instead simply treat matching patches classiﬁcation ignores many practical issues. instance important based features able distinguish different patch positions position also determined accurately. furthermore performing architectures slow used patch matching requires matching several patches every pixel reference image. siamese networks distance reasonably fast testing time still outperform engineered features regarding classiﬁcation found usually underperforming engineered features regarding patch matching. think among things convolutional structure cnns neighboring patches share intermediate layer outputs much easier cnns learn matches neighboring patches neighboring patches. however propagation correctly matched patches close usually contribute less patch matching patches apart other. classiﬁcation differentiate here. ﬁrst solution succeed based patch matching pixel-wise batch normalization weakens unwanted convolutional structure computationally expensive test time. thus instead improve features level allows outperform existing approaches. ﬁrst contribution novel loss function siamese architecture distance show hinge embedding loss commonly used siamese architectures variants important design decrease distance unlimitedly correct matches although small distances patches differ effects like illumination changes partial occlusion costly also unnecessary long false matches larger distances. demonstrate signiﬁcantly increase matching quality relaxing ﬂaw. moreover introduce novel matching robustness measure tailored binary decision problems like patch matching plotting measure different displacements distances wrong patch correct reveal interesting properties different loss functions scales. main contributions demonstrate effectiveness approach obtaining performance three major evaluation portals kitti mpisintel former learning based approaches always trailed heuristic approaches least them. regularized optical estimation goes back horn schunck randomized patch matching relatively ﬁeld ﬁrst successfully applied approximate nearest neighbor estimation data term well-deﬁned. success optical estimation started publications like recent works flow fields showed proper multi-scale patch matching performing optical results achieved. regarding patch descriptor matching learned data terms exists fair amount literature approaches treat matching abstract level present pipeline solve problem like optical estimation reconstruction although many reconstruction datasets evaluation. zagoruyko komodakis compared different architectures compare patches. simo-serra used siamese architecture distance. argued useful practical applications. recently several successful based approaches stereo matching appeared however still approaches successfully learning compute optical ﬂow. worth mentioning flownet tried solve optical problem whole cnns images input optical output. results good regarding runtime still state-of-the-art quality. also network tailored speciﬁc image resolution knowledge training large images several megapixel still beyond todays computational capacity. ﬁrst approach using patch matching based features patchbatch managed obtain stateof-the-art results kitti dataset pixelwise batch normalization loss includes batch statistics. however pixel-wise batch normalization computationally expensive test time. furthermore even pixel-wise normalization approach trails heuristic approaches mpi-sintel recent approach deepdiscreteflow uses discreteflow basis instead patch matching. despite using recently invented dilated convolutions also trail original discreteflow approach datasets. approach based siamese architecture siamese networks learn calculate meaningful feature vector image patch training distance feature vectors matching patches reduced distance between feature vectors non-matching patches increased detailed description). siamese architectures strongly speed testing time neighboring patches image share convolutions. details speedup works described supplementary material. network used experiments shown table similar tanh nonlinearity layers also found outperform relu siamese based patch feature creation. loss function batch selection figure sample pushed although clearly correct side decision boundary samples also move weight change. samples classiﬁed correctly beforehand creates false decision boundary crossings correct ones. performs unnecessary push not. architectural poorly treated existing loss functions fact loss pushes feature distances matching patches unlimit think training edly zero might argue better train property directly. known function based loss keeps distance matching non-matching pairs given loss functions common loss gradient sometimes zero. ordinary approaches still back propagate zero gradient. makes approach slower necessary also leads variable effective batch size training samples actually back propagated. limited issue hinge embedding loss training samples obtain zero gradient tests. however samples obtain zero gradient. training consists several pairs images known optical displacement pixels. ﬁrst subtract mean image divide standard derivation. create training samples randomly extract patches corresponding matching patches positive training samples. also extract non-matching patch negative training samples. negative samples sampled distribution prefers patches close matching patch minimum distance pixels also allows sample patches exact distribution found supplementary material. train pairs patches center pixel occluded matching patch otherwise network would train occluding object positive match. however patch center visible expect network able deal partial occlusion. learning rate decreases linearly exponential space batch i.e. learnrate e−xt learnrate multi-scale matching flow fields approach basis optical pipeline compares patches different scales using scale spaces i.e. scales full image resolution. creates feature maps different scales low-pass ﬁltering feature highest scale siftflow features used low-pass ﬁltering features performs better recalculating features scale different resolution observed effect based features even also trained lower resolutions. however modiﬁcations shown figure right possible obtain better results recalculating features different resolutions. trained applied highest image resolution highest second highest scale. furthermore trained resolutions calculate feature maps third fourth scale applied resolution respectively. multi-resolution probability select patch lower resolution training probability respective next higher resolution. lower resolutions also distribution leads wide spread distribution figure modiﬁcation feature creation flow fields approach clearly better performance. note flow fields expects feature maps scales full image resolution details). reasons design decision found section feature maps created cnns used directly. instead perform low-pass ﬁlter them using them. low-pass ﬁltering image data creates matching invariance increasing ambiguity assuming cnns unable create perfect matching invariance expect similar effect feature maps created cnns. fact small low-pass ﬁlter clearly increases matching robustness. flow fields approach uses secondary consistency check different patch size. approach would require train execute additional cnns. keep simple perform secondary check features. possible fact flow fields randomized approach. still tests original features show real secondary consistency check performs better. reasoning design decisions figure found section evaluation methodology patch matching previous works evaluation matching robustness features performed evaluation methods commonly used classiﬁcation problems like however patch matching classiﬁcation problem binary decision problem. freely label data classiﬁcation problems patch matching requires choose iteration proposal patches better exception rule outlier ﬁltering. really issue better approaches outlier ﬁltering like forward backward consistency check robust matching-error based outlier ﬁltering. evaluation matching robustness network determined probability wrong patch confused correct patch rdist vary strongly different locations. makes differences different networks hard visualize. better visualization plot relative matching robustness errors edist computed respect pre-selected network net. deﬁned examine approach kitti training datasets contains ground truth non-synthetic large displacement optical estimation. patches taken images training patches remaining images validation. tested network trained million negative million positive samples total. furthermore publicly validate performance approach submitting results kitti recently published kitti mpi-sintel evaluation portals original parameters flow fields approach except outlier ﬁlter distance random search distance best value network random search distance four iterations additional iterations increase accuracy. batch size figure relative matching robustness errors edist. features created lower resolutions accurate large distances less accurate small ones. downsampling horizontal line results normalized details text. well occluded non-occluded areas direct measure cnns trained here. however interpolation occluded areas that) also depends good matches close occlusion boundary matching especially difﬁcult partial occlusions patches. furthermore like measure percentage pixels threshold pixels table compare original feature creation approach approach respect features. also examine variants approach table nolowpass contain low-pass blocks resolutions uses xxxx up/downsampling four scales reason resolutions work well demonstrated figure starting distance pixels based features created down-sampled image match robustly based features created full image resolution. insufﬁcient random search distance scale pixels. thus scale siftflow siftflow* table results kitti validation set. best result bold best underlined. siftflow uses pipeline tailored cnns. siftflow* uses original pipeline raising extremely amount close-by samples reduces accuracy threshold pixels. using smaller patches instead patches raise accuracy either– even clearly decreases figure shows downsampling decreases matching robustness error signiﬁcantly larger distances. fact distance pixels relative error downsampling reduced nearly compared downsampling remarkable. bustness performed best corresponds however even best performs signiﬁcantly worse probably fact variance var) much larger shown figure case positive samples. think affects test negatively follows assume unlearned test patches clear condition likely violated var) large compared learned gap. possible force network keep variance small compared gap. possible control variance keeps variance small cannot limit gap. matching robustness plots loss functions perform worse others although larger matching robustness mostly explained fact perform poorly large displacements here correct matches usually important missing matches lead larger endpoint errors. averaged pixels consider this. figure also shows effect parameter distances displacements improved small distances displacements beneﬁt larger improvement happens unnecessary destructive training avoided patches small distances beneﬁt form larger likely real greal smaller large displacements patches chaotic forces larger variances distances thus larger required counter larger variance. multi-resolution network training examine three variants training multi-resolution network training resolution although used resolution testing time training resolutions used testing time training resolution seen table training resolutions clearly performs best. likely mixed training data performs best samples highest resolution provide largest entropy samples lower resolutions better problem. however training samples lower resolutions seem harm training higher resolutions. therefore extra highest resolution. compare loss state-of-the-art losses hard mining figure table shown table thresholded loss clearly outperforms losses. drlim reduces mentioned hinge loss training samples small hinge loss less. clearly reduces error compared hinge cannot compete thresholded loss furthermore speedup training possible like approach. cent. variant drlim performs worse drlim tests. hard mining trains hardest samples largest hinge loss thus also speeds training. however percentage samples trained batch ﬁxed adapt requirements training data like approach. data hard mining becomes unstable mining factor i.e. loss negative samples becomes much larger loss positive samples. leads poor performance think fact hardest negative samples much harder train hardest positive samples. patches e.g. fully white overexposure also many negative samples have contrast samples small spatial distance positive counterpart. makes training even harder positive samples change. make sure dynamic loss based mining approach cannot become unstable towards much larger negative loss values tested extreme randomly removed negative training samples keeping positive. stayed stable even used smaller positive/negative sample mining ratio approach training samples possibly choose harder positive samples contribute training. even removal possible samples achieved matching rofigure relative matching robustness errors different loss functions plotted different distances displacements note plot horizontal line normalized text details. here. figure also shows low-pass ﬁltering feature increases matching robustness distances displacements. tests lowpass performed best engineered siftflow features beneﬁt much larger low-pass ﬁlters makes original pipeline extremely efﬁcient them. however using pipeline shows matching robustness justiﬁed siftflow also performs better outlier ﬁltering. effects directly trained still challenging beat well designed purely heuristic approaches learning. fact existing based approaches often still underperform purely heuristic approaches even direct predecessors public results kitti mpi-sintel evaluation portals shown table public results used extra iterations best possible subpixel accuracy similar runtime flow fields kitti approach best measures although smaller patch size patchbatch patchbatch patch size similar performs even worse. patchbatch* like work without pixel-wise batch normalization even trails purely heuristic methods like flow fields. kitti approach also clearly outperforms patchbatch general optical methods including deepdiscreteflow that despite using cnns trails engineered predecessor discreteflow many measures. methods outperform approach rigid segmentation based methods require segmentable rigid objects moving front rigid background thus suited scenes contain non-rigid objects objects easily segmentable. despite making assumptions approach outperforms challenging foreground furthermore approach clearly fastest performing methods although still optimization potential especially segmentation based methods slow. rigid mpi-sintel datasets approach best non-occluded areas matched features. interpolation occluded areas epicflow works less well surprise aspects like good outlier ﬁltering important occluded areas learned approach. still obtained best overall result challenging ﬁnal contains motion blur. contrast patchbatch lags behind mpi-sintel deepdiscreteflow clearly trails predecessor discreteflow clean ﬁnal set. approach never trails relevant matchable part. detailed runtime cnns patch matching up/downsampling lowpass parts approach likely signiﬁcantly sped using versions like based propagation scheme patch matching. contrary patchbatch based already takes majority time also ﬁnal tests able improve architecture needs marginal change quality validation set. table results kitti test set. numbers brackets show patch size learning based methods. best result published methods bold best underlined. patchbatch* patchbatch without pixel-wise batch normalization. table results kitti test set. numbers brackets shows used patch size learning based methods. best result published general optical methods bold best underlined. bold segmentation based method shows result better best general method. rigid segmentation based methods designed urban street scenes similar containing segmentable rigid objects rigid background general methods work optical problems. paper presented novel extension hinge embedding loss outperforms losses learning robust patch representations also allows increase training speed robust respect unbalanced training data. presented multi-scale feature creation approach cnns proposed evaluation measures plotting matching robustness respect patch distance motion displacement. furthermore showed low-pass ﬁltering feature maps created cnns improves matching result. together proved effectiveness approach submitting kitti kitti mpi-sintel evaluation portals ﬁrst learning based approach achieved state-of-the-art results three datasets. results also show transferability contribution ﬁndings made section solely based kitti validation still work unchanged kitti mpi-sintel test sets well. future work want improve network architecture using techniques like batch normalization dilated convolutions furthermore want low-pass ﬁltering invariance also helps application like sliding window object detection want improve loss function e.g. dynamic depends properties training samples. tested patch size pixels although showed larger patch sizes perform even better. might interesting largest beneﬁcial patch size. frames mpisintel large optical showed especially challenging. lack training data rarity still large impact average want create training data tailored frames examine learning based approaches beneﬁt", "year": 2016}