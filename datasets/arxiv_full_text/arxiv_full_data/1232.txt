{"title": "Accelerating Deep Learning with Shrinkage and Recall", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Deep Learning is a very powerful machine learning model. Deep Learning trains a large number of parameters for multiple layers and is very slow when data is in large scale and the architecture size is large. Inspired from the shrinking technique used in accelerating computation of Support Vector Machines (SVM) algorithm and screening technique used in LASSO, we propose a shrinking Deep Learning with recall (sDLr) approach to speed up deep learning computation. We experiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network (DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data sets. Results show that the speedup using shrinking Deep Learning with recall (sDLr) can reach more than 2.0 while still giving competitive classification performance.", "text": "abstract—deep learning powerful machine learning model. deep learning trains large number parameters multiple layers slow data large scale architecture size large. inspired shrinking technique used accelerating computation support vector machines algorithm screening technique used lasso propose shrinking deep learning recall approach speed deep learning computation. experiment shrinking deep learning recall using deep neural network deep belief network convolution neural network data sets. results show speedup using shrinking deep learning recall reach still giving competitive classiﬁcation performance. deep learning become powerful machine learning model. differs traditional machine learning approaches following aspects firstly deep learning contains multiple non-linear hidden layers learn complicated relationships inputs outputs. deep architectures using multiple layers outperform shadow models secondly need extract human design features reduce dependence quality human extracted features. mainly study three deep learning models work deep neural networks deep belief network convolution neural network deep neural network basic deep learning model. contains multiple layers many hidden neurons non-linear activation function layer. figure shows simple example deep neural network model. deep neural network input layer hidden layers output layer. training process deep neural network includes forward propagation back propagation. forward propagation uses current connection weight give prediction based current state model. back propagation computes amount weight changed based difference ground truth label forward propagation prediction. back propagation deep neural network non-convex problem. different initialization affects classiﬁcation accuracy convergence speed models. several unsupervised pretraining methods neural network proposed improve performance random initialized using stacks rbms autoencoders compared random initialization pretraining followed ﬁnetuning backpropagation improve performance signiﬁcantly. deep belief network generative unsupervised pretraining network uses stacked rbms pretraining. corresponding conﬁgured often produces much better results. undirected connections ﬁrst layers directed connections lower layers convolution neural network proposed deal images speech time-series. standard limitations. firstly images speeches usually large. simple neural network process image size layer hidden neurons require weight parameters. many variables lead overﬁtting easily. computation standard model requires expensive memory too. secondly standard consider local structure topology input. example images strong local structure. many areas image similar. speeches strong structure variables temporally nearby highly correlated. forces extraction local features restricting receptive ﬁelds hidden neurons local however training process deep learning algorithms including computationally expensive. large number training data large number parameters multiple layers. inspired shrinking technique used accelerating computation support vector machines algorithm minimizing amount slack. since learning problem much less support vectors training examples shrinking proposed eliminate training samples large learning tasks fraction support vectors small compared training sample size many support vectors upper bound lagrange multipliers. testing point ℜp×n dictionary dimension size parameter controls sparsity representation large usually case practical applications denoising classiﬁcation difﬁcult time-intensive compute. screening technique used reduce size dictionary using rules order accelerate computation lasso. either shrinking screening lasso approaches trying reduce size computation data. inspired techniques propose faster reliable approach deep learning shrinking deep learning. given testing point class number indicator vector testing samples number classes except indicate class test point. output neural network testing point ℜ×c. contains continuous values algorithm gives framework standard deep learning. epoch standard deep learning ﬁrst runs forward-propagation training data computes output output function weight parameters deep learning tries optimal minimize error loss squared error loss softmax loss backpropagation process deep learning updates weight parameter vector using gradient descent. training data gradient descent denoted screening technique used lasso propose accelerating algorithm shrinking deep learning recall main contribution sdlr reduce running time signiﬁcantly. though trade-off classiﬁcation improvement speedup training time data sets sdlr approach even improve classiﬁcation accuracy. noted approach sdlr general model thinking applied large data large network small data small network sequential parallel implementations. study impact proposed accelerating approaches using data sets computer vision high energy physics biology science. amount data world exploding. analyzing large data sets so-called data become basis competition underpinning waves productivity growth innovation consumer interest data technologies including cloud computing dimensionality reduction proposed analyzing data machine learning algorithms requires special hardware implementations large amount running time. training sample corresponding label positive slack variable mapping function gives solution known weight vector controls relative importance maximizing margin linear related data points larger error larger training data less data training. algorithm gives outline shrinking deep learning compared standard deep learning algorithm requires inputs elimination rate stop threshold percentage indicating amount training data eliminated epoch number indication stop eliminating training data nepoch nepoch current number training data. maintain index vector algorithm forward backward propagation apply training let’s relation softmax loss function gradient respect weight parameter example given point class large softmax loss function small. gradient softmax loss function close also close summary softmax loss function small gradient also small. experiment test algorithms data sets different domains using different random initialization. data sets used listed table mnist standard data handwritten digits; cifar- data. algorithm training process applied subset training data. ﬁrst epoch include training indexes. forward backward propagation epoch select nepochs indexes training data smallest error nepoch size current number training data eliminate indexes update nepoch stop eliminating training data anymore. lemma gives theoretical foundation samples small error smaller impact gradient. thus eliminating samples impact gradient signiﬁcantly. figure shows errors using smaller errors using proves gives stronger correction signal reduce errors faster. eliminating samples elimination rate denotes percentage samples removed. select nepochs indexes training data smallest error epoch different batches threshold used eliminate samples different. assume nbatch batches epoch every batch need drop nepochs/nbatch samples average. batch threshold drop nepochs/nbatch smallest error batch threshold ti+. differ lot. exponential smoothing adjust threshold used batch instead using threshold eliminate samples following weight parameter controls intuition importance past threshold values using exponential smoothing want threshold used epoch consistent. samples errors less batch eliminated. close smoothing effect threshold obvious; close threshold deviate ti+. practical good setting terms smoothing threshold. show experiment part. training data becomes less less weight parameter trained based subset training data. optimized entire training dataset. introduce shrinking deep learning recall deal situation. order utilize training data number active training samples nepoch start training samples shown algorithm algorithm ensures model trained optimized entire training data. shrinking recall algorithm produce competitive classiﬁcation performance standard deep learning algorithm experiment also investigate impact threshold classiﬁcation results contains tiny natural images; higgs boson dataset high energy physics. alternative splicing features used predicting alternative gene splicing. implementation implementation experiments conducted laptop intel core .ghz windows -bit mnist standard data handwritten digits containing classes. contains training samples testing samples. image size figure shows examples mnist dataset. testing training classiﬁcation error different network settings. results show better setting lower testing error converges faster training. network mnist. learning rate activation function tangent function output unit sigmoid function. figure shows testing error training error using standard sdnn sdnnr results show sdnnr improves accuracy standard dnn. training error sdnnr give almost training error. figure shows training time number active samples iteration experiments sdnn sdnnr eliminate rate sdnnr recall process entire training samples shown figure number active samples less total training samples stop eliminating samples. speedup using sdnnr compared recall technique number training samples decreased threshold start training samples. trade-off speedup classiﬁcation error setting lower could reduce computation time more could increase classiﬁcation error. figure shows effect using different recall threshold sdnnr mnist data. bring training samples back best testing error. worth noting classiﬁcation error sdnnr improved compared standard could imply less overﬁtting data set. deep belief network figure shows classiﬁcation testing error training time using deep belief network shrinking recall mnist. network setting experiment. sdbnr reduces classiﬁcation error using sdbnr. convolution neural networks network architecture used mnist convolutional layers ﬁrst convolutional layers followed maxpooling layer layer followed relu layer layer followed softmax layer. ﬁrst convolutional layers receptive ﬁeld applied stride pixel. convolutional layer receptive ﬁeld layer followed softmax loss output layer. table shows classiﬁcation error training time. top- classiﬁcation testing error table means predict label determined considering class maximum probability only. higgs boson subset data training testing. sample signal process either produces higgs bosons particle not. high-level features derived physicists help discriminate particles classes. activation function output function sigmoid function. batchsize recall threshold test different network settings choose best. table shows experiment results using different network. alternative splicing sequences used bioinfomatics. contains cassette-type mouse exons features exon. randomly select exons training rest testing. exon dataset contains three real-valued positive prediction targets qnc] corresponding probabilities data contains color image classes images class. training testing images. cifar- object dataset includes airplane bird classes completely mutually exclusive. experiment network evaluate performance terms classiﬁcation error. network architecture uses convolutional layers ﬁrst three layers convolutional layer followed pooling layer; convolutional layer followed relu layer; conclusion proposed shrinking deep learning recall approach main contribution sdlr reduce running time signiﬁcantly. extensive experiments datasets show shrinking deep learning recall reduce training time signiﬁcantly still gives competitive classiﬁcation performance. vincent larochelle lajoie bengio p.-a. manzagol stacked denoising autoencoders learning useful representations deep network local denoising criterion journal machine learning research vol. zheng ding kernel alignment inspired linear discriminant analysis joint european conference machine learning knowledge discovery databases. springer berlin heidelberg zheng z.-y. shae zhang jamjoom fong analysis modeling social inﬂuence high performance computing workloads european conference parallel processing. springer berlin heidelberg", "year": 2016}