{"title": "Learning to Select Pre-Trained Deep Representations with Bayesian  Evidence Framework", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We propose a Bayesian evidence framework to facilitate transfer learning from pre-trained deep convolutional neural networks (CNNs). Our framework is formulated on top of a least squares SVM (LS-SVM) classifier, which is simple and fast in both training and testing, and achieves competitive performance in practice. The regularization parameters in LS-SVM is estimated automatically without grid search and cross-validation by maximizing evidence, which is a useful measure to select the best performing CNN out of multiple candidates for transfer learning; the evidence is optimized efficiently by employing Aitken's delta-squared process, which accelerates convergence of fixed point update. The proposed Bayesian evidence framework also provides a good solution to identify the best ensemble of heterogeneous CNNs through a greedy algorithm. Our Bayesian evidence framework for transfer learning is tested on 12 visual recognition datasets and illustrates the state-of-the-art performance consistently in terms of prediction accuracy and modeling efficiency.", "text": "propose bayesian evidence framework facilitate transfer learning pre-trained deep convolutional neural networks framework formulated least squares classiﬁer simple fast training testing achieves competitive performance practice. regularization parameters ls-svm estimated automatically without grid search cross-validation maximizing evidence useful measure select best performing multiple candidates transfer learning; evidence optimized efﬁciently employing aitken’s delta-squared process accelerates convergence ﬁxed point update. proposed bayesian evidence framework also provides good solution identify best ensemble heterogeneous cnns greedy algorithm. bayesian evidence framework transfer learning tested visual recognition datasets illustrates state-of-the-art performance consistently terms prediction accuracy modeling efﬁciency. image representations deep models trained speciﬁc image classiﬁcation tasks turn powerful even general purposes useful transfer learning domain adaptation. therefore cnns trained speciﬁc problems datasets often ﬁne-tuned facilitate training tasks domains even simpler approach— application off-the-shelf classiﬁcation algorithms representations deep cnns getting attractive many computer vision problems. however ﬁne-tuning entire deep network still requires efforts resources svm-based methods also ∗this work done jang postech. figure address problem select best multiple candidates shown ﬁgure. additionally algorithm capable identifying best ensemble multiple cnns improve performance. involve time consuming grid search cross validation identify good regularization parameters. addition multiple pre-trained deep models available unclear pre-trained models appropriate target tasks classiﬁers would maximize accuracy efﬁciency. unfortunately existing techniques transfer learning domain adaptation limited empirical analysis ad-hoc application speciﬁc approaches. propose simple effective algorithm transfer learning pre-trained deep cnns based bayesian least squares formulated bayesian evidence framework ls-svm approach automatically determines regularization parameters principled shows comparable performance standard svms based hinge loss squared hinge loss. importantly bayesian ls-svm provides effective solution select best multiple candidates identify good ensemble heterogeneous cnns performance improvement. figure illustrates approach. also propose fast bayesian ls-svm maximizes evidence efﬁciently based aitken’s delta-squared process argue ls-svm classiﬁcation least squares loss function lssvm tends penalize well-classiﬁed examples. however least squares loss often used training multilayer perceptron shows comparable performance svms addition bayesian ls-svm provides technically sound formulation outstanding performance terms speed accuracy transfer learning deep representations. also propose fast bayesian ls-svm maximizes evidence efﬁciently based aitkens delta-squared process considering simplicity accuracy claim fast bayesian ls-svm reasonable choice transfer learning deep learning representation visual recognition problems. based approach achieved promising results compared state-of-the-art techniques visual recognition tasks. rest paper organized follows. section describes examples transfer learning domain adaptation based pre-trained cnns visual recognition problems. then discuss bayesian evidence framework applicable problem section acceleration technique using aitken’s delta-squared process section performance algorithm various applications demonstrated section since alexnet demonstrated impressive performance imagenet large scale visual recognition challenge deep cnns different architectures e.g. googlenet proposed subsequent events. instead training deep cnns scratch people attempted reﬁne pre-trained networks tasks datasets updating weights neurons adopted intermediate outputs existing deep networks generic visual feature descriptors. strategies interpreted transfer learning domain adaptation. reﬁning pre-trained called ﬁne-tuning architecture network preserved weights updated based training data. finetuning generally useful improve performance requires careful implementation avoid overﬁtting. second approach regards pre-trained cnns feature extraction machines combines deep representations off-the-shelf classiﬁers linear logistic regression multi-layer neural network techniques category successful many visual recognition tasks combining classiﬁcation algorithm image representations pre-trained deep cnns often face critical issue. although several deep models trained large scale image repositories publicly available principled select multiple candidates best ensemble multiple cnns performance optimization. existing algorithms typically rely ad-hoc methods model selection fail provide clear evidence superior performance section discusses bayesian evidence framework select best model presence transferable multiple candidates identify reasonable regularization parameter ls-svm classiﬁer automatically. suppose pre-trained deep models denoted {cnnm|m goal identify best performing deep model among networks transfer learning. na¨ıve approach perform tuning network target task requires substantial efforts training. another option replace fully connected layers off-the-shelf classiﬁer check performance target task parameter tuning network would also computationally expensive. adopt bayesian evidence framework based lssvm achieve goal principled evidence network maximized iteratively maximum evidences used select reasonable model. evidence maximization procedure regularization parameter ls-svm identiﬁed automatically without time consuming grid search cross-validation. addition bayesian evidence framework also applied construction ensemble multiple cnns accomplish performance improvement. deal multi-label multi-class classiﬁcation problem number categories k}n=...n training feature vector binary variable label given otherwise. then class minimize least squares loss regularization penalty follows optimization procedures described above determine regularization parameter α/β. although estimated parameters optimal still reasonable solutions since obtained maximizing marginal likelihood evidence computed previous subsection single class overall evidence entire classes denoted obtained summation evidences individual classes given compute overall evidence corresponding deep model choose model maximum evidence transfer learning. expect selected model performs best among candidates veriﬁed experiment. addition ensemble deep cnns needs constructed target task approach selects subset good pre-trained cnns greedy manner. speciﬁcally network largest evidence stage test whether augmented network improves evidence not. network accepted evidence increases rejected otherwise. last candidate tested obtain ﬁnal network combination associated model learned concatenated feature descriptors accepted networks. eigen-decomposition identity matrix. regularized least squares approach clear beneﬁt requires eigendecomposition obtain solution combinations optimization regularized least squares formulation presented equivalent maximization posterior ﬁxed hyperparamters denoted α/β. posterior decomposed terms bayesian theorem illustrated figure neither convex concave illustrated supplementary ﬁle. however show sufﬁcient condition existence ﬁxed point using following theorem. theorem denote update rule binary variable normalized nonnegative vector ﬁxed point. proof. ﬁrst show asymptotically linear bayesian evidence framework discussed section useful identify good transfer learning reasonable regularization parameter. make framework even practical present faster algorithm accomplish goal theory guarantees converges algorithm. figure failure cases aitken’s delta-squared process. ﬁrst case arises initial ﬁxed point results second case occurs approximating line parallel fast bayesian learning algorithm regularized least squares problem summarized algorithm algorithm ﬁrst compute eigen-decomposition time consuming part needs performed since result reused every label that obtain regularization parameter iterative procedure. apply aitken’s delta-squared process potential failure cases figure ﬁrst case often arises initial ﬁxed point second case occurs approximating line parallel fortunately failures rarely happen practice handled easily skipping procedure updating figure demonstrates relative convergence rates three different techniques—aitken’s delta-squared process algorithm ﬁxed point update rules update method aitken’s delta-squared process signiﬁcantly faster others convergence. accelerate ﬁxed point update rule using aitken’s delta-squared process figure illustrates aitken’s delta-squared process. let’s focus points line going features visual recognition problems. used single image scale experiment. liblinear package used training regularization parameters selected grid search cross validations. table presents complete results experiment. bayesian ls-svm competitive terms prediction accuracy even signiﬁcantly reduced training time. training getting slower bayesian lssvm number classes increases particularly slow caltech datasets. another notable observation table order prediction accuracy highly correlated evidence. means selected model bayesian ls-svm produces reliable testing accuracy proper deep learning image representation obtained without time consuming grid search cross validation. note cross validations ls-svm play role less reliable slower bayesian evidence framework. capability select appropriate model corresponding regularization parameter important properties algorithm. show bayesian ls-svm identiﬁes combination multiple cnns improve accuracy withgrid search cross validation. task select subset pre-trained cnns greedy manner; cnns selection evidence increase. algorithm compared decaf zeiler inria kth-s kthft zhang tubfi addition ensembles identiﬁed greedy evidence maximization compared oracle combinations—the ones highest accuracy test found exhaustive search—and best combinations found exhaustive evidence maximization. table presents ensembles approach achieves best performance tasks. identiﬁed ensembles greedy approach consistent selections exhaustive evidence maximization even oracle selections made testing accuracy maximization. note network selections natural reasonable; googlenet-imagenet selected frequently googlenet-place preferred googlenet-imagenet indoor sun- since datasets constructed scene recognition. turns proposed algorithm tends choose networks higher accuracies target task even though makes selections based evidence greedy manner. interesting observation result less figure comparison aitken’s delta-squared process ﬁxed point update rules update rules pascal dataset aitken’s delta-squared process signiﬁcantly faster methods. benchmark datasets involve various visual recognition tasks object recognition photo annotation scene recognition grained recognition visual attribute detection action recognition. table presents characteristics datasets. experiment followed given train test split evaluation measure dataset. datasets bounding annotations cub- uiuc object attribute human attribute stanford actions enlarged bounding boxes consider neighborhood context suggested deep learning representations selected pretrained cnns caffe model googlenet alexnet trained imagenet googlenet trained places generic image representations used dimensional activations ﬁrst fully connected layer alexnet dimensional vector obtained global average pooling layer located right ﬁnal softmax layer googlenet. table characteristics datasets. number training data number test data number classes average number labels image average precision acc. accuracy area curve. object recognition object recognition object recognition object recognition photo annotation scene recognition scene recognition ﬁne-grained recognition ﬁne-grained recognition attribute detection attribute detection action recognition table bayesian ls-svm versus svm. without time consuming cross validation procedure bayesian ls-svm achieves prediction accuracy competitive svm. addition bayesian ls-svm selects proper task using evidence best accuracy ls-svm denotes maximum achievable accuracy test dataset using available learned models. note selected model bayesian evidence framework cross validation best testing. following sets regularization parameters tested cross validation ls-svm respectively consistent selections oracle exhaustive evidence maximization stanford actions dataset googlenet-place seems provide complementary information even accuracy helpful improve table comparison existing methods benchmark datasets. best ensembles identiﬁed maximizing evidence exhaustive search mostly coincide oracle combinations—the ones highest accuracy test also found exhaustive search. ensembles identiﬁed greedy search similar ones exhaustive search methods algorithm consequently performs best many tested datasets. used three scales done simply averaged prediction scores three scales. described simple efﬁcient technique transfer deep models pre-trained speciﬁc image classiﬁcation tasks another tasks. approach based bayesian ls-svm combines bayesian evidence framework least squares loss. addition presented faster ﬁxed point update rule evidence maximization aitken’s delta-squared process. fast bayesian ls-svm demonstrated competitive results compared standard selecting deep model popular visual recognition problems. also achieved state-of-the-art performance identifying good ensemble candidate models bayesian ls-svm framework. work partly supported institute information communications technology promotion grant funded korea government development predictive visual intelligence technology national research foundation korea", "year": 2015}