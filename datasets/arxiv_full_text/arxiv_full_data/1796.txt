{"title": "Neural Probabilistic Model for Non-projective MST Parsing", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "In this paper, we propose a probabilistic parsing model, which defines a proper conditional probability distribution over non-projective dependency trees for a given sentence, using neural representations as inputs. The neural network architecture is based on bi-directional LSTM-CNNs which benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM and CNN. On top of the neural network, we introduce a probabilistic structured layer, defining a conditional log-linear model over non-projective trees. We evaluate our model on 17 different datasets, across 14 different languages. By exploiting Kirchhoff's Matrix-Tree Theorem (Tutte, 1984), the partition functions and marginals can be computed efficiently, leading to a straight-forward end-to-end model training procedure via back-propagation. Our parser achieves state-of-the-art parsing performance on nine datasets.", "text": "paper propose probabilistic parsing model deﬁnes proper conditional probability distribution nonprojective dependency trees given sentence using neural representations inputs. neural network architecture based bi-directional lstmcnns automatically beneﬁts wordcharacter-level representations using combination bidirectional lstms cnns. neural network introduce probabilistic structured layer deﬁning conditional log-linear model nonprojective trees. exploiting kirchhoff’s matrix-tree theorem partition functions marginals computed efﬁciently leading straightforward end-to-end model training procedure back-propagation. evaluate model different datasets across different languages. parser achieves state-of-the-art parsing performance nine datasets. dependency parsing ﬁrst stages deep language understanding gained interest natural language processing community usefulness wide range applications. many systems machine translation entity coreference resolution low-resource languages processing word sense disambiguation becoming sophisticated part utilizing syntactic knowledge dependency trees represent syntactic relationships labeled directed edges heads dependents past years several dependency parsing algorithms proposed whose high performance heavily rely hand-crafted features task-speciﬁc resources costly develop making dependency parsing models difﬁcult adapt languages domains. recently non-linear neural networks recurrent neural networks long-short term memory convolution neural networks input distributed word representations also known word embeddings broadly applied great success problems like part-of-speech tagging named entity recognition utilizing distributed representations inputs systems capable learning hidden information representations directly data instead manually designing hand-crafted features yielding end-to-end models previous studies explored applicability neural representations traditional graph-based parsing models. work replaced linear scoring function traditional models neural networks used margin-based objective model training. work formalized dependency parsing independently selecting head word cross-entropy objective without guarantee general non-projective tree structure output. moreover previous work deriving neural probpaper propose probabilistic neural network-based model non-projective dependency parsing. parsing model uses bi-directional lstm-cnns backbone learn neural information representations probabilistic structured layer constructed conditional log-linear model deﬁning conditional distribution non-projective dependency trees. architecture blstm-cnns similar used sequence labeling tasks cnns encode character-level information word character-level representation blstm models context information word. probabilistic structured output layer negative log-likelihood training objective partition function marginals computed kirchhoff’s matrix-tree theorem process optimization efﬁciently back-propagation. test time parsing trees decoded maximum spanning tree algorithm evaluate model treebanks across different languages achieving state-of-the-art performance treebanks. contributions work summarized proposing neural probabilistic model non-projective dependency parsing. giving empirical evaluations model benchmark data sets languages. achieving stateof-the-art performance parser nine different treebanks. paper following notation represents generic input sentence word. represents generic dependency tree represents syntactic relationships labeled directed edges heads dependents. example figure shows dependency tree sentence economic news discussed dozat manning bi-linear form score function related bilinear attention mechanism bi-linear score function differs traditional score function proposed kiperwasser goldberg adding bi-linear term. similar score function proposed dozat manning difference score function used linear term head words heads modiﬁers. labeled dependency trees edge represented tuple head word modiﬁer respectively label dependency type edge. extend original model labeled dependency parsing extending score function include dependency labels weights bias corresponding dependency label suppose different dependency labels sufﬁces deﬁne adjacency matrix assigning weight edge weights different dependency labels partition function marginals labeled dependency trees obtained operating adjacency matrix time complexity becomes practice probably large. english number edge labels stanford basic dependencies number treebank conll- shared task while average length sentences english penn treebank around thus negligible comparing noticed labeled model different dependency label vector representation word dependency labels distinguished parameters corresponding them. advantage signiﬁcantly reduces memory requirement comparing model dozat manning distinguishes different label previous studies presented variant kirchhoff’s matrix-tree theorem used evaluate partition function marginals efﬁciently. section brieﬂy revisit method. sentence words denote root-symbol. deﬁne complete graph nodes node corresponds word edge corresponds dependency words. then assign non-negative weights edges complete graph nodes yielding weighted adjacency matrix rn+×n+ bi-directional lstm lstm unit. recurrent neural networks powerful family connectionist models widely applied tasks language modeling sequence labeling machine translation capture context information languages. though theory rnns able learn long-distance dependencies practice fail gradient vanishing/exploding problems lstms variants rnns designed cope gradient vanishing problems. basically lstm unit composed three multiplicative gates control proportions information pass forget next time step. blstm. many linguistic structure prediction tasks beneﬁt access past future contexts lstm’s hidden state takes information past knowing nothing future. elegant solution whose effectiveness proven previous work bi-directional lstm basic idea present sequence forwards backwards separate hidden states capture past future information respectively. hidden states concatenated form ﬁnal output. discussed dozat manning advantages apply multilayer perceptron output vectors blstm score function reducing dimensionality overﬁtting model. follow work using one-layer perceptron activation function. blstm-cnns finally construct neural network model feeding output vectors blstm parsing layer. figure illustrates architecture network detail. word figure character embeddings inputs encodes characterlevel representation. character-level representation vector concatenated word embedding vector feed blstm network. enrich word-level information also embeddings. finally output vecfigure convolution neural network extracting character-level representations words. dashed arrows indicate dropout layer applied before character embeddings input cnn. remaining question obtain vector representation word neural network. following subsections describe architecture neural network model representation learning. cnns previous work shown cnns effective approach extract morphological information characters words encode neural representations proven particularly useful out-of-vocabulary words architecture model uses extract character-level representation given word used hovy architecture shown figure following hovy dropout layer applied character embeddings input cnn. hyper-parameter window size number ﬁlters number layers state size initial state peepholes number layers dimension embeddings lstm hidden states lstm layers optimizer initial learning rate decay rate gradient clipping nese dutch english german spanish structured-skipgram embeddings languages polyglot embeddings. dimensions embeddings english chinese languages. weights matrices bias vectors. matrix parameters randomly initialized uniform number rows columns structure bias vectors initialized zero except bias forget gate lstm initialized optimization algorithm parameter optimization performed adam optimizer choose initial learning rate learning rate adapted using schedule learning rate annealed figure main architecture parsing model. character representation word computed figure character representation vector concatenated word embedding feeding blstm network. dashed arrows indicate dropout layers applied input hidden output vectors blstm. tors neural netwok parsing layer jointly parse best dependency tree. shown figure dropout layers applied input hidden output vectors blstm using form recurrent dropout proposed ghahramani section provide details implementing training neural parsing model including parameter initialization model optimization hyper parameter selection. multiplying ﬁxed decay rate epochs respectively. used trained networks total epochs. adam optimizer automatically adjusts global learning rate according past gradient magnitudes additional decay consistently improves model performance across settings languages. reduce effects gradient exploding gradient clipping explored optimization algorithms stochastic gradient descent momentum adadelta rmsprop none meaningfully improve upon adam learning rate annealing preliminary experiments. dropout training. mitigate overﬁtting apply dropout method regularize model. shown figure apply dropout character embeddings inputting input hidden output vectors blstm. apply dropout rate embeddings. blstm recurrent dropout dropout rate hidden states layers. found model using recurrent dropout converged much faster standard dropout achiving similar performance. hyper-parameter selection table summarizes chosen hyper-parameters experiments. tune hyper-parameters development sets random search. hyper-parameters across models different treebanks languages time constrains. note -layer blstm followed -layer mlp. state size lstm dimension tuning parameters signiﬁcantly impact performance model. evaluate neural probabilistic parser data setup kuncoro namely english penn treebank penn chinese treebank german conll corpus following previous work experiments evaluated metrics unlabeled attachment score labeled attachment score ﬁrst construct experiments dissect effectiveness input information neural network architecture ablation studies. compare performance four versions model different inputs basic +pos +char full basic model utilizes pretrained word embeddings inputs +pos +char models augments basic embedding character information respectively. according results shown table +char model obtains better performance basic model three languages showing character-level representations important dependency parsing. second english german +char +pos achieves comparable performance chinese +pos signiﬁcantly outperforms +char model. finally full model achieves best accuracy english german chinese +pos obtains best. thus guess information useful system bohnet nivre chen manning ballesteros dyer kiperwasser goldberg graph ballesteros wang chang zhang cheng andor kuncoro dozat manning work basic work +char work +pos work full table gives performance parsers trained different objective functions cross-entropy objective word objective based likelihood entire tree. parser global likelihood objective outperforms simple crossentropy objective demonstrating effectiveness global structured objective. table illustrates results four versions model three languages together twelve previous top-performance systems comparison. full model signiﬁcantly outperforms graph-based parser proposed kiperwasser goldberg used similar neural network architecture representation learning moreover model achieves better results parser distillation method three languages. results parser slightly worse scores reported dozat manning possible reason that mentioned section labeled dependency parsing dozat manning used different vectors different dependency labels represent word making model require much memory ours. experiments conll treebanks datasets. make thorough empirical comparison previous studies also evaluate system treebanks conll shared task dependency parsing english treebank conll- shared task treebanks conll- shared task treebanks conll- shared task following cheng randomly select training data development set. evaluated using ofﬁcial scorer conll- shared task. baselines. compare model third-order turbo parser low-rank tensor based model randomized greedy inference based model labeled dependency parser inner-to-outer greedy decoding algorithm bi-direction attention based parser also compare parser best published results individual languages. comparison includes four additional systems martins zhang mcdonald pitler mcdonald table treebanks conll shared tasks together several state-of-the-art parsers. best published includes accurate parsers term among martins martins zhang zhang mcdonald pitler mcdonald hovy cheng results. table summarizes results model along state-of-the-art baselines. average across languages approach signiﬁcantly outperforms baseline systems. noted average parser languages better best published different systems achieved best results different languages. individual languages parser achieves state-of-the-art performance languages bulgarian chinese czech dutch english german japanese spanish. arabic danish portuguese slovene swedish parser obtains best las. aninteresting observation full model outperforms +pos model languages. exception chinese matches observation section recent years several different neural network based models proposed successfully applied dependency parsing. among neural models three approaches similar model graphbased parsers blstm feature representation neural bi-afﬁne attention parser learn sentence segment embedding based extra forward lstm network. parsers trained parsing models optimizing margin-based objectives. three main differences models ours. first used linear form score function instead using bi-linear term vectors heads modiﬁers. second employ cnns model character-level information. third proposed probabilistic model non-projective trees neural representations trained models margin-based objective. dozat manning proposed neural parsing model using bi-afﬁne score function similar bi-linear form score function model. model mainly differ model using model character-level information. moreover model formalized dependency parsing independently selecting head word cross-entropy objective probabilistic parsing model jointly encodes decodes parsing trees given sentences. paper proposed neural probabilistic model non-projective dependency parsing using blstm-cnns architecture representation learning. experimental results treebanks across languages show parser signiﬁcantly improves accuracy dependency structures edge labels several previously state-of-the-art systems. research supported part darpa grant fa--- funded deft program. opinions ﬁndings conclusions recommendations expressed material authors necessarily reﬂect views darpa. references rami al-rfou bryan perozzi steven skiena. polyglot distributed word representations proceedings conllmultilingual nlp. soﬁa bulgaria pages daniel andor chris alberti david weiss aliaksei severyn alessandro presta kuzman ganchev slav petrov michael collins. globally norpromalized transition-based neural networks. ceedings acl- berlin germany pages miguel ballesteros chris dyer noah smith. improved transition-based parsing modeling characters instead words lstms. proceedings emnlp-. lisbon portugal pages miguel ballesteros yoav goldberg chris dyer noah smith. training exploration improves greedy stack lstm parser. proceedings emnlp-. austin texas pages bernd bohnet joakim nivre. transitionbased system joint part-of-speech tagging labeled non-projective dependency parsing. proceedings emnlp-. jeju island korea pages kyunghyun bart merri¨enboer dzmitry bahdanau yoshua bengio. properties neural machine translation encoder-decoder approaches. arxiv preprint arxiv. ronan collobert jason weston l´eon bottou michael karlen koray kavukcuoglu pavel kuksa. natural language processing scratch. journal machine learning research yann dauphin harm vries junyoung chung yoshua bengio. rmsprop equilibrated adaptive learning rates non-convex optimization. arxiv preprint arxiv. marie-catherine marneffe bill maccartney christopher manning generating typed dependency parses phrase structure parses. proceedings lrec-. pages chris dyer miguel ballesteros wang ling austin matthews noah smith. transitionbased dependency parsing stack long shortterm memory. proceedings acl- beijing china pages nicolas fauceglia yiu-chang xuezhe eduard hovy. word sense disambiguation propstore ontonotes event mention detecproceedings workshop tion. events deﬁnition detection coreference representation. denver colorado pages yarin zoubin ghahramani. theoretically grounded application dropout recurrent neural networks. advances neural information processing systems. xavier glorot yoshua bengio. understanding difﬁculty training deep feedforward neural networks. international conference artiﬁcial intelligence statistics. pages hajiˇc massimiliano ciaramita richard johansson daisuke kawahara maria ant`onia mart´ı llu´ıs m`arquez adam meyers joakim nivre sebastian pad´o ˇstˇep´anek conll- shared task syntactic semantic dependencies proceedings conllmultiple languages. shared task. pages eliyahu kiperwasser yoav goldberg. simple accurate dependency parsing using bidirectransactions tional lstm feature representations. association computational linguistics terry amir globerson xavier carreras michael collins. structured prediction models matrix-tree theorem. proceedings emnlp-. prague czech republic pages terry alexander rush michael collins tommi jaakkola david sontag. dual decomposition parsing non-projective head proceedings emnlp-. camautomata. bridge pages adhiguna kuncoro miguel ballesteros lingpeng kong chris dyer noah smith. distilling ensemble greedy dependency parsers proceedings emnlpone parser. austin texas pages wang ling chris dyer alan black isabel trancoso. two/too simple adaptations proceedings wordvec syntax problems. naacl-. denver colorado pages thang luong hieu pham christopher manning. effective approaches attentionbased neural machine translation. proceedings emnlp-. lisbon portugal pages xuezhe eduard hovy. efﬁcient inner-toouter greedy algorithm higher-order labeled dependency parsing. proceedings emnlp-. lisbon portugal pages xuezhe xia. unsupervised dependency parsing transferring distribution parproallel guidance entropy regularization. ceedings acl-. baltimore maryland pages andre martins noah smith mario figueiredo pedro aguiar. dual decomposition proceedings many overlapping components. emnlp-. edinburgh scotland pages ryan mcdonald joakim nivre yvonne quirmbachbrundage yoav goldberg dipanjan kuzman ganchev keith hall slav petrov zhang oscar t¨ackstr¨om claudia bedini n´uria bertomeu castell´o jungmee lee. universal dependency annotation multilingual parsing. proceedings acl-. soﬁa bulgaria pages xuezhe yingkai zhiting yaoliang yuntian deng eduard hovy. dropout expectation-linear regularization. proceedings international conference learning representations toulon france. ryan mcdonald fernando pereira kiril ribarov hajic. non-projective dependency parsing using spanning tree algorithms. proceedings hlt/emnlp-. vancouver canada pages tomas mikolov martin karaﬁ´at lukas burget cernock`y sanjeev khudanpur. recurrent neural network based language model. interspeech. volume page vincent supervised noun phrase coreference research ﬁrst ﬁfteen years. proceedings acl-. association computational linguistics uppsala sweden pages nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research mihai surdeanu richard johansson adam meyers llu´ıs m`arquez joakim nivre. conll shared task joint parsing syntactic semantic dependencies. proceedings conll. pages", "year": 2017}