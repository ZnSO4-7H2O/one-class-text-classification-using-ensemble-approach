{"title": "Convergence Analysis of Gradient Descent Algorithms with Proportional  Updates", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "The rise of deep learning in recent years has brought with it increasingly clever optimization methods to deal with complex, non-linear loss functions. These methods are often designed with convex optimization in mind, but have been shown to work well in practice even for the highly non-convex optimization associated with neural networks. However, one significant drawback of these methods when they are applied to deep learning is that the magnitude of the update step is sometimes disproportionate to the magnitude of the weights (much smaller or larger), leading to training instabilities such as vanishing and exploding gradients. An idea to combat this issue is gradient descent with proportional updates. Gradient descent with proportional updates was introduced in 2017. It was independently developed by You et al (Layer-wise Adaptive Rate Scaling (LARS) algorithm) and by Abu-El-Haija (PercentDelta algorithm). The basic idea of both of these algorithms is to make each step of the gradient descent proportional to the current weight norm and independent of the gradient magnitude. It is common in the context of new optimization methods to prove convergence or derive regret bounds under the assumption of Lipschitz continuity and convexity. However, even though LARS and PercentDelta were shown to work well in practice, there is no theoretical analysis of the convergence properties of these algorithms. Thus it is not clear if the idea of gradient descent with proportional updates is used in the optimal way, or if it could be improved by using a different norm or specific learning rate schedule, for example. Moreover, it is not clear if these algorithms can be extended to other problems, besides neural networks. We attempt to answer these questions by establishing the theoretical analysis of gradient descent with proportional updates, and verifying this analysis with empirical examples.", "text": "rise deep learning recent years brought increasingly clever optimization methods deal complex non-linear loss functions methods often designed convex optimization mind shown work well practice even highly non-convex optimization associated neural networks. however signiﬁcant drawback methods applied deep learning magnitude update step sometimes disproportionate magnitude weights leading training instabilities vanishing exploding gradients idea combat issue gradient descent proportional updates. gradient descent proportional updates introduced independently developed algorithm) abu-el-haija basic idea algorithms make step gradient descent proportional current weight norm independent gradient magnitude objective function optimized. norm lars norm percentdelta algorithms apply update independently layer. mentioned before attempts alleviate fact magnitude gradients reliable step size indicator replacing magnitude current weight vector leading proportional updates. common context optimization methods prove convergence derive regret bounds assumption lipschitz continuity convexity however even though lars percentdelta shown work well practice theoretical analysis convergence properties algorithms. thus clear idea gradient descent proportional updates used optimal could improved using different norm speciﬁc learning rate schedule example. moreover clear algorithms extended problems besides neural networks. project attempt answer questions establishing theoretical analysis gradient descent proportional updates verifying analysis empirical examples. ﬂurry activity deep learning recent years kicked results presented krizhevsky since then research done improve fundamental deep learning techniques typically focused areas changes network architecture changes training mechanism. latter includes improved regularization initialization schemes optimization techniques known play signiﬁcant role performance deep networks practice alexnet achieved excellent results using stochastic gradient descent momentum known face difﬁculties optimizing certain types problems also known perform poorly badly tuned learning rates decay schedules number gradient-based optimization methods proposed remedy using adaptive learning rates. include rmsprop adagrad adadelta adam although techniques interesting theoretical properties often improve training stability deep architectures practice popular optimization method remains momentum tends outperform methods terms accuracy given enough training epochs. explained earlier papers relevant introduced idea gradient descent proportional updates. methods additional advantage traditional adaptive learning rate methods like mentioned learning rate computed layer instead weight leading improved stability. formalize test proportional update method lars assumes norm used update rule start analyzing functions operating -dimensional space since convergence analysis greatly simpliﬁed case. using insights found derivations present convergence analysis lars general functions section gain intuition performance lars algorithm start analyzing simplest quadratic case -dimensional space assume constant throughout iterations yielding following update rule thus we’ve shown reach desired interval number steps stay interval ever after. however exactly optimum speciﬁc points doesn’t reach optimum exactly keep oscillating interval without convergence. thus order reach \u0001-convergence need make interval \u0001-small means origin ﬁxed attractive point iterates situation. intuitively surprising lars iterates never change sign since next point proportional current point coefﬁcient making smaller smaller moving direction origin. overcome problem switching usual stochastic gradient descent vicinity origin predeﬁned follow strategy experiments although modifying lars avoid problem natural direction future research. somewhat surprisingly analysis presented previous sections directly applied differentiable convex function indeed lars updates depend sign gradient thus function minw update equation suggests general function convergence analysis carried without assuming anything except convexity differentiability. particular suggests analysis presented next sections improved relaxing lipschitz gradient assumption. section present convergence analysis general convex functions lipschitz gradient. seen -dimensional case lars algorithm ﬁxed learning rate converge region around optimal point depends norm optimal solution learning rate establish similar result general convex differentiable function lipschitz gradient. ﬁrst prove lars updates never diverge inﬁnity given learning rate sufﬁciently small. using result show size convergence region becomes smaller decrease thus \u0001-convergence guaranteed setting small enough. finally show lars algorithm least sublinear convergence rate ﬁxed learning rate converges standard assumptions learning rate decay schedule. might possible improve established convergence rate using precise lower bounds. section show given sufﬁciently small learning rate lars iterates never diverge inﬁnity meaning lars always converges region around optimal point. statement intuitive since order lars diverge inﬁnity either follow mostly direction inﬁnite number iterations alternate directions inﬁnitely increasing step sizes. ﬁrst possible since update direction always acute angle direction towards optimum. thus lars passes optimum reverse direction. step sizes stay ﬁnite since norm next point proportional norm current point cannot consistently increasing given lars periodically reverse update direction. even though statement intuitively clear able explicitly construct convergence region possible -dimensional case. instead conducted non-constructive proof showing points next lars step farther away optimum current step bounded. formally proved following lemma lemma strongly convex function lipschitz gradient following bounded i.e. supw∈s providing small enough minw reason need strong convexity show direction towards optimum current update direction cannot become orthogonal seems intuitive similar results hold general case however able prove directly. state following conjecture conjecture convex function lipschitz gradient which true mean following results hold without strong convexity assumptions. conclude section following theorem theorem strongly convex function lipschitz gradient lars sequence updates ﬁxed learning rate diverge inﬁnity i.e. seen section lars algorithm converge iterative sequence needs close origin. thus order show convergence region shrinks decrease learning rate need assume situation never happens. given assumption result theorem prove following theorem suppose function strongly convex differentiable gradient lipschitz continuous constant additional assumption mink lars sequence updates ﬁxed learning rate satisﬁes following inequality notice ﬁrst term bound goes zero means lars iterates converge region around optimum size shrinking decrease thus making small enough keeping track minimal value achieve \u0001-convergence summarized following corollary conditions theorem lars least sublinear \u0001-convergence rate convergence decaying learning rate finally let’s establish convergence following result holds theorem assumptions theorem lars converges optimal value decaying learning rate sequence satisfying even though result establishes convergence decaying learning rate schedule cannot used improve convergence rate established corollary current analysis let’s summarize obtained results. shown lars algorithm ﬁxed learning rate converges area around optimal value made arbitrarily small decreasing learning rate. provide empirical justiﬁcation claim conducted simple dimensional experiment quadratic function. results comparing lars gradient descent presented figure guarantee \u0001-convergence learning rate needs decreased convergence region becomes small. following strategy able ones used theorems another potential advantage lars gradient descent might possible show convergence less strict assumptions since case algorithm perform exactly iterates convex differentiable function position minimum. thus likely lipschitz gradient assumption aforementioned analysis relaxed might imply lars perform better gradient descent ill-conditioned problems. conﬁrm aforementioned claims experimentally showing lars performs comparable optimization algorithms three simple benchmarks. figure simple experiments comparing lars ﬁxed learning rate gradient descent ﬁxed learning rate. plots show lars converge faster since update approximately size. however converges certain region around optimal point. plots show convergence region increases increase learning rate. also clear symmetric around optimal point consistent analysis presented section implemented lars tensorflow using standard optimizer extension. speciﬁcally reuse built-in momentumoptimizer apply lars-speciﬁc logic using opt.apply_gradients method. tensorflow weights biases network represented tensorflow variables. lars tensorflow variable given momentumoptimizer momentum maintained independently still computing gradients variable update. additional tweaks mentioned lars algorithm first introduced small constant epsilon term denominator local learning rate calculation improves numerical stability close optimal value. second norm variable close zero switch discussed section figure comparison convergence adam lars convex problems. lars decay refers lars decaying learning rate schedule. note epoch indicates training conducted single batch. problem involves simple linearly separable data tackled using linear svm. hinge loss minimized using tensorﬂow’s gradientdescentoptimizer adamoptimizer along custom tensorﬂow implementation lars. experiments mini-batch full-batch version optimizers. note mini-batch version batches size used. tuning done decide learning rates three algorithms using adam using lars using graph comparing convergence three algorithms shown left side fig. note three algorithms converge ﬁnal optimum lars tends converge much quicker two. initial spike loss likely initialized weights relatively high norm. convergence pattern similar across full-batch mini-batch versions former lower variance expected. also indicates analysis full-batch well stochastic variants lars improved match convergence rates standard optimization methods. also note lars appears oscillate near optimum towards training. described earlier fact lars guarantee convergence area around optimum dependant learning rate. decaying learning rate schedule starts goes training lars convergences exactly optimum adam indicated yellow line fig. second convex problem experimented logistic regression trained classify mnist digits. full-batch mini-batch versions tested again. mini-batch versions large batch size tuned learning rates three algorithms adam lars using respectively. results shown right side fig. lars appears outperform adam terms epochs needed reach optimum slower converge sgd. three converge ﬁnal optimum. similar case lars ﬁxed learning rate appears deviating away optimum towards training. ﬁxed using decaying learning rate schedule. examples illustrate decaying learning rate plays vital role performance lars traditional optimizers like adam. tensorflow includes resnet model trains cifar- dataset.the change made tensorflow code swap optimizer. resnet model trained dedicated nvidia titan memory. three optimizers achieved similar average training throughput. single train step took average milliseconds milliseconds adam milliseconds lars. lars performed similarly lars achieving slightly better performance adam initially performed best ﬁrst train steps fell behind lars that. loss function optimizer similar trends. figure train error loss function comparison adam lars resnet model cifar- dataset tuned initial learning rates respectively. optimizers batch size demonstrated lars guaranteed converge standard decaying learning rate schedule assumptions. even though theoretical results indicate lars might converge slower methods empirically observe reverse effect stochastic full-batch cases. thus theoretical derivations likely improved ﬁnding tighter bounds relaxing certain assumptions. future hope accomplish this well analyze variants lars involving stochastic updates momentum updates. would like thank aarti singh pradeep ravikumar teaching convex optimization. finally would like thank yang boris ginsburg nvidia initial work lars.", "year": 2018}