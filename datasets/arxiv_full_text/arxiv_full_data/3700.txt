{"title": "Understanding Deep Neural Networks with Rectified Linear Units", "tag": ["cs.LG", "cond-mat.dis-nn", "cs.AI", "cs.CC", "stat.ML"], "abstract": "In this paper we investigate the family of functions representable by deep neural networks (DNN) with rectified linear units (ReLU). We give an algorithm to train a ReLU DNN with one hidden layer to *global optimality* with runtime polynomial in the data size albeit exponential in the input dimension. Further, we improve on the known lower bounds on size (from exponential to super exponential) for approximating a ReLU deep net function by a shallower ReLU net. Our gap theorems hold for smoothly parametrized families of \"hard\" functions, contrary to countable, discrete families known in the literature. An example consequence of our gap theorems is the following: for every natural number $k$ there exists a function representable by a ReLU DNN with $k^2$ hidden layers and total size $k^3$, such that any ReLU DNN with at most $k$ hidden layers will require at least $\\frac{1}{2}k^{k+1}-1$ total nodes. Finally, for the family of $\\mathbb{R}^n\\to \\mathbb{R}$ DNNs with ReLU activations, we show a new lowerbound on the number of affine pieces, which is larger than previous constructions in certain regimes of the network architecture and most distinctively our lowerbound is demonstrated by an explicit construction of a *smoothly parameterized* family of functions attaining this scaling. Our construction utilizes the theory of zonotopes from polyhedral theory.", "text": "paper investigate family functions representable deep neural networks rectiﬁed linear units give algorithm train relu hidden layer global optimality runtime polynomial data size albeit exponential input dimension. further improve known lower bounds size approximating relu deep function shallower relu net. theorems hold smoothly parametrized families hard functions contrary countable discrete families known literature. example consequence theorems following every natural number exists function representable relu hidden layers total size relu hidden layers require least total nodes. finally family dnns relu activations show lowerbound number afﬁne pieces larger previous constructions certain regimes network architecture distinctively lowerbound demonstrated explicit construction smoothly parameterized family functions attaining scaling. construction utilizes theory zonotopes polyhedral theory. deep neural networks provide excellent family hypotheses machine learning tasks classiﬁcation. neural networks single hidden layer ﬁnite size represent continuous function compact subset arbitrary well. universal approximation result ﬁrst given cybenko sigmoidal activation function later generalized hornik arbitrary bounded nonconstant activation function hornik furthermore neural networks ﬁnite dimension therefore learnable using sample size polynomial size networks anthony bartlett however neural networks based methods shown computationally hard learn mixed empirical success. consequently dnns fell favor late recently resurgence dnns advent deep learning lecun deep learning loosely speaking refers suite computational techniques developed recently training dnns. started work hinton gave empirical evidence dnns initialized properly good solutions reasonable amount runtime. work soon followed series early successes deep learning signiﬁcantly improving state-of-the-art speech recognition hinton since then deep learning received immense attention machine learning community several state-of-the-art systems speech recognition image classiﬁcation natural language processing based deep neural nets hinton dahl krizhevsky sutskever less evidence pre-training actually helps several solutions since forth ∗department computer science email aroracs.jhu.edu †department applied mathematics statistics email basu.amitabhjhu.edu ‡department computer science email mianjyjhu.edu §department applied mathematics statistics email amukhejhu.edu address issue efﬁciently training dnns. include heuristics dropouts srivastava also considering alternate deep architectures convolutional neural networks sermanet deep belief networks hinton deep boltzmann machines salakhutdinov hinton addition deep architectures based non-saturating activation functions suggested effectively trainable successful widely popular rectiﬁed linear unit activation i.e. max{ focus study paper. paper formally study deep neural networks rectiﬁed linear units; refer deep architectures relu dnns. work inspired recent attempts understand reason behind successes deep learning terms structure functions represented dnns telgarsky kane williams shamir well efforts tried understand non-convex nature training problem dnns better kawaguchi haeffele vidal investigation function space represented relu dnns also takes inspiration classical theory circuit complexity; refer reader arora barak shpilka yehudayoff jukna saptharishi allender various surveys deep fascinating ﬁeld. particular results inspired results like ones hastad hastad razborov razborov smolensky smolensky show strict separation complexity classes. make progress towards similar statements deep neural nets relu activation. extend relu activation function vectors entry-wise operation denote class afﬁne linear transformations respectively. deﬁnition number hidden layers input output dimensions rwk+ relu given specifying sequence natural numbers representing widths hidden layers afﬁne transformations rwi− linear transformation rwk+ corresponding weights hidden layers. relu called -layer relu said hidden layers. function computed denotes function composition. depth relu deﬁned width relu max{w wk}. size relu deﬁnition denote class rwk+ relu dnns hidden layers widths {wi}k f{wi}k+ deﬁnition function continuous piecewise linear exists ﬁnite polyhedra whose union afﬁne linear polyhedron number pieces number maximal connected subsets afﬁne linear functions representable relu dnns. moreover show structural properties relu dnns speciﬁcally depth width affects expressive power. clear deﬁnition function. follows show converse also true function representable relu dnn. particular following theorem establishes one-to-one correspondence class relu dnns functions. proof sketch clear function represented relu function. converse ﬁrst note function represented linear combination piecewise linear convex functions. formally theorem every since function form maxi∈sj piecewise linear convex function pieces equation says continuous piecewise linear function obtained linear combination piecewise linear convex functions afﬁne pieces. furthermore lemmas appendix show composition addition pointwise maximum functions also representable relu dnns. particular lemma note max{x implementable layer relu network construction inductive manner show maximum numbers computed using relu depth log. theorem gives upper bound depth networks needed represent continuous piecewise linear functions give tight bounds size networks needed represent given piecewise linear function. give tight bounds size follows theorem given piecewise linear function pieces exists -layer nodes represent moreover -layer represents size least finally main result section follows theorem well-known facts piecewise linear functions dense family compactly supported continuous functions family compactly supported continuous functions dense theorem every function arbitrarily well-approximated |f|q)/q) relu function hidden layers. moreover function arbitrarily well-approximated -layer tight bounds size terms approximation. proofs theorems provided appendix would like remark weaker version theorem observed along universal approximation theorem similar theorem authors goodfellow also used previous result wang obtaining result. subsequent work boris hanin among things found width depth upper bound relu representation positive functions width upperbound general positive functions convex positive functions. convex positive functions depth upper bound sharp disallow dead relus. success deep learning largely attributed depth networks i.e. number successive afﬁne transformations followed nonlinearities shown extracting hierarchical features data. contrast traditional machine learning frameworks including support vector machines generalized linear models kernel machines seen instances shallow networks linear transformation acts single layer nonlinear feature extraction. section explore importance depth relu dnns. particular section best knowledge ﬁrst explicit construction relu functions whose number afﬁne pieces grows exponentially input dimension. proofs theorems section provided appendix circuit lower bounds relu dnns section concerned relu dnns i.e. input output theorem every pair natural numbers exists family hard functions representable -layer relu width also representable )-layer relu )-layer relu size least theorem pair neural nets considered telgarsky theorem telgarsky depths purpose approximation −norm would size lower bound shallower scales exponentially larger lower bound telgarsky scenario. telgarsky’s family hard functions parameterized single natural number contrast show every pair natural numbers point equation exists hard function represented depth network extra ﬂexibility choosing parameter would need size least purpose showing gaps representation ability deep nets shows size lower bounds super-exponential depth explained corollaries characteristic feature hard functions boolean circuit complexity usually countable family functions smooth family hard functions. fact last section telgarsky telgarsky states weakness state-of-the-art results hard functions boolean circuit complexity neural nets research. contrast provide smoothly parameterized family hard functions section continuum hard functions wasn’t demonstrated work. point telgarsky’s results apply deep neural nets host different activation functions whereas results speciﬁcally neural nets rectiﬁed linear units. sense telgarsky’s results general results paper weaker guarantees. eldan-shamir exponential number nodes approximated within constant -layer dnn. results immediately comparable telgarsky’s results interesting open question extend results constant depth hierarchy statement analogous recent result rossman also note last years much effort community show size lowerbounds relu dnns trying approximate various classes functions necessarily exactly representable relu dnns continuum hard functions measure complexity family hard functions represented relu dnns asymptotics number pieces function dimension depth size relu dnns. precisely suppose family functions every family contains least function representable relu depth maximum width following deﬁnition formalizes notion complexity deﬁnition deﬁned maximum number pieces function represented relu comp respectively. section would explain precise sense improve numbers. analysis complexity measure done using integer programming techniques left speciﬁc hard function induced norm zonotope note case function seen composition -norm γz). middle typical hard function zonotope note increasing number zonotope generators makes function note increasing depth make function complex. deﬁnition denote zonotope subset whose complement zero lebesgue measure rnm. lemma given exists -layer relu size represents function deﬁnition deﬁne function piecewise linear segments deﬁned follows linear continuation piece interval note function pieces leftmost piece slope furthermore denote composition functions firstly note construction requires hidden layers width least input dimensionality contrast impose restrictions network size construction independent input dimensionality. thus result probes networks bottleneck architectures whose complexity cant seen result. secondly terms complexity measure seem regimes bound better. regime example setting construction thirdly clear whether construction gives smoothly parameterized family functions introducing small perturbations construction paper. contrast smoothly parameterized family one-to-one correspondence well-understood manifold like higher-dimensional torus. convex loss function hinge loss function given max{ yy}). main result section gives algorithm solve empirical risk minimization problem global optimality. theorem exists algorithm global optimum problem time onwpoly). note running time onwpoly) polynomial data size ﬁxed proof sketch full proof theorem included appendix provide sketch proof. empirical risk minimization problem viewed optimization problem space weights relu nonconvex quadratic problem. however instead search space functions representable -layer dnns writing form similar breaks problem parts combinatorial search convex problem essentially linear regression linear inequality constraints. enables guarantee global optimality. algorithm implements empirical risk minimization rule training relu hidden layer. best knowledge known algorithm solves problem global optimality. note known hardness results exponential dependence input dimension unavoidable blum rivest shalev-shwartz bendavid algorithm runs time polynomial number data points. best knowledge hardness result known rules empirical risk minimization deep nets time polynomial circuit size data size. thus training result step towards resolving complexity literature. related result improperly learning relus recently obtained goel contrast algorithm returns relu class learned. another difference result considers notion reliable learning opposed empirical risk minimization objective considered discussion running time algorithm give work exact global minima layer relu-dnn exponential input dimension number hidden nodes exponential dependence removed unless shalev-shwartz ben-david blum rivest dasgupta however aware complexity results would rule possibility algorithm trains global optimality time polynomial data size and/or number hidden nodes assuming input dimension ﬁxed constant. resolving dependence network size would another step towards clarifying theoretical complexity training relu dnns good open question future research opinion. perhaps even better breakthrough would optimal training algorithms dnns hidden layers seems like substantially harder crack. would also signiﬁcant breakthrough results consecutive constant depths logarithmic constant depths. would like thank christian tjandraatmadja pointing subtle error previous version paper affected complexity results number linear regions constructions section anirbit would like thank ramprasad saptharishi piyush srivastava rohit gurjar extensive discussions boolean arithmetic circuit complexity. paper immensely inﬂuenced perspectives gained extremely helpful discussions. amitabh basu gratefully acknowledges support grant cmmi. raman arora supported part bigdata grant iis-. george dahl tara sainath geoffrey hinton. improving deep neural networks lvcsr using rectiﬁed linear units dropout. ieee international conference acoustics speech signal processing ieee geoffrey hinton deng dong george dahl abdel-rahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara sainath deep neural networks acoustic modeling speech recognition shared views four research groups. ieee signal processing magazine alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems quoc building high-level features using large scale unsupervised learning. ieee international conference acoustics speech signal processing ieee yann lecun yoshua bengio geoffrey hinton. deep learning. nature shiyu liang srikant. deep neural networks function approximation? jiri matousek. lectures discrete geometry volume springer science business media razvan pascanu guido montufar yoshua bengio. number response regions deep feed forward networks piece-wise linear activations. arxiv preprint arxiv. maithra raghu poole kleinberg surya ganguli jascha sohl-dickstein. exbenjamin rossman rocco servedio li-yang tan. average-case depth hierarchy theorem boolean circuits. foundations computer science ieee annual symposium ieee h.l. royden p.m. fitzpatrick. real analysis. prentice hall itay safran ohad shamir. depth-width tradeoffs approximating natural functions neural saptharishi. survey lower bounds arithmetic circuit complexity pierre sermanet david eigen xiang zhang michael mathieu fergus yann lecun. overfeat integrated recognition localization detection using convolutional networks. international conference learning representations arxiv preprint arxiv. tions. foundations trends theoretical computer science roman smolensky. algebraic methods theory lower bounds boolean circuit complexity. proceedings nineteenth annual symposium theory computing nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research expressing piecewise linear functions using relu dnns proof theorem continuous piecewise linear function pieces speciﬁed three pieces information slope left piece coordinates −tuple {}m− right) slope rightmost piece. tuple uniquely speciﬁes piecewise linear function vice versa. given tuple construct -layer computes piecewise linear function. notes function equal max{−|t| implemented -layer relu size parameters called slopes function called breakpoint function.if write given piecewise linear function functions form lemma would done. turns decomposition piece function ﬂaps always arranged breakpoints ﬂaps contained breakpoints first observe adding constant function change complexity relu expressing since corresponds bias output node. thus assume value last break point single function form slope breakpoint functions form slopes breakpoints respectively. thus wish express gm−. decomposition would valid values slope slope gm−. corresponds asking existence solution following simultaneous linear equations better terms size rightmost piece given function i.e. case means thus decomposition size similar construction done gives following statement useful constructing forthcoming hard functions. corollary rightmost leftmost piece piecewise linear function slope compute piece function using -layer size proof theorem since piecewise linear function representable relu wise linear functions dense space deﬁnition piecewise linear function pieces leftmost piece slope thus corollary represented -layer relu size using lemma ha...ak represented layer size fact hidden layer exactly nodes. thus representable relu width depth distance triangles unit height. p-piecewise linear function breakpoints interval triangles afﬁne. following demonstrate figure pieces range bottom pieces range dotted line bottom panel corresponds function panel. shows every piece dotted graph full copy graph middle panel. exact empirical risk minimization proof theorem convex loss function given data points. stated problem requires afﬁne transformation linear transformation minimize empirical loss stated note given matrix rw×n vector similarly represented vector denote i-th matrix write make following observation. given data point i-th term contribute loss function data point thus every data point i∈sj particular given expression right hand side reduces linear function ˜bi. ﬁxed sets induce partition data observe parts. particular deﬁne partition also induced hyperplane given linear regression constraint regression’s decision variables induce guessed partition. formally algorithm following. algorithm guesses partition data hyperplane. label partitions follows vector {+−}w algorithm ﬁxed selection partitions decision variables). feasible region optimization given constraints imposed thus total constraints. subject yj). assuming loss function convex function ﬁrst argument objective convex function. thus minize convex objective subject linear inequality constraints ﬁnally count many possible partitions vectors algorithm search through. well-known matousek total number possible hyperplane whenever thus guess total partitions. vectors +}w. gives total wdnw guesses partitions vectors guess convex optimization problem decision variables constraints solved time poly. putting everything together running time claimed statement. holds similar algorithm designed uses characterization achieved theorem convex loss function given data points. using theorem solve problem sufﬁces piecewise linear function pieces minimizes total loss. observation ﬁtting piecewise linear functions minimize loss step away linear regression special case function contrained exactly afﬁne linear piece. algorithm ﬁrst guess optimal partition data points points class partition correspond afﬁne piece linear regression class partition. altenatively think guessing interval data points breakpoints piecewise linear function linear regression breakpoints. formally parametrize piecewise linear functions pieces slope-intercept values different pieces. means breakpoints function given aj+x ﬁrst last pieces respectively. deﬁne -tuples natural numbers given ﬁxed tuple wish search piecewise linear functions whose breakpoints order appear intervals deﬁne also }w−. following interpretation aj+. every requiring piecewise linear function respects conditions imposed easily seen equivalent imposing following linear inequalities parameters right hand side equation problem minimizing convex objective subject linear constraints. solve need simply solve problem convex optimization problems taking time therefore total running time owpoly). lemma functions represented relu dnns depths size function deﬁned max{f represented relu depth max{k size similarly function min{f represented relu depth max{k size proof. prove induction base case trivial. consider fm}. induction hypothesis represented relu dnns depths respectively max{k respectively. sizes therefore function given implemented relu depth max{k size show represent function deﬁned max{x |x−y| fact lemma proof. simply fact right hand side represented -layer relu size using lemma lemma function represented relu depth widths hidden layers. function pieces. proof. prove induction base case -layer relu dnn. since every activation node produce breakpoint piecewise linear function breakpoints i.e. pieces. induction step assume relu depth widths hidden layers produces pieces. consider relu depth widths hidden layers. observe input node last layer output relu depth widths induction hypothesis input node last layer piecewise linear function pieces. apply activation function max{ output node twice number pieces original piece intersected x-axis; figure thus going layer take afﬁne combination functions pieces. therefore ··w·. .·wk)·wk+ pieces equal k··w·. .·wk·wk+ induction step completed. lemma piecewise linear function pieces. represented kp/k conversely piecewise relu depth must size least linear function represented relu depth size means setting p/k. ﬁrst implies statement follows. second statement follows using am-gm inequality again time restriction", "year": 2016}