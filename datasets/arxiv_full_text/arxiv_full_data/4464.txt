{"title": "What Makes Good Synthetic Training Data for Learning Disparity and  Optical Flow Estimation?", "tag": ["cs.CV", "stat.ML"], "abstract": "The finding that very large networks can be trained efficiently and reliably has led to a paradigm shift in computer vision from engineered solutions to learning formulations. As a result, the research challenge shifts from devising algorithms to creating suitable and abundant training data for supervised learning. How to efficiently create such training data? The dominant data acquisition method in visual recognition is based on web data and manual annotation. Yet, for many computer vision problems, such as stereo or optical flow estimation, this approach is not feasible because humans cannot manually enter a pixel-accurate flow field. In this paper, we promote the use of synthetically generated data for the purpose of training deep networks on such tasks.We suggest multiple ways to generate such data and evaluate the influence of dataset properties on the performance and generalization properties of the resulting networks. We also demonstrate the benefit of learning schedules that use different types of data at selected stages of the training process.", "text": "since beginning research artiﬁcial intelligence constant shift engineering solutions special scenarios towards methodologies generally applicable. increased popularity learning methods already started success deep learning another chapter progress towards generic approaches. methods allow restricting engineering algorithmic side time much inﬂuence given data. classical computer vision methods required training data sense unsupervised methods ﬁnal performance approaches following dominant supervised learning paradigm depends much size quality training datasets. especially large potential deep learning fully exploited large datasets. signiﬁcant research eﬀorts made create datasets e.g. imagenet coco cityscapes dataset datasets enabled progress computer vision recent years. notably datasets cover ﬁeld visual recognition object detection classiﬁcation. reasons deep learning computer vision mainly focused visual recognition long time. abstract ﬁnding large networks trained eﬃciently reliably paradigm shift computer vision engineered solutions learning formulations. result research challenge shifts devising algorithms creating suitable abundant training data supervised learning. eﬃciently create training data? dominant data acquisition method visual recognition based data manual annotation. many computer vision problems stereo optical estimation approach feasible because humans cannot manually enter pixel-accurate ﬁeld. paper promote synthetically generated data purpose training deep networks tasks. suggest multiple ways generate data evaluate inﬂuence dataset properties performance generalization properties resulting networks. also demonstrate beneﬁt learning schedules diﬀerent types data selected stages training process. pre-print article published international journal computer vision. ﬁnal authenticated version available online https//doi.org/./s--. acknowledge funding starting grant videolearn consolidator grant reloaded grant br-/- grant horizon project trimbot. thank benjamin ummenhofer code kick-started creation datasets. nikolaus mayer eddy philipp fischer alexey dosovitskiy thomas brox university freiburg e-mail {mayernilgﬁscherdosovitsbrox}cs.uni-freiburg.de caner hazirbas daniel cremers technical university munich e-mail {hazirbascremers}in.tum.de paper present analyze ﬁrst approaches create training data learning optical disparity estimation—two classical computer vision tasks—on large scale. creating data tasks requires diﬀerent approach visual recognition data conjunction manual annotation mostly untrained persons yield large datasets reasonable time monetary eﬀort. case optical estimation instance obvious derive ground truth large sets real videos. notable works include middlebury dataset kitti datasets diﬃculty computing accurate ground truth restricted pairs frames respectively needed train powerful deep network. therefore datasets used mainly test sets classical non-learning-based methodology. situation similar disparity estimation stereo pairs depth estimation camera tracking monocular videos. mpi-sintel dataset demonstrated feasibility using existing data open source movie render videos together desired ground truth sintel provides ground truth optical disparities occlusion areas largely adopted serious benchmark dataset despite synthetic nature. still dataset small deep learning standards training consists image pairs suﬃcient train network powerful show paper. moreover approach scale arbitrarily limited availability open source movies. inspired sintel dataset take approach rendering images together various ground truth outputs level. embracing procedural generation abstract instead naturalistic data focus training rather testing much larger scale. contrast sintel dataset intended benchmarking allows ignore many subtle problems appear rendering synthetic data rather focus size diversity dataset. many ways generate training data synthetic manner using existing scene data sintel manually designing scenes creating randomized scenes procedural manner. paper investigate options powerful properties dataset important training network generalize also data particularly real data. focus investigation networks trained optical estimation speciﬁcally flownet disparity estimation speciﬁcally dispnet study leads many interesting ﬁndings. instance data realistic make good training dataset. fact simplistic flyingchairs dataset yields good data start training network optical estimation. moreover that multistage training multiple separate datasets works better either datasets itself also full both synthetic rendered data used benchmarking purposes long time. heeger barron used virtual scenes quantitatively evaluate optical estimation methods. mccane composed images objects onto backgrounds varied complexity scenes motions quantify changes aﬀect estimation accuracy. complementary works otte nagel recorded real scenes simple geometry camera motion ground truth could annotated manually meister kondermann acquired ground truth real scene recreating virtual model. middlebury dataset contains real synthetic scenes. vaudrey created synthetic sequences benchmarking scene estimation. somewhere synthetic real datasets dwibedi created collages real photographs training data instance detection. recent synthetic datasets include dataset aodha larger sintel benchmark butler software blender used obtain dense accurate disparity ground truth complex rendered middlebury kitti sintel sund middlebury kitti flyingchairs flyingthingsd monkaa driving virtual kitti synthia scannet scenenet rgb-d mccormac table datasets optical depth estimation. multitude variation among datasets makes choosing nontrivial. overview ordered date publication. many datasets focus speciﬁc settings features. scenes. onkarappa sappa used comparable software maya render small driving-speciﬁc dataset study optical accuracy. table gives overview datasets optical depth estimation technical properties. examples shown fig. hooking popular unreal game engine yuille enabled data generation using existing assets made game content creators. technique used zhang make disparity evaluation dataset diﬃcult visual eﬀects specularity. game engine approach also taken taylor build test data video surveillance systems well dosovitskiy build carla driving simulator. work uses concept synthetic data focuses large scale training data rather benchmark datasets. moreover present multiple ways creating data using open source movie butler manually building custom scenes random procedural generation scenes. several recent concurrent works focused building large scale synthetic training datasets various computer vision tasks. datasets typically specialized narrow scenarios automotive driving kitti-lookalike virtual kitti dataset synthia dataset dataset richter indoor scenes scenenet scenenet rgb-d icl-nuim suncg isolated objects modelnet shapenet human action recognition souza relatively little work analyzing properties synthetic training data help generalization real data. used rendered data train network object viewpoint estimation; results indicate better performance real scenes training data contains real background textures varying lighting. movshovitz-attias investigated lighting quality rendered images aﬀects performance neural network viewpoint estimation found realistic lighting helps. zhang studied inﬂuence synthetic data quality semantic segmentation. work perform in-depth analysis synthetic datasets optical estimation varying lighting conditions also shapes motions textures objects scene. present paper consolidates extends three earlier conference papers focus training data. conference papers datasets shapes chairssdhom sophisticated datasets objects introduced used train cnns optical disparity estimation. work provide principled study simplistic sophisticated training data analyze eﬀects data properties network training generalization. supervised training deep neural networks requires large amounts data. visual recognition large training datasets acquired data manual annotations class labels bounding boxes object outlines. dense low-level tasks optical depth estimation manual annotation feasible especially exact pixel-accurate ground truth desired. motivates synthetic data i.e. data taken real world artiﬁcially generated idea perfect knowledge control virtual world create image data thus able produce perfect ground truth annotations along images. diﬃculties creating photo-realistic image data trivial obvious aspects real world relevant must modeled section recapitulates prior works’ discussions synthetic training data generation data augmentation. describe ways generate synthetic data procedural randomization manual modeling. using randomized datasets examples ﬁrst discuss starting simple general scene combining randomly sampled dynamic elements yield datasets abstract powerful potentially unlimited size. dynamic elements motivated dataset able teach e.g. optical object rotation. following this discuss manual data modeling specialized datasets created. achieved either rendering existing scenes monkaa dataset manually recombining objects existing scenes specially designed scenes. used latter method monkaa driving datasets. mixtures methods also possible demonstrated virtual kitti e.g. cars randomly modiﬁed left ﬁxed scene video game engines allow tackling another perspective using game means oﬄoading model scene design developers. mechanics game world used generate large amounts data within constraints game. used e.g. richter richter optical estimation task taking images show basically content variation determining apparent pixel motion pixel ﬁrst second image. low-level nonsemantic task; images correspondences determined valid input. means learn task image pair computable pixelwise ground truth displacements serve training data. simple creating data ground truth displacements take images objects paste onto background randomized transformation applied ﬁrst second image. fig. illustrates flyingchairs sample created way. used images chairs created models aubry chose chairs because ﬂying chairs oﬀer nowhere near semantic content real-world scenes chairs non-convex shapes structure topologically diverse. superimposed chair images onto real-world background images urban landscape environments downloaded flickr. detail generate ﬁrst image sample pair random aﬃne transformations applied objects i.e. chairs randomly scaled rotated placed randomly transformed background. generate second image additional small random aﬃne transformations generated objects. second transformations model motion ﬁrst second image. simulate camera motion eﬀects i.e. objects tend move scene composing background’s motion onto object’s motion. since transformation parameters known accurate ground truth optical available every single pixel even regions become occluded second image. resulting images resemble mccane crucial diﬀerence thousands randomized scenes instead manually created ones. fig. samples real synthetic datasets optical annotation. real datasets small restricted diversity diﬃculty derive ground truth optical ﬂow. synthetic data based manual scene modeling open-source sintel monkaa movies procedural generation flyingchairs flyingthingsd datasets. flyingchairs generated using simple aﬃne transformations. since datasets cannot contain information depth scene i.e. rotation translation space camera motion took randomization approach created flyingthingsd dataset rendered true models ground truth stereo disparity optical full scene task obtain scenes created simple structured background simple geometric random shapes. dynamic foreground objects used models shapenet foreground objects follow linear trajectories space camera. animate camera’s viewing direction invisible moving object added scene render models images used freely available blender suite. modiﬁed internal rendering engine directly produce fully dense accurate ground truth depth disparity optical object segmentation views virtual stereo camera. flyingthingsd dataset combines sophisticated rendering procedural generation scenes allows arbitrary amounts data without manual eﬀort. generated scenes means realistic naturalistic sense e.g. sintel allow large diverse dataset. fig. flyingchairs two-frame sample created composing foreground object onto backgrounds. object background initial transform well transform frames induces optical ﬂow. transforms aﬃne makes computing ground truth ﬁeld easy. schematic shows simpliﬁcation single foreground object; datasets multiple objects individual transform. note foreground object’s transform composed onto background i.e. object move with background transform identity. correlates object motion background motion simulates optical induced camera motion. dice icons indicate randomization. previous section presented approaches generate large amounts data automatically. section discusses generation synthetic data involves manual engineering. model rendering approach described section contrast flyingthingsd data generated movie deterministic way. original scenes objects modeled artists. custom scenes manually collected composed animated original pieces objects movie producing entirely environments movements keeping visual style monkaa movie. obtain suﬃcient amount data rendered longer scenes instead procedurally generating many variations. contrary datasets mentioned above monkaa contains articulated non-rigid motion animals work monkaa encountered many problems described creators sintel benchmark dataset changing focal length scenes containing objects exact locations viewed camera optical tricks break using stereo setup greatly limited amount usable scenes contributes fact approach data generation cannot scaled easily produce data. videos captured camera driving provide special setting usually diﬀer signiﬁcantly video material. demonstrated well kitti benchmark suite wideangle lenses typically used cover large areas scene motion dominated forward camera motion driving car. scene static arrangement objects scenes similar road bottom mostly walls parked vehicles sides. motion object types covered previously presented data hence require special treatment. created dataset represents setting resembles typical aspects real dataset like kitti. scenes contain simple blocks cylinders imitate buildings models cars trees street lamps taken warehouse shapenet database. stereo baseline focal length properties scene lighting object materials chosen match kitti dataset. virtual camera cars scene manually animated simple urban traﬃc scenario. dataset make much sense high level replicate rough semantic structure typical kitti scene. experiments paper conﬁrm intuition ﬁne-tuning data teach network useful priors e.g. road surface bottom thus largely improve performance kitti compared training data without priors. term augmentation refers artiﬁcially increasing amount data presented training machine learning algorithm goal improving generalization capabilities. done transforming original data small incremental ways. relies assumption learning algorithm unable ﬁlter small changes instead perceives modiﬁed data actual data information. datasets oﬀer much training data previously available data augmentation within training process much eﬃcient additional data need stored read disk training generated split augmentation options color geometry augmentations. experiments indicate types oﬀer complementary beneﬁts. train optical networks used color augmentation options geometric changes include every change easily directly compute accurate ground truth ﬁeld shift rotation scaling. simply apply transformation input images additionally apply second transformation images. latter produce greater variety augmentations e.g. inducing scaling motion frames. note described geometry augmentations still yields valid optical training samdisparity networks color augmentations optical ﬂow. however disparity estimation stereo cameras geometrically constrained setting optical transformation cameras ﬁxed e.g. rotating views sample pair zooming view relative would disturb epipolar geometry. means disparity setting restricted fewer augmentation options available. flownet introduced dosovitskiy network ﬁrst trained end-to-end optical estimation. designed receive images video full resolution input generate corresponding optical ﬁeld output. dosovitskiy architectures proposed flownets flownetc share common underlying idea. flownets input consists input images stacked six-channel input blob. processing starts contracting part compresses spatial image information feature blobs lower resolution feature channels. subsequent expanding part uses up-convolutions successively increase resolution reaches desired output size. skipconnections transfer higher resolution information directly resolution level contracting part corresponding resolution expanding part thus information pass architecture’s bottleneck. convolutional block consists convolution relu nonlinearity. ﬁlter size decreases number feature maps increases shown fig. spatial reduction resolution convolution contracting part strided factor expanding part up-convolutions stride increase resolution again. expansion step endpoint error loss aids network additional training gradients deep-supervision manner. details refer dosovitskiy flownetc variant architecture employs custom layer computes correlation scores patches input feature streams. variant input images processed separately ﬁrst three layers fig. flownetc architecture dosovitskiy ﬁgure shows full -layer architecture contracting expanding part. gray skip connections parts allow high-resolution predictions without passing high-resolution information bottleneck. architecture fewer feature channels optical experiments. dispnetcorrd mayer implements idea correlation. found flownetc consistently outperforms flownets. therefore unless mentioned otherwise experiments optical paper based flownetc architecture. speciﬁcally used flownet-c flownetc layer’s number channels reduced original size. reduced computational cost training allows large evaluations paper. unless speciﬁcally mentioned trained mini-batch iterations batch size following sshort learning rate schedule starting learning rate successively dropping iterations respectively. experiments section slong sﬁne schedules intended pretraining ﬁnetuning diﬀerent datasets. disparity experiments used dispnet mayer dispnetcorrd variant. like flownetc network receives images input initially processes separate streams shared weights features streams correlated correlation layer jointly processed further. exploiting known ﬁxed epipolar geometry rectiﬁed stereo pairs correlation reduced unidirectional horizontal scanlines. thus compared flownet correlation layer dispnetpaper analysis relevant dataset properties make good dataset training large networks optical disparity estimation. might properties make good test benchmarking. since benchmarking paper established benchmark datasets sintel kitti measure performance networks trained different data. evaluation sintel covers generalization network synthetic datasets kitti experiments cover generalization real-world data restricted setting driving scenario. cases report average endpoint error dosovitskiy showed optical estimation task trained data semantically matches test data chairs sintel dataset network trained flyingchairs dataset performs well sintel. fact table shows network trained samples training split flyingchairs dataset performs better network trained subset sintel training dataset samples tested remaining samples validation. table flownet trained existing synthetic datasets. sintel flyingchairs split training validation set. expected training driving dataset works best kitti. sintel dataset although similar validation small yield great performance. surprisingly flyingchairs datasets yields consistently better numbers sophisticated flyingthingsd monkaa datasets. observation motivates investigation makes good dataset. positive result dosovitskiy clearly indicate relevant dataset properties apart size. interestingly table also shows training diverse realistic flyingthingsd dataset yields inferior performance training flyingchairs. surprising motivates eﬀorts insights into properties flyingchairs dataset make successful training optical networks training data potentially improved. performed extensive ablation study evaluate contributions object shape types motion. primarily explaining generally good training behavior flyingchairs dataset. additionally tested importance surface features imparted textures eﬀect lighting lifting flyingchairs dataset happens combining flyingchairs flyingthingsd datasets. investigate underlying reason flyingchairs dataset outperforms sophisticated flyingthingsd. experiments conducted optical task. complementary step focusing real data looked data characteristics originate observed scene imaging system itself. particular interested explicit modeling e.g. camera lens distortion bayer artifacts training data help improve performance resulting network applied images real camera system showing characteristics. experiments used disparity estimation task found comparing disparity maps assessing details much easier ﬁelds. optical disparity estimation. considering closely related tasks disparity results presumably valid optical well optical results likely applied disparity estimation. ﬁrst experiment investigated inﬂuence object shapes move. general setup similar flyingchairs dataset—d motions objects front background image—and used composition approach shown fig. however instead chairs used diﬀerent randomly shaped polygons ellipses; instead arbitrary aﬃne motions used diﬀerent subsets thereof plus optionally non-rigid deformations. used random flickr images flyingchairs dataset background foreground objects. every dataset variant trained network scratch evaluated performance three benchmark datasets sintel kitti flyingchairs. designed series increasingly complex datasets starting axis-aligned rectangular boxes motions restricted translation only going complex thin non-convex objects non-rigid motions. examples tested scenarios shown figure training applied color geometry augmentations used optical training dosovitskiy however geometry augmentations restricted respect class motions training dataset; instance applied rotation augmentation rotation motion dataset. polygons+ellipses+rotations polygons+ellipses+scaling polygons+ellipses+holes objects polygons+ellipses+thin objects polygons+ellipses+deformations table object shape motion. corresponds training containing certain object shapes motion types. reference also show results flownet trained flyingchairs dataset trained networks tested three benchmark datasets. even simplest training dataset leads surprisingly good results adding complex shapes motions yields improvement. nonrigid deformations help sintel kitti rigid flyingchairs. hand chairs contain holes whereas training holes objects weakens results sintel kitti. example images fig. fig. appendix. results shown table even simplest dataset consisting translated axis-aligned rectangles leads reasonable performance benchmark datasets. expected general trend complexity diversity shapes motion training improves performance resulting networks. best seen sintel dataset where except addition holes objects additional level complexity slightly improves performance. adding rotations lead much better scores even decreases score kitti adding scaling motions yields large improvement. holes objects seem counter-productive perhaps objects benchmark datasets rarely holes. nonrigid deformations objects background lead overall best result. observations explained looking types object motion present test dataset adding rotation motion yields biggest improvement flyingchairs also testset strongest object rotation. dominant motion kitti scaling motion induced scene moving towards camera. thus training scaling motion helps case. nonrigid deformations approximate patterns exhibited rotating objects; eﬀect training noticeable sintel strongest kitti. results conﬁrm training data improved possible reason target domain. hand table object background texture. trained flownet three texture types illustrated table tested datasets sintel. flickr textures yield best performance sintel. experiments reported previous section used real-world photographs obtained flickr background object textures. provide textures realistic image statistics lack natural semantic context. much choice textures matter performance trained networks? section compare three texture variants illustrated table addition flickr images created additional texture sets plasma clouds. plasma-type textures segmented cells whose sizes range pixels quarter image width. cells clear boundaries; cell table textures. example texture images training samples textures ablation experiment. plasma clouds types procedurally generated random textures flickr corresponds using random natural images flickr textures. examples fig. fig. single color cell colors gradually change across image. clouds-type images made composing color noise multiple scales. resulting textures exhibit structure frequencies contain almost constant-color regions sharp outlines. images appear confusing self-similar human eye. trained network scratch texture tested other’s testing subsets table shows flickr textures generalize best datasets. particular yield best results sintel dataset. shows important diversity textures training ﬁnal network performance explains poor performance training network monkaa dataset comprises complex objects motion patterns highly repetitive monotonous textures. classical optical estimation methods sensitive displacement magnitude. variational methods clear advantages case small displacements whereas large displacements require extra treatment additional combinatorial techniques approaching problem deep network obvious whether similar restrictions apply. however would expect distribution displacements training data plays crucial role. section picks expands experiments done thus evaluated performance network changes displacement distribution training deviates distribution test set. doubled tripled size displacements flyingchairs dataset doubling/tripling object background motion distribution parameters fig. shows histogram plots datasets’ displacement magnitudes; sintel-like datasets exaggerated displacements. results shown table clearly matching displacement distribution test important. using displacement statistics sintel leads best results whereas large deviations statistics lead large drop performance. general importance matching displacement statistics test data disappointing since would like netfig. displacement statistics. statistics displacement size flyingchairs dataset matches statistics sintel dataset fairly closely. flyingthingsd dataset tuned match speciﬁc histogram contains relatively small displacements. greater total count pixels makes histogram curve appear smoother. sintel-like datasets intentionally exaggerate sintel’s distribution table flow magnitude distributions. results training data diﬀerent motion statistics. histogram diﬀers sintel’s worse trained network performs sintel. sintel-like fig. real world interaction light object surfaces generates reﬂections highlights shadows. phenomena pose challenge vision algorithms violate photoconsistency assumption. indeed highlights shadows seem like moving objects generally desired eﬀects ignored. table performance depending displacement magnitude. comparison error contributions testing flownet sintel train clean. training flyingthingsd improves displacements larger pixels cases worse. thus displacement statistics fig. cannot reason inferior performance training flyingthingsd. tion lighting. must provided training data shows correct lighting eﬀects. tested whether flownet able exploit sophisticated lighting models training optical estimated well simpler potentially eﬃciently computed lighting. experiment used flyingchairs dataset re-modeled described section order apply lighting eﬀects. rendered three diﬀerent lighting settings shown fig. shadeless static dynamic. examples shown fig. appendix. fig. lighting quality. data samples crops lighting realism ablation experiment. used three diﬀerent render settings data. background images always photographs. crops lower appear blurred upsampling visualization. fig. examples. table lighting. comparison lighting models shown fig. network trained realistic dynamic lighting also expects realistic lighting test data. network trained static lighting cannot exploit dynamic lighting features performs best sintel whose realistic lighting setup generates static dynamic eﬀects. static/dynamic mixture perform better. objects shadeless variant show lighting eﬀects all. object models textured many simply uniform color. estimating optical objects hard usable features given shape object contour. statically objects shaded environmental lighting uniform directions consider self-shadowing objects. corresponds applying ﬁxed shadow texture object. known ambient occlusion baking approach often used computer graphics make objects look realistic without recompute lighting whenever object light moves. dynamic lighting scenario uses raytracing light source shining scene randomized direction. objects case cast realistic shadows object rotates shading surface changes accordingly. chairs also show specular highlights. scenario presents realistic eﬀects. allows network learn lighting eﬀects also makes learning task harder network must learn distinguish diﬀertable data augmentation. results show eﬀect flownet performance diﬀerent types augmentation applied training data data augmented applying color and/or geometry changes. also choose apply changes input images additionally apply second incremental changes second image. trained separate network scratch scenario tested other’s test data well sintel’s clean training data. results table show network exploit complex lighting training data perform better test data. largest eﬀect lighting though larger number visual features object surfaces. eﬀect dynamic lighting much smaller highlights sparse image. results also show network trained dynamic lighting gets confused objects generated dynamic lighting. also explain network performs marginally worse sintel network trained static lighting strong directional light source sintel scene objects static features hard shadows produced dynamic setup. surfaces sintel real scenes lambertian others shiny. network must learn distinguish diﬀerent surface materials fully exploit lighting cue. training contains chairs possibly suﬃcient capture diﬀerent surface materials. moreover makes learning task diﬃcult. latter would also partially explain flyingthingsd dataset comes realistic lighting many object categories leads network inferior trained flyingchairs. interpretation supported experiments section compared eﬀect augmentation truly changing size dataset. evaluate this trained flownets diﬀerent amounts data either allowed full data augmentation none results given fig. show data indeed always better randomized augmentation yields another huge improvement even inﬁnite amount training samples i.e. many samples training iterations therefore augmentation allows -fold reduction training data still provides better results. section previous sec. conclude data augmentation serves purposes replicates eﬀects found original data augmentation increases eﬀective size training without changing domain data. help dataset enough. hand augmentation able cover types data variation complementary original data case network learn cope wider range inputs. however augmentation inherent limits achieved best results using augmentation much data possible. results table section show training network flyingthingsd dataset yields worse results sintel training network flyingchairs although former diverse uses realistic modeling. experimental results presented give explanation behavior. section complements dataset schedule experiments fig. reveals flyingthingsd much fewer small displacements moreover table shows displacement statistics cannot reason inferior performance flyingthingsd flyingchairs although displacements pixels well represented flyingthingsd still performs worse flyingchairs displacement range. fig. amount training data. indicates many training samples training iterations ﬁgure complements augmentation modes experiment table uses training data setting. flownet trained without data augmentation gets much greater relative beneﬁts training data compared augmented training. however even inﬁnite data networks without augmentation perform worse. split data augmentation augmentations color augmentations geometry analyze eﬀects individual options). table show ablation study full augmentation options augmentation augmentations either color geometry both. split experiments whether images sample pair receive augmentation whether second incremental transformation additionally applied second image. evaluation shows almost types data augmentation complementary important improve performance network. notable exception additional geometry augmentation frames sample. dataset seems already contain suﬃciently varied displacements additional augmentation frames cannot generate helpful data. contrary this color changes frames always introduce data unaugmented dataset contain eﬀect applying color augmentation frames change this. generally accepted training data leads better results supervised training. data augmentation increases eﬀective size training dataset. however augmented existing sample section focus artifacts real-world imaging systems data. eﬀects imaging process obtained images recently studied klein murray there rendering pipeline designed artiﬁcially apply artifacts real camera produces rendered objects. result augmented-reality objects blended well real images; without adjustment rendered objects stood looked unnaturally clean clear. ovvv dataset taylor uses comparable approach simulating analog video eﬀects make evaluation data video surveillance systems realistic. took idea applied training data usually train networks synthetic data real data good ground truth always available quantity even possible; fact synthetic data recorded physical camera already makes diﬀerent. investigated happens corrupt training data ﬂaws real cameras. applying data degradation leads possible counteracting eﬀects artiﬁcially degraded data relationship image pixels ground truth labels muddied blur etc. network already cope inherent discretization artifacts raster images case complicating could lead worse performance. hand network able learn certain ﬂaws artifacts convey meaning subsequently able ignore eﬀects. network transfer knowledge real-world test data performance improve. performed disparity estimation experiments test conjectures observed qualitative effects degraded synthetic training data images taken commercial stereo camera low-quality wideangle lenses; quantiﬁed whether training data adapted camera artifacts kitti improve dispnet performance kitti dataset cases degradation tuned simulate actual lens camera eﬀects seen respective real data. visual eﬀects artists long using knowledge perfect pictures perceived real humans; hence artiﬁcial grain chromatic aberrations lens ﬂare eﬀects applied movies computer games. fig. learning rate schedules. experiments work sshort used lower computational cost. section uses slong combination sﬁne sﬁne used ﬁne-tuning second dataset schedules introduced). table learning schedules. results sintel train clean training flownet mixtures flyingchairs flyingthingsd schedules slong sﬁne. mixture entire datasets. observe sequential combination flyingchairs flyingthingsd gives signiﬁcantly better results. using slong sﬁne schedules results given table show that fact using combination flyingchairs flyingthingsd training data yields best results. moreover holds datasets used separately speciﬁc order training flyingchairs ﬁrst ﬁne-tuning flyingthingsd. reveals content data matters also point time presented network during training phase. strongly connected curriculum learning previously applied shape recognition tasks. observation conclude presenting sophisticated dataset early disadvantageous later stage actually helps. reason might simpler flyingchairs dataset helps network learn general concept ﬁnding correspondences without developing possibly confusing priors motion early. fig. lens distortion blur training data. dispnet trained clean synthetic data perform well images real camera; input images bumblebee stereo camera show signiﬁcant radial blur undistortion well general blur. adding degradation eﬀects training data make look like images camera leads signiﬁcantly detailed disparity estimates without eﬀects especially away image center radial blur strongest. case network learns deal faults training data transfer knowledge real images. content data section rather look pixel-level data characteristics independent images actually depict. perform experiments used established datasets prior works additional experimental ones paper. notable degradation images produced bumblebee stereo camera radial blur general blur over-saturated colors replicated eﬀects oﬄine entire flyingthingsd training dataset using ﬁlters gimp image processing tool. test degradation aﬀects performance trained dispnetcorrd networks scratch original dataset degraded data. fig. highlights qualitative diﬀerences disparity maps estimated networks training artiﬁcially degraded data produces much ﬁner details especially areas towards image boundaries lens distortion largest eﬀect. network even makes sensible guesses areas occluded view imaging sensors generally sense light intensity color. color images sensor common strategy cover sensor repeating pattern color ﬁlters pixel becomes sensitive certain part color spectrum. classic bayer pattern. reconstruct full-color image pixel’s color channels must inferred within neighborhood lead signiﬁcant artifacts. test whether network preconditioned deal bayerinterpolation artifacts emulated flyingthingsd dataset ﬁrst simulating bayer sensor would perceive scene interpolating image virtual sensor image. evaluated eﬀects ﬁnetuning dispnet flyingthingsd data without bayer artifacts testing network kitti training set. fig. shows examples original degraded data well eﬀects network output. degraded data improves kitti score shows adding eﬀects helps qualitative results might expect larger diﬀerence. fig. bayer-interpolation artifacts training data. color fringing naive bayer-pattern interpolation poses problems dispnet trained+ﬁnetuned clean synthetic flyingthingsd data finetuning purposefully degraded version dataset instead helps network deal eﬀect disparity diﬀerence image highlights networks’ outputs diﬀer mostly along object contours artifacts greatest impact. accompanying table shows degraded training data produces better results artifact-free original data. best-case scenario training ground truth data target domain still yields best results; expected experiment targets low-level data characteristics flyingthingsd data cannot teach disparity priors kitti’s street scenes. kitti ground truth sparse gaps along object contours nature acquisition method using laser scanner perspective diﬀerence scanner camera. diﬀerence image disparity estimates synthetic-data networks reveals changes actually occur object boundaries highlighted crops located image region ground truth all. paper performed detailed analysis synthetic datasets deep network training introduced earlier conference papers optical disparity estimation. analysis several ﬁndings summarize important ones here diversity important. network trained specialized data generalizes worse datasets network trained diverse data. realism overrated. learning task accomplished simplistic data data augmentation. realistic eﬀects sophisticated lighting models important learn basic optical merely induce minor improvements. learning schedules matter. learning schedules combine multiple diﬀerent datasets especially simpler complex ones greatly improve generic performance trained networks. camera knowledge helps. modeling distortions camera training data largely improves network’s performance camera. hold high-level tasks object recognition. increasing necessity jointly solve lowlevel high-level tasks major future challenge designing datasets serve regimes ﬁnding learning procedures eﬃciently combine advantages real-world synthetic data.", "year": 2018}