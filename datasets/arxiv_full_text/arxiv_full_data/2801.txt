{"title": "An Empirical Analysis of Proximal Policy Optimization with  Kronecker-factored Natural Gradients", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "In this technical report, we consider an approach that combines the PPO objective and K-FAC natural gradient optimization, for which we call PPOKFAC. We perform a range of empirical analysis on various aspects of the algorithm, such as sample complexity, training speed, and sensitivity to batch size and training epochs. We observe that PPOKFAC is able to outperform PPO in terms of sample complexity and speed in a range of MuJoCo environments, while being scalable in terms of batch size. In spite of this, it seems that adding more epochs is not necessarily helpful for sample efficiency, and PPOKFAC seems to be worse than its A2C counterpart, ACKTR.", "text": "deep reinforcement learning methods shown tremendous success large variety tasks atari continuous control policy gradient methods important family methods model-free reinforcement learning current state-of-the-art policy gradient methods proximal policy optimization acktr methods however take different approaches better sample efﬁciency considers particular clipping objective mimics trust-region whereas acktr considers approximated natural gradients balances speed optimization. technical report consider approach combines objective k-fac natural gradient optimization call ppokfac. perform range empirical analysis various aspects algorithm sample complexity training speed sensitivity batch size training epochs. observe ppokfac able outperform terms sample complexity speed range mujoco environments scalable terms batch size. spite this seems adding epochs necessarily helpful sample efﬁciency ppokfac seems worse counterpart acktr. consider agent agent interacting discounted markov decision process inﬁnite horizon agent selects action policy given state time environment produces reward transitions next state according transition probability agent aims maximize γ-discounted cumulative reward θold vector policy parameters update kl-divergence. particular proximal policy optimization algorithm propose modiﬁcation objective penalizes policy policy without explicitly enforcing trust region constraint. objective following hyperparameter. goal clipping term penalize policies updates large similar trust region constraint. objective optimized stochastic gradients descent natural gradients performs steepest descent metric constructed fisher information matrix local quadratic approximation divergence. unlike usual euclidean norm used stochastic gradient descent norm independent model parameterization class probability distributions. recently proposed technique based kronecker-factored approximate curvature uses kronecker-factored approximation fisher matrix perform efﬁcient approximate natural gradient updates. k-fac approxmates fisher matrix ﬁrst assuming independence parameters different layers approximating local fisher matrix kronecker factorization. technique gives rise efﬁcient actor-critic algorithm called actor critic using kroneckerfactored trust region shown signiﬁcantly outperform stochastic gradient descent counterpart acktr state-of-the-art respective regimes obtains advantage clipping objective function easily optimized acktr achieves higher performance natural gradient updates. signiﬁcant contributing factor success difrerent nature mutually exclusive. therefore naturally interesting question would similar acktr approach consider fisher information matrix policy function deﬁnes distribution actions given current state take expectation trajectory function case learning rate selected according ﬁxed linear decaying schedule. however case k-fac often difﬁcult select reasonable learning rate large updates. therefore adopt trust region formulation k-fac choose learning rate according following schedule. outer loop pposgd algorithm large batch experience collected stochastic gradient optimizer operates smaller minibatch number epochs. example mujoco experiments batch consists state-action pairs updated epochs minibatch size acktr case however k-fac optimizer takes single large batch updates once. case k-fac consider batch size schedule acktr feeding optimizers large batches time fewer number updates. superior performance empirically updating minibatches. larger batches seems reduce variance fisher matrix estimation k-fac. moreover consider updating batch epochs basically updates. example consider batch size k-fac would perform updates would perform updates. suggests k-fac update much efﬁcient update. ppokfac compare pposgd terms sample complexity? performance ppokfac change terms batch size? performance ppokfac change terms number epochs? ppokfac compare pposgd terms speed? pposgd hyperparameters except humanoid-v environment learning rate seems give better performance. ppokfac initial learning rate trust region radius batch size number epochs layer fully connected neural networks neurons actor critic tasks except humanoid figure present mean rewards last episodes training function training timesteps. notably ppokfac outperforms pposgd halfcheetah reacher hopper similar sample complexity inverteddoublependulum. interestingly halfcheetah environment observe ppokfac underperforms ﬁrst catches surpasses pposgd while. also observe phenomenon humanoid ppokfac surpasses pposgd around million timesteps. swimmer environment however observe different result pposgd vastly outperforms ppokfac. also considered using linearly decreasing learning rate schedule also achieved similar results ppokfac. section compare ppokfac would perform different batch sizes. figures show performance mujoco environments different batch sizes compared number timesteps number updates respectively. acktr performance relatively stable different batch sizes even number timesteps suggests updates larger batches efﬁcient. seems smaller batch size seems provide better performance terms number timesteps distributed settings number updates often bottleneck since larger batch sizes obtained multiple machines parallel. experiments algorithm batch size would times number updates batch size number timesteps gives small batches advantage comparing number timesteps. interesingly efﬁciency large batch sizes clearly demonstrated humanoid environment complex benchmark mujoco. possible effect large batch sizes signiﬁcant batch size large. unclear many epochs update k-fac optimizer. update ppokfac once essentially acktr algorithm notice ﬁrst update ratio πold clipping ﬁrst update ﬁrst update would clipping function effect. necessarily help sample complexity; fact updating generally best performance. might caused fact acktr component enforces trust region overlaps effect objective. moreover large steps taken k-fac estimation fisher information matrix could also negatively affect performance over-optimizing multiple steps. important factors algorithm computational complexity. although case many benchmarks simulation often bottleneck rather optimization necessarily case distributed setting since simulations often embarrisingly parallel. although second order gradient update k-fac optimizers competitive ﬁrst order optimizers terms speed. enhanced fact k-fac optimizers would generally require less updates optimizers figure show optimization time spent single algorithms. turns k-fac approach faster since requires less epochs acktr state-of-the-art methods deep seems combining advantages sides seem improve sample complexity expected; objective optimization procedure works necessarily work k-fac. would interesting explore explanation behind phenomenon. assume reason objective implicitly deﬁnes trust region ratio clipping overlaps learning rate selection criteria acktr. moreover acktr takes much larger step-sizes taking iterations step size might hurt performance", "year": 2018}