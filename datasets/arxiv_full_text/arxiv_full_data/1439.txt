{"title": "Stochastic Downsampling for Cost-Adjustable Inference and Improved  Regularization in Convolutional Networks", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "It is desirable to train convolutional networks (CNNs) to run more efficiently during inference. In many cases however, the computational budget that the system has for inference cannot be known beforehand during training, or the inference budget is dependent on the changing real-time resource availability. Thus, it is inadequate to train just inference-efficient CNNs, whose inference costs are not adjustable and cannot adapt to varied inference budgets. We propose a novel approach for cost-adjustable inference in CNNs - Stochastic Downsampling Point (SDPoint). During training, SDPoint applies feature map downsampling to a random point in the layer hierarchy, with a random downsampling ratio. The different stochastic downsampling configurations known as SDPoint instances (of the same model) have computational costs different from each other, while being trained to minimize the same prediction loss. Sharing network parameters across different instances provides significant regularization boost. During inference, one may handpick a SDPoint instance that best fits the inference budget. The effectiveness of SDPoint, as both a cost-adjustable inference approach and a regularizer, is validated through extensive experiments on image classification.", "text": "desirable train convolutional networks efﬁciently inference. many cases however computational budget system inference cannot known beforehand training inference budget dependent changing real-time resource availability. thus inadequate train inference-efﬁcient cnns whose inference costs adjustable cannot adapt varied inference budgets. propose novel approach cost-adjustable inference cnns stochastic downsampling point training sdpoint applies feature downsampling random point layer hierarchy random downsampling ratio. different stochastic downsampling conﬁgurations known sdpoint instances computational costs different other trained minimize prediction loss. sharing network parameters across different instances provides signiﬁcant regularization boost. inference handpick sdpoint instance best inference budget. effectiveness sdpoint cost-adjustable inference approach regularizer validated extensive experiments image classiﬁcation. convolutional networks greatly accelerated progress many computer vision areas applications recent years. despite powerful visual representational capabilities cnns bottlenecked immense computational demands. recent architectures residual networks inception require billions ﬂoating-point operations perform inference single input image. furthermore amount visual data grows need increasingly higher-capacity cnns shown better utilize large visual data compared lower-capacity counterparts sues deep cnns mainly lowering numerical precisions pruning network weights adopting separable convolutions methods result efﬁcient models ﬁxed inference costs models ﬁxed inference costs cannot work effectively certain resource-constrained vision systems computational budget allocated inference depends real-time resource availability. system lower resources preferable allocate lower budget efﬁcient cheaper inference vice versa. moreover cases exact inference budget cannot known beforehand training time. simple solution concern could train several models different inference cost select matches given budget inference time. however extremely time-consuming train many models mention computational storage required store weights many models. work focus cnns whose computational costs dynamically adjustable inference time. costadjustable inference trained once allows users control trade-off inference cost network accuracy/performance. different inference instances derived model parameters. cost-adjustable inference cnns propose novel training method stochastic downsampling point sdpoint instance network conﬁguration consisting unique downsampling point network layer hierarchy well unique downsampling ratio. illustrated fig. every training iteration sdpoint instance randomly selected downsampling happens based downsampling point ratio instance. earlier downsampling happens lower total computational costs given spatially smaller feature maps cheaper process. figure progression feature spatial sizes training conventional sdpoint. costs refer computational costs measured numbers ﬂoating-point operations tically handpicked match given inference budget. existing approaches achieve cost-adjustable inference cnns work evaluating subparts network therefore network parameters utilized cheaper inference. contrast existing approaches sdpoint makes full network parameters regardless inference costs thus making better network representational capacity. moreover parameter sharing across sdpoint instances provides signiﬁcant improvement terms model regularization. advantages sdpoint architecture-neutral adds parameter training overheads. carry experiments image classiﬁcation variety recent network architectures validate effectiveness sdpoint terms cost-accuracy performances regularization beneﬁts. code reproduce experiments released. cost-adjustable inference representative method achieve cost-adjustable inference train intermediate classiﬁers branch intermediate network layers. lower inference cost attained early-exiting based intermediate classiﬁers’ output conﬁdence entropy threshold. lower threshold lower inference cost vice versa. intermediate softmax classiﬁers trained base network completely trained downside intermediate classiﬁer losses backpropagated ﬁne-tuning base network weights. make networks aware intermediate classiﬁers branchynet intermediate classiﬁers ﬁnal classiﬁer trained jointly using weighted classiﬁcation losses. unlike works sdpoint method relies ﬁnal classiﬁer different inference costs. fractalnets cnns designed many parallel subnetworks paths stochastically dropped regularization training. cost-adjustable inference fractalnet’s paths left out. path-dropping regularization gives inconsistent/marginal improvements data augmentation used. another line work somehow related cost-adjustable inference adaptive computation recurrent networks cnns inference costs adaptive computation networks adaptive given inputs harder examples cost easier ones. learned policies choosing amount computation however cannot modiﬁed inference cost-adjustable inference. stochastic regularization work closely related stochastic regularization methods apply certain stochastic operations network training regularization. dropout drops network activations dropconnect drops network weights. stochastic depth allows nonlinear residual building blocks dropped training. methods similar inference stochastically dropped elements present. methods different stochastic instances seen training rather comparable forward pass costs making unﬁt cost-adjustable inference. multiscale parameter-sharing multiscale training cnns ﬁrst introduced quite similar sdpoint. training algorithm network trained images alternatively idea also applied training tasks multiscale training downsamples input images different sizes sdpoint downsamples feature maps downsampling feature level encourages earlier network layers learn better preserve information compensate loss spatial information caused stochastic downsampling later. apply multiscale training input images downsampled interpolation operations happen network training takes place. conventionally downsampling feature maps happens cnns several predeﬁned ﬁxed locations/points layer hierarchy depending architectural designs. example resnet- spatial pooling strided convolutions used achieve downsampling. downsampling layers network stages. downsampling cnns trades low-level spatial information richer highlevel semantic information gradual fashion. ﬁxed downsampling points followed exactly conﬁgured training optimal accuracy performance. work beyond ﬁxed downsampling points develop novel stochastic downsampling method named stochastic downsampling point restrict downsampling happen every time ﬁxed points layer hierarchy. proposed method complementary ﬁxed downsampling points existing network architectures replace them. sdpoint simply plugged existing network architectures major architectural modiﬁcations required. stochastic downsampling point instance unique downsampling point unique downsampling ratio stochastically/randomly selected network training. stochastically selected beginning network training iteration downsampling occurs selected point samples current training mini-batch. downsampling points downsampling ratios discussed thoroughly upcoming sections. downsampling performed downsampling function makes downsampling operations. selected point falls lower layer layer hierarchy downsampling happens earlier causing quicker loss spatial information feature maps computation savings. conversely spatial information better preserved higher computational costs stochastic downsampling happens later. sdpoint effectively turn feature spatial sizes right prediction layers different original sizes could cause shape incompatibility prediction layer weights convolutional outputs prevent this preserve feature spatial size last network stage regardless stochastic downsampling taking place adjusting convolution strides and/or pooling sizes accordingly. example image classiﬁcation networks consider global average pooling layer ﬁnal classiﬁcation layer last network stage. therefore regardless spatial size incoming feature maps globally pool spatial size discussed sect. downsampling operation employed either pooling strided convolution. average pooling rather strided convolutions pooling several reasons. strided convolutions preferred downsampling recent network architectures extra parameters therefore improving representational capability. work want rule possible performance improvements increase parameter numbers moreover strided convolutions integer-valued strides cannot work well arbitrary downsampling ratios hand average pooling preferred pooling paper fact pooling form nonlinearity. using pooling downsampling operation could either push greater non-linearity network unfair baselines could exacerbate vanishing gradient problem commonly associated deep networks besides effectiveness average pooling validated extensive roles recent architectures densenets’ transition every training iteration downsampling point sdpoint instance drawn discrete uniform distribution predeﬁned downsampling point indices ...n number points. work downsampling point candidates points consecutive basic building blocks mirroring placements ﬁxed downsampling layers conventional cnns. keep original network instance assigning index perform full-cost inference later. denote function carried i-th basic building block denote network weights involved block. given input downsampling ratio downsampling carried following non-residual cnns basic building block comprises consecutive convolutional batch normalization non-linear activation layers. hand residual networks residual blocks considered basic building blocks. downsampling point stochastically selected point basic building blocks network downsampling happens. since residual block involves streams information identity skip connection non-linear function consisting several network layers apply stochastic downsampling function davg point right residual addition operation. also experiment densely connected networks paper. densenets sdpoint downsampling points points right behind block concatenation operation mirroring ﬁxed downsampling densenets. principle mini-batch sample could unique downsampling point practical reasons resort using samples mini-batch. possible downsampling points training iteration number possible combinations sdpoint instances would become excessively large. instances would deviate much original network terms computational cost accuracy performance. single stochastic downsampling point work. downsampling ratios consider downsampling ratios sdpoint instance stochastically draw downsampling ratio from current training iteration. sect. downsampling ratios drawn according discrete uniform distributions. ratios cannot hamper training convergence consider small number downsampling ratios prevent excessive number sdpoint instances would cause great difﬁculty experimentally evaluating sdpoint instances cost-adjustable inference. recent experimental study cnns ﬁnds sufﬁcient make qualitative conclusions optimal network structure hold full-sized imagenet classiﬁcation task using input images. conceivably network structure/architecture works well certain image resolution likely work well resolution double/half that. motivated above-mentioned heuristics experimental ﬁnding come downsampling ratio ratios also used multiscale-input semantic segmentation. hyperpameter used across experiments paper. downsampling fractional downsampling ratios cannot trivially achieved integer-valued pooling hyperparameters. example pooling feature cannot easily done tuning pooling size stride. adopt spatial pooling strategy akin spatial pyramid pooling generates ﬁxed-length representation adaptive calculations pooling sizes strides. sdpoint gives rise training algorithm cnns. training algorithm consolidating previously introduced sdpoint concepts given algorithm denotes generic nonlinear building network block cnns. simplicity sake omit network layers basic building blocks typically starting ending layers. nutshell algorithm shows whenever building block index equal downsampling point downsampling switch turned stochastic downsampling happens output i-th building block stochastic downsampling ratio sdpoint seen regularizer cnns. stochastic downsampling takes place receptive ﬁeld size becomes larger causes sudden shrinkage spatial information feature maps. network learn adapt variations training perform parameter-sharing across downsampled feature maps originally sized feature maps addition robustness terms receptive ﬁeld size spatial shrinkage sdpoint also necessitates convolutional layers accommodate different padded pixel non-padded pixel ratios. example applying convolutional ﬁlter feature gives padded-pixel ratio compared ratio resulted applying ﬁlter feature map. zero-padded pixels quite similar zero-ed activations caused dropout sense missing values. thus higher paddedpixel ratio akin higher number dropped-out activations vice versa. form variation provides further regularization boost. experimentally even heavy data augmentation scale aspect ratio augmentation sdpoint still help. network perform inference different computational costs depending user requirements considered capable cost-adjustable inference. opting lower inference cost usually results lower prediction accuracy vice versa. sdpoint naturally supports cost-adjustable inference given sdpoint instances varying computational costs given different downsampling point locations downsampling ratios. importantly instances trained minimize prediction loss helps work relatively well inference. inference handpick sdpoint instance make inference cost particular inference budget. instance-speciﬁc batch normalization mentioned sect. sdpoint instances trained every training mini-batch iteration shares sdpoint instance. sdpoint instance prediction loss minimization training based batch normalization statistics particular instance. therefore using statistics accumulated many training iterations inference causes inference-training mismatch. similar form inference-training mismatch caused statistics also observed another context. statistics required sdpoint instance differ another instance. using statistics perform cost-adjustable inference inference accuracies could jeopardized. address mismatch issue compute sdpoint instance-speciﬁc statistics costadjustable inference. disentangling different sdpoint instances unsharing statistics makes inference accurate. computational storage overhead resulted instance-speciﬁc statistics relatively statistics earlier layers shared among certain sdpoint instances downsample later layers. experiments carried image classiﬁcation tasks evaluate sdpoint. consider image classiﬁcation datasets varying dataset scales terms numbers categories/classes sample counts cifar- cifar imagenet inference cost comparison measure model costs terms ﬂoating-point operation numbers needed forward propagation single image. treat addition multiplication separate operations. implementations pytorch baseline architectures wide-resnet densenetbc-d-g stand network depth widen factor growth rate densenetbc respectively. training hyperparameters follow ones original papers except training epoch numbers all. original learning rate schedules still apply numbers sdpoint downsampling points {wrn-d-w wrn-d-w densenetbc-d-g} respectively. mentioned sect. downsampling ratios drawn uniformly baseline comparison compare sdpoint baseline methods related ours terms cost-adjustable inference performance. classiﬁcation error-cost performance plots cifar- cifar- figure wrns’ densenetbc’s cost-error plots cifar- cifar- observed models trained sdpoint consistently outperform non-sdpoint counterparts given computational budgets. model resnext-d-c resnext-d-c densenetbc-d-g densenetbc-d-g wrn-d-w wrn-d-w sdpoint wrn-d-w wrn-d-w dropout wrn-d-w sdpoint densenetbc-d-g densenetbc-d-g sdpoint shown fig. note sdpoint baseline methods instances model appear plots; higher-cost instance performs worse lower-cost instance shown. model trained obtain cost-error plot. early-exits train models based intermediate classiﬁers allow earlyexits following design branchynet network stage main network evenly spaced branches branches single-repetition building block branch network stage. blocks branches follow hyperparameters blocks original network. cost-adjustable inference evaluate every branch make samples exit branch. early-exit models considerably parameters baseline models sdpoint-based models. conjecture relatively worse performance lack full network parameter ultilization. also forces features classiﬁcation-ready early stage thus causing higher layers rely heavily classiﬁcationready features instead learning better features own. multiscale training multiscale training baseline method inspired input images downsampled using bilinear interpolations integer-valued size randomly chosen sizes ranging half full size step size pixel. done every training iteration similar sdpoint. number instances resulted multiscale training close downsampling point numbers applying sdpoint wrns densenetbc. also ranges cost-adjustable inference costs among comparable. instance-speciﬁc statistics applied. cost-adjustable performance consistently trails behind sdpoint input downsampling causes drastic information loss feature downsampling uniform batch normalization validate effectiveness sdpoint instance-speciﬁc show results sdpoint baseline whose statistics averaged many training iterations uniform instances. consistent classiﬁcation performance gaps using statistics instance-speciﬁc statistics suggesting preferable keep instance-speciﬁc statistics inference. cifar validation results state-of-the-art resnext densenetbc models comparison ours. sdpoint-enabled model show results bestperforming sdpoint instance among instances. notably wrn-d-w sdpoint competitive sota models cifar- outperforms cifar. overall sdpoint considerably improves classiﬁcation performance without bringing additional parameters computational costs unlike sota models require model complexity attain slight improvements. fact best sdpoint-enabled models cifar reduced inference costs reckon prolonged preservation spatial details feature maps crucial dataset relatively label complexity cifar. reveals drawback current practice using cnns one-size-ﬁts-all fashion. imagenet consider resnext-d-c preresnetd baseline architectures. stands resnext’s cardinality. sdpoint downsampling points model. train models imagenet-k training evaluate validation models trained using training hyperparameters scale aspect ratio augmentation identical note allocate training epochs models sdpoint. cost-error plots given fig. preresnetd resnext-d-c respectively along ﬁxed-cost carefully designed baseline models architecture families. overall models trained sdpoint roughly match performance baseline models lower-cost range surpass upper-cost range. notably obtain cost-error plots sdpoint-enabled models trained once. ablation study study effects choice sdpoint downsampling points downsampling ratios cost-adjustable inference performance. this train resnext-d-c default sdpoint hyperparameters well baseline models downsampling points every residual block dubbed alternate downsampling ratio dubbed shown fig. either removing downsampling ratio alternating blocks downsampling gives worse results reduced stochasticity figure imagenet validation examples grouped according minimum inference costs required resnext-d-c classify correctly terms top- accuracy. ground-truth label names shown corresponding images. respectively attainable sota models roughly inference costs parameter counts. also display results spatially adaptive computation time paired preresnet-d compare sdpoint instance preresnet-d achieves similar classiﬁcation errors. sdpoint merely needs flops needed sact achieve similar results. sact saves computation skipping layers certain locations feature maps according learned policy inputs sdpoint downsamples feature maps save computation contend costaccuracy trade-off inference reducing feature spatial sizes less harmful accuracy skipping network parameters/layers. cost-dependent misclassiﬁcations group imagenet validation images according minimum inference costs required classify correctly present examples fig. difﬁcult examples require higher inference costs classiﬁed correctly generally size-dominant interfering objects/scenes contrast easier examples intuitively pooling-based downsampling causes information loss smaller objects larger objects especially occurs early layer semantic/context information still relatively weak distinguish objects interest interfering objects. difﬁcult examples makes sense preserve spatially informative object details scale sensitivity training cnns sdpoint involves stochastic downsampling intermediate feature maps hypothesize beneﬁcial scale sensitivity/invariance mentioned sect. validate hypothesis vary pre-cropping sizes imagenet validation images range step size resulting pre-cropping sizes. every precropping size center image regions cropped evaluation. models involved sdpointenabled resnext-d-c baseline without sdpoint. compute mean pairwise cosine similarities resulted different precropping sizes terms imagenet k-class probability scores. done entire imagenet validation set. pairwise cosine-similarity mean obtained baseline model sdpoint-enabled model higher cosine similarity strong indicator model less sensitive scales. demonstrates sdpoint indeed beneﬁt cnns terms scale sensitivity. propose stochastic downsampling point novel approach train cnns downsampling intermediate feature maps. extra parameter training costs sdpoint facilitates effective cost-adjustable inference greatly improves network regularization experiments additionally sdpoint help identify optimal sub-networks sort input examples various levels classiﬁcation difﬁculties making cnns less scale-sensitive", "year": 2018}