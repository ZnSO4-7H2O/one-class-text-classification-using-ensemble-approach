{"title": "Generalizable Features From Unsupervised Learning", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Humans learn a predictive model of the world and use this model to reason about future events and the consequences of actions. In contrast to most machine predictors, we exhibit an impressive ability to generalize to unseen scenarios and reason intelligently in these settings. One important aspect of this ability is physical intuition(Lake et al., 2016). In this work, we explore the potential of unsupervised learning to find features that promote better generalization to settings outside the supervised training distribution. Our task is predicting the stability of towers of square blocks. We demonstrate that an unsupervised model, trained to predict future frames of a video sequence of stable and unstable block configurations, can yield features that support extrapolating stability prediction to blocks configurations outside the training set distribution", "text": "humans learn predictive model world model reason future events consequences actions. contrast machine predictors exhibit impressive ability generalize unseen scenarios reason intelligently settings. important aspect ability physical intuition work explore potential unsupervised learning features promote better generalization settings outside supervised training distribution. task predicting stability towers square blocks. demonstrate unsupervised model trained predict future frames video sequence stable unstable block conﬁgurations yield features support extrapolating stability prediction blocks conﬁgurations outside training distribution. humans learn tremendous amount knowledge world almost supervision construct predictive model world. model world interact environment. also argued lake core ingredients human intelligence intuitive physics. children learn predict common physical behaviors world observing interacting without direct supervision. form sophisticated predictive model physical environment expect world behave based mental model reasonable expectation unseen situations t´egl´as despite impressive progress last years training supervised models quite able achieve similar results unsupervised learning remains challenging research areas ﬁeld. full potential application unsupervised learning realized. work leverage unsupervised learning train predictive model sequences. imagined predicted future sequence data help physical environment prediction model generalize better unseen settings. speciﬁcally focus task predicting tower square bricks fall introduced lerer showed deep convolution neural network could predict fall towers super-human accuracy. despite strengths convolution neural networks zhang shows deep neural networks hard time generalizing novel situations humans simulation-based models work show deep neural networks capable generalizing novel situations form unsupervised learning. core idea observe world without supervision build future predictive model later stage leverage utilize imagined future train better fall prediction model. supervised learning. since krizhevsky many regularization srivastava weight initialization glorot bengio normalization ioffe szegedy techniques architecture designs introduced diminish effect pre-training. although pre-training still could useful data scarce domains many ways applications unsupervised learning still interesting models active area research. name applications semi-supervised learning kingma salimans dumoulin super resolution sønderby video generation active area research many applications many recent works using states neural networks video generation. srivastava uses lstm recurrent neural networks train unsupervised future predictive model video generation. similar architecture described section mathieu combines common mean-squared-error objective function adversarial training cost order generate sharper samples. lotter introduce another form unsupervised video prediction training scheme manages predict future events direction turn could potential training self-driving cars. model-based reinforcement learning active research area holds promise making agents less data hungry. learning agents could explore learn unsupervised world learn even dreaming future states. believe action-condition video prediction models important ingredient task. fragkiadaki learn dynamics billiards balls supervised training neural net. action-conditioned video prediction models applied atari playing agent well robotics recent datasets predicting stability block conﬁgurations provide binary labels stability exclude video simulation block conﬁguration. therefore construct dataset similar setup lerer zhang includes video sequence. javascript based physics engine generate data. construct towers made square blocks. sample random tower conﬁguration uniformly shift block position touches block below. taller towers unstable shift smaller blocks. simplify learning setting balance number stable unstable block conﬁgurations. tower height create video clips training validation test respectively. video clips sub-sampled time include noticeable changes blocks conﬁgurations. decided keep number frames sub-sampling rate enough time unstable towers collapse. video frame image size addition binary stability label include number blocks fell down. core idea paper future state predictions generative video model enhance performance supervised prediction model. architecture consists separate modules stability predictor original task stability predicted static image block conﬁguration. explore whether addition initial conﬁguration last frame prediction unsupervised model improves performance stability prediction. consider different model architectures task. ﬁrst named convdeconv takes ﬁrst frame input predicts last frame video sequence. architecture consist block convolution max-pooling layers. compensate dimensionality reduction max-pooling layers fully-connected layer following last max-pooling layer. ﬁnally subsequent block deconvolution layers output size model input size. activation functions relu. table details architecture. objective function mean squared error generated last frame ground-truth frame; result training require labels. also experimented additional adversarial cost mathieu observe improvement stability prediction task. hypothesize although adversarial objective function helps sharper images improved sample quality transfer better stability prediction. figure shows examples generated data test set. mean squared error minimized using adam optimizer earlystopping validation loss improve epochs. extend convdeconv model second architecture named convlstmdeconv predict next frame timestep. model composed lstm architecture. convolutional deconvolutional blocks convdeconv utilized respectively input current frame lstm transition output next frame current lstm state. details convlstmdeconv model architecture shown table figure shows diagram architectures. training time step ground-truth data feeds model test time initial time step gets ﬁrst frame data subsequent time steps generated frames previous time steps feed model. similar setup recurrent neural network language models mikolov necessary test time access ﬁrst frame. before model trained predict next frame time step minimizing predictive mean-squared-error using adam optimizer early-stopping. training subsample time dimension reduce sequence length -time steps. figure shows sample generated sequences test set. figure samples convlstmdeconv model. different sample. left sequence data right sequence generated data. note generation model ﬁrst frame next time steps uses output last timestep. architecture trained baseline model different tower heights call single model name experiments respectively number blocks trained second model using generated data takes input ﬁrst frame generated last frame. consisted layer resnet blocks parallel ﬁrst frame last frame last hidden layer models concatenated together logistic regression layer resnet blocks share parameters. based whether generated data coming convdeconv model convlstmdeconv model labeled experiments respectively. none models pre-trained weights randomly initialized. adam stopped training validation accuracy improved epochs. images contrast normalized independently augment training using random horizontal images randomly changing contrast brightness. figure different model architectures. ﬁrst left convdeconv convlstmdeconv described section right models used supervised fall prediction described section single frame predictor baseline model. double frame predictor model uses generated data. figure shows classiﬁcation results models described section tested blocks. test case shown different color. table shows test case results’ numerical values. almost cases generated data improves generalization performance test cases different number blocks trained comparison included results zhang table since zhang reports results models trained tower blocks corresponding results would second block table models cld. even though datasets same observed range performance baseline model consistent range performance alexnet model table seen results model signiﬁcantly better human performance reported zhang baselines similar performances. observation fact improvements signiﬁcant it’s tested scenarios bricks training. also improves reverse case i.e. fewer bricks training improvement signiﬁcant. worth mentioning testing lower number bricks much harder problem pointed zhang too. case prediction performance almost random going blocks blocks case experiments. possible explanation performance loss balanced tower fewer blocks corresponds unstable conﬁguration tower blocks e.g. tower blocks classiﬁed unstable prediction model trained towers blocks. solution could train models predict many blocks fallen instead binary stability label. access data dataset explored experiments using labels. unfortunately observe signiﬁcant improvement. main reason could distribution number fallen blocks extremely unbalanced. hard collect data balanced number fallen blocks conﬁgurations thus unlikely e.g. tower blocks blocks falls another observation fact models convdeconv generated data performed slightly better convlstmdeconv. seen figure samples convlstmdeconv case noisy less sharper figure could caused since ﬁrst time step model outputs last time step used input next time step samples degenerates longer sequence data augmentation crucial increase generalization performance stability prediction e.g. model tested bricks achieved without data augmentation reaching accuracy data augmentation. signiﬁcant improvement data augmentation could partly dataset relatively small. paper showed data generated unsupervised model could help supervised learner generalize unseen scenarios. argue ability transfer learning generalization observing world could ingredients construct model world could applications many tasks model-based extend work future looking videos robots manipulating objects able predict failure beforehand could help agent explore intelligently. would like thank harm vries laurent dinh help feedback writing paper. also thank adam lerer jiajun sharing dataset. thank nserc cifar canada research chairs google samsung funding. vincent dumoulin ishmael belghazi poole alex lamb martin arjovsky olivier mastropietro aaron courville. adversarially learned inference. arxiv preprint arxiv. diederik kingma shakir mohamed danilo jimenez rezende welling. semi-supervised learning deep generative models. advances neural information processing systems alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems vinod nair geoffrey hinton. rectiﬁed linear units improve restricted boltzmann machines. proceedings international conference machine learning junhyuk xiaoxiao honglak richard lewis satinder singh. action-conditional video prediction using deep networks atari games. advances neural information processing systems nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research ern˝o t´egl´as edward vittorio girotto michel gonzalez joshua tenenbaum luca bonatti. pure reasoning -month-old infants probabilistic inference. science renqiao zhang jiajun chengkai zhang william freeman joshua tenenbaum. comparative evaluation approximate probabilistic simulation deep neural networks accounts human physical scene understanding. arxiv preprint arxiv.", "year": 2016}