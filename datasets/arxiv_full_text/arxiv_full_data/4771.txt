{"title": "Exploring Bayesian Models for Multi-level Clustering of Hierarchically  Grouped Sequential Data", "tag": ["cs.LG", "cs.AI"], "abstract": "A wide range of Bayesian models have been proposed for data that is divided hierarchically into groups. These models aim to cluster the data at different levels of grouping, by assigning a mixture component to each datapoint, and a mixture distribution to each group. Multi-level clustering is facilitated by the sharing of these components and distributions by the groups. In this paper, we introduce the concept of Degree of Sharing (DoS) for the mixture components and distributions, with an aim to analyze and classify various existing models. Next we introduce a generalized hierarchical Bayesian model, of which the existing models can be shown to be special cases. Unlike most of these models, our model takes into account the sequential nature of the data, and various other temporal structures at different levels while assigning mixture components and distributions. We show one specialization of this model aimed at hierarchical segmentation of news transcripts, and present a Gibbs Sampling based inference algorithm for it. We also show experimentally that the proposed model outperforms existing models for the same task.", "text": "hierarchical bayesian model show many existing ones special cases show adapted news transcript segmentation give inference algorithm demonstrate experimental results. notations consider datapoints type based application. associated group membership variables specify grouping datapoints. levels grouping datapoint associated observed variables example text corpus consists documents consists word-tokens. consider wordtokens data-points {yi} tagged document memberships using token indices capture sequential ordering. standard setting used topic models text documents. addition possible consider -level grouping sentences within documents. word-token associated sentence membership variable document membership variable paper overload indicate higher-level group-memberships lower-level-groups. example index level group level- group covers datapoints group i.e. please illustration. wide range bayesian models proposed data divided hierarchically groups. models cluster data different levels grouping assigning mixture component datapoint mixture distribution group. multi-level clustering facilitated sharing components distributions groups. paper introduce concept degree sharing mixture components distributions analyze classify various existing models. next introduce generalized hierarchical bayesian model existing models shown special cases. unlike models model takes account sequential nature data various temporal structures different levels assigning mixture components distributions. show specialization model aimed hierarchical segmentation news transcripts present gibbs sampling based inference algorithm also show experimentally proposed model outperforms existing models task. introduction many applications come across hierarchically grouped data. example text corpus data grouped documents paragraphs sentences. data clustered multiple levels based notion topics. large number hierarchical bayesian models proposed data many quite similar various aspects. however best knowledge much research aimed placing models perspective making comparative study them except empirical comparisons. attempt paper. main aspect models compare share mixture components distributions across groups different levels. contributions paper follows introduce novel classiﬁcation hierarchical bayesian models grouped data based degree sharing mixture components distributions introduce generalized datapoints grouped all. mixture components gaussian distributions i.e. general mixture components need gaussian. mixture distribution kdimensional multinomial. datapoint assigned mixture component deﬁnes clustering dat∼ sequential apoints. assignment structure datapoints considered. number mixture components ﬁxed known. non-parametric model dirichlet process mixture model considers inﬁnitely many mixture components though used ﬁnite number datapoints. mixture distribution inﬁnite-dimensional multinomial drawn stick-breaking distribution. parameters mixture components drawn base distribution one-level nonparametric model consider sequential structure data hdp-hmm model considers θ-distributions chosen conditioned previous assignments -assignment datapoint done prev prev predecessor current datapoint sequential order encoded i.e. prev next move two-level models i.e. standard setting document modelling word-tokens grouped documents document membership variables encoded standard model kind latent dirichlet allocation considers mixture components ﬁxed known. mixture component multinomial distribution vocabulary size level- groups clustered i.e. distinct document. consequently used here group-speciﬁc. -variables datapoints within group assigned draws again sequential structure considered. note mixture components shared groups. non-parametric generalization hierarchical dirichlet process also -level extension dp-mm discussed above. here number components ﬁxed known document-speciﬁc {θ}-distributions inﬁnitedimensional drawn dirichlet process/stickbreaking process instead ﬁnite-dimensional dirichlet. another nonparametric -level model nested dirichlet process level- groups clustered using drawn according discrete distribution cluster induced uses however unlike previous models mixture components speciﬁc clusters induced data. avoided current representation sequential relations word-tokens encoded using indices takes integer values. accordingly datapoint deﬁne sequential neighbors prev next. even sequential ordering higher-level groups like sentences documents captured variables respectively. case sequential ordering irrelevant level group membership variables level simple identiﬁers. groups different levels clustered applications like multi-level clustering. this associate group cluster variable group-index again overload indicate higher-level cluster memberships lowerlevel groups. index level- group level- cluster covers datapoints i.e. causes hierarchical clusz tering datapoints speciﬁed tuple bayesian modelling involves mixture components mixture distributions. consider mixture components known. also need mixture distributions level{θ}{θ} {θl}. discrete distributions index variables cluster indices lower layer. note cluster indices level indices mixture components. level distributions speciﬁc group clusters deﬁned group cluster variables example groups level clustered groups cluster i.e. access level basic inferdistribution ence problem learn cluster assignments estimate mixture components review existing models section make short review several well-known models using notation. models classiﬁed based number levels grouping data consider. simplest models -level mixture models like concept already discussed hierarchical bayesian models mixture components mixture components {θl} shared among different groups. seen three types sharing components/distributions speciﬁc clusters groups accessible outside clusters. example mlc-hdp θ-distribution accessible cluster level- groups θ-distribution accessible cluster level- groups. models mixture components speciﬁc clusters level- groups based notions introduce degree-of-sharing given model ﬁrst specify mixture components shared levelsfull group-speciﬁc cluster-speciﬁc call {φ}. type sharing different levels hyphen-separated. next regarding distributions {θl} level specify shared levels upwards call {θl}. also indicate sequential structure considered different levels levels considered. finally indicate groups clustered different levels levels clustering groups levels number clusters ﬁxed levels clustering non-parametric. note indicates dimensionality {θl}p indicates ﬁnitedimensional indicates inﬁnite-dimensional indicates use. combining {φ}{θ} order dos-classiﬁcation model. different variables semicolon-separated. number components models dos-classiﬁcation model semicolonseparated parts. also ﬁrst part consist hyphen-separated letters number letters keep decreasing following parts followed letters specifying dimensionality sequence strcuture. classiﬁcation models illustrate concept dos-classiﬁcation case study models discussed section level- parametric models like mixturecomponents speciﬁc clusters datapoints mixture distribution fully shared next look -level models. mlc-hdp attempted compromise groups clustered mixture components cluster-speciﬁc moreover data grouped levels observed group variables groups clustered random variables drawn discrete distributions θ{θ}{θ} respectively. three-level model considers sequential nature data topic segmentation model within document sentences clustered using analogous hdp-hmm distributions speciﬁc values particular sentence distribution values speciﬁc sentence-clusters. documents clustered used. somewhat unusual case subtle topic model considers multiple documentspeciﬁc distibutions mixture components distributions speciﬁc sentences distributions. neither documents sentences clustered. effectively {θ}-distributions present shared across sentences document across documents. however process assigning -variables requires sentence-speciﬁc variables addition {θ}. dos-classiﬁcation models discussion focussed major aspects) number layers grouping mixture components mixture distributions shared whether sequential structure considered different layers. based aspects propose nomenclature models. datapoints number clusters formed level ﬁxed sequential structure considered. hence dos-classiﬁcation case dpmm inﬁnitedimensional i.e. dos-classiﬁcation case hdp-hmm mixture-components speciﬁc clusters datapoints here non-parametric sequential structure also considered. dos-classiﬁcation hdp-hmm note collection distributions chosen data-point depending assignment prev. level- models mixture-components shared cluster-speciﬁc level- fully level- speciﬁc level- groups sequential structure considered level. case number clusters datapoints ﬁxed level- groups clustered either model. dos-classiﬁcation case cluster-speciﬁc levels speciﬁc clusters level- groups nonparametric also non-parametric dos-classiﬁcation level- models mlc-hdp mixture-components cluster-speciﬁc level- fully levels speciﬁc clusters level- groups fully shared level- groups nonparametric notation speciﬁc clusters level- groups nonparametric ﬁnally nonparametric. dosclassiﬁcation mlc-hdp topic-segmentation model topics shared sentences documents i.e. speciﬁc clusters sentences inside indidos vidual documents i.e. ﬁxed dimension used cluster sentences document-speciﬁc number clusters sentences formed ﬁxed sequential structure also taken account notation finally documents clustered dos-classiﬁcation finally come subtle topic model topics shared sentences documents i.e. c−f−f shared sentences document speciﬁc documents nonparametric i.e. notation sentences documents clustered dos-classiﬁcation topic models several dos-classiﬁcations existing models. instead trying point classiﬁcations individually propose models following them propose generalized bayesian model grouped sequential data. show speciﬁc settings model possible recover previously discussed models models explored also obtained consider sequential data l-levels grouping groups sequential every level consider clustering happens levels i.e. {θl} exist. capture sequential nature assume every level collection distributions {θl} chosen group conditioned previous assignments also consider distributions inﬁnite-dimensional i.e. neither number mixture components number clusters formed level known advance. also consider mixture components accessible level groups introduce binary random vector speciﬁc datapoint. vector indicates mixture components accessible datapoint. show using vector make mixture components group-speciﬁc cluster-speciﬁc also capture intricate structures would possible without generative process hierarchically clusters groups bottom level. every intermediate level assigns group level access θl-distributions speciﬁc cluster group result clustering level group part group level θl-distributions corresponding must used. finally level datapoint assigned binary vector conditioned b-vectors corresponding previous datapoints. distribution convoluted vector subset components available datapoint model proposed different combinations exchangeability properties different layers here level- groups sentences word-tokens themselves. successful models sequential structure considered level- level- i.e. assignment conprev respectively. clusterditioned level- level- nonparametric. dosclassiﬁcation ladp modeling temporal structure news transcripts characteristic temporal features regarding assignments gbp-gsd needs modiﬁed appropriately. features discussed below. ladp insufﬁcient news transcripts capture them. number level- clusters ﬁxed known. case news transcripts particular source expected news categories ﬁxed order segmentation task linear clustering words/sentences i.e. word/sentence prev ladp assigned either datapoint assigned value based assignments prev segmentation happens based assignments. guarantee formation segments. overcome issue known model observed data sequence level- segments. sequence partitioned parts sizes sizes modeled dirichlet distribution parameters signify relative lengths/importance news categories. topic coherence considered various text segmentation paper like property within level- segment successive datapoints likely assigned topic easily modelled markovian approach i.e. means i-th datapoint assigned value predecessor pred probability value probability available values dictated discussed next. similar mixture model level- segments share mixture components because individual news story come news category. also topics repeat inside level- segment. inside level- segment successive datapoints expected assigned mixture component temporal coherence. however news transcript news story told once means recovery existing models level- models recovered easily. setting vector datapoints making conditioned prev gem-distributed back hdp-hmm. case also independent previous assignments dp-mm ﬁnitedimensional provided base distribution gaussian. recover need deﬁne groups groups clustered. vector make independent previous assignments drawn gem. ﬁnite-dimensional drawn dirichlet {φk} also drawn dirichlet lda. involves nonparametric clustering level- groups without sequential ordering generation independent previous assignments drawn gem. also special characteristic different level- clusters share mixture components. managed setting appropriate function return vector mixture components assigned level- clusters i.e. mlc-hdp recovered removing conditioning previous assignments assignment setting vector {θl} drawn gems. ensure documents clustered vector assignment independent previous assignments. regarding ensure sentence either news transcript segmentation want extend generative framework grouped sequential data modeling news transcripts. data hierarchical since broad news categories like politics sports individual stories topics. bayesian approach consider mixture components correspond stories broad categories represented distributions stories. usual θ-distribution speciﬁc level- cluster clustering induced speciﬁc level- groups transcripts clustered. observed datapoints word-tokens represented integer deﬁne prev sentence otherwise prev similarly next deﬁned within sentences. also prev next deﬁned sentences. indicates news story indicates news category token associated with. sentence level- group. ladp model news transcripts news transcripts ﬁrst modelled layered dirichlet process several versions particular component present single chunk cannot reappear non-contiguous parts segment. purpose generative process needs manipulated initially whenever component sampled point following points segment cannot sampled again. generative process follows number transcripts number sentences across transcripts. clearly model levels sequential structure considered level level topic belongs broad category corresponding category base distribution turn drawn common base distribution dir. helps capture fact mixture components speciﬁc level- segments. documents clustered sentences clustered ﬁxed number segments number topics ﬁxed. topics shared across transcripts speciﬁc clusters sentences θ-distributions speciﬁc level- segments shared across transcripts θ-distributions transcripts-speciﬁc used. dos-classiﬁcation generative model news transcripts c−c−f c−f−n p−s; g−p−s; discuss inference model. need inference algorithm ensures segments formed. start joint distribution. collapse variables {h}{φ} perform gibbs sampling. feature likelihood function presence {ng} variables. handle these introduce auxiliary variables igk− level- changepoints i.e. datapoints prev. also note deterministically related. introduce variables simplify sampling. initialize variables sampling level- segmentation datapoints segments. variables sampled accordingly. iteration gibbs sampling consider statespace {igs− igs+} i.e. level- potential changepoints lying igs− igs+. process described algorithm here {bset} datapoints transcript segment major part gibbs sampling sample values segment conditioned remaining variables. done using chinese restaurant process component prosampled portional number times sampled provided bprevk procedure detailed algorithm called global inference considers overall structure transcript news transcript segmentation used news transcripts used hierarchical segmentation. here transcript news categoriespolitics national affairs international affairs business sportsﬁxed order. overall transcript tokens long news stories spread categories. task segment transcript levels. level segment correspond single story level segment correspond news category. endpoints sentences assumed known used deﬁne level- groups. sentences transcript. moreover already explained story occurs transcript thus reducing learnability. hence considered randomly chosen transcripts using initial segmentations sequence level- changepoints topics learnt using hdp. topics form initial estimate using performed inference individual sequences. inference provides variables based infer segmentation levels. gold standard segmentation available layers compute segmentation errors layers. computed taking average pk-measure three different values namely maximum minimum average lengths gold-standard segments look upon segmentation retrieval problem deﬁne precision recall level- segments also level- segments starting ending points inferred segment prev i.e. next then exists deﬁnes gold-standard segment satisfying inferred segment aligned gold standard segment precision recall segmentation deﬁned level- alignment threshold level baseline sticky hdp-hmm ladp level- using topics. level- ladp baseline. be-be-ce version since successful according news transcripts learnt topics selected report segmentation. also selected another outside learning used initial values results reported table clear terms measures considered gi-bgs outperformed competitors level-. level- also gi-bgs competitive three measures transcripts except trans. conclusions carried study various bayesian models hierarchically grouped data emphasis share mixture components distributions among groups. also introduced notion degree-of-sharing nomenclature models. described generalized bayesian model type data showed various existing models recovered next used develop model news transcripts several peculiar temporal structures also provided inference algorithm hierarchical unsupervised segmentation transcripts. showed model outperform existing ladp model task. concept opens table above comparison news transcript segmentation level- sticky hdp-hmm ladp gi-bgs. below news transcript segmentation level- ladp gi-bgs. lower value indicate better segmentation. bhattacharya bhattacharyya kanchi. subtle topic models discovering subprotly manifested software concerns automatically. ceedings international conference machine learning mitra ranganath bhattacharya. layered dirichlet process hierarchical segmentation sequential grouped data. european conference machine learning knowledge discovery databases. wulsin jensen litt. hierarchical dirichlet process model multiple levels clustering human seizure modeling. proceedings international conference machine learning", "year": 2015}