{"title": "Exploring Asymmetric Encoder-Decoder Structure for Context-based  Sentence Representation Learning", "tag": ["cs.NE", "cs.CL", "cs.LG"], "abstract": "Context information plays an important role in human language understanding, and it is also useful for machines to learn vector representations of language. In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning. As a result, we build an encoder-decoder architecture with an RNN encoder and a CNN decoder. We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance. Our model is trained on two different large unlabeled corpora, and in both cases transferability is evaluated on a set of downstream language understanding tasks. We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.", "text": "context information plays important role human language understanding also useful machines learn vector representations language. paper explore asymmetric encoder-decoder structure unsupervised context-based sentence representation learning. result build encoderdecoder architecture encoder decoder show neither autoregressive decoder decoder required. combine suite effective designs signiﬁcantly improve model efﬁciency also achieving better performance. model trained different large unlabeled corpora cases transferability evaluated downstream language understanding tasks. empirically show model simple fast producing rich sentence representations excel downstream tasks. learning distributed representations sentences important hard topic deep learning natural language processing communities since requires machines encode sentence rich language content ﬁxed-dimension vector ﬁlled continuous values. interested learning build distributed sentence encoder unsupervised fashion exploiting structure relationship large unlabeled corpus. since humans interpret sentences composing meanings words decompose task learning sentence encoder essential components learning distributed word representations learning compose sentence representation representations words given sentence. numerous studies human language processing claimed context words sentences understood plays important role human language understanding idea learning context information recently successfully applied vector representation learning words mikolov pennington collobert proposed uniﬁed framework learning language representation unlabeled data able generalize various tasks. inspired prior work incorporating context information representation learning kiros proposed skipthought model encoder-decoder model unsupervised sentence representation learning. paper exploits semantic similarity within tuple adjacent sentences supervision successfully built generic distributed sentence encoder. rather applying conventional autoencoder model skip-thought model tries reconstruct surrounding sentences instead input sentence. learned sentence representation encoder outperforms previous unsupervised pretrained models evaluation tasks ﬁnetuning results comparable models trained directly datasets supervised fashion. usage independent decoders skip-thought model matches intuition that given current sentence inferring previous sentence inferring next different. recently tang proposed skip-thought neighbor model decodes next sentence performance downstream tasks similar implementation skip-thought model. paper follow idea skip-thought neighbor model exploits subsequent context information learning representation bring asymmetry structure design well. proposed model asymmetric encoder-decoder structure keeps encoder decoder referred rnn-cnn model. components model design summarized tying word embeddings encoder word prediction layer decoder constrains input output space same also reduces number parameters regularizes model. model highly asymmetric terms training pairs model structure. speciﬁcally model encoder decoder. training encoder takes sentence input generates ﬁxed-dimension vector sentence representation; decoder applied reconstruct next sentence subsequent contiguous words difference generated sequence target sequence measured cross-entropy loss. illustration figure encoder encoder bi-directional gated recurrent unit experimented long-short term memory gru. since lstm didn’t give signiﬁcant performance boost generally runs faster lstm experiments stick using encoder. suppose sentence contains words transformed embedding matrix word vectors. bi-directional take word vector time forward backward direction; sets hidden states concatenated form hidden state matrix rd×m dimension representations representation provide model faster training speed better transferability existing algorithms thus choose apply parameter-free composition function concatenation outputs global mean pooling time global pooling time computed sequence hidden states. composition function represented figure proposed model composed encoder decoder. training batch sentences sent model encoder computes vector representation sentences; decoder needs reconstruct paired target sequence contains contiguous words right input sentence given vector representation. dimension word vectors. dimension sentence representation varies along change encoder size. operation d-th matrix outputs scalar. thus representation dimension decoder decoder -layer reconstruct paired target sequence needs expand length length intuitively decoder could stack deconvolution layers. fast training speed optimized architecture make plausible fullyconnected layers convolution layers decoder since generally convolution layers faster deconvolution layers modern deep learning frameworks. suppose target sequence words ﬁrst layer deconvolution expand could considered sequence length feature length easily implemented concatenation outputs linear transformations parallel. second third layer d-convolution layers kernel size respectively. output feature dimension word vectors. note decoder autoregressive model brings high training efﬁciency. discuss reason choosing decoder call predict-all-words decoder. objective softmax layer applied decoder produce probability distribution words position softmax training objective minimize negative log-likelihood positions target sequence follow idea encoder-decoder model using context information learning sentence representations unsupervised fashion. since decoder won’t used training quality generated sequences main focus important study design decoder. generally fast training algorithm preferred thus proposing decoder high training efﬁciency also strong transferability crucial encoder-decoder model. design decoder basically -layer convnet predicts words next sequence once. contrast existing work skip-thought kiros cnn-lstm autoregressive rnns decoders. table models bi-directional encoder default producing representation concatenation outputs global mean-pooling global max-pooling ·-max refers model global maxpooling. bold numbers best results among presented models. found inputting correct words autoregressive decoder necessary; predict-all-words decoders work roughly autoregressive decoders; mean+max pooling provides stronger transferability max-pooling alone does. table supports choice predict-all-words decoder producing vector representations bi-directional encoder. autoregressive model good generating sequences high quality language speech. however autoregressive decoder seems unnecessary encoder-decoder model learning sentence representations since won’t used training runs quite slow training. therefore conducted experiments test necessity using autoregressive decoder learning sentence representations ﬁndings. finding necessary input correct words autoregressive decoder terms learning good sentence representations. experimental design inspired bengio model designed experiment bi-directional encoder autoregressive decoder including cnn. started analyzing effect different sampling strategies input words learning auto-regressive decoder. compared autoregressive decoding settings using ground-truth words using previously predicted words using uniformly sampled words dictionary decoding settings named bengio results presented table generally three different decoding settings didn’t make much difference terms performance selected downstream tasks decoder. results tell that terms learning good sentence representations autoregressive decoder doesn’t require correct ground-truth words inputs. finding model autoregressive decoder works roughly model predict-all-words decoder. finding noticed correct ground-truth input words autoregressive decoder necessary terms learning sentence representations. therefore makes sense test whether need autoregressive model all. predict-all-words decoder described section stack convolutional layers words predicted output decoder. predict-all-words decoder built based decoder. keep number parameters roughly same replaced last convolutional layers bidirectional gru. results also presented table performance predict-all-words decoder signiﬁcantly differ autoregressive decoders observation observed decoders. since encoder bi-directional model multiple ways select/compute generated hidden states produce sentence representation. skip-thought sdae hidden state last time step produced encoder regarded vector representation given sentence expressive vector representing input sentence. followed idea proposed chen built model supervised snli task concatenates outputs global mean pooling global pooling serve sentence representation showed performance boost snli dataset. also conneau found model global pooling function stronger transferability model global mean pooling function supervised training snli. proposed rnn-cnn model empirically show mean+max pooling provides stronger transferability pooling does results presented table concatenation mean-pooling pooling function actually parameter-free composition function computation load negligible compared heavy matrix multiplications. also non-linearity pooling function augments mean pooling function building representation captures complex composition syntactic information. choose share parameters word embedding layer encoder word prediction layer decoder. tying proposed press wolf inan generally helps learn better language model. model tying also drastically reduces number parameters could prevent overﬁtting. furthermore initialize word embeddings pretrained word vectors wordvec glove since shown pretrained word vectors serve good initialization deep learning models likely lead better results random samples uniform distribution. studied hyperparameters model design based downstream tasks including sick-r sick-e ﬁrst model created reported section decent design following variations didn’t give much performance change except small improvements increasing dimensionality encoder. however think worth mentioning effect hyperparameters model design. present table supplementary material summarize follows decoding subsequent words adopted skip-thought training code gave reasonable good performance. words decoding didn’t give signiﬁcant performance gain took longer train. adding layers decoder enlarging dimension convolutional layers indeed sightly improved performance downstream tasks training efﬁciency main concerns decided wasn’t worth sacriﬁcing training efﬁciency minor performance improvement. increasing dimensionality encoder improved model performance additional training time brought less adding layers enlarging dimension convolutional layers decoder. reported results smallest largest models table large corpus used unsupervised training bookcorpus dataset contains million sentences books total. stable training adam algorithm optimization gradient clipping norm gradient exceeds certain value. since didn’t signiﬁcant difference wordvec glove initialization terms performance stick using word vectors wordvec initialize word embedding layer models. vocabulary unsupervised training contains frequent words bookcorpus. order generalize model trained relatively small ﬁxed vocabulary much larger possible english words kiros proposed word expansion method learns linear projection pretrained word embeddings wordvec learned word embeddings. thus model beneﬁts generalization ability pretrained word embeddings. downstream tasks evaluation include semantic relatedness paraphrase detection question-type classiﬁcation benchmark sentiment subjective datasets includes movie review sentiment customer product reviews subjectivity/objectivity classiﬁcation opinion polarity semantic textual similarity unsupervised training bookcorpus dataset parameters encoder apply sentence representation extractor tasks. order compare effect different corpora also trained models amazon book review dataset largest subset amazon review dataset million sentences tokenization twice large bookcorpus. training evaluation models conducted pytorch used senteval provided conneau evaluate transferability models different settings. models trained number iterations batch size performance measured training models. table presented results evaluation tasks proposed rnn-cnn models related work. small rnn-cnn refers model dimension representation large rnn-cnn refers results model snli found table work inspired analyzing skip-thought model skip-thought model successfully applied form learning context information unsupervised representation learning sentences model learns encode current sentence decode surrounding sentences then augmented lstm proposed layer-normalization improved skip-thought model generally model unsupervised training unordered sentences unigram-tfidf paragraphvec wordvec fasttext glove sdae unsupervised training ordered sentences bookcorpus discsent‡ fastsent fastsent+ae skip-thought skip-thought+ln combine cnn-lstm small rnn-cnn† large rnn-cnn† unsupervised training ordered sentences amazon book review small rnn-cnn† large rnn-cnn† unsupervised training ordered sentences amazon review byte m-lstm supervised training transfer learning en-to-fr captionrep dictrep bilstm-max bilstm-max supervised task-dependent training transfer learning nb-svm adasent tree-lstm tf-kld table related work comparison. presented table designed asymmetric rnn-cnn model strong transferability overall better existing unsupervised models terms fast training speed good performance evaluation tasks. table presents model comparison. refer models small/large refers dimension representation indicates discsent model trained additional data wikipedia gutenberg project. bold numbers best ones among models training transferring setting underlined numbers best results among unsupervised representation learning models. performance measures pearson’s spearman’s score. msrp performance measures accuracy score. downstream tasks. instead applying rnns model hill proposed fastsent model learns source target word embeddings generalization cbow sentence-level learning composition function word embeddings summation operation. later applied encoder called cnn-lstm model. proposed composition model follows idea encoding current sentence predicting next sentence; proposed hierarchical model leverages context information sentence-level paragraph-level learning encode current sentence predict next model another process sentence representation time paragraph-level. model falls category encoder-decoder model. however propose efﬁcient effective model. instead decoding surrounding sentences skip-thought fastsent compositional cnn-lstm model decodes subsequent sequence ﬁxed length. compared hierarchical cnn-lstm model showed that proper model design next-words context information sufﬁcient learning sentence representations. another unsupervised approach learn discriminative model distinguishing whether target sentence context source sentence also discourse information. discsent proposed learn classiﬁer representations judges whether sentences adjacent other whether sentences correct order whether second sentence starts conjunction phrase. dissent pointed human annotated explicit discourse relations also good learning sentence representations. promising research direction since proposed models generally computational efﬁcient clear intuition. however performance downstream tasks still worse encoder-decoder models. proposed radford byte m-lstm model uses multiplicative lstm unit learn language model amazon review data mcauley model works reasonably well downstream tasks since rnns able produce distributed representation given left-context information sentence document. experiment also trained rnn-cnn model amazon book review largest subset amazon review dataset indeed performance gain single-sentence classiﬁcation tasks. performance gain experiment also byte m-lstm brought matching corpus domain domain downstream tasks raises questions corpus good learning sentence representations whether downstream tasks comprehensive cover sufﬁcient aspects sentence. previously mentioned models learned ordered sentences unordered sentences also used learning representations sentences. paragraphvec learns ﬁxed-dimension vector sentence predicting words within given sentence. however training representation sentence hard derive since requires optimizing sentence representation towards objective. sdae learns sentence representations denoising auto-encoder model. noise added encoder replacing words ﬁxed token swapping words speciﬁc probability. proposed rnn-cnn model trains faster sdae does since decoder runs faster decoder sdae since utilized sentence-level continuity supervision sdae doesn’t model largely performs better sdae. supervised transfer learning also promising able large enough labeled data. conneau applied bi-directional lstm sentence encoder multiple fully-connected layers deal snli multinli trained model demonstrates impressive transferability downstream tasks including supervised unsupervised. direct discriminative training signal pushes encoder focus semantics given sentence learns boost performance beats methods. rnn-cnn model trained amazon book review data better results supervised classiﬁcation tasks bilstm-max does performance semantic relatedness tasks inferior bilstm-max. argue labeling large amount training data time-consuming costly; unsupervised learning could potentially provide great initial point human labeling making less costly efﬁcient. table implemented classiﬁer mentioned vendrov features computed model. proposed rnn-cnn model gets similar result snli skip-thought much less training time. inspired learning exploit contextual information present adjacent sentences proposed asymmetric encoder-decoder model suite techniques improving context-based unsupervised sentence representation learning. since believe simple model faster training easier analyze simple techniques proposed model including encoder predict-all-words decoder learning inferring next contiguous words mean+max pooling tying word vectors word prediction. thorough discussion extensive evaluation justify decision making component rnn-cnn model. terms performance efﬁciency training justify model fast simple algorithm learning generic sentence representations unlabeled corpora. research focus maximize utility context information design simple architectures best make gratefully thank jeffrey elman benjamin bergen ndapa nakashole seana coulson insightful discussion thank mengting reina mizrahi thomas donoghue suggestive chatting. also thank adobe research gpus support thank support references eneko agirre carmen banea claire cardie daniel mona diab aitor gonzalez-agirre weiwei rada mihalcea german rigau janyce wiebe. semeval- task multilingual semantic textual similarity. semevalcoling ronan collobert jason weston léon bottou michael karlen koray kavukcuoglu pavel kuksa. natural language processing scratch. journal machine learning research alexis conneau douwe kiela holger schwenk loïc barrault antoine bordes. supervised learning universal sentence representations natural language inference data. emnlp marco marelli stefano menini marco baroni luisa bentivogli raffaella bernardi roberto zamparelli. sick cure evaluation compositional distributional semantic models. lrec richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment treebank. yukun ryan kiros richard zemel ruslan salakhutdinov raquel urtasun antonio torralba sanja fidler. aligning books movies towards story-like visual explanations watching movies reading books. iccv table architecture comparison. shown table designed asymmetric rnn-cnn model works better asymmetric models models symmetric structure addition larger encoder size model demonstrates stronger transferability. default setting decoder learns reconstruct words right next every input sentence. represents decoder length outputs represents length outputs indicates decoder learns reconstruct next sentence. indicates results reported future predictor. encoder experiment noted based adasent zhao conneau al.. bold numbers best results among models dimension underlined numbers best results among models. performance measures pearson’s spearman’s score. msrp performance measures accuracy score.", "year": 2017}