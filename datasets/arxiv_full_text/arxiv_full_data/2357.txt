{"title": "Adaptive Importance Sampling for Estimation in Structured Domains", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Sampling is an important tool for estimating large, complex sums and integrals over high dimensional spaces. For instance, important sampling has been used as an alternative to exact methods for inference in belief networks. Ideally, we want to have a sampling distribution that provides optimal-variance estimators. In this paper, we present methods that improve the sampling distribution by systematically adapting it as we obtain information from the samples. We present a stochastic-gradient-descent method for sequentially updating the sampling distribution based on the direct minization of the variance. We also present other stochastic-gradient-descent methods based on the minimizationof typical notions of distance between the current sampling distribution and approximations of the target, optimal distribution. We finally validate and compare the different methods empirically by applying them to the problem of action evaluation in influence diagrams.", "text": "sampling important large complex sums integrals dimensional spaces. instance sampling methods inference want sampling vides optimal-variance present distribution obtain information stochastic-gradient-descent tially direct present based minimization distance bution approximations distribution. finally validate different problem action evaluation diagrams. sampling provides computing posed alternative particular references belief inference shachter charnes shenoy references ortiz kaelbling form simpler used \"prior\" importance-sampling dis­ tribution value evidence. tribution timates peot instance tion case belief inference served variables posterior given observed would know answer belief often interested computing large sums expectations domains. instance works requires remaining interest. variables order solve problem action selection diagrams time decision different action choices. several estimation tion discussed above samples paper pro­ sequentially pose methods systematically update importance-sampling process learning stochastic-gradient imization importance stochastic-gradient methods one-dimensional stance means component denote possible values values take take possible also denote capital nodes graph. denote parents directed per. denote operator function variabies expression hlo=o stands function variables values assignment values remain unassigned. hlo=o notation means variable influence decision-making decision example build shown fig­ square decision node. diamond utility butions assume simplicity graph. joint distribution action choice assigned value given evidence fore resulting variables \"prior\" importance-sampling method. context called likelihood-weighting \"likelihood\" \"likelihood.\" weights weight function also note need avoid letting small respect matter fact variance. least value implies importance-sampling distributions suf­ ficiently view tion variables importance-sampling view sampling. sampling factorization estimation able represent need arcs connect observations parents already connected. however size model particularly ditional compact parametric paper deal issue instead trate problem learning ture original update local conditional probability obtain samples. update methods subsection idea minimizing different notions current distribu­ tion optimal importance-sampling distribution build samples. hope empirical distribution pling distribution. rameterized interpret divergence importance-sampling version kl-divergence pirical values relative estimated state. one. overestimated version rithm function subtract therefore log­ arithm brings amount underestimation mation scale adds subtracts weight cordingly. different problems discussed druzdzel references belong class forward samplers distribution these self-importance peot shwe methods proposed sampling samples. ilar derived annealed importance technique sampling sets sequence optimal distribution chains. \"anneal\" pling distribution. pendent sample distribution process finally estimate. distributions ple. technique whether applied paper. currently tions methods technique. evaluated squared-error tation interest adaptive sampling method. first results show methods achieve problem. competitive. method based minimization variance \"kl\" \"kls\" methods based global respectively. minimization update methods take shows graph true variance tion learned using different tion total number samples used. horizontal line shows variance tribution original bn). ther analysis general tradeoff setting updates forms better function defined pling distribution sampling both. stable methods gesting undertaking. variance cases error function error surface conjecture would like thank milos hauskrecht mann kee-eung thomas dean many discus­ sions feedback. functionality thank kevin also like thank anonymous sightful", "year": 2013}