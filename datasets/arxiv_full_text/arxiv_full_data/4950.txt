{"title": "Repeated Inverse Reinforcement Learning", "tag": ["cs.AI", "cs.LG"], "abstract": "We introduce a novel repeated Inverse Reinforcement Learning problem: the agent has to act on behalf of a human in a sequence of tasks and wishes to minimize the number of tasks that it surprises the human by acting suboptimally with respect to how the human would have acted. Each time the human is surprised, the agent is provided a demonstration of the desired behavior by the human. We formalize this problem, including how the sequence of tasks is chosen, in a few different ways and provide some foundational results.", "text": "introduce novel repeated inverse reinforcement learning problem agent behalf human sequence tasks wishes minimize number tasks surprises human acting suboptimally respect human would acted. time human surprised agent provided demonstration desired behavior human. formalize problem including sequence tasks chosen different ways provide foundational results. challenge building agents learn experience goals rewards. reinforcement learning setting interesting answer question inverse agent infers rewards human observing human’s policy task unfortunately problem ill-posed typically many reward functions observed behavior optimal single task heuristics select among feasible reward functions successful applications problem learning demonstration identifying reward function poses fundamental challenges question well safely agent perform using learned reward function tasks. formalize multiple variations repeated problem agent human face multiple tasks time. separate reward function components invariant across tasks viewed intrinsic human second task speciﬁc. motivating example consider human tasks throughout work e.g. getting coffee driving work interacting co-workers tasks task-speciﬁc goal human brings task intrinsic goals correspond maintaining health ﬁnancial well-being violating moral legal principles etc. repeated setting agent presents policy task thinks human would agent’s policy surprises human sub-optimal human presents agent optimal policy. objective agent minimize number surprises human i.e. generalize human’s behavior tasks. addition addressing generalization across tasks repeated problem introduce results interest resolving question unidentiﬁability rewards observations standard irl. results also interest particular aspect concern make sure systems build safe safety. speciﬁcally issue reward misspeciﬁcation often mentioned safety articles articles mostly discuss broad ethical concerns possible research disummary form contributions include efﬁcient reward-identiﬁcation algorithm agent choose tasks observes human behavior; upper bound number total surprises assumptions made tasks along corresponding lower bound; extension setting human provides sample trajectories instead complete behavior; identiﬁcation guarantees agent choose task rewards given ﬁxed task environment. speciﬁed state space action space initial state distribution transition function reward function discount factor assume ﬁnite space distributions policy describes agent’s behavior specifying action take state. often difﬁcult explicitly describe. contrast task-speciﬁc reward could reward agent successfully completing parallel parking. easier construct completely capture human deems good driving. formally task deﬁned pair task environment assume tasks share differ initial distribution dynamics task reward task-specifying quantities known agent. task human’s optimal behavior always respect reward function emphasize intrinsic human remains across tasks. possible parameters r|s| contains throughout assume human’s reward bounded normalized magnitude kθ⋆k∞ demonstration reveals optimal environment agent. common assumption literature full mapping revealed unrealistic states unreachable initial distribution. address issue requiring state occupancy vector section show also allows easy extension setting human demonstrates trajectories instead providing policy. agent chooses tasks observes human’s behavior them infers reward function. setting agent powerful enough choose tasks arbitrarily show agent able identify human’s reward function course implies ability generalize tasks. nature chooses tasks agent proposes policy task. human demonstrates policy agent’s policy signiﬁcantly suboptimal setting derive upper lower bounds number mistakes agent make. note impossible identify watching human behavior single task. fundamentally indistinguishable inﬁnite reward functions yield exactly policy observed task. introduce idea behavioral equivalence tease apart separate issues wrapped challenge identifying rewards. argue task identifying reward function amount identifying equivalence class belongs. particular identifying equivalence class sufﬁcient perfect generalization tasks. remaining unidentiﬁability merely representational real consequence. next present constraint captures reward functions belong equivalence class. proposition reward functions behaviorally equivalent tasks all- vector length |s|. section protocol agent chooses sequence tasks task human reveals optimal environment reward function goal design algorithm chooses identiﬁes desired accuracy using tasks possible. theorem shows simple algorithm identify tasks tasks chosen. roughly speaking algorithm amounts binary search component manipulating task reward proof algorithm speciﬁcation. noted before agent identiﬁed within appropriate tolerance compute sufﬁciently-near-optimal policy tasks thus completing generalization objective stronger identiﬁcation objective setting. theorem exists algorithm outputs r|s| satisﬁes θ⋆k∞ demonstrations. proof. algorithm chooses following ﬁxed environment tasks {sref} action self-loop action transitions sref. sref actions cause selfloops. initial distribution states uniformly random {sref}. task differs task reward always). observing state occupancy demonstrated optimal policy chooses sref ﬁrst time step consequently task learn relationship {sref} conducting binary search manipulating identify ǫ-accuracy tasks. setting suffer degeneracy issue. however generalization future tasks ultimate goal easy special case initial demonstration agent mimic behave optimally subsequent tasks without requiring demonstrations. generally nature repeats similar tasks agent obtains little information presumably knows behave cases; nature chooses task unfamiliar agent agent likely learn mistake. less human satisﬁed demonstration needed; otherwise mistake counted computed agent needed knowledge task). main goal section design algorithm provable guarantee total number mistakes. human supervision require human evaluate agent’s policies addition providing demonstrations. argue reasonable assumption binary signal needed opposed precise value policy suboptimal human fails realize arguably treated mistake. meanwhile also provide identiﬁcation guarantees section human relieved supervision duty identiﬁed. using equation effectively given environment round induces state µtpt want agent choose vector largest product exponential size concern main result dependence number vectors depends dimension vectors. result enabled studying linear bandit version problem subsumes setting purpose also model independent interest. linear bandit setting ﬁnite action space size task denoted pair task speciﬁc reward function before. feature matrix feature vector i-th action reduce mdps linear bandits element corresponds policy feature vector before task reward human’s unknown reward respectively. initial uncertainty value i-th action calculated action maximizes value. every round agent proposes action note straightforward conversion letting µtpt also preserves losses. perform succinct conversion example canonicalizing dropping coordinate sref relevant vectors. ered reward linear state features setting reward parameters actual reward setting also reduced linear bandits similarly example except state occupancy replaced discounted expected feature values. main result theorem still apply automatically guarantee depend dimension feature space dependence |s|. include conversion discuss example consider problem state features deﬁned task reward background reward state r⊤φt respectively suppose kφtk∞ always holds convert linear bandit problem follows remain same. purpose normalization prove theorem quote result linear programming literature lemma found standard lecture notes theorem also lemma ..). lemma given non-degenerate ellipsoid centered non-zero vector minimum-volume enclosing ellipsoid vol/vol proof theorem whenever mistake made induce constraint meanwhile since greedy w.r.t. mistakes inevitable worst case nature chooses tasks. provide proof sketch below complete proof deferred appendix proof sketch. randomize sampling element i.i.d. unif). prove exists strategy choosing algorithm’s expected number mistakes proves theorem less average. phases remains phase. every phase rounds enumerated adversary shift posterior centered around origin; agent probability make error posterior interval halved. overall agent makes mistakes phase phases total gives lower bound. applying lower bound mdps lower bound stated linear bandits. principle need prove lower bound mdps separately linear bandits general mdps purpose hard instances linear bandits corresponding instances. lemma below show certain type linear bandit instances always emulated mdps number actions hard instances constructed theorem indeed satisfy conditions type; particular require feature vectors theorem successfully controls number total mistakes completely avoids identiﬁcation problem guarantee recover section explore conditions obtain identiﬁcation guarantees nature chooses tasks. ﬁrst condition stated proposition implies made possible mistakes indeed identiﬁed identiﬁcation accuracy determined tolerance parameter deﬁnes counted mistake. space limit proof deferred appendix proposition shows identiﬁcation guaranteed agent exhausts mistakes agent ability actively fulﬁll condition nature chooses tasks. stronger identiﬁcation guarantee need grant agent freedom choosing tasks. unfortunately degenerate case easily constructed prevents revelation information particular i.e. environment completely uncontrolled actions equally optimal nothing learned. generally never recover along direction fact proposition viewed instance result remove redundancy example or|s| discuss identiﬁcation mdps. therefore guarantee identiﬁcation ﬁxed environment feature vectors must signiﬁcant variation directions capture intuition deﬁning diversity score spread showing identiﬁcation accuracy depends inversely score previous sections assumed human evaluates agent’s performance based state occupancy agent’s policy demonstrates optimal policy terms state occupancy well. practice would like instead assume task agent rolls trajectory human shows optimal trajectory he/she ﬁnds agent’s trajectory unsatisfying. still concerned upper bounding number total mistakes provide parallel version theorem unlike traditional setting agent also acting gives rise many subtleties. first total reward agent’s single trajectory random variable deviate expected value policy. therefore generally impossible decide agent’s policy near-optimal instead assume human check action agent takes trajectory near-optimal agent takes state error counted resolves issue agent’s side human provide his/her optimal trajectory? straightforward protocol human rolls trajectory initial distribution task argue reasonable protocol reasons expectation reward collected human less agent conditioning event error spotted introduce selection bias; human encounter problematic state his/her trajectory hence information provided trajectory irrelevant. discuss prove parallel theorem protocol. first let’s assume demonstration still given form state occupancy vector starting problematic state. case reduce setting section changing point mass problematic state. apply algorithm analysis section remains show notion error section implies notion error section problematic state agent’s policy tackle remaining issue demonstration terms single trajectory update mistake algorithm make update every mini-batch mistakes aggregate form accurate update rules. algorithm formal guarantee algorithm stated theorem whose proof deferred appendix existing work focused inferring reward function using data acquired ﬁxed environment prior work using data collected multiple exogenously ﬁxed environments predict agent behavior also applications methods singleenvironment mdps adapted multiple environments nevertheless works consider objective mimicking optimal behavior presented environment generalization tasks main contribution paper. recently hadﬁeld-menell proposed cooperative inverse reinforcement learning human agent environment allowing human actively resolve agent’s uncertainty reward function. however consider single environment unidentiﬁability issue still exists. combining interesting framework resolution unidentiﬁability interesting future direction. work supported part grant part rackham predoctoral fellowship university michigan opinions ﬁndings conclusions recommendations expressed authors necessarily reﬂect views sponsors.", "year": 2017}