{"title": "Hierarchical Mixture-of-Experts Model for Large-Scale Gaussian Process  Regression", "tag": ["stat.ML", "cs.AI", "cs.LG", "stat.CO"], "abstract": "We propose a practical and scalable Gaussian process model for large-scale nonlinear probabilistic regression. Our mixture-of-experts model is conceptually simple and hierarchically recombines computations for an overall approximation of a full Gaussian process. Closed-form and distributed computations allow for efficient and massive parallelisation while keeping the memory consumption small. Given sufficient computing resources, our model can handle arbitrarily large data sets, without explicit sparse approximations. We provide strong experimental evidence that our model can be applied to large data sets of sizes far beyond millions. Hence, our model has the potential to lay the foundation for general large-scale Gaussian process research.", "text": "propose practical scalable gaussian process model large-scale nonlinear probabilistic regression. mixture-of-experts model conceptually simple hierarchically recombines computations overall approximation full gaussian process. closed-form distributed computations allow efﬁcient massive parallelisation keeping memory consumption small. given sufﬁcient computing resources model handle arbitrarily large data sets without explicit sparse approximations. provide strong experimental evidence model applied large data sets sizes beyond millions. hence model potential foundation general large-scale gaussian process research. probabilistic gaussian processes method choice probabilistic regression non-parametric nature allows ﬂexible modelling without specifying low-level assumptions advance. moreover standard gaussian likelihood inference performed closed form principled simply applying bayes’ theorem. substantial impact various research areas including geostatistics optimisation data visualisation robotics reinforcement learning spatio-temporal modelling active learning strength model used fairly reliably black-box function approximator i.e. produces reasonable predictions without manual parameter tuning. practical limitation model computational demand standard implementations training predicting scale respectively size training data set. large often sparse approximations typically sparse approximations lower computational burden implicitly using subset data. scale multiple tens hundreds thousands data points. however even sparse approximations inconceivable apply data sizes tens hundreds millions data points. alternative sparse approximations distribute computations using local models. local models typically require stationary kernels notion distance locality. shen used kd-trees recursively partition data space means multiresolution tree data structure allows scale multiple tens thousands training points. however approach proposed provide solutions variance predictions limited stationary kernels. similarly used heuristic partitioning scheme based locality notion stationary kernels real-time mean predictions gps. along lines exploiting locality mixture-of-experts models applied regression however models primarily used speed regression rather allow heteroscedasticity non-stationarity. local model possesses hyper-parameters optimised. predictions made collecting predictions local expert models weighing using responsibilities assigned gating network. models dirichlet process prior placed multinomial responsibility vector local expert allows data-driven partitioning unfortunately inference models computationally intractable requiring mcmc sampling variational inference assign data points expert computationally demanding process. within context spatio-temporal models data points propose efﬁcient inference exploits computational advantages separable compactly supported kernels leading sparse kernel matrices. authors propose approximate sampling methods deal noisy missing data. computationally demanding computations inversion determinant scale standard implementation. given hyper-parameters training test input posterior predictive distribution corresponding function value gaussian mean variance given working hypothesis standard model latent function however data size standard applicable. instead sparse approximation address computational memory issues full distributing computational memory load many individual computational units operate subsets data. results approximation full approximation computed efﬁciently exploiting massive parallelisation. consider training data deﬁne subsets data subsets full training expert. local expert models computes means variances conditioned respective training data. predictions recombined mean/variance parent node subsequently play role expert next level model architecture. recursively applying recombinations model results tree structured architecture recently proposed distributed approach scales variational sparse exploiting distributed computations. particular derive exact re-parameterisation variational sparse model titsias update variational parameters independently different nodes. implemented within map-reduce framework corresponding architecture consists central node many local nodes i.e. single-layer architecture. paper follow approach orthogonal sparse approximations order scale full large data sets exploiting massive parallelisation. particular propose hierarchical mixture experts model distributes computational load amongst large independent computational units. model recursively recombines computations independent units overall inference/training procedure. idea related tresp’s bayesian committee machine combines estimators. advantage model computations performed analytically i.e. sampling required. given sufﬁcient computing power model handle arbitrarily large data sets. experiments demonstrate model applied easily data sets size exceeds typical data sizes sparse deal orders magnitude. however even limited computing resources model practical sense train full million training points less half hour laptop. consider regression setting i.i.d. gaussian noise. small data data {xi}n sizes gaussian process methods choice probabilistic non-parametric regression. gaussian process non-parametric bayesian model often used regression. deﬁned collection random variables ﬁnite number gaussian distributed. fully speciﬁed mean function covariance function paper assume prior mean function furthermore consider gaussian kernel train model make predictions assumption expert independent experts level tree allows parallelise distribute computations independent processing units. assumption level independent effectively results blockdiagonal covariance matrix efﬁciently inverted distribution. partition covariance matrix would composed block-diagonal elements original gp’s covariance matrix however partition information structure data lost would otherwise captured cross-covariance terms full since model aims replicate full interested mitigating effects independence assumption. sharing arts training amongst multiple subsets thereby account covariance effects points degree. approach illustrated bottom path fig. parts training shared amongst individual nodes. note memory consumption kept constant since training modiﬁed creating subsets training data using subset forms basis hierarchical model divided problem number using smaller training data set. done recursively subdividing problem desired training size leaf-gps achieved. efﬁcient implementation number data subsets correspond multiple number computing units available. disjoint data sets every leaf-gp would possess data points; allow shared data sets number scales linearly degree sharing. instance every data point appears local experts single-layer model would size n/c. various ways assigning data points experts leaves tree fig. instance random assignment fast work quite well. results however based different approach first kd-tree recursively divide input space non-overlapping regions. terminate division required number partitions reached. second region partitioned disjoint groups inputs. third construct data local experts contains exactly group region. procedure data contain points across entire input space rather clustered region input space. note neither method assigning data points experts relies locality notion induced kernel. respect kernel hyper-parameter shared amongst individual gps. factorising approximation implied independence assumption individual models. term given figure single-layer model partitioning data leads block-diagonal approximation kernel matrix duplicating data points clear separation blocks smoothed effects independence assumption mitigated. requires inversion determinant matrix size data associated expert computations performed time standard implementation. note signiﬁcantly smaller i.e. size full data set. memory consumption individual model. note number parameters optimised relatively small since consider additional variational parameters inducing inputs optimise. gradients respect computed independently nodes allows massive parallelisation signiﬁcant speed-up training compared training full evaluate training time model computed amount time required compute logmarginal likelihood gradients respect kernel hyper-parameters. typical optimisation procedure kernel hyper-parameters e.g. conjugate gradients bfgs requires values. full training time proportional time takes compute log-marginal likelihood gradient chose computer architecture nodes cores each. furthermore chose three-layer model varying widths data sets data points leaf nodes possessed data points each data sizes fig. shows time required computing logmarginal likelihood gradient respect hyper-parameters. horizontal axis shows size training left vertical axis shows computation time seconds model full sparse inducing inputs sparse model chose number inducing inputs size training i.e. computation time order offsets curve full taking even fewer inducing inputs would push sparse approximation towards multiple-hundred thousand data points. however done data possesses high degree redundancy. right vertical axis shows number leaf i.e. number experts amongst distribute computation. training time full reaches impractical number data sizes sparse model reasonably trained data points. computational time required compute marginal likelihood gradients significantly lower full scaled computational overhead. mean predictive distribution written weighted means child-gps’ predictive distributions. weights child-gps’ predictions proportional inverse variances prediction allows accurate predictions bigger weights ﬁnal prediction less accurate predictions weights closer zero. this general allows remain effective across various methods assigning data points child-gps. thus described single-level version model child-gps standard gps. since possesses interface standard hgps child-gps hgp. done recursively build tree structure arbitrary depth width. training expressed log-marginal likelihood marginal likelihoods child-gps. child-gps also hgps expanded expressed log-marginal likelihood child-gps child-hgps. generalises hgps arbitrary number levels always write log-marginal likelihood logmarginal likelihood leaf equation shows log-marginal likelihood multi-level marginal-likelihoods leaves equivalent log-marginal likelihood single-level leaves child-gps hence structure leaves effect computation log-marginal likelihood. figure computing time log-marginal likelihood gradient respect kernel hyper-parameters function size training data. scales favourably large-scale data sets. increasing number child-gps scales data points. training data points required amount time training full sparse data points. ﬁgure shows problem size able architecture allows compute marginal likelihood within feasible amount time. even computing infrastructure available model useful practice performed full training cycle data points standard laptop minutes. also clear indicator memory consumption relatively small. however immediately obvious predictive distribution equivalent predictive distribution single-level leaves. show this provide simple proof resulting distribution product arbitrary number gaussians turn product gaussians gaussian equivalent distribution resulting product sub-gaussians. i.e. distribution product gaussian distributions equivalent distribution product sub-gaussians. result generalises number levels gaussian products completes proof equivalence multi-level single-level leaves therefore mathematically matter whether choose shallow deep architecture leaf same. however multi-level still makes sense computational point view. multi-level hgps given leaf-gps depth effect model training prediction shown section although mathematically necessary construct level practice multi-level allows fully utilise given distributed computing hardware. implement single-level distributed system master node responsible computational work computational work child-gps distributed evenly across slave nodes. set-up imposes heavy communication computational load master node since manage communication slave nodes perform computations required combining child-gps optimal resources exploit fact model figure ﬂexibility choosing amongst equivalent architectures enables evenly distribute computational work. blue vertical rectangle represents distributed computing unit coloured node denotes hgp. coloured rectangles represent overall responsibility corresponding coloured nodes. overlap coloured rectangles blue represent computing resources available computational work related particular hgp. main computations performed leaf-experts invariant presence intermediate layers propose better solution illustrated fig. starting tree divide number computational nodes available groups number child-gps possesses assign child-gp/child-hgp group. recursively reach leaves single node available hgp. approach leads uniform distribution network communication computational load amongst nodes. number experts fig. illustrates effect number leaf-gps accuracy hgp. constructed hgps levels respectively training data size resulted experts data sets sizes respectively. number experts increases accuracy decreases. especially leaves expert enough data points model underlying latent function reasonably well. however training data experts recovers prediction accuracy full figure comparison hgps varying depths ground truth mean functions corresponding predictive intervals shown. model accuracy decreases number experts. bottom ﬁxed depth model becomes similar ground truth model increasing number data points. lock single thread python code executed point time. therefore threads python context provides logical concurrency terms programs true simultaneous computations. exists workaround true concurrency problem python processes instead threads perform simultaneous computations. posix model threads lightweight units computations belonging process thus sharing memory space. processes memory space come increased system overheads compared threads. however linux creation duplicate processes incur large memory costs since linux implements copyon-write model. means process forks memory copied unless process attempts modify context implementation make modiﬁcation training data shared amongst child-gps. terms memory usage child-gp needs compute kernel matrix corresponding jacobian matrix hyperparameter interaction childgp. therefore computing child-gp using separate section apply model real-world data sets. full model optimised kernel hyper-parameters maximising log-marginal likelihood using bfgs. following experiments used single standard desktop computer four cores applied model kink data kink data generated simulation forward dynamics -link all-revolute robot arm. task data predict distance endeffector target using joint positions twist angles. training size test size input dimension trained model various architectures ranging single-layer model four experts model layers experts. chose different architec distributed sparse variational line results reported compared sparse variational methods achieves substantially better predictive performance. additionally converged tens iterations whereas sparse variational methods require hundreds thousands iterations. approach scaling conceptually straightforward practical recursively recombines computations independent lower-level experts overall prediction. unlike approach scaling large data sets model explicit sparse approximation full therefore leaf nodes still perform full computations i.e. computations scale cubically number data points. however number data points leaf controlled adjusting number leaves. limit single expert hierarchical model equivalent standard additionally even single expert hierarchical mixture-of-experts model still gaussian process ﬁnite number function values gaussian distributed although make implicit block-diagonal approximation covariance matrix. note deep model actually model kernel hyper-parameters shared amongst local experts. makes sense objective reproduce full vanilla which practical reasons cannot applied problem. shared hyper-parameters also suffer much overﬁtting problems even local expert poor model gradient small contribution number local models high. training model relatively easy besides shared hyper-parameters additional parameters inducing inputs i.e. compared approaches less likely local optima. main purpose model scale vanilla distributing computational load. therefore kinds variations conceivable standard could also implemented model. particular includes classiﬁcation sparse approximations heteroscedastic models non-gaussian likelihoods. note models would necessary level leaf computations recombinations computations leaves. compared mixture-of-experts models chose simplifying assumption know number experts case corresponds individual computing units. thus need dirichlet process table summarises results. report training time bfgs iteration number data points computational unit likelihood ratio lrgp tells close model full likelihood ratio distributions deﬁned basic single-level four experts able achieve similar results signiﬁcantly shorter amount time. performance decreased increasing depth since number data points expert becomes small discussed fig. considered data reporting ﬂight arrival departure times every commercial ﬁght january april dataset contains extensive information almost million ﬂights including delay reaching destination. data followed procedure described randomly selected data points used random subset samples train model test chose eight input variables aircraft distance needs covered airtime departure arrival times week month month. data evaluated sparse variational methods deal training size. applied model using data duplication experts data points each. data assigned randomly expert gps. repeated experiment four times. average training time minutes bfgs iterations. table reports average rmse values predicting airline delays. table also relates results predicting airline delays corresponding results reported inducing points used sparse variational presented conceptually straightforward effective hierarchical model allows scale probabilistic gaussian processes arbitrarily large data sets. idea behind model massively parallelise computations distributing amongst independent computational units. recursive recombination independent computations results practical hierarchical mixture-of-gp-experts model computationally memory efﬁcient. compared recent sparse approximations model performs well learns fast requires little memory suffer high-dimensional variational optimisation inducing inputs. demonstrated model scales well large data sets training million data points takes less minutes laptop computing power training data points done hours. model presented paper lays foundation variety future work context gaussian processes including classiﬁcation non-gaussian likelihoods regression combination sparse methods level leaf nodes mixture-of-experts model.", "year": 2014}