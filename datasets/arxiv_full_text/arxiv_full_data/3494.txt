{"title": "Clustering of Data with Missing Entries using Non-convex Fusion  Penalties", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "The presence of missing entries in data often creates challenges for pattern recognition algorithms. Traditional algorithms for clustering data assume that all the feature values are known for every data point. We propose a method to cluster data in the presence of missing information. Unlike conventional clustering techniques where every feature is known for each point, our algorithm can handle cases where a few feature values are unknown for every point. For this more challenging problem, we provide theoretical guarantees for clustering using a $\\ell_0$ fusion penalty based optimization problem. Furthermore, we propose an algorithm to solve a relaxation of this problem using saturating non-convex fusion penalties. It is observed that this algorithm produces solutions that degrade gradually with an increase in the fraction of missing feature values. We demonstrate the utility of the proposed method using a simulated dataset, the Wine dataset and also an under-sampled cardiac MRI dataset. It is shown that the proposed method is a promising clustering technique for datasets with large fractions of missing entries.", "text": "abstract—the presence missing entries data often creates challenges pattern recognition algorithms. traditional algorithms clustering data assume feature values known every data point. propose method cluster data presence missing information. unlike conventional clustering techniques every feature known point algorithm handle cases feature values unknown every point. challenging problem provide theoretical guarantees clustering using fusion penalty based optimization problem. furthermore propose algorithm solve relaxation problem using saturating non-convex fusion penalties. observed algorithm produces solutions degrade gradually increase fraction missing feature values. demonstrate utility proposed method using simulated dataset wine dataset also under-sampled cardiac dataset. shown proposed method promising clustering technique datasets large fractions missing entries. widely used discover natural groupings large datasets labeled pre-classiﬁed samples available apriori. speciﬁcally assigns object group similar objects within group dissimilar objects groups. example applications include analysis gene experssion data image segmentation identiﬁcation lexemes handwritten text search result grouping recommender systems wide variety clustering methods introduced years; review classical methods. however consensus particular clustering technique works well tasks pros cons existing algorithms. common clustering techniques k-means k-medians spectral clustering implemented using lloyd’s algorithm non-convex thus sensitive initialization. recently linear programming semi-deﬁnite programming based convex relaxations k-means k-medians algorithms introduced overcome dependence initialization. unlike lloyd’s algorithm relaxations provide certiﬁcate optimality. however mentioned techniques require apriori knowledge desired number clusters. hierarchical clustering methods produce easily interpretable visualizable clustering results varying number clusters introduced overcome challenge. drawback sensitivity initial guess perturbations data. recent convex clustering technique retains advantages hierarchical clustering invariant initialization producing unique clustering path. theoretical guarantees successful clustering using convex-clustering technique also available clustering algorithms cannot directly applied real-life datasets large fraction samples missing. example gene expression data often contains missing entries image corruption fabrication errors contaminants rendering gene cluster analysis difﬁcult. likewise large databases used recommender systems usually huge amount missing data makes pattern discovery challenging presence missing responses surveys failing imaging sensors astronomy reported make analysis applications challenging. several approaches introduced extend clustering missingdata applications. example partially observed dataset converted fully observed using either deletion imputation deletion involves removal variables missing entries imputation tries estimate missing values performs clustering completed dataset. extension weighted sumof-norms algorithm proposed weights estimated data points using imputation techniques missing entries kernel-based methods clustering also extended deal missing entries replacing euclidean distances partial distances majorize minimize algorithm introduced solve cluster-centres cluster memberships offers proven reduction cost iteration. data points assumed mixture distributions known. algorithms alternate maximum likelihood estimation distribution parameters missing entries. challenge algorithms lack theoretical guarantees successful clustering presence missing entries. contrast work recent years matrix completion different data models. algorithms along theoretical guarantees proposed low-rank matrix completion subspace clustering data missing entries however algorithms theoretical guarantees cannot trivmain focus paper introduce algorithm clustering data missing entries theoretically analyze conditions needed perfect clustering presence missing data. proposed algorithm inspired sum-of-norms clustering technique formulated optimization problem auxiliary variable assigned data point estimate centre cluster point belongs. fusion penalty used enforce equality many auxiliary variables. since experimentally observed non-convex fusion penalties provide superior clustering performance focus analysis clustering using fusion penalty presence missing entries arbitrary number clusters. analysis reveals perfect clustering guaranteed high probability provided number measured entries high enough; required number measured entries depends several parameters including intra-cluster variance inter-cluster distance. observe required number entries critically dependent coherence measure concentration inter cluster differences feature space. speciﬁcally clustering points determined small subset available features clustering becomes quite unstable particular feature values unknown points. factors inﬂuence clustering technique number features number clusters total number points. also extend theoretical analysis case without missing entries. analysis setting shows improved bounds uniform random distribution points respective clusters considered compared worst case analysis considered missing-data setting. expect improved bounds also derived case missing data uniform random distribution considered. also propose algorithm solve relaxation penalty based clustering problem using non-convex saturating fusion penalties. algorithm demonstrated simulated dataset different fractions missing entries cluster separations. observe algorithm stable changing fractions missing entries clustering performance degrades gradually increase number missing entries. also demonstrate algorithm clustering wine dataset reconstruction dynamic cardiac dataset fourier measurements. clustering using fusion penalty background consider clustering points drawn distinct clusters denote center clusters simplicity assume points clusters. individual points cluster modelled sum-of-norms method recently proposed convex clustering algorithm here surrogate variable introduced point estimate centre cluster belongs. example without loss generality assume belong belong then expect arrive solution order optimal following optimization problem solved fusion penalty ujp) enforced using different norms norms used literature sparsity promoting fusion penalties encourages sparse differences facilitates clustering points {ui}. appropriately chosen ui’s corresponding xi’s cluster converge point. main beneﬁt convex scheme classical clustering algorithms convergence algorithm global minimum. optimization problem solved efﬁciently using alternating direction method multipliers algorithm alternating minimization algorithm truncated norms also used recently fusion penalty resulting non-convex optimization problems shown penalties provide superior performance traditional convex penalties. convergence local minimum using iterative algorithm also guaranteed non-convex setting. sum-of-norms algorithm also used visualization exploratory tool discover patterns datasets clusterpath diagrams common visualize data. involves plotting solution path function regularization parameter small value solution given i.e. point forms individual cluster. large value solution given i.e. every point belongs cluster. intermediate values interesting behaviour seen various {ui} merge reveal cluster structure data. paper extend algorithm account missing entries data. present theoretical guarantees clustering without missing entries using fusion penalty. next approximate penalty non-convex saturating penalties solve resulting relaxed optimization problem using iterative reweighted least squares strategy proposed algorithm shown perform clustering correctly presence large fractions missing entries. quantity measure difﬁculty clustering problem. small values suggest large intercluster separation compared cluster size; recovery well-deﬁned clusters expected easier case large values. note norm used deﬁnition norm used deﬁne value special importance since requirement successful recovery main results. study problem clustering points {xi} presence entries missing uniformly random. arrange points {xi} columns matrix rows matrix referred features. assume entry observed probability entries measured column denoted sampling matrix formed selecting rows identity matrix. consider solving following optimization problem obtain cluster memberships data missing entries similar scheme expect ui’s correspond cluster equal ui’s different clusters equal. consider cluster recovery successful misclassiﬁcations. claim algorithm successfully recover clusters high probability when moving formal statement proof result consider simple special case illustrate approach. order reader following results important symbols used paper summarized table noiseless clusters missing entries consider simple case points belonging cluster identical. thus every cluster noiseless have hence optimization problem reduces fig. central assumptions illustrate different instances points belonging separated different clusters assumptions related cluster separation cluster size respectively illustrated importance assumption related feature concentration also appreciated comparing points blue clusters cannot distinguished solely basis feature green clusters cannot distinguished solely basis feature thus difﬁcult correctly cluster points either feature values unknown. coherence problem arise. require clusters nonoverlapping. high corresponds well separated clusters. cluster size maximum separation points within cluster sense thus cluster contained within cube size center feature concentration coherence vector deﬁned deﬁnition intuitively vector high coherence large values several small ones. speciﬁcally non-zero value. contrast entries equal. bound coherence difference points different clusters indicative difﬁculty clustering problem presence missing data. clusters differ single feature suggesting difﬁcult assign correct cluster point feature high high probability. coherence deﬁned assumption high probability vector entries equal words cluster memberships determined features. thus small value high ensure occurs probability. generalize result obtain following result assume i|i| points chosen randomly multiple clusters solution exists following equations message result large misclassiﬁed clusters highly unlikely. show feasible solutions containing small mis-classiﬁed clusters associated higher cost correct solution. thus conclude algorithm recovers ground truth solution high probability summarized following result. noisy clusters missing entries consider general case noisy clusters missing entries determine conditions required yield successful recovery clusters. reasoning behind proof general case similar special case discussed previous sub-section. proceeding statement lemmas theorems deﬁne following quantities number clusters number points cluster number features point cluster centre point random permutation points {zk} sampling matrix matrix formed arranging {xi} columns column probability sampling entry parameter related cluster separation deﬁned parameter related cluster size deﬁned deﬁned parameter related coherence deﬁned deﬁned deﬁned deﬁned deﬁned upper bound case clusters deﬁned parameter related cluster centre separation deﬁned deﬁned deﬁned probability failure theorem next state results special case order provide intuition problem. results stated mathematical rigour accompanied proofs. next sub-section consider general case provide lemmas theorems generalize results stated here. speciﬁcally lemmas theorem generalize results respectively. ﬁrst consider data consistency constraint determine possible feasible solutions. observe points speciﬁed cluster share centre without violating data consistency constraint result generalized consider large number points multiple clusters. choose points belong cluster shown high probability cannot share without violating constraints idea expressed following lemma lemma assume i|i| points chosen randomly multiple clusters solution exist following equations proof lemma appendix note decreases decrease small implies less variability within clusters large implies wellseparated clusters together resulting value characteristics desirable clustering result value lemma also demonstrates coherence assumption important ensuring sampled entries sufﬁcient distinguish proof theorem appendix note result consider particular distribution points cluster. instead consider points cluster sampled certain particular probability distributions uniform random distribution larger sufﬁcient ensure success high probability. general case distribution assumed cannot make probabilistic argument smaller required. consider special case noise zero mean uniform random variable thus points within cluster uniformly distributed cube side note random variable fig. different penalty functions norm penalty function non-convex convex penalty function. penalties closely approximate norm values respectively. arrive following result points different clusters lemma points cluster follow uniform random distribution points belonging different clusters solution exists following equations proof lemma appendix implies points different clusters cannot misclassiﬁed single cluster high probability. expressed terms also express terms following guarantee perfect clustering theorem points cluster follow uniform random distribution solution optimization problem identical ground-truth clustering probability exceeding thus result allows values results show consider distribution points arrive bound without missing entries seen theorems respectively. uniform random distribution also assumed case missing entries. similar theorem expect improved bound case missing entries well. functions approximate penalty accurately lower values illustrated reformulate problem using majorize-minimize strategy. speciﬁcally majorizing penalty using quadratic surrogate functional obtain unconstrained formulation larger datasets might computationally intensive solve constrained problem. case propose solve following unconstrained problem cluster-centres estimated using penalty inaccurate. penalty out-performs penalties accurately estimates cluster-centres. explain behaviour intuitively observing plots norm penalizes differences pairs points. semi-norm penalizes differences points close. saturating nature penalty heavily penalize differences points away. true penalty. however note penalty saturates quickly similar norm. behaviour missing penalty. reason seen penalty also penalizes inter-cluster distances shrinks distance estimated centres different clusters. initialization strategies experiments emphasize need good initialization weights convergence correct cluster centre estimates. dependence initial value arises non-convexity optimization problem. consider different strategies initializing weights partial distances consider pair points observed sampling matrices respectively. common indices deﬁne partial distance entries restricted index instead actual distances available partial distances used computing weights. observe experimentally good approximation initial weights correct clustering. conversely clustering fails initial guess. experiments demonstrate superiority partial distance based initialization strategy zeroﬁlled initialization. results study proposed theoretical guarantees theorem different settings. also test proposed algorithm simulated real datasets. simulations used study performance algorithm change parameters fraction missing entries number points clustered etc. also study effect different initialization techniques algorithm performance. demonstrate algorithm publicly available wine dataset algorithm reconstruct dataset under-sampled cardiac images. fig. comparison different penalties. show signiﬁcant principal components solutions obtained using irls algorithm. seen penalty unable cluster points even though clusters well-separated. penalty able cluster points correctly. however cluster-centres correctly estimated. penalty correctly clusters points also gives good estimate centres. compare performance different penalties used surrogate norm. purpose simulated dataset points belonging wellseparated clusters points cluster. particular experiment considered consider presence missing entries experiment. solve problem cluster points using penalties. results shown purpose visualization take data matrix retain signiﬁcant principal components matrix points points plotted ﬁgure blue green representing points different clusters. similarly obtain signiﬁcant components estimated centres plot resulting points black. note thus penalty penalty able correctly cluster points. behaviour seen thus concluded convex penalty unable cluster points. fig. study theoretical guarantees. quantities deﬁned section studied respectively. assumed. gives probability points different clusters share centre. expected value decreases increase decrease considering clusters lower bound probability successful clustering using proposed algorithm shown different values approximate values computed using shown observe behaviour quantities ηapprox function parameters shows plots illustrate change quantities different parameters varied. upper bound probability pair points entries observed common locations. change shown function different values subsequent plots upper bound probability pair points different clusters share common centre given entries observed common locations. change shown function different values behaviour shown probability mentioned lemma consider cluster setting subsequent plots. probability failure clustering algorithm plots shown function different values here ηapprox upper bound computed using expected probability success clustering algorithm increases increase decrease simulated datasets disjoint clusters varying number points cluster points cluster follow uniform random distribution. study probability success penalty based constrained clustering algorithm function particular parameters experiment conducted times compute probability success algorithm. trials cluster-centers remain same points sampled clusters different locations missing entries different. shows result datasets theoretical guarantees successfully clustering dataset shown note theoretical guarantees assume points taken uniform random distribution. also theoretical bounds assume solving original problem using norm whereas experimental results generated penalty. theoretical guarantees hold however demonstrate even challenging case clustering algorithm successful. note theoretical guarantees case. however assuming uniform random distribution points expect better theoretical guarantees data consists results chemical analysis wines different cultivars. data point features. clusters points respectively resulting total data points. created dataset without outliers retaining points cluster resulting total data points. undersampled datasets using uniform random sampling different fractions missing entries. results displayed using technique explained previous sub-section. seen clustering quite stable degrades gradually increasing fractions missing entries. cardiac image reconstruction apply proposed algorithm reconstruction cardiac image time series. samples collected fourier domain. however slow nature acquisition small fraction fourier samples acquired time frame. goal image reconstruction recover image series incomplete fourier observations. case cardiac different images time series appear clusters determined cardiac respiratory phase. thus proposed algorithm applied image reconstruction problem. cardiac data acquired siemens aera scanner university iowa. subject asked breathe freely radial lines fourier data acquired reconstruct image frame. fourier data corresponding frames acquired image series reconstructed using proposed unconstrained algorithm. performed spectral clustering reconstructed images form clusters. reconstructed frames belonging different clusters illustrated images displayed minimal artefacts diagnostic quality. discussion proposed technique cluster points feature values points unknown. theoretically studied performance algorithm minimizes fusion penalty subject certain constraints relating consistency known features. concluded favourable clustering conditions well-separated clusters intra-cluster variance proposed method performs correct clustering even presence missing entries. however since problem np-hard propose penalties approximate norm. observe experimentally penalty good surrogate norm. fig. experimental results probability success. guarantees shown simulated dataset clusters. clustering performed using penalty partial distance based initialization. assumed shows experimentally obtained probability success clustering clusters points uniform random distribution. shows theoretical lower bound probability success. shows experimentally obtained probability success challenging dataset note theoretical guarantees case since analysis assumes clustering results simulated clusters shown simulated dataset- disjoint clusters points cluster. order generate dataset cluster centres chosen uniform random distribution. distances pairs cluster-centres units respectively. cluster centres noisy instances generated adding zero-mean white gaussian noise variance dataset sub-sampled varying fractions missing entries locations missing entries chosen uniformly random full data matrix. also generate dataset- halving distance cluster centres keeping intra-cluster variance ﬁxed. test constrained unconstrained formulations algorithm datasets. proposed initialization techniques irls algorithm also tested here. since points take points estimated centres plot signiﬁcant components. colours distinguish points according ground-truth clusters. point joined centre estimate line. expected observe clustering algorithms stable fewer missing entries. also note results fig. clustering results simulated datasets. penalty used cluster datasets varying fractions missing entries. constrained unconstrained formulation results presented different initialization techniques show signiﬁcant principal components solutions. original points {xi} connected cluster centre estimates {ui} lines. inter-cluster distances dataset half dataset intra-cluster distances remain same. consequently dataset performs better higher fraction missing entries. unconstrained clustering formulation partial-distance based initialization cluster centre estimates relatively stable varying fractions missing entries. non-convex saturating penalty shown perform better clustering task previously used convex norms penalties. describe irls based strategy solve relaxed problem using surrogate penalty. theoretical analysis reveals various factors determine whether points clustered correctly presence missing entries. obvious performance degrades decrease fraction sampled entries moreover shown difference points different clusters coherence means expected clustering dependent features points. intuitively points different clusters distinguished features point missing particular feature values cannot clustered correctly. moreover note high number points cluster high number features number clusters make data less sensitive missing entries. finally well-separated clusters intra-cluster variance desirable correct clustering. experimental results show great promise proposed technique. particular simulated data note cluster-centre estimates degrade gradually increase fraction missing entries. depending characteristics data number points cluster separation distance clustering algorithm fails particular fraction missing entries. also show importance good initialization irls algorithm proposed initialization technique using partial distances shown work well. proposed algorithm performs well image reconstruction task resulting images minimal artefacts diagnostic quality. noted images reconstructed satisfactorily fourier samples. case fraction observed samples around however simulated datasets wine datasets cannot clustered high fraction missing samples. fundamental diffig. cardiac reconstruction results. images reconstructed highly under-sampled fourier data using unconstrained formulation. sampling mask particular frame shown along fourier data frame missing fourier entries ﬁlled zeros inverse fourier transform taken corrupted image clustering algorithm applied data resulting images clustered clusters using spectral clustering. shows reconstructed images different clusters. ference dataset datasets coherence data acquire fourier samples. since know frequency samples important image reconstruction scanner acquires frequency samples. case high coherence helpful clustering. however simulated wine data know apriori features important. case sampling pattern random predicted theory useful coherence. conclusion sampling pattern within control useful high coherence relative importance different features known apriori. unknown random sampling preferred useful coherence. future work focus deriving guarantees case high locations important features known conﬁdence sampling pattern adapted accordingly. theory assumes well-separated clusters consider presence outliers. theoretical experimental analysis clustering performance presence outliers needs investigated. improving algorithm performance presence outliers direction future work. moreover shown improved bounds clustering success absence missing entries points within cluster assumed conclusion propose clustering technique data presence missing entries. prove theoretically constrained norm minimization problem recovers clustering correctly even presence missing entries. efﬁcient algorithm solves relaxation problem presented next. demonstrated cluster centre estimates obtained using proposed algorithm degrade gradually increase number missing entries. algorithm also used cluster wine dataset reconstruct images under-sampled fourier data. presented theory results demonstrate utility proposed algorithm clustering data feature values data unknown. computing surveys vol. macqueen some methods classiﬁcation analysis multivariate observations proceedings ﬁfth berkeley symposium mathematical statistics probability vol. oakland usa. awasthi bandeira charikar krishnaswamy villar ward relax need round integrality clustering formulations proceedings conference innovations theoretical computer science. ward hierarchical grouping optimize objective function journal american statistical association vol. appendix proof lemma proof. since cluster x−x∞ points particular cluster feature bounded max. construct vector max). following condition since satisﬁed particular choice appendix lemma lemma consider pair points observed sampling matrices respectively. assume common indices size then following result holds true regarding partial distance si∩i using applying triangle inequality obtain translates |i∩i| number commonly observed locations. need show high probability partial distances satisfy appendix proof lemma proof. construct graph point represented node. lemma implies pair points belonging cluster yield feasible solution probability hence assume exists edge nodes cluster probability lemma indicates pair points belonging different clusters yield feasible solution probability assume exists edge nodes different clusters probability evaluate probability exists fully-connected sub-graph size nodes taken cluster. follow methodology similar gives expression probability distribution maximal clique size random graph. unlike proof graph every edge present equal probability. note cannot probability since involves solution cluster size evaluate best solution possible value range compare solutions solution highest cost. note feasible region polyhedron objective function convex. thus value need check cost vertices polyhedron formed constraints since cost points feasible region lower. vertex points formed picking constraints setting equal possible extremal values. note vertex points either non-zero values. simple example choose vertex points polyhedron given possible permutations following clusters clusters clusters clusters easily checked candidate solution list maximum cost. mixed clusters size cannot formed probability thus probability solution optimization problem identical ground-truth clustering. concludes proof theorem. form solution {mj} need intercluster edges present. recall edges present probability thus probability collection nodes forms clique gives following result appendix proof theorem proof. lemma indicates fully connected original clusters size likely probability lemma shows size misclassiﬁed large clusters cannot exceed high probability. results enable re-express optimization problem simpler maximization problem. show high probability feasible solution ground-truth solution results cost higher ground-truth solution. candidate solution groups sizes respectively. centre estimates points within group equal. different centre estimates groups. without loss generality assume groups points belonging single ground-truth cluster i.e. pure. rest clusters candidate solution mixed clusters. candidate solution greater pure clusters always merged form pure clusters; merged solution always result lower cost. points different clusters cannot misclassiﬁed single cluster. among feasible solutions clearly solution problem minimum cost points cluster merge thus ensures correct clustering.", "year": 2017}