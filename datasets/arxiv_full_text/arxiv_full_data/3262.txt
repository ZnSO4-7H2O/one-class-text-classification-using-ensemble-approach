{"title": "10,000+ Times Accelerated Robust Subset Selection (ARSS)", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Subset selection from massive data with noised information is increasingly popular for various applications. This problem is still highly challenging as current methods are generally slow in speed and sensitive to outliers. To address the above two issues, we propose an accelerated robust subset selection (ARSS) method. Specifically in the subset selection area, this is the first attempt to employ the $\\ell_{p}(0<p\\leq1)$-norm based measure for the representation loss, preventing large errors from dominating our objective. As a result, the robustness against outlier elements is greatly enhanced. Actually, data size is generally much larger than feature length, i.e. $N\\gg L$. Based on this observation, we propose a speedup solver (via ALM and equivalent derivations) to highly reduce the computational cost, theoretically from $O(N^{4})$ to $O(N{}^{2}L)$. Extensive experiments on ten benchmark datasets verify that our method not only outperforms state of the art methods, but also runs 10,000+ times faster than the most related method.", "text": "figure comparisons four algorithms optdigit. conclusions drawn. first method highly faster others; help elegant theorem rrssour signiﬁcantly faster authorial algorithm rrssnie. second arssour achieves highly promising prediction accuracies. relies assumption data points multiple low-dimensional subspaces. speciﬁcally rank revealing selects subsets give best conditional sub-matrix. unfortunately method suboptimal properties assured globally optimum polynomial time. another category assumes samples distributed around centers center nearest neighbour selected exemplars. perhaps kmeans kmedoids typical methods methods employ em-like algorithm. thus results depend tightly initialization highly unstable large recently methods assume exemplars samples best represent whole dataset. however optimization combinatorial problem computationally intractable solve. besides representation loss measured least square measure sensitive outliers data subset selection massive data noised information increasingly popular various applications. problem still highly challenging current methods generally slow speed sensitive outliers. address issues propose accelerated robust subset selection method. speciﬁcally subset selection area ﬁrst attempt employ -norm based measure representation loss preventing large errors dominating objective. result robustness outlier elements greatly enhanced. actually data size generally much larger feature length i.e. based observation propose speedup solver highly reduce comdue explosive growth data subset selection methods increasingly popular wide range machine learning computer vision applications kind methods offer potential select highly representative samples exemplars describe entire dataset. analyzing roughly know all. case important summarize visualize huge datasets texts images videos etc. besides using selected exemplars succeeding tasks cost memories computational time greatly reduced additionally outliers generally less representative side effect outliers reduced thus boosting performance subsequent applications several subset selection methods. intuitional method randomly select ﬁxed number samples. although highly efﬁcient guarantee effective selection. methods depending mechanism representative exemplars mainly three categories selection methods. category copyright association advancement artiﬁcial intelligence rights reserved. table complexity comparison three algorithms iteration step. generally data size much larger feature length i.e. compared rrssnie rrssour arssour signiﬁcantly simpliﬁed. rrssour {··· rk×n corresponding linear combination coefﬁcients. minimizing could select highly informative representative samples well represent samples although well modeled—very accurate intuitive bottlenecks. first objective combinatorial optimization problem. np-hard exhaustively search optimal subset reason author approximate sequential optimization problem solved inefﬁcient greedy optimization algorithm. second similar existing least square loss based models machine learning statistics sensitive presence outliers nonnegative parameter; constrained row-sparse thus select representative informative samples representation loss accumulated -norm among samples compared robustness outlier samples enhanced. equivalently rewritten matrix format seems perfect solve objective because looks simple global optimum theoretically guaranteed unfortunately terms speed usually infeasible incredible computational demand case large iteration computational complexity contributions. paper propose accelerated robust subset selection method highly raise speed hand boost robustness other. -norm based robust measure representation loss preventing large errors dominating objective. result robustness outliers greatly boosted. then based observation data size generally much larger feature length i.e. propose speedup solver. main acceleration owing augmented lagrange multiplier equivalent derivation. them reduce extensive results benchmark datasets demonstrate average method times faster nie’s method. selection quality highly encouraging shown fig. additionally another equivalent derivation give accelerated solver nie’s method theoretnotations. boldface uppercase letters denote matrices boldface lowercase letters represent vectors. matrix rl×n denote column respectively. -norm feature length. goal select representative informative samples effectively describe entire dataset solely using exemplars subsequent tasks could greatly reduce computational costs largely alleviate side effects outlier elements data. motivation could formulated transductive experimental design model penalty parameter. guarantee equality constraint requires approaching inﬁnity cause numerical conditions. instead introducing lagrangian multiplier longer requiring thus rewrite standard formulation recently proposed generalized iterative shrinkage algorithm solve algorithm easy implement able achieve accurate solutions current methods. thus problem remark since unnxt x+γv∈rn×n major computational cost focuses linear system. solved cholesky factorization method costs factorization well forward backward accelerated robust subset selection huge computational costs nie’s method infeasible case large n—the computational time hours case samples. besides nie’s model imposes -norm among features prone outliers features. tackle issues propose robust model norm. although resulted objective challenging solve speedup algorithm proposed dramatically save computational costs. task costs method minutes achieving times acceleration compared speed nie’s method. modeling. boost robustness outliers samples features formulate discrepancy p-norm. theoretical empirical evidences verify compared norms p-norm able prevent outlier elements dominating objective enhancing robustness thus following objective balancing parameter; sparse matrix used select informative representative samples. minimizing energy could capture essential properties dataset obtaining optimal indexes sorted row-sum value absolute decreasing order. samples speciﬁed indexes selected exemplars. note model could applied unsupervised feature selection problem transposing data matrix case sparse matrix used select representative features. accelerated solver arss objective although objective challenging solve propose effective highly efﬁcient solver. acceleration owes equivalent derivation. intractable challenge that -norm non-convex non-smooth notdifferentiable zero point. therefore beneﬁcial augmented lagrangian method solve resulting several easily tackled unconstrained subproblems. solving iteratively solutions subproblems could eventually converge minimum speciﬁcally introduce auxiliary variable rl×n thus objective becomes according theorem corollary solver model highly simpliﬁed feature length generally much smaller data size similarly nie’s method could highly accelerated theorem obtaining times speedup shown fig. table theorem nie’s solver equivalent following linear system nonnegative parameter rl×n since convex optimum could found differentiating setting derivative zero. amounts tackling following linear system means good choice real applications large following equivalent derivation proposed signiﬁcantly save computational complexity. theorem linear system equivalent following linear system complex operation matrix multiplications linear system. corollary equivalent updating rules objective using otherwise using shown algorithm comn highly reduced complexity figure speed increasing letter mnist waveform. compared authorial solver rrssnie method arss rrssour dramatically reduce computational time. larger data size larger gaps methods are. note selection time sensitive number selected samples speed comparisons parts speed comparisons. first speed varies increasing illustrated fig. comparison speciﬁc speed summarized table note rrssnie denote authorial solver rrssour accelerated solver nie’s model theorem arss proposed method. corollary since feature length generally much smaller data size i.e. accelerated solver nie’s model highly faster authorial solver theoretically reduce computational complexexperimental settings part experimental settings introduced. experiments conducted server -core intel xeon cache using matlab brief descriptions benchmark datasets summarized table ‘total’ denotes total samples data. high computational complexity methods handle small datasets thus randomly choose candidate total reduce sample size i.e. ‘candid.’ table remainder used test. speciﬁcally simulate varying quality samples percentage candidate samples class randomly selected arbitrarily added following three kinds noise gaussian speed increasing verify great superiority method state-of-the-art methods speed three experiments conducted. results illustrated fig. three sub-ﬁgures showing speed four methods benchmark datasets letter mnist waveform respectively. shall selection time rrssnie increases dramatically increases. surprisingly rrssnie incredibly time-consuming grows— order curves looks higher quadratic. actually compared rrssnie curve arss surprisingly lower highly stable increasing almost rise selection time growing owing speedup techniques equivalent derivations. them reduce computational cost corollary moreover help theorem rrssour second faster algorithm signiﬁcantly accelerated compared authorial algorithm rrssnie. speed ﬁxed speed four algorithms summarized table shows results dataset last displays average results. four conclusions drawn table first arss fastest algorithm rrssour second fastest algorithm. second help theorem rrssour highly faster rrssnie averagely obtaining times acceleration. third arss dramatically faster table performances rrss arss speed seconds prediction accuracies. terms speed help theorem rrssour averagely times faster authorial algorithm i.e. rrssnie; arss achieves surprisingly times acceleration compared rrssnie. robust loss p-norm prediction accuracy arss highly encouraging. rrssnie ted; results table verify average acceleration times faster rrssnie times faster ted. means example takes rrssnie years subset selection task takes method days address problem. finally apply arss whole sample data. results displayed column table showing capability process large datasets. prediction accuracy accuracy comparison conduct experiments benchmark datasets. dataset representative samples selected training. prediction accuracies reported table including results popular classiﬁers. three observations drawn table. first linear generally outperforms knn. second general method performs best; cases method achieves comparable results best performances. third compared rrss arss achieve appreciable advantage. analyses better illustrated last table results demonstrate loss model well suited select exemplars sample sets various quality. prediction accuracies increasing give detailed comparison fig. shows prediction accuracies versus growing rows four columns sub-ﬁgures. shows results bottom shows results svm. column gives result dataset. shall prediction accuracies generally increase increases. case consistent common view training data boost prediction accuracy. sub-ﬁgure arss generally among best. case implies robust objective p-norm feasible select subsets data varying qualities. deal tremendous data varying quality propose accelerated robust subset selection method. p-norm exploited enhance robustness outlier samples outlier features. although resulted objective complex solve propose highly efﬁcient solver techniques equivalent derivations. them greatly reduce computational complexity smaller data size i.e. extensive results benchmark datasets verify method runs times faster related method also outperforms state methods. moreover propose accelerated solver highly speed nie’s method theoretically reducing computational complexity", "year": 2014}