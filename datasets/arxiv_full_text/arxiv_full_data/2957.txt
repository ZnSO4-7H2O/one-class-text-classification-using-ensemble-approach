{"title": "Deep Learning for Medical Image Segmentation", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "This report provides an overview of the current state of the art deep learning architectures and optimisation techniques, and uses the ADNI hippocampus MRI dataset as an example to compare the effectiveness and efficiency of different convolutional architectures on the task of patch-based 3-dimensional hippocampal segmentation, which is important in the diagnosis of Alzheimer's Disease. We found that a slightly unconventional \"stacked 2D\" approach provides much better classification performance than simple 2D patches without requiring significantly more computational power. We also examined the popular \"tri-planar\" approach used in some recently published studies, and found that it provides much better results than the 2D approaches, but also with a moderate increase in computational power requirement. Finally, we evaluated a full 3D convolutional architecture, and found that it provides marginally better results than the tri-planar approach, but at the cost of a very significant increase in computational power requirement.", "text": "report provides overview current state deep learning architectures optimisation techniques uses adni hippocampus dataset example compare eﬀectiveness eﬃciency diﬀerent convolutional architectures task patch-based dimensional hippocampal segmentation important diagnosis alzheimer’s disease. found slightly unconventional stacked approach provides much better classiﬁcation performance simple patches without requiring signiﬁcantly computational power. also examined popular tri-planar approach used recently published studies found provides much better results approaches also moderate increase computational power requirement. finally evaluated full convolutional architecture found provides marginally better results tri-planar approach cost signiﬁcant increase computational power requirement. deep learning techniques applied wide variety problems recent years prominently computer vision natural language processing computational audio analysis many applications algorithms based deep learning surpassed previous state-of-art performance. heart deep learning algorithms domain-independent idea using hierarchical layers learned abstraction eﬃciently accomplish high level tasks report ﬁrst provide overview traditional artiﬁcial neural network concepts introduced introducing recent discoveries made training deep networks practical eﬀective. finally present results applying multiple deep architectures adni hippocampus segmentation problem comparing classiﬁcation performances computational power requirements. artiﬁcial neural networks machine learning technique inspired loosely based biological neural networks similar sense large number identical linked simple computational units achieve high performance complex tasks modern anns heavily optimized eﬃcient implementation electronic computers bear little resemblance biological counterpart. particular time-dependent integrateand-ﬁre mechanism bnns replaced steady state values representing frequency ﬁring anns also vastly simpliﬁed connection architectures allow eﬃcient propagation. current architectures don’t allow loops connections also don’t allow connections made broken training unless otherwise speciﬁed references neural networks report refer artiﬁcial neural networks. typical neural network nodes placed layers ﬁrst layer input layer last layer output layer. input nodes special outputs simply value corresponding features input vector. example classiﬁcation task -dimensional input binary output possible network design input nodes output node. input output layers usually considered ﬁxed network design. input layer output layer input nodes connected output nodes network essentially implements matrix multiply linear transformation. type networks solve simple problems feature space linearly-separable. however linearly-separable problems simpler techniques linear regression logistic regression usually achieve similar performance diﬀerence training methods. modern applications neural networks hidden layers layers input layer output layers allow network model non-linearity feature space. number hidden layers number hidden nodes layer hyper-parameters always easy determine. rules-of-thumb proposed still part determined trial-and-error. risk using too-small network enough representative power model useful patterns input risk using too-large network overﬁt data start modeling noise training usually better side larger networks many eﬀective techniques exist combat overﬁtting detailed later sections report. using network size limit overﬁtting error-prone time-consuming eﬀective. methods proposed automatic hyperparameter tuning evolving cascade networks trains large network iterative process ﬁrst starting minimal network iteration train candidate networks nodes diﬀerent layers keeping best. also tuning methods based genetic algorithms links turned genes however methods seen widespread adoption large increase training time marginal beneﬁts overﬁtting avoided using methods limiting network size. proven network hidden layer approximate continuous function accuracy network hidden layers approximate function accuracy given inﬁnite computational power memory training theoretically reason hidden layers. however explained later report much eﬃcient solve complex problems using deeper network hidden layers. output node computed shown equation xi’s inputs node wi’s weights associated link bias associated node function associated node known activation function. activation functions widespread use. output nodes regression networks linear activation function commonly used give networks range real numbers. output nodes classiﬁcation networks softmax function often used transform outputs something interpreted probability distribution. diﬀerentiability property important must able take derivative function point training using gradient-based methods. necessary networks trained using non-gradient-based methods genetic algorithm. non-linearity important otherwise network lose ability model non-linear patterns training set. non-linearity achieved using saturation case hyperbolic practice although logistic function biologically plausible hyperbolic tangent usually allows faster training since linear around means nodes start training saturation even inputs zero negative training algorithms neural networks fall major categories gradient-based nongradient-based. report focuses gradient-based methods much commonly used recent times usually converges much faster well. mentioned section node network weight associated incoming link scalar bias. weight bias node parameters node. concatenate weights biases nodes network vector completely deﬁnes behaviour network goal training process therefore approximates function trying model. words given inputs desired outputs trying minimizes diﬀerence desired outputs network outputs entries training set. that measurement error needed. error measure mean-squared-error commonly used error measure. given equation training input desired output training pair number entries training network output. goal minimize given small networks feasible exhaustive search point parameter space mean-squared-error minimized networks reasonable sizes exhaustive search practical. gradient descent commonly used optimisation algorithm neural networks. gradient descent ﬁrst initialized random point parameter space. weights biases typically drawn keeps nodes linear region beginning training. popular method draw uniform distribution chosen based shape activation function number inputs node initialization gradient descent algorithm performs walk parameter space guided gradient error surface. simplest form iteration algorithm takes step opposite direction gradient step size proportional magnitude gradient ﬁxed learning rate. shown equation learning rate symbols previously deﬁned. diﬀerence output layer hidden layers hidden layers really error. however still derive error terms calculating contribution input activation function next node. equivalent another application chain rule since input activation function simply contributions node previous layer. convenient propagation backwards ﬁnal error multiplying derivative activation function time assigning blame error proportionally weights connecting previous layer nodes node examining. basis back-propagation require activation function diﬀerentiable. detailed derivation omitted brevity. many variants gradient descent proposed diﬀerent performance characteristics. popular gradient descent momentum instead calculating gradient time step combine fraction weight update previous iteration. allows faster convergence situations gradient much larger dimensions others another common variation learning rate scheduling changing learning rate training progresses. goal somewhere close local minimum quickly slow avoid overshooting. idea taken resilient back-propagation sign gradient used. rprop weight independent learning rate increased sign gradient changed previous iteration reduced gradient changed signs allows weights train close optimal learning rate eliminates learning rate parameter must manually tuned gradient descent variants. initial learning rate still needs chosen doesn’t signiﬁcantly aﬀect training time result training limited size usually dangerous train without constraint since network eventually start model noise training specialized generalize beyond training set. popular combat overﬁtting regularization idea encouraging weights smaller values. common form regularization regularization norm added error function shown equation parameter controls strength regularization needs tuned. network would overﬁt. high network would underﬁt. mentioned earlier section neural network hidden layers already theoretically universal function approximator capable approximating function continuous arbitrary accuracy. light that seem pointless pursue networks hidden layers. main beneﬁt using deep networks node-eﬃciency often possible approximate complex functions accuracy using deeper network much fewer total nodes compared -hidden-layer network large hidden layers. besides computational beneﬁts model smaller degree freedom requires smaller dataset train size training often limiting factor neural network training. example suppose given bitmap images containing triangles rectangles pentagons hexagons task count number shape image. case deep network ﬁrst layers perform level task calculating image gradients identifying lines image transform convenient form. higher layers perform classiﬁcation counting simpler representation. hand shallow network level tasks would performed multiple times since little cross-feeding intermediate results. would result redundant work done. deep networks possible model functions work many layers abstraction example classifying gender images faces breed dogs. practical perform tasks using shallow networks redundant work done exponential number layers equivalent shallow network would require exponentially computational power exponentially larger training sets neither usually available. fact deeper networks computationally eﬃcient known long time deep networks attempted early lecun successfully applied -hidden-layer network code recognition. however able scale higher number layers complex problems slow training reason understood. what’s happening errors propagated back output layer it’s multiplied derivatives activation function point activation. soon propagation gets node saturation error reduced level noise nodes behind saturated node train extremely slowly. eﬀective solution problem found many years. hinton proposed solution problem started current wave renewed interest deep learning. idea ﬁrst train layer unsupervised greedy fashion identify application-independent features layer ﬁnally training entire network labeled data implemented idea generative model form restricted boltzmann machine layer contains binary variables associated probability distributions layer trained predict previous layer using algorithm known contrastive divergence idea ﬁrst start input layer hidden layer train network produce original input essentially training network model identity function hidden layer fewer nodes input layer output hidden layer compressed abstract representation input. process repeated train hidden layer output previous layer input ﬁnal network trained actual output layer using standard back-propagation labeled data. works around vanishing gradient problem ﬁnal back-propagation performed earlier layers already trained provide application-independent abstractions. modern implementations artiﬁcial noise often injected input autoencoders encourage robustness autoencoders testing ability reconstruct clean input partially-corrupted input autoencoders known denoising autoencoders. denoising autoencoders perform similarly systems based rbms stacked variant deep belief networks simpler solution vanishing gradients problem proposed glorot solution vanishing gradients problem simply activation function reduce error it’s propagated back non-diﬀerentiability zero inconsequential since neural networks work real numbers highly unlikely exactly time. practice deﬁne derivative zero either doesn’t real world eﬀect. unbounded nature function positive side problematic result large activations later layers cause numerical problems. solved using regularization limits magnitude weights also enforces sparsity sparse network given input small subset nodes activated. fact regularization encourage sparsity contrast regularization discourages sparsity. regularization encourages desired output constructed many small weights instead large weight would higher norm equal norm lower norm shown figure essence regularization encourage nodes independent another co-evolve others. result eﬃcient usage available nodes also computational advantages networks using activation functions hard saturation since node’s output need broadcasted nodes next layer. rectiﬁed linear activation regularization pre-training glorot reports achieving slightly superior similar performance previous results based pre-training system nodes trained extract discriminating patterns input data even patterns irrelevant task hand. without pre-training irrelevant patterns would encoded thus freeing nodes important patterns task. however also identiﬁed situation pre-training still advantageous semi-supervised problems. semi-supervised problem large training available small subset labeled. case starting pre-training using entire dataset training whole network labeled subset result network generalizes better suﬃcient research decide whether relu also superior traditional activation functions shallower networks. shallow networks still hyperbolic tangent logistic activation aﬀected vanishing gradient problem. hinton proposed technique greatly improves network performance cases limited training data idea stems observation training small many possible models perform well training perform well testing set. traditional solution train ensemble networks diﬀerent initialization and/or subsets training combine outputs produce ﬁnal output approach known work well improving model performance often computationally feasible deep learning training single network take many hours days even fast gpu. many trained neural networks also used real-time applications using ensemble would make model many times slower evaluate. hinton al.’s proposal termed dropout randomly de-activate nodes network training iteration. disabled node would participate forward propagation would block error signal propagating node backpropagation. training done nodes re-enabled weights halved maintain output range disabling nodes random iteration forces nodes independently evolve instead co-evolving others. co-evolution optimal because example nodes evolve correct mistakes made nodes instead modeling useful patterns. dropout nodes cannot assume nodes exist forced independently useful mentioned section ensembles networks often achieve higher performance constituent network computationally infeasible real-world applications. especially true problems small labeled datasets single networks likely overﬁt. bucilua proposed semi-supervised problems large dataset available small subset labeled beneﬁcial train ensemble labeled subset label data ﬁnally training single network continuing theme model compression caruana showed help high performance deep network shallow network trained perform much better similar shallow network trained directly training algorithm ﬁrst trains deep original training provide extended labels entries training case classiﬁcation problems output layer often softmax extended labels inputs softmax layer inputs softmax layer probabilities class. finally shallow network trained predict probabilities instead original single-class label. makes training much easier shallow network multi-class probabilities labels provide much information single-class label. modelling probabilities shallow network also mimic-ing deep generalize unseen data mostly depend relative values probabilities classes highest performances shallow networks much higher networks trained single-class labels. result signiﬁcant proves reason shallow networks perform worse deep networks entirely increase representative power ﬂexibility deep networks. also current training algorithms sub-optimal shallow networks develop better training algorithms potentially signiﬁcantly improve performance shallow networks. however performance mimic-ing shallow networks still quite good deep networks ensembles mimic-ing therefore option creating mimic-ing shallow network allows tradeoﬀ made accuracy speed. convolutional neural networks neural network architecture uses extensive weight-sharing reduce degrees freedom models operate features spatially-correlated includes images also recently successfully applied natural language processing convolutional neural networks inspired observation inputs like images many level operations local positiondependent example operation useful many computer vision applications edge detection. fully-connected deep network edge detector would separately trained part image even though would likely arrive similar results. would better kernel trained edge detection entire image time. convolutional max-pooling fully-connected typical arrangement alternating convolutional maxpooling layers ﬁnishing fully-connected hidden layers. convolutional layer dimensions input pixel activated region pixels centered around pixels location input images. weights also shared output pixel. eﬀect convolutional layer performs convolution input images learned kernel. max-pooling layers perform downsampling images. typical downsample factor averaging also used empirical results suggest downsampling taking maximum sub-region gives best performance cases max-pooling responsible summarizing sub-region gives network translational rotational invariance. existing applications convolutional neural networks images idea also extended images kernels. used process actual images videos using time third dimension. hippocampus component human brains responsible committing short-term episodic declarative memory long-term memory well navigation hippocampus segmentation important diagnosis alzheimer’s disease components ﬁrst aﬀected disease. reduction hippocampal volume used marker diagnosis humans hippocampi shaped like seahorses shown figure goal classify voxel image non-hippocampus left hippocampus right hippocampus. using problem evaluate diﬀerent deep learning techniques patch-based segmentation. images labeled human expert. unfortunately none images labeled human expert determine variances human labeling. begin labeling image ﬁrst crop rectangular bounding perform masking normalized coordinates. case adni dataset images already orientation rotation needed. going images training determined hippocampi always region relative dimension bounding respective brains. enlarged region side mask. voxels outside mask automatically classiﬁed non-hippocampus. training patches drawn within mask. would dangerous draw training voxels uniformly randomly within mask even within mask vast majority voxels non-hippocampus hence would positive samples. another problem edge voxels would severely under-represented even though likely diﬃcult voxels classify. drawing scheme ensures none important types voxels under-represented. biggest downside scheme distorts prior probabilities class possible solutions discussed later report. ﬁrst method tried stack patches around voxel want sample. example patch size layer count would extract three patches around voxel question parallel above parallel below. network architecture kernels ﬁrst convolutional layer kernels second convolutional layer nodes fully-connected layer ﬁnally softmax layer exponential normalization. max-pooling used since network performs slightly worse max-pooling. voxel extract square patches around voxel perpendicular axis. example want patch size would extract patch plane centered around voxel question patch plane another patch plane. since corresponding pixels patches spatially correlated case network architecture consists convolutional layers patches connections feed outputs nodes fully-connected layer ﬁnal classiﬁcation. max-pooling used. network architecture kernels ﬁrst convolutional layer kernels second convolutional layer nodes fully-connected layer before. max-pooling used. network trained label image patches extracted every voxel mask region result used label voxel. voxel outside mask region automatically classiﬁed negative. beginning training termination iteration validation period. validation done every pass training every time validation score improves current best validation score terminating iteration twice current iteration count. means training terminated signiﬁcant improvement least second half elapsed time. besides comparing output labeling convolutional neural networks also want kind performance simple post-processing clean results. post-processing applied convolutional architectures. ﬁrst experiment determine eﬀect number layers performance convolutional architecture. shown table clear improvements going layers clear improvements beyond layers. however also note number layers little impact speed terms time iteration. labeling results images testing positive voxels negative voxels. false positive false negative values post-processing since total number positive negative voxels same false positive false negative values next experiment test diﬀerent patch sizes also architecture. shown table little beneﬁts going beyond however case training becomes much slower larger patch sizes therefore optimal patch size seems next experiment patch size tri-planar architecture similar results optimal patch size shown table also signiﬁcant reductions training speed patch size increased. finally look patch sizes architecture. unfortunately constrained available memory case patches. unlike previous architectures architecture performs well even small patch size clear whether actually beneﬁts larger patches shown table results validation testing sets reasonably consistent conﬁguration actual labeling performance entire image much less consistent. fact class distribution patches used training/validation/testing sets distribution actual image. discrepancy image labeling performance could models better classifying classes others classes common actual image. architecture performs consistently better tri-planar architecture classifying patches advantage seem translate well image labeling performance seems perform slightly worse. also slowest train approximately iterations minute longest taking hours train. project investigated three diﬀerent convolutional network architectures patchbased segmentation hippocampi region images. discovered popular triplanar approach oﬀers good tradeoﬀ accuracy training time. approach performs marginally better patch classiﬁcation seem perform well labeling entire image. likely sampling method altering prior probabilities classes presented training algorithm problem solved approach perform marginally better tri-planar approach whole-image labeling well much higher computational power requirement. many possible avenues future investigation. example learning rate scheduling algorithms available signiﬁcantly shorten training time without aﬀecting quality results zeiler’s famous adadelta algorithm assigns independent learning rate weight depending often activated oft-used connections slower learning rates achieve higher precision rarely-used connections still trained higher learning rate reduce bias. also beneﬁcial give coordinate patch fully-connected layers neural networks along prior probability class coordinate determined statistical analysis training set. possible extension tri-planar architecture include images multiple scales plane similarly applied traﬃc sign recognition sermanet lecun would allow networks global context largest scales enough include boundaries brain compensate lack positional information triplanar architecture. either alternative complement idea giving statistical coordinate-based prior probabilities network.", "year": 2015}