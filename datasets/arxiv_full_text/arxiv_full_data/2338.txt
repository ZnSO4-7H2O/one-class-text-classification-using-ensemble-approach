{"title": "Learning Feature Hierarchies with Centered Deep Boltzmann Machines", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Deep Boltzmann machines are in principle powerful models for extracting the hierarchical structure of data. Unfortunately, attempts to train layers jointly (without greedy layer-wise pretraining) have been largely unsuccessful. We propose a modification of the learning algorithm that initially recenters the output of the activation functions to zero. This modification leads to a better conditioned Hessian and thus makes learning easier. We test the algorithm on real data and demonstrate that our suggestion, the centered deep Boltzmann machine, learns a hierarchy of increasingly abstract representations and a better generative model of data.", "text": "deep boltzmann machines principle powerful models extracting hierarchical structure data. unfortunately attempts train layers jointly largely unsuccessful. propose modiﬁcation learning algorithm initially recenters output activation functions zero. modiﬁcation leads better conditioned hessian thus makes learning easier. test algorithm real data demonstrate suggestion centered deep boltzmann machine learns hierarchy increasingly abstract representations better generative model data. deep boltzmann machines principle powerful models extracting hierarchical structure data unfortunately attempts train layers jointly mostly unsuccessful. argue later greater detail possible reason could mapping activities onto sigmoid nonlinearities centered zero default. paper propose recenter output unit zero rewriting energy function centered states oﬀset parameter. reparameterization energy function leads better conditioned hessian estimated model log-likelihood. centered boltzmann machine easy implement reparameterization leaves associated gibbs distribution invariant. train centered deep boltzmann machine mnist data set. empirical results show centered able learn top-layer representation contains useful discriminative features produce good generative model data. addition centered learns faster stable non-centered counterpart. related work case using centered nonlinearities already made lecun glorot bengio context backpropagation networks showing logistic function generally performs poorly compared centered counterpart hyperbolic tangent. idea centering also proposed section introduce centered boltzmann machine. following sigmoid function deﬁned sigm denotes variable drawn randomly bernouilli distribution parameter h·ip denotes expectation operator respect probability distribution operations apply elementwise input vector. groups model parameters. matrix size mx×mx symmetric contains connection strength units. vector size contains biases associated unit. probability associated state according gibbs distribution vector contains oﬀsets associated unit network. setting sigm initial bias enforces initial centering boltzmann machine. equations derive conditional probability figure example sigmoids diﬀerent biases oﬀsets. three non-dashed sigmoids said centered cross origin. show centering sigmoids leads better conditioned hessian. h·iw designates expectation respect probability distribution associated model weight parameter using deﬁnition directional derivative second derivative respect random direction expressed last line hessian decomposed data-dependent term data-independent term. remarkable fact absence hidden units data-dependent part hessian zero model—and therefore perturbation model—have inﬂuence states. conditioning optimization problem therefore analyzed exclusively perspective model without even looking data. data-dependent term likely small even presence hidden variables sharp reduction entropy caused clamping visible units data. think well-conditioned model model perturbation model parameter direction causes well-behaved perturbation state expectations hξξ⊤iw pearlmutter showed boltzmann machine hidden units projected hessian reduced thus getting limit leading numerically accurate estimates. lecun showed stability optimization problem quantiﬁed conditioning number deﬁned ratio largest eigenvalue smallest eigenvalue geometrical interpretation conditioning number given figure rank approximation hessian obtained columns form basis independent unit vectors projects hessian low-dimensional random subspace. conditioning number estimated performing singular value decomposition projected hessian taking ratio largest smallest resulting eigenvalues. numerical estimates clearly exhibit better conditioning occuring sigmoid centered. -fold factor conditioning number non-centered centered boltzmann machines striking. figure left diagram two-layer deep boltzmann machine along parameters. right diﬀerent sampling methods feed-forward pass network starting data point path followed alternate gibbs sampler path followed alternate gibbs sampler input clamped data. technical practical reasons common introduce structure boltzmann machine restricting connections units. typical structure deep boltzmann machine units organized deep layered architecture. layered structure advantages ﬁrst gives speciﬁc role units layer easily build layer kernels exploit hierarchical structure data. second layered structure folded bipartite graph easy derive eﬃcient alternate gibbs sampler. case two-layer deep boltzmann machine shown figure energy function associated state }mx+my+mz takes form gibbs sampler used sampling data-dependent states diﬀerence input units clamped data. show basic algorithm based persistent contrastive divergence training two-layer centered present method introduced montavon measures representation evolves layer layer deep network. based theoretical insight projection input distribution onto hidden units layer provides function space thought representation feature extractor. method aims characterize function space constructing kernel layer approximates implicit transfer function input layer measuring much kernels match task interest. approach theoretically motivated work braun showing projections leading components implicit kernel feature obtained ﬁnite typically small number samples close essentially multiplicative errors asymptotic counterparts. following lines describe principal steps analysis subsume mapping performed layers deep network kernel scale kernel compute empirical kernel size eigenvectors sorted decreasing magnitude respective eigenvalues spans leading kernel principal components empirical kernel. error obtained residuals projection labels leading components mapped distribution curves represent well task solved principal components data distribution. curves interpreted learning curves regularization imposed rank kernel feature space determines number samples necessary order train model eﬀectively. therefore number observed kernel principal components closely relates amount label information given learning machine. small values cover oneshot learning regime model asked generalize observations. hand large values cover extreme case label information abundant representation rich enough order encode subtle variation learning problem. practical purposes curves reduced follows describe basic analysis. salakhutdinov hinton introduced elaborate procedures particular types boltzmann machines restricted semi-restricted deep boltzmann machines. unnormalized probability state computing analytically intractable exponential number elements involved sum. rewrite ratio partition functions follows shown sequence models evolves slowly enough importance weight obtained annealed importance sampling procedure estimate ratio partition function model partition function base rate model. ωais importance weight resulting annealing process freely running gibbs sampler νais importance weight resulting annealing input units clamped data point. substituting equation equation obtain generally computing average importance weight νais data point take long time. practice approximation computation estimate computed single point. case follows jensen’s inequality consequently approximation tends produce slightly pessimistic estimates model log-likelihood however variance νais compared variance ωais clamping visible units data points sharply reduces diversity runs. approximation suﬃciently accurate purpose paper demonstrating importance centering deep boltzmann machines. section describe diﬀerent parameters used train deep boltzmann machines perform discriminative generative analysis. parameters correspond reasonable choices validated previous research work. architecture consider two-layer deep boltzmann machines made input units intermediate units units. initial biases oﬀsets visible units sigm− sigm. consider diﬀerent initial biases oﬀsets sigm sigm) hidden units. oﬀsets initial biases correspond sigmoids plotted figure data train dbms binary version mnist handwritten digits data activation threshold mnist training consists samples. sample binary image size representing handwritten digit -dimensional binary vector. inference persistent contrastive divergence train network keep track free particles background learning procedure. gibbs sampling estimation collect data-independent data-dependent statistics. rationale classical mean ﬁeld estimation data statistics tends artiﬁcially drive sparsity convex/concave shape sigmoid function. step learning procedure iterations alternate gibbs sampler collecting data-dependent statistics iteration updating data-independent statistics. learning stochastic gradient descent approximate log-likelihood minibatches size learning rate layer. practical purposes minibatch size equivalent number particles persistent contrastive divergence consider models trained epochs. model averaging variant averaged stochastic gradient descent reducing parameter noise. compute step parameter estimate θavg θavg order remember last training procedure. discriminative analysis analysis performed subset samples drawn randomly mnist test set. representations layer built running gibbs sampler iterations input clamped data taking mean activation unit. discriminative performance measured projection residuals labels area error curve results produced candidate scale parameters gaussian kernel generative analysis generative analysis performed subset samples drawn randomly mnist test set. generative performance measured estimated log-likelihood model given test data estimate partition function using runs. estimate partition functions using single run. length model parameter step annealing process deﬁned sequence parameters implies annealing starts large parameter updates ﬁnishes small updates. table corroborates importance centering better discriminating layer deep boltzmann machine. seen figure discriminative performance layer improved training network longer time. table discriminative performance epochs layer deep boltzmann machine measured equation diﬀerent conﬁgurations initial bias oﬀset. lower error better. case centering sigmoids leads better discrimination layer. table generative performance epochs terms estimated model log-likelihood hlog pidata diﬀerent conﬁgurations initial bias oﬀset. generative performance less sensitive initial conditioning layer discriminative performance top-level units simply discarded leading essentially robust one-layer generative model. table supports importance centering showing centered dbms learn better generative model data. however advantage strong discriminative case. indeed units layer critical generative performance learning algorithm simply discard learn one-layer shallow generative model instead. figure highlight importance centering faster stable learning. models emerging centered deep boltzmann machine systematically better discriminative properties layer good generative properties. noncentered ultimately learn model good produced centered also diverge. figure show model able learn reasonable ﬁrst-layer ﬁlters second-layer ﬁlters learned centered tend varied learned non-centered dbm. higher variety second layer ﬁlters suggests centered produces richer top-level representation. argument corroborated figure showing that absence centering mechanism projection data layer representation tends form simplistic low-dimensional manifold still contain useful features hand also discards potentially useful discriminative features. research work authors computing lower bound probablity instead direct estimate thus making direct comparison impossible. also estimates probability become increasingly inaccurate model complexiﬁes. figure left residuals projection labels leading components layer kernel epochs. right layerwise evolution representation terms area error curve epochs. centered dbms stable non-centered ones. layer representations clearly better input. figure convergence speed centered non-centered dbms centered dbms learn faster stable non-centered ones. note estimate log-likelihood equation becomes inaccurate model becomes complex figure examples ﬁrst-layer ﬁlters diﬀerent bias oﬀset parameters epochs. ﬁlters rendered using linear backprojection layer units onto input space. model producing reasonable ﬁrst-layer ﬁlters suggesting one-layer networks less sensitive quality conditioning parameter space. figure examples second-layer ﬁlters diﬀerent bias oﬀset parameters epochs. ﬁlters rendered using linear backprojection intermediate layer units onto input space. here clearly diversity ﬁlters higher centered. figure examples digits generated diﬀerent bias oﬀset parameters epochs. degenerated second layer non-centered seems negative impact balance diﬀerent classes. figure -kpca visualization top-level representation diﬀerent bias oﬀset parameters diﬀerent stages training. points colored according label non-centered dbms tend collapse data onto simplistic low-dimensional manifold layer representation. hand centered clearly observe late stage training emergence clusters corresponding labels. presented simple modiﬁcation deep boltzmann machine centers output sigmoids rewriting energy function function centered states. centered version deep boltzmann machine easy implement simply involves reparameterization energy function. theoretical motivation centering leads better conditioning hessian optimization criterion. simple modiﬁcation allows learn eﬃciently deep boltzmann machine without greedy layer-wise pretraining. experiments real data corroborate beneﬁts centering showing centered deep boltzmann machine learns faster stable non-centered counterpart. addition centered deep boltzmann machine produces useful discriminative features layer good generative model data. training hierarchies many layers still tedious requires many iterations. understanding whether diﬃculty comes diﬃcult optimization problem exhaustion statistical information data remains done. also despite initial good conditioning hessian excluded solution progressively drifts towards degenerate regions parameter space throughout learning procedure. strategies dynamically maintain solution within well-behaved regions parameter space better descend objective function also need investigated.", "year": 2012}