{"title": "Mix-nets: Factored Mixtures of Gaussians in Bayesian Networks With Mixed  Continuous And Discrete Variables", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Recently developed techniques have made it possible to quickly learn accurate probability density functions from data in low-dimensional continuous space. In particular, mixtures of Gaussians can be fitted to data very quickly using an accelerated EM algorithm that employs multiresolution kd-trees (Moore, 1999). In this paper, we propose a kind of Bayesian networks in which low-dimensional mixtures of Gaussians over different subsets of the domain's variables are combined into a coherent joint probability model over the entire domain. The network is also capable of modeling complex dependencies between discrete variables and continuous variables without requiring discretization of the continuous variables. We present efficient heuristic algorithms for automatically learning these networks from data, and perform comparative experiments illustrated how well these networks model real scientific data and synthetic data. We also briefly discuss some possible improvements to the networks, as well as possible applications.", "text": "recently sible quickly functions spaces. mixtures fitted data quickly celerated olution propose low-dimensional ferent bined coherent entire modeling crete variables requiring ables. present automatically data perform trating entific data synthetic discuss works well possible bayesian variables modeled typically simple parametric researchers continuous distributions within sums gaussians ample weighted proximate conditional probability density morrell tinuous sive learn. expense problematic appropriate bayesian network structure forehand. variables learned data butions learning algorithm unmanageably large. bayesian joint probability bayesian acyclic vertex graph variable main. directed edges graph specify pendence variables ents\" v;;. independence ified follows information variables scendants allows decompose fortunately turned marginalized exactly. distribution consistent ditional case given directed bayesian acquire models marginalize models tion entire joint models subsets conditionalized within bayesian distribution ents modeled conditionalizing ded\" bayesian variable parents compute marginalization number gaussians ture a's. means covariances marginal original variable suitable functions suming variables class models employ continuous paper although analogous fashion. functional similar ditionalizing gaussian tresp differences tions fewer gaussians weights fewer gaussians able applications speed inference. curate models many situations experimentally. suppose moment contains values. powerful density mixture represent assigns ture model assume data generated independently nature begins randomly discrete multidimensional gaussian covariance algorithm models data number entries table estimates marginalized table. combination gaussian appropriate table mixture original mixture ized table. discrete combination combine several different gaussian various table marginalized calcu­ lated least find \"good\" network general ture model given dataset complete discrete ables. popular model discrete structures criterion large number networks. learning gaussian fast complex task performs sive re-run hundreds variable subsets networks practice relatively however networks much less computation. class algorithms involves constructing actions variables work models strongest interac­ tions tomatically order measure pairwise interactions variables network arcs i.e. variables sumed independent. gorithm calculate score. actly arc. variables networks. denote network single note compute score need recompute since others simply copied define \"importance\" depen­ dency variable algorithm generates pair­ variables wise dependency strengths heuristic tree algorithm negative). networks variable maximum spanning maximizes ization algorithm case unpenalized jective heuristic dependencies least high score b�pt details perfectly often worth simply assuming since learning joint expensive times measure take prohibitive used choose order algorithm selects select arcs adding graph. tual values irrelevant matter ranks whether greater zero. thus order reduce expense com­ puting computing dataset discretized dataset continuous generally computed would expect highly correlated circumstances. structure-learning dependence learn networks networks potential ensure ternatively striction candidate generated network structures performed greedily. similar press discrete single-gaussian mary network-learning differences. single multidimensional sian. contain variable marginalized gaussians spaces mixtures gorithm domain minus one. also allow algorithm datapoints sample dataset rather computing pseudo-discrete network-learning greedy algorithm network. duces employ gaussian tributions would modeled bayesian discretized able domain boundaries datapoints equal. conditional table containing parent variables value discretized lected parent variable. tains histogram bility xi's buckets. associated density spread uniformly bucket. variables except translation conditional dataset scores significantly four model learners. fact significantly outper­ formed independent learning algorithm indicates relationships useful relationships dependencies. pseudo-discrete works indicates network nodes' parameterizations achieve parameterizations. primary took hour half time pentium generate validation tested previously ferent \"bio\" dataset ical cell assay. records continuous; variables crete. discrete variable different taken sloan digital tronomical tains records continuous; ranging values scaled within small amount uniform noise added prevent point distributions verify mix-net's pseudo-discrete mixtures structed values original dataset identical networks discretized value translated back real value sampling uniformly sulting original posed piecewise precisely networks discrete test-set performance would expect mix-nets synthetic however space restrictions prevent tail possible sion tech report version moore networks applicable observed. datasets real values compressed greedy network-learning classification similar rithms (friedman flexible developed ations particular rithm. wide variety rithms discrete bayesian herskovits bacchus eta. friedman employed dirtier learning mix-net gorithms networks networks dependencies proaches datasets (monti current deal well discrete sible values variables. also grounds mixture able determining uous values come also determines datapoint's assumed conditionally given class variable. previously stutz accelerated generalized ever. another rather discrete explored nique previously crete domains (friedman vious work (koller closely bination order represent ploration", "year": 2013}