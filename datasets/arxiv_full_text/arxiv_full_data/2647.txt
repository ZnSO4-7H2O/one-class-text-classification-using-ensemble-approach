{"title": "Inverse Reinforcement Learning from Incomplete Observation Data", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Inverse reinforcement learning (IRL) aims to explain observed strategic behavior by fitting reinforcement learning models to behavioral data. However, traditional IRL methods are only applicable when the observations are in the form of state-action paths. This assumption may not hold in many real-world modelling settings, where only partial observations are available. In general, we may assume that there is a summarizing function $\\sigma$, which acts as a filter between us and the true state-action paths that constitute the demonstration. Some initial approaches to extending IRL to such situations have been presented, but with very specific assumptions about the structure of $\\sigma$, such as that only certain state observations are missing. This paper instead focuses on the most general case of the problem, where no assumptions are made about the summarizing function, except that it can be evaluated. We demonstrate that inference is still possible. The paper presents exact and approximate inference algorithms that allow full posterior inference, which is particularly important for assessing parameter uncertainty in this challenging inference situation. Empirical scalability is demonstrated to reasonably sized problems, and practical applicability is demonstrated by estimating the posterior for a cognitive science RL model based on observed user's task completion time only.", "text": "however limitation traditional problem formulation assumption full paths containing actions states observed. many real-world situations ﬁne-grained observations available multiple reasons. example costly sensors could gather ﬁne-grained observations even impossible change measurement devices owned third party. also even accurate sensors used various environmental factors cause unavoidable occlusion censoring distortion measurements. furthermore existing datasets unlikely contain full path data data collected mind. elaborate motivations later. initial approaches addressing issue. earliest assume instead actual paths might observe expected amount state features agent encounters demonstrated paths known feature expectations later approaches relaxed assumption state observations accurate probabilistic instead observing states assume probability distribution state-space given timestep however existing methods applicable general situations external observer partial observability path level. summary contributions paper formulates summary data problem extends problem situations full paths directly available. assume summarizing function acts ﬁlter external observer true paths. demonstrate even general case prior assumptions summarizing function inference still possible problem class thus signiﬁcantly extending scope problems performed. derive exact likelihood problem approximations signiﬁcantly faster evaluate. ﬁrst approximation monte-carlo estimate second uses approximate bayesian computation approach. demonstrate approximations feasible mdps optimal policies estimated reasonable time. using grid world example demonstrate exact approximate methods able recover parameters reward function good accuracy approximate inverse reinforcement learning aims explain observed strategic behavior ﬁtting reinforcement learning models behavioral data. however traditional methods applicable observations form state-action paths. assumption hold many real-world modelling settings partial observations available. general assume summarizing function acts ﬁlter true state-action paths constitute demonstration. initial approaches extending situations presented speciﬁc assumptions structure certain state observations missing. paper instead focuses general case problem assumptions made summarizing function except evaluated. demonstrate inference still possible. paper presents exact approximate inference algorithms allow full posterior inference particularly important assessing parameter uncertainty challenging inference situation. empirical scalability demonstrated reasonably sized problems practical applicability demonstrated estimating posterior cognitive science model based observed user’s task completion time only. inverse reinforcement learning generally formulated given markov decision-process rewardfunction unknown parameters; state-action paths demonstrating optimal behavior given true optionally prior methods solving problem used parameter inference multiple real-world modelling situations complex behavior observed form state-action paths. examples include driver route modelling helicopter acrobatics learning perform motor tasks dialogue systems pedestrian activity prediction commuting routines feature expectations paths traveled agent available feature expectations computed true paths function yielding vector state features. reward function linear state features inference problem formulated function ˆµe. probabilistic observations states environment available assumed instead observing state external observer observes distribution natural assumption example assuming measurement noise. general approach estimate state visitation frequencies based observations turn estimate feature expectations standard methods used. inference approaches common approaches solving problem. mcmc applied computing samples posterior unnormalized likelihood evaluated closed form gradient descent applied giving point estimates gradient likelihood evaluated closed form also point estimation based linear programming classiﬁcation considered. relationship imitation learning formulation problem close imitation learning also known apprenticeship learning interested recovering underlying parameters model able replicate behavior expert sufﬁcient. thus goal recover policy behavior generated policy matches demonstrated expert instead explicitly recovering parameters underlying mdp. general complex problem parameter recovery problem generally under-determined depending formulation also degenerate solutions reason approach either recover full posterior quantiﬁes uncertainty point estimates maximally robust solution problem generally solves corresponding problem might give robust solution reward structure often generalizable compared policy replicate. example clear policy behave state covered examples parameters recovered methods scale signiﬁcantly better. using recent model cognitive science literature demonstrate sensible approximate posterior inferred based task completion times collected user experiments. methods additional interesting properties. first differentiate different types parameters allows inference easily extended interesting parameters generative process besides traditional reward function. second also allow non-linear reward functions used case many existing methods. third approximate methods also used situations transition function known long generate draws model assumptions standard modeling assumption agent interacting environment demonstrating optimal behavior independent episodes thus creating paths path sequence states actions denoted state action timestep length trajectory deﬁned tuple states actions state transition function reward function temporal discount rate. deﬁned terms unknown parameters instance ﬁxed parameters denoted agent partial observability environment state situation deﬁned analogously pomdp possible observations observation function. observation assumptions regarding observations external observer agent’s behavior four types settings studied policy agent known words know exactly agent behave situation. noise-free observations states environment actions agent available probably common formulation literature. beneﬁt assumption allows likelihood factorized state transition. exact likelihood derive computable likelihood assume ﬁnite maximum number actions within observed episode tmax. denote ﬁnite plausible trajectories stmax+ atmax. monte-carlo estimate likelihood possibility deal monte-carlo estimate. approach paths simulated using policy path drawn probability unnormalized likelihood individual observation estimated monte-carlo contribution sample weighted probability path cancels existing term product. beneﬁt approach transition probabilities need deﬁned closed form generating monte-carlo samples enough draw samples. also need assume ﬁnite size. issue approach might paths monte-carlo sample non-zero observation probability certain observation dataset common negligible support path distribution tail sufﬁciently covered ﬁnite sample. alleviate problem small constant value likelihood observation a-priori estimate. example might sensible heuristic vanishes large enough sample. problem deﬁnition parametrized ﬁnite parameters interest true parameters assume agent whose behavior agrees optimal policy mθ∗. know prior assume agent taken paths observed summaries paths stochastic summary function transforms path another type observation generally contains less information original path inverse reinforcement learning problem summary data problem given summaries optimal behavior; summary function unknown; optionally prior determine posterior traditional setting would parameters reward function. formulation extends inference problem parameters well. similar extension traditional setting recently considered herman coarse-grained noisy observations generally cheaper acquire compared accurate path observations. example signiﬁcantly easier keyboard mouse clicks computer users compared eye-tracking think-aloud observations. full path data takes space summaries makes likely relevant features data stored later analysis. also bandwidth restrictions might prevent transmitting full path data observations done remotely. modelling adversary likely prevent observing full paths. example games incomplete information poker starcraft opponent hides details states actions possible. privacy guarantees result data released non-identifying summaries. complementary previous; data summarized prevent possible adversary identifying speciﬁc types information. estimate likelihood third alternative avoid evaluating likelihood function entirely approximate bayesian computation approach instead. also uses monte-carlo samples estimating likelihood comparing samples directly observation data using discrepancy function often chosen similar prediction error function. essentially means monte-carlo sample transformed simulated summary observations using discrepancy observation data computed. make assumptions type summary observations choice ﬁxed here. often literature norm general features summary datasets prediction error function logarithm used. approach seen imitation learning estimating parameter likelihood behaviour similarity. extension matching feature expectations generalized global features behavior available beneﬁt approach observation probabilities need available closed form long draw samples inference recent work shown feasibility gaussian process surrogates expensive likelihoods also setting approach well likelihoods work expensive evaluate. algorithm summarizes estimation likelihood surface based exact approximate methods. performing global non-convex optimization make additional assumption likelihood mainly contained within bounded region utilize generic subroutines function given ﬁnds optimal policy function given policy simulates path using policy. data hyperparameters denote predicted mean standard deviation full posterior denoted denote number samples estimating surrogate nopt acquisition function value denotes threshold minimum predicted value discrepancy represents best model given available information. infer parameters agent’s reward function based summarized path observations. approximate methods able recover reward function parameters comparable quality exact method considerably faster. also demonstrate approach scales realistic modeling cases well. show approximation able infer reasonable approximate posterior rl-based cognitive model literature based measurements real user behavior. details experiments collected supplementary material. grid world grid world well-known problem type literature problem agent located cell discrete two-dimensional grid cells. agent enters cell receives reward based features cell features agent’s reward function according rstep. case agent initially located random cell edge grid. cell center grid goal entering goal gives agent large positive reward ends episode. grid cell binary features generated placing walls feature random grid summary function deﬁned yielding initial state edge number steps took reach goal center problem infer likely values simulated behavior values matches observations given summary observations deﬁnition. simulated observation sets grids various sizes. used grids features avoid long paths would make exact method infeasible evaluate. computed ﬁrst iteration step algorithms recorded elapsed wall-clock time. empirical run-time exact algorithm grows rapidly size grid increases expected |ξap| grows exponentially length path grows linearly. hand run-times approximate algorithms scale comparatively much better. equally expensive monte-carlo expected. experiment inference quality compared quality inference exact approximate methods small grids. also investigate performance approximate methods larger grids exact method computationally infeasible. experiments performed comparing exact method limited length paths observation dataset keep computation time feasible also random baseline uniform random draw parameter space. measure inference quality accuracy parameter recovery quantiﬁes performance prediction accuracy quantiﬁes imitation learning performance. accuracy parameter recovery measured rmse likelihood mean ground truth. mean used instead likelihoods sometimes broad; mean robust estimate initial trials. prediction error measured path length individual starting location measured separate dataset generated ground truth parameters. discrepancy used logarithm prediction error computed observation dataset observe approximate methods perform well compared exact method. approximate methods able recover reward function parameters comparable accuracy exact method shown figure demonstrates monte-carlo sampling feasible approach estimating true likelihood directly matching global features predicted behavior abc. also discrepancy predicted behavior relatively methods suggesting policies recovered methods good approximations true policy. statistically signiﬁcant differences ground truth errors prediction errors methods except random baseline worse approximate methods able perform well larger grids exact method computationally infeasible. able recover parameter values reliably discrepancy also increases predictably grid size ratio rewards well identiﬁed still uncertainty left scale rewards. would possible infer insight point estimate demonstrates beneﬁt estimating full likelihood surface. experiment modelling computer users ﬁnal experiment infer full posterior recent rl-based cognitive model using realistic observation data. task estimate parameters modelling oculomotor system user searching speciﬁc menu-item computer drop-down menu although large computer screens traditional methods used detailed actions measured eye-tracking small menus accuracy eye-tracking often poor comparison. however simple summary statistics time opening menu clicking target item simple measure accurately require solving irl-sd problem. recently kangasr¨a¨asi¨o found parameter estimates model using summary observations user study bailly summary observation included task completion time milliseconds whether target present absent menu. extend analysis showing full posteriors figure representative example likelihood densities estimated different methods montecarlo able produce reasonable approximation exact likelihood. left exact. center montecarlo. right abc. color maps chosen maxima functions white minima black. likelihood mean marked square ground truth parameters star. examples found supplementary material. getting average predicted correctly primary goal model getting variation correct well secondary goal. reason discrepancy function chosen logarithm squared differences means plus absolute differences standard deviations summed menu conditions. infer posteriors three parameters duration ﬁxations fdur duration moving mouse select item dsel probability recalling full menu layout memory prec. reward function agent receives penalty equal milliseconds spent performing action. duration action saccade duration fdur dsel. perspective fdur dsel also seen parameters reward function. finding correct item leads reward quitting target item menu. quitting target present results penalty posterior visualized figure using slices location observe posteriori correlation fdur prec similarly fdur dsel. understandable increasing fdur would increase predicted would decreasing prec increasing dsel. posterior fdur centered around still uncertainly left dsel prec. uncertainty dsel explained difﬁculty pointing precisely target item cursor causes variation duration. uncertainty prec explained fact menus encountered early experiments completely subjects experiment progressed subjects likely recall menus. also observe signiﬁcant posterior correlation prec dsel. indicates although affect effects orthogonal; increasing probability recalling menu fully compensated increasing selection duration. figure approximate posterior inferred demonstrates parameters identiﬁed remaining uncertainty well characterized. left ﬁxation duration fdur menu recall probability prec. center ﬁxation duration fdur selection delay dsel. right menu recall probability prec selection delay dsel. color chosen maximum posterior white minimum black. paper deﬁned irl-sd problem task inverse reinforcement learning based summarized incomplete observations agent’s behavior. proposed exact approximate methods inference. monte-carlo approximation used summary function available probability distribution non-negligible support even evaluated. demonstrated proposed methods able produce feasible results exact method computationally expensive. however approximate methods used even full posterior inference realistic mdps real observation data. methods presented feasible baselines specialized inference algorithms take advantage assumptions state-of-the-art situations currently out-of-reach existing speciﬁc methods. scalability inference large parameter spaces even complex mdps interesting challenging issue requires still work. ﬁrst problem inherent scalability non-convex optimization highdimensional spaces. alleviated likelihood mainly varies smaller-dimensional subspace second problem ﬁnding solutions problems take notable amount time. alleviated using transfer learning similarly done ramachandran amir need choose discrepancy function threshold unavoidable abc; recent summary different methods provided lintusaari promising choices either domain knowledge naturally task-speciﬁc generally learn data classiﬁer used form discrepancy function inference would feasible. challenging deﬁne requirements observation data without considering speciﬁc application evaluating feasibility inference needs made based expert knowledge empirical experiments. beneﬁt proposed bayesian approach full posterior allows remaining uncertainty directly estimated. cases methods exist agent partial observability environment state pomdp model used external observer partial observability environment state traditional methods extended proposed methods computational complexity provide rough analysis estimating computational cost presented methods. simplify analysis assume computational budget decided various operations. denote budget solving problem budget minimizing bounded non-convex function min. also assume costs evaluating thus total complexity performing exact inference worst-case complexity ﬁnding given summary observation size largest possible given summary observation. similarly total complexity performing approximin +nopt)). mate inference demonstrated regret made converge exponentially function nopt although base number close extension pomdp situation pomdp exact likelihood becomes even infeasible evaluate complex belief-states continuous. however approximations remain feasible long exists method estimating monte-carlo sample constructed using estimated policy methods applied similarly case. experiments model details walls deﬁne grid cell features generated follows choose grid cell random; choose vertical horizontal direction random; choose another cell random along chosen direction; feature values cells cells except cell goal cell. example grid three features shown figure figure visualization grid three features generated placing random walls feature. feature shown individually black squares denoting presence feature. feature thought different type obstacle agent four actions allow move neighboring cells. action fails probability pslip resulting agent moving random neighboring cell. attempting move outside grid returns agent current cell. used pslip rstep tmax steps assumed sufﬁcient reasonable strategy. estimation likelihood mean estimated mean likelihood using mcmc samples drew samples burnout thinning starting center boundaries. proposal distribution symmetric gaussian standard deviation exact monte-carlo estimates sampling distribution log-likelihood surfaces computing acceptance ratio sample drew realization posterior computed likelihood ratio realization. found taking uncertainty account superior using predictive mean. samples predictive mean often resulted unrealistically narrow likelihood take account remaining uncertainty log-likelihood surface. figure left predictive mean samples. right unnormalized log-likelihood surface mean. bottom left samples log-likelihood sample mean unnormalized log-likelihood surface mean. bottom right estimated likelihood density sample mean. inference details number possible paths given observation ﬁnite likelihood function evaluated principle. given start state path length paths form tree branching factor depth equal path length tree pruned requiring absorbing goal state appears exactly path ﬁnal state. also case deterministic policy prune actions state disagree policy. monte-carlo estimate likelihood added estimated likelihood observation prevent likelihood sample set. thought prior observation likelihood. experiment details used evenly spread constant values parameter ground truth used used done remove noise results caused variation ground truth promote identiﬁability parameters. states semantic relevance item either unobserved medium high item target item. length item either unobserved correct incorrect. constitute state agent observes. initially items unobserved agent observe ﬁxating item. agent always observes semantic relevance ﬁxated item. semantic relevance neighboring item observed probability psem. probability agent observe length current item probability length neighboring item. also probability prec agent recall full menu layout ﬁrst ﬁxation duration ﬁxation fdur agent ﬁxates target item agent select takes additional dsel seconds. agent also quit instantaneous. probability target item appear menu maximum number actions session deﬁne explicit probability distribution. implementation generates full menu layout beginning episode state transitions episode based menu realization. inference similar prior kangasr¨a¨asi¨o fdur truncated normal dsel truncated normal prec beta fdur dsel prec optimal policy estimated q-learning sessions batches sessions. step size learning rate exploration rate training done ﬁxed menus predictions done separate menus. bayesian optimization details radial basis function kernel used initial lengthscale kernel bound width variance roughly maximum difference minimum maximum sample values observed initial tests noise variance batch parameters optimized initial values. computing batch sample locations used acquisition rule combined local penalization", "year": 2017}