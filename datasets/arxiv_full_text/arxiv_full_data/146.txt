{"title": "Compatible Value Gradients for Reinforcement Learning of Continuous Deep  Policies", "tag": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "abstract": "This paper proposes GProp, a deep reinforcement learning algorithm for continuous policies with compatible function approximation. The algorithm is based on two innovations. Firstly, we present a temporal-difference based method for learning the gradient of the value-function. Secondly, we present the deviator-actor-critic (DAC) model, which comprises three neural networks that estimate the value function, its gradient, and determine the actor's policy respectively. We evaluate GProp on two challenging tasks: a contextual bandit problem constructed from nonparametric regression datasets that is designed to probe the ability of reinforcement learning algorithms to accurately estimate gradients; and the octopus arm, a challenging reinforcement learning benchmark. GProp is competitive with fully supervised methods on the bandit task and achieves the best performance to date on the octopus arm.", "text": "paper proposes gprop deep reinforcement learning algorithm continuous policies compatible function approximation. algorithm based innovations. firstly present temporal-diﬀerence based method learning gradient value-function. secondly present deviator-actor-critic model comprises three neural networks estimate value function gradient determine actor’s policy respectively. evaluate gprop challenging tasks contextual bandit problem constructed nonparametric regression datasets designed probe ability reinforcement learning algorithms accurately estimate gradients; octopus challenging reinforcement learning benchmark. gprop competitive fully supervised methods bandit task achieves best performance date octopus arm. reinforcement learning agent learns maximize discounted future rewards structure environment initially unknown agent must learn rewards associated various action-sequence pairs optimize policy. natural approach tackle subproblems separately critic actor critic estimates value diﬀerent actions actor maximizes rewards following policy gradient policy gradient methods proven useful settings high-dimensional continuous action spaces especially taskrelevant policy representations hand supervised setting representation deep learning algorithms recently demonstrated remarkable performance range benchmark problems. however problem learning features reinforcement learning remains comparatively underdeveloped. dramatic recent success uses q-learning ﬁnite action spaces essentially build neural network critic here consider continuous action spaces develop algorithm simultaneously learns value function gradient uses optimal policy. paper presents value-gradient backpropagation deep actor-critic algorithm continuous action spaces compatible function approximation. starting point deterministic policy gradient associated compatibility conditions derived roughly speaking compatibility conditions temporal diﬀerence methods directly estimate gradient value function. instead temporal diﬀerence methods applied learn approximation form estimates value state given current policy estimates advantage deviating current policy although advantage related gradient value function thing. representations used compatible approximation scale badly neural networks. second problem prior work restricted advantage functions constructed pends gradient policy. representation easy handle linear policies. however policy neural network standard state-action representation ties critic closely actor depends internal structure actor example result weight updates cannot performed backpropagation section method directly learn gradient value function. ﬁrst contribution modify temporal diﬀerence learning directly estimates gradient value-function. gradient perturbation trick lemma provides simultaneously estimate value function point gradient perturbing function’s input uncorrelated gaussian noise. plugging neural network instead linear estimator extends trick problem learning function gradient entire state-action space. moreover trick combines naturally temporal diﬀerence methods theorem therefore well-suited applications reinforcement learning. deviator-actor-critic model compatible function approximation. second contribution propose deviator-actor-critic model deﬁnition consisting three coupled neural networks value-gradient backpropagation algorithm backpropagates three diﬀerent signals train three networks. main result theorem gprop compatible function approximation implemented model neural network consists linear rectilinear units. proof relies decomposing actor-network individual units considered actors right based ideas also suggests interesting connections work structural credit assignment multiagent reinforcement learning contextual bandit task probe accuracy gradient estimates. third contribution independent interest contextual bandit setting designed probe ability reinforcement learning algorithms estimate gradients. supervised-to-contextual bandit transform proposed method turning classiﬁcation datasets k-armed contextual bandit datasets. interested continuous setting paper. therefore adapt transform twist. sarcos barrett datasets robotics features corresponding positions velocities accelerations seven joints labels corresponding torques. joints cases feature label spaces dimensional respectively. datasets traditionally used regression benchmarks labeled sarcos sarcos task predict torque single joint similarly barrett. convert datasets continuous contextual bandit tasks reward signal negative distance correct label -dimensional. algorithm thus told label lies sphere -dimensional space. missing information required label’s position precisely gradient. algorithm make predictions competitive fully supervised methods necessary extremely accurate gradient estimates. experiments. section evaluates performance gprop contextual bandit problems described challenging octopus task show gprop able simultaneously solve seven nonparametric regression problems without observing labels instead using distance actions correct labels. turns gprop competitive recent fully supervised learning algorithms task. finally evaluate gprop octopus benchmark achieves best performance reported date. proposal builds ideas introduced deep q-learning replay. however deep q-learning restricted ﬁnite action spaces whereas concerned continuous action spaces. policy gradients introduced used extensively deterministic policy gradient introduced also proposed algorithm copdac-q. relationship gprop copdac-q discussed detail section alternate approach based idea backpropagating gradient value function developed unfortunately algorithms compatible function approximation general guarantees actor-critic interactions. section discussion. analysis used prove compatible function approximation relies decomposing actor neural network collection agents corresponding units network. relation gprop diﬀerence-based objective proposed multiagent learning discussed section boldface denote vectors subscripts time superscripts individual units network. sets parameters capitalized refer matrices parameters neural networks. section recalls previous work policy gradients. basic idea simultaneously train actor critic. critic learns estimate value diﬀerent policies; actor follows gradient value-function optimal policy terms expected rewards. environment modeled markov decision process consisting state space action space initial distribution states stationary transition distribution reward function policy function case policy function probability distributions denote distribution states time given policy since agent direct access value function must instead learn estimate suﬃcient condition plugging estimate policy gradient |a=µθ] yields unbiased estimator stated compatibility condition worth revisiting problems propose tackle paper. ﬁrst problem directly estimate gradient value function required condition standard approach used literature estimate value function closely related advantage function using temporal diﬀerence learning compute derivative estimate. next section shows gradient estimated directly. section tackle ﬁrst problem modifying temporal-diﬀerence learning directly estimates gradient value function. first developed approach estimating gradient black-box function point based perturbing function gaussian noise. turns approach extends easily learning gradient black-box function across entire domain. moreover easy combine neural networks temporal diﬀerence learning. gradient estimates intensively studied bandit problems rewards observed labels not. thus contrast supervised learning possible compute gradient loss bandit problems gradient must estimated. formally consider following setup. black-box model optimization introduced recent exposition. papers black-box consists ﬁrst-order oracle provide zeroth-order information ﬁrst-order information remark reward function black since nature provide gradient information. value function expected value state-action pair given policy never observed directly since computed discounting future rewards. td-learning popular approach estimating dynamic programming unfortunately impossible minimize mean-square error directly since valuefunction expected discounted future reward rather reward. value function provided explicitly environment even black-box. bellman error therefore used substitute mean-square error given good enough representation lemma guarantees minimizing perturbed gradient error yields gradient value function. unfortunately discussed above value function cannot queried directly. therefore introduce bellman gradient error proxy residual gradient gradient temporal diﬀerence methods introduced similar names confusing. methods methods derived gradient descent. contrast develop td-based approach learning gradients. approaches thus complementary straightforward combine. however paper restrict extending vanilla learning gradients. idea reformulate tdg-learning td-learning slightly diﬀerent reward function function approximation. since function approximation still linear guarantees convergence td-learning transfered automatically tdg-learning. gaussian noise plays roles. firstly controls explore/exploit tradeoﬀ controlling extent actor deviates current optimal policy. secondly controls resolution deviator estimates gradient. three networks trained backpropagating three diﬀerent signals. critic deviator actor backpropagate tdg-error perturbed tdg-error deviator’s gradient estimate respectively; algorithm explicit description weight updates individual units provided appendix deviator estimates gradient value-function respect deviations current policy. backpropagating gradient actor allows estimate inﬂuence actor-parameters value function function eﬀect policy. critic deviator learn representations suited estimating value function gradient respectively. note even though gradient linear function point highly nonlinear function general. similarly actor learns policy representation. experiments section however perturbation eﬀect slowing stabilizing deviator updates theorem suppose units rectilinear linear. actor-unit actornetwork exists reparametrization value-gradient approximator satisﬁes compatibility conditions theorem actor-network thus collection interdependent agents individually follow correct policy gradients. experiments show also collectively converge useful behaviors. guarantees temporal diﬀerence learning policy gradients typically based assumption value-function approximation linear function learned parameters. however interested case actor critic deviator neural networks therefore highly nonlinear functions parameters. goal thus relate representations learned neural networks prior work linear function approximations. remark neural network linear rectilinear units considered submodels corresponding diﬀerent subsets units. active submodel time consists active units feedforward sweep rectiﬁer network thus disentangled steps ﬁrst step highly nonlinear applies gating operation selects active submodel rendering various units inactive. second step computes output neural network matrix multiplication. important emphasize although active submodel linear function inputs outputs linear function weights. actor right that. time step actor-unit active interacts deviator-submodel corresponding current active submodel deviatornetwork. proof shows actor-unit compatible function approximation. first recall basic facts backpropagation case rectilinear units. recent work shown replacing sigmoid functions rectiﬁers improves performance neural networks consider neural network units equipped weight vector hidden units rectiﬁers; output units linear. units total. network function network error function gradient ∇xout denote output unit {ii→j} denote input note depends supressed notation. subsection proves condition compatible function approximation minimal linear deviator-actor-critic model. next subsection shows minimal model arises level actor-units. proof compatibility condition follows lemmas compatibility condition holds since critic deviator minimize bellman gradient error respect also implicitly minimizes bellman gradient error respect corresponding reparametrized ˜w’s actor-unit. interesting relate approach literature multiagent reinforcement learning particular consider structural credit assignment problem within populations interacting agents reward individual agents population rewards based collective behavior? propose train agents within populations diﬀerence-based objective form gprop causes actor-units optimize diﬀerence-based objective without requiring compute diﬀerence explicitly. although topic beyond scope current paper worth exploring suitably adapted variants backpropagation applied reinforcement learning problems multiagent setting. comparison copdac-q. extending standard value function approximation example setting actor neural network yields following representation used applying copdac-q octopus task short approximation example used copdac-q thus welladapted deep learning. main reason learning advantage function requires coupling vector parameters actor. comparison computing gradient value-function approximation. perhaps natural approach estimating gradient simply estimate value function gradient estimate derivative main problem approach that date show resulting updates critic actor compatible. also guarantees gradient critic good approximation gradient value function although intuitively plausible. problem becomes particularly severe value-function estimated neural network uses activation functions smooth rectifers. rectiﬁers becoming increasingly popular superior empirical performance evaluate gprop standard reinforcement learning benchmarks mountain pendulum puddle world since already handled linear actor-critic algorithms. contribution gprop ability learn representations suited nonlinear problems. cloning replay. temporal diﬀerence learning unstable neural network. recent innovation introduced stabilizes tdlearning clone separate network compute targets parameters cloned network updated periodically. implement similar modiﬁcation tdg-error algorithm also experience replay gprop well-suited replay since critic deviator learn values gradients full range previously observed stateaction pairs oﬄine. goal contextual bandit tasks probe ability reinforcement learning algorithms accurately estimate gradients. experimental setting thus independent interest. figure performance contextual bandit tasks. reward runs shown averaged performance variation gprop barely visible. epochs refer multiples dataset; algorithms ultimately trained number random samples datasets. description. converted robotics datasets sarcos barrett contextual bandit problems supervised-to-contextual-bandit transform datasets training points respectively features corresponding positions velocities accelerations seven joints. labels -dimensional vectors corresponding torques joints. contextual bandit task agent samples -dimensional state vectors i.i.d. either sarcos barrett training data executes -dimensional actions. negative mean-square distance action label. note reward scalar whereas correct label -dimensional vector. gradient reward agent thus receives less information bandit setting fully supervised setting. intuitively negative distance tells algorithm correct label lies surface sphere -dimensional action space centred recent action. contrast supervised setting algorithm given position label action space. bandit setting algorithm must estimate position label surface sphere. equivalently algorithm must estimate label’s direction relative center sphere given gradient value function. goal contextual bandit task thus simultaneously solve seven nonparametric regression problems observing distances-to-labels instead directly observing labels. value function relatively easy learn contextual bandit setting since task sequential. however value function gradient highly nonlinear precisely gradient speciﬁes labels spheres. network architectures. gprop copdac-q implemented actor deviator network layers critic hidden layers rectiﬁers. updates computed rmsprop momentum. variance gaussian noise decrease linearly reaching point remained ﬁxed. performance. figure compares test-set performance policies learned gprop copdac-q. ﬁnal policies trained gprop achieved average mean-square test error seven sarcos barrett benchmarks respectively. remarkably gprop competitive fully-supervised nonparametric regression algorithms sarcos barrett datasets figure results important note results reported papers algorithms given labels solve regression problem time. best knowledge prior examples bandit reinforcement learning algorithm competitive fully supervised methods regression datasets. comparison implemented backprop actor-network full-supervision. backprop converged sarcos barrett compared gprop. note backprop trained -dim labels whereas gprop receives -dim rewards. figure gradient estimates contextual bandit tasks. normalized gradient estimates compared true gradients i.e. shown runs copdac-q gprop along averages accuracy gradient-estimates. true value-gradients computed compared algorithm’s estimates contextual bandit task. fig. shows performance algorithms. gprop’s gradient-error converges tasks. copdac-q’s gradient estimate implicit advantage function converges conﬁrms gprop yields signiﬁcantly better gradient estimates. copdac-q’s estimates signiﬁcantly worse barrett compared sarcos line worse performance copdac-q barrett fig. unclear copdac-q’s gradient estimate gets worse barrett period time. hand since guarantees copdac-q’s estimates follows erratic behavior perhaps surprising. comparison bandit task note although contextual bandit problems investigated lower-dimensional bandit problem nevertheless much harder. optimal action bandit problem cases constant vector consisting contrast sarcos barrett nontrivial benchmarks even fully supervised. desciption. objective learn target simulated octopus settings taken importantly action-space simpliﬁed using macro-actions. compartments attached rotating base. state variables action variables controlling clockwise counter-clockwise rotation base three muscles compartment. network architectures. applied gprop actor-network hidden rectiﬁers linear output units clipped critic deviator networks hidden layers rectiﬁers linear output units. updates figure performance octopus task. runs gprop copdac-q -segment octopus action state dimensions. thick lines depict average values. left panel number steps/episode reach target. right panel corresponding average rewards/step. implemented copdac-q variety architectures; best results shown obtained using similar architecture gprop sigmoidal hidden units sigmoidal output units actor. linear rectilinear clipped-linear output units also tried. gprop cloning experience replay used increase stability. policy uses less steps target average gprop converges quicker better solution copdac-q. reader strongly encouraged compare results reported best knowledge gprop achieves best performance date octopus task. clear variability displayed ﬁgures policy stability. gradients learned gprop stable copdac-q. note higher variability exhibited gprop right-hand panel fig. misleading. arises dividing number steps lower gprop since hits target quickly training inﬂates gprop’s apparent variability. value-gradient backpropagation ﬁrst deep reinforcement learning algorithm compatible function approximation continuous policies. builds deterministic actor-critic copdac-q developed decisive modiﬁcations. first incorporate explicit estimate value gradient algorithm. second construct model decouples internal structure actor critic deviator three trained backpropagation. gprop achieves state-of-the-art performance contextual bandit problems simultaneously solves seven regression problems without observing labels. note gprop competitive recent fully supervised methods solve single regression problem time. further gprop outperforms prior state-of-the-art octopus task quickly converging onto policies rapidly ﬂuidly target.", "year": 2015}