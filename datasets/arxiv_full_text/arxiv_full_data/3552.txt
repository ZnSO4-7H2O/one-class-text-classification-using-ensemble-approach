{"title": "Data Dropout in Arbitrary Basis for Deep Network Regularization", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "An important problem in training deep networks with high capacity is to ensure that the trained network works well when presented with new inputs outside the training dataset. Dropout is an effective regularization technique to boost the network generalization in which a random subset of the elements of the given data and the extracted features are set to zero during the training process. In this paper, a new randomized regularization technique in which we withhold a random part of the data without necessarily turning off the neurons/data-elements is proposed. In the proposed method, of which the conventional dropout is shown to be a special case, random data dropout is performed in an arbitrary basis, hence the designation Generalized Dropout. We also present a framework whereby the proposed technique can be applied efficiently to convolutional neural networks. The presented numerical experiments demonstrate that the proposed technique yields notable performance gain. Generalized Dropout provides new insight into the idea of dropout, shows that we can achieve different performance gains by using different bases matrices, and opens up a new research question as of how to choose optimal bases matrices that achieve maximal performance gain.", "text": "abstract—an important problem training deep networks high capacity ensure trained network works well presented inputs outside training dataset. dropout effective regularization technique boost network’s generalization random subset elements given data extracted features zero training process. paper randomized regularization technique withhold random part data without necessarily turning neurons/dataelements proposed. proposed method conventional dropout shown special case random data dropout performed arbitrary basis hence designation generalized dropout. also present framework whereby proposed technique applied efﬁciently convolutional neural networks. presented numerical experiments demonstrate proposed technique yields notable performance gain. generalized dropout provides insight idea dropout shows achieve different performance gains using different bases matrices opens research question choose optimal bases matrices achieve maximal performance gain. recent years deep neural networks made major breakthroughs computer vision natural language processing many applications machine learning deep networks remarkable capacity merge high-level features end-to-end multi-layer differentiable framework. modern networks highly non-linear millions parameters features make notably expressive capable learning complicated data models. however training data limited overﬁtting occur network could learning irrelevant patterns training data signiﬁcantly degrade generalizability learned model. overﬁtting best alleviated using data training obtaining labeled data quite expensive practice. thus network regularization central problems machine learning many ideas proposed reduce overﬁtting. brieﬂy review deep network regularization techniques. subsequently proposed random data dropout method presented validated numerical simulations. shown proposed regularization technique yield notable performance gain. idea data dropout necessarily early regularization techniques introduce norm penalties network weights -norm norm penalties) training cost function penalty functions limit network’s capacity adding norm penalties equivalent constraining network weights inside norm balls. thus equivalent method directly control -norm -norm weights training process different idea enforce desired structure weights. successful example technique convolutional neutral network spatial invariance directly obtained convolution-based design weight matrices. early stopping another simple regularization technique stops learning process error separate validation starts increasing. words seeks prevent network becoming over-speciﬁc training data learning irrelevant patterns. paper focuses effective regularization technique generate fake training data order obtain valid fake data training example transformation applied data change class. instance object recognition image translation ﬂipping rotation proved effective end. another data augmentation technique perturbation noise data different idea generating fake data withholding part data given efﬁcient network expected correctly classify data reduced information. instance imagine image horse showing legs body head. intelligent classiﬁer able identify horse seeing part image part shows head horse. dropout uses similar idea generate artiﬁcial training data randomly sets zero random subset elements training example. instance middle image fig. shows original image applying dropout element zero probability equal interestingly euclidean distance data point data point dropout large expect network assign class label. recent years many variants dropout method ﬁrst introduced presented given randomized technique proposed special case training iteration sample binary random variables form matrix contrast conventional dropout choose different basis matrices different layers network. addition change basis matrices course training. instance presented numerical simulations generate random basis epoch. example algorithm demonstrates proposed approach generalized dropout forward pass k-layer fully connected network training process. algorithm {wi}k deﬁned weight matrices bias vectors network respectively. addition deﬁned dimension given data dimension data layer. algorithm generalized dropout k-layer fully connected network input data example initialization deﬁne {gi}k orthonormal bases rn×n rni×ni respectively. apply proposed data dropout given data dent binary random variables update apply proposed data dropout layers apply weight matrix bias vector wix+bi. apply element-wise non-linear function course training neural network observes training example several times. network highly expressive become over-speciﬁc training example. however random data dropout network experiences different variants training example different iterations. randomly generated training data relatively apart euclidean space. thus random data dropout help network over-speciﬁc training examples. multi-layer neural network output layer plays role data network composed next layers. thus apply random data dropout features’ vectors network layer generate random fake data subsequent layers. fig. left mnist training image depicts integer middle image shown left setting element zero independently probability right training image shown left applying projection matrix suppose training example ambient dimension. deﬁne {ei}n standard basis vector whose elements zero except element equal training example written {αi}n independent binary random variables probability p{αi however n-dimensional space spanned inﬁnite number different bases. instance suppose rn×n orthonormal matrix different identity matrix. thus vector represented respect columns instance build sampling random unit -norm vector. then matrix formed concatenating random vector orthonormal basis subspace orthogonal randomly chosen vector. channels height width plane respectively. order perform proposed data dropout apply random projection multiple ways. ﬁrst reshape data vector apply proposed data dropout vectorized then reshape data back original third order tensor. second computationally efﬁcient perform data dropout along feature planes dimension. words imagine third order tensor rc×a×b vector whose elements matrices dimension accordingly apply random data dropout applying projection matrix linear operator vector. suppose dimension projection matrix reshaped third order tensor matrix size then data dropout performed applying projection matrix reshaped data pdx. data projection reshape data back third order tensor prepare next convolutional layer. third apply projection matrix feature planes. words ﬁrst rc×a×b reshaped data dropout performed addition combine second third way. presented numerical experiments second i.e. random projection applied along feature planes dimension section validate proposed approach applying generalized dropout neural networks. addition compare performance gains achieved proposed method different basis matrices ﬁrst experiment -layer fully-connected network implemented. second experiment -layer used. element-wise non-linear function used experiments rectiﬁed linear unit mnist data consists training test examples representing digit image. experiments three choices matrix choice identity matrix corresponds conventional dropout. second choice normalized hadamard matrix. able hadamard matrix basis matrix zero given image data changing size third choice random orthonormal basis. form random orthonormal basis ﬁrst build matrix independent gaussian random variables left singular vectors basis matrix order form matrix input data next layers experiment apply generalized dropout layers -layer fully-connected neural network. fig. illustrates network structure. structure data dropout steps ﬁrst applied given data. fig. shows dimension data layer network. order accentuate overﬁtting phenomena network trained whole training data. instead data used training example images randomly sampled mnist training data. cross-entropy cost function network parameters updated using stochastic gradient descent momentum term overﬁtting occur early stages training process. thus ﬁrst train network epochs without data dropout. train saved network epochs using different basis matrices random data dropout. ﬁrst epochs learning rate last epochs learning rate reduced size mini-batches fig. shows test error network epoch. observe best performance gains achieved hadamard random bases higher corresponding gain conventional dropout. table summarizes best performance gains. best basis matrix achieve maximal performance gain interesting question future research. experiment -ﬁve layer neural network consisting convolutional fully-connected layers implemented. convolutional layer followed maxpooling layer kernel size stride equal fig. illustrates network structure dimension data layer. random data dropout applied given data output second third layers output fully-connected layer. data dropout output ﬁrst convolutional layer number extracted feature planes ﬁrst layer large. shown fig. ﬁrst vectorize data apply data projection. data projection convolutional layer applied along channel dimension. thus dimension projection matrices second third convolutional layers extract feature planes respectively similar previous experiment order emphasize overﬁtting phenomena train network full training data. experiment network trained images sampled randomly mnist training data images. previous experiment ﬁrst train network epochs data dropout saved network trained epochs using different basis matrices random data dropout. network parameters updated using adam algorithm learning rate equal fig. shows test merri¨enboer gulcehre bahdanau bougares schwenk bengio learning phrase representations using encoder-decoder statistical machine translation arxiv preprint arxiv. sermanet eigen zhang mathieu fergus lecun overfeat integrated recognition localization detection using convolutional networks arxiv preprint arxiv. hinton srivastava krizhevsky sutskever salakhutdinov improving neural networks preventing co-adaptation feature detectors arxiv preprint arxiv. error epoch. comparison fully-connected network performance gain achieved data dropout smaller. experiment random basis yields largest performance gain. table summarizes performance gains different basis matrices.", "year": 2017}