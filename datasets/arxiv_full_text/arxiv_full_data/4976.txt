{"title": "Bridging the Gap between Probabilistic and Deterministic Models: A  Simulation Study on a Variational Bayes Predictive Coding Recurrent Neural  Network Model", "tag": ["cs.AI", "cs.LG"], "abstract": "The current paper proposes a novel variational Bayes predictive coding RNN model, which can learn to generate fluctuated temporal patterns from exemplars. The model learns to maximize the lower bound of the weighted sum of the regularization and reconstruction error terms. We examined how this weighting can affect development of different types of information processing while learning fluctuated temporal patterns. Simulation results show that strong weighting of the reconstruction term causes the development of deterministic chaos for imitating the randomness observed in target sequences, while strong weighting of the regularization term causes the development of stochastic dynamics imitating probabilistic processes observed in targets. Moreover, results indicate that the most generalized learning emerges between these two extremes. The paper concludes with implications in terms of the underlying neuronal mechanisms for autism spectrum disorder and for free action.", "text": "abstract. current paper proposes novel variational bayes predictive coding model learn generate ﬂuctuated temporal patterns exemplars. model learns maximize lower bound weighted regularization reconstruction error terms. examined weighting aﬀect development diﬀerent types information processing learning ﬂuctuated temporal patterns. simulation results show strong weighting reconstruction term causes development deterministic chaos imitating randomness observed target sequences strong weighting regularization term causes development stochastic dynamics imitating probabilistic processes observed targets. moreover results indicate generalized learning emerges extremes. paper concludes implications terms underlying neuronal mechanisms autism spectrum disorder free action. cognitive agents dealing changing environment need develop internal models accounting ﬂuctuations extracting underlying structures learning. recently many schemes proposed learning ﬂuctuated temporal patterns extracting latent probabilistic structures. schemes include conventional dynamic bayesian networks kalman ﬁlters recently developed variational bayes recurrent neural network models time alternative trials deterministic dynamics approach predictive models probabilistically generated target sequences learned embedding extracted probabilistic structures deterministic chaos self-organized models approaches probabilistic deterministic developed relatively independently research bridging seems worthwhile. direction murata developed predictive coding-type stochastic model inspired free energy minimization principle model learns predict mean variance sensory input next time step multiple perceptual sequences mapping current latent state. learning optimizes connectivity weights also latent state initial step training sequence back-propagation time murata colleagues experimented degree initial state dependency learning imitate probabilistic sequences examined internal dynamic structure develops diﬀerently diﬀerent cases. turned initial state dependency arbitrated development deterministic probabilistic dynamic structure. case strong initial state dependency deterministic chaos dominantly developed stochastic dynamics develops estimating larger variance case weak initial state dependency. however model suﬀers considerable drawback. cannot estimate variance latent variables signiﬁcantly constrains capability model learning latent probabilistic structures target patterns. current paper proposes version variational bayes model estimate variance context unit time step proposed model simpler vbrnn models employs predictive coding scheme rather autoencoder. thus model referred variational bayes predictive coding assumes learning aims maximize lower bound represented weighted negative regularization term posterior distribution latent variable likelihood term output generation. current study investigates diﬀerently weighting terms summation learning inﬂuences development diﬀerent types information processing model. conducted simulation experiment vbp-rnn learned predict/generate ﬂuctuated temporal patterns containing probabilistic transitions prototypical patterns. consistent murata results showed diﬀerent weighting arbitrates extremes model develops either deterministic dynamic structure probabilistic one. analysis simulation results clariﬁes degree generalization learning well strength top-down intentionality generating patterns changes extreme another. generative model provide probabilistic prediction ﬂuctuating sensation. joint probability sensation latent variable generative model written product likelihood prior likelihood parameterized learning parameter which however becomes intractable likelihood nonlinear function then problem maximize joint probability given sensory dataset {xt}n inferring true posterior approximated using recognition model variational bayes well known approximation means minimization kl-divergence model approximation true posterior equivalent maximizing value referred lower bound lower bound maximized written ﬁrst term right hand side regularization term posterior distribution latent variable constrained similar prior usually taken unit gaussian distribution. second term minimizes reconstruction error. formula maximizing lower bound equivalent principle free energy minimization provided friston here describe implementation aforementioned formulation continuous-time model well multiple timescale teaching sensory sequence pattern used training vbp-rnn model regularization term lower bound teaching sequences written finally objective learning maximize total lower bound optimizing learning parameter latent state initial step latent state sequence given speciﬁc value following kingma welling’s reparameterization trick random value sampled used standard normal distribution time step i.e. sample current implementation latent state represented ensemble internal state values context units step then internal state context unit time step sequence computed estimation time varying mean time constant context unit. although vbp-ctrnn uses time constant value context units vbp-mtrnn uses diﬀerent time constant values diﬀerent units. learning parameters wµ|σ bµ|σ activation context unit value computed ti). gaussian regularization term lower bound numbers context units rewritten dimension prediction output computed form probabilistic distribution using softmax function elements. eventually distribution computed mapping current context unit activation patterns time step. probability dimension fig. scheme variational bayes predictive mtrnn abbreviation higher lower layers respectively. initial time step optimal initial latent states given network shown word sample shows context unit random value sampled standard figure outlines information vbp-mtrnn. learning process starts random initialization learning parameters initial latent state latent sequence. lower bound given obtained epoch computing latent state sequences well output sequences using equations maximized optimizing learning parameters initial latent state latent sequence using back-propagation time algorithm optimal convergence training expected model’s posterior sequence approximate true model simple compared variational bayes models models built separate functions decoder encoder rnn. optimizing connectivity weights initial latent states current model computes prediction output sequences means forward dynamics posterior latent state sequences means bptt. conducted simulation experiments determine learning proposed model depends meta-prior well number training patterns. figure illustrates simulation analysis performed study. first human generated patterns like probable probability transitions probabilistic ﬁnite state machine shown bottom left figure then trained types mtrnns target generator mtrnn output classiﬁer mtrnn prototypical patterns. tar-gen-mtrnn prepared autonomous generation target temporal patterns used training vbp-mtrnn. class-mtrnn prepared autonomous segmentation temporal patterns sequences labels assigned diﬀerent prototypical patterns used n-gram analysis. patterns generated tar-gen-mtrnn used target teaching patterns main experiment training vbp-mtrnn diﬀerent conditions. training characteristics output patterns generated vbpmtrnn quantitatively compared tar-gen-mtrnn. using trained class-mtrnn computed probabilistic distribution consecutive labels corresponding diﬀerent prototypical patterns classiﬁed output patterns generated steps tar-gen-mtrnn vbp-mtrnn. finally n-gram kl-divergence probability distributions computed order obtain measure similarity output generation tar-gen-mtrnn vbp-mtrnn trained diﬀerent conditions. network models consisted context layers context units trained epochs. time constants context layers lowest highest layer. input output units network. order target patterns tar-gen-mtrnn prepared autonomous generation ﬂuctuating target patterns steps used main experiment training vbp-mtrnn. temporal patterns training tar-genmtrnn provided human generate pattern compositions diﬀerent prototypical patterns using tablet input device. target sequence pattern generated concatenating prototypical patterns. prototypical pattern diﬀerent periodic pattern cycles ﬂuctuating amplitude periodicity appearance. prototypical pattern example generated times training patterns human. generated fig. illustration simulation analysis procedure. human gen. prob. patterns tar-gen mtrnn class. mtrnn class. labels label. seq. prob. dist. abbreviations human-generated probabilistic patterns target generator mtrnn classiﬁer mtrnn classiﬁed labels label sequences probabilistic distribution respectively. human tablet prototypical pattern expressed diﬀerent amplitudes periodicity trial. training human expressed patterns tar-gen-mtrnn generated output sequence pattern steps. output sequence pattern generated closed-loop adding gaussian noise zero mean constant internal state context unit time step. done order make network output patterns stochastic maintaining certain probabilistic structure transitions prototypical patterns. sampled sequence pattern generated step step. then groups target patterns sampled consisting sequence patterns step length sequence patterns step length. target sequence patterns used main experiment training vbp-mtrnn. order prepare class-mtrnn mtrnn model trained classify diﬀerent prototypical patterns humangenerated sequence pattern consisting consecutive prototypical patterns used teaching input pattern. corresponding label sequence pattern used target softmax output elements labels. tar-gen-mtrnn. training epochs condition closed-loop output patterns generated starting diﬀerent initial latent states obtained target sequences learning. figure compares target sequence pattern corresponding closed-loop regeneration vbp-mtrnn trained target sequences. ﬁrst shows target pattern second third fourth rows show regenerated patterns respectively. pattern associated sequence labels classiﬁed class-mtrnn. target sequence pattern completely regenerated steps. target regenerated patterns begin signiﬁcantly diverge around steps. local deviation prototypical pattern arose soon onset. divergence starts earlier. observations suggest vbp-mtrnn trained develops deterministic dynamics. additional analysis output sequence generated steps revealed periodicity suggesting deterministic chaos transient chaos developed learning condition. hand increased larger values training model developed probabilistic processing randomness generated internally. sigma context state values selected neural units inside second context layer vbp-mtrnns trained target sequences equal given figure ﬁrst fourth rows target sequence patterns corresponding closed-loop regeneration context states sigma values. sigma values become close zero accounting development deterministic dynamics. sigma values ﬂuctuate accounting development stochastic dynamics. summarily changing metaprior aﬀects sigma values vbp-mtrnn alters deterministic stochastic dynamics. order quantify diﬀerences network characteristics trained under diﬀerent meta-prior settings diﬀerent numbers training sequence patterns average divergence step n-gram kl-divergence tar-gen-mtrnn vbp-mtrnn computed condition. computed target sequences taking average step target sequence pattern regenerated diverged. starting initial step target sequences divergence detected mean square error target generated pattern exceeded threshold n-gram kl-divergence obtained described previously. setting trigram computed tar-gen-mtrnn. purpose sample label sequence generated feeding class-mtrnn sequence pattern generated steps tar-gen-mtrnn. trigram vbp-mtrnn trained diﬀerent condition computed generating sample sequence pattern step length. table shows results analysis. expected target sequences decreases increases. obtained target case decreases value increases much overall target case. suggests top-down intention regenerating learned sequence pattern decays number training sequences increases. tri-gram kl-divergence minimized extreme settings value smaller targets value training condition training target sequences turned generate minimum kl-divergence. interestingly found probability distribution tri-grams generated tar-gen-mtrnn vbp-mtrnn become quite similar condition. good generalization learning achieved condition extracting precise probabilistic structures. taken account average divergence step computed time step length target teaching sequence pattern generated patterns exhibited time step length higher value signify better generalization capability. proper measure exact similarity target teaching patterns generated output patterns. however tri-gram kl-divergence value computed comparing long sequence patterns test-generated tar-gen-mtrnn trained vbp-mtrnn. value represents capability generalization learning shows much vbpmtrnn model able extract probabilistic structure latent teaching target patterns. current paper proposed novel variational bayes predictive coding model learn predict/generate ﬂuctuated temporal patterns exemplars order shed light deterministic probabilistic modeling. model network characterized meta-prior balances cost functions regularization term reconstruction error term. investigated meta-prior parameter along number fig. typical comparison teaching target sequence pattern closed-loop output generated vbp-mtrnn model trained target sequences diﬀerent values capital letters segmenting sequence patterns indicate label sequences classiﬁed class-mtrnn. taught sequence patterns aﬀects model learning performance simulation experiments involved learning sequence patterns exhibit probabilistic transitions prototypical patterns. results summarized follows. meta-prior model learned imitate probabilistic transitions observed taught sequences development deterministic chaos. able repeat taught sequences exactly long periods. however strong top-down intentionality developing initial latent states generalization learning turned poor. larger values model exhibited stochastic dynamics adapted time varying sigma noise sampling. high reconstruction error increased additional randomness generated patterns taught sequences could repeated shorter periods. every value number taught patterns increased generalization learning improved. moreover highest degree generalization learning exhibited middle extreme values. related this means optimize values automatically. speculated values layer could optimized independently feeding back system performance terms generalization using reinforcement learning. challenge left future study. current study involve simulation experiments active inference latent state. active inference done back-propagating error signal context units past window real-time update activation values direction minimizing error. expected smaller value eﬀective active inference becomes stronger causality latent state perceived sensation. otherwise active inference becomes less eﬀective less causality. expectation analogous current results bear task learning extract latent structures observed ﬂuctuating temporal patterns inform inquiry mechanism underlying autism spectrum disorders cruys suggested might caused overly strong top-down prior potentiation minimize prediction error enhance capacities rote learning losing capacity generalize learned pathology typical asd. meta-prior threshold value proposed model naturally reﬂects pathology. unbalance setting might caused dysfuction gaba signaling shown mice experiments conducted chao colleagues future study expected validate hypothesis. another implication current study involves mechanisms underlying spontaneous free generation action sequences seemingly freely selecting action accounted either deterministic chaos unconscious intentionality stochastic dynamics driven sampled noise without strong intentionality could intermediate extremes shown current paper. future studies concern scaling proposed model various real world applications including robot learning inevitably involve dealing ﬂuctuating temporal patterns. include investigation optimization scheme meta-prior level feeding back system performance mentioned previously. time studies explore organizing principles cognitive brains normal abnormal conditions selectively extending model comparing model empirical data. finally study understanding qualitative meaning free action diﬀers generated diﬀering conditions important considering attribution responsibility human action example.", "year": 2017}