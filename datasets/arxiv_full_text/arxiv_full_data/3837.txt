{"title": "Multimodal Attention for Neural Machine Translation", "tag": ["cs.CL", "cs.NE"], "abstract": "The attention mechanism is an important part of the neural machine translation (NMT) where it was reported to produce richer source representation compared to fixed-length encoding sequence-to-sequence models. Recently, the effectiveness of attention has also been explored in the context of image captioning. In this work, we assess the feasibility of a multimodal attention mechanism that simultaneously focus over an image and its natural language description for generating a description in another language. We train several variants of our proposed attention mechanism on the Multi30k multilingual image captioning dataset. We show that a dedicated attention for each modality achieves up to 1.6 points in BLEU and METEOR compared to a textual NMT baseline.", "text": "attention mechanism important part neural machine translation reported produce richer source representation compared ﬁxed-length encoding sequence-to-sequence models. recently effectiveness attention also explored context image captioning. work assess feasibility multimodal attention mechanism simultaneously focus image natural language description generating description another language. train several variants proposed attention mechanism multik multilingual image captioning dataset. show dedicated attention modality achieves points bleu meteor compared textual baseline. introduction dealing multimodal stimuli order perceive surrounding environment understand world natural human beings case artiﬁcial intelligence systems efﬁcient integration multimodal and/or multilingual information still remains challenging task. tractable grounding multiple modalities enable natural language interaction computers. recently deep neural networks achieved state-of-the-art results numerous tasks computer vision natural language processing speech processing input signals monomodal i.e. image/video characters/words/phrases audio signal. successes mind researchers attempt design systems beneﬁt fusion several modalities order reach wider generalization ability. machine translation another ﬁeld purely neural approaches challenge classical phrase-based approach possible formulating translation problem sequence-to-sequence paradigm read produce text discrete steps special recurrent building blocks model long range dependencies. another straightforward application paradigm incorporate dnns task generating natural language descriptions images task commonly referred image captioning. number approaches proposed karpathy fei-fei vinyals achieved state-of-the-art results popular image captioning datasets flickrk mscoco paper propose multimodal architecture able attend multiple input modalities realize extrinsic analysis speciﬁc attention mechanism. furthermore attempt determine optimal attending multiple input modalities context image captioning. although several attempts made incorporate features different languages different tasks improve performance according knowledge attentional neural translation captioning approach trained using auxiliary source modality. architecture introduce achieving multimodal learning either seen enriched convolutional image features auxiliary source representation image captioning system producing image descriptions language supported source descriptions another language modality speciﬁc pathways proposed architecture inspired previously published approaches attention mechanism learned focus different parts input sentence. integration implies careful design underlying blocks many strategies considered order correctly attend multiple input modalities. along line different possibilities also assessed taking account multimodal information decoding process target words. compare proposed architecture single modality baseline systems using recently published multik multilingual image captioning dataset empirical results show obtain better captioning performance terms different evaluation metrics compared baselines. provide analysis descriptions multimodal attention mechanism examine impact multimodality quality generated descriptions attention. end-to-end machine translation using deep learning ﬁrst proposed kalchbrenner blunsom extended sutskever bahdanau achieve competitive results compared state-of-the-art phrase-based method dominant approaches differ representing source sentence sutskever used last hidden state encoder ﬁxed size source sentence representation bahdanau introduced attention mechanism attentional weights assigned recurrent hidden states encoder obtain weighted states instead taking last. idea attention still continues active area research community end-to-end neural image description models basically recurrent language models conditioned different ways image features. image features generally extracted powerful state-of-theart architectures trained large scale image classiﬁcation tasks like imagenet karpathy fei-fei vinyals proposed multimodal differs selection integration image features made multimodal layer fuses image features current word embedding current hidden state common multimodal space. image features experimented authors extracted different cnns namely alexnet karpathy fei-fei takes simpler approach used vanilla incorporates image features ﬁrst time step bias term. finally vinyals trained ensemble lstm image features extracted batch-normalized googlenet presented lstm sentence generator ﬁrst input special start word. different previously cited works applied attention mechanism convolutional image features size extracted makes image context collection feature maps instead single vector. training lstm network jointly learns generate image description selectively attending presented image features. model similar attentional introduced bahdanau except source word annotations produced encoder replaced convolutional features. another attention based model proposed introduced separate attention mechanisms input output attention models applied visual attributes detected using different methods like k-nn neural networks. image representation extracted googlenet context multimodality elliott explore effectiveness conditioning target language model image features last fully-connected layer features source language model using iapr-tc multilingual image captioning dataset. multi-task learning several recent studies literature explored multi-task learning different tasks. dong proposed multi-target translates single source language several target languages. speciﬁcally made single encoder multiple language-speciﬁc decoders embedded attention mechanism. firat extended idea multi-way multilingual translate multiple languages using single attention mechanism shared across languages. luong experimented one-to-many many-to-one many-to-many schemes order quantify mutual beneﬁt several tasks other. bi-directional encoder hidden units reads input sequentially forwards backwards produce sets hidden states based current source word embedding previous hidden state encoder. call atxt textual annotation vector obtained time step concatenating forward backward hidden states encoder convolutional feature maps size extracted so-called resf relu layer resnet- trained imagenet order make dimensionality compatible textual annotation vectors atxt linear transformation applied image features leading visual annotation vectors {aim decoder conditional decoder extended multimodal context equipped multimodal attention mechanism order generate description target language. cgru consists stacked activations name respectively. experimented several ways initializing hidden state decoder found model performs better learned mean textual annotation using feed-forward layer tanh nonlinearity. hidden state initialized time step multimodal attention mechanism computes modality speciﬁc context vectors {ctxt textual/visual annotations {atxt aim}. multimodal context vector obtained applying tanh nonlinearity fusion modality speciﬁc context vectors. experimented different fusion techniques refer concat context multi-way multilingual firat beneﬁt single attention mechanism shared across different language pairs. although reasonable within multilingual task inputs outputs solely based textual representations optimal approach modalities completely different. order verify statement consider different schemes integrating attention mechanism multimodal learning. first shared feed-forward network used produce modalityspeciﬁc attention weights {αtxt call basic approach linear transformations contexts attentional weights shared encoder-independent attention encoder-dependent attention contrast distinct modality means projected differently noted mechanisms depicted still common projection layer applied hidden state ﬁrst gru. order objectively assess impact modalitydependency decoder side also need consider cases shared distinct transformations used projection layer variants respectively referred independent dependent decoder state projection rest paper. propose four variants multimodal attention mechanism terms modality dependency respect encoder decoder. encoder-independent attention dependent decoder state projection might seem unnatural single target task interesting contrastive system. computed modality-speciﬁc context vectors dataset used multik dataset extended version flickrk entities dataset multik extends original flickrk contains images english descriptions independently crowdsourced descriptions german. emphasized provided bilingual descriptions direct translations could considered somewhat comparable. tokenized sentences tokenizer.perl moses removed punctuation lowercased sentences. kept sentence pairs sentence lengths length ratio results ﬁnal training dataset sentences picked frequent german words replaced rest token target side. image part convolutional image features size extracted resf relu layer resnet- trained imagenet. images resized without cropping prior extraction. ﬁnal features used dimension matrices image training testing. trained monomodal baselines namely imgtxt different attentional variants proposed mnmt. baselines exactly architecture presented throughout work single source modality. models word embeddings recurrent layers dimensionality respectively. used adam stochastic gradient descent variant minibatch size weights networks initialized using xavier scheme biases initially regularization applied training cost avoid overﬁtting. classical left right beam-search beam size used sentence generation test time. besides evaluating performance ﬁrst validation split also experimented best source selection strategy image obtain german hypotheses correspond english descriptions pick highest log-likelihood least number tokens. clear table mnmt concat fusion operator improves imgtxt baselines combined modality-dependent attention mechanism results slightly worse baseline regardless multimodal attention type fusion realized operator. difference attributed fact concatenation makes linear layer learns integrate modality-speciﬁc activations multimodal context vector. improvement terms automatic metrics signiﬁcant best source selection strategy compared ﬁrst validation split. reason ﬁrst split contains source sentences much longer detailed target ones words/sentence average source side compared words/sentence target side. best source selection method models compensate discrepancy choosing source sentences different lengths observe completely independent attention mechanism worst performance among concat variants. empirical evidence initial statement single shared attention optimal approach case different input modalities. also notice dependency encoder decoder side established topology performance improves compared baseline mnmt. results obtained mnmt previously conjectured unnatural also conﬁrm this. table present german descriptions generated baseline best performing mnmt model using source description ﬁrst validation split. ﬁrst image mnmt clearly produces richer description provides additional visual information like color type clothing also positioning woman. second case mnmt generates coherent rich description ignores pink laptop mentioned output. lastly third image shows situation wrongly describes color object mnmt correctly although meteor penalizes phenomenon observed references contain advantage attention mechanism ability visualize exactly network pays attention. although visualization textual context trivial apply inferring spatial region original image attended model not. adopt approach proposed upsamples attention weights factor order original image white smoothed regions. compare multimodal attention completely independent mnmt encoder-dependent mnmt figure figure impact sharing attention attention precision completely independent attention. encoder-dependent attention independent decoder state projection. although models achieve comparable attention pattern image regions textual alignment completely disturbed mnmt. failure shared attention contrasts firat attributed representational discrepancy convolutional image features source side textual annotations. moreover number visual annotations single image times higher number textual annotations bias joint attention towards visual side learning stage. paper proposed architecture performs multimodal machine translation multimodal attention. quantitatively qualitatively analyzing different attention schemes demonstrated modality-dependent mnmt outperforms textual baseline bleu/meteor cider-d points. difference even signiﬁcant best source selection strategy reach gain almost bleu/meteor cider-d points. visualize multimodal attention show modality-dependent attention mechanism able learn alignment source words image features case shared attention textual alignment completely disturbed. future work ﬁne-tuning extracted image features learnable convolutional layer inside mnmt and/or extracting image features based source language description interesting paths explore order gain insight multimodality.", "year": 2016}