{"title": "Learning Representations for Counterfactual Inference", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, \"Would this patient have lower blood sugar had she received a different medication?\". We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art.", "text": "fredrik johansson∗ chalmers university technology g¨oteborg sweden shalit∗ david sontag cims york university mercer street york equal contribution observational studies rising importance widespread accumulation data ﬁelds healthcare education employment ecology. consider task answering counterfactual questions would patient lower blood sugar received different medication?. propose algorithmic framework counterfactual inference brings together ideas domain adaptation representation learning. addition theoretical justiﬁcation perform empirical comparison previous approaches causal inference observational data. deep learning algorithm signiﬁcantly outperforms previous state-of-the-art. inferring causal relations fundamental problem sciences commercial applications. problem causal inference often framed terms counterfactual questions would patient lower blood sugar received different medication? would user clicked different color?. paper propose method learn representations suited counterfactual inference show efﬁcacy simulated real world tasks. focus counterfactual questions raised known observational studies. observational studies studies interventions outcomes recorded along appropriate context. example consider electronic health record dataset collected several years patient tests past diagnoses well data relating diabetic status causal question interest existing anti-diabetic medications better given patient. observational studies rising importance widespread accumulation data ﬁelds healthcare education employment ecology. believe machine learning called help make better decisions ﬁelds researchers careful attention ways studies differ classic supervised learning explained section below. work draw connection counterfactual inference domain adaptation. introduce form regularization enforcing similarity distributions representations learned populations different interventions. example representations patients received medication versus received medication reduces variance ﬁtting model distribution applying another. section give several methods learning representations. section show methods approximately minimizes upper bound regret term counterfactual regime. general method outlined figure work commonalities recent work learning fair representations learning representations transfer learning cases learned representation invariance speciﬁc aspects data either identity certain group racial minorities fair representations identity data source domain adaptation case counterfactual learning type intervention enacted population. choice without knowing would feedback possible choices. sometimes referred bandit feedback setup comes diverse areas example off-policy evaluation reinforcement learning learning logged implicit exploration data logged bandit feedback understanding designing complex real world ad-placement systems note contextual bandit robotics applications researcher typically knows method underlying action choice observational studies usually control even full understanding mechanism chooses actions performed feedback reward revealed. instance anti-diabetic medication afﬂuent patients might insensitive price drug less afﬂuent patients could bring account choice. given know beforehand particulars determining choice action question remains learn data course action would better outcomes. bringing together ideas representation learning domain adaptation method offers novel leverage increasing computation power rise large datasets tackle consequential questions causal inference. contributions paper follows. first show formulate problem counterfactual inference domain adaptation problem speciﬁcally covariate shift problem. second derive families representation algorithms counterfactual inference based linear models variable selection based deep learning representations finally show learning representations encourage similarity treated control populations leads better counterfactual inference; contrast many methods attempt create balance re-weighting samples show merit learning balanced representations theoretically theorem empirically experiments across datasets. problem setup potential interventions actions wish consider contexts possible outcomes. example patient interventions interest might different treatments outcomes might indicating blood sugar levels mg/dl. slot webpage interventions might possible inventory slot potential outcomes could {click click}. context potential intervention potential outcome fundamental problem causal inference potential outcome observed given context even give patient medication later other patient exactly state. machine learning type partial feedback often called bandit feedback. model described known rubin-neyman causal model interested case binary action action often known treated case quantity action control. high interest known individualized treatment effect context knowing quantity enables choosing best actions confronted choice example choosing best treatment speciﬁc patient. however fact access outcome actions prevents known. another commonly sought quantity average treatment effect ex∼p] population distribution binary action setting refer observed unobserved outcomes factual outcome counterfactual outcome respectively. principle function ﬁtting model might used estimating important note task differs standard supervised learning. problem follows observed sample consists however calculating requires inferring outcome call empirical factual distribution empirical counterfactual distribution respectively. need equal problem causal inference counterfactual prediction might require inference different distribution samples given. machine learning terms means feature distribution test differs train set. case covariate shift special case doaccomplish low-error prediction usual means error minimization training regularization order enable good generalization error. accomplish second objective penalty encourages counterfactual predictions close nearest observed outcome respective treated control set. finally accomplish third objective minimizing so-called discrepancy distance introduced mansour hypothesis class dependent distance measure tailored domain adaptation. hypothesis space denote discrepancy distance disch. section formal deﬁnition motivation. discrepancy measures maximum mean discrepancy could also used purpose. intuitively representations reduce discrepancy between treated control populations prevent learner using unreliable aspects data trying generalize factual counterfactual domains. example sample almost ever received medication inferring would react medication highly prone error conservative gender feature might warranted. {xi}n denote observed units treatment assignments factual outcomes respectively. assume metric space metric minj∈{...n} s.t. =−ti nearest neighbor among group received opposite treatment unit note nearest neighbor computed once input space change representation objective minimize representations hypotheses hyperparameters control strength imbalance penalties disc discrepancy measure deﬁned hypothesis class class linear functions term disc closed form brought below ti]. complex hypothesis spaces general exact closed form disc main adaptation somewhat similar connection noted sch¨olkopf respect covariate shift context simple causal model. speciﬁcally difference observed sample sample must perform inference lies precisely treatment assignment mechanism example randomized control trial typically independent. contextual bandit setting typically algorithm determines choice action given context observational studies focus work treatment assignment mechanism under control general independent context therefore general counterfactual distribution different factual distribution. balancing counterfactual regression propose perform counterfactual inference amending direct modeling approach taking account fact learned estimator must generalize factual distribution counterfactual distribution. method figure learns representation function learned representation trades three objectives enabling low-error prediction observed outcomes factual representation enabling lowerror prediction unobserved counterfactuals taking account relevant factual outcomes distributions treatment populations similar balanced. na¨ıve obtaining balanced representation features already well balanced i.e. features similar distribution treated control sets. however imbalanced features highly predictive outcome always discarded. middle-ground restrict inﬂuence imbalanced features predicted outcome. build idea learning sparse re-weighting features minimizes bound theorem re-weighting determines inﬂuence feature trading predictive capabilities balance. implement re-weighting diagonal matrix forming representation diag subject simplex constraint achieve sparsity. denote space representations. apply algorithm space linear hypotheses. because hypotheses linear disc function distance weighted population means minimize discrepancy features differ between treatment groups receive smaller weight minimizing overall objective involves trade-off maximizing balance predictive accuracy. minimize using alternating sub-gradient descent. deep neural networks shown successfully learn good representations high-dimensional data many tasks show used counterfactual inference crucially accommodating imbalance penalties. propose modiﬁcation standard feed-forward architecture fully connected layers figure ﬁrst hidden layers used learn representation input output drth layer used calculate discrepancy disch. layers following ﬁrst layers take additional input treatment assignment generate prediction ti]) outcome. fect involve interactions interactions could introduced example feature expansion case neural networks adding non-linear layers concatenation approaches however longer closed form expression disc observe factual outcome sample factual counterfactual sample. note know factual outcomes know coun. repreterfactual outcomes sentation function denote range. denote empirical distribution representations treatment assignments similarly empirical distribution representations counterfactual treatment assignments tn). hypothesis linear functions deﬁnition given hypothesis loss function empirical discrepancy empirical distributions note case variable re-weighting neural nets single linear outcome layer hypothesis space comprises linear functions discrepancy disch expressed closedform. less desirable consequence models cannot capture difference individual treatment efrelative error ridge regression model factual outcomes evaluated counterfactual compared ridge regression unobserved counterfactual outcomes. take account obtained applies even convex e.g. neural net. since bound theorem true representations attempt minimize done algorithm term line bound includes unknown counterfactual outcomes measures well could principle factual counterfactual outcomes together using linear hypothesis representation example dimension representation greater number samples addition exist constants term upper bounded general however cannot directly control magnitude. term line measures discrepancy factual counterfactual distributions representation below show term closely related norm difference means representation control group treated group. representation means treated control close time allows good prediction factuals counterfactuals guaranteed yield structural risk minimizers similar generalization errors factual counterfactual. show term line canevaluated since know upper bounded terms lines term includes empirical data ﬁtting terms |ˆyf ﬁrst simply ﬁtting observed factual outcomes using linear function representation second term form nearest-neighbor regression counterfactual outcomes treated instance similar factual outcome among control similarity measured original space finally term line quantity independent representation measures average distance treated instance nearest control vice-versa scaled lipschitz constants true treated control outcome functions. term small when true outcome functions relatively smooth overlap treated control groups leading small average nearest neighbor distance across groups. well-known much overlap treated control causal inference general difﬁcult since extrapolation treated control vice-versa extreme deﬁned respect hypothesis class loss function therefore useful obtaining generalization bounds involving different distributions. throughout section always denote squared loss. prove following based cortes mohri theorem sample −tn). assume metric space metric potential outcome functions lipschitz continuous constants respectively space functions exlet pected loss distribution maximum expected radius distributions. minβ∈hl i.e. ˆβcf ˆβcf similarly ridge regression solutions factual counterfactual empirical distributions respectively. outputs hypothesis representation factual counterfactual settings respectively. finally minj∈{...n} s.t. =−ti nearest neighbor among group received opposite treatment unit have parametric methods hand attempt concretely model relation context intervention outcome. methods include type regression including linear logistic regression random forests regression trees doubly robust methods combine aspects parametric non-parametric methods typically using propensity score weighted regression especially treatment assignment probability known case off-policy evaluation learning logged bandit data. treatment assignment probability estimated case observational studies efﬁcacy might wane considerably evaluate variants algorithm proposed section focus questions effect imposing imbalance regularization representations? methods fare established methods counterfactual inference? refer variable selection method section balancing linear regression neural network approach balancing neural network. report rmse estimated individual treatment effect denoted absolute error estimated average treatment effect denoted section further following hill report precision estimation heterogeneous effect pehe y)). unlike obtaining good pehe requires accurate estimation factual counterfactual responses counterfactual. standard methods hyperparameter selection including cross-validation unavailable training counterfactual models realworld data samples counterfactual outcome. experiments outcomes simulated access counterfactual samples. avoid ﬁtting parameters test generate multiple repeated experiments different outcome function pick hyperparameters once models based held-out experiments. possible real-world data approach gives indication robustness parameters. upper bound theorem suggests following approach counterfactual regression. first minimize terms functions representation obtained perform ridge regression factual outcomes using representations treatment assignments input. terms bound ensure would good data removing aspects treated control create large discrepancy term example feature much strongly associated treatment assignment outcome might advisable treated control means space. exactly difference means treated control groups weighted respective sizes. consequence minimizing discrepancy linear hypotheses constitutes matching means feature space. counterfactual inference determining causal effects observational studies studied extensively statistics economics epidemiology sociology well machine learning non-parametric methods attempt model relation context intervention outcome. methods include nearest-neighbor matching propensity score matching propensity score re-weighting sprop small weight decay evaluate architectures. bnn-- consists relu representation-only layers single linear output layer bnn-- consists relu representation-only layers relu output layers treatment added single linear output layer figure ihdp data layers hidden units each. news data representation layers units output layers units. nearest neighbor term section improve empirical performance omitted models. neural network models hypothesis representation jointly. include several different linear models comparison including ordinary linear regression doubly robust linear regression also include method variables ﬁrst selected using lasso used ridge regression regularization parameters picked based held sample. estimate propensity scores using logistic regression clip weights news dataset perform logistic regression ﬁrst principal components data. bayesian additive regression trees non-linear regression model used successfully counterfactual inference past compare results bart using implementation provided bayestree rpackage like attempt tune parameters default. finally include standard feed-forward neural network trained hidden layers predict factual outcome based without penalty imbalance. refer nn-. hill introduced semi-simulated dataset based infant health development program ihdp data covariates real randomized experiment studying effect high-quality child care home visits future cognitive test scores. experiment proposed hill uses simulated outcome artiﬁcially introduces imbalance treated control subjects removing subset treated population. total dataset consists subjects represented covariates measuring properties child mother. details hill repeated experiments hyperparameter selection evaluation log-linear response surface implemented setting npci package figure visualization news sets represents single news item radius represents outcome color treatment black dots represent centroids. histogram news introduce dataset simulating opinions media consumer exposed multiple news items. item consumed either mobile device desktop. units different news items represented word counts outcome readers experience intervention represents viewing device desktop mobile assume consumer prefers read certain topics mobile. model this train topic model large documents represent topic distribution news item deﬁne centroids topic space readers opinion news item device determined similarity scaling factor here mobile centroid topic distribution randomly sampled document average topic representation documents. assume assignment news item device biased towards device preferred item. model using softmax function +eκ·z determines strength bias. note implies completely random device assignment. sample news items outcomes according model based topics trained documents times corpus data available algorithms word counts vocabulary words selected union probable words topic. scaling parameters sample realizations evaluation. figure shows visualization outcome device assignments sample documents. note device assignment becomes increasingly random outcome lower away centroids. results ihdp news experiments presented table table respectively. that general non-linear methods perform better terms individual prediction further proposed balancing neural network bnn-- performs best datasets terms estimating pehe competitive average treatment effect ate. particularly noteworthy comparison network without balance penalty nn-. results indicate proposed regularization help avoid overﬁtting representation factual outcome. figure plots performance bnn-- various imbalance penalties valley region fact don’t experience loss performance smaller values show penalizing imbalance representation desired effect. news lasso ridge perform equally well again although time qualitatively different results select variables. interestingly bnn-- lasso ridge perform better news standard neural network nn-. performance bart news likely hurt dimensionality dataset could improve hyperparameter tuning. machine learning becoming major tool researchers policy makers across different ﬁelds healthcare economics causal inference becomes crucial issue practice machine learning. paper focus counterfactual inference widely applicable special case causal inference. cast counterfactual inference type domain adaptation problem derive novel learning representations suited problem. models rely novel type regularization criteria learning balanced representations representations similar distributions among treated untreated populations. show trading balancing criterion standard data ﬁtting regularization terms practically theoretically prudent. references austin peter introduction propensity score methods reducing effects confounding observational studies. multivariate behavioral research ben-david shai blitzer john crammer koby pereira fernando analysis representations domain adaptation. advances neural information processing systems bengio yoshua courville aaron vincent pierre. representation learning review perspectives. pattern analysis machine intelligence ieee transactions beygelzimer alina langford john lihong reyzin schapire robert contextual bandit alarxiv gorithms supervised learning guarantees. preprint arxiv. bottou l´eon peters jonas quinonero-candela joaquin charles denis chickering portugaly elon dipankar simard patrice snelson counterfactual reasoning learning systems example computational advertising. journal machine learning research gani yaroslav ustinova evgeniya ajakan hana germain pascal larochelle hugo laviolette franc¸ois marchand mario lempitsky victor. domainadversarial training neural networks. arxiv preprint arxiv. kang joseph schafer joseph demystifying double robustness comparison alternative strategies estimating population mean incomplete data. statistical science weiss jeremy kuusisto finn boyd kendrick page david machine learning treatment assignment improving individualized risk attribution. american medical informatics association annual symposium sch¨olkopf janzing peters sgouritsa zhang mooij causal anticausal learning. proceedings international conference machine learning york omnipress. swaminathan adith joachims thorsten. batch learning logged bandit feedback counterfactual journal machine learning rerisk minimization. search tian alizadeh gentles andrew tibshirani robert. simple method estimating interactions treatment large number covariates. journal american statistical association restate theorem prove theorem sample addition deﬁne ti)y tiy. given representation function tn). assume metric space metric potential outcome functions lipschitz continuous constants respectively ktc. space linear functions expected loss distribution outputs hypothesis representation factual counterfactual settings respectively. finally minj∈{...n} s.t. =−ti nearest neighbor among group received opposite treatment unit have result implicit proof theorem cortes mohri case linear hypotheses ﬁxed representation cortes mohri state result case domain adaptation case factual distribution so-called source domain counterfactual distribution target domain. theorem using notation assumptions theorem work cortes mohri assume reproducing kernel hilbert space universal kernel consider role representation since rkhs hypothesis space much stronger linear space often reasonable assume second term bound small. however cannot make assumption therefore wish explicitly bound term minh∈hl using fact control representation lemma addition tiy. deﬁne assume functions lipschitz continuous constants respectively ktc. deﬁne minj∈{...n} s.t. =−ti nearest neighbor among group received opposite treatment unit", "year": 2016}