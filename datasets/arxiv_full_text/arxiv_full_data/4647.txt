{"title": "Multi-objective Reinforcement Learning with Continuous Pareto Frontier  Approximation Supplementary Material", "tag": ["cs.AI", "cs.LG"], "abstract": "This document contains supplementary material for the paper \"Multi-objective Reinforcement Learning with Continuous Pareto Frontier Approximation\", published at the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15). The paper is about learning a continuous approximation of the Pareto frontier in Multi-Objective Markov Decision Problems (MOMDPs). We propose a policy-based approach that exploits gradient information to generate solutions close to the Pareto ones. Differently from previous policy-gradient multi-objective algorithms, where n optimization routines are use to have n solutions, our approach performs a single gradient-ascent run that at each step generates an improved continuous approximation of the Pareto frontier. The idea is to exploit a gradient-based approach to optimize the parameters of a function that defines a manifold in the policy parameter space so that the corresponding image in the objective space gets as close as possible to the Pareto frontier. Besides deriving how to compute and estimate such gradient, we will also discuss the non-trivial issue of defining a metric to assess the quality of the candidate Pareto frontiers. Finally, the properties of the proposed approach are empirically evaluated on two interesting MOMDPs.", "text": "document contains supplementary material paper multi–objective reinforcement learning continuous pareto frontier approximation published twenty–ninth aaai conference artiﬁcial intelligence paper learning continuous approximation pareto frontier multi-objective markov decision problems propose policy-based approach exploits gradient information generate solutions close pareto ones. diﬀerently previous policy-gradient multi-objective algorithms optimization routines solutions approach performs single gradient-ascent step generates improved continuous approximation pareto frontier. idea exploit gradient-based approach optimize parameters function deﬁnes manifold policy parameter space corresponding image objective space gets close possible pareto frontier. besides deriving compute estimate gradient also discuss non-trivial issue deﬁning metric assess quality candidate pareto frontiers. finally properties proposed approach empirically evaluated interesting momdps. paper multi–objective reinforcement learning continuous pareto frontier approximation published twenty–ninth aaai conference artiﬁcial intelligence supplement follows structure main article. section report complete proofs additional details. proof. equation performance measure follows directly deﬁnition volume integral manifold deﬁnition function composition. following give detailed derivation i–th component gradient. dθjdtφρ described paper addition present normalization takes account area approximate pareto frontier. area manifold deﬁned volume integral unitary function ﬁrst case study discrete-time linear-quadratic gaussian regulator multidimensional continuous state action spaces problem deﬁned following dynamics easily extended account multi-conﬂicting objectives. particular problem minimizing distance origin w.r.t. i-th axis taken account considering cost action axes since maximization i-th objective requires null action axes objectives conﬂicting. reward formulation violates positiveness matrix change reward adding ξ-perturbation case using algorithm able learn good approximation pareto– frontier terms accuracy covering. using utopia point reference point frontier learned collapses point knee front. behaviour occurs using using antiutopia point reference point solutions returned dominated loss function able learn parametrization figure shows iterations learning process using starting figure shows indicator function iterations. possible notice converges constant value. similar behaviour algorithm returns almost line extreme points order reduce frontier length. using ﬁrst normalization behaviour true pareto frontier second normalization instead critical problem diﬀerent magnitudo loss function area frontier therefore diﬃcult properly choose solution could ignore constraint -objectives case diﬀerence magnutudo able suitable solutions tend concentrate center frontier order minimize distance utopia point area frontier. normalization able correct behaviour result bump frontier slightly increasing area. eﬀect seems indipendent normalization used parameters expected without normalization algorithm tried produce frontier wide possible order increase distance antiutopia point. behaviour dominated solutions learning process converge. using ﬁrst normalization able correct behaviour algorithm still able cover frontier completely using smaller frontier still wide contained dominated solutions higher smaller ones. second normalization instead ineﬀective. again diﬀerent magnitudo loss function area frontier makes choice critical. proved best among three returning good approximation pareto frontier terms accuracy covering without using normalization. figure shows pareto frontier parameter space. water reservoir modelled momdp continuous state variable representing water volume stored reservoir continuous action controls water release state-transition model depends also stochastic reservoir inﬂow conﬂicting objectives. complete description problem reader refer like original work discount factor objectives initial state drawn ﬁnite set. however diﬀerent settings used learning evaluation. learning phase episodes steps used evaluation phase exploits episodes steps. figure results water reservoir domain. using utopia-based loss function trend convergent frontier returned comparable ones obtained state-of-the-art algorithms. figure reports ﬁnal frontier obtained diﬀerent algorithms. approximation obtained algorithm comparable results however approach able produce continuous frontier approximation. section want examine deeply tuning mixed metric parameters order provide reader better insights correct metric. pmga performance indeed strongly depends indicator used thereby setting critical. precise mixed metric obtained best approximate pareto–frontiers experiments includes trade-oﬀ accuracy covering expressed parameters. penalization term i.e. monotonic function decreases increases. previous sections proposed take advantage expansive behavior antiutopia–based indicator accuracy optimality–based indicator section going study performance metric varying proposing simple tuning process. idea initial value increase approximate frontier contains dominated solutions figure shows diﬀerent approximate frontiers obtained diﬀerent starting indicator behaves like approximate frontier still contains dominated solutions. increasing dominated solutions disappear. finally approximate frontier becomes shorter pareto–optimal solutions discarded meaning increased much.", "year": 2014}