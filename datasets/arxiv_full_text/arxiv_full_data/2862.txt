{"title": "Efficient Algorithms for Outlier-Robust Regression", "tag": ["cs.LG", "cs.AI", "cs.DS", "stat.ML"], "abstract": "We give the first polynomial-time algorithm for performing linear or polynomial regression resilient to adversarial corruptions in both examples and labels.  Given a sufficiently large (polynomial-size) training set drawn i.i.d. from distribution D and subsequently corrupted on some fraction of points, our algorithm outputs a linear function whose squared error is close to the squared error of the best-fitting linear function with respect to D, assuming that the marginal distribution of D over the input space is \\emph{certifiably hypercontractive}. This natural property is satisfied by many well-studied distributions such as Gaussian, strongly log-concave distributions and, uniform distribution on the hypercube among others. We also give a simple statistical lower bound showing that some distributional assumption is necessary to succeed in this setting.  These results are the first of their kind and were not known to be even information-theoretically possible prior to our work.  Our approach is based on the sum-of-squares (SoS) method and is inspired by the recent applications of the method for parameter recovery problems in unsupervised learning. Our algorithm can be seen as a natural convex relaxation of the following conceptually simple non-convex optimization problem: find a linear function and a large subset of the input corrupted sample such that the least squares loss of the function over the subset is minimized over all possible large subsets.", "text": "subsequently corrupted fraction points algorithm outputs linear function whose squared error close squared error best-ﬁtting linear function respect natural property satisﬁed many well-studied distributions gaussian strongly log-concave distributions uniform distribution hypercube among others. also give simple statistical lower bound showing distributional assumption necessary succeed setting. approach based sum-of-squares method inspired recent applications method parameter recovery problems unsupervised learning. algorithm seen natural convex relaxation following conceptually simple nonconvex optimization problem linear function large subset input corrupted sample least squares loss function subset minimized possible large subsets. austin klivanscs.utexas.edu †princeton university institute advanced study. kotharics.princeton.edu. ‡university california angeles raghumcs.ucla.edu inﬂuential recent line work focused developing robust learning algorithms– algorithms succeed data contaminated adversarially corrupted outliers. important achievements eﬃcient algorithms robust clustering estimation moments unsupervised learning eﬃcient learning halfspaces respect malicious nasty noise classiﬁcation. paper continue line work give ﬁrst eﬃcient algorithms performing give simple examples showing unlike classical regression achieving non-trivial guarantee robust regression information-theoretically impossible without making assumpwell-studied class hypercontractive distributions. many natural distributions gaussians strongly log-concave distributions product distributions hypercube bounded marginals fall category. complement algorithmic results also show class hypercontractive distributions bounds loss linear function output algorithm optimal dependence fraction corruptions multiplicative constants. elaborate notion certiﬁability later time being note many well-studied distributions including gaussians aﬃne transformations isotropic strongly log-concave distributions uniform distribution boolean hypercube generally product distributions bounded domains known satisfy condition ﬁxed constant. theorem distribution marginal distribution certiﬁably -hypercontractive. minℓ errd polynomial bit-complexity. universal constant exists algorithm run-time poly given polynomial-size η-corrupted training outputs linear function probability least covariance thus particular holds poorly conditioned covariances. alternately give similar guarantee regression condition number covariance bounded without need hypercontractivity show application learning boolean functions nasty noise. work immediate applications learning boolean functions nasty noise model learner presented corollary class boolean functions variables every exists polynomial degree …x∼d assume constant hypercontractive polynomials degree learned nasty noise model time output hypothesis …x∼d section give outline theorem high level approach resembles several recent works starting pioneering work sum-of-squares method designing eﬃcient algorithms learning problems. important conceptual diﬀerence however previous works focused parameter recovery problems. problems paradigm involves showing there’s simple proof small sample uniquely identiﬁes underlying hidden parameters small error. contrast setting samples uniquely determine good hypothesis multiple hypotheses low-error true distribution. approach thus involves establishing there’s simple proof low-error hypotheses inferred observed sample low-error true distribution output good solution approach crucially rely convexity empirical loss function. natural strategy brute-force search subsets size perform leastsquares regression obtain linear function empirical loss then output minimal empirical loss subsets since subset size proper subset uncorrupted sample empirical loss clearly small. however priori there’s nothing rule existence another subset size optimal regression hypothesis loss smaller large error part ineﬃcient algorithm polynomial optimization. coming back question eﬃcient algorithms approach appear hopeless general since simultaneously non-convex quadratic optimization problem. high-level able around intractability observing proof robust certiﬁability lemma simple precise technical sense. simplicity allows convert certiﬁability proof eﬃcient algorithm principled manner. describe connection ﬁrst translate naive idea algorithm polynomial optimization problem. standard generalization error) setting i’th sample uncorrupted. arguments solutions program satisfy bound stated theorem unfortunately quadratic optimization problem np-hard general. ready describe idea allows essentially turn hopelessly ineﬃcient algorithm eﬃcient one. exploits close relationship simplicity proof robust certiﬁability success canonical semi-deﬁnite relaxation priori appear made harder. computing distribution solutions easier computing single solution even describing distribution solutions appears require exponential resources general. however utilizing convexity square loss show access ﬁrst moments enough recover good solution. sum-of-squares proof remains valid even replace pseudo-distribution large enough degree. roughly speaking degree proof measures simplicity proof words facts simple proofs holds distributions also pseudo-distributions. thus important remaining steps show inequality convexity argument lowdegree proof. establish claims relying standard tools versions cauchy-schwarz hölder’s inequalities. literature grappling outliers context regression vast attempt survey here. many heuristics developed modifying ordinary least squares objective intent minimizing eﬀect outliers another active line research concerned parameter recovery label training assumed generative model form noise parameter unknown weight vector example recovery properties lasso related algorithms context intensely studied challenging noise models recent work balakrishnan singh studies sparse recovery gaussian generative setting huber’s ε-contamination model similar formally weaker noise model consider here. common robust regression refer scenario labels allowed corrupted adversarially references therein) noise obeys special structure labels subject small adversarial corruption technical standpoint discussed work follows recent paradigm converting certiﬁability proofs algorithms. previous applications machine learning focused various parameter-recovery problems unsupervised learnings. work closely related recent works robust unsupervised learning following notations conventions throughout distribution function deﬁne errd y)]. vector abuse notation write errd …∼d]. real-valued random variable integer kxkk …/k. algorithmic results wide class distributions include gaussian distributions others log-concave product distributions. next deﬁne properties need marginal distribution examples satisfy. many natural distribution families satisfy certiﬁable hypercontractivity reasonably growing functions instance gaussian distributions uniform distribution boolean hypercube aﬃne transformations isotropic distributions satisfying poincaré inequality also certiﬁably hypercontractive. particular includes strongly log-concave distributions. certiﬁable hypercontractivity also satisﬁes natural closure properties simple operations aﬃne transformations taking bounded weight mixtures taking products. refer reader detailed overview certiﬁable hypercontractivity referred certiﬁable subgaussianity. conceptual core results following robust certiﬁability result nice distributions regression hypothesis inferred large enough ε-corrupted sample low-error uncorrupted distribution. error incur depends squared loss best ﬁtting regression hypothesis particular obtain consistency statistical sense error incurred regression hypothesis vanish even realizable case when true uncorrupted distribution there’s linear function correctly computes labels. section show make assumption distribution indeed inherent achieving consistency adversarial corruptions provably impossible without making assumptions. following subsection show assuming moments underlying uncorrupted distribution bounded guarantee consistency even presence adversarial outliers. certiﬁability statements independently interpretable purpose robust regression might helpful keep mind corresponds uniform distribution large enough sample unknown uncorrupted distribution corresponds uniform distribution sample serves certiﬁcate. shows every level-∞-pseudo distribution satisﬁes thus actual probability distribution. deﬁne pseudo-expectation function respect pseudodistribution denoted particular moment tensor entry corresponding pseudo-expectation monomials degree degree-ℓ moment tensors probability distribution convex set. similarly degree-ℓ moment tensors degree pseudodistributions also convex. algorithmic utility pseudo-distributions fact eﬃcient separation oracle convex degree-ℓ moment tensors actual probability distribution there’s separation oracle running time convex degree-ℓ moment tensors level-ℓ pseudodistributions. fact together equivalence weak separation optimization allows eﬃciently optimize pseudo-distributions —this algorithm referred sum-of-squares algorithm. form {kxk following fact consequence fact fact exists o-time algorithm that given explicitly bounded satisﬁable system polynomial constraints variables outputs level-ℓ pseudo-distribution satisﬁes approximately. write denote uncorrupted input sample size drawn according bound bit-complexity linear functions write optimum least squares error linear function complexity recall complexity linear function number bits required write uniform distribution sample denote marginal distribution note algorithm direct access write optimum least squares error linear function complexity bounded bit-complexity assumption linear functions mainly obtain generalization bounds linear regression often used even regression without corruptions. note specializing case gives theorem analyze algorithm prove theorem analysis broken modular steps bounding optimization error bounding generalization error. concretely break analysis following exploit robust setting around issue essentially truncating large values since distribution truncated values close statistical distance actual distribution. remark proof lemma follows standard generalization arguments part. lemma every distribution exists distribution bounded absolutely bounded support v/η. proof. bounded support further markov’s inequality probability event conditioned least completes proof. observe corrupted sample thought corrupted sample since bounded allows hoeﬀding bound concentration show empirical expectation converges expectation proof theorem i.i.d. sample size uniform distribution …dk. without loss generality using fact assume bounded using hoeﬀding’s inequality log/ηε distributions collection polynomial inequalities sos-imply another polynomial inequality pseudo-distribution appropriately high degree satisﬁes inequalities also satisﬁes further algorithm allows compute pseudo-distributions satisfying show version robust certiﬁability lemma proof; viewing inequality polynomial inequality variables inequality proof starting polynomial inequalities puη. thus property pseud-densities also need following lemma stating uniform distribution suﬃciently large i.i.d samples hypercontractive distribution also satisfy hypercontractivity. allows argue lemma -certiﬁably hypercontractive distribution i.i.d. sample size ω)k/). then probability least draw uniform distribution -certiﬁably hypercontractive. solution optimization progam ˜µk/] follows coptsos errbd. next argue errbd close errd suﬃciently big. random variable note errbd average independent draws theorem arbitrary distribution positive real ratio maximum minimum eigenvalue covariance matrix marginal minimum complexity bounded minimizer upper bound fraction corruptions. previous section algorithm pseudo-distributions satisfying polynomial inequalities encode hypotheses robust certiﬁability lemma error polynomial. analysis algorithm. plan subsection proofs essentially analogous ones presented previous subsection. split analysis bounding optimization generalization errors before. optsos error output algorithm optimal hypothesis proofs lemmas entirely analogous ones presented previous section. main technical ingredient version robust certiﬁability result. since technical novelty subsection present statement proof result omit proofs. exhibit statistical lower bounds achieved outlier-robust regression. particular simple examples illustrate strong separations regression regression presence contamination also demonstrate several assumptions make necessary. algorithm given η-corrupted samples distributions ﬁnds hypothesis vector proof. suppose algorithm above. suﬃciently large chosen later. distribution random variable samples follows sample uniformly random probability output probability output note follows elementary calculations errd errd′ follows universal constant errd′) finally distribution random variable sampled follows sample uniformly random probability output probability output probability output proof sketch. chosen later. brevity uniform distribution distribution random variable sampled follows probability sample uniformly random output probability output /δ/; probability output −/δ/. ﬁrst claim d′ktvk this consider coupling obtained choosing both. then probability xi−hℓ′ further whenever happens statistical distance therefore d′ktv combining errd′) note minℓ∗ errd holds well. finally d′ktv setting appropriately. therefore could η-corruptions could error better them. claim", "year": 2018}