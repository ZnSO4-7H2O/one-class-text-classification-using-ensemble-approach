{"title": "Regularized Orthogonal Tensor Decompositions for Multi-Relational  Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "Multi-relational learning has received lots of attention from researchers in various research communities. Most existing methods either suffer from superlinear per-iteration cost, or are sensitive to the given ranks. To address both issues, we propose a scalable core tensor trace norm Regularized Orthogonal Iteration Decomposition (ROID) method for full or incomplete tensor analytics, which can be generalized as a graph Laplacian regularized version by using auxiliary information or a sparse higher-order orthogonal iteration (SHOOI) version. We first induce the equivalence relation of the Schatten p-norm (0<p<\\infty) of a low multi-linear rank tensor and its core tensor. Then we achieve a much smaller matrix trace norm minimization problem. Finally, we develop two efficient augmented Lagrange multiplier algorithms to solve our problems with convergence guarantees. Extensive experiments using both real and synthetic datasets, even though with only a few observations, verified both the efficiency and effectiveness of our methods.", "text": "abstract—multi-relational learning received lots attention researchers various research communities. existing methods either suffer superlinear per-iteration cost sensitive given ranks. address issues propose scalable core tensor trace norm regularized orthogonal iteration decomposition method full incomplete tensor analytics generalized graph laplacian regularized version using auxiliary information sparse higher-order orthogonal iteration version. ﬁrst induce equivalence relation schatten p-norm multi-linear rank tensor core tensor. achieve much smaller matrix trace norm minimization problem. finally develop efﬁcient augmented lagrange multiplier algorithms solve problems convergence guarantees. extensive experiments using real synthetic datasets even though observations veriﬁed efﬁciency effectiveness methods. tant high value hidden relational data also many applications various domains social networks semantic bioinformatics linked data cloud class relational learning methods focus mostly problem modeling single relation type relational learning latent attributes models relations objects resulting intrinsic latent attributes objects. reality relational data typically involve multiple types relations objects attributes similar. example social networks relationships individuals personal familial professional. type relational data learning often referred multi-relational learning needs model large-scale sparse relational databases efﬁciently people usually make semantic web’s formalism represent relational data relations modeled triples form relation either denotes relationship entries entity attribute value. considering multiple types relationships natural stacking matrices observed relationships sparse three-order tensor. fig. shows illustration modeling method. recent years tensors become ubiquitous multi-channel images videos become popular ability discover complex interesting latent structures correlations data recently growing interest tensor decomposition popular tool multi-relational prediction problems example bader proposed three-way component decomposition model analyzing intrinsically asymmetric relationships. addition nickel incorporated collective learning tensor factorization designed account inherent structure relational data. popular tensor factorizations tucker decomposition candecomp/parafac decomposition address incomplete tensor estimation weighted alternating leastsquares methods proposed. however methods require ability reliably estimate rank involved tensor recently rank tensor recovery problem intensively studied. ﬁrst extended trace norm schatten -norm regularization partially observed multilinear rank tensor recovery. tensor recovery problem transformed convex combination trace norm minimization matrix unfolding along mode. recently al.’s subsequent paper proposed three efﬁcient algorithms solve multi-linear rank tensor completion problem. similar algorithms exact analogue deﬁnition matrix rank rank tensor deﬁned smallest number rank-one tensors generate sum. however straightforward determine rank tensor. fact problem np-hard fortunately multi-linear rank tensor easy compute consists ranks mode-n unfoldings. introduce often used tensor decomposition models problems. acar presented weighted candecomp/parafac decomposition model sparse third-order tensors positive integer denotes non-negative indicator tensor size incomplete tensor wijk tijk observed wijk otherwise ri×r ri×r ri×r referred factor matrices combination vectors rank-one components also found addition theoretical developments guarantee reconstruction rank tensor partial measurements solving trace norm minimization reasonable conditions however tensor trace norm minimization problems solved iteratively involve multiple singular value decompositions iteration. therefore existing algorithms suffer high computational cost making impractical realworld applications address issues mentioned above i.e. robustness given ranks computational efﬁciency propose scalable core tensor trace norm regularized orthogonal iteration decomposition method full incomplete tensor analytics. ﬁrst induce equivalence relation schatten p-norm multi-linear rank tensor core tensor. trace norm core tensor replace whole tensor achieve much smaller scale matrix trace norm minimization problem. particular roid method generalized graph laplacian regularized version using auxiliary information relationships sparse higher-order orthogonal iteration version. finally develop efﬁcient augmented lagrange multiplier algorithms problems. moreover theoretically analyze convergence property algorithms. experimental results real-world datasets veriﬁed efﬁciency effectiveness methods. rest paper organized follows. review preliminaries related work section section propose novel core tensor trace norm regularized tensor decomposition models develop efﬁcient algorithms extend algorithm solve shooi problem section provide theoretical analysis algorithms section report experimental results section section conclude paper point potential extensions future work. notations problem formulations third-order tensor denoted calligraphic letter e.g. ri×i×i entries denoted xiii fibers higher-order analogue matrix rows columns. mode-n ﬁbers third-order tensor respectively. mode-n unfolding also known matricization third-order tensor ri×i×i denoted rin×πj=nij arranges mode-n ﬁbers columns resulting matrix moden ﬁber becomes index modes become column indices. tensor element mapped matrix element core tensor given multi-linear rank since decomposition rank general much smaller sense storage tucker decomposition form signiﬁcantly smaller original tensor. moreover unlike rank tensor multi-linear rank clearly computable. factor matrices tucker decomposition constrained orthogonal classical decomposition methods referred higher-order singular value decomposition higher-order orthogonal iteration latter leads estimation best rank- approximations truncation hosvd achieve good rank- approximation general best possible hence particularly interested extending hooi method sparse problems. addition several extensions tensor decomposition models developed tensor estimation problems however methods suitable rank value needs given shown wtucker models usually sensitive given ranks least-squares formulations poor performance data high rank core tensor trace norm regularized orthogonal decomposition models achieve three smaller-scale matrix trace norm minimization problems. section develop efﬁcient algorithms solving problems. ri×r ri×r ri×r column-wise orthonormal matrices thought principal components mode. entries core tensor rr×r×r show level interaction different components. recommend matrix rank estimation approach recently developed compute good values multi-linear rank involved tensor. give relatively large integers satisfying proof theorem given appendix since trace norm tightest convex surrogate rank function mainly consider trace norm case paper. according equivalence relation trace norm multi-linear rank tensor core tensor tensor completion model formulated following form clear core tensor size much smaller size whole tensor i.e. therefore core tensor trace norm regularized orthogonal tensor decomposition models alleviate computational burden much larger unfoldings models besides core tensor trace norm term promotes multi-linear rank tensor decompositions enhances robustness multi-linear rank selection traditional tensor decomposition methods usually sensitive given multi-linear rank αn’s pre-speciﬁed weights indices observed entries. proposed three efﬁcient algorithms solve addition similar convex tensor completion algorithms tomioka suzuki proposed latent trace norm minimization model recently shown tensor trace norm minimization models mentioned substantially suboptimal however order involved tensor three models often perform better balanced matrix model indeed unfolding shares entries thus cannot optimized independently. therefore must apply variable splitting introduce multiple additional equal-sized variables unfoldings moreover existing algorithms involve multiple svds iteration suffer high computational cost assumed size tensor core tensor trace norm regularized tensor decomposition address poor scalability existing multi-linear rank tensor recovery algorithms present scalable clear smaller size matrices need perform svd. therefore shrinkage operator signiﬁcantly lower computational complexity nπj=ndj computational complexity nπj=nij iteration. hence algorithm much lower complexity unlike hooi algorithm propose orthogonal iteration scheme update matrices optimization moreover conventional hooi seen special case estimate matrices optimal solution respect given following theorem. please appendix proof theorem moreover propose orthogonal iteration scheme solving alternating orthogonal procrustes scheme solve rank- problem. analogous theorem ﬁrst state minimization problem formulated follows section propose efﬁcient method augmented lagrange multipliers solve roid problem extend proposed algorithm solve variant standard alternating direction method multipliers received much attention recently tremendous demand large-scale machine learning applications similar proposed problem difﬁcult solve interdependent tensor trace norm term kgk∗. therefore ﬁrst introduce three much smaller auxiliary variables rdn×πj=ndj reformulate following equivalent form detailed proof theorem given appendix according theorem orthogonal iteration scheme proposed successively solve ﬁxing variables. imagine matrices ﬁxed optimization problem merely quadratic function unknown matrix consisting orthonormal columns primal residual maxkf ρkk×t×t×t dual residual iteration require primal dual residuals -iteration small satisfy optimal conditions following efﬁcient strategy update iteratively discuss time complexity roid groid methods. solving main running time algorithms taken performing svds multiplications. time complexity performing nin). time complexity multiplication operators thus total time complexity roid groid algorithms essentially gauss-seidel-type schemes admm update strategy jacobi version easily implemented well suited parallel computing. section evaluate effectiveness efﬁciency roid method multi-linear rank tensor completion synthetic data multi-relational learning real-world data network data three popular multi-relational data sets. except large-scale multi-relational prediction experiments performed intel core running windows main memory. following generated multi-linear rank thirdorder tensors ri×i×i used ground truth data. generated tensor data follows tucker model i.e. rr×r×r core tensor whose entries generated independent identically distributed numbers uniform distribution entries rin×r random samples drawn uniform distribution range construction multi-linear rank third-order tensors equals almost surely. also apply algorithm solve shooi problem note halrtc latent shooi roid apply admm algorithm solve problems. roid method regularization parameter ⌊.r⌋. relative square error recovered tensor algorithms deﬁned evaluate robustness roid method respect multi-linear rank parameter changes ﬁrst conduct experiments synthetic tensors size illustrate results tensor decomposition methods sampling ratio rank parameter roid shooi wtucker chosen average results independent runs shown fig. number given rank increases tensor decomposition methods gradually increase especially shooi. speciﬁcally shooi gives extremely accurate solutions exact multi-linear rank tensor completion problems. however number given rank increases shooi increases dramatically. contrast roid settings consistently outperforms wtucker terms performs robust shooi. conﬁrms roid model core tensor trace norm regularization reasonable provide good estimation observed tensor even though observations. note convex algorithms halrtc latent tensors size respectively. also report recovery results wtucker halrtc latent roid method different fractions observed entries tensor multi-linear ranks synthetic tensors size fig. sampling ratio varies increment multi-linear ranks chosen increment observe settings roid method consistently outperforms methods terms rse. halrtc able yield accurate solutions using adequate large sampling ratio however fraction observed entries underlying tensor multilinear ranks high performance halrtc poor. main reason wtucker roid method multiple structured methods similar matrix case need observations exactly recover nth-order multilinear rank tensor high probability observations required recovering true tensor convex tensor trace norm minimization methods halrtc latent stated moreover conduct experiments evaluate robustness roid method respect regularization parameter report results latent roid method synthetic tensors size fig. tuned grid sampling ratio note solid dashed lines denote results roid ⌊.r⌋ respectively. clear increases latent roid method exact multi-linear rank give much better performance tensor completion problems. suggested setting gives accurate solutions noiseless problem. practical applications parameter roid following noisy problems. moreover roid method settings signiﬁcantly outperforms latent terms rse. running time roid method methods varying sizes thirdorder tensors shown fig. running time wtcuker latent halrtc dramatically grows increase tensor size whereas running time shooi roid methods increase slightly. addition wtcuker latent halrtc could yield experimental results largest synthetic tensor completion problems sizes part examine roid graph regularized methods real-world network data sets youtube data youtube currently popular video sharing site allows users interact various forms contacts subscriptions sharing favorite videos etc. total data contains users users sharing information types includes -dimension interactions contact network co-contact network co-subscription network co-subscribed network favorite network. additional information data found experiments machine -core intel xeon .ghz memory. address multi-relational prediction problem tensor completion problem. graph regularized weighted decomposition graph regularized weighted tucker decomposition roid groid methods tensor rank multi-linear rank regularization parameter halrtc roid groid methods weights tolerance value methods ﬁxed methods could yield experimental results whole youtube data ﬁrst chose users interactions subset consists users types interactions i.e. randomly select entries training remainder testing data. report average prediction accuracy memory. roid method times faster wtcuker times faster halrtc times faster latent size input tensors shows roid method good scalability address large-scale problems. notice latent converges slowly consider following experiments. moreover table summarizes time complexities major computations related weighted tensor decomposition algorithms convex trace norm minimization algorithms assumed sizes tensor core tensor respectively. table although wtucker computational complexity similar roid method much slower practice roid polakribiere nonlinear conjugate gradient algorithms timeconsuming line search scheme evaluate performances method full tensor decomposition compare roid method multi-linear rank approximation method hooi noisy tensors i.e. denotes noise factor denotes standard gaussian random noise. fig. illustrates results lmlra hooi roid noisy tensors different noise factors. observe roid performs robust stable noise methods. moreover also report running time tensors different sizes fig. roid method times faster methods. addition lmlra hooi could generate experimental results largest problem size memory. average running time independent runs figs. number users gradually increased. moreover evaluate robustness roid method respect given multi-linear ranks shown fig. given ranks gwtucker gwcp roid groid methods chosen range observe three trace norm minimization algorithms i.e. roid groid methods halrtc signiﬁcantly outperform gwtucker gwcp terms prediction accuracy. moreover roid groid remarkably faster methods robust gwtucker gwcp respect given multi-linear ranks. running time roid groid increase slightly number users increases. contrast running time methods increases dramatically could complete computation within hours largest problem sizes users. shows roid groid good scalability address large-scale problems. groid performs signiﬁcantly better methods terms prediction accuracy auxiliary information. finally examine well roid method performs three popular multi-relational datasets previously used kemp link prediction including kinship nations umls data sets. kinship data consists kinship relationships among members alyawarra tribe central australia data contains tribe members types kinship relations formﬁtting three-order tensor size nations data consists international relations among different countries world data contains countries types relations three-order tensor size umls data collected uniﬁed medical language system mccray data includes semantic network concepts binary predicates three-order tensor size compare roid method several state-of-theart approaches including nonparametric bayesian fig. clustering results four relations nations date set. black squares indicate existing relation countries. gray squares indicate missing data. mehtod hidden variable discovery method rescal halrtc three data sets. since wtucker shooi yield similar results three data sets report results wcp. then area precision-recall curve evaluation metric test relation prediction performance order obtain comparable results rescal follow experimental requirements preform -fold cross validation. roid method multi-linear rank kinship umls data sets nations data regularization parameter paper proposed scalable roid method graph regularized version full incomplete tensor analytics multi-relational learning. first induced equivalence relation schatten p-norm multi-linear rank tensor core tensor. presented novel orthogonal tensor decomposition model core tensor trace norm regularization. also introduced regularization version using graph laplacians induced relationships sparse higher-order orthogonal iteration version. finally developed efﬁcient admm algorithms solve problems. moreover analyzed theoretically local convergence algorithms. convincing experimental results real-world problems veriﬁed efﬁciency effectiveness methods especially observations. moreover roid method extended various higher-order tensor recovery completion problems higher-order robust principal component analysis robust tensor completion. future work interested exploring ways regularize model auxiliary information semantic information contained social network unitary invariant property norms stated theorem superiority schatten-p quasi-norm trace norm would interesting research direction future investigate general core tensor schatten quasi-norm regularization. roid method halrtc consistently outperform four methods. reason halrtc roid method efﬁciently explore impact different relations improve accuracy relation prediction. rescal halrtc roid method similar results also report results four methods table clear roid method consistently performs better three methods terms recovery accuracy. moreover demonstrate relation-based clustering capabilities roid method nations data set. apply k-means clustering method matrix illustrate results four types relationships fig. similar results obtained. moreover present comparison running time related methods three multi-relational data sets. shown rescal much faster well irm. therefore report running time rescal halrtc roid method three data sets different ranks listed table clear roid method much faster rescal suitable largescale multi-relational data. rescal usually scale worse regard ranks roid method. words increase given tensor ranks running time rescal dramatically grows whereas roid method changes slightly. also evaluate robustness roid method parameters given tensor ranks regularization parameter three real-world data sets shown fig. roid method robust parameter variations especially kinship umls date sets. note three rank parameters lesser given multi-linear rank corresponding size complement lemma sequence bounded k+−x gk+−gk bolzano-weierstrass theorem bounded sequence must convergent subsequence limit point denoted limj=∞ moreover lies compact thus accumulation point nickel tresp kriegel three-way model collective learning multi-relational data proc. int. conf. mach. learn. domingos statistical predicate invention proc. int. conf. mach. learn. siam review vol. papadimitriou qian multivis content-based social network exploration multiway visual analysis proc. siam int. conf. data min. block coordinate descent method regularized multiconvex optimization applications nonnegative tensor factorization completion siam imaging sci. vol. boyd parikh peleato eckstein distributed optimization statistical learning alternating direction method multipliers found. trends mach. learn. vol. haeffele young vidal structured low-rank matrix factorization optimality algorithm applications image processing proc. int. conf. mach. learn. currently post-doctoral research fellow department computer science engineering chinese university hong kong. prior that post-doctoral research associate department electrical computer engineering duke university durham usa. current research interests include machine learning data mining james cheng assistant professor department computer science engineering chinese university hong kong hong kong. current research interests include distributed computing systems large-scale network analysis temporal networks data. hong cheng associate professor department systems engineering engineering management chinese university hong kong hong kong. primary research interests include data mining machine learning database systems. supplementary material give detailed admm algorithms solving core tensor trace norm regularized full tensor decomposition problem sparse tensor hooi problem addition also provide theoretical analysis relationship detailed complexity analysis algorithms. supplementary material ﬁrst give details design efﬁcient admm algorithm outlined algorithm solving core tensor trace norm regularized full tensor decomposition problem similar also introduce three much smaller auxiliary variables rdn×πj=ndj reformulate following equivalent form also give details design efﬁcient admm algorithm outlined algorithm solve sparse tensor hooi problem whose partial augmented lagrangian function given follows", "year": 2015}