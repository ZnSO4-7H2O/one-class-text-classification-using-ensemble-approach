{"title": "Multigrid Neural Architectures", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "We propose a multigrid extension of convolutional neural networks (CNNs). Rather than manipulating representations living on a single spatial grid, our network layers operate across scale space, on a pyramid of grids. They consume multigrid inputs and produce multigrid outputs; convolutional filters themselves have both within-scale and cross-scale extent. This aspect is distinct from simple multiscale designs, which only process the input at different scales. Viewed in terms of information flow, a multigrid network passes messages across a spatial pyramid. As a consequence, receptive field size grows exponentially with depth, facilitating rapid integration of context. Most critically, multigrid structure enables networks to learn internal attention and dynamic routing mechanisms, and use them to accomplish tasks on which modern CNNs fail.  Experiments demonstrate wide-ranging performance advantages of multigrid. On CIFAR and ImageNet classification tasks, flipping from a single grid to multigrid within the standard CNN paradigm improves accuracy, while being compute and parameter efficient. Multigrid is independent of other architectural choices; we show synergy in combination with residual connections. Multigrid yields dramatic improvement on a synthetic semantic segmentation dataset. Most strikingly, relatively shallow multigrid networks can learn to directly perform spatial transformation tasks, where, in contrast, current CNNs fail. Together, our results suggest that continuous evolution of features on a multigrid pyramid is a more powerful alternative to existing CNN designs on a flat grid.", "text": "encompassing entire input. work following mold includes lenet breakthrough alexnet many architectural enhancements followed googlenet residual networks like coupled large datasets compute power pipeline drives state-of-the-art vision systems. however sufﬁciency design speak optimality. revolution performance blinded community investigating whether unfolding computation standard manner best choice. fact shortcomings typical pipeline conﬂates abstraction scale. early layers cannot coarser scales later layers them. tasks requiring ﬁne-scale output semantic segmentation necessitates specialized designs reintegrating spatial information ﬁne-to-coarse processing within standard opposition near universal principle efﬁcient algorithm design coarse-to-ﬁne processing. ﬁrst layer standard consists many ﬁlters independently looking tiny almost meaningless regions image. would reasonable system observe coarse-scale context deciding probe details? communication inefﬁcient. neuron’s receptive ﬁeld determined units input layer could propagate signal standard cnns implement slow propagation scheme diffusing information across single grid rate proportional convolutional ﬁlter size. reason extremely deep networks appear necessary; many layers needed counteract inefﬁcient signal propagation. points summarized inherent deﬁciencies representation computation communication. multigrid architecture endows cnns additional structural capacity order dissolve deﬁciencies. explicitly multiscale pushing choices scale-space representation training process. propose multigrid extension convolutional neural networks rather manipulating representations living single spatial grid network layers operate across scale space pyramid grids. consume multigrid inputs produce multigrid outputs; convolutional ﬁlters within-scale cross-scale extent. aspect distinct simple multiscale designs process input different scales. viewed terms information multigrid network passes messages across spatial pyramid. consequence receptive ﬁeld size grows exponentially depth facilitating rapid integration context. critically multigrid structure enables networks learn internal attention dynamic routing mechanisms accomplish tasks modern cnns fail. experiments demonstrate wide-ranging performance advantages multigrid. cifar imagenet classiﬁcation tasks ﬂipping single grid multigrid within standard paradigm improves accuracy compute parameter efﬁcient. multigrid independent architectural choices; show synergy combination residual connections. multigrid yields dramatic improvement synthetic semantic segmentation dataset. strikingly relatively shallow multigrid networks learn directly perform spatial transformation tasks where contrast current cnns fail. together results suggest continuous evolution features multigrid pyramid powerful alternative existing designs grid. since fukushima’s neocognitron basic architectural design convolutional neural networks persisted form similar shown figure processing begins high resolution input ﬁlters examine small local pieces. stacking many layers combination occasional pooling subsampling receptive ﬁelds slowly grow depth eventually figure multigrid networks. standard architectures conﬂate scale abstraction filters limited receptive ﬁeld propagate information slowly across spatial grid necessitating deep networks fully integrate contextual cues. bottom multigrid networks convolutional ﬁlters across scale space thereby providing communication mechanism coarse grids. reduces required depth mixing distant contextual cues logarithmic spatial separation. additionally network free disentangle scale depth every layer learn several scale-speciﬁc ﬁlter sets choosing represent pyramid level. traditional pooling subsampling similarly multigrid reducing size entire pyramid. computation occurs parallel scales; every layer process coarse representations. section also explores coarse-to-ﬁne variants transition processing coarse pyramid processing full pyramid network deepens. pyramids provide efﬁcient computational model uniﬁed one. viewing network evolving representation living pyramid combine previous task-speciﬁc architectures. classiﬁcation attach output coarsest pyramid level; segmentation attach output ﬁnest. multigrid structure facilitates cross-scale information exchange thereby destroying long-established notion receptive ﬁeld. neurons receptive ﬁeld equivalent entire input; ﬁeld size grows exponentially depth progressive multigrid networks begins full input. quick communication pathways exist throughout network enable capabilities. speciﬁcally demonstrate multigrid cnns trained pure end-to-end fashion learn attend route information. emergent behavior dynamically emulate routing circuits articulated olshausen construct synthetic task standard cnns completely fail learn multigrid cnns accomplish ease. here attentional capacity key. section reviews recent architectural innovations ignore scale-space routing capacity focusing instead aspects like depth. multigrid section details complements work. section measures performance improvements multigrid classiﬁcation tasks synthetic semantic segmentation tasks. multigrid boosts baseline residual cnns. synthetic spatial transformation task multigrid boost; required residual networks alone possess attentional capacity. section discusses implications. wake alexnet exploration cnns across computer vision distilled rules thumb design. small spatial ﬁlters many successive layers make efﬁcient parameter allocation feature channels increase spatial resolution reduction deeper networks better long means overcoming vanishing gradients engineered training process network width matters desire adapt image classiﬁcation cnns complex output tasks semantic segmentation catalyzed development ad-hoc architectural additions restoring spatial resolution. include skip connections upsampling hypercolumns autoencoder-like hourglass u-shaped networks reduce re-expand spatial grids latter group methods reﬂects classic intuition connecting bottom-up top-down signals. work differs earlier models virtual decoupling pyramid level feature abstraction. representations scales evolve depth network. dynamics also separates past multiscale work consider embedded continual cross-scale communication. figure multigrid layers. left implement multigrid convolutional layer using readily available components. grid input pyramid rescale neighboring grids spatial resolution concatenate feature channels. convolution resulting single grid approximates scale-space convolution original pyramid. downscaling max-pooling upscaling nearest-neighbor interpolation. right building block residual connectivity multigrid networks. prior designs varied ﬁlter size choice instead vary grid resolution crucial. applying multiple ﬁlter sets different spatial size single grid could emulate multigrid computation. however exponential grid size scaling. emulating larger ﬁlters quickly becomes cost prohibitive terms parameters computation; impractical implement style inception modules dilated atrous convolution related fully capture pooling interpolation aspects multigrid operator. recent efforts improve computational efﬁciency cnns though numerous list full often attack parameter precision reduction. concurrent work extends exploration include cascades explicit routing decisions allow partial evaluation network. unlike work crossscale connections bi-directional explore routing different sense internal transport information solving particular task. focus coarse-to-ﬁne aspect efﬁciency borrowing multigrid concepts image segmentation figure conveys intention wire cross-scale connections network structure lowest level. think multigrid standard every grid transformed pyramid. every convolutional ﬁlter extends spatially within grids across grids multiple scales within pyramid corresponding feature channels pyramidal slice across scalespace preceding layer contributes response particular corresponding neuron next. scheme enables neuron transmit signal pyramid coarse grids back again. even signals jumping pyramid level consecutive layer network exploit structure quickly route information spatial locations ﬁnest grid. communication time logarithmic difference information routing capability particularly dramatic light recent trend towards stacking many layers small convolutional ﬁlters standard cnns virtually guarantees either deep networks manually-added pooling unpooling stages needed propagate information across pixel grid. multigrid allows faster propagation minimal additional design complexity. moreover unlike ﬁxed pooling/unpooling stages multigrid allows network learn data-dependent routing strategies. instead directly implementing multigrid convolution depicted figure implement close alternative easily built standard components existing deep learning frameworks. figure illustrates multigrid convolutional layer drop-in replacement standard convolutional layers converting network grids pyramids. multigrid convolution choose learn independent ﬁlter sets scale within layer alternatively parameters scales learn shared ﬁlters. learn independent sets assuming affords network chance squeeze maximum performance scale-speciﬁc representations. channel counts grids need match particular manner. figure independent. comparisons either channel count ﬁnest grid match baseline models calibrate channel counts total parameters similar baselines. section reveals quite good performance achievable halving channel count coarser grid thus multigrid adds minimal computational overhead; coarse grids cheap. given recent widespread residual networks consider multigrid extension them. right side figure shows multigrid analogue original residual unit convolution acts jointly across multigrid pyramid batch normalization relu apply separately grid. extensions also possible figure network diagrams. baseline multigrid cnns cifar- classiﬁcation well residual progressive versions. vary depth duplicating layers within colored block. bottom semantic segmentation spatial transformation architectures u-net progressive multigrid alternative also consider u-mg baseline replaces u-net grids pyramids still shrinks re-expands pyramids. within mg-conv layer max-pooling subsampling acts lateral communication mechanism coarse grids. similarly upsampling facilitates lateral communication coarse grids. rather locating operations ﬁxed stages pipeline actually inserting everywhere. however action lateral combining different-scale grids layer rather grids different scales layers consecutive depth. figure shows additional max-pooling subsampling stages acting depth-wise entire pyramids also consider pipelines shrink pyramids. rather simply attach output particular grid success strategy motivates rethinking role pooling cnns. instead explicit summarization employed select stages multigrid yields view pooling implicit communication pervades network. computation pattern underlying multigrid cnn’s forward pass analogy multiscale multigrid scheme maire view eigensolver linear diffusion process multiscale pyramid. view multigrid nonlinear process similar pyramid communication structure. extending analogy port progressive multigrid computation scheme setting. rather starting directly full pyramid progressive multigrid spend several layers processing coarse grid. following additional layers processing small pyramid remainder network commits work full pyramid. multigrid progressive multigrid experiments ﬁne-scale input grid original image simply feed downsampled versions independent initial convolution layers. outputs initial layers form multigrid pyramid processed coherently rest network. figure diagrams variety network architectures evaluate. classiﬁcation take network midifferences simonyan zisserman baseline. abuse notation reuse name. -layer version vgg- consists sections convolutional layers each pooling subsampling between. ﬁnal softmax layer produces class predictions convolutional layers ﬁlters. instantiate variants different depth changing number layers section residual baselines follow layout residual units within section. recall residual unit contains convolutional layers. multigrid residual multigrid networks starting respectively simply grids pyramids convolution multigrid convolution. progressive variants expand ﬁrst section order gradually work computation complete pyramid. even depth increases signiﬁcant portion layers progressive networks avoid processing full pyramid. diagrammed multigrid networks match ﬁne-grid feature count baselines hence capacity parameters. classiﬁcation also consider smaller multigrid variants fewer channels grid order closer parameter count baseline networks. semantic segmentation spatial transformation tasks detailed next section networks produce per-pixel output. baseline employ unet design ronneberger again convolutional ﬁlters except layers immediately following upsampling; ﬁlters following examine progressive multigrid alternatives continue operate multiscale pyramid. unlike classiﬁcation setting reduce pyramid resolution. networks drop coarse grids towards pipelines sole reason grids communicate output. experiments focus systematic exploration evaluation architectural design space goal quantifying relative beneﬁts multigrid synergistic combination residual connections. table task cifar- image classiﬁcation. data. whiten enlarge images random patches training center patch test examples. training test images. training. batch size weight decay rate iterations epoch epochs. learning rate exponentially decays whereas r-mg rpmg decays rate every epochs. results. table adding multigrid capacmethod vgg- vgg- vgg- vgg- mg-sm- mg-sm- mg-sm- mg-sm- pmg-sm- pmg-sm- pmg-sm- pmg- pmg- pmg- res- res- res- res- r-mg-sm- r-mg-sm- r-mg-sm- r-mg-sm- r-mg- r-mg- r-mg- r-mg- r-pmg-sm- r-pmg-sm- r-pmg-sm- r-pmg- r-pmg- r-pmg- consistently improves basic well residual cnns progressive multigrid nets similar depth achieve better comparable accuracy reduced expense quantiﬁed terms parameters ﬂoating point operations semantic segmentation generate synthetic data semantic segmentation randomly scaling rotating translating pasting mnist digits onto canvas limiting digit overlap task decompose result per-pixel digit class labels. addition networks already mentioned consider single grid baseline mirrors removes grids ﬁnest. also consider u-mg changes resolution like u-net works pyramids. training. batch size weight decay rate iterations epoch epochs. learning rate schedules non-residual residual networks manner done cifar-. results. table figure show dramatic performance advantages progressive multigrid network. report scores mean intersection union true output regions well mean per-pixel multiclass labeling accuracy. results strong indicator continuous operation multigrid pyramid replace networks pass resolution bottleneck. jaderberg engineer system inverting spatial distortions combining neural network estimating parameters transformation dedicated unit applying inverse. split problem components appears necessary using standard cnns; cannot learn task end-to-end. progressive multigrid networks able learn tasks. generate synthetic spatial transformation dataset manner semantic segmentation digit image additional afﬁne transformation uniformly random sheering angle training segmentation except iterations epoch. results. table figure show networks except r-pmg fail learn task. figure reveals reason u-net cannot learn translation. makes sense single grid methods propagating information across grid would require deeper tested. u-net’s failure seems stem confusion pooling/upsampling. appears paste subregions target digit output correct arrangement. figure reveals that unlike figure semantic segmentation synthetic mnist. compare u-net residual progressive multigrid networks synthetic task disentangling randomly distorted overlapped mnist digits. figure speciﬁes -layer u-net variants used here. -layer single-grid baseline operates ﬁnest scale. analogous residual versions layers deep. thus performs similarly deeper single-grid residual network r-pmg bests all. figure spatial transformations synthetic mnist. left multigrid cnns able learn take distorted translated digit input produce centered original. right u-net learn undo rotation afﬁne transformations translation. u-net’s failure stems inability properly attend route information sub-region input. select designs train scratch imagenet r-mg- multigrid analogue resnet- ﬁnest grid matching resnet additional grids half quarter spatial resolution pooling resolution reduction mirror resnet- r-mg diagram figure r-pmg--seg similar semantic segmentation network adapted imagenet maps input onto grids maintained throughout network withpooling stages. increase feature count depth attach classiﬁer ﬁnal coarsest grid. training uses batch size weight decay iterations epoch epochs starting learning rate decay rate every epochs. table compares performance resnets wide residual networks observe grid. compare r-mg- double-wide residual network wrn- match performance fewer parameters require drastically fewer flops. r-pmg--seg outperforms resnet-. design used segmentation great classiﬁcation lending support idea single unifying architecture evolves multigrid representation. proposed multigrid extension cnns yields improved accuracy classiﬁcation semantic segmentation tasks. progressive multigrid variants open pathway towards optimizing cnns efﬁciency. multigrid appears unique extending range tasks cnns accomplish integrating network structure capacity learn routing attentional mechanisms. abilities suggest multigrid could replace adhoc designs current architectures. speculative note multigrid neural networks might also broader implications neuroscience. feedforward computation sequence multigrid pyramids looks similar combined bottom-up/top-down processing across single larger structure neurons embedded computational substrate according spatial grid rather depth processing chain. figure attention maps mnist spatial transformers. produce attention maps sweep occluder input image measure sensitivity output green locations. u-net fails learn translation attends approximately regions regardless input. r-pmg instead exhibit attention translates input. figure detailed visualization attention generation process.", "year": 2016}