{"title": "Compacting Neural Network Classifiers via Dropout Training", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We introduce dropout compaction, a novel method for training feed-forward neural networks which realizes the performance gains of training a large model with dropout regularization, yet extracts a compact neural network for run-time efficiency. In the proposed method, we introduce a sparsity-inducing prior on the per unit dropout retention probability so that the optimizer can effectively prune hidden units during training. By changing the prior hyperparameters, we can control the size of the resulting network. We performed a systematic comparison of dropout compaction and competing methods on several real-world speech recognition tasks and found that dropout compaction achieved comparable accuracy with fewer than 50% of the hidden units, translating to a 2.5x speedup in run-time.", "text": "introduce dropout compaction novel method training feed-forward neural networks realizes performance gains training large model dropout regularization extracts compact neural network run-time efﬁciency. proposed method introduce sparsity-inducing prior unit dropout retention probability optimizer effectively prune hidden units training. changing prior hyperparameters control size resulting network. performed systematic comparison dropout compaction competing methods several realworld speech recognition tasks found dropout compaction achieved comparable accuracy fewer hidden units translating speedup run-time. dropout well-known regularization method used successfully feed-forward neural networks. training large models strong regularization dropout provides state-of-the-art performance numerous tasks method inserts dropout layer stochastically zeroes individual activations previous layer probability training. test time stochastic layers deterministically approximated rescaling output previous layer account stochastic dropout training. empirically dropout effective applied large models would otherwise overﬁt although training large models usually issue anymore multipaper propose novel form dropout training provides performance beneﬁts dropout training large model producing compact model deployment. method inspired annealed dropout training retention probability slowly annealed multiple epochs. method introduce independent retention probability parameter hidden unit bimodal prior distribution sharply peaked encourage posterior retention probability converge either similarly annealed dropout units converge never dropped however unlike annealed dropout units converge always dropped. units removed network without accuracy degradation. annealing schedule compaction rate resulting network controlled changing hyperparameters prior distribution. general model compaction well investigated. conventionally problem addressed sparsifying weight matrices neural network. example introduced pruning criterion based second-order derivative objective function. regularization also widely used obtain sparse weight matrices. however methods achieve weight-level sparsity. relative efﬁciency dense matrix multiplication compared sparse matrix multiplication approaches improve test time efﬁciency without degrading accuracy. contrast proposed method directly reduces dimension weight matrices. another approach singular value decomposition used obtain approximate low-rank representations weight matrices directly reduce dimensionality internal representations however typically implemented linear bottleneck layer introduces additional parameters. inefﬁciency requires additional compression degrades performance. example compress number parameters half compacted model would restrict internal dimensionality knowledge distillation also used transfer knowledge larger models small model however requires separate optimization steps makes difﬁcult directly apply existing denotes element-wise multiplication activation function layer denotes j-th element vector function mask vectors independent draws bernoulli distribution parameterized retention probability hyperparameters def= l}}. conventional dropout training retention probabilities belonging layer tied test time tractable marginalize mask vectors instead crude approximation applied. average network outputs possible mask vectors replaced output network average mask i.e. vector replaced expectation vector optimization conﬁgurations. introduced additional multiplicative parameters output hidden layers regularize parameters penalty. method conceptually similar dropout compaction methods introduce regularization reduce dimensionality internal representations. dropout compaction seen extension method replacing regularization dropout-based regularization. similar svd-based compaction technique several approaches achieving faster evaluation neural networks assuming certain structure weight matrices. example introduces toeplitz weight matrices building block neural networks. deﬁnes structured matrix discrete cosine transform enabling fast matrix multiplication fast fourier transform algorithm. methods successfully reduce computational cost neural network prediction; however since methods restrict parameter space prior training method restrict ﬂexibility neural networks. proposed method attempts jointly optimize structure neural network parameters. optimization algorithm determine effective low-dimensional representation hidden layers. several approaches reducing numerical precision speed training evaluation proposed approaches complemental method since proposed method changes dimensionality hidden layers structure network stays same. particular also apply basic weight quantization technique experiments automatic speech recognition tasks. remainder paper follows section introduce probabilistic formulation dropout training cast ensemble learning method. then derive method optimizing dropout retention probabilities section section present experimental results. section conclude paper suggest future extensions. section describe conventional dropout training feed-forward neural networks. hereafter denote input vectors x··· target labels k··· parameters neural network def= rd)×d−) rd)| number layers number output units layer. approximated weight function computed feed forward passes stochastic binary mask expectation scaling. partial derivatives respect retention probability parameters expressed follows standard approach uses unbiased monte carlo estimate true gradient vector. however gradients respect retention probability parameters exhibit high variance. reduce estimator’s variance using control variates closely following exploit fact random mini-batch training data indices mask vectors randomly drawn element before approximate make computation tractable. optimal closed-form solution. however reasonable choice choice obtain interpretable update rule training example contributes update retention probabilities predictive distribution changes applying random dropout mask i.e. order encourage posterior place mass compact models prior distribution strongly prefers required. powered beta distribution prior distribution leveraging retention probabilities unit pruning criterion propose untie retention probability parameters bimodal prior them. then seek optimize joint log-likelihood next subsection compute parameter gradients. second subsection describe control variates used reduce variance gradients. next describe prior used encourage posterior retention probabilities converge finally summarize algorithm. prior probability distribution retention probability parameters. objective function respect weight parameters unchanged follow conventional dropout parameter updates partial derivative respect retention probability u-th unit layer similarly prediction conventional dropout method computing denominator weight function intractable summation binary mask vectors. therefore employ approximation i.e. denominator computed using hence weight function approximated setting prior probability density goes inﬁnity approaches respectively. thus encourage optimization result converge either exponent introduced order control relative importance prior distribution. setting sufﬁciently large ensure retention probabilities converge either finally stochastic updates retention probabilities control variates summarized algorithm experiments alternate optimization neural network parameters retention probabilities speciﬁcally updates computed algorithm applied epoch conventional dropout training. algorithm shows overall structure. epoch remove hidden units retention probability smaller threshold without degrading performance. therefore already beneﬁt compaction training phase. first pilot study evaluated dropout compaction mnist handwritten digit classiﬁcation task. experiments demonstrate efﬁcacy method widely used publicly available dataset. next conalgorithm alternating updates parameters retention probabilities data training data initial values result updated dropout probabilities neural experiments compared proposed method dropout annealing method svd-based compaction. dropout annealing chosen proposed method also varies dropout retention probabilities optimizing weights. svd-based compaction method chosen since technique widely used applicable many tasks. mnist dataset consists training test images handwritten digits. simplicity focus permutation invariant version task without data augmentation. used layer neural networks rectiﬁed linear units parameters dnns initialized random values uniform distribution adaptive width computed glorot’s formula standard split training data images training images hyperparameter optimization. learning rates momentum prior parameters selected based development accuracy. minibatch size stochastic gradient descent evaluated networks various numbers hidden units dropout compaction training produce approximately compression. compacted models trained applying hidden-to-hidden weight matrices best performing neural networks conﬁguration sizes bottleneck layer achieve compression terms number parameters hidden-to-hidden matrix. decomposition svd-compacted networks ﬁne-tuned compensate approximation error. based manual tuning development accuracies learning rate momentum respectively. regularization found effective baseline system. optimal regularization constants dropout annealed dropout dropout compaction. dropout annealing method increased retention probability ﬁrst epochs. figure table show results proposed method methods comparison. ﬁgure plots ﬁrst show differences average cross-entropy loss computed test plots second show differences classiﬁcation error rate. green lines markers denote proposed method blue lines markers denote compared method i.e. baseline feed forward conventional dropout dropout annealing compacted dnns. error bars plots represent standard deviations estimated trials different random initialization. table shows results small large networks. numbers weights table average numbers trials. terms test-set cross-entropy loss dropout compaction performs consistently better methods comparison. application automatic speech recognition main interest performance terms crossentropy loss decisive used estimator label probability behavior terms error rate differs. increasing model size error rate eventually saturates point methods. however small networks dropout compaction also clearly outperforms methods terms error rate. case small networks relevant apply dropout compaction training small models. here \"small\" must understood relative complexity task. neural example real-world application requires large-scale deployment neural networks applied dropout compaction large-vocabulary continuous speech recognition performed experiments three tasks voicesearchlarge contains voice queries voicesearchsmall subset voicesearchlarge. genericfarfield contains farﬁeld speech signal obtained applying frontend processing seven channels microphone array. used voicesearchsmall conducting preliminary experiments ﬁnding optimal hyperparameters used tasks. input vectors dnns extracted dimensional mel-ﬁlterbank energies frames every acoustic model processed preceding middle frame following frames stacked vector thus feature extraction number training examples respectively. used random training data validation used newbob-performance based learning rate control speciﬁcally halved learning rate improvement last epoch less threshold. analogy cross validation-based model selection used validation optimizing retention probabilities. baseline model size designed total latency certain threshold. number hidden units layer determined number hidden layers sigmoid activation function used nonlinearity hidden layers. following standard dnn/hmm-hybrid modeling approach output targets clustered hmm-states obtained triphone clustering. used clustered states voicesearchsmall voicesearchlarge genericfarfield tasks respectively. fast evaluation quantized values weight matrices used integer operations feed-forward computations. networks sufﬁciently small achieving latency speech recognition service. therefore experiments focus cases enabling larger network within given ﬁxed budget achieving faster evaluation compressing current network. figure averages test-set loss number test-set errors mnist experiments functions numbers weights. column shows results dropout compaction compared conventional feed-forward network conventional dropout dropout annealing singular value decomposition respectively. green lines marker blue lines marker denote results proposed compared method respectively. error bars show works pretrained greedy layer-wise training method dropout compaction annealing methods retention probabilities kept ﬁxed during pretraining. annealed dropout schedule ﬁrst epochs. mnist experiments ﬁxed hyperparameters obtain approximately compression setting number non-silence frames data set. conventional dropout improve performance baseline system experiments annealed dropout reference system dropout-based training. small size neural networks relative training corpus size. fig. shows histograms retention probabilities functions numbers epochs voicesearchsmall task. designed retention probabilities rapidly diffused initial value converged ﬁrst epochs. observe signiﬁcant differences regard pruning rate convergence speed different hidden layers. compression rates hidden layers around layers. fig. shows evolution frame error rate voicesearchsmall task. observed annealed dropout started overﬁtting later epochs retention probability annealed hand dropout compaction methods exhibited performance gain probabilities converged completely. suggests that similar svd-based compaction proposed method requires ﬁne-tuning structure ﬁxed even though ﬁne-tune compaction processes smoothly connected proposed method. might reason optimal prior parameters dropout compaction selected development implied retention probabilities converge already epochs whereas optimal parameters dropout annealing yield deterministic model epochs. table shows word error rates development evaluation sets. standard crossentropy models ﬁne-tuned according sequencediscriminative criterion case boosted maximum mutual information criterion dropout compaction applied cross-entropy phase optimization pruned structure used bmmi training phase. results table show dropout compaction models starting larger structure yielded best error rates cases except bmmi result genericfarfield task always performed better baseline. differences especially large comparison cross-entropy trained models. reason might dropout compaction method determines structure based cross-entropy-based compression hidden units yields roughly compression hidden-to-hidden weight matrices compression input output layer weight matrices. leads roughly speedup feed-forward computation. regarding case dropout compaction achieved better results baseline network tasks. further gains annealed dropout retained although dropout compaction models roughly times smaller. compared svd-based model compaction proposed method performed better almost cases. similar comparison case relative advantage dropout compaction became smaller additional bmmi training step. therefore adapting proposed method compatible sequence discriminative training bmmi promising future research direction. paper introduced dropout compaction novel method training neural networks converge smaller network starting larger network. time method retains performance gains dropout regularization. method based estimation unit-wise dropout probabilities sparsity-inducing prior. real-world speech recognition tasks demonstrated dropout compaction provides comparable accuracy even ﬁnal network fewer original parameters. since computational costs scale proportionally number parameters neural network results speed evaluation. future work want study whether results dropout compaction improved using sophisticated methods estimating expectation mask patterns. further application proposed method convolutional recurrent neural networks promising direction.", "year": 2016}