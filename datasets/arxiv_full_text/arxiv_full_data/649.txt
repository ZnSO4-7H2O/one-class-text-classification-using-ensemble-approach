{"title": "Empirical Explorations in Training Networks with Discrete Activations", "tag": ["cs.NE", "cs.AI", "cs.CV"], "abstract": "We present extensive experiments training and testing hidden units in deep networks that emit only a predefined, static, number of discretized values. These units provide benefits in real-world deployment in systems in which memory and/or computation may be limited. Additionally, they are particularly well suited for use in large recurrent network models that require the maintenance of large amounts of internal state in memory. Surprisingly, we find that despite reducing the number of values that can be represented in the output activations from $2^{32}-2^{64}$ to between 64 and 256, there is little to no degradation in network performance across a variety of different settings. We investigate simple classification and regression tasks, as well as memorization and compression problems. We compare the results with more standard activations, such as tanh and relu. Unlike previous discretization studies which often concentrate only on binary units, we examine the effects of varying the number of allowed activation levels. Compared to existing approaches for discretization, the approach presented here is both conceptually and programatically simple, has no stochastic component, and allows the training, testing, and usage phases to be treated in exactly the same manner.", "text": "present extensive experiments training testing hidden units deep networks emit predeﬁned static number discretized values. units provide beneﬁts real-world deployment systems memory and/or computation limited. additionally particularly well suited large recurrent network models require maintenance large amounts internal state memory. surprisingly despite reducing number values represented output activations little degradation network performance across variety diﬀerent settings. investigate simple classiﬁcation regression tasks well memorization compression problems. compare results standard activations tanh relu. unlike previous discretization studies often concentrate binary units examine eﬀects varying number allowed activation levels. compared existing approaches discretization approach presented conceptually programatically simple stochastic component allows training testing usage phases treated exactly manner. almost popular neural network training algorithms rely gradient-based learning. reliable computation gradients useful hidden unit activations continuous smooth. activation large plateaus discontinuities gradient-based learning becomes diﬃcult even impossible. large part motivated move neural networks based synthetic discrete neurons hard thresholds units sigmoid non-linearity well back-propagation algorithm compute gradients interestingly despite sigmoid non-linearity smooth derivatives even early neural network research observed often training units clustered activations around extrema thereby potentially under-utilizing full representational capacity. recently renewed interest using discrete outputs activation hidden units weights trained network. though perhaps closer regards biologically plausible spiking neurons much research discretization outputs weights stemmed pragmatic concerns. units provide beneﬁts deployment systems memory and/or computation limited cell-phones specialized hardware designed forward propagation large networks. additionally particularly well suited large recurrent network models require maintenance large amounts internal state memory. date commonly research focused binarizing networks weights activations focus paper discretization outputs hidden units. unlike previous studies empirically examine happens units allowed output discrete values. trade-oﬀs higher cardinality alphabets number employed hidden units examined experiments. last decade rectiﬁed linear units well number non-linearities shown possible train networks strictly adhere smoothness constraints often considered necessary even changes learning algorithms. purely discrete outputs desired however binary units number additional steps normally taken evolutionary strategies used high level many methods employ stochastic binary unit inject noise forward pass sample units associated eﬀect network’s outputs. estimation possible calculate gradient pass network. interesting beneﬁt method generative networks. generative networks units employed stochastically forward propagation phase beneﬁcial generating multiple responses single input. example work extend show learning stochastic units even necessary used within larger deterministic network. existing binarization methods liken process dropout regularization eﬀects. instead randomly setting activations zero computing gradients binarize activations weights. section describe sudo unit sigmoid-u nderlying discrete-output units. simplest implementation sudo unit instantiated pre-deﬁned output discretization levels output value bounded range e.g. either figure output scaled discretized output computed follows practical note replicate results study sudo units within standard neural network packages potential unexpected behavior avoided. tanh function approximated small large negative values input function shown yields extra discretization level. noted subtle problem popular language/package. many simple tweaks used avoid issue change eﬀectiveness procedure clipping outputs multiplying tanh number smaller than close figure sudo units shown levels. regions largest slope underlying sigmoid/tanh function discretization levels change fastest. requirement constrain number discretization levels power though practice preferred memory eﬃciency. eﬀects increasing number discretization levels demonstrate increased many diﬃculties training discretized units mitigated simple mechanisms perform well. allows currently popular training algorithms modiﬁcation. course naively backpropagating errors sudo-units quickly problems activations well suited calculating derivatives discontinuous largely characterized piece-wise constant functions. contrast standard sigmoid tanh activations suﬀer problem. relu units partially share diﬃculties; however unbounded positive coupled fact single discontinuity gradient based methods continue work. order gradient based methods sudo units need deﬁne suitable derivative. simply derivative underlying function discretizing. case shown above derivative simply derivative tanh tanh. forward pass network output unit discretized output. backward pass simply ignore discretization derivatives underlying function. could work training? important facets activation function consider. first tried discretized outputs backpropagation phase plateaus would given usable derivatives. ignoring discretizations weights network still move wanted directions. diﬀerence however unlike standard tanh units single move actually change unit’s output. extreme case possible enough learning rate entire network’s output change despite weight changes made. extreme case cause slow training importantly training. instead next backprop phase weights directed move move direction cross discretization threshold. changes unit’s eventually network’s output. second need carefully examine figure note plateaus evenly sized. easily noticed middle plots. note magnitude derivative underlying tanh function maximum plateau smallest size. beneﬁcial practice; unit’s output change rapidly derivative tanh changes rapidly thereby keeping expected movement closer real movement discretization. intuition units perform practice present three ﬁgures showing sudo tanh relu perform tiny networks trained dimensional parabola. ﬁrst figure shows well parabola training progresses variety activations. networks single linear output unit hidden units. network trained stochastic gradient descent momentum perhaps telling results training curves sudo resulting parabola matches closely intuition; diﬀerent levels discretization symmetrically reduce error straightforward manner. increased performance matches networks trained tanh relu activations. figures show same networks hidden units respectively. again performance follows close intuition. figure training parabola hidden units. area error actual predicted. shown hidden unit activations tanh relu sudo sudo- sudo- sudo- sudo-. bottom progress epochs. provides intuitive demonstration discretized units change network’s performance. clear example sudo- eﬀect binary discretization levels approximating parabola. network found reasonable symmetric approximation cannot overcome discretization artifacts hidden units. figure training parabola hidden units. area error actual predicted. shown hidden unit activations tanh relu sudo- sudo- sudo- sudo- sudo-. bottom progress epochs. figure training parabola hidden units. area error actual predicted. shown hidden unit activations tanh relu sudo- sudo- sudo- sudo- sudo-. bottom progress epochs. section present experiments. modest scope comparison large-scale vision tasks addressed recent deep-learning research serve elucidate important facets performance sudo units. explore large number architectures network models hyper-parameters ensure using sudo units eﬀectively. also allow exact amount tuning baseline models fairness. simple binary classiﬁcation checkerboard goal problem correctly classify points plane based real-valued coordinates belonging either ’black’ ’red’ class. target classiﬁcation follows checkerboard pattern shown figure real-valued version traditional x-or-based problem used analyze study early neural network training know whether sudo units would work well single layer multiple even many units used layer large variety experiments performed. three basic architectures used. ﬁrst input units single hidden layer units type single tanh output unit. tested {tanh relu results shown table training regime three learning rates attempted activation/architecture combination best learning rate selected experiment. experiment replicated three times. results table show average performance best setting chosen individually experiment. note problem well others explored paper relu tanh units given exact parameter tuning setup. accuracies measured uniformly spaced points. tanh units; general trend observed problems explored. training larger networks provide substantial beneﬁt either tanh sudo units. next lets examine happens number hidden layers increased. second architecture similar network created ﬁrst identically sized hidden layers. variants explored architecture. layers fully connected previous layer skip connections layers. results shown table here results dramatically changed. first note tanh sudo- match outperform relu units. further despite fact sudo- output unique values tanh perform better. fact reducing often rivals best performances. simple binary outputs lower cardinality units? looking sudo- although match performance activations given small number hidden units layer number hidden units increases simplest binary activations also perform well. third architecture examine performance sudo units deeper network. here network hidden layers number hidden units previous experiments. results consistent found earlier even number discrete levels matches best performance. sudo units able keep even surpass full resolution tanh relu units. beyond quantitative error measurements illuminating examine decision surfaces created trained networks figure single hidden layer relu layers perform better tanh sudo units; samples single hidden layer shown ﬁrst figure ﬁnal figure results hidden layers shown. units results look close perfect. last hidden units relu appears signiﬁcantly worse hidden units signiﬁcantly worse sudo-. interesting note problem single hidden layer sudo-// performed well settings not. likely number decision boundaries need placed solve problem accurately trend problems. figure decision surfaces checkerboard problem. networks single hidden layer units. relu tanh sudo- activation units shown. networks hidden layers. networks hidden layers hidden units layer. simple regression important unanswered question remains examining classiﬁcation problems checkerboard problem described previous section. discretized nature activation provide advantage continuous activations easily creating sharp decision boundaries? advantage present sudo units fare worse asked smoothly approximate response surfaces? experiment examine well sudo units used approximate smooth curves. goal understand whether possible networks discrete outputs represent regression function accurately represented continuous relu units. consider function sin∗cos range discretized output aﬀect ability network approximating surface? basic network architectures employed experiments. ﬁrst hidden layers second architecture number hidden units hidden layer varied three learning rates attempted experiment best learning rate used setting trial replicated times random weights. results shown tables compared previous experiments here setting exaggerated role performance approximation especially networks hidden units. small values perform signiﬁcantly worse larger tanh relu units. intuitively makes sense fewer hidden units fewer ways combining limited number output values approximate real values. another important method looking results network’s memory bits allocated. example allow bits state kept allocated given table possibility create network hidden layers hidden units layer table would yield error alternatively hidden units architecture could double bits hidden unit. however would yield larger error. number bits allocated representation table experiment repeated hidden layers. again cell best learning rate determined independently. beyond improved performance across experiments notice sudo units able perform similarly relu tanh units. before bits allocate increasing improves performance. even units note performance simple binary units well settings remains relatively poor. eﬀective network capacity memorization though networks simple associative memory devices largely fallen research favor comparing much network memorize diﬀerent activation functions yield insight relative size networks needed. here examine accurately network reconstruct image given pixel’s coordinate. unlike tasks presented here well commonly explored research literature explicit notion generalization. network trained single grayscale image queried arbitrary pixel’s coordinates retrieve intensity. experiments original image grayscale intensity image size image post-processed pseudo-hdr ensure variation intensity values. shown figure results indicative theoretical measure network capacity. however indicative practical accessible capacity given particular learning approach. learning approach and/or sgd+momentum) large role determining network represents information form eﬃciency. diﬀerent learning approaches able better utilize network’s capacity. nonetheless propose interesting experiment tested optimization procedures commonly used. learning procedure used across trials hidden units yielding relative results valid comparison. using single hidden layer discrete activations achieve similar performance tanh units across sizes hidden layers. however discretization levels required. relu units clear advantage both. next table table examine performance hidden layers. peculiarities results tanh larger network consistently yield improved results. since performance sudo units often approximates tanh trend also extended many sudo settings. likely settings training hyper-parameters. nonetheless change experiments keep consistent across paper. tables performances tanh sudo- similar relu continues outperform both. largest diﬀerence performance tanh/sudo relu witnessed paper. disparity continues extra experiments constructed. note task autoencoding images autoencoding entire image presented once usually many images reconstructed network. contrast experiments network given coordinates pixel must recall pixel’s correct intensity value single training image. first advantage continue deeper architectures? table shows results network depth relu units outperform tanh sudo results. increased depth equalize performance tanh sudo units relu activations. regain lost performance comparison relu? rectify sudo units well; appendix in-depth details experiments. second look distribution activations sudo units compared activation tanh units discretized equal sized bins. common trend discretized non-discretized units extrema activations. histograms shown figure figure distributions hidden unit output activations post-training. histogram shows results across layers -hidden layers memorization networks trained hidden units. sudo- tanh shown. figure performance memorization task. original image pier water composed pixels shown top. networks hidden layers. sudo- yields unrecognizable image. discretization levels dock becomes recognizable. reconstruction vastly improved. autoencoding section examine standard task autoencoding images. tests networks trained imagenet images scaled tested independent test also imagenet experiments diﬀerent network architectures used series convolutions fully connected. described below bottleneck convolution much larger conv-nets fully connected nets thereby convolution-net outperformed fully connected network comparable settings. attempt made balance architecture’s bottleneck sizes study sole important comparison hidden unit activations architecture. table shows results convolution architecture. ﬁrst experiments conducted layers convolutions stride layers ﬁlters layer. followed layers decode step d-transpose depth ﬁnally layer recreating image. ﬁnal activations outputs tanh. ease reading table performance conv-net tanh activation function baseline. accordingly ﬁrst column table shows relative results hidden activation types explored. lower number lower therefore better performance. subsequent three columns table show performance achieved increasing number ﬁlters layer factor ×××. expected sharp decrease error across hidden unit activations number ﬁlters layer increases. importantly note good performance relative tanh relu units achieved sudo- continues discretization levels increased every network size. next repeat experiments using networks fully connected layers. experiments fully connected architecture layers encoding; consisted hidden units. next layers decoding hidden units. followed reconstruction layer size tanh units. table shows results relative score tanh scale conv-nets. table columns table show performance relative number hidden units layer. like results conv-nets larger networks improved performance. also again little diﬀerence performance sudo activations better tanh relu increased. mnist digit classiﬁcation ﬁnal test examine performance sudo units standard mnist digit classiﬁcation task test fully connected networks vary number hidden units layer number hidden layers. table shows results network employing single hidden layer. table provides similar results using network hidden layers. entry shown tables average networks trained randomly initialized starting weights. task discretization levels provide competitive performance number hidden units example sudo- sudo- often perform well tanh surpass relu performance throughout range number hidden units used hidden layer architecture. though alternate architectures convolutions known provide state results clarity eﬀects number hidden units fully connected layers. again before interested seeing relative performance sudo units tanh relu activations. salient ﬁnding work reducing number outputs allowed network’s hidden units unique outputs noticeable impact performance. valuable many modern scenarios computation needs limited memory premium pixel recurrent neural networks. second interesting trend noted role network depth. lower values shallow networks exhibited severely degraded performance. however depth network increased networks small exhibited large improvements performance. example memorization task doubling number units single hidden layer less performance improvement compared adding another layer number units. similar performance improvements seen common activations. though signiﬁcant previous work binarizing networks diﬃculties experienced training using networks greatly dissipated discretization level increased. large modiﬁcations training algorithms required. study standard tensorflow adam sgd+momentum used exact settings training validation testing. study used sigmoid tanh underlying activation function discretized. cases sudo units perform well relu generally tanh either. straightforward method address change underlying function. appendix examine happens sudo units replaced rectiﬁed-sudo units. analogously relu units units emit large parts activation. experiments paper repeated rectiﬁed-sudo units. performance much closer standard relu units. simplicity approach opens number potential avenues research. example future work conducted units diﬀerent discretization levels within network. witnessed examining real-valued checkerboard problems middle discretization settings performed better smaller larger. simple method capturing beneﬁt diﬀerent tackling problem within network. alternative approach priori guessing right make learnable parameter. similar approach taken piece-wise linear activation function learned neuron promising results compared static rectiﬁed linear units.", "year": 2018}