{"title": "Learning to Search with MCTSnets", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Planning problems are among the most important and well-studied problems in artificial intelligence. They are most typically solved by tree search algorithms that simulate ahead into the future, evaluate future states, and back-up those evaluations to the root of a search tree. Among these algorithms, Monte-Carlo tree search (MCTS) is one of the most general, powerful and widely used. A typical implementation of MCTS uses cleverly designed rules, optimized to the particular characteristics of the domain. These rules control where the simulation traverses, what to evaluate in the states that are reached, and how to back-up those evaluations. In this paper we instead learn where, what and how to search. Our architecture, which we call an MCTSnet, incorporates simulation-based search inside a neural network, by expanding, evaluating and backing-up a vector embedding. The parameters of the network are trained end-to-end using gradient-based optimisation. When applied to small searches in the well known planning problem Sokoban, the learned search algorithm significantly outperformed MCTS baselines.", "text": "planning problems among important well-studied problems artiﬁcial intelligence. typically solved tree search algorithms simulate ahead future evaluate future states back-up evaluations root search tree. among algorithms monte-carlo tree search general powerful widely used. typical implementation mcts uses cleverly designed rules optimised particular characteristics domain. rules control simulation traverses evaluate states reached back-up evaluations. paper instead learn where search. architecture call mctsnet incorporates simulation-based search inside neural network expanding evaluating backing-up vector embedding. parameters network trained end-to-end using gradient-based optimisation. applied small searches well-known planning problem sokoban learned search algorithm signiﬁcantly outperformed mcts baselines. many success stories artiﬁcial intelligence based application powerful tree search algorithms challenging planning problems well documented planning algorithms highly optimised tailoring domain example performance often dramatically improved modifying rules select trajectory traverse states expand evaluation function performance measured backup rule evaluations propagated search tree. contribution particularly powerful general method planning monte-carlo tree search used recent alphago program typical mcts algorithm consists several phases. first simulates trajectories future starting root state. second evaluates performance leaf states either using random rollout using evaluation function ’value network’. third backs-up evaluations update internal values along trajectory example averaging evaluations. present neural network architecture includes processing stages typical mcts inside neural network itself dynamic computational graph. idea represent internal state search node memory vector. computation network proceeds forwards root state like simulation mcts using simulation policy based memory vector select trajectory traverse. leaf state processed embedding network initialize memory vector leaf. network proceeds backwards trajectory updating memory visited state according backup network propagates child parent. finally root memory vector used compute overall prediction value action. major beneﬁt planning architecture compared traditional planning algorithms exposed gradient-based optimisation. allows replace every component mcts richer learnable equivalent maintaining desirable structural properties mcts model iterative local computations structured memory. jointly train parameters evaluation network backup network simulation policy optimise overall predictions mcts network majority network fully differentiable allowing efﬁcient training gradient descent. still internal action sequences directing control network cannot differentiated learning internal policy presents challenging credit assignment problem. address this propose novel generally-applicable approximate scheme credit assignment leverages anytime property computational graph allowing also effectively learn part search network data. sokoban domain classic planning task justify network design choices show learned search algorithm able outperform various model-free model-based baselines. network computation. mctsnets generalise extend ideas behind introducing tree structured memory stores node-speciﬁc statistics; learning simulation tree expansion strategy rather rolling possible action root state ﬁxed policy. similar predictron architecture also aggregates multiple simulations; however case simulations roll implicit transition model rather concrete steps actual environment. recent extension farquhar performs planning ﬁxed-tree expansion implicit model. signiﬁcant previous work learning evaluation functions using supervised learning reinforcement learning subsequently combined search algorithm however learning process typically decoupled search algorithm awareness search algorithm combine evaluations overall decision. several previous search architectures learned tune parameters evaluation function achieve effective overall search results given speciﬁed search algorithm. learning-to-search framework learns evaluation function effective context beam search. samuel’s checkers player algorithm treestrap algorithm apply reinforcement learning evaluation function combines minimax search produce accurate root evaluation comparison training applies supervised learning problem; methods successful chess checkers shogi. cases evaluation function scalar valued. variety previous efforts frame learning internal search decisions meta-reasoning problem optimized directly kocsis apply black-box optimisation learn meta-parameters controlling alpha-beta search learn ﬁne-grained control search decisions. considering action choices tree nodes bandit problem widely used variant mcts russell also studied meta-problem mcts considered myopic policy without function approximation. pascanu also investigate learning-to-plan using neural networks approach differentiable potentially limiting scalability. mctsnet architecture understood distinct equivalent perspectives. first understood search algorithm control closely mirrors simulation-based tree traversals mcts. second understood neural network represented dynamic computation graph processes input states performs intermediate computations hidden states outputs ﬁnal decision. present perspectives turn starting original unmodiﬁed search algorithm. goal planning optimal strategy maximises total reward environment deﬁned deterministic transition model mapping state action successor state reward model describing goodness transition. mcts simulation-based search algorithm converges solution planning problem. high level idea mcts maintain statistics node visit count mean evaluation statistics decide branches tree visit. mcts proceeds running number simulations. simulation traverses tree selecting promising child according statistics leaf node reached. leaf node evaluated using rollout value-network value propagated back-up phase updates statistics tree along traversed path tracking visit counts mean evaluation following state action search proceeds anytime fashion statistics gradually become accurate simulations focus increasingly promising regions tree. ponents search described previous paragraph. internally sub-networks manipulate memory statistics node tree vector representation embedding network evaluates state computes initial ’raw’ statistics; understood generalisation value network algorithm simulation policy used select actions simulation based statistics backup network hparent updates propagates statistics search tree. finally overall decision made readout network algorithmic description search network follows algorithm search completes selects action root visit counts. simulation policy chosen trade-off exploration exploitation tree. variant mcts inspired bandit algorithm mctsnet proceeds executing simulations start root state simulation reaches leaf node node expanded evaluated initialize vector statistics. back-up phase updates statistics node traversed simulation. speciﬁcally parent node statistics updated values depend child values also previous values. finally selected action chosen according statistics root search tree. present mctsnet neural network architecture. algorithm described effectively deﬁnes form tree-structured memory node tree maintains corresponding statistics statistics initialized embedding network otherwise kept constant next time node visited. updated using backup network. ﬁxed tree expansion allows mctsnet deep residual recursive network numerous skip connections well large number inputs corresponding different potential useful introduce index simulation count tree memory simulation tree nodes conditioned tree path a··· simulation mctsnet memory gets updated follows note computation mcts network deﬁned ﬁnal tree also order nodes visited furthermore taken whole mctsnet stochastic feed-forward network single input single output however thanks tree-structured memory mctsnet naturally allows partial replanning fashion similar mcts. assume root-state mcts network chooses action transitions state initialize mcts network subtree rooted initialize node statistics subtree previously computed values resulting network would recurrent across real time-steps. learned simulation policy basic unstructured form simulation policy network simple mapping statistics logits deﬁne exp). also consider adding structure modulating logit side-information corresponding action. form action-speciﬁc information obtained child statistic another form information comes learned policy prior actions log-probabilities puct case policy prior comes learning small model-free residual network data. combined obtain following modulated network version simulation policy logits embedding readout network embedding network standard residual convolution network. readout network simple transforms memory vector root required output format case action distribution. appendix details. readout network mctsnet ultimately outputs overall decision evaluation entire search. ﬁnal output principle trained according loss function value-based policy gradient reinforcement learning. however order focus novel aspects architecture choose investigate mctsnet architecture supervised learning setup labels desired actions state objective train mctsnet predict action state denote actions sampled stochastically during simulation; stochastic actions taken simulation figure diagram shows execution search evolution search tree rooted simulation last simulation path highlighted red. computation graph mctsnet resulting simulations. black arrows represent application embedding network initialize tree node arrows represent forward tree traversal simulation using simulation policy environment model leaf node reached. blue arrows correspond backup network updates memory statistics along traversed simulation path based child statistic last updated parent memory diagram makes clear backup mechanism skip simulations particular node visited. example fourth simulation updates based second simulation since visited third simulation. finally readout network green outputs action distribution based last root memory additional diagrams available appendix. performing desired number simulations network output action probability vector random function state stochastic actions choose optimize prediction average correct minimizing average cross entropy prediction correct label pair loss ﬁrst term gradient corresponds differentiable path network. second term corresponds gradient respect simulation distribution uses reinforce score-function method. term ﬁnal likelihood plays role ‘reward’ signal effect quality search determined conﬁdence correct label higher conﬁdence better tree expansion stochastic actions induced tree expansion reinforced. addition follow common method adding neg-entropy regularization term loss prevent premature convergence. although unbiased reinforce gradient high variance; difﬁculty credit assignment problem number decisions contribute single decision large simulations) understanding decision contributed error better tree expansion structure intricate. order address issue design novel credit assignment technique anytime algorithms casting loss minimization single example sequential decision problem using reinforcement learning techniques come family estimators allowing manipulate bias-variance trade-off. distribution parameter maximum value letting leads greedy behavior actions simulation chosen maximize immediate improvement loss m−). myopic mode linked single-step assumption proposed russell wefald analog context. figure evolution success ratio sokoban training using continuous evaluator. mctsnet model-free baselines. case copy-model access number parameters subnetworks. baseline also matches amount computation. also provide performance mcts variable number simulations. investigate architecture game sokoban classic challenging puzzle game described above results obtained supervised training regime. however continuously evaluate network training running agent random sokoban levels report success ratio solving levels. throughout experimental section keep architecture size embedding readout network ﬁxed detailed appendix. ﬁrst compare mctsnet architecture simulations couple model-free baselines. assess whether mctsnet leverages information contained simulations consider version network uses sham environment model otherwise identical architecture. case baseline number parameters mctsnet uses subnetwork exactly once. also test architecture case case model-free baseline number parameters tial state arbitrary number internal steps mctsnet simulations. step number stochastic decisions taken step candidate output distribution evaluated loss function value loss step denoted assume objective maximize terminal negative loss letting rewrite terminal loss telescoping words stochastic decisions step simply terminal loss reinforcement signal rather difference terminal loss loss computed step started estimate still unbiased lower variance especially later steps algorithm. next trade bias variance introducing discount term essence simulation choices choose reward short term improvements later ones since relation simulation later improvements harder ascertain likely γm−mrm mostly appear noise. letting ﬁnal gradient estimate mctsnet loss becomes perform amount computation also evaluate standard model-based method environment mcts prelearned value function given access simulations step mctsnet observe success ratio mcts sokoban. requires times simulations version mcts reach level performance mctsnet. overall mctsnet performs favorably modelbased model-free baselines fig. comparisons validate ingredients approach. first comparison mctsnet model-free variant conﬁrms extracts information contained states visited search section show also able learn nontrivial search policies. second test time mctsnet mcts environment model therefore principle access information. higher performance mctsnet demonstrates beneﬁts learning propagating vector-valued statistics richer informative tracked mcts. using architecture detailed sec. simulations mctsnets reach levels solved close obtained although different setting consider detailed comparisons justify understand different design choices mctsnet. section justify backup network choice made sec. comparing simple version gated residual architecture suggested. gated residual architecture backup network advantageous terms stability accuracy. fig. illustrates simulations gated residual version systematically achieves better performance. larger number simulations found nonresidual backup network simply numerically unstable. explained large number updates recurrently applied cause divergence network arbitrary form. instead gated network quickly reduce inﬂuence update coming subtree slowly depart identity skipconnection; behavior we’ve observed practice. experiments therefore employed gated video mctsnet solving sokoban levels available https//goo.gl/buhd. shows visualisation search tree’s principal variation step evolution probability selected action function number simulations. previously mentioned learning simulation policy challenging noisy estimation pseudoreturn selected sequence investigate effectiveness proposed designs proposed approximate credit assignment scheme learning parameters basic setting despite estimation issues veriﬁed nevertheless lift simple form performance mctsnet using uniform random simulation policy. note mctsnet random simulation strategy already performs reasonably since still take advantage learned statistics backups readout. blue curves fig. comparison. improved credit assignment technique vanilla gradient enough learn simulation policy performs better random search strategy still hampered noisy estimation process simulation policy gradient. effective search strategy learned using proposed credit assignment scheme sec. modulated policy architecture sec. show this train mctsnets different values discount using modulated policy architecture. results fig. demonstrate value network optimizing true loss ideal choice mctsnet. lower values perform better different stages training. late training estimation problem stationary advantage reduces remains. best performing mctsnet architecture shown comparison others fig. also investigated whether simply providing policy prior term could match results. policy prior learned right entropy regularization indeed well-performing simulation policy mctsnet simulations match best performing learned policy. thanks weight sharing mctsnets principle arbitrary number simulations larger number simulations search progressively opportunities query environment model. check whether search network could take advantage training compare mctsnet different number simulations applied training evaluation. fig. approach able query extract relevant information additional simulations generally achieving better results less training steps. shown possible learn successful search algorithms framing dynamic computational graph optimized gradient-based methods. viewed ﬁrst step towards long-standing ambition meta-reasoning internal processing agent. particular proposed neural version mcts algorithm. maintain desirable properties mcts allowing ﬂexibility improve choice nodes expand statistics store memory propagated using gradient-based learning. pronounced departure existing search algorithms could also considered although come expense harder optimization problem. also assumed true environment available simulator could also relaxed model environment could learned separately even end-toend advantage approach search algorithm could learn make imperfect model. although focused supervised learning setup approach could easily extended reinforcement learning setup leveraging policy iteration mcts focused small searches similar scale plans processed human brain massive-scale searches high-performance games planning applications. fact learned search performed better standard mcts order-of-magnitude computation suggesting learned approaches search ultimately replace handcrafted counterparts. pascanu razvan yujia vinyals oriol heess nicolas buesing lars racani`ere sebastien reichert david weber th´eophane wierstra daan battaglia peter. learning model-based planning scratch. arxiv preprint arxiv. baxter jonathan tridgell andrew weaver lex. knightcap chess program learns combining game-tree search. proceedings international conference machine learning chang kai-wei krishnamurthy akshay agarwal alekh daume langford john. learning search proceedings better teacher. international conference machine learning j¨unger michael liebling thomas naddef denis nemhauser george pulleyblank william reinelt gerhard rinaldi giovanni wolsey laurence years integer programming early years state-of-the-art. springer russell stuart. rationality intelligence. proceedings international joint conference artiﬁcial intelligence-volume morgan kaufmann publishers inc. russell stuart wefald eric. optimal game-tree proceedings search using rational meta-reasoning. international joint conference artiﬁcial intelligence-volume jonathan hlynka markian jussila vili. temporal difference learning applied highproceedings performance game-playing program. international joint conference artiﬁcial intelligence-volume morgan kaufmann publishers inc. schulman john heess nicolas weber theophane abbeel pieter. gradient estimation using stochastic computation graphs. advances neural information processing systems silver david huang maddison chris guez arthur sifre laurent driessche george schrittwieser julian antonoglou ioannis panneershelvam veda lanctot marc mastering game deep neural networks tree search. nature silver david schrittwieser julian simonyan karen antonoglou ioannis huang guez arthur hubert thomas baker lucas mastering game without human knowledge. nature weber th´eophane racani`ere s´ebastien reichert david buesing lars guez arthur rezende danilo jimenez badia adria puigdom`enech vinyals oriol heess nicolas yujia imagination-augmented agents deep reinforcement learning. arxiv preprint arxiv. sokoban domain layout four boxes targets. level generation gained access level generator described weber directly provide symbolic representation environment coded feature fig. visual representation. vanilla mcts deep value network long search times employed generate good quality trajectories oracle surrogate. speciﬁcally pre-trained value network leaf evaluation depth-wise transposition tables deal symmetries reuse relevant search subtree real step. solving methods could used since generate labels supervised learning. training dataset consists trajectories distinct levels; approximately levels solved agent; solved levels take average steps unsolved levels interrupted steps. also create testing trajectories. embedding network convolution network residual blocks. residual block composed -channel convolution layers kernels applied stride residual blocks preceded convolution layer properties followed convolution layer kernel channels. linear layer maps ﬁnal convolutional activations vector size readout network simple single hidden layer size non-linearities layers relus. policy prior network similar architecture embedding network residual blocks -channel convolutions. figure diagram illustrating mctsnet search ﬁrst simulations detailed well extract last simulation readout network employed output action distribution root memory statistic. detail longer simulation given figure figure diagram represents simulation mctsnet leftmost part represents simulation phase tree leaf node using current state tree memory simulation right part diagram illustrates embedding backup phase memory vectors traversed tree path updated bottom-up fashion. memory vectors nodes visited tree path stay constant.", "year": 2018}