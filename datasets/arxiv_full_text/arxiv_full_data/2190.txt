{"title": "Demystifying Relational Latent Representations", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Latent features learned by deep learning approaches have proven to be a powerful tool for machine learning. They serve as a data abstraction that makes learning easier by capturing regularities in data explicitly. Their benefits motivated their adaptation to relational learning context. In our previous work, we introduce an approach that learns relational latent features by means of clustering instances and their relations. The major drawback of latent representations is that they are often black-box and difficult to interpret. This work addresses these issues and shows that (1) latent features created by clustering are interpretable and capture interesting properties of data; (2) they identify local regions of instances that match well with the label, which partially explains their benefit; and (3) although the number of latent features generated by this approach is large, often many of them are highly redundant and can be removed without hurting performance much.", "text": "abstract. latent features learned deep learning approaches proven powerful tool machine learning. serve data abstraction makes learning easier capturing regularities data explicitly. beneﬁts motivated adaptation relational learning context. previous work introduce approach learns relational latent features means clustering instances relations. major drawback latent representations often black-box diﬃcult interpret. work addresses issues shows latent features created clustering interpretable capture interesting properties data; identify local regions instances match well label partially explains beneﬁt; although number latent features generated approach large often many highly redundant removed without hurting performance much. latent representations created deep learning approaches proven powerful tool machine learning. traditional machine learning algorithms learn function directly maps data target concept. contrast deep learning creates several layers latent features original data target concept. results multi-step procedure simpliﬁes given task solving progress learning latent representations predominantly focused vectorized data representations. likewise utility recognized relational learning community models learned instances relationships well prevalent latent representations paradigm direction embeddings vector spaces core idea behind embeddings replace symbols numbers logical reasoning algebra. precisely relational entities transformed low-dimensional vectors relations matrices functions vectors. learning latent features corresponds learning lowdimensional representations relational entities relations. many variations formalization exist share underlying principle. assuming facts entities whereas existing relations them. goal corresponding vectorized representations respectively together matrix representations respectively. precisely goal vectorized representations products high values. contrast given false fact product value. embeddings approaches several drawbacks. first latent features created inherent meaning created satisfy aforementioned criteria. thus major obstacle interpretability approach important many aspects strengths relational learning. second huge amounts data needed order extract useful latent features. knowledge bases used training often contain millions facts. third clear approaches handle unseen entities without re-training entire model. recently dumanˇci´c blockeel introduced complementary approach titled curled takes relational learning stance focuses learning relational latent representations unsupervised manner. viewing relational data hypergraph instances form vertices relationships among form hyperedges authors rely clustering obtain latent features. core component approach declarative intuitive speciﬁcation similarity measure used cluster instances relationships. consequently makes entire approach transparent respect meaning latent features intuitive meaning similarity precisely speciﬁed. beneﬁts latent representations clearly shown respect performance complexity. complexity models learned latent features consistently lower compared models learned original data representation. moreover models learned latent features often resulted improved performance large margin well. results jointly show latent representations capture complex dependencies simple manner. work investigate properties relational latent representations created curled. start asking question latent features mean? introduce simple method extract meaning latent features show capture interesting properties. next makes latent representations eﬀective? initial work showed beneﬁts latent representations however explanation oﬀered case. hope shed light behind scene oﬀer answer case. following section ﬁrst brieﬂy introduce neighbourhood trees central concept curled. describe approach used extracting knowledge form latent features investigating properties latent representation. results presented discussed next followed conclusion. fig. snapshot knowledge base corresponding neighbourhood trees profa entity knowledge base describes students professors courses teach. entities represented node attributes rectangles relationships edges. attribute values left brevity. central concept curled neighbourhood tree. neighbourhood tree rooted directed graph describing instance together instances relates properties. viewing relational data hypergraph neighbourhood tree provides summary path pre-deﬁned length originate particular vertex instances represented neighbourhood trees instances compared comparing corresponding neighbourhood trees. authors introduce versatile declarative similarity measure analyses neighbourhood trees multiple aspects introducing following core similarities attribute similarity root vertices attribute similarity neighbouring vertices connectivity root vertices similarity vertex identities neighbourhood similarity edge types continuing example figure person instances clustered based attributes yields clusters professors students. clustering person instances based vertex identities neighbourhood yields clusters research groups professor students. core similarities form basic building blocks variety similarity measure deﬁned neighbourhood trees. ﬁnal similarity measure linear weighted combination core similarities. weights simply deﬁne relative importance core similarities ﬁnal similarity measure. value assignments weights deﬁnes similarity interpretation. details core similarities similarity measure ideas central curled. first learns latent features clustering instances relationships. second uses multiple similarity interpretations obtain variety features. ideas realised means neighbourhood trees. instances relations represented neighbourhood trees similarity interpretation result core similarities consider certain parts neighbourhood trees. latent features learned curled repeated clustering instances relations alternating similarity measure iteration. latent feature corresponding cluster instances associated latent predicate. truth instantiations latent predicates reﬂect cluster assignments i.e. instantiations latent predicate true instances belong cluster; therefore latent features deﬁned extensionally lack interpretable deﬁnition. however intuitive speciﬁcation similarity measure makes curled transparent method clear description elements neighbourhood trees make instances similar. consequently discovering meaning latent features substantially easier embedding approaches latent feature corresponds cluster meaning features reﬂected prototype cluster. approximate mean prototypical neighbourhood tree search elements common neighbourhood trees forming cluster. elements either attribute values edge types vertex identities. similarity interpretations used obtain cluster limits elements considered part deﬁnition. moreover neighbourhood trees compared relative frequencies elements existence only. therefore mean neighbourhood tree meaning latent feature search elements similar relative frequencies within neighbourhood tree forming cluster. calculate relative frequencies elements within individual neighbourhood tree level vertex type. case discrete attributes corresponds distribution values. case numerical attributes consider mean value. case vertex identities edge types simply look frequencies respect depth neighbourhood tree. example figure neighbourhood tree profa contains advisedby relations thus frequency fig. discovering meaning latent features analysing relations. properties describe latent features ones similar relative frequency neighbourhood trees. starting cluster instances viewed neighbourhood trees relative frequencies elements calculated neighbourhood tree next mean standard deviation relative frequencies calculated individual element within cluster elements explain latent features decided θ-conﬁdence. setting identiﬁes advisedby teaches relevant elements select relevant elements. ﬁnal step involves decision elements form deﬁnition latent feature. relevant elements identiﬁed notion θ-conﬁdence captures allowed amount variance order element relevant. above-described procedure explains latent features terms distribution elements neighbourhood instance pros cons. downside type explanation conform standard ﬁrst-order logic syntax common within relational learning. despite reduced readability explanations substantially transparent interpretable ones produced embeddings approaches. however beneﬁt approach increases expressivity relational learner extensionally deﬁning properties otherwise inexpressible ﬁrstorder logic. latent features produced curled proven useful reducing complexity models improving performance. however explanation oﬀered case. second part work look properties latent representations oﬀer partial explanation usefulness. answer question introduce following properties label entropy sparsity redundancy. entropy sparsity. label entropy sparsity serve proxy quantiﬁcation learning diﬃculty i.e. diﬃcult learn deﬁnition target concept. considering particular predicate label entropy reﬂects purity true groundings respect provided labels. intuitively true groundings predicates tend predominantly focus particular label expect model learning easier. sparse representations cornerstones deep learning refer notion concepts explained based local properties instance space. even though many properties might exist particular problem sparse representations describe instances using small subset properties. intuitively concept spread across small number local regions expected easier capture concept spread globally entire instance space. quantifying sparsity relational data challenging task approached multiple directions either analysing number true groundings interaction entities instance. adopt simple deﬁnition number true groundings predicate. label entropy sparsity jointly describe compelling property data representation instances space divided many local regions match labels well consequently make learning substantially easier. redundancy. downside curled high number created features. despite proven usefulness high number latent features enlarges search space relational model increases diﬃculty learning. similarity interpretations provided user possible almost identical clusterings obtained diﬀerent similarity interpretations. thus many features redundant removing simpliﬁes learning. measure redundancy adjusted rand index standard measure overlap clusterings study impact performance. evaluate inﬂuence redundant features modify curled adding additional overlap parameter every time clustering obtained check overlap previously discovered clusterings using ari. calculated value bigger clustering rejected. latent features result models lower complexity and/or improved performance exhibit lower label entropy compared original data representation? results obtained divided three categories. ﬁrst category contains imdb uwcse datasets; datasets present easy relational learning tasks original data representation suﬃcient almost perfect performance. main beneﬁt latent representations tasks reduction model complexity. second category includes terroristattack dataset. main beneﬁt latent representation reduction complexity performance. third category involves hepatitis mutagenesis webkb datasets. tasks beneﬁted latent representations performance reduced model complexity. especially true hepatitis webkb datasets performance improved large margin. take representative task categories. precisely imdb uwcse hepatitis terroristattack datasets experiments. imdb uwcse datasets included easy understand without domain knowledge thus useful analysing interpretability relational latent features. parameters latent representation take best parameters individual datasets selected model selection procedure analysing interpretability illustrate interpretability relational features show examples latent features created diﬀerent datasets. show relational decision trees learned original latent representations. explanations latent features provided well. figure shows decision trees learned imdb dataset. task distinguish actors directors simple relational learning task original latent decision tree achieve perfect performance single node. even though latent representation seem beneﬁcial particular case interesting selected latent feature captures information decision tree learned original data person instances cluster ones relationship movie instances worked another person fig. relational decision trees learned original latent data representation imdb dataset. dashed ellipse indicates target predicate arguments. ﬁrst argument marked declared input denotes person. second argument marked declared output states label instance given values leaves decision trees assignments dashed rectangle describes latent feature level mean neighbourhood tree θ-conﬁdent elements listed mean standard deviation. figure shows decision trees uwcse dataset beneﬁt latent features. despite simplicity distinguishing students professors decision tree learned latent features compact single node whereas decision tree learned original features consists three nodes. latent feature captures similar knowledge original decision tree expressed simpler manner professor someone either position faculty connected people currently certain phase study program program certain number years. particularly interesting examples that even though latent features created unsupervised manner match provided label well. moreover seem almost perfectly capture labelled information features needed outperform decision tree learned original data representation. observation shows curled indeed capturing sensible knowledge latent space. aforementioned examples easy understand interpret without extensive domain knowledge. tasks beneﬁted latent features substantially diﬃcult understand. instance latent features created mutagenesis dataset reduce complexity relational decision tree nodes improving accuracy similarly hepatitis dataset latent features reduced complexity decision tree nodes improving accuracy examples require extensive knowledge interpret them leave work. label entropy. figure summarizes label entropy dataset. cases representation learning proved helpful latent representations substantially larger number predicates label entropy compared original data representation. latent representation terroristattack datasets however shows diﬀerent behaviour latent features high entropy dominate representation. results agree expectation high number entropy features makes learning easier. however latent features label entropy. expected labels considered learning latent features. also pose problem latent features less consistent particular task easily might case features useful diﬀerent task. sparsity. figure summarizes number groundings i.e. sparsity. distribution number true groundings latent representations heavily skewed towards small number groundings contrast original representation. especially case hepatitis dataset proﬁts latent features. exception behaviour terroristattack dataset original representation already sparse. results indicates latent features indeed describe smaller groups instances local properties instead global properties instances. connecting label entropy sparsity. potential explanation discussed results might many latent features capture small number instances consequently leads large number features label entropy. features would largely useless make generalization diﬃcult. verify case figure plots label entropy versus number groundings predicate. latent features label entropy would indeed capture small number instances many points would condensed bottom left corner plot. however case many latent predicates label entropy actually number groundings comparable predicates original representation. exception terroristattacks dataset. results jointly point following conclusion latent features successfully identify local regions instance space match well provided labels. consequence local regions easier capture represent. redundancy. figure summarizes inﬂuence accuracy number latent features. results show performance classiﬁer aﬀected removing features based overlap clusterings deﬁne. performance tilde remains approximately same whereas number latent features reduced number features directly related size search space relational model encouraging result indicating size search space naively reduced without sacriﬁcing performance. fig. redundancy features latent representations. accuracy number latent features number facts reported varying values accuracy percentage correctly classiﬁed examples reported. number features facts report ratio number features/facts latent representation obtained speciﬁc value number features/facts latent representation work closely inspect properties latent representations relational data. focus relational latent representations created clustering instances relations among them introduced curled ﬁrst property analyse interpretability latent features. introduce simple method explain meaning latent features show capture interesting sensible properties. second identify properties latent representation partially explain usefulness namely label entropy sparsity. using properties show obtained latent features identify local regions instance space match well labels. consequently explains predictive model learned latent features less complex often perform better model learned original features. third show latent features tend redundant", "year": 2017}