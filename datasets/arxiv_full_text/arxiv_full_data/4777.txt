{"title": "The Online Coupon-Collector Problem and Its Application to Lifelong  Reinforcement Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "Transferring knowledge across a sequence of related tasks is an important challenge in reinforcement learning (RL). Despite much encouraging empirical evidence, there has been little theoretical analysis. In this paper, we study a class of lifelong RL problems: the agent solves a sequence of tasks modeled as finite Markov decision processes (MDPs), each of which is from a finite set of MDPs with the same state/action sets and different transition/reward functions. Motivated by the need for cross-task exploration in lifelong learning, we formulate a novel online coupon-collector problem and give an optimal algorithm. This allows us to develop a new lifelong RL algorithm, whose overall sample complexity in a sequence of tasks is much smaller than single-task learning, even if the sequence of tasks is generated by an adversary. Benefits of the algorithm are demonstrated in simulated problems, including a recently introduced human-robot interaction problem.", "text": "probabilities reward functions. furthermore tasks elements ﬁnite collection mdps initially unknown. setting particularly motivated applications user personalization domains like education health care online marketing consider task interacting particular individual goal leverage prior experience improve performance later users. indeed partitioning users several groups similar behavior found uses various application domains offers form partial personalization allowing system quickly learn good interactions user still offering much personalization modeling individuals same. critical issue transfer lifelong learning leverage information previous tasks solving current one. task represents different different optimal policy leveraging prior task information actually result substantially worse performance learning prior information phenomenon known negative transfer intuitively partly leveraging prior experience prevent agent visiting states different rewards task would visited under optimal policy task. words lifelong addition exploration typically needed obtain optimal policies single-task agent also needs sufﬁcient exploration uncover relations among tasks agent faces online discovery problem task prior tasks novel one. agent treat task seen discover whether novel. failing correctly treat novel task treating existing task prior task lead sub-optimal performance. main contributions three-fold. first inspired need online discovery llrl formulate study novel online coupon-collector problem progiven ﬁnite sets states action mdps similar transition/reward parameters similar value functions. thus ﬁnitely many policies sufﬁce represent near-optimal policies. transferring knowledge across sequence related tasks important challenge reinforcement learning despite much encouraging empirical evidence little theoretical analysis. paper study class lifelong problems agent solves sequence tasks modeled ﬁnite markov decision processes ﬁnite mdps state/action sets different transition/reward functions. motivated need cross-task exploration lifelong learning formulate novel online coupon-collector problem give optimal algorithm. allows develop lifelong algorithm whose overall sample complexity sequence tasks much smaller single-task learning even sequence tasks generated adversary. beneﬁts algorithm demonstrated simulated problems including recently introduced human-robot interaction problem. transfer learning ability take prior knowledge perform well task essential capability intelligence. tasks often involve multiple steps decision making uncertainty. therefore lifelong learning across multiple reinforcement-learning tasks llrl signiﬁcant interest. potential applications broad leveraging information across customers speeding robotic manipulation environments. last decades much previous work problem predominantly focuses providing promising empirical results little formal performance guarantees wilson taylor stone schmidhuber many references therein) ofﬂine/batch setting multi-armed bandits paper focus special case lifelong reinforcement learning captures class interesting challenging applications. assume tasks modeled ﬁnite markov decision processes mdps state action spaces differ transition copyright association advancement artiﬁcial intelligence rights reserved. viding algorithms optimal regret guarantees. results independent interest given wide application classic coupon-collector problem. second propose novel llrl algorithm essentially occp algorithm uses sample-efﬁcient single-task algorithms black box. solving sequence tasks compared single-task llrl algorithm shown substantially lower sample complexity exploration theoretical measure learning speed online finally provide simulation results simple gridworld simulation simulated human-robot collaboration task recently introduced nikolaidis exist ﬁnite different human user types different preferences desired robot collaboration interaction. results illustrate beneﬁts relative advantage approach prior ones. related work. substantial interest lifelong learning across sequential decision making tasks decades; e.g. ring schmidhuber white modayil sutton lifelong closely related transfer information source mdps used accelerate learning target distinctive element lifelong every task target source task. consequently agent explore current task allow better knowledge transferred better solve future tasks—this motivation online coupon-collector problem formulate study here. setting solving mdps sampled ﬁnite related konidaris doshi-velez hidden parameter mdps cover setting others latent variable captures aspects task. wilson tackle similar problem hierarchical bayesian approach modeling task-generation processes. prior work lifelong/transfer focused algorithmic empirical innovations little theoretical analysis online exception two-phase algorithm provably lower sample complexity single-task makes critical assumptions. setting general tasks selected adversarially instead stochastically consequently assume minimum task sampling probability knowledge cardinality mdps. allows algorithm applied realistic problems personalization domains number user types typically unknown advance. addition ammar tutunov eaton recently introduced provided regret bounds policy-search algorithm llrl. task’s policy parameter represented linear combination shared latent variables allowing used continuous domains. however addition local optimality guarantees typical policy-search methods lack sufﬁcient exploration approach also lead suboptimal policies. bianchi also require efﬁcient exploration. bandits every action leads observed loss occp action observable loss. apple tasting similar ﬂavor occp different structure loss matrix; furthermore analysis mistake-bound model suitable here. langford zinkevich kakade study abstract model exploration setting assumes non-decreasing deterministic reward sequence allow non-monotonic stochastic reward sequences. consequently explore-ﬁrst strategy optimal setting occp. furthermore analyze competitive ratios focus excessive loss. bubeck ernst garivier tackle different problem called optimal discovery quick identiﬁcation hidden elements assuming access different sampling distributions. finally compared missing mass problem pure predictions occp involves decision making thus requires balancing exploration exploitation. motivated need cross-task exploration discover novel mdps llrl formulate study novel problem online version classic coupon-collector problem solutions online play crucial role developing lifelong algorithm next section. moreover problem independent interest many disciplines like optimization biology communications cache management operating systems found important applications well meta-learning problems require efﬁcient exploration uncover cross-task relation. formulation coupon-collector problem multinomial distribution coupon types. round type sampled much research done study probabilistic properties time coupons ﬁrst collected especially expectation references therein). online coupon-collector problem occp unknown. given coupon learner probe type skip; thus binary action set. learner also given four constants specifying loss matrix table table occp loss matrix rows indicate actions; columns indicate whether current item novel not. known constants specify costs actions different situations. {mt}; else beginning round deﬁne history algorithm admissible chooses actions based possibly external source randomness. distinguish settings. stochastic setting environment samples unknown distribution i.i.d. fashion. adversarial setting sequence generated adversarial arbitrary depends learner knew type optimal strategy would choose otherwise. loss type otherwise. hence rounds number distinct items sequence ideal strategy loss challenge course learner know mt’s type choosing thus balance exploration exploitation clearly overunder-exploration result suboptimal strategies. therefore interested ﬁnding algorithms smallest cumulative loss possible. formally occp algorithm possibly stochastic function maps histories actions tot= -round regret algorithm expectation expectation taken respect randomness environment well explore-first strategy stochastic case shown algorithm chooses total times expected regret smallest actions chosen beginning. resulting strategy sometimes called explore-first expfirst short multi-armed bandit literature. knowledge minm∈m types discovered ﬁrst phase consisting rounds high probability. results high-probability regret bound used establish expected regret bound summarized below. proof given appendix proposition minm∈m then probability moreover expected regret small moreover many scenarios sampling process non-stationary even adversarial study general algorithm forcedexp based forced exploration prove regret upper bound. next subsection present matching lower bound indicating algorithm’s optimality. game starts algorithm chooses ﬁxed sequence probing rates round chooses actions accordingly p{at p{at main result subsection following proved appendix theorem parameter then given results show forcedexp eventually performs well hypothetical optimal strategy knows type every round matter generated. moreover per-round regret decays order lower bounds main result subsection theorem shows regret bound forcedexp essentially regret suffered unless types discovered. sufﬁalgorithm expected regret probability ciently small regret probing phrase; otherwise chance able discover type regret. value bound proposition dependence. preliminaries consider discrete-time ﬁnite mdps speciﬁed ﬁve-tuple states actions transition probability function reward function discount factor. initially unknown. given policy state state–action value functions denoted respectively. optimal value functions finally vmax known upper bound much smaller. various frameworks studied capture learning speed single-task online algorithms regret analysis here focus another useful notion known sample complexity exploration sample complexity short. results especially related cross-task exploration occp also regret analysis. algorithm viewed nonstationary policy whose value functions deﬁned similarly stationary-policy case. unknown call mistake step algorithm chooses suboptimal action namely deﬁne sample complexity maximum number mistakes probability least polynomial called pac-mdp pac-mdp algorithms work assigning maximum reward state–action pairs visited often enough obtain reliable transition/reward parameters. finite-model-rl algorithm used llrl leverages similar idea current task close ﬁnite known models. cross-task exploration lifelong lifelong agent seeks maximize total reward acts sequence tasks. tasks related learning speed expected improve transferring knowledge obtained prior tasks. following previous work motivated many applications assume ﬁnite possible mdps. agent solves sequence tasks denoting task solving task agent know whether encountered before. acts steps given take advantage information extracted solving prior tasks mt−}. setting general allowing tasks chosen adversarially contrast prior work focused stochastic case pac-explore parameters fully explore states every action taken every state least times. pac-explore ﬁnishes choose actions optimal policy empirical model ˆmt. existing models nonoverlapping conﬁdence intervals state– action pair’s transition/reward parameters help agent perform better future tasks. indeed prior work demonstrated learning latent structure possible mdps encountered lead signiﬁcant reductions sample complexity later tasks. realize beneﬁt explicitly identifying latent shared structure. observation inspired abstraction occp formalize relation llrl. here probing action corresponds full exploration current task skipping action corresponds applying transferred knowledge accelerate learning. occp forcedexp algorithm resulting algorithm overloading terminology refer llrl algorithm forcedexp. contrast two-phase llrl algorithm brunskill essentially uses expfirst discover mdps referred expfirst. round probing happen forcedexp performs pac-explore outlined algorithm appendix full exploration accurate empirical model ˆmt. determine whether algorithm checks ˆmt’s parameters’ conﬁdence intervals disjoint every least state–action pair. probing happen agent assumes follows finite-model-rl algorithm extension rmax work ﬁnitely many models. finite-model-rl amount exploration scales number models rather number state–action pairs. therefore algorithm gains sample complexity reducing unnecessary exploration transferring prior knowledge current task already remark. forcedexp appear na¨ıve simplistic decides whether probe task seeing data easy allow algorithm switch nonprobing probing acting whenever appears different mdps although change beneﬁcial practice improve worst-case sample complexity non-probing case running finite-model-rl guarantee identify current task one. assuming current models learner follow policy never sufﬁciently explores informative state–action pair could revealed current novel. therefore theoretical perspective critical allow algorithm switch probing mode. similarly switching probing non-probing middle task general helpful shown following example. contain single state mdps differ reward function. suppose round learner discovered mdps past chooses probe thus running pac-explore. steps learner switches non-probing trying every action times states risk under-exploration rewards optimal actions even higher reward another action optimal terminating exploration early learner fail identify optimal action ending poor policy. sample-complexity analysis section gives sample-complexity analysis algorithm convenience denote dynamics -dimensional vector ﬁrst components giving transition probabilities corresponding next states last component average immediate reward model difference denoted distance vectors. finally upper bound number next states transition models mdps note larger much smaller many problems. following assumptions made analysis exists known quantity every distinct mdps exists known diameter that states policy takes agent navigate steps average; steps solve task ﬁrst assumption requires distinct mdps differ sufﬁcient amount dynamics least state– action pair made convenience encode prior knowledge note known beforehand mdps differ every state–action pair \u0001-optimal policy o-optimal policy another. second third assumptions major ones needed analysis. diameter introduced jaksch ortner auer typically needed single-task sample-complexity analysis seems nontrivial avoid lifelong learning setting. without diameter long-horizon assumption learner stuck subset states prevent identifying current mdp. situations unclear learner reliably transfer knowledge better solve future tasks. assumptions main result follows. note possible reﬁned single-task analysis lattimore hutter better constants below. defer future work instead focus showing beneﬁts lifelong learning. theorem algorithm proper choices parameters sequence tasks mdps. then prob. number steps algorithm \u0001-optimal across tasks single-task typically per-task sample complexity least scales linearly algorithm converges per-task sample complexity often much lower. furthermore bound expected sample complexity obtained similar corresponding expected-regret bound theorem intuitively occp setting quantiﬁed loss llrl loss corresponds number non-\u0001-optimal steps loss bound translates directly sample-complexity bound. proof proceeds analyzing sample complexity bounds four possible cases solving combining theorem yield desired results. step ensure probing happens type discovered successfully high probability. achieved couple technical lemmas below also elucidate assumptions used analysis. ﬁrst lemma ensures state–actions visited sufﬁciently often ﬁnite steps small diameter. convenience deﬁne lemma given pac-explore input visit state–action pairs least times steps probability second lemma establishes fact pacexplore sequence tasks high probability successfully infers whether included every result consequence lemma assumption involving lemma input parameters max{γ− algorithm following holds probability every task sequence algorithm detects task corresponding seen before. llrl simulation task randomly sampled models learned nikolaidis domain much larger grid world environment involving states actions. typical personalization problems user types frequency. here chose sampling distribution length expfirst’s initial proving period dominated experiµm ments repeated runs consisting tasks. long probing phase expfirst costly especially total number tasks small since much time spent discovering mdps. shown table forcedexp demonstrates signiﬁcant advantage leveraging past experience much earlier expfirst leading signiﬁcantly higher reward during phase overall course eventually expfirst exhibit near-optimal performance second phase whereas forcedexp continue probing diminishing probability. however forcedexp exhibit substantial jump-start beneﬁt underlying mdps drawn stationary nonuniform distribution. results suggest forcedexp achieves comparabe substantially better performance prior methods especially nonuniform nonstationary llrl problems. paper consider class lifelong problems capture broad range interesting applications. work emphasizes need efﬁcient cross-task exploration unique lifelong learning. novel online coupon-collector problem give optimal algorithms matching upper lower regret bounds. tool develop lifelong algorithm analyze total sample complexity across sequence tasks. theory quantiﬁes much gain obtained lifelong learning compared single-task learning even tasks adversarially generated. algorithm empirically evaluated simulated problems including simulated human-robot collaboration task demonstrating relative strengths compared prior work. future interested extending work llrl continuous mdps. also interesting investigate empirical theoretical properties bayesian approaches thompson sampling lifelong algorithms allow rich information encoded prior distribution empirically often effective taking advantage prior information. simulation results illustrate lifelong setting capture interesting domains demonstrate beneﬁt introduced approach prior algorithm formal sample-complexity guarantees based expfirst. space limitations full details provided appendix gridworld. ﬁrst consider simple stochastic gridworld domain distinct mdps illustrate salient properties forcedexp. mdps corner offers high reward rewards corner rewarding opposite corner bernoulli parameters stochastic setting tasks sampled equal probability compared expfirst forcedexp hmtl—a bayesian hierarchical multi-task algorithm expected approaches well setting. next focus comparing expfirst forcedexpwhich ﬁnite sample guarantees. ﬁrst consider tasks sampled nonstationary distributions. across tasks mdps identical frequencies adversary chooses select mdps ﬁrst phrase expfirst switching tasks switching back randomly selecting ﬁrst three mdps. obtain similar rewards using policy obtain higher rewards agent explicitly explores discover state higher reward. forcedexp randomly probe thus identifying optimal policy eventually picks obtains higher reward expfirst sometimes successfully infers task belongs happens encounter state distinguished mdps illustrates beneﬁt continued active exploration nonstationary adversarial settings. simulated human-robot collaboration. next consider interesting human-robot collaboration problem studied nikolaidis work authors learned models user types based prior data collected paired interaction task human collaborates robot paint box. using types latent state mixed-observability enabled signiﬁcant imlazaric restelli transfer multiple mdps. nips unifying framework computational reinforcement learning theory. ph.d. dissertation rutgers university brunswick koedinger variations learning rate student classiﬁcation based systematic residual error patterns across practice opportunities. edm. mcallester schapire convergence rate good-turing estimators. colt nikolaidis ramakrishnan shah efﬁcient model learning joint-action demonstrations human-robot collaborative tasks. osband russo efﬁcient reinforcement learning posterior sampling. nips ring child ﬁrst step towards continual learning. schmidhuber powerplay training increasingly general problem solver continually searching simplest still unsolvable problem. frontiers psychology strehl littman reinforcement learning ﬁnite mdps analysis. jmlr sutton barto reinforcement learning introduction. cambridge press. taylor stone transfer learning reinforcement learning domains survey. jmlr schelling coupon collecting unequal probabilities. american mathematical monthly white modayil sutton scaling life-long off-policy learning. ieee icdl-epirob wilson fern tadepalli multi-task reinforcement learning hierarchical bayesian approach. icml", "year": 2015}