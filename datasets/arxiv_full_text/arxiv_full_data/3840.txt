{"title": "Deep Biaffine Attention for Neural Dependency Parsing", "tag": ["cs.CL", "cs.NE"], "abstract": "This paper builds off recent work from Kiperwasser & Goldberg (2016) using neural attention in a simple graph-based dependency parser. We use a larger but more thoroughly regularized parser than other recent BiLSTM-based approaches, with biaffine classifiers to predict arcs and labels. Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95.7% UAS and 94.1% LAS on the most popular English PTB dataset. This makes it the highest-performing graph-based parser on this benchmark---outperforming Kiperwasser Goldberg (2016) by 1.8% and 2.2%---and comparable to the highest performing transition-based parser (Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over other graph-based approaches.", "text": "paper builds recent work kiperwasser goldberg using neural attention simple graph-based dependency parser. larger thoroughly regularized parser recent bilstm-based approaches biafﬁne classiﬁers predict arcs labels. parser gets state near state performance standard treebanks different languages achieving popular english dataset. makes highest-performing graph-based parser benchmark— outperforming kiperwasser goldberg .%—and comparable highest performing transition-based parser achieves las. also show hyperparameter choices signiﬁcant effect parsing accuracy allowing achieve large gains graph-based approaches. dependency parsers—which annotate sentences designed easy humans computers alike understand—have found extremely useful sizable number tasks especially involving natural language understanding however frequent incorrect parses severely inhibit ﬁnal performance improving quality dependency parsers needed improvement success downstream tasks. current state-of-the-art transition-based neural dependency parser substantially outperforms many much simpler neural graph-based parsers. modify neural graphbased approach ﬁrst proposed kiperwasser goldberg ways achieve competitive performance build network that’s larger uses regularization; replace traditional mlp-based attention mechanism afﬁne label classiﬁer biafﬁne ones; rather using recurrent states lstm biafﬁne transformations ﬁrst operations reduce dimensionality. furthermore compare models trained different architectures hyperparameters motivate approach empirically. resulting parser maintains simplicity neural graph-based approaches approaching performance sota transition-based one. transition-based parsers—such shift-reduce parsers—parse sentences left right maintaining buffer words parsed stack words whose head seen whose dependents fully parsed. step transition-based parsers access manipulate stack buffer assign arcs word another. train multi-class machine learning classiﬁer features extracted stack buffer previous actions order predict next action. chen manning make ﬁrst successful attempt incorporating deep learning transition-based dependency parser. step network assigns probability action parser take based word label embeddings certain words figure dependency tree parse casey hugged including part-of-speech tags special root token. directed edges labels connect verb root arguments verb head. stack buffer. number researchers attempted address limitations chen manning’s chen manning parser augmenting additional complexity weiss andor augment beam search conditional random ﬁeld loss objective allow parser undo previous actions ﬁnds evidence incorrect; dyer instead lstms represent stack buffer getting state-of-the-art performance building composing parsed phrases together. transition-based parsing processes sentence sequentially build parse tree time. consequently parsers don’t machine learning directly predicting edges; predicting operations transition algorithm. graph-based parsers contrast machine learning assign weight probability possible edge construct maximum spaning tree weighted edges. kiperwasser goldberg present neural graph-based parser uses kind attention mechanism bahdanau machine translation. kiperwasser goldberg’s model lstm’s recurrent output vector word concatenated possible head’s recurrent vector result used input scores resulting arc. predicted tree structure training time word depends highestscoring head. labels generated analogously word’s recurrent output vector gold predicted head word’s recurrent vector used multi-class mlp. similarly hashimoto include graph-based dependency parser multi-task neural model. addition training model multiple distinct objectives replace traditional mlp-based attention mechanism kiperwasser goldberg bilinear makes analogous luong al.’s proposed attention mechanism neural machine translation. cheng likewise propose graph-based neural dependency parser attempts circumvent limitation neural graph-based parsers unable condition scores possible previous parsing decisions. addition bidirectional recurrent network computes recurrent hidden vector word additional unidirectional recurrent networks keep track probabilities previous together predict scores next arc. make modiﬁcations graph-based architectures kiperwasser goldberg hashimoto cheng shown figure biafﬁne attention instead bilinear traditional mlp-based attention; biafﬁne dependency label classiﬁer; apply dimension-reducing mlps recurrent output vector applying biafﬁne transformation. choice biafﬁne rather bilinear mechanisms makes classiﬁers model analogous traditional afﬁne classiﬁers afﬁne transformation single lstm output state predict vector scores classes think proposed biafﬁne attention mechanism traditional afﬁne paper follow convention using lowercase italic letters scalars indices lowercase bold letters vectors uppercase italic letters matrices uppercase bold letters higher order tensors. also maintain notation indexing; matrix would represented figure bilstm deep biafﬁne attention score possible head dependent applied sentence casey hugged kim. reverse order biafﬁne transformation clarity. addition arguably simpler mlp-based approach conceptual advantage directly modu eling prior probability word receiving dependents term analogously also likelihood receiving speciﬁc dependent term biafﬁne classiﬁer predict dependency labels given gold predicted head likewise directly models prior probability class likelihood class given word likelihood class given head word likelihood class given word head applying smaller mlps recurrent output states biafﬁne classiﬁer advantage stripping away information relevant current decision. every recurrent state need carry enough information identify word head dependents exclude non-dependents assign correct label assign dependents correct labels well transfer relevant information recurrent states words thus necessarily contains signiﬁcantly information needed compute individual score training superﬂuous information needlessly reduces parsing speed increases risk overﬁtting. reducing dimensionality applying nonlinearity addresses problems. call deep bilinear attention mechanism opposed shallow bilinear attention uses recurrent states directly. aside architectural differences graph-based parsers make number hyperparameter choices allow outperform theirs laid table -dimensional uncased word vectors vectors; three bilstm layers -dimensional relu layers. also apply dropout every stage model drop words tags drop nodes lstm layers applying dropout mask every recurrent timestep drop nodes layers classiﬁers likewise applying dropout mask every timestep. optimize network annealed adam steps rounded nearest epoch. show test results proposed model english penn treebank converted stanford dependencies using version version stanford dependency converter chinese penn treebank; conll shared task dataset following standard practices dataset. omit punctuation evaluation ptb-sd ctb. english ptb-sd datasets tags generated stanford tagger chinese dataset gold tags; conll dataset provided predicted tags. hyperparameter search done ptb-sd validation dataset order minimize overﬁtting popular ptb-sd benchmark hyperparameter analysis following section report performance ptb-sd test shown tables examined effect different classiﬁer architectures accuracy performance. deep bilinear model outperforms others respect speed accuracy. model shallow bilinear label classiﬁers gets unlabeled performance deep model settings label classiﬁer much larger opposed runs much slower overﬁts. decrease overﬁtting increasing dropout course doesn’t change parsing speed; another decrease recurrent size hinders unlabeled accuracy without increasing parsing speed levels deeper model. also implemented mlp-based approach attention classiﬁcation used kiperwasser goldberg found version compute trained embedding matrix composed words occur least twice training dataset embeddings corresponding pretrained embeddings. words don’t occur either embedding matrix replaced separate token. exclude japanese dataset evaluation access version tensorflow used model’s memory requirements training exceeded available memory single default settings used reduced hidden size also examine closely network size inﬂuences speed accuracy. kiperwasser goldberg’s model network uses layers -dimensional bidirectional lstms; hashimoto al.’s model layer -dimensional bidirectional lstms dedicated parsing cheng al.’s model layer -dimensional cells. using three four layers gets signiﬁcantly better performance layers increasing lstm sizes dimensions likewise signﬁcantly improves performance. cells promoted faster simpler alternative lstm cells used approach cheng however model drastically underperformed lstm cells. also implemented coupled input-forget gate lstm cells suggested greff ﬁnding resulting model still slightly underperforms popular lstm cells difference much smaller. additionally gate candidate cell activations computed simultaneously matrix multiplication cif-lstm model faster version even though number parameters. hypothesize output gate cif-lstm model allows maintain sparse recurrent output state helps adapt high levels dropout needed prevent overﬁtting cells unable increase parser’s power also increase regularization. addition using relatively extreme dropout recurrent layers mentioned table also regularize input layer. drop words tags training dropped scaled factor compensate dropped together model simply gets input zeros. models trained word dropout wind signﬁcantly overﬁtting hindering label accuracy and—in latter case—attachment accuracy. interestingly using tags actually results better performance using tags without dropout. choose optimize adam keeps moving average norm gradient parameter throughout training divides gradient parameter moving average ensuring magnitude gradients average close one. however value recommended kingma controls decay rate moving average—is high task value large magnitude current update heavily inﬂuenced larger magnitude gradients past effect optimizer can’t adapt quickly recent changes model. thus setting instead makes large positive impact ﬁnal performance. model gets nearly performance ptb-sd current sota model kuncoro spite substantially simpler architecture gets sota performance well sota performance conll languages. worth noting conll datasets contain many non-projective dependencies difﬁcult impossible transition-based—but graph-based—parsers predict. account large consistent difference model andor al.’s transition-based model applied datasets. model appears behind sota model indicating possibilities. firstly result inefﬁciencies errors glove embeddings tagger case using alternative pretrained embeddings accurate tagger might improve label classiﬁcation. secondly sota model speciﬁcally designed capture phrasal compositionality; another possibility doesn’t capture compositionality effectively results worse label score. similarly result general limitation graph-based parsers access less explicit syntactic information transition-based parsers making decisions. addressing latter limitations would require innovative architecture relatively simple used current neural graph-based parsers. paper proposed using modiﬁed version bilinear attention neural dependency parser increases parsing speed without hurting performance. showed larger regularized network outperforms neural graph-based parsers gets comparable performance current sota transition-based parser. also provided empirical motivation proposed architecture conﬁguration similar ones existing literature. future work involve exploring ways bridging labeled unlabeled accuracy augment parser smarter handling out-of-vocabulary tokens morphologically richer languages. gabor angeli melvin johnson premkumar christopher manning. leveraging linguistic structure open domain information extraction. proceedings annual meeting association computational linguistics miguel ballesteros yoav goldberg chris dyer noah smith. training exploration improves greedy stack-lstm parser. proceedings conference empirical methods natural language processing danqi chen christopher manning. fast accurate dependency parser using neural networks. proceedings conference empirical methods natural language processing chris dyer miguel ballesteros wang ling austin matthews noah smith. transitionbased dependency parsing stack long short-term memory. proceedings conference empirical methods natural language processing klaus greff rupesh kumar srivastava koutn´ık steunebrink j¨urgen schmidhuber. lstm search space odyssey. ieee transactions neural networks learning systems kazuma hashimoto caiming xiong yoshimasa tsuruoka richard socher. joint many-task model growing neural network multiple tasks. arxiv preprint arxiv. eliyahu kiperwasser yoav goldberg. simple accurate dependency parsing using bidirectional lstm feature representations. transactions association computational linguistics adhiguna kuncoro miguel ballesteros lingpeng kong chris dyer graham neubig noah smith. recurrent neural network grammars learn syntax? corr abs/. http//arxiv.org/abs/.. ankur parikh hoifung poon kristina toutanova. grounded semantic parsing complex knowledge extraction. proceedings north american chapter association computational linguistics kristina toutanova klein christopher manning yoram singer. feature-rich part-ofspeech tagging cyclic dependency network. proceedings conference north american chapter association computational linguistics human language technology-volume association computational linguistics david weiss chris alberti michael collins slav petrov. structured training neural network transition-based parsing. annual meeting association computational linguistics", "year": 2016}