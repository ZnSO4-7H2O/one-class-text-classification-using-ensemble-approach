{"title": "Recovery guarantees for exemplar-based clustering", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "For a certain class of distributions, we prove that the linear programming relaxation of $k$-medoids clustering---a variant of $k$-means clustering where means are replaced by exemplars from within the dataset---distinguishes points drawn from nonoverlapping balls with high probability once the number of points drawn and the separation distance between any two balls are sufficiently large. Our results hold in the nontrivial regime where the separation distance is small enough that points drawn from different balls may be closer to each other than points drawn from the same ball; in this case, clustering by thresholding pairwise distances between points can fail. We also exhibit numerical evidence of high-probability recovery in a substantially more permissive regime.", "text": "certain class distributions prove linear programming relaxation kmedoids clustering—a variant k-means clustering means replaced exemplars within dataset—distinguishes points drawn nonoverlapping balls high probability number points drawn separation distance balls suﬃciently large. results hold nontrivial regime separation distance small enough points drawn diﬀerent balls closer points drawn ball; case clustering thresholding pairwise distances points fail. also exhibit numerical evidence high-probability recovery substantially permissive regime. consider collection points euclidean space forms roughly isotropic clusters. centroid given cluster found averaging position vectors points medoid exemplar point within collection best represents cluster. distinguish clusters popular pursue k-means objective partition points clusters average squared distance point cluster centroid minimized. problem general np-hard further obvious convex relaxation could recover global optimum admitting eﬃcient solution; practical algorithms like lloyd’s hartigan-wong typically converge local optima. k-medoids clustering also general np-hard admit linear programming relaxation. objective select points medoids average squared distance point medoid minimized. paper obtains guarantees exact recovery unique globally optimal solution k-medoids integer program relaxation. commonly used algorithms converge local optima include partitioning around medoids aﬃnity propagation illustrate diﬀerence centroid medoid faces points. yale face database grayscale images several faces captured wearing range expressions—normal happy sleepy surprised winking. suppose every point encodes image database vector pixel values. intuitively facial expressions represent perturbations background composed distinguishing image features; thus natural expect faces cluster individual rather expression. lloyd’s algorithm aﬃnity propagation shown recover partitioning figure also displays centroids using aﬃnity propagation lloyd’s algorithm. medoids identiﬁed aﬃnity propagation representative faces clusters centroids found lloyd’s algorithm averaged faces. medoids clusters. centroids averaged faces medoids actual faces dataset. indeed applications k-medoids clustering numerous diverse besides ﬁnding representative faces gallery images group tumor samples gene expression levels pinpoint inﬂuencers social network vertices although recovery guarantees proved case vertices correspond points euclidean space edge weight squared distance points connects. characters boldface refer matrices/vectors italicized counterparts subscripts refer matrix/vector elements. denote nonnegative weight edge connecting vertices note since simple. k-medoids clustering ﬁnds minimum-weight bipartite subgraph every vertex unit degree. vertices medoids. expressed among edges medoids edge smallest weight. otherwise cluster identiﬁed maximal vertices share given medoid. like many clustering programs kmed general np-hard thus computationally intractable large replacing binary constraints nonnegativity constraints obtain linear program relaxation linkmed points one-dimensional euclidean space relaxation k-medoids clustering invariably recovers clusters unsquared distances used measure dissimilarities points suppose squared distances used measure dissimilarities points exist values following statement holds probability exceeding optimal solution k-medoids clustering unique agrees unique optimal solution assigns points ball cluster. remark uniform distribution satisﬁes dimension distribution satisfying concentrates probability mass towards center ball. means recovery results theorem stronger smaller however applying random projection remark centers unit balls separated distance points within ball necessarily closer distance points diﬀerent balls. kmedoid problem cluster recovery guarantees regime given authors aware theorem provides ﬁrst recovery guarantees k-medoids beyond regime. spirit tradition compressed sensing community sought probabilistic recovery guarantees convex relaxations nonconvex problems. reference presents guarantees densest k-clique problem partition complete weighted graph disjoint cliques average edge weights minimized. also notable recovery guarantees correlation clustering variants. correlation clustering outputs partitioning vertices complete graph whose edges papers mentioned previous paragraph probabilistic recovery guarantees apply stochastic block model generalizations. consider graph vertices initially without edges. partition vertices clusters. stochastic block model random model draws edge graph model edge weights drawn independently include graphs represent points drawn independently metric space. graphs edge weights interdependent distances. recent paper builds derive probabilistic recovery guarantees subspace clustering union subspaces lies closest points. problem trivial overlap ours; exemplars zero-dimensional hyperplanes close clustered points zero-dimensional subspace rd—the origin. reference hand introduces tractable convex program medoids. program recast dualized form k-medoids clustering. however deterministic guarantee applies case clusters recoverable thresholding pairwise distances; points cluster must closer points diﬀerent clusters. probabilistic guarantees include regime thresholding fail. speciﬁes regularization parameter objective function must lower critical value medoids recovered. essentially dual variable associated k-medoids remains unchosen karush-kuhn-tucker conditions used derive guarantee number medoids obtained thus unspeciﬁed. contrast guarantee recovery speciﬁc number medoids. antees random model points drawn isotropic distributions supported nonoverlapping balls. steps removed gaussian mixture model. starting work dasgupta several papers already report probabilistic recovery guarantees learning parameters gaussian mixture models using algorithms unrelated convex programming. hard clusters found obtaining parameters associating point gaussian whose contribution mixture model largest questions diﬀerent ours conditions given polynomial-time algorithm—not convex program admits many algorithmic solution techniques—recover global optimum? close parameters obtained true values? progression line research towards reducing separation distances centers gaussians guarantees; fact separation distances zero covariance matrices gaussians diﬀer results intended compete guarantees. rather seek provide complementary insights often clusters points euclidean space recovered tioned above conﬁguration points one-dimensional euclidean space relaxation k-medoids clustering exactly recovers medoids dissimilarities unsquared distances dimension nonintegral optima whose costs lower optimal integral solution realized. large literature approximation algorithms k-medoids clustering based rounding solution methods. literature encompasses family related problems known facility location. diﬀerences uncapacitated facility location problem k-medoids clustering certain points allowed medoids constraint number clusters cost associated choosing given point medoid. constant-factor approximation algorithms obtained metric ﬂavors k-medoids clustering measures distance points used objective function must obey triangle inequality. reference obtains ﬁrst polynomial-time approximation algorithm metric ufl; comes within factor optimum. several subsequent works give algorithms improve approximation ratio established unless dtime) lower bounds approximation ratios metric metric k-medoids clustering respectively here solution unpublished work sviridenko strengthens complexity criterion lower bounds best known approximation ratios metric metric k-medoids clustering respectively algorithm available since still large current best approximation ratio k-medoids clustering theoretical limit ﬁnding novel approximation algorithms remains active area research. along related lines recent paper gives ﬁrst constant-factor approximation algorithm generalization k-medoids clustering medoid assigned point. emphasize results recovery guarantees; instead ﬁnding novel rounding scheme solutions give precise conditions solving yields kmedoids clustering. addition proofs squared distances respect triangle inequality. next section paper uses linear programming duality theory derive suﬃcient conditions optimal solution k-medoids integer program kmed coincides unique optimal solution linear programming relaxation linkmed. third section obtains probabilistic guarantees exact recovery integer solution linear program focusing recovering clusters points drawn separated balls equal radius. numerical experiments demonstrating eﬃcacy linear programming approach recovering clusters beyond analytical results reviewed fourth section. ﬁnal section discusses open questions appendix contains proofs. index vertex medoid argminj∈mj=m wij. points euclidean space index second-closest medoid point simplicity presentation take medoid. denote points whose medoid indexed refer positive part term enclosed parentheses. begin writing necessary suﬃcient condition unique integral solution linkmed. proposition rewrites karush-kuhn-tucker conditions corresponding linear program linkmed convenient way; refer appendix derivation. number points cluster point choose u/ni obtain following tractable suﬃcient condition medoid recovery. remark choose points within balls radius centers balls separated distance least measure units ball’s radius setting take distance points inequality satisﬁed assigning points chosen ball cluster. here nmax nmin maximum minimum numbers points drawn balls respectively. limit becomes proof. impose points cluster points condition holds deﬁnition medoid unless optimal solution kmed unique. event possible nonmedoid medoid cluster trade roles maintaining solution optimality making vanish phrasing corollary accommodates edge case. corollary illustrate utility solving kmed. given conditions recovery guarantee clustering could performed without using distance threshold place points cluster distance smaller ensure points diﬀerent clusters distance greater separated balls model remark guarantees points ball closer points diﬀerent balls. next corollary needed obtain results corollary imposes extra upper bounds extra lower bounds proof. points diﬀerent clusters closer points cluster cannot simultaneously satisfy upper lower bounds. break thresholding barrier corollary imposes extra lower bounds permits large stronger recovery guarantees obtained large medoids sparsely distributed among points. next subsection obtains probabilistic guarantees using corollary variant separated balls model remark theorem stated introduction proved section. consider nonoverlapping ddimensional unit balls euclidean space centers balls separated distance least take squared distance mild assumption points drawn within ball exact recovery guarantee remark extended subsection regime points cluster necessarily closer points diﬀerent clusters. particular points ball correspond independent samples isotropic distribution supported ball obeys above vector extending center ball given point refers norm dimensions assumption holds uniform distribution supported ball. larger requires distributions concentrate probability mass closer ball’s center. simplicity assume sequel number points drawn ball equal. denote expectation variance. state preliminary lemma. above xmin) xmin. since drawn isotropic distribution direction unit vector xmin) xmin independent xmin drawn uniformly random. follows i.i.d. zero-mean random variables despite xmin depends indeed follows statement holds probability exceeding e−α. moreover statements together hold probability exceeding ne−α. condition them prove statement contradiction suppose exceeds α−/. xmin αn−/ suppose squared distances used measure dissimilarities points exist values following statement holds probability exceeding unique optimal solution k-medoids clustering linear bounds further dimension exists ﬁnite large enough satisﬁed. similar checks performed obtain valid combinations parameters; combinations contained remark proof above sole events conditioned lemma holds cluster hoeﬀding inequality nouter holds cluster. recorded theorem probability events occur exceeds k/n. components theorem euclidean distance center ball vector uniform distribution. equation saturates inequality distributional assumption probabilistic recovery guarantees. figure failed ball recovery clusters point left ball placed cluster points ball. failed ball recovery three clusters four points bottom right ball placed cluster points ball. remarkably cluster recovery failed times across sets simulations case therefore appears high-probability cluster recovery always realized drawing samples distributions consider. however since conditions require assumption points cluster general cluster recovery guarantees diﬃcult prove. previous section obtain guarantees assuming points cluster balls drawn. ball recovery results simulations cases depicted respectively figures note vertical axis plot measures number failed ball recoveries. conclude section following observations. exception case concentrates probability mass towards centers balls case typically making points drawn ball cluster tightly. plots figures correspond draws uniform distributions supported balls; repetitions thus look essentially same. drawn diﬀerent balls tend apart. high-probability ball recovery appears guaranteed greater somewhere even small values considered here. considerably better guarantee theorem holds least shown toward proof. ﬁxed ball recoveries balls three balls. uniform distributions case thus substantial room improving recovery guarantees require concentrating probability mass towards centers balls increases. figure exact recovery appears guaranteed high probability uniform distributions values double digits values substantially better theorem suggests. proved high probability k-medoids clustering problem relaxation share unique globally optimal solution nontrivial regime points cluster apart points diﬀerent clusters. however theoretical guarantees preliminary; fall short explaining success distinguishing points drawn diﬀerent balls small separation distance points ball. generally simulations present here k-medoids relaxation appeared recover integer solutions extreme conﬁgurations points—in presence extreme outliers well nonisotropic clusters vastly diﬀerent numbers points. thus conclude open questions interest thank chris white helpful suggestions. extremely grateful sujay sanghavi oﬀering expertise clustering pointing right directions navigated literature. a.n. especially grateful song constructive suggestions general support preparation work. r.w. supported part research fellowship alfred sloan foundation grant n--- career award afosr young investigator program award. a.n. supported song’s grant national institutes health.", "year": 2013}