{"title": "AFT*: Integrating Active Learning and Transfer Learning to Reduce  Annotation Efforts", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "The splendid success of convolutional neural networks (CNNs) in computer vision is largely attributed to the availability of large annotated datasets, such as ImageNet and Places. However, in biomedical imaging, it is very challenging to create such large annotated datasets, as annotating biomedical images is not only tedious, laborious, and time consuming, but also demanding of costly, specialty-oriented skills, which are not easily accessible. To dramatically reduce annotation cost, this paper presents a novel method to naturally integrate active learning and transfer learning (fine-tuning) into a single framework, called AFT*, which starts directly with a pre-trained CNN to seek \"worthy\" samples for annotation and gradually enhance the (fine-tuned) CNN via continuous fine-tuning. We have evaluated our method in three distinct biomedical imaging applications, demonstrating that it can cut the annotation cost by at least half, in comparison with the state-of-the-art method. This performance is attributed to the several advantages derived from the advanced active, continuous learning capability of our method. Although AFT* was initially conceived in the context of computer-aided diagnosis in biomedical imaging, it is generic and applicable to many tasks in computer vision and image analysis; we illustrate the key ideas behind AFT* with the Places database for scene interpretation in natural images.", "text": "abstract—the splendid success convolutional neural networks computer vision largely attributed availability large annotated datasets imagenet places. however biomedical imaging challenging create large annotated datasets annotating biomedical images tedious laborious time consuming also demanding costly specialty-oriented skills easily accessible. dramatically reduce annotation cost paper presents novel method naturally integrate active learning transfer learning single framework called aft∗ starts directly pre-trained seek worthy samples annotation gradually enhance continuous ﬁne-tuning. evaluated method three distinct biomedical imaging applications demonstrating annotation cost least half comparison random selection. performance attributed several advantages derived advanced active continuous learning capability method including starting completely empty labeled dataset requiring labeled seed samples; incrementally improving learner continuous ﬁne-tuning rather repeatedly re-training; actively selecting informative representative candidates naturally exploiting expected consistency among patches within candidate; computing selection criteria locally small number patches within candidates saving computation time considerably; automatically handling noisy labels majority selection; autonomously balancing training samples among classes; combining newly selected candidates misclassiﬁed candidates eliminate easy samples training efﬁciency focus hard samples preventing catastrophic forgetting; incorporating randomness active selection strike balance exploration exploitation. although aft∗ initially conceived context computer-aided diagnosis biomedical imaging generic applicable many tasks computer vision image analysis; illustrate ideas behind aft∗ places database scene interpretation natural images. index terms—active learning annotation cost reduction convolutional neural networks computer-aided diagnosis tuning ground truth creation labeling biomedical images medical image analysis medical image computing transfer learning brought revolution computer vision thanks large annotated datasets imagenet places evidenced ieee special issue recent books intense interest applying cnns biomedical image analysis widespread success impeded lack large annotated datasets biomedical imaging. annotating biomedical images tedious time consuming also demanding costly specialty-oriented knowledge skills easily accessible. therefore seek answer critical question dramatically reduce cost annotation applying cnns biomedical imaging well accompanying question given labeled dataset determine sufﬁciency covering variations objects interest. present novel method called shorter version zhou shin zhang gurudu gotway liang fine-tuning convolutional neural networks biomedical image analysis actively incrementally ieee conference computer vision pattern recognition zhou shin liang department biomedical informatics arizona state university scottsdale usa. aft∗ naturally integrate active learning transfer learning single framework. aft∗ method starts directly pre-trained seek salient samples unannotated annotation continuously ﬁne-tuned newly annotated samples combined misclassiﬁed samples. evaluated method three different applications including colonoscopy frame classiﬁcation polyp detection pulmonary embolism detection demonstrating cost annotation least half. outstanding performance attributed simple powerful observation boost performance cnns biomedical imaging multiple patches usually generated automatically candidate data augmentation; patches generated candidate share label naturally expected similar predictions current expanded training dataset. result entropy diversity provide useful indicator power candidate elevating performance current cnn. however automatic data augmentation inevitably generates hard samples candidates injecting noisy labels; therefore signiﬁcantly enhance robustness method compute entropy diversity selecting portion patches candidate according predictions current cnn. several researchers demonstrated utility ﬁnealgorithm aft∗ active continuous ﬁne-tuning hybrid data inputu {ci} {unlabeled pool contains candidates} pre-trained cnn; majority selection ratio; batch size; category labeled candidates; ﬁne-tuned model step sort according descending order compute sampling probability using sorted list associate labels candidates sampling probabilities {outputs given select misclassiﬁed candidates based annotation tuning cnns biomedical image analysis performed one-time ﬁne-tuning simply ﬁne-tuning pre-trained available training samples involving active selection processes knowledge proposed method among ﬁrst integrate active learning ﬁne-tuning cnns continuous fashion make cnns amicable biomedical image analysis annotation cost dramatically. compared conventional active learning method summarized alg. offers eight advantages actively selecting informative representative candidates naturally exploiting expected consistency among patches within candidate computing selection criteria locally small number patches within candidates saving computation time considerably combining newly selected candidates misclassiﬁed candidates eliminating easy samples improve training efﬁciency focusing hard samples prevent catastrophic forgetting fig. illustrate ideas behind aft∗ utilizing places scene interpretation natural images. simplicity without loss generality limit categories kitchen living room ofﬁce. places images category while imagine possible layouts appearances kitchen living room ofﬁce vary dramatically real world. systems deployed closed environment results reviewed errors corrected radiologists; result false positives supposed dismissed false negatives supplied instant on-line feedback make systems self-learning improving possible deployment given continuous ﬁne-tuning capability method. fig. referring alg. aft∗ aims annotation cost recommending informative representative samples iteratively experts label minimize number labeled samples required achieve satisfactory performance. places actively selecting images aft∗ offer performance images random selection terms thus saving annotation cost relative rft. furthermore actively-selected images aft∗ reach equivalent performance full training images; thereby saving annotation cost relative full training. sec. perform thorough experiments demonstrating that small subset labeled samples aft∗ approximately achieve performance full training tuning using samples three distinct medical imaging applications including colonoscopy frame classiﬁcation polyp detection pulmonary embolism detection however places aft∗ keeps improving performance images available places used indicating places database still small—this comes surprise given images category places relative large variations layouts appearances kitchen living room ofﬁce shown fig. therefore aft∗ suggest samples worthy labeling also help determine labeled dataset sufﬁcient covering variations objects interest. please note that following standard active learning experimental setup aft∗ select samples remaining training dataset; eventually whole training dataset naturally yielding similar performance end. however goal active learning sweet spots learner achieve acceptable performance minimal number labeled samples. transfer learning medical imaging gustavo replaced fully connected layers pre-trained logistic layer trained appended layer labeled data keeping rest network same yielding promising results classiﬁcation unregistered multiview mammograms. ﬁne-tuned pre-trained applied localizing standard planes ultrasound images. ﬁne-tuned layers pre-trained automatic classiﬁcation shin used ﬁne-tuned pre-trained cnns automatically medical images document-level topics documentlevel sub-topics sentence-level topics. ﬁnetuned pre-trained cnns used automatically retrieve missing noisy cardiac acquisition plane information magnetic resonance imaging predict common cardiac views. schlegl explored unsupervised pre-training cnns inject information sites image classes annotations available showed across site pre-training improved classiﬁcation accuracy compared random initialization model parameters. tajbakhsh systematically investigated capabilities transfer learning several medical imaging applications. however performed one-time ﬁne-tuning—simply ﬁne-tuning pre-trained integrating active learning deep learning research integrating active learning deep learning sparse wang shang ﬁrst incorporate active learning deep learning based approach stacked restricted boltzmann machines stacked autoencoders. similar idea reported hyperspectral image classiﬁcation stark applied active learning improve performance cnns captcha recognition rahhal exploited deep learning active electrocardiogram classiﬁcation. recently yang presented active learning framework reduce annotation effort judiciously suggesting effective annotation areas segmentation based uncertainty similarity information provided fully convolutional networks approach expensive computation need train models scratch bootstrapping order compute uncertainty measure based models’ disagreements must solve generalized version maximum cover problem np-hard determined representative areas. word aforementioned approaches fundamentally different aft∗ step repeatedly re-trained learner scratch fig. illustrated images augmented image patches arranged according predictions dominant category step intuitively image would contribute little boosting current cnn’s performance predictions augmented patches highly certain consistent; naturally entropy diversity augmented patches provide useful indicator power elevating current cnn. however automatic data augmentation inevitably generates hard samples need classify conﬁdently intermediate stages; therefore select patches highest predictions dominant category computing entropy diversity. found works well across three distinct medical imaging applications. case entropy diversity images respectively show image uncertain diverse image therefore worthy labeling. matter fact label living room places; thus augmented patches classiﬁed mostly wrong current including training great value. comparison image labeled ofﬁce current classiﬁes augmented patches ofﬁce high conﬁdence; thereby labeling would prove fruitless. note computing entropy diversity entire augmented patches yields images image would mislead selection indicates images close entropy image diverse image therefore majority selection presented sec. critical component aft∗. continuously ﬁne-tune incremental manner offering several advantages listed sec. leading dramatic annotation cost reduction computation efﬁciency. work presented method integrated active learning deep learning continuous ﬁne-tuning cvpr paper limited binary classiﬁcations biomedical imaging used labeled samples available step thereby demanding long training time large computer memory. paper signiﬁcant extension cvpr paper making several contributions generalizing binary classiﬁcation multi-class classiﬁcation; extending computer-aided diagnosis medical imaging scene interpretation natural images; combining newly selected samples hard samples eliminate easy samples reducing training time concentrate hard samples preventing catastrophic forgetting; injecting randomness enhance robustness active selection; experimenting extensively reasonable combinations data models search optimal strategy; demonstrating consistent annotation reduction different architectures; illustratively explaining active selection process gallery patches associated predictions. proposed method aft∗ conceived context computer-aided diagnosis biomedical imaging. system typically candidate generator quickly produce candidates among which true positives false positives. candidate generation task train classiﬁer eliminate many possible false positives keeping many possible true positives. train classiﬁer candidates must labeled. assume candidate takes illustrates case binary classiﬁcation. assuming prediction patch call histogram prediction pattern candidate shown table binary classiﬁcation seven typical prediction patterns pattern patches’ predictions mostly concentrated higher degree uncertainty. active learning algorithms favor type candidates good reducing uncertainty. pattern ﬂatter pattern patches’ predictions spread widely higher degree inconsistency among patches’ predictions. since patches belonging candidate generated data argumentation expected similar predictions. type candidates potential contribute signiﬁcantly enhancing current cnn’s performance. pattern patches’ predictions clustered ends higher degree diversity. type candidates likely associated noise labels patch level illustrated fig. least favorable active selection cause confusion ﬁne-tuning cnn. patterns patches’ predictions clustered either higher degree certainty. type candidates postponed annotating stage mostly likely current predicted correctly would contribute little ﬁne-tuning current cnn. patterns higher degree certainty patches’ predictions associated outliers patches’ predictions. type candidates valuable capable smoothly improving cnn’s performance. might make dramatic contributions would cause signiﬁcant harms enhancing cnn’s performance. active learning develop criterion determining worthiness candidate annotation. criterion based simple powerful observation patches augmented candidate share label; expected similar predictions current cnn. result entropy diversity provide useful indicator power candidate elevating performance current cnn. intuitively entropy captures classiﬁcation certainty— higher uncertainty value denotes higher degree information diversity indicates prediction consistency among patches candidate—a higher diversity value denotes higher degree prediction inconsistency formally assuming candidate takes possible labels deﬁne entropy table relationships among seven prediction patterns four methods active candidate selection. assume candidate patches probabilities predicted current listed entropyα diversityα operate candidate’s patches based prediction dominant category described sec. illustration choose meaning selection criteria computed based patches within candidate. ﬁrst choice method highlighted blue second choice light blue. combing entropy diversity would highly desirable striking balance trivial demands applicationspeciﬁc requires research. possible labels. boost performance cnns systems multiple patches usually generated automatically candidate data augmentation; patches generated candidate inherit candidate’s label. words labels acquired candidate level. mathematically given candidates ...cn} number candidates candidate associated patches aft∗ algorithm iteratively selects candidates labeling illustrated alg. however aft∗ generic applicable many tasks computer vision image analysis. clarity illustrate ideas behind aft∗ places database scene interpretation natural images candidate generator needed image directly regarded candidate. simplicity illustration without loss generality limit three categories divide places images category training validation test overlaps. designing active learning algorithm involves issues determine worthiness candidate annotation update classiﬁer/learner. formally addressing ﬁrst issue secs. second sec. illustrate ideas sec. fig. tab. injecting randomization active selection discussed simple random selection outperform active selection beginning active selection depends current model select examples labeling; result poor selection made early stage adversely affect quality subsequent selections; random selection gets less frequently locked poor hypothesis. words active selection concentrates exploiting knowledge gained labels already acquired explore decision boundary random selection concentrates exploration able locate areas feature space classiﬁer performs poorly. therefore effective active learning strategy must balance exploration exploitation. inject randomization method selecting actively according sampling probability sorted according value descending order named random extension. suppose number candidates required annotation. instead selecting candidates extend candidate selection pool select candidates pool sampling probabilities comparing various learning strategies discussion above several active learning strategies derived summarized tab. comprehensive comparisons show unstable; needs careful parameter adjustment; reliable comparison requires ﬁne-tune original model beginning using presently-available-labeled develop optimized version aft∗ continuously ﬁne-tunes current model using newly annotated candidates enlarged misclassiﬁed candidates tuning offers better performance robust training scratch. moreover experiments show aft∗ saves training time converging faster repeatedly ﬁne-tuning original pre-trained boosts performance eliminating easy samples focusing hard samples preventing catastrophic forgetting. fig. compares aft∗ using places database. different sequences generated systematic random sampling. ﬁnal curve plotted average performance runs. shown fig. aft∗ candidate queries achieve performance candidate queries terms while candidate queries achieve performance full training candidates. thereby labeling cost could saved labeling cost could saved full training. nearly training data used performance still keeps handling noise labels majority selection automatic data augmentation essential boost cnn’s performance inevitably generates hard samples candidates shown fig. injecting noisy labels; therefore signiﬁcantly enhance robustness method compute entropy diversity selecting portion patches candidate according predictions current cnn. specially candidate ﬁrst determine dominant category deﬁned category highest conﬁdence mean prediction outputs current given label sorting according dominant category apply patches constructing score matrix size candidate proposed majority selection method automatically excludes patches noisy labels conﬁdences. note idea combining entropy diversity inspired fundamental difference computed across whole unlabeled dataset time complexity computational expensive compute locally selected patches within candidate saving computation time considerably time complexity experiments. matter fact computationally infeasible apply method three real-world applications selection criteria involves unlabeled samples instance training patches polyp detection computing would demand memory algorithms batch selection based truncated power method unable solution even smallest application furthermore conventional method advantages listed sec. speciﬁcally used base classiﬁer cannot effectively start completely empty labeled dataset cannot incrementally improve learner continuous ﬁne-tuning. concept candidate cannot exploit expected consistency among patches within candidate active selection cannot automatically handle noisy labels using majority selection. selection active active active active random active diversity diversityα diversityω diversityα labeled candidates. newly selected candidates. misclassiﬁed candidates. pre-trained lenet alexnet resnet densenet etc. increasing; therefore considering layers googlenet architecture size dataset still enough. aft∗ general algorithm useful biomedical dataset also datasets; aft∗ works multi-class problems. applications apply methods aft∗ three applications including colonoscopy frame classiﬁcation polyp detection pulmonary embolism detection. terms active selection criteria eight strategies compared experiment i.e. diversity diversityα diversityω diversityα entropy entropyα entropyω entropyα applications tajbakhsh reported state-ofthe-art performance ﬁne-tuning learning scratch based whole datasets used baseline performance comparison. also investigated performance ﬁne-tuning sequence partial training datasets partition datasets different; therefore fair comparison idea introduce ﬁne-tunes original model beginning using presently-availablestep. three applications aft∗ begins empty training dataset directly uses pre-trained models imagenet. colonoscopy frame classiﬁcation image quality assessment major role objective quality assessment colonoscopy procedures. typically colonoscopy video contains large number noninformative images poor colon visualization suitable inspecting colon performing therapeutic actions. larger fraction non-informative images video lower quality colon visualization thus lower quality colonoscopy. therefore measure quality colonoscopy procedure monitor quality captured images. quality assessment used live procedures limit low-quality examinations post-processing setting quality monitoring purposes. technically image quality assessment colonoscopy viewed image classiﬁcation task whereby input image labeled either informative non-informative. fig. shows examples informative non-informative colonoscopy frames. fig. three examples colonoscopy frames informative non-informative ambiguous labeled informative experts label frames based overall quality frame clear considered informative. result ambiguous candidate contains clear blur parts generates noise labels patch level automatic data argumentation. example whole frame labeled informative patches associated frames informative although inherit informative label. main motivation majority selection aft∗ method. frames illustrated fig. total colonoscopy candidates complete colonoscopy videos. trained expert manually labeled collected images informative non-informative gastroenterologist reviewed labeled images corrections. labeled frames video level separated training test sets containing approximately colonoscopy frames. data augmentation extracted patches frame. fig. shows aft∗ around candidate queries achieve performance training dataset ﬁne-tune alexnet candidate queries achieve performance training dataset learning scratch candidate queries achieve performance candidate queries. thereby around labeling cost could save colonoscopy frame classiﬁcation. early steps yields great performance active selecting strategies random selection gives samples positive-negative ratio compatible testing validation dataset; pre-trained model gives poor predictions biomedical image domain trained natural images. output probabilities mostly confused even opposite yielding poor selection scores. however randomness injected described sec. aft∗−diversityα aft∗−entropyα performance better even early stages increase fig. comparison eight active selection strategies three medical applications including colonoscopy frame classiﬁcation polyp detection pulmonary embolism detection testing dataset using alexnet. solid black line current state-of-the-art performance ﬁne-tuning using full training data dashed black line performance training scratch using full training data. polyp detection colonoscopy preferred technique colon cancer screening prevention. goal colonoscopy remove colonic polyps—precursors colon cancer. polyps shown fig. appear substantial variations color shape size. challenging appearance polyps often lead misdetection particularly long back-to-back colonoscopy procedures fatigue negatively affects performance colonoscopists. polyp miss-rates estimated however recent clinical study suggestive misdetection rate high missed polyps lead late diagnosis colon cancer associated decreased survival rate less metastatic colon cancer computer-aided polyp detection enhance optical colonoscopy screening reducing polyp misdetection. dataset contains patients video each. training dataset composed videos testing dataset composed videos application polyp candidate regarded candidate. video level candidates divided training dataset testing dataset candidates). candidate contains patches. scale extracted patches candidate translated percent resized bounding vertical horizontal directions. rotated resulting patch times mirroring ﬂipping. patches generated data augmentation belong candidate. fig. shows aft∗ around candidate queries achieve performance training dataset ﬁne-tune alexnet candidate queries achieve performance training dataset learning scratch candidate queries achieve performance candidate queries. thereby nearly labeling cost could save polyp detection. fast convergence outstanding performance aft∗ attributed majority randomization method efﬁciently select informative representative candidates excluding noisy labels boost initial performance early stages. diversity strongly favors candidates whose prediction pattern resembles pattern works even poorer noisy labels generated data augmentation. pulmonary embolism detection pulmonary embolism major national health problem computer-aid detection improve diagnostic capabilities radiologists. blood clot travels lower extremity source lung causes blockage pulmonary arteries. mortality rate untreated approach decreases early diagnosis appropriate treatment pulmonary angiography primary means diagnosis wherein radiologist carefully traces branch pulmonary artery suspected pes. ctpa interpretation time-consuming task whose accuracy depends human factors attention span sensitivity visual characteristics pes. major role improving diagnosis decreasing reading time ctpa datasets. fig. candidates selected four aft∗ methods step colonoscopy frame classiﬁcation. positive candidates negative candidates blue. aft∗−diversityα prefers pattern aft∗−diversity suggests pattern aft∗−entropy aft∗−entropyα favor pattern higher degrees uncertainty. however case step aft∗−entropyα candidates pattern therefore candidates pattern selected. optimized version aft∗ shares comparable performance outperforms occasionally eliminating easy samples focusing hard samples preventing catastrophic forgetting. majority selection automatically handle noisy labels overcome drawback diversity especially colonoscopy frame classiﬁcation severe noisy labels shown fig. majority selection involves fewer patches within annotation unit saves computation time. fig. five different standard -channel representation well -channel representation adopted work achieves greater classiﬁcation accuracy accelerates training convergence. ﬁgure used permission. candidate patches. divided candidates patient level training dataset true positives false positives testing dataset true positives false positives. overall probability calculated averaging probabilistic prediction generated patches within candidate data augmentation. fig. shows aft∗ candidate queries almost achieve performance training dataset ﬁne-tune alexnet learning scratch candidate queries achieve performance candidate queries. based analysis cost annotation least half method pulmonary embolism detection. comparison methods summarized tab. several active learning strategies derived. prediction performance evaluated according learning curve plots computed testing dataset function number labels queried tab. shows aft∗ comparing rft. comprehensive experiments demonstrated that active ﬁne-tuning methods using entropy diversity majority randomization selection achieve better performance random selection. using newly selected candidates ﬁne-tuning current model unstable pretrained samples forgotten classiﬁer trained newly selected samples along steps leading lower alc. requires careful parameter adjustment. although performance acceptable requires computing time meaning advantage continuously ﬁne-tuning current model. maintains reliable performance comparison aft. table comparison methods. baseline performance colonoscopy frame classiﬁcation polyp detection pulmonary embolism detection using alexnet. bolded values indicate outstanding learning strategies using certain active selection criteria values represent best performance taken learning strategies active selection criteria consideration. patterns dominant earlier stages aft∗ ﬁne-tuned properly target domain. patterns dominant later stages aft∗ largely ﬁne-tuned target dataset. diversityα prefers pattern diversity prefers pattern diversity cause sudden disturbances cnn’s performance diversityα preferred general. addition impression newly selected images look like show bottom images selected four active selection strategies places step fig. associated predictions current tab. gallery offers intuitive analyze most/least favored images helped develop different active selection strategies. real-world applications datasets usually unbalanced. order achieve good classiﬁcation performance better balance training dataset terms classes. random selection ratio roughly whole training dataset. observe active learning methods aft∗ capable making selected training dataset balanced automatically. datasets unbalanced—more negatives fewer positives. shown fig. colonoscopy frame classiﬁcation ratio positives negatives around polyp detection pulmonary embolism detection ratios around monitoring active selection process generalizability aft∗ architectures based experiments alexnet architecture offers nice balance depth deep enough investigate impact aft∗ performance pre-trained cnns also shallow enough conduct experiments quickly. learning parameters used training ﬁne-tuning alexnet experiments summarized tab. alternatively deeper architectures googlenet resnet densenet could used shown relatively high performance challenging computer vision tasks. however purpose work achieve highest performance different biomedical image tasks answer critical question dramatically reduce cost annotation applying cnns biomedical imaging. purposes alexnet reasonable architectural choice. nevertheless experimented three applications googlenet demonstrating consistent patterns alexnet googlenet shown figs. result given generalizability could focus comparing prediction patterns learning strategies instead running experiments various deep neural network architectures. fig. comparing aft∗ googlenet three applications including colonoscopy frame classiﬁcation polyp detection pulmonary embolism detection demonstrates consistent patterns alexnet table learning parameters used training ﬁne-tuning alexnet experiments. momentum learning rate weights last layer learning rate weights rest layers determines decreases epochs. epochs indicates number epochs used step. aft∗ parameters except learning rate aft. conclusion paper developed novel method dramatically cutting annotation cost integrating active learning transfer learning. comparison stateof-the-art method method annotation cost least half three biomedical applications nature image database relative random selection. performance attributed eight advantages associated method choose select classify label samples candidate level. labeling patient level would certainly reduce cost annotation introduce severe label noise; labeling patch level would cope label noise impose much heavier burden experts annotation. believe labeling candidate level offers sensible balance three applications. acknowledgments research supported partially award number mayo clinic seed grant innovation grant. content solely responsibility authors necessarily represent ofﬁcial views nih. greenspan ginneken summers guest editorial deep learning medical imaging overview future promise exciting technique ieee transactions medical imaging vol. zheng carneiro yang deep learning convolutional neural networks medical image computing precision medicine high performance large-scale datasets. springer chen j.-z. cheng p.-a. heng automatic fetal ultrasound standard plane detection using knowledge transferred recurrent neural networks international conference medical image computing computer-assisted intervention. springer schlegl ofner langs unsupervised pre-training across image domains improves lung tissue classiﬁcation medical computer vision algorithms data. springer chen yang wang heng standard plane localization fetal ultrasound domain transferred deep neural networks biomedical health informatics ieee journal vol. sept nascimento bradley unregistered multiview mammogram analysis pre-trained deep learning models medical image computing computer-assisted intervention miccai ser. lecture notes computer science navab hornegger wells springer international publishing vol. frangi eds. available http//dx.doi.org/./ ---- h.-c. shin seff summers interleaved text/image deep mining large-scale radiology database proceedings ieee conference computer vision pattern recognition bagci buty h.-c. shin roth papadakis depeursinge summers holistic classiﬁcation attenuation patterns interstitial lung diseases deep convolutional neural networks workshop margeta criminisi cabrera lozoya ayache fine-tuned convolutional neural nets cardiac acquisition plane recognition computer methods biomechanics biomedical engineering imaging visualization tajbakhsh shin gurudu hurst kendall gotway liang convolutional neural networks medical image analysis full training tuning? ieee transactions medical imaging vol. wang zhou chen wang comparison machine learning methods classifying mediastinal lymph node metastasis non-small cell lung cancer f-fdg pet/ct images ejnmmi research vol. mosinska tarnawski active learning proofreading delineation curvilinear structures international conference medical image computing computer-assisted intervention. springer active learning hyperspectral image classiﬁcation stacked autoencoders based neural network ieee international conference image processing sept shelhamer long darrell fully convolutional networks semantic segmentation ieee transactions pattern analysis machine intelligence vol. efron tibshirani introduction bootstrap. chakraborty balasubramanian panchanathan active batch selection convex relaxations guaranteed solution bounds ieee transactions pattern analysis machine intelligence vol. pabby schoen weissfeld burt kikendall lance shike lanza schatzkin analysis colorectal cancer occurrence surveillance colonoscopy dietary polyp prevention trial gastrointest endosc vol. rijn reitsma stoker bossuyt deventer dekker polyp miss rate determined tandem colonoscopy systematic review american journal gastroenterology vol. heresbach barrioz lapalus mg.and coumaros miss rate colorectal neoplastic polyps prospective multicenter study back-to-back video colonoscopies endoscopy vol. calder herbert henderson mortality untreated pulmonary embolism emergency department patients. annals emergency medicine vol. available http//dx.doi.org/./j. annemergmed... sadigh kelly cronin challenges controversies topics pulmonary embolism imaging american journal roentgenology vol. available http//dx.doi.org/./ajr.. tajbakhsh gotway liang computer-aided pulmonary embolism detection using novel vessel-aligned multi-planar image representation convolutional neural networks international conference medical image computing computer-assisted intervention. springer zongwei zhou student department biomedical informatics arizona state university. received b.sc. honors computer science dalian university technology research interests computer vision machine learning biomedical imaging. shin deep learning researcher department biomedical informatics arizona state university. earned m.s. degree computer science arizona state university interested applications deep learning health informatics. suryakanth gurudu associate professor medicine mayo medical school. received robert wood johnson university hospitals umdnj completed fellowship gastroenterology hepatology university hospital cleveland case western reserve university practicing gastroenterologist director colorectal neoplasia clinic mayo clinic arizona. research interests include improving colonoscopy outcomes video image analysis. michael gotway professor radiology mayo clinic college medicine. diplomate american board radiology. clinical interests include imaging aids pulmonary embolism interstitial lung disease pulmonary ﬁbrosis high-resolution lung cardiovascular imaging imaging patients pectus excavatum computer technology detection diagnosis various disorders affecting thorax. bibliography includes peer reviewed articles books editorships book chapters related ﬁelds interest. jianming liang associate professor arizona state university. drawing upon computer vision machine learning visualization mathematics research focuses developing computational methodologies addressing profound challenge facing biomedicine image data explosion multidisciplinary teambased approach. addition peerreviewed publications holds patents patents pending. received president’s award innovation. table detailed predictions bottom candidates step three probabilities predicted bolded columns represent dominated predictions lighted blue numbers used calculate different active selection criteria.", "year": 2018}