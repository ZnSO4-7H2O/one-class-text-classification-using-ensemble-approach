{"title": "Normalized Gradient with Adaptive Stepsize Method for Deep Neural  Network Training", "tag": ["cs.LG", "cs.AI"], "abstract": "In this paper, we propose a generic and simple algorithmic framework for first order optimization. The framework essentially contains two consecutive steps in each iteration: 1) computing and normalizing the mini-batch stochastic gradient; 2) selecting adaptive step size to update the decision variable (parameter) towards the negative of the normalized gradient. We show that the proposed approach, when customized to the popular adaptive stepsize methods, such as AdaGrad, can enjoy a sublinear convergence rate, if the objective is convex. We also conduct extensive empirical studies on various non-convex neural network optimization problems, including multi layer perceptron, convolution neural networks and recurrent neural networks. The results indicate the normalized gradient with adaptive step size can help accelerate the training of neural networks. In particular, significant speedup can be observed if the networks are deep or the dependencies are long.", "text": "paper propose generic simple algorithmic framework ﬁrst order optimization. framework essentially contains consecutive steps iteration computing normalizing mini-batch stochastic gradient; selecting adaptive step size update decision variable towards negative normalized gradient. show proposed approach customized popular adaptive stepsize methods adagrad enjoy sublinear convergence rate objective convex. also conduct extensive empirical studies various non-convex neural network optimization problems including multi layer perceptron convolution neural networks recurrent neural networks. results indicate normalized gradient adaptive step size help accelerate training neural networks. particular signiﬁcant speedup observed networks deep dependencies long. continuous optimization core technique training non-convex sophisticated machine learning models deep neural networks compared convex optimization global optimal solution expected non-convex optimization usually aims stationary point local optimal solution objective function iterative algorithms. among large volume optimization algorithms ﬁrst-order methods gradient information objective functions update solution widely used relatively requirement memory space computation time. paper particularly interested solving deep neural network training problems ﬁrst order method. however compared non-convex problems deep neural network training following challenges gradient computation expensive. full gradient usually diﬃcult compute especially applied machine learning problems data calculation requires going whole dataset. challenges indicate iteration stochastic gradient might best practical ﬁrst order information get. obtain information develop certain mechanism avoid exploding vanishing phenomenon eventually ensure faster convergence. paper solve challenges ingredients ﬁrst-order optimization algorithm iteration i.e. updating direction step size updating direction indicates next iterated point move step size determines move with. propose stochastic normalized gradient constructed dividing gradient norm. compared regular gradient normalized gradient provides updating direction incorporate local steepness objective magnitude help control change solution well-designed step length. intuitively constrains magnitude gradient extent prevent gradient vanishing exploding phenomenon. fact showed normalized gradient descent methods numerically stable better theoretical convergence properties regular gradient descent method non-convex optimization. updating direction determined step size next important component design ﬁrst-order methods. convex problems well studied strategies stepsize ensuring convergence non-convex optimization choice step size diﬃcult critical either enlarge reduce impact aforementioned vanishing exploding gradients. normalized gradient used step size fully controls progress solution convergence possible step size carefully designed. among diﬀerent choices step sizes adaptive feature-dependent step size attracted lots attention great performance applied training deep neural networks diﬀerent standard step-size rule multiplies number coordinate gradient adaptive feature-dependent step-size rule multiplies diﬀerent numbers coordinates gradient diﬀerent parameters learning model updated diﬀerent paces. example adaptive step size invented constructed aggregating coordinates historical gradients. discussed method dynamically incorporate frequency features step size frequently occurring features small step size infrequent features long step size. similar adaptive step size proposed historical gradients integrated feature-dependent step size diﬀerent weighting scheme. paper propose generic framework using mini-batch stochastic normalized gradient updating direction step size adaptive coordinate hope resulting methods beneﬁt techniques generate even better performance existing methods. framework starts computing regular mini-batch stochastic gradient immediately normalized. normalized version plugged current popular adaptive step size methods adam adagrad numerical results shows normalized gradient always helps improve performance original methods especially network structure deep best algorithms comparison adam normalized gradients. seems ﬁrst thorough empirical study various types neural networks idea. besides although focus empirical studies deep learning objective highly non-convex also provide convergence proof framework problem convex. rest paper organized follows. section brieﬂy previous work related ours. section formalize problem solve propose generic algorithm framework. provide concrete example type algorithm show convergence property convex setting. section conduct comprehensive experimental studies compare performance diﬀerent algorithms various neural network structures. finally conclude paper section pioneering work normalized gradient descent method nesterov shown \u0001-optimal solution within iterations objective function quasi-convex. kiwiel hazan extended upper semi-continuous quasi-convex objective functions local-quasi-convex objective functions respectively achieved iteration complexity. moreover hazan showed ngd’s iteration complexity reduced objective function local-quasi-convex locally-smooth. stochastic algorithm also proposed hazan which mini-batch used construct stochastic normalized gradient iteration ﬁnds \u0001-optimal solution high probability locally-quasi-convex functions within iterations. levy proposed saddle-normalized gradient descent method adds zero-mean gaussian random noise stochastic normalized gradient periodically. applied strict-saddle functions additional assumption shown saddle-ngd evade saddle points local minimum point approximately high probability. analogous orthogonal gradient normalization ideas proposed deep neural network training. example batch normalization used address internal covariate shift phenomenon deep learning training. beneﬁts making normalization part model architecture performing normalization training mini-batch. weight normalization hand aims reparameterization weight vectors decouples length weight vectors direction. recently proposes path normalization approximate path-regularized steepest descent respect path-wise regularizer related max-norm regularization achieve better convergence vanilla adagrad. perhaps related idea gradient clipping. proposed avoid gradient explosion pulling magnitude large gradient certain level. however method anything magnitude gradient small. adaptive step size studied years optimization community. celebrated method line search scheme. however exact line search usually computational infeasible inexact line search also involves full gradient evaluation. hence suitable deep learning setting. recently provably correct algorithms adaptive step sizes convex scenario start applied non-convex neural network training adagrad adam rmsprop. however directly unnormalized gradient diﬀerent framework. best knowledge loss function probability distribution. goal distilled minimize objective function usually called parameter model machine learning literature. propose generic optimization framework algorithm iteration ﬁrstly computes gradient mini-batch data normalizes direction next adaptive step size move current parameter towards negative direction transformation i.e. fact framework customized existing ﬁrst order methods adaptive step sizes adagrad rmsprop adam adopting step size rules respectively. concrete example present normalized gradient customization adagrad convergence following denote xhtx mahalanobis norm associated positive deﬁnite diagonal matrix convergence normalized adagrad basic experiment setup section conduct comprehensive numerical experiments diﬀerent types neural networks. algorithms testing momentum adagrad adam normalized gradient counterparts denoted suﬃx note sgdm algorithms without adaptive stepsize adam adagrad popular algorithms used deep neural network learning adaptive stepsize. therefore algorithms test covered combinations with/without normalized gradient and/or adaptive step size. experiments four diverse tasks ranging image classiﬁcation natural language processing. neural network structures investigation include multi layer perceptron long-short term memory convolution neural networks. exclude potential eﬀect might introduced advanced techniques experiments adopt basic version neural networks unless otherwise stated. loss functions classiﬁcations cross entropy language modeling perplexity. since computational time proportional epochs show performance versus epochs. running time similar omit brevity. algorithms default settings. speciﬁcally adam/adamng initial step size scale ﬁrst order momentum second order momentum parameter avoid division zero adagrad/adagradng initial step size scale sgdm/sgdmng. learning rate momentum codes written deep learning package pytorch. general answers ﬁrst questions aﬃrmative long neural networks either long dependence deep architecture answer third empirical study adamng. give readers quick clear idea answers extract information ﬁgures lstm experiment section visualize figure details experiment please section left ﬁgure shows adam normalized gradient faster original version. middle ﬁgure shows adam adagrad faster sgdm. right ﬁgure shows adamng fastest convergence compared normalized version adagrad sgdm. following elaborate details individual experiments. ﬁrst network structure going test upon multi layer perceptron adopt handwritten digit recognition data mnistlecun which data image hand written digits training testing examples task tell right number contained test image. approach applying learn end-to-end classiﬁer input images output label probability. predicted label largest probability. middle layer hidden unit number ﬁrst last layer respectively contain units. activation functions layers sigmoid batch size algorithms. choose diﬀerent numbers layer results shown figure column ﬁgures corresponds training testing objective curves given layer number. left right layer numbers respectively that network shallow containing layers normalized stochastic gradient descent outperform unnormalized counterpart adam adagrad even slightly better unnormalized versions. networks become deeper acceleration brought gradient normalization turns signiﬁcant. example starting second column adamng outperforms adam terms training testing convergence. fact network depth adamng still converge small objective value adam gets stuck beginning. observe similar trend comparison adagrad adagradng hand algorithms adaptive step sizes usually generate stable learning curve. example last column sgdng causes signiﬁcant ﬂuctuation training testing curves. finally setting adamng always best algorithm terms convergence performance. figure training testing objective curves mnist dataset multi layer perceptron. left right layer numbers respectively. ﬁrst training curve second testing. algorithms image classiﬁcation task cifar- data approached deep convolution neural network cifar- collection class natural images consisting color images images class. experiment adopt network short visual geometry group deep convolution neural network ﬁnish task. proposed attack imagenet challenge localization classiﬁcation tracks. choose original version networks containing layers. interested readers might take look know details network. results summarized figure take-home messages similar last section. particular depth network relatively shallow gradient normalization adaptive stepsize strategy hardly bring beneﬁt convergence. fact vanilla momentum achieves fastest convergence training testing. nevertheless depth increases power proposed strategy becomes clear. speciﬁcally sgdm adam stuck beginning training gradient normalization versions push objective down. among converging methods adamng achieves best testing objective fastest speed. therefore safely draw conclusion ngas framework also suitable train deep convolution neural networks. figure training testing objective curves cifar- dataset convolution neural networks. left right ﬁgures respectively vgg-training vgg-testing vgg-training vgg-testing. ngas framework powerful training image classiﬁcation tasks conjecture idea also applied learning recurrent neural networks conﬁdence based fact unrolled considered another type deep network stretching direction horizontal weights shared. section test performance proposed algorithm word-level language modeling task popular type i.e. single directional long-short term memory networks data penn tree bank data which preprocessed contains training words validation test words. vocabulary size lstm layers containing hidden units. word embedding dimensions trained scratch. batch size vary length backprop time within range prevent overﬁtting dropout regularization rate settings. results shown figure conclusions drawn ﬁgures similar last experiments. however slightly diﬀerent cheering observations adamng uniformly better competitors training sequence length. superiority terms convergence speedup exists training testing. experiments previous sections mainly focus comparison convergence speed among diﬀerent methods. section going quantitatively show proposed ngas framework also enables neural network better generalization ability. achieve this network necessarily deep previous experiments. task sentiment analysis convolution neural network. dataset rotten tomatoes pang movie review dataset containing documents half positive half negative. randomly select around training validation. model single layer convolution neural network follows setup word embedding randomly initialized -dimension. figure training testing objective curves penn tree bank dataset lstm recurrent neural networks. ﬁrst training objective second testing. left right training sequence length respectively dropout imposed. algorithm epochs training data report best validation accuracy table messages conveyed table three-fold. firstly algorithms using normalized gradient achieve much better validation accuracy unnormalized versions. secondly adaptive stepsize always obtain better accuracy without. easily seen comparison adam sgdm. last point direct conclusion previous algorithm using normalized gradient adaptive step sizes namely adamng outperforms remaining competitors. paper propose generic algorithm framework ﬁrst order optimization. particularly eﬀective addressing vanishing exploding gradient challenge training non-convex loss functions context convolutional recurrent neural networks. method based normalizing gradient establish descending direction regardless magnitude separately estimating ideal step size adaptively. method quite general applied diﬀerent types networks various architectures. although primary application algorithm deep neural network training provide convergence method convex setting. empirically proposed method exhibits promising performance training diﬀerent types networks across multiple well-known data sets general positive performance diﬀerential compared state striking deep networks shown comprehensive experimental study. john duchi alekh agarwal martin wainwright. dual averaging distributed optimization convergence analysis network scaling. automatic control ieee transactions pang lillian lee. seeing stars exploiting class relationships sentiment categorization respect rating scales. proceedings annual meeting association computational linguistics pages association computational linguistics", "year": 2017}