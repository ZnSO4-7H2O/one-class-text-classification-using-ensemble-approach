{"title": "Cross-scale predictive dictionaries", "tag": ["cs.CV", "stat.ML"], "abstract": "We propose a novel signal model, based on sparse representations, that captures cross-scale features for visual signals. We show that cross-scale predictive model enables faster solutions to sparse approximation problems. This is achieved by first solving the sparse approximation problem for the downsampled signal and using the support of the solution to constrain the support at the original resolution. The speedups obtained are especially compelling for high-dimensional signals that require large dictionaries to provide precise sparse approximations. We demonstrate speedups in the order of 10-100x for denoising and up to 15x speedups for compressive sensing of images, videos, hyperspectral images and light-field images.", "text": "fig. left right bayer image image reconstructed using image reconstructed using proposed method. takes minutes proposed method takes minutes little loss reconstruction quality. paper propose multi-scale dictionary model visual signals naturally enables cross-scale prediction thus combining adaption signal classes speed offered predictive models. contributions follows. model. propose novel signal model uses multiscale sparsifying dictionaries provide cross-scale prediction wide array visual signals. speciﬁcally given sparsifying dictionaries scale non-zero support patterns signal downsampled counterparts constrained exhibit speciﬁc predetermined patterns. computational speedups. proposed signal model constrained support pattern across scales naturally enables cross-scale prediction used speedup runtime algorithms like orthogonal matching pursuit figure shows speed-ups obtained demosaicing images; here obtain speed little loss accuracy similar-sized dictionary. term algorithm zero tree since sparse representation forms zero tree. learning. given large collections training data propose simple training method modiﬁed classical k-svd algorithm obtain dictionaries consistent proposed model. note short version paper appeared ieee international conference image processing journal paper extends results large class signals including light-ﬁelds well shows results real data captured compressive sensing hardware. abstract—sparse representations using learnt dictionaries provide efﬁcient model especially signals enjoy alternate analytic sparsifying transformations. however solving inverse problems sparsifying dictionaries computationally expensive especially dictionary consideration large number atoms. paper incorporate additional structure dictionary-based sparse representations enable speedups solving sparse approximation problems. speciﬁc structure endow onto sparse models multi-scale modeling sparse representation scale constrained sparse representation coarser scales. show cross-scale predictive model delivers signiﬁcant speedups often range little loss accuracy denoising compressive sensing wide range visual signals including images videos light ﬁelds. visual signals exhibit strong correlation across scales often modeled exploited enhance image processing algorithms important example idea multi-scale coding images using wavelet-tree model provides sparse well predictive model occurrence non-zero wavelet coefﬁcients across scales speciﬁcally wavelet tree model arranges wavelet coefﬁcients image onto tree nodes tree correspond coefﬁcients level corresponds coefﬁcients associated particular scale. organization dominant non-zero coefﬁcients form connected rooted sub-tree i.e. children node small wavelet coefﬁcients expected take small values well. wavelet tree model central many compression sensing processing algorithms wavelet tree model provides excellent approximation capabilities images similar models cross-scale predictive property largely unknown visual signals including videos hyperspectral images light-ﬁelds. overcomplete dictionaries learned dataset provide alternate approach large class signals terms enabling sparse representations given large amount data many approaches learn dictionary training dataset expressed sparse linear combination elements/atoms dictionary. reliance learning opposed analytic constructions case wavelets provides immense ﬂexibility towards obtaining dictionary tuned speciﬁcs particular signal class application. much work exists learning dictionaries adapt signal classes much attention paid incorporating predictive models enable speed exploiting correlations across denote vectors bold font scalars/matrices capital letters. vector said k-sparse non-zero entires. list indices non-zero entries sparse vector termed support; support vector denoted -norm vector number non-zero entries. finally given dictionary rn×t support refers matrix size ×|ω| formed selecting columns corresponding elements similarly given vector refers |ω|-dimensional vector formed selecting entries corresponding fig. time versus accuracy varying dictionary size denoising video patches. curve generated varying sparsity level multiples observe computationally beneﬁcial dictionary larger number atoms smaller sparsity level opposed smaller dictionary higher sparsity. problem np-hard many greedy relaxed approaches solving particular interest paper greedy approach solving recovers support sparse vector element time ﬁnding column dictionary correlated current residue. iteration algorithm three steps ﬁrst index atom closest angle current residue added support; second solving least square problem updated support obtain current estimate; third updating residue removing contribution current estimate. outline procedure given algorithm proxy step projection step computationally intensive steps omp. time complexity proxy step projection step iterations. large dictionaries sparse representation proxy step dominating term grows linearly dictionary size. number techniques devoted speeding different steps omp. problems high-dimensionality i.e. large approach project lower dimension obtaining random projections dictionary specifically opposed objective minimize rm×n random matrix preserves geometry problem thereby allowing computations m-dimensional space. context high-dimensional data typical dictionaries large number atoms setting proxy step becomes bottle neck. need large dictionaries driven higher accuracy reconstruction. empirically larger dictionaries needed higher accuracy evident time accuracy plot figure denoising videos. various methods proposed speed sparse approximation imposing structure coefﬁcients. methods employ tree-like arrangement sparse coefﬁcients give logarithmic complexity improvement. approach using approximate nearest neighbors shallow-tree based matching speed proxy step. instead searching elements dictionary closest match dictionary arranged shallow tree fast search. certain conditions search complexity obtained. however results reduction accuracy closest dictionary atom computed approximate nearest neighbor method. another approach restrict search space imposing tree structure sparse coefﬁcients complexity search would reduce restricting search space prediction explored approximation chirplet atoms. proposed method ﬁrst ﬁnds approximation input signal gabor atom ﬁrst subsequently scale chirp optimized locally approximate chirplet atom. though methods provide signiﬁcant speedups usage restricted signals known structure wavelets images hierarchical structure chirplet atoms sound. signal classes obvious sparsifying transforms promising approach learn dictionary provides sparse representations speciﬁc signal class interest. field olshausen seminal contribution showed patches natural images sparsiﬁed dictionary containing gabor-like atoms provided connection sparse coding receptor ﬁelds visual cortex. recently aharon proposed k-svd algorithm viewed extension k-means clustering algorithm dictionary learning. given collection training data k-svd aims learn dictionary rn×t k-sparse. ﬁrst forays learning good dictionaries sparse representation. however increasing complexity signal dimension larger dictionaries needed requires larger computation time. learning dictionary atoms innately clustered intuitive speeding approximation process large dictionaries. particularly visual signals clustering incorporating scale spatial complexity signal explored before. jayaraman learn dictionaries multi-level representation image patches simple patches captured early stages complex textures resolved higher levels. given dictionaries multi-level dictionary learning posed following optimization problem residue training dl−. provides speedups solving sparse approximation problems since patches occur often captured earlier levels. speedups constant compared dictionary size scale well high dimension signals. propose similar multiple levels representation data across scales captures complex patterns ﬁner scales also incorporating predictive framework gracefully scales higher dimensions. jenatthon present hierarchical dictionary learning mechanism impose tree structure sparsity forces dictionary atoms cluster like tree. though give higher accuracy reconstruction much said speed obtained. mairal learn dictionary based quad-tree models patch sub-divided four non-overlapping patches. method gives better accuracy algorithm slow involves approximations successive decomposition image patch smaller image patches. none multi-scale learning algorithms exploit cross-scale structure especially visual signals. goal paper construct dictionaries endowed structured sparse representations similar wavelettree model enable computational speedups solving sparse approximation problems large class visual signals include images videos lightﬁelds. application sparse representations sense signals far-fewer measurements dimensionality relies low-dimensional representations sensed signal sparse representation transform dictionary example this. rich body work applying compressive sensing imaging sensing visual signals including images videos light ﬁelds relevant paper video work hitomi sparsifying dictionary used video patches recover high-speed videos low-frame rate sensors. hitomi also demonstrated accuracy enabled large dictionaries; speciﬁcally obtain remarkable results dictionary atoms video patches dimension success recovery signals compressive sensing needs high accuracy fast recovery methods something large dictionaries lack example hitomi claim recovery frames videos took hour atom dictionary. calls faster recovery methods better signal models. particular visual signals multi-scale predictive representation well exploited wavelet representation images. extending higher dimensional signals non-trivial known sparsifying wavelet bases known. proposed method inspired multi-resolution representations tree-models enabled wavelets. particular baraniuk shows non-zero wavelet coefﬁcients form rooted sub-tree signals trends anomalies hence piecewise-smooth signals enjoy sparse representation structured support pattern non-zero wavelet coefﬁcients forming rooted sub-tree. similar properties also shown images separable haar basis however spite elegant results images obvious sparsifying bases higher-dimensional visual signals like videos light-ﬁeld images. address this build cross-scale predictive models similar wavelet tree model replacing basis over-complete dictionary capable providing sparse predictive representation wide class signals. signal model extends notion multiresolution representation signals beyond images. instead relying analytical bases sparseness prediction propose signal model based learned dictionaries retain properties overcomplete dictionaries wavelet trees. given signal represent multi-resolution manner projection operator scale space form wavelet basis scale. particular piece wise signals like images bases separable haar coefﬁcients form rooted sub-tree. hard analytical bases arbitrary signal instead retain framework replacing bases overcomplete dictionaries. hence propose signal model predicts support signal across scales present model two-scale scenario ease understanding. given collection signals proposed signal model consists sparsifying dictionaries dhigh rn×thigh dlow rnlow×tlow satisfy following three properties. sparse approximation ﬁner scale. signal enjoys khigh-sparse representation dhigh dhighshigh shigh khigh. sparse approximation coarser scale. given downsampling operator rnlow downsampled signal xlow enjoys sparse representation dlow i.e. xlow dlowslow slow klow. downsampling operator domain speciﬁc. cross-scale prediction. support shigh constrained support slow; speciﬁcally ωshigh mapping known priori. make observations. observation thigh tlow since nhigh nlow. increase dimension signal complex patterns emerge require larger number redundant elements. empirically found number atoms dictionary increases super linearly increasing dimension signal given approximation accuracy observation recall computational time proportional number atoms dictionary since iteration algorithm need compute inner product residue atoms dictionary. fig. proposed cross-scale signal model sparse coefﬁcients across scales forming rooted subtree. analyze signal multiple scales obtained downsampling successively k-times. k-th scale learn dictionary sparsiﬁes downsamped signal arrange sparse coefﬁcients onto tree enforce cross-scale prediction property follows child atom take nonzero values parent nonzero. proposed model obtains speedups ﬁrst solving sparse approximation problem coarse scale subsequently exploiting cross-scale prediction property constrain support ﬁner scale. source speedups relies intuitive ideas ﬁrst solving sparse approximation problem problem fewer atoms faster omp’s runtime linear number atoms dictionary used second knew support slow simply discard atoms dhigh belong since support shigh guaranteed within element support ωslow coarser scale controls inclusion/exclusion non-overlapping block locations sparse vector ﬁner scale. consequence cardinality qklow. here up-sampling operator identity rnlow. experiments used uniform down-sampler nearest neighbour sampler speciﬁc domain signal. step recovers sparse approximation problems steps solved using omp. proposed mapping across scales sparse support forms zero tree coefﬁcient zero corresponding coefﬁcient coarser scale zero. hence refer algorithm zero tree omp. algorithm outlines zero tree procedure. provide expressions expected speedups traditional single-scale omp. since analysis speedup account complexity implementing consider denoising problem identity matrix. amount time required solve sparse-approximation problem using dictionary size sparsity level hence obtaining shigh directly compute dependence recall iteration algorithm need operations ﬁnding inner product residue dictionary atoms operations maximally aligned vector operations least-squares step. thus dictionaries large number atoms i.e. large small values sparsity level linear dependence dominates total computation time. here speedup provided algorithm approximately thigh/. step learning dlow. learn coarse-scale dictionary dlow applying k-svd downsampled training dataset xlow by-product learning dictionary supports {ωslowklow} sparse approximations downsampled training dataset. optimization problem solved simply modifying sparse approximation step k-svd restrict support appropriately. figures show examples learnt low-resolution atoms corresponding highresolution atoms images videos light ﬁelds. observe constraining sparse support high-resolution approximation alone learns patches similar appearance low-resolution patches supports proposed signal model. consequence speed approximation step dictionary learning proposed method also faster. recall k-svd alternates dictionary learning sparse approximation. since modiﬁed k-svd algorithm replaces proposed zero-tree overall time taken iteration reduces thus speeding learning dictionaries proposed algorithm. fig. visualization select low-resolution atoms corresponding atoms high resolution dictionary; resolution atoms scaled show features clearly; bottom corresponding high resolution atoms resolution atom. restricting support higher resolution approximation method learns child atoms similar parent atom. fig. visualization parent atom frames select high resolution child atoms. resolution atoms scaled factor show features clearly. various sub-aperture views child atoms similar parent atoms added spatial details. purposes lower sparsity promises better reconstruction results. hence klow gives better results. found klow range works well. khigh greater equal klow least atom corresponding resolution atom picked. tlow thigh chosen would appropriate signal dimension nlow nhigh respectively. since dictionary learning objective well multiscale dictionary learning objective non-convex solution obtained depends initialization. elad proposed initializing higher resolution dictionaries resolution dictionary information. dlow available ﬁrst step multi-scale dictionary training. αjkdlowk then dhighj∀j randomly initialized training samples x|ωk. validate signal model show signal model performs good large dictionary runtimes compared small dictionary. trained dictionaries various classes visual signals emphasize ubiquity signal model. comparisons made small dictionary nlow atoms large dictionary nhigh atoms proposed multiscale dictionary nlow resolution nhigh high resolution atoms ﬁnally nhigh multi-level dictionary nlow levels proposed jayaraman quantify approximation accuracy using recovered deﬁned follows given signal trained dictionaries nlow nhigh image patches downscaled patches dimension figure shows demosaicing bayer pattern using large single scale dictionary proposed method. trained atom high resolution dictionary kodak true color images atom resolution dictionary patches downscaled compare atom single scale dictionary. took minutes single scale approximation accuracy whereas minutes approximation accuracy scale dictionary. figure shows image denoising perform denoising trained dictionaries patch patch overlap pixels. hardly reduction accuracy method performs faster. figure compares performance various dictionaries denoising tasks representative images. fig. visualization results image denoising. clockwise left original image noisy image recovered image using proposed method recovered image using k-svd learned dictionary. obtain speedup less reduction accuracy. retained known pixel values fraction locations recovered complete image. cases dictionary outperforms methods. speed obtained denoising summarized table less loss accuracy method offers signiﬁcant speed image processing tasks. fig. time accuracy denoising videos input noise single scale dictionary proposed multiscale dictionary. dictionaries sizes compared zero tree dictionary atoms high resolution atoms resolution klow khigh high approximation accuracies method outperforms large dictionaries run-time time. fig. reconstruction performance denoising additive gaussian noise compressive sensing using imaging architecture hitomi different videos sharpner drop. here represents number frames reconstructed coded image. fig. performance denoising additive gaussian noise inpainting randomly removed pixels images peppers budgerier. here represents number unknowns known. section iv-a details dictionary. proposed method provides better reconstruction accuracy competing methods. trained dictionaries nlow nhigh video patches downscaled video patches dimension show empirically signal model outperforms single scale dictionaries terms speed accuracy. figure show comparison single scale dictionaries various size zero-tree dictionary denoising videos. time approximation method gives highest accuracy. another accuracy signal model takes least time. figure shows performance various dictionaries denoising tasks figure show results videos. combined multiple frames coded image proposed hitomi speedup denoising depending number measurements. performance video denoising summarized table speedups obtained videos much higher images less loss accuracy. results signiﬁcantly better visually seen figure smoother spatial proﬁle compared reconstruction single scale dictionary. various dictionaries denoising tasks figure shows results lightﬁelds. simulated acquisition images multiple coded aperture settings proposed performance metrics denoising fig. visualization reconstructed frames sharpner drop videos; ground truth reconstruction single scale dictionary; reconstruction scale dictionary. proposed method provides speedup modest improvement reconstruction snr. tested algorithm real data collected hitomi video reconstructed coded image. compared reconstruction zero-tree dictionary dictionaries atoms trained hitomi figure show reconstruction results bouncing ball airplane respectively time taken reconstruction. notice results visually similar method faster compared stock atom dictionary. case bouncing ball number better resolved result well. table figures quantify performance proposed signal model obtained using ksvd wide range parameters well signals. across board observe proposed framework provides accuracy good obtained k-svd speedups small-sized problems larger problems. speedups obtained comparable results higher approximation accuracies proposed method. result speed sparse coding step also signiﬁcant speed training phase using modiﬁed k-svd makes feasible deal large problems. fig. reconstruction performance denoising imaging architecture liang different lightﬁelds buddha dragons. here represents number sub-aperture views recovered coded aperture image. summarized table method offers speed increase reconstruction accuracy. improved results observed visually reconstruction results dragons buddha datasets fig. visualization reconstructed sub-aperture views buddha dragon light ﬁelds; ground truth reconstruction single scale dictionary; reconstruction scale dictionary. epipolar slices scaled show features clearly. proposed method enables speed along increase reconstruction snr. fig. reconstruction real data scenes scenes clockwise left coded image reconstruction atom dictionary reconstruction atom dictionary reconstruction proposed two-scale dictionary. proposed method provides speed atom dictionary improved visual quality example number rendered lesser artifacts reconstruction. presented signal model enables cross scale predictability visual signals. method particularly appealing simple extension existing k-svd algorithms providing signiﬁcant speed little loss accuracy. computational gains provided algorithm especially signiﬁcant problems involving high-dimensional dictionaries large number atoms. table table speed various dictionary sizes patch sizes sparsity. speed shown solving sparse approximation problems quantify ratio time taken using k-svd learnt dictionary zero tree proposed model. also shown approximation errors training dataset k-svd proposed algorithm. order higher accuracy construction sparsity levels need higher large scale dictionaries number atoms high resolution dictionary. however major drawback speedups still signiﬁcant spite increased sparsity levels. table shows model accuracy proposed signal model lower large dictionary. reasons k-svd algorithm non-convex optimization framework sensitive initialization. initialization proposed paper best heuristic. better results obtained better initialization methods. dictionary update step proposed modiﬁed ksvd algorithm runs independent sparse approximation step. better approach would modify dictionary update step incorporate interdependence sparse coefﬁcients. propose signal model method solving sparse approximation signal model. future work direction would involve better algorithms provided framework stronger theoretical results. roman learn pair resolution high resolution dictionary using sparsity pattern dictionaries. given resolution patch ylow solve sparse approximation problem ylow dlows super resolve image dhighs. contrast method requires high resolution image uses sparse representation downscaled image predict high resolution sparse representation. primary image-based super resolution method accommodate inverse problem based sparse approximation. chen asif sankaranarayanan veeraraghavan. fpa-cs focal plane array-based compressive imaging short-wave ieee conf. computer vision pattern recognition infrared. pages hitomi gupta mitsunaga nayar. video single coded exposure photograph using learned over-complete dictionary. ieee intl. conf. computer vision tree-based orthogonal matching pursuit algorithm signal reconstruction. ieee intl. conf. image processing c.-k. liang t.-h. b.-y. wong chen. programmable aperture photography multiplexed light ﬁeld acquisition. trans. graphics mailh´e gribonval bimbot vandergheynst. complexity orthogonal matching pursuit sparse signal approximation shift-invariant dictionaries. ieee intl. conf. acoustics speech signal processing pati rezaiifar krishnaprasad. orthogonal matching pursuit recursive function approximation applications wavelet decomposition. asilomar conf. signals systems computers reddy veeraraghavan chellappa. programmable pixel compressive camera high speed imaging. computer vision pattern recognition ieee conf. pages shapiro. embedded image coding using zerotrees wavelet coefﬁcients. ieee trans. signal processing tambe veeraraghavan agrawal. towards motion aware ieee intl. conf. computer vitaladevuni natarajan prasad. efﬁcient orthogonal matching pursuit using sparse random projections scene video classiﬁcation. ieee intl. conf. computer vision", "year": 2015}