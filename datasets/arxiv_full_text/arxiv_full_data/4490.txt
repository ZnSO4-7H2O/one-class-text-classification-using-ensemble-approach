{"title": "A Note on the PAC Bayesian Theorem", "tag": ["cs.LG", "cs.AI", "I.5.1"], "abstract": "We prove general exponential moment inequalities for averages of [0,1]-valued iid random variables and use them to tighten the PAC Bayesian Theorem. The logarithmic dependence on the sample count in the enumerator of the PAC Bayesian bound is halved.", "text": "prove general exponential moment inequalities averages valued random variables tighten bayesian theorem. logarithmic dependence sample count enumerator bayesian bound halved. refers vector empirical loss hypothesis expression refers relative entropy probability measures importance learning theory comes fact depend note implies drive learning algorithm select posterior minimizing sample-dependent right side among applications bayesian bound applied prove generalisation error bounds large margin classiﬁers support vector machines relative entropy interpreted information gain specializing information normally extracted sample term expresses usual dependence conﬁdence parameter remaining diﬃcult understand need can’t altogether eliminated least reduced? relative entropy however dependent posterior thus implicitely sample sample-size cases grows faster logarithmically bounds therefore weaker above. since average linear convex exponential function nondecreasing convex function also convex. clearly permutation symmetric arguments. lemma immediately gives", "year": 2004}