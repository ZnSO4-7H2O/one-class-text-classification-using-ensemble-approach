{"title": "Phoneme recognition in TIMIT with BLSTM-CTC", "tag": ["cs.CL", "cs.NE", "I.2.7; I.5.4"], "abstract": "We compare the performance of a recurrent neural network with the best results published so far on phoneme recognition in the TIMIT database. These published results have been obtained with a combination of classifiers. However, in this paper we apply a single recurrent neural network to the same task. Our recurrent neural network attains an error rate of 24.6%. This result is not significantly different from that obtained by the other best methods, but they rely on a combination of classifiers for achieving comparable performance.", "text": "compare performance recurrent neural network best results published phoneme recognition timit database. published results obtained combination classiﬁers. however paper apply single recurrent neural network task. recurrent neural network attains error rate result signiﬁcantly diﬀerent obtained best methods rely combination classiﬁers achieving comparable performance. spontaneous speech production continuous dynamic process. continuity reﬂected acoustics speech sounds particular transitions speech sound another. consequence boundaries speech sounds clearly deﬁned. fact signiﬁcantly contributes making segmentation labelling speech data interrelated tasks. interrelation automatic speech recognition best performed methods hidden markov models require segmented data development. contrary developing neural networks traditionally relied segmented data. objective functions require network output target value every speciﬁc time-steps data sequence. connectionist temporal classiﬁcation overcomes limitation. allows developing neural network classiﬁers using sequence labels desired output target labels correspond events occurring input data sequence phones speech data stream. number labels target labelling therefore typically much shorter number time-steps input data sequence. also timing information target labelling except labels order events occur input data sequence. recurrent neural networks interesting alternative hmms speech recognition. continuous internal state naturally well suited modelling speech dynamics. moreover capability model data dependencies potential modelling coarticulatory eﬀects speech. contrast hmms built number independence assumptions data. showed ctc-based recurrent neural networks outperform state-of-the-art algorithms phoneme recognition timit database. contrast algorithms compared rely single type classiﬁer perform task glass’ uses committee-based classiﬁer whereas deng al.’s combines scores related algorithms systems achieved best phoneme recognition rates published timit. paper compare performance single ctc-based recurrent neural network glass’ deng al.’s systems. main diﬀerences respect experimental setup used ﬁrst data divided training validation test sets described second standard phonetic categories instead used experimental setup allows direct comparison three systems. darpa timit acoustic-phonetic continuous speech corpus contains recordings prompted english speech accompanied manually segmented phonetic transcripts timit contains total sentences sentences spoken speakers major dialect regions united states. experiments sentences discarded remaining data split training validation test according training contains sentences validation contains sentences test contains sentences selected modelling. confusions among number phones counted errors. therefore results presented phonetic categories. decided train network transcriptions based lexicon phones. categories folded onto categories described shown table speech data transformed frequency cepstral coeﬃcients software package spectral analysis carried channel ﬁlter bank khz. pre-emphasis coeﬃcient used correct spectral tilt. twelve mfcc plus order coeﬃcient computed hamming windows long every delta acceleration coeﬃcients added giving vector coeﬃcients total. network coeﬃcients normalised mean zero standard deviation training set. division timit three aforementioned data sets presentation results phones also adopted acoustic features deng used frequency-warped cepstra instead mfcc. part glass tried number variations combinations mfcc perceptual linear prediction cepstral coeﬃcients energy duration glass’ system built models phones timit results tabulated using standard phones. method employed described brieﬂy phoneme recognition performed recurrent neural network. long short-term memory recurrent neural network used ability bridge long time delays hidden units lstm network called memory blocks. memory block memory cells controlled input output forget gate. input gate open incoming data stored memory cell output gate open data stored memory cell sent output layer. forget gate resets memory cell. gates optionally access data stored memory cell gates memory block input typically connected units network. connections trainable thus behaviour gates pre-determined rather learned training. phoneme recognition anticipatory carry-over coarticulatory eﬀects important bi-directional neural network suitable. bi-directional lstm separate recurrent hidden layers connected input output layers. forward recurrent network presented sequential data forward time beginning data sequence time-step backward recurrent network presented sequential data backwards time data sequence time-step time-step network access information data sequence. blstm recurrent neural network trained algorithm using list phones speech utterances target labellings network trained predicted labelling speech utterance directly read outputs. method however guaranteed probable labelling. second method consists calculating probabilities successive extensions labelling preﬁxes used probable labelling. however procedure computationally intensive separately calculated sections output sequence. consequence preﬁx search decoding guaranteed probable labelling practice generally outperforms best path decoding experiments reported paper blstm-ctc network input layer size forward backward hidden layers blocks each output layer size gates used logistic sigmoid function range input training blstm-ctc network done gradient descent weight updates every training example. cases learning rate momentum weights initialized randomly range training gaussian noise standard deviation added inputs improve generalisation. preﬁx search decoding activation threshold used description parameter). deng al.’s hidden trajectory models type probabilistic generative model aimed modelling speech dynamics adding long-contextual-span capabilities missing hidden markov models thorough description system available uses bi-directional ﬁlter estimate probabilistic speech data trajectories given hypothesized phone sequence. estimate used compute model likelihood score observed speech data. search phone sequence highest likelihood performed based lattice search rescoring algorithm speciﬁcally developed htm. glass’s system segment-based speech recogniser based detection landmarks speech signal acoustic features computed hypothesized segments boundaries. standard decoding framework modiﬁed extended deal paradigm shift. results shown table error rates include errors substitutions insertions deletions respect reference transcription. deng al.’s best result achieved lattice-constrained search weighted language model scores glass’s best results achieved many heterogeneous information sources classiﬁer combinations single blstm-ctc recurrent neural network attains error rate signiﬁcantly diﬀerent deng al.’s glass’s best results. likely blstm-ctc achieve improved performance sources information added table error rates timit. results blstm-ctc average standard error runs. average networks trained epochs horizontal lines divide list systems groups performing signiﬁcantly diﬀerent networks. blstm-ctc best path decoding signiﬁcantly diﬀerent deng al.’s baseline blstm-ctc preﬁx search decoding glass’s classiﬁer blstm-ctc preﬁx search decoding signiﬁcantly diﬀerent either deng al.’s htm-hmm glass’s classiﬁer. provided results phoneme recognition blstm-ctc using timit database. experiments standard data sets phonetic inventory employed systems reportedly best performance date. finally compared blstm-ctc’s performance achieved systems blstm-ctc achieves comparable performance without relying combination multiple classiﬁers. also blstm-ctc makes fewer assumptions task domain.", "year": 2008}