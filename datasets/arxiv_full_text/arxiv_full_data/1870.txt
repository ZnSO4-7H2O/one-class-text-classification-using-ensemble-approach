{"title": "Beyond Word Importance: Contextual Decomposition to Extract Interactions  from LSTMs", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "The driving force behind the recent success of LSTMs has been their ability to learn complex and non-linear relationships. Consequently, our inability to describe these relationships has led to LSTMs being characterized as black boxes. To this end, we introduce contextual decomposition (CD), an interpretation algorithm for analysing individual predictions made by standard LSTMs, without any changes to the underlying model. By decomposing the output of a LSTM, CD captures the contributions of combinations of words or variables to the final prediction of an LSTM. On the task of sentiment analysis with the Yelp and SST data sets, we show that CD is able to reliably identify words and phrases of contrasting sentiment, and how they are combined to yield the LSTM's final prediction. Using the phrase-level labels in SST, we also demonstrate that CD is able to successfully extract positive and negative negations from an LSTM, something which has not previously been done.", "text": "driving force behind recent success lstms ability learn complex non-linear relationships. consequently inability describe relationships lstms characterized black boxes. introduce contextual decomposition interpretation algorithm analysing individual predictions made standard lstms without changes underlying model. decomposing output lstm captures contributions combinations words variables ﬁnal prediction lstm. task sentiment analysis yelp data sets show able reliably identify words phrases contrasting sentiment combined yield lstm’s ﬁnal prediction. using phrase-level labels also demonstrate able successfully extract positive negative negations lstm something previously done. comparison simpler linear models techniques deep learning achieved impressive accuracy effectively learning non-linear interactions features. however inability describe learned interactions improvement accuracy come cost state predictive algorithms commonly regarded black-boxes. domain natural language processing long short term memory networks become basic building block yielding excellent performance across wide variety tasks remaining largely inscrutable. work introduce contextual decomposition novel interpretation method explaining individual predictions made lstm without modiﬁcations underlying model. extracts information words contributed lstm’s prediction also combined order yield ﬁnal prediction. mathematically decomposing lstm’s output able disambiguate contributions made step different parts sentence. validate interpretations extracted lstm evaluate problem sentiment analysis. particular demonstrate capable identifying words phrases differing sentiment within given review. also used successfully extract positive negative negations lstm something previously done. consequence analysis also show prior interpretation methods produce scores document-level information built complex unspeciﬁed ways. instance prior work often identiﬁes strongly negative phrases contained within positive reviews neutral even positive. relevant prior work interpreting lstms focused approaches computing word-level importance scores evaluation protocols varying greatly. murdoch szlam introduced decomposition lstm’s output embedding word coefﬁcients demonstrated coefﬁcients meaningful using distill lstms rules-based classiﬁers. took black approach called leave observing change probability resulting replacing given word vector zero vector relied solely anecdotal evaluation. finally sundararajan presents general gradient-based technique called integrated gradients validated theoretically empirical anecdotes. contrast proposed method line work limited word-based importance scores ignoring interactions variables make lstms accurate. another line work focused analysing movement gate activations sequence. karpathy able identify co-ordinates cell state correspond semantically meaningful attributes whether text quotes. however cell co-ordinates uninterpretable clear co-ordinates combine contribute actual prediction. decomposition-based approaches interpretation also applied convolutional neural networks however limited producing pixel-level importance scores ignoring interactions pixels clearly quite important. approach similar computes exact decomposition leverage unique gating structure lstms order extract interactions. attention based models offer another means providing interpretability. models successfully applied many problems yielding improved performance contrast word importance scores attention limited provides indirect indicator importance directionality i.e. class word important for. although attention weights often cited anecdotally evaluated empirically otherwise interpretation technique. prior work attention also incapable describing interactions words. given arbitrary phrase contained within input present novel decomposition output lstm contributions resulting solely given phrase involving factors. insight behind decomposition gating dynamics unique lstms vehicle modeling interactions variables. past years lstms become core component neural systems. given sequence word embeddings cell state vector computed element iteratively applying equations initialization rd×d rd×d denotes element-wise multiplication. often referred output forget input gates respectively fact values bounded used element-wise multiplication. processing full sequence ﬁnal state treated vector learned features used input multinomial logistic regression often called softmax return probability distribution classes introduce contextual decomposition proposed method interpreting lstms. given arbitrary phrase decompose output cell state equations contributions. decomposition constructed corresponds contributions made solely given phrase corresponds contributions involving least part elements outside phrase. using decomposition ﬁnal output state equation yields provides quantitative score phrase’s contribution lstm’s prediction. score corresponds input logistic regression interpreted standard logistic regression coefﬁcient. cell update equation neuron values independently determined contribution step well prior context provided γt−. thus computing element-wise product often referred gating contributions made interact contributions made vice versa. leverage simple insight construct decomposition. first assume linearizing gates updates equations write linear contributions inputs. linearization cell update equation products gates become products linear sums contributions different factors. upon expanding products resulting cross-terms yield natural interpretation interactions variables. particular cross-terms assigned whether resulted solely phrase e.g. ltanh interaction phrase factors e.g. ltanh purely factors e.g. ltanh. mirroring recurrent nature lstms insights allow recursively compute decomposition initializations derive update equations case current time step contained within phrase. case similar general recursion formula provided appendix clarity decompose products cell update equation separately. discussed above simply linearize gates involved expand resulting product sums group cross-terms according whether contributions derive solely speciﬁed phrase otherwise. terms determined derive solely speciﬁed phrase involve products combination within phrase products involving treated deriving phrase. computed decomposition relatively simple compute resulting transformation linearizing tanh function note could similarly decompose output gate treated forget gate above empirically found produce improved results. however setting {yi} contains terms clear ordering. thus natural order equation instead compute average orderings. letting denote permutations score given below. note corresponding term equal equation linearization presented scalar context trivially generalizes vector setting also viewed approximation shapely values discussed lundberg shrikumar describe empirical validation task sentiment analysis. first verify that standard problem word-level importance scores compares favorably prior work. examine behavior word phrase level importance situations involving compositionality showing able capture composition phrases differing sentiment. finally show capable extracting instances positive negative negation. ﬁrst describe process ﬁtting models used produce interpretations. primary intent paper predictive accuracy used standard best practices without much tuning. implemented models torch using default hyperparameters weight initializations. models optimized using adam default learning rate using early stopping validation set. linear model used vectors model pre-trained glove vectors additional linear layer word embedding dimension number classes tuned word vectors linear parameters. data sets described validate method. trained lstm model binary version stanford sentiment treebank standard benchmark consists movie reviews ranging words long. addition review-level labels also provides labels phrase binarized constituency parse tree. following hyperparameter choices word hidden representations lstm word vectors initialized pretrained glove vectors lstm attains accuracy also train logistic regression model words features attains accuracy. prediction whether review positive negative reviews relatively long average length words. following guidelines zhang implement lstm model attains error ngram logistic regression model attains error. computational reasons report interpretation results random subset sentences length words. computing integrated gradient scores found numerical issues produced unusable outputs roughly samples. reviews excluded. compare interpretations produced four state baselines cell decomposition integrated gradients leave gradient times input. refer reader section descriptions algorithms. gradient baseline compute gradient output probability respect word embeddings report product word vector gradient. integrated gradients producing reasonable values required extended experimentation communication creators regarding choice baselines scaling issues. ultimately used sequences periods baselines rescaled scores review standard deviation scores review trick previously mentioned literature. obtain phrase scores word-based baselines integrated gradients cell decomposition gradients scores words contained within phrase. examining novel phrase-level dynamics ﬁrst verify compares favorably prior work standard case producing unigram coefﬁcients. sufﬁciently accurate terms prediction logistic regression coefﬁcients generally treated gold standard interpretability. particular applied sentiment analysis ordering words given coefﬁcient value provides qualitatively sensible measure importance. thus determining validity coefﬁcients extracted lstm expect meaningful relationship scores logistic regression coefﬁcients. order evaluate word-level coefﬁcients extracted method construct scatter plots point consisting single word validation set. values plotted correspond coefﬁcient logistic regression importance score extracted lstm. quantitative measure accuracy pearson correlation coefﬁcient. report quantitative qualitative results appendix integrated gradients correlations respectively substantially better methods correlations yelp still competitive correlation methods ranging veriﬁed reasonably strong results base case proceed show beneﬁts show that phrases words existing methods unable recognize subphrases differing sentiments. example consider phrase used favorite negative sentiment. word favorite however strongly positive logistic regression coefﬁcient percentile. nonetheless existing methods consistently rank favorite highly negative neutral. contrast shown table able identify favorite strongly positive used strongly negative. similar dynamic also occurs phrase worth time. main justiﬁcation using lstms simpler models precisely able capture kinds interactions. thus important interpretation algorithm able properly uncover interactions handled. using motivating example show similar trend holds throughout yelp polarity dataset. particular conduct search situations similar above strongly positive/negative phrase contains strongly dissenting subphrase. phrases scored using logistic regression n-gram features described section included absolute effective interpretation algorithm distribution scores positive negative dissenting subphrases signiﬁcantly separate positive subphrases positive scores vice versa. however seen appendix prior methods distributions nearly identical. distributions hand signiﬁcantly separate indicating observed anecdotally holds general setting. show prior methods struggle identify cases sizable portion review polarity different lstm’s prediction. instance consider review table ﬁrst phrase clearly positive second phrase causes review ultimately negative. method able accurately capture dynamic. leveraging phrase-level labels provided show pattern holds general case. particular conduct search reviews similar example. search criteria whether review contains phrase labeled opposing sentiment review-level label thirds length review. appendix show distribution resulting positive negative phrases different attribution methods. successful interpretation method would sizable distributions positive phrases mostly positive scores negative phrases mostly negative. however prior methods struggle satisfy criteria. positive phrases labelled negative integrated gradients cell decompositions even distributions ﬂipped negative phrases yielding positive scores positive phrases. hand provides clear difference distributions. quantify separation positive negative distributions examine two-sample kolmogorovsmirnov one-sided test statistic common test difference distributions values ranging produces score indicating strong difference positive negative distributions methods achieving scores indicating weaker distributional differences. given gradient leave weakest performers unigram scores provides strong evidence superiority using phrase labels search training instances negation. particular search phrases length less ﬁrst child containing negation phrase ﬁrst words second child positive negative sentiment. noise labels also included phrases entire phrase non-neutral second child contained non-neutral phrase. identify positive negation isn’t negative negation isn’t interesting direction given sst-provided label phrase. given negation phrase extract negation interaction computing score entire phrase subtracting scores phrase negated negation term itself. resulting score interpreted n-gram feature. note that methods compare against leave capable producing interaction scores. reference also provide distribution interactions phrases length less present distribution extracted scores figure clear distinction positive negative negations negation interactions centered outer edges distribution interactions. leave able capture interactions noticeable overlap positive negative negations around zero indicating high rate false negatives. another beneﬁt using interpretation that addition providing importance scores also provides dense embeddings arbitrary phrases interactions form discussed section anecdotally show similarity embedding space corresponds semantic similarity context sentiment analysis. particular words binary interactions compute average embedding produced across training validation sets. table show nearest neighbours using cosine similarity metric. results qualitatively sensible three different kinds interactions positive negation negative negation modiﬁcation well positive negative words. figure distribution scores positive negative negation coefﬁcients relative interaction coefﬁcients. leave capable producing interaction scores. paper proposed contextual decomposition algorithm interpreting individual predictions made lstms without modifying underlying model. general applications lstms produces importance scores words phrases word interactions using sentiment analysis datasets empirical validation ﬁrst show information also produced prior methods word-level scores method compares favorably. importantly show capable identifying phrases varying sentiment extracting meaningful word interactions. movement beyond word-level importance critical understanding model complex highly non-linear lstms. research started summer internship google brain later supported postgraduate scholarship-doctoral nserc data science research award adobe. partial support acknowledged grant n--- grant wnf. references sebastian bach alexander binder gr´egoire montavon frederick klauschen klaus-robert m¨uller wojciech samek. pixel-wise explanations non-linear classiﬁer decisions layer-wise relevance propagation. plos jeffrey pennington richard socher christopher manning. glove global vectors word representation. proceedings conference empirical methods natural language processing richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment treebank. proceedings conference empirical methods natural language processing hendrik strobelt sebastian gehrmann bernd huber hanspeter pﬁster alexander rush. visual analysis hidden state dynamics recurrent neural networks. arxiv preprint arxiv. sheng richard socher christopher manning. improved semantic representations tree-structured long short-term memory networks. arxiv preprint arxiv. kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhudinov rich zemel yoshua bengio. show attend tell neural image caption generation visual attention. international conference machine learning figure distribution attributions positive sub-phrases contained within negative phrases length yelp polarity dataset. positive negative distributions nearly identical methods except indicating inability prior methods distinguish positive negative phrases occurring context phrase opposite sentiment figure distribution positive negative phrases length thirds full review sst. positive negative distributions signiﬁcantly separate methods indicating even coarse level granularity methods still struggle. figure logistic regression coefﬁcients versus coefﬁcients extracted lstm sst. include least squares regression line. stronger linear relationships plots correspond better interpretation techniques.", "year": 2018}