{"title": "Vision-and-Language Navigation: Interpreting visually-grounded  navigation instructions in real environments", "tag": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "abstract": "A robot that can carry out a natural-language instruction has been a dream since before the Jetsons cartoon series imagined a life of leisure mediated by a fleet of attentive robot helpers. It is a dream that remains stubbornly distant. However, recent advances in vision and language methods have made incredible progress in closely related areas. This is significant because a robot interpreting a natural-language navigation instruction on the basis of what it sees is carrying out a vision and language process that is similar to Visual Question Answering. Both tasks can be interpreted as visually grounded sequence-to-sequence translation problems, and many of the same methods are applicable. To enable and encourage the application of vision and language methods to the problem of interpreting visually-grounded navigation instructions, we present the Matterport3D Simulator -- a large-scale reinforcement learning environment based on real imagery. Using this simulator, which can in future support a range of embodied vision and language tasks, we provide the first benchmark dataset for visually-grounded natural language navigation in real buildings -- the Room-to-Room (R2R) dataset.", "text": "robot carry natural-language instruction dream since jetsons cartoon series imagined life leisure mediated ﬂeet attentive robot helpers. dream remains stubbornly distant. however recent advances vision language methods made incredible progress closely related areas. signiﬁcant robot interpreting naturallanguage navigation instruction basis sees carrying vision language process similar visual question answering. tasks interpreted visually grounded sequence-to-sequence translation problems many methods applicable. enable encourage application vision language methods problem interpreting visuallygrounded navigation instructions present matterportd simulator large-scale reinforcement learning environment based real imagery. using simulator future support range embodied vision language tasks provide ﬁrst benchmark dataset visually-grounded natural language navigation real buildings room-to-room dataset. idea might able give general verbal instructions robot least reasonable probability carry required task long-held goals robotics artiﬁcial intelligence despite signiﬁcant progress number major technical challenges need overcome robots able perform general tasks real world. primary requirements techniques linking natural language vision action unstructured previously unseen environments. navigation version challenge refer vision-and-language navigation figure room-to-room navigation task. focus executing natural language navigation instructions previously unseen real-world buildings. agent’s camera rotated freely. blue discs indicate nearby navigation options. although interpreting natural-language navigation instructions received signiﬁcant attention previously recent success recurrent neural network methods joint interpretation images natural language motivates task associated room-to-room dataset described below. dataset particularly designed simplify application vision-and-language methods might otherwise seem distant problem. previous approaches natural language command robots often neglected visual information processing aspect problem. using rendered rather real images example constrains visible objects hand-crafted models available renderer. turns robot’s challenging open-set problem relating real language real imagery figure differences vision-and-language navigation visual question answering tasks formulated visually grounded sequence-to-sequence transcoding problems. however sequences much longer uniquely among vision language benchmark tasks using real images model outputs actions manipulate camera viewpoint. simpler closed-set classiﬁcation problem. natural extension process adopted works images replaced labels limiting variation imagery inevitably limits variation navigation instructions also. distinguishes challenge agent required interpret previously unseen natural-language navigation command light images generated previously unseen real environment. task thus closely models distinctly open-set nature underlying problem. enable reproducible evaluation methods present matterportd simulator. simulator large-scale interactive reinforcement learning environment constructed densely-sampled panoramic rgb-d images real-world building-scale indoor environments compared synthetic environments real-world image data preserves visual linguistic richness maximizing potential trained agents transferred real-world applications. based matterportd simulator collect room-to-room dataset containing openvocabulary crowd-sourced navigation instructions average length words. instruction describes trajectory traversing typically multiple rooms. illustrated figure associated task requires agent follow natural-language instructions navigate goal location previously unseen building. investigate difﬁculty task particularly difﬁculty operating unseen environments using several baselines sequence-to-sequence model based methods successfully applied vision language tasks summary main contributions introduce matterportd simulator software framework support visual reinforcement learning using matterportd panoramic rgb-d dataset; navigation language natural language command robots unstructured environments research goal several decades within natural language processing community existing approaches abstract away problem visual perception significant degree. typically achieved either assuming navigation goals objects acted upon enumerated identiﬁed label operating severely restricted simulated environments requiring limited perception work contributes ﬁrst time navigation benchmark dataset linguistically visually rich moving closer real scenarios still enabling reproducible evaluations. vision language recent work vision language emanating computer vision community emphasizes beneﬁts end-to-end training pixel data using large datasets natural images. particular development benchmark datasets image captioning visual question answering visual dialog spurred considerable progress vision language understanding. however although many tasks combining visual linguistic reasoning motivated potential robotic applications none tasks allow agent move control camera. proposed benchmark ﬁlls gap. compare vision-and-language navigation figure navigation based simulator simulator similar spirit recently proposed environments based game engines vizdoom deepmind thor main advantage framework pixel observations come real images diverse indoor scenes making almost every coffee pot-plant wallpaper texture unique. results considerably greater visual diversity richness compared game engines utilize limited assets textures rendered different layouts. seeking strike favorable balance interactivity visual realism share similar motivations much smaller active vision dataset also based real images. navigation number recent papers reinforcement learning train navigational agents although works address language instruction. language-based navigation studied however settings visually linguistically less complex. example chaplot develop model execute template-based instructions virtual doom environment misra introduce complex language instructions keeping environment fully-observable visually limited. releasing matterportd simulator dataset hope provide realistic partially-observable setting encourage research. section introduce matterportd simulator large-scale visual reinforcement learning simulation environment research development intelligent agents using real imagery. room-to-room navigation dataset discussed section rgb-d datasets derived video sequences; e.g. nyuv rgb-d scannet datasets typically offer paths scene making inadequate simulating robot motion. contrast datasets recently released matterportd dataset contains comprehensive panoramic views. best knowledge also largest currently available rgb-d research dataset. detail matterportd dataset consists panoramic views constructed rgb-d images building-scale scenes. average panoramic viewpoints distributed throughout entire walkable ﬂoor plan scene average separation panoramic view comprised rgb-d images captured single position approximate height standing person. image annotated accurate camera pose collectively images capture entire sphere except poles. dataset also includes globally-aligned textured meshes annotated class instance segmentations regions objects. selected matterport scenes encompass range buildings including houses apartments hotels ofﬁces churches varying size complexity. buildings contain enormous visual diversity posing real challenges computer vision. many scenes dataset viewed matterport spaces gallery. support navigation tasks involving embodied agents take actions interact environment. therefore extend matterportd dataset constructing interactive reinforcement learning environment building-scale scene. refer resulting framework matterportd simulator. construct simulator allow embodied agent virtually ‘move’ throughout scene adopting poses coinciding panoramic viewpoints. agent poses deﬁned terms position heading camera elevation points associated panoramic viewpoints scene. step simulator outputs image observation corresponding agent’s ﬁrst person camera view. images generated perspective projections precomputed cube-mapped images viewpoint. future extensions simulator also support depth image observations additional instrumentation form rendered object class object instance segmentations main challenge implementing simulator determining state-dependent action space. naturally wish prevent agents teleporting walls ﬂoors traversing non-navigable regions space. therefore step simulator also outputs next step reachable viewpoints agents interact simulator selecting viewpoint nominating camera heading elevation adjustments. actions deterministic. determine scene simulator includes weighted undirected graph panoramic viewpoints presence edge signiﬁes robot-navigable transition viewpoints weight edge reﬂects straight-line distance them. construct graphs ray-traced between viewpoints matterportd scene meshes detect intervening obstacles. ensure motion remains localized removed edges longer finally manually veriﬁed navigation graph correct missing obstacles captured meshes current viewpoint region space enclosed left right extents camera view frustum step effect agent permitted follow edges navigation graph provided destination within current ﬁeld view visible glancing down. alternatively agent always choice remain viewpoint simply move camera. figure illustrates partial example typical navigation graph. average graph contains viewpoints average vertex degree compares favorably grid-world navigation graphs which walls obstacles must average degree less such although agent motion discretized constitute signiﬁcant limitation context high-level tasks. indeed many simulators notionally support continuous motion also discretized action spaces practice simulator deﬁne place restrictions agent’s goal reward function additional context aspects environment task dataset dependent example described section matterportd simulator written using opengl. addition python bindings also provided allowing simulator easily used deep learning frameworks caffe tensorflow within platforms parlai openai various conﬁguration options offered parameters image resolution ﬁeld view. separate simulator also developed webgl figure example navigation graph partial ﬂoor building-scale scene matterportd simulator. navigable paths panoramic viewpoints illustrated blue. stairs also navigated move ﬂoors. browser-based visualization library collecting text annotations navigation trajectories using amazon mechanical turk make available researchers. reluctant introduce dataset without least attempt address limitations biases matterportd dataset observed several selection biases. first majority captured living spaces scrupulously clean tidy often luxurious. second dataset contains people animals mainstay many vision language datasets finally observe capture bias selected viewpoints generally offer commanding views environment alleviating limitations extent simulator extended collecting additional building scans. refer stanford d-d-s recent example academic dataset collected matterport camera. illustrated figure task requires embodied agent follow natural language instructions navigate starting pose goal location matterportd simulator. formally beginning episode agent given input natural language instruction length instruction single word token. agent obpath collect three associated navigation instructions using amazon mechanical turk provide workers interactive webgl environment depicting path start location goal location using colored markers. workers interact trajectory ‘ﬂy-through’ tilt camera viewpoint along path additional context. workers ‘write directions smart robot goal location starting start location’. workers instructed necessary follow exactly path indicated merely reach goal. video demonstration also provided. full collection interface result several rounds experimentation. used us-based workers screened according performance previous tasks. workers participated data collection contributing around hours annotation time. total collected navigation instructions average length words. considerably longer visual question answering datasets questions range four words however given focused nature task instruction vocabulary relatively constrained consisting around words illustrated examples included figure level abstraction instructions varies widely. likely reﬂects differences people’s mental models ‘smart robot’ works making handling differences important aspect task. distribution navigation instructions based ﬁrst words depicted figure serves initial image determined agent’s initial pose comprising tuple position heading elevation agent must execute sequence actions action leading pose generating image observation ot+. episode ends agent selects special stop action augmented simulator action space deﬁned section task successfully completed sequence actions leads agent intended goal location revealed agent generate navigation data matterportd region annotations sample start pose goal location pairs different rooms. pair shortest path relevant weighted undirected navigation graph discarding paths shorter paths contain less model agent recurrent neural network policy using lstm-based sequence-to-sequence architecture attention mechanism recall agent begins natural language instruction initial image observation encoder computes representation step decoder observes representations current image previous action input applies attention mechanism hidden states language encoder predicts distribution next action using approach decoder maintains internal memory agent’s entire preceeding history essential navigating partially observable environment discuss details following sections. language instruction encoding word language instruction presented sequentially encoder lstm embedding vector. denote output encoder step lstmenc denote encoder context used attention mechanism. model action space simulator action space statedependent allowing agents make ﬁne-grained choices different forward trajectories presented. however initial work simplify model action space actions corresponding left right down forward stop. forward action deﬁned always move reachable viewpoint closest centre agent’s visual ﬁeld. left right actions deﬁned move camera degrees. image action embedding image observation resnet- pretrained imagenet extract mean-pooled feature vector. analogously embedding instruction words embedding learned action. encoded image previous action features concatenated together form single vector decoder lstm operates action prediction attention mechanism predict distribution actions step ﬁrst attention mechanism identify relevant parts navigation instruction. achieved using global general alignment function described luong ¯h). compute instruction context figure distribution navigation instructions based ﬁrst four words. instructions read center outwards. lengths proportional number instructions containing word. white areas represent words individual contributions small show. deﬁne navigation error shortest path distance navigation graph agent’s ﬁnal position goal location consider episode success navigation error less threshold allows margin error approximately viewpoint comfortably minimum starting error central evaluation requirement agent choose episode goal location identiﬁed. consider fundamental aspect completing task demonstrating understanding also freeing agent potentially undertake tasks goal. however acknowledge requirement contrasts recent works vision-only navigation train agent stop disentangle problem recognizing goal location also report success agent oracle stopping rule i.e. agent stopped closest point goal trajectory. misra also evaluation. dataset splits follow broadly train/val/test split strategy matterportd dataset test consists scenes instructions. reserve additional scenes instructions validating unseen environments remaining scenes pooled together instructions split train seen. following best practice goal locations test released. instead evaluation server agent trajectories uploaded scoring. calculate predictive distribution next action softmax although visual attention also proved highly beneﬁcial vision language problems leave investigation visual attention vision-and-language navigation future work. investigate supervised training regimes previously described model. first however observe viewpoint determine shortest path target location second given starting pose determine shortest sequence actions neighboring viewpoint observations deﬁne groundtruth shortest-path trajectory instruction sequence. based ground-truth deﬁnition initial approach training uses cross entropy loss step maximize likelihood next ground-truth action given previous ground-truth action. approach ground-truth action action samples back model. however limits exploration results changing input distribution between training testing. address limitations also investigate ‘student-forcing’ sampling actions model step. note unlike language generation problems trivial determine correct target action preceding sequence. approaches fail account temporal credit assignment problem. example navigation error beginning episode usually much costly end. hope investigate advanced approaches future work. implementation details perform minimal text pre-processing converting sentences lower case tokenizing white space ﬁltering words occur least times. simulator image resolution vertical ﬁeld view degrees. number hidden units lstm size input word embedding size input action embedding embeddings learned random initialization. dropout embeddings features within attention model. discretized agent’s heading elevation changes degree increments fast training extract pre-cache feature vectors. train using adam optimizer weight decay batch size cases train convergence report results training iteration lowest training loss. evaluation single-shot test time greedy decoding following standard practice test submission trained training validation data. models implemented pytorch. seen shortest random teacher-forcing student-forcing unseen shortest random teacher-forcing student-forcing test shortest random human student-forcing table average navigation results using evaluation metrics deﬁned section seq--seq model trained studentforcing achieves promising results previously explored environments generalization previously unseen environments challenging. additional baselines learning free report learning-free baselines denote random shortest. random agent exploits characteristics dataset turning randomly selected heading completing total successful forward actions shortest agent always follows shortest path goal. human quantify human performance collecting human-generated trajectories third test using amt. collection procedure similar dataset collection procedure described section major differences. first workers provided navigation instructions. second entire scene environment freely navigable ﬁrst-person clicking nearby viewpoints. effect workers provided information received agent simulator. ensure high standard paid workers bonuses stopping within true goal location. illustrated table exploitative random agent achieves success test appearing slightly challenging validation sets. comparison humans achieved average success rate test illustrating high quality dataset instructions. nevertheless people infallible comes navigation. example dataset figure validation loss navigation error success rate training. experiments suggest neural network approaches strongly overﬁt training environments even regularization. makes generalizing unseen environments challenging. well previously unseen environments. techniques practices used optimize performance existing vision language datasets unlikely sufﬁcient models expected operate environments. vision-and-language navigation important represents signiﬁcant step towards capabilities critical practical robotics. investigation paper introduced matterportd simulator. simulator achieves unique desirable tradereproducibility interactivity visual realism. leveraging advantages collected roomto-room dataset. dataset ﬁrst dataset evaluate capability follow natural language navigation instructions previously unseen real images building scale. explore task investigated several baselines sequence-to-sequence neural network agent. work reach three main conclusions. first interesting existing vision language methods successfully applied. second challenge generalizing previously unseen environments signiﬁcant. anticipate solutions require models beyond simply learning training data. third crowd-sourced reconstructions real locations highly-scalable underutilized resource. existing matterportd data release constitutes building scans already uploaded users process used generate applicable host related vision language problems particularly robotics. hope simulator beneﬁt community providing visually-realistic framework investigate problems navigation instruction generation embodied referring expression comprehension embodied visual question answering human-robot dialog. transfer trained agents real-world settings also important direction future research. regard sequence-to-sequence model studentforcing effective training regime teacherforcing although takes longer train explores environment. methods improve signiﬁcantly random baseline illustrated figure using student-forcing approach establish ﬁrst test result achieving success rate. surprising aspect results signiﬁcant difference performance seen unseen validation environments better explain results figure plot validation performance training. even using strong regularization performance unseen environments plateaus quickly training continues improve performance training environments. suggests visual groundings learned quite speciﬁc training environments. would like thank matterport allowing matterportd dataset used academic community well matterport photographers agreed data included. collection navigation dataset generously supported facebook parlai research award. research also supported australian government research training program scholarship australian research council centre excellence robotic vision australian research council’s discovery projects funding scheme", "year": 2017}