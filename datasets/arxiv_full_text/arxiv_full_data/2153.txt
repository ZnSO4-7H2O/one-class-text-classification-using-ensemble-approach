{"title": "Deep Amortized Inference for Probabilistic Programs", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Probabilistic programming languages (PPLs) are a powerful modeling tool, able to represent any computable probability distribution. Unfortunately, probabilistic program inference is often intractable, and existing PPLs mostly rely on expensive, approximate sampling-based methods. To alleviate this problem, one could try to learn from past inferences, so that future inferences run faster. This strategy is known as amortized inference; it has recently been applied to Bayesian networks and deep generative models. This paper proposes a system for amortized inference in PPLs. In our system, amortization comes in the form of a parameterized guide program. Guide programs have similar structure to the original program, but can have richer data flow, including neural network components. These networks can be optimized so that the guide approximately samples from the posterior distribution defined by the original program. We present a flexible interface for defining guide programs and a stochastic gradient-based scheme for optimizing guide parameters, as well as some preliminary results on automatically deriving guide programs. We explore in detail the common machine learning pattern in which a 'local' model is specified by 'global' random values and used to generate independent observed data points; this gives rise to amortized local inference supporting global model learning.", "text": "probabilistic programming languages powerful modeling tool able represent computable probability distribution. unfortunately probabilistic program inference often intractable existing ppls mostly rely expensive approximate sampling-based methods. alleviate problem could learn past inferences future inferences faster. strategy known amortized inference; recently applied bayesian networks deep generative models paper proposes system amortized inference ppls. system amortization comes form parameterized guide program. guide programs similar structure original program richer data including neural network components. networks optimized guide approximately samples posterior distribution deﬁned original program. present ﬂexible interface deﬁning guide programs stochastic gradient-based scheme optimizing guide parameters well preliminary results automatically deriving guide programs. explore detail common machine learning pattern ‘local’ model speciﬁed ‘global’ random values used generate independent observed data points; gives rise amortized local inference supporting global model learning. probabilistic models provide framework describing abstract prior knowledge using reason uncertainty. probabilistic programs powerful tool probabilistic modeling. probabilistic programming language deterministic programming language augmented random sampling bayesian conditioning operators. performing inference programs involves reasoning space executions satisfy constraints observed values. universal built turing-complete language represent computable probability distribution including open-world models bayesian non-parameterics stochastic recursion consider probabilistic program deﬁne distribution intermediate variable output data sampling distribution easy program forward. however computing posterior distribution hard involving intractable integral. typically ppls provide means approximate posterior using monte carlo methods dynamic programming analytic computation. inference methods expensive solve intractable integral scratch every separate invocation. many inference problems shared structure reasonable expect computing give information compute fact reason believe people able perform certain inferences visual perception quickly—we perceived world many times before leverage accumulated knowledge presented perception task idea using results previous inferences precomputation general make later inferences efﬁcient called amortized inference learning generative model many data points particularly important task leads many related inferences. wishes update global beliefs true generative model individual data points many algorithms possible task require form ‘parsing’ data point posterior inference current generative model guess values local latent variable given observation. local parsing inference needed many many times good candidate amortization. plausible learning local inference amortization would support faster better global learning gives useful local inferences leading virtuous cycle. paper proposes system amortized inference ppls applies model learning. instead computing scratch system instead constructs program takes input forward produces samples distributed approximately according true posterior call guide program following terminology introduced previous work system spend time up-front constructing good approximation inference time sampling fast accurate. huge space possible programs might consider task. rather posing search general program induction problem restrict control original program different data ﬂow. samples random choices order data ﬂowing choices comes different computation. system represent computation using neural networks. design choice reduces search much simpler continuous problem optimizing weights networks done using stochastic gradient descent. system’s interface specifying guide programs ﬂexible enough subsume several popular recent approaches variational inference including perform inference model learning. facilitate common pattern introduce mapdata construct represents boundary global model variables variables local data points. system leverages independence data points implied mapdata enable mini-batches data variance reduction gradient estimates. evaluate proof-of-concept system variety bayesian networks topic models deep generative models. system implemented extension webppl probabilistic programming language source code found webppl repository additional helper code https//github.com/probmods/webppl-daipp. prior probability distribution decomposes product conditionals random choice program. indicates random choice potentially depend previous choices made program. likelihood data need proper probability distribution note vary length across executions probabilistic program sample variable number random variables. system implemented probabilistic programming language webppl examples throughout paper webppl embedded javascript; adds sampling conditioning inference operators purely-functional subset javascript. following example program illustrates basic features program uses mcmc compute approximate posterior distribution return value function model. model simple generative model latent bernoulli variable observed gaussian variable example observed value mean observed gaussian variable dependent value since model returns result program posterior marginal distribution variable rest paper build language adding guide programs amortized inference model-learning constructs. instead approximating posterior collection samples could instead approximate parameterized distribution easy sample from. premise behind variational inference goal parameters close possible closeness typically measured kl-divergence. variational inference must ﬁrst choose parameterized family distributions common choice mean-ﬁeld family fully-factored distribution approximates true posterior independent product parameterized marginals random variable. several existing general-purpose variational inference systems scheme easy work with capture dependencies variables occur true posterior. limitation often acceptable deﬁned relative particular observation thus parameters re-optimized thus scheme provides alternative monte carlo methods faster reliable still solves inference problem scratch. mentioned section amortized inference refers previous inference solutions solve subsequent inference problems faster. exists experimental evidence people leverage experience prior inference tasks asked solve related ones idea inspired research developing amortized inference systems bayesian networks systems model inverting network topology attempting learn local conditional distributions inverted graphical model. amortized inference also achieved variational inference. instead deﬁning parametric family speciﬁc given instead deﬁne general family conditional takes input. setting mean ﬁeld family longer applies factors must functions however extend mean ﬁeld family handle input data using neural networks here parameters local conditional guide computed neural network function variational family supports amortized inference invest time upfront optimizing neural network weights close given never-before-seen guide program forwards trained networks fast inference. several recent approaches ‘neural variational inference’ instantiation design pattern neural guide families easy express extensions webppl. system also allows generalizations pattern providing neural nets previously-made random choices additional input amortized variational inference framework presented also used learn parameters generative model also parameterized i.e. parameters optimized along parameters guide program system supports learning generative model parameters addition guide parameters. setting natural think particular model pattern global parameters random choices affect local ‘observation model’ turn assumed generate data point independently; call mapdata model pattern. show easy pattern maximum-likelihood learning variational bayes even different methods within model. section describe language extensions webppl allow speciﬁcation guide programs. focus manually-speciﬁed guide programs. section build interface automatically derive guide programs. previously mentioned system restricts guide programs control original program meaning guide program samples variables order. implementation enforces restriction deﬁning guide program inline regular program. sample statement addition distribution program samples from also specify distribution guide program sample from. example using simple program section example guide samples bernoulli different success probability particular value happpens give true posterior program since problem simple enough solve closed form. note guide distribution need family prior distribution; later property useful. here param declares optimizable real-valued parameter named we’ll later function also take dims argument specifying dimensions vector matrix tensorvalued parameter. since standard deviation sigma gaussian guide distribution must positive softplus function unbounded value returned param system includes similar transforms parameters domains parameters must named disambiguated optimization engine. using variational parameters directly guide distribution parameters results mean ﬁeld approximation variable mentioned section also compute guide parameters neural network explicitly declaring parameters deﬁning structure large neural networks become verbose instead adnn neural library include neural nets programs previous examples thus conditioned single observation. real models condition multiple observations. system expresses pattern mapdata function mapdata operates much like typical functional programming language important features optimization engine treats every execution mapped function independent thus optimization engine operate stochastic mini-batches data sized according batchsize option. property clearly important efﬁcient scalable optimization; section property also directly leveraged improve optimization. thus focused deﬁning parameterized guides inference. parameterized guides also used make models learnable. following three code blocks show possible replacements line previous example replacing hardcoded constant mu_x learnable version code left block results maximum likelihood estimation. using delta distribution guide inference engine optimize single best parameter value maximum likelihood behavior comes using improper uniform distribution prior. pattern common enough system provides convenient shorthand middle code block demonstrate l-regularized maximum likelihood learning replacing improper uniform prior gaussian prior. inference engine still predict point estimate mu_x gaussian prior results regularization. finally right block shows variational bayesian model mu_x gaussian prior distribution guide samples mu_x approximate variational gaussian posterior optimizable parameters. form learns distribution generative models maintaining estimate uncertainty true model. note could alternatively implemented maximum likelihood direct parameterization e.g. mu_x param. however style results parameterized addition complicates implementation theoretical analyses show later paper. contrast chosen scheme guide parameterized; learning model part learning guide. example code built section describes bayesian network continuous latent variable continuous observation. figure shows fully assembled code along graphical model depiction using notation kigma welling diagram solid arrows indicate dependencies generative model given main program dashed arrows indicate dependencies guide program. shorthand neural network parameters guide program. figure bottom shows modify code instead discrete latent variable observation; equivalent gaussian mixture model. example simplex function maps vector n-dimensional simplex process produces vector weights suitable component probabilities discrete random variable. figure shows slightly complex bayesian network latent continuous variables. note guide program example predicts latent variables independently given observation figure bottom make small changes code highlighted green) instead guide program predict second latent variable function ﬁrst latent variable small change allows guide capture posterior dependency ingored ﬁrst version. section mentioned goal variational inference values parameters guide program close possible true posterior closeness measured kl-divergence. kl-divergence general distributions intractable compute; however straightforward algebra produces objective tractable figure webppl code corresponding graphical models simple bayesian networks latent variables observation. guide program predicts latents independently. bottom changing guide program treat second latent variable conditional ﬁrst last inequality follows kl-divergence non-negative. turn implies lower bound marginal likelihood data accordingly sometimes referred ‘evidence lower bound’ elbo maximizing elbo corresponds minimizing kl-divergence. include unnormalized factors properly called variational free energy. alternative derivation elbo using jensen’s inequality mnih gregor jordan estimator goes names ‘likelihood ratio estimator’ ‘score function estimator’ also equivalent reinforce policy gradient algorithm reinforcement learning literature derivations estimator relevant setting found wingate weber mnih gregor intuitively gradient update step using estimator pushes parameters direction q—that direction maximize probability guide. since goal optimization learn approximation true posterior update weighted based probable joint distribution high probability joint probability current guide term produces large update joint guide assign equal probability update zero magnitude. joint assigns lower probability guide does resulting gradient update move parameters opposite direction estimator straightforward compute requiring differentiable respect however known exhibit high variance. problem amenable several variance reduction techniques employ later section. pathwise estimator equation suffers high variance gradient longer pushed inside expectation expectation respect depends parameters respect differentiating. however certain cases possible re-write elbo expectation distribution depend situation occurs whenever latent variables expressed samples un-parameterized distribution followed parameterized deterministic transformation parameter naturally support type transformation types continuous variables often well-approximated deterministic transformations unit normal variables using ‘reparameterization trick’ allows elbo gradient rewritten estimator called ‘pathwise derivative estimator’ transforms guide target distributions distributions independent random ‘noise’ variables followed complex parameterized deterministic transformations. given ﬁxed assignment noise variables derivatives propagate ﬁnal probabilities back input parameters leading much stable gradient estimates estimator. general probabilistic program makes many random choices amenable reparameterization trick others not. discrete random choices never reparameterizable. continuous random choices reparameterizable expressed parameterized deterministic transformation parameter-less random choce. implementation every continuous random choice type either property well-approximated random choice type thus rest paper equate continuous reparameterizable discrete non-reparameterizable. optimize elbo probabilistic program associated guide program seek single uniﬁed gradient estimator handles discrete continuous choices. first simplify notation drop dependency derivations follow. equivalent making parameters globally available also true system second assume random choices made guide program ﬁrst drawn distribution transformed deterministic function assumption distribution deﬁned guide program factors mentioned section length vary across executions. however since guide program construction samples random choices order target program factors given execution. distributions functions different meanings depending whether variable continuous discrete continuous unit normal uniform distribution parameterized transform. direct application reparameterization trick. case local transformation also depend previous noise variables choices occuring later program compound transformations earlier choices. estimator includes estimators special cases. random choices reparameterized depend thus zero term drops leaving term. random choices reparameterized q|y) term drops using identity term also zero since dependent leaves term. equation correct estimator elbo gradient like estimator presence discrete random choices lead high variance. thus modify estimator three variance reduction techniques sets indices continuous discrete random choices second line used fact program execution respectively. reparameterized choices identity non-reparameterized choices. equation similar ‘surrogate loss function’ gradient estimator schulman execution probabilistic program corresponds stochastic computation graphs. analysis concerned general stochastic objectives however focus particularly elbo. section showed webppl programs infer function perform non-amortized inference model function. optimize parameters amortized inference webppl provides optimize function similar interface code performs gradient update steps model using adam stochastic optimization method return value params function parameter names optimized parameter values. learned guide used make inferences never-before-seen observations. example we’ll gaussian mixture model program figure bottom show predict cluster assignments observations. note observations used program assigned variable passed mapdata. webppl purely functional language support assigning dataset obs. however provide special globalstore object whose ﬁelds re-assigned. mind modify gaussian mixture model program follows make predictions either running guide program forward true posterior complex learned guide partially approximates guide program importance sampler within sequential monte carlo. forward sampling guide also used generate synthetic data learned distribution. make slightly modiﬁed version gaussian mixture model samples data instead observing used forward sampling optimized parameters params synthesize data points useful e.g. ﬁne-tuning existing parameters dataset. indeed model even need program originally used learn params; needs declare parameters names parameters params. useful example making modiﬁcation existing model without re-train guide program scratch bootstrap training simpler model complex one. detailed specify optimize guide programs system section experimentally evaluate well programs written system learn generative models approximate posterior samplers. unless stated otherwise following settings experiments section ﬁrst consider simple gaussian mixture model program figure bottom. program samples discrete random choices gradient estimator include term. alternatively could re-write program slightly explicitly marginalize discrete random choices; appendix marginalizing choices leads tighter bound marginal likelihood would expect version program achieve higher elbo. extra beneﬁt gradient estimator program reduces estimator lower variance. beneﬁts come cost amortized inference however version program guide predict latent cluster assignment given observed point. also consider non-amortized mean ﬁeld version program comparison. figure illustrates performance programs training steps synthetic datset points. left show elbo changes optimizaton. expected elbo progress asymptotes higher value marginalized model. right show estimated negative likelihood separate synthetic test program optimization. here also include true model comparsion. suggested optimization performance model discrete choices marginalized performs best. note amortized guide program slightly out-performs mean ﬁeld guide program indicating generalization provided amortization beneﬁts training generative models addition enabling fast predictions latent variables previously-unseen observations. next consider complicated bayesian network model based qmr-dt medical diagnosis network qmr-dt bipartite graphical model layer nodes corresponding latent causes second layer observed effects nodes binary cause nodes connected effects directed noisy-or links. appendix shows implementation. amortized guide program model uses neural network jointly predict probabilities latent cause variables given observed effects. since qmr-dt model contains large number discrete random variables expect variance reduction strategies introduced section signiﬁcant effect. thus consider training guide program variance reduction per-choice likelihood ratio weights per-choice weights baselines point reference also include mean ﬁeld model uses variance reduction strategies. data experiments sampled randomly-generated graph figure performance simple gaussian mixture model program. elbo optimization progress training. optimization objective marginalized model tighter bound marginal likelihood thus higher asymptote. negative log-likelihood held-out test set. causes effects. sample observations training additional held-out test set. figure shows results experiments. left plot shows optimization progress condition. without variance reduction strategies gradients extremely noisy optimization makes almost progress. using local per-variable likelihood ratio weights allows optimization progress adding per-variable baselines boosts performance. though uses variance reduction strategies mean ﬁeld model trains signiﬁcantly slowly variance-reduced amortized models. happens mean ﬁeld model separate parameters training observation rather single parameter shared neural network i.e. many parameters updated gradient steps. amortization thus facilitates fast posterior prediction exhibits faster training parameter sharing. next evaluate guide program’s posterior prediction ability. learned guide sample latent causes given observed effects test sample effects given causes record percentage active effects test observation correctly predicted effects ‘hallucinated’ model. speciﬁcally vector effect variables length metric etrue effects test esampled hallucinated model. figure right plots average score runs compare amortized guide program using prior program sample latent causes. learned guide program correctly predicts twice many active effects. addition guide program described above predicts latent causes jointly given observed effects also experimented ‘factored’ guide programs predict latent cause one-by-one given observed effects. consider guide predicts latent cause independently well guide introduces dependencies latent causes recurrent neural network recurrent network receives input value latent cause sampled maintaining persistent hidden state that theory capture values latent causes sampled thus far. gated recurrent unit architecture ability capture longer-range dependencies hidden state dimension code programs shown appendix stepsize optimization. figure compares guide programs joint guide used earlier section. independent factored guide performs slightly less well joint guide adding recurrent neural network capture posterior dependencies improves performance slightly better joint guide. caveat factored guide takes signiﬁcantly longer train figure performance qmr-dt model. elbo optimization progress training. percentage test active effects correctly predicted using latent causes sampled either prior guide program. figure experimenting factored guide programs qmr-dt. elbo optimization progress training. percentage test active effects correctly predicted using latent causes sampled either prior guide program. also used system implement amortized inference latent dirichlet allocation topic models data abstracts taken stanford computation cognition lab’s publication page. experimented different amortized guide programs. ﬁrst local word document learns predict latent topic word given word latent topic distribution document. second local document learns predict latent topic distribution document given words document latent word distributions topic. guides support amortized inference different granularities thus different parameter sharing characteristics lead different learning behavior. comparison also included non-amortized conditions mean ﬁeld model mean ﬁeld model latent choice topic word marginalized code programs found appendix topics learning rate experiments. figure shows results experiments. optimization progress plot left marginalized mean ﬁeld model achieve highest elbo. consistent results gaussian mixture model experiments marginalizing latent variables possible leads tighter bound marginal likelihood. amortized guides word-level guide performs better likely increased parameter sharing. document-level guide performs least well mean ﬁeld model also able efﬁciently predict topic distributions previously-unseen documents. figure right shows highest probability words inferred topic marginalized mean ﬁeld model. left right topics appear experiments pragmatic language models knowledge acquisition probabilistic programming languages grab remaining topics. figure performance latent dirichlet allocation models. elbo optimization progress training. optimization objective marginalized model tighter bound marginal likelihood thus higher asymptote. highest probability words inferred topic best-performing model. system naturally supports generative models neural network components. prominent examples models class include variational autoencoder sigmoid belief networks models sample latent variables multivariate distribution transform result neural network produce observed variables often form image. uses latent multivariate gaussian distribution whereas uses latent multivariate bernoulli. appendix shows implementations models system. implementation follows original description model kingma welling implementation follows mnih gregor uses -dimensional latent code uses single layer hidden variables. system cannot express two-layer mnih gregor guide model samples latent variables reverse order generative model. figure left shows results training models mnist dataset using adam step size models train quickly ﬁrst sbn’s training slows noticeably vae’s discrete nature. takes three times many iterations achieve elbo. figure right qualitatively evaluate models using reconstruct images mnist test set. guide program sample latent variables conditional images target column transform latent variables using generative model’s neural networks produce reconstructed images columns. suggested training behavior able generate higher-quality reconstructions less training. optimization exhibits differences previous work. kingma welling exploit closed-form solution divergence gaussians create figure evaluating variational autoencoder sigmoid belief network programs mnist dataset. elbo optimization progress training. reconstructing images target column using models. even lower-variance estimator elbo gradient. general formulation system still successfully train model. mnih gregor neural networks compute per-variable baselines equation whereas simpler approach however point models described simple webppl program neural guides optimized default system without need additional implementation efforts. thus shown succesfully create train guide programs several types generative models. however writing guide programs sometimes tedious repetitive; example note large amount shared structure guides shown figures furthermore always obvious write good guide program. figure knowledge structure simple generative model direct dependency latent variables guide. general programs—especially large complex ones—it always clear dependencies capture guide. section describes early experience automatically deriving guide programs. ﬁrst describe system provides sensible default behavior make writing guides less cumbersome. outline system might extended automatically derive guides program using recurrent neural networks. parameter bounding transforms softplus applied based bounds metadata provided primitive distribution type. reparameterizable guides continuous distributions since process declares optimizable parameters automatically must automatically generate names parameters. system names parameters according declared program execution trace using naming technique used random choices probabilistic programming mcmc engines since names parameters tied structure program cannot re-used programs section experimented factored guide program qmr–dt model. think general style guide—predicting random choice sequence conditional hidden state recurrent neural network—might generalized automatic guide program probabilistic program decomposed sequence random choices. qmr-dt experiments used separate neural network predict latent variable complex models large data sets approach would lead computationally unfeasible explosion number parameters. furthermore likely prediction computations many random choices program related. example qmr-dt program latent causes share many dependent effects well predicted similar networks. given insights imagine universally-applicable guide uses single prediction network random choices random choice provides additional identifying input. elements vector space ‘similar’ random choices close another distance metric vector space. possible obtain would learn embedding program-structural addresses random choice might learned end-to-end fashion making learnable parameter vectors overall variational optimization paper presented system amortized inference probabilistic programs. amortization achieved parameterized guide programs mirror structure original program trained approximately sample posterior. introduced interface specifying guide programs ﬂexible enough reproduce state-of-the-art variational inference methods. also demonstrated interface supports model learning addition amortized inference. developed proved correctness optimization method training guide programs evaluated ability optimize guides bayesian networks topic models deep generative models. many exciting directions future work pursue improving amortized inference probabilistic programs. system presented paper provides platform explore possibilities modeling paradigms paper focused common machine learning modeling paradigm global generative model generates many data points. many modeling paradigms consider. example time series data common machine learning applications. developed mapdata facilitate efﬁcient inference data models might develop analogous data processing function time series data using neural guides setup would permit amortized inference models deep kalman filters computer vision computer graphics common paradigm generative image models factor image generation multiple steps condition step partially-generated image thus ‘yield-so-far’ models also possible implement system. better gradient estimators variance reduction strategies employed optimizer make inference discrete variables tractable still noticeably less efﬁcient purely continuous models. fortunately ongoing efforts develop better general-purpose discrete estimators stochastic gradients possible adapt methods probabilistic programs. automatic guides discussed section believe automatically deriving guide programs using recurrent neural networks soon possible. recent enhancements recurrent networks necessary make reality. example external memory neural turing machine better capturing certain long-range posterior dependencies might also draw inspiration neural programmer-interpreter whose stack recurrent networks communicate arguments might better capture posterior dataﬂow arbitrary programs. learning objectives paper focused optimizing elbo. direction divergence equation resulting functional upper bound marginal likelihood data—an ‘evidence upper bound’ eubo. computing eubo gradient requires samples true posterior thus unusable many applications entire goal amortized inference tractably generate samples. however applications beneﬁt goal speed existing tractable inference algorithm posterior execution traces available means also less extreme ways exploit idea learning. example mapdata-style program might interleave normal elbo updates steps hallucinate data posterior predictive train local guide correctly parse ‘dreamed-up’ examples. scheme bears resemblance wake-sleep algorithm control system’s one-to-one mapping random choices guide target program makes deﬁnition analysis guides simple scenarios ﬂexibility useful. cases want insert random choices guide occur target program models natural hierarchy latent variables observed variables guide ‘backwards’ observed variables top-most latents shown useful worth exploring support control deviations general-purpose probabilistic programming inference system. material based research sponsored darpa agreement number fa---. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright notation thereon. views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied darpa u.s. government.", "year": 2016}