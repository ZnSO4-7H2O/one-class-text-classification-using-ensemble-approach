{"title": "Causal Conclusions that Flip Repeatedly and Their Justification", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Over the past two decades, several consistent procedures have been designed to infer causal conclusions from observational data. We prove that if the true causal network might be an arbitrary, linear Gaussian network or a discrete Bayes network, then every unambiguous causal conclusion produced by a consistent method from non-experimental data is subject to reversal as the sample size increases any finite number of times. That result, called the causal flipping theorem, extends prior results to the effect that causal discovery cannot be reliable on a given sample size. We argue that since repeated flipping of causal conclusions is unavoidable in principle for consistent methods, the best possible discovery methods are consistent methods that retract their earlier conclusions no more than necessary. A series of simulations of various methods across a wide range of sample sizes illustrates concretely both the theorem and the principle of comparing methods in terms of retractions.", "text": "past decades several consistent procedures designed infer causal conclusions observational data. prove true causal network might arbitrary linear gaussian network discrete bayes network every unambiguous causal conclusion produced consistent method non-experimental data subject reversal sample size increases ﬁnite number times. result called causal ﬂipping theorem extends prior results eﬀect causal discovery cannot reliable given sample size. argue since repeated ﬂipping causal conclusions unavoidable principle consistent methods best possible discovery methods consistent methods retract earlier conclusions necessary. series simulations various methods across wide range sample sizes illustrates concretely theorem principle comparing methods terms retractions. past decades several procedures designed infer causal conclusions observational data. however essential diﬀerence causal conclusions based experiments inferred non-experimental data. consider randomized experimental trial determine whether causes following usual logic statistical testing suspend judgment regarding relationship statistically signiﬁcant correlation detected. sample size suﬃciently large safely conclude missed eﬀect small practical consequence even exists. cannot said causal relations discovered even unambiguously nonexperimental data underlying causal truth assumed linear gaussian discrete bayes orientation arbitrarily large causes retracted reversed sample size increases cases impossible non-trivial interval estimates eﬀect policy; depending orientation relevant causal connection result policy might either null quite large paper extend result causal discovery cannot reliable given sample size. prove every causal conclusion drawn consistent causal discovery procedure orientation number times arbitrarily high chance sample size increases long truth might linear gaussian network discrete bayes network. case regardless sample size ﬁrst conclusion sequence produced. also true regardless strength causal connection practical consequences ﬂips could immense. finally remains true even truth might arbitrary linear gaussian discrete bayes net. application causal ﬂipping theorem causal ﬂips arrow depicted ﬁgure unavoidable regardless parameter settings ﬁrst network sequence causal discovery procedure consistent networks either linear gaussian discrete. illustrate causal ﬂipping theorem showing simulations four published causal discovery algorithms perform causal ﬂips ﬁgure high chance provided random samples increasing size generated ﬁxed setting free parameters third causal network series. sampling density i.i.d. samples size typically question whether bound chance error given sample size least choose method converges truth probability. consistent exists sample size claimed many published algorithms causal discovery proposal described greater detail below discovery procedures motivated justiﬁed better reﬁnes concept consistency minimizing ﬂuctuations chances producing alternative theories prior convergence unavoidable principle. familiar fact causal questions necessarily reducible statistical dependency since statistical dependency compatible cause cause common cause bold insights recent literature causal discovery that nonetheless interesting causal conclusions determined entirely probability under plausible assumptions. section present basic ideas behind approach order notation—extended motivated presentations found ﬁxed ﬁnite random variables denote probability measures induced joint probability densities variables denote directed acyclic graphs ﬁxed variable arrows directed edges dags understood indicate causal inﬂuence patterns statistical dependencies. conditional values variables call statement conditional independence constraint denote cics deﬁne pattern cics satisﬁed markov variables case causal ﬂipping proper justiﬁcation causal discovery sense best causal discovery algorithms minimize ﬂipping prior convergence true model. shown causal inference methods systematically prefer simpler models exactly also minimize retractions earlier judgments. result called ockham eﬃciency theorem provides theoretical justiﬁcation existing algorithms favor simpler models; justiﬁcation depend questionbegging simplicity-biased prior probabilities. moreover methods retract conclusions less said improve upon methods retract more retraction minimization provides objective standard progress design causal discovery algorithms. simulations conclusion paper illustrate concrete comparisons published methods. probability measures common σ-ﬁeld. statistical question presupposition partition countable collection mutually exclusive exhaustive theories. theory contains unknown true probability measure means samples example might whether mean unknown one-dimensional normal distribution standard variance exactly case normal measures standard variance measures mean measures non-zero mean. question question since function theory uniquely determines hence dags potential causal signiﬁcance. dags share feature feature entailed statistical hypothesis statistical question strong causal implications. therefore causal discovery decomposes naturally parts purely empirical problem determining sampling purely deductive problem recovering much recent progress causal discovery centered deductive phase problem proceeds folthus causal skeleton true unshielded collisions recoverable uniquely purely probabilistic information also possible derive conclusions results orienting unshielded collision follows also dags acyclic directed path edge safe conclude conditions iterated orientations obtained. resulting method called algorithm initials inventors authors produced several eﬃcient sophisticated variants often pertinent causal question concerns local feature causal structure whether causes causes neither rather global causal structure like pattern concerns variables causal edge essential graph contains directed edge deﬁne preceding section focused entirely purely deductive inference gip. must inferred samples. approach assume given true rejected statistical test. signiﬁcance levels tests adjusted downward suﬃciently slow rate procedure converges probability application deductive rules computes pattern uniquely determines gip. resulting method consistent sample size increases number times sample size increases. reversal chance inferred causal orientation causal ﬂip. causal ﬂips surprising algorithm since algorithm bases conclusions entirely pattern currently accepted rejected null hypotheses accepted null hypotheses rejected high chance higher sample sizes. causal ﬂipping theorem however asserts unavoidability arbitrarily many causal ﬂips causal discovery method consistent arbitrary linear gaussian discrete bayes networks linear gaussian theoretical possibilities. discrete bayes cases every consistent method essentially forced base causal conclusions inferred patterns conditional independence inferred independence hypotheses always overturned sample size increases. includes methods based consistent scoring rules like even methods based bayesian posterior probabilities. recent work establishes questions exclude linear gaussian models discrete bayes nets possibilities true causal structure identiﬁed without risking retractions earlier conclusions. however approach avoids retractions producing causal conclusions whatever departure linear gaussian case noticeable data. therefore unless foregoes causal conclusions linear gaussian models causal ﬂipping remains unavoidable feature causal discovery non-experimental data. tance natural measure indistinguishability since bounds diﬀerence chances measures assign arbitrarily chosen acceptance zone test distinguishes them. total variation space topology induced open ρ-balls. applied sampling densities statistical applications assume ﬁrst converts sampling densities induced inﬁnite product measures. forced ﬁrst conjecture arbitrarily high probability later conjecture arbitrarily high probability larger sample size. why? suppose consistent solution small. exists since size deﬁniρ tion follows since event consistency guarantees thus chance produces plummets whopping sample size sample size change heart increasing information mere matter sampling noise; represents fundamental reversal signal method sends user regarding theory consequence nothing consistency unavoidable structural fact question addressed namely much implicit standard textbook discussions statistical power argument iterates sample size increases without bound. suppose confronted question emconsistent forced return various theories arbitrarily high probability sample size increases. matter clever subtle consistent method mining empirical samples—in fact sensitive informative data-mining techniques leap successive theories empirical approximation chain quickly dull insensitive ones. moreover turns bound time theories produced—they happen nature chooses method paying attention. ideas made precise following proposition central lemma causal ﬂipping theorem represents total retractions chance sample size increases worst-case bound respect tuning preceding proposition arbitrarily obtains following lower bounds retractions chance dependency driven-ness second important concept paper. dependency driven statistical dependencies suﬃce disambiguate causal questions disambiguated. happens studied applications causal discovery dependency driven proof crucially employs fact unfaithful linear gaussian discrete multinomial distributions lebesgue measure zero; linear gaussian case discussed discrete case causal edge ﬂips times. proposition trivial suppose suppose contains edge either tx→y tx−y extend complete non-isolated vertices edge proposition results sequence edge edge obtain then isolated unshielded collision proposition edge essential hence iterate construction isolated variables used trials sample size examined four causal search algorithms implemented tetrad version ..-. algorithms simulated retractions chance algorithms causal scenario depicted ﬁgure depicted ﬁgures algorithm hesitant false conclusion example depicted slight consistent method edge orientation question dependency driven subject ﬂipping number times matter strong evidence conclusion happens present. bound causal ﬂips based number demonstrated that causal inference instances scientiﬁc theorizing generally methods minimize retractions employ ockham’s razor i.e. systematic preference simpler models basic idea simpler theories imply fewer detectable eﬀects eﬀects arbitrarily subtle phenomena encountered arbitrarily late arbitrarily large samples. therefore positing theory implies potentially subtle eﬀect detected leaves open retract theory effect never appears retraction gets added unavoidable retractions follow resulting sub-optimal retraction bound. ought produce complex theories imply extra eﬀects potentially subtle eﬀects veriﬁed. standard causal discovery procedures like implement version ockham’s razor employ systematic bias toward causal networks fewer edges. explains nearly optimal performance terms retractions contrast simulations. algorithms violate ockham’s razor exhibit spikes chance producing orientation around sample size algorithm example seeks minimize conditional independence tests deducing cics others assuming data faithful easily false sample variation. consequence pc’s computational heuristics result less direct convergence truth directness measured terms total retractions chance. penalty violating ockham’s razor unreliability—reliability impossible question event unless adopts unrealistic background assumptions stronger faithfulness. however demonstrable penalty violating ockadvocates causal discovery techniques invoke principles stronger causal faithfulness rule possibility causal ﬂips bayesians prior probabilities make ﬂips improbable. contrast propose justiﬁcation existing causal discovery procedures consequence unavoidability ﬂipping rather assurances ﬂipping happen. since repeated causal ﬂips unavoidable consistent methods best truth-seeker expect minimize ﬂips retractions hence propose retraction minimization provides best available explanation causal discovery algorithms true causes better alternative strategies biased simple causal networks least application linear gaussian discrete causal networks. comparisons retraction minimization approach alternative foundations model selection ham’s razor additional retraction chance incurred downside probability spike. contrast algorithm cross-check recommended manner result avoids violating ockham’s razor manner described. thus avoids spike score-based method. therefore simulation study concretely illustrates real methods retraction minimization theory motivate justify concrete improvements data mining technology. material based upon work supported national science foundation grant opinions ﬁndings conclusions recommendations expressed material author necessarily reﬂect views nsf. paper beneﬁted detailed criticisms clark glymour. authors also indebted peter spirtes ramsey richard scheines theoretical discussions help tetrad. tetrad interface implemented ramsey extremely well-designed facilitated simulation studies. would also like thank cosma shalizi comments concerning related paper formal epistemology workshop few.", "year": 2012}