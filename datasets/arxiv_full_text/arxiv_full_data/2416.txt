{"title": "Identifying the Relevant Nodes Without Learning the Model", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We propose a method to identify all the nodes that are relevant to compute all the conditional probability distributions for a given set of nodes. Our method is simple, effcient, consistent, and does not require learning a Bayesian network first. Therefore, our method can be applied to high-dimensional databases, e.g. gene expression databases.", "text": "propose method identify nodes relevant compute conditional probability distributions given nodes. method simple eﬃcient consistent require learning bayesian network ﬁrst. therefore method applied high-dimensional databases e.g. gene expression databases. part project atherosclerosis gene expression data analysis want learn bayesian network answer query state certain atherosclerosis genes given state genes. denotes nodes denotes target nodes want learn answer query form unfortunately learning impossible resources contains thousands nodes. however really need learn able answer query suﬃces learn irrelevant nodes maximal subset prove unique thus optimal domain learn because minimal subset contains nodes relevant answer queries propose following method identify exists sequence nodes starting ending every consecutive nodes sequence marginally dependent. prove method consistent assumptions strict positivity composition weak transitivity. argue assumptions restrictive. worth noting method eﬃcient terms runtime data requirements thus applied high-dimensional domains. believe method helpful working domains identifying optimal domain learning reduce costs drastically. although knowledge problem addressed paper studied before exist papers address closely related problems. geiger shachter show identify structure nodes whose parameters needed answer particular query. druzdzel show identify nodes removed without aﬀecting answer particular query. mahoney laskey propose algorithm based work druzdzel construct fragments minimal answer particular query. madsen jensen show identify operations junction tree skipped without aﬀecting answer particular query. main diﬀerences works contribution. first focus single query focus queries target nodes. second require learning structure ﬁrst not. going details contribution review concepts following section. following deﬁnitions results found books bayesian networks e.g. pearl studen´y denote non-empty ﬁnite random variables. bayesian network pair structure directed acyclic graph whose nodes correspond random variables parameters parameters specifying probability distribution given parents represents probability distribution factorization therefore clear answer query hereinafter probability distributions dags deﬁned unless otherwise stated. denote three mutually disjoint subsets x⊥⊥y|z denote independent given probability distribution denote separated graph true every undirected path node node exists node path either parents path parents path neither descendants hand undirected graph true every path node node exists path. probability distribution faithful sep. independence sep. minimal independence removing edge makes cease independence strictly positive unique minimal undirected independence built edge exclusion algorithm nodes adjacent alternatively built markov boundary algorithm nodes adjacent belongs markov boundary markov boundary u\\{x} ⊥⊥u\\ x\\{x}|x proper subset satisﬁes denote four mutually disjoint subsets probability distribution satisﬁes following four properties symmetry x⊥⊥y|z decomposition x⊥⊥y x⊥⊥y|z weak union x⊥⊥y∪w|z x⊥⊥y|z∪w contraction w|z. strictly positive satisﬁes intersection property x⊥⊥y|z∪ x⊥⊥w|z∪ x⊥⊥y w|z. dag-faithful ug-faithful satisﬁes following properties composition y|z∧ weak transitivity x⊥⊥y|z∧x⊥⊥y|z∪{w} x⊥⊥{w}|z∨{w}⊥⊥y|z decomposition. therefore really need learn able answer query suﬃces learn following theorem characterizes sets nodes irrelevant. theorem denote nodes that {xi}. then maximal subset i⊥⊥t|z u\\t\\i. proof since x⊥⊥t|z∪{x} x}⊥⊥t|z contraction. together x⊥⊥t|z implies x}⊥⊥ contraction again. continuing process rest nodes proves i⊥⊥t|z. assume exists then decomposition weak union. contradiction consequently maximal subset follows theorem nodes irrelevant subset decomposition weak union. therefore optimal domain learn minimal subset contains nodes relevant answer queries theorem characterizes t\\{x}. unfortunately practical characterization potentially huge number conditioning sets consider. following theorem gives practical characterization proof exists path like described theorem t\\{x} undirected independence consequently theorem denote sequence nodes shortest path node since minimal undirected independence x⊥⊥t|u\\t\\{x xn−} weak union thus theorem true equation prove start proving xi⊥⊥ xj|u\\{xi consecutive sequence. assume proof analogous equation seen equations sequence satisﬁes equation replacing equations ensure every consecutive nodes dependent equation ensures every non-consecutive nodes independent. therefore repeat calculations sequence replacing {x}. allows successively remove nodes sequence conclude sequence satisﬁes equation replacing u\\{x xn−}. then xn|u xn}. theorem above really need learn suﬃces identify nodes connected components include nodes identiﬁed follows. first initialize second repeat following step possible node considered before nodes adjacent finally remove second step solved help edge exclusion algorithm markov boundary algorithm. conditioning every independence test edge exclusion algorithm performs size |u|−. hand largest conditioning tests markov boundary algorithm performs least size largest markov boundary connected components. therefore algorithms require large learning database return true adjacent nodes high probability conditioning sets tests quite large. problem small learning database available instance gene expression data analysis collecting data expensive. following theorem gives characterization tests marginal independence thus method identify gives rise requires minimal learning data. theorem strictly positive probability distribution satisfying composition weak transitivity. then exists sequence xi⊥⊥ xi+|∅ proof denote nodes exists sequence like described theorem. denote nodes exists path minimal undirected independence nodes sequence except last must otherwise contradiction adjacent nodes sequence marginally independent then thus theorem moreover otherwise contradiction exists sequence like described theorem. then composition thus decomposition weak union. consequently theorem thus method identify follows theorem above really need perform assume markov boundary node obtained incremental association markov boundary algorithm which knowledge existing algorithm satisﬁes requirements scalability consistency assuming composition |u|/ marginal independence tests adopt following implementation. first initialize second repeat following step possible node considered before nodes marginally dependent finally remove therefore implementation eﬃcient terms runtime data requirements thus applied high-dimensional domains identifying optimal domain learning reduce costs drastically. also worth noting irrelevant nodes necessarily mutually independent i.e. arbitrary dependence structure. goes without saying guarantee still large learn case reduce another solution consider queries conditioning includes context nodes words learnt able answer query rest. repeating reasoning above suﬃces learn maximal subset following theorem shows obtained applying theorem assumption independencies discuss assumption next section. theorem strictly positive probability distribution satisfying composition weak transitivity independencies then result applying theorem equivalently exists sequence xi⊥⊥ xi+|c proof denote three mutually disjoint subsets since independencies then x⊥⊥y|z implications. first theorem applied satisﬁes strict positivity composition weak transitivity properties since satisﬁes them. second result applying theorem however xi+|∅ xi⊥⊥ xi+|c note theorem implies unique. also worth noting claimed optimal domain learn reason minimal e.g. includes nodes identify optimal domain learn purged follows running theorem find remove continue purging resulting set. prove purging node never makes irrelevant node become relevant. suﬃces prove ⊥⊥t| hand because then contraction decomposition. consequently ⊥⊥t|∪z u\\t\\\\{y thus optimal domain proves context nodes help reduce finally worth mentioning theorems prove corresponding methods identify correct independence tests correct also prove methods consistent tests consistent since number tests methods perform ﬁnite. kernel-based independence tests consistent probability distribution exist gaussian distributions commonly used independence test fisher’s test consistent well speciﬁcally papers show probability error tests decays exponentially zero sample size goes inﬁnity. argue assumptions strict positivity composition weak transitivity made theorems restrictive. assumption strict positivity justiﬁed real-world applications typically involve measurement noise. note gaussian distributions strictly positive. recall section dag-faithful ug-faithful probability distributions satisfy composition weak transitivity. gaussian distributions satisfy composition weak transitivity important families probability distributions. following theorem extends proposition chickering meek shows composition weak transitivity conserved hidden nodes selection bias exist. instance probability distribution results hiding nodes instantiating others dag-faithful probability distribution dag-faithful satisﬁes composition weak transitivity. important result gene expression data analysis theorem probability distribution satisfying composition weak transitivity. then satisﬁes composition weak transitivity. moreover independencies satisﬁes composition weak transitivity. proof denote three mutually disjoint subsets then thus satisﬁes composition weak transitivity properties satisﬁes them. moreover independencies then then satisﬁes composition weak transitivity properties satisﬁes them. interested queries subset them seems reasonable remove nodes take part queries interest starting analysis section denote nodes removed. theorem guarantees satisﬁes assumptions analysis section satisﬁes them. theorem above assume independencies although assumption made proposition chickering meek authors agree necessary proposition correct without assumption exist context-speciﬁc independencies violate composition weak transitivity. example follows. probability distribution represented four binary nodes structure parameters then faithful structure thus satisﬁes composition not. argue restrictive assume like theorems independencies speciﬁcally show important families probability distributions context-speciﬁc independencies exist rare. gaussian distribution independencies independencies depend variancecovariance matrix focus multinomial distributions independence denote following theorem inspired theorem meek shows probability randomly drawing probability distribution context-speciﬁc independencies zero. proof proof basically proceeds theorem meek refer reader paper details. denote three disjoint subsets constraint true false following equations must satisﬁed equation polynomial parameters corresponding term equations summation products parameters furthermore polynomial non-trivial i.e. values parameters corresponding solutions polynomial. suﬃces rename because originally denote solutions polynomial then lebesgue measure zero number linearly independent parameters corresponding consists solutions non-trivial polynomial then lebesgue measure zero ﬁnite union intersection sets lebesgue measure zero lebesgue measure zero too. consequently probability distributions context-speciﬁc independencies lebesgue measure zero because contained sol. finally since positive lebesgue measure probability distributions contextspeciﬁc independencies lebesgue measure zero includes selection bias context nodes. gaussian distribution faithful ﬁgure probability distributions exist consider selection bias then faithful ﬁgure thus dag-faithful assume want learn answer query context nodes since gaussian distribution satisﬁes assumptions theorem therefore apply method follows theorem identify result thus {i}. moreover optimal domain learn solved problem correctly without learning ﬁrst. cases learnt ﬁrst tempting identify studying structure learnt. however lead erroneous conclusions. instance best structure learnt minimal directed independence because discussed above probability distribution dag-faithful. dags ﬁgure minimal directed independence maps assume structure learnt then seems reasonable conclude because false structure learnt exist probability distributions faithful words studying structure learnt possible detect whether dealing probability distribution faithful thus safer declare relevant. know correct solution. problem occurs ﬁgure want answer query context nodes authors note algorithms geiger shachter also suﬀer drawback. hand method avoids studying probability distribution instead possibly inaccurate structure learnt reported method identify nodes relevant compute conditional probability distributions given nodes without learn ﬁrst. showed method eﬃcient consistent assumptions. argued assumptions restrictive. instance composition weak transitivity main assumptions weaker faithfulness. believe work helpful dealing high-dimensional domains. paper builds fact minimal undirected independence strictly positive probability distribution satisﬁes weak transitivity used read certain dependencies introduce sound complete graphical criterion purpose. currently studying even less restrictive assumptions paper. objective develop method whose assumptions satisﬁed important family probability distributions family conditional gaussian distributions because general family satisfy composition. example follows. random variable continuous binary. gaussian distribution gaussian distributions mean diagonal variance-covariance matrix diﬀerent oﬀ-diagonal. then note exist conditional gaussian distributions satisfy composition weak transitivity e.g. dag-faithful. also currently applying method proposed paper atherosclerosis gene expression database. believe unrealistic assume probability distribution underlying data satisﬁes strict positivity composition weak transitivity. cell functional unit organisms includes information necessary regulate function. information encoded cell divided genes coding proteins. proteins required practically functions cell. amount protein produced depends expression level coding gene which turn depends amount proteins produced genes. therefore dynamic rather accurate model cell nodes represent genes proteins edges parameters represent causal relations gene expression levels protein amounts. important dynamic gene regulate regulators even time delay. since technology measuring state protein nodes widely available data projects gene expression data analysis sample probability distribution represented dynamic bayesian network hiding protein nodes. probability distribution node hidden almost surely faithful dynamic bayesian network thus satisﬁes composition weak transitivity thus probability distribution after hiding protein nodes assumption probability distribution sampled strictly positive justiﬁed measuring state gene nodes involves series complex wet-lab computer-assisted steps introduces noise measurements thank anonymous referees comments pointers relevant literature. thank chickering meek kindly answering questions proposition chickering meek work funded swedish research council swedish foundation strategic research link¨oping university.", "year": 2012}