{"title": "Progressive Neural Architecture Search", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We propose a method for learning CNN structures that is more efficient than previous approaches: instead of using reinforcement learning (RL) or genetic algorithms (GA), we use a sequential model-based optimization (SMBO) strategy, in which we search for architectures in order of increasing complexity, while simultaneously learning a surrogate function to guide the search, similar to A* search. On the CIFAR-10 dataset, our method finds a CNN structure with the same classification accuracy (3.41% error rate) as the RL method of Zoph et al. (2017), but 2 times faster (in terms of number of models evaluated). It also outperforms the GA method of Liu et al. (2017), which finds a model with worse performance (3.63% error rate), and takes 5 times longer. Finally we show that the model we learned on CIFAR also works well at the task of ImageNet classification. In particular, we match the state-of-the-art performance of 82.9% top-1 and 96.1% top-5 accuracy.", "text": "yuille reinforcement learning genetic algorithms neural network speciﬁcation encoded string random mutations recombinations strings performed search process; string trained evaluated validation performing models generate children. reinforcement learning formulation system performs sequence actions speciﬁes structure model; model trained validation performance returned reward function used update controller. although methods able learn network structures outperform manually designed architectures require signiﬁcant computational resources. example method zoph trains evaluates neural networks across gpus days. paper describe method able learn matches previous state requiring roughly half many model samples architecture search. starting point structured search space proposed zoph search algorithm tasked searching good convolutional cell opposed full cnn. cell contains blocks block combination operator applied inputs transformed combined. cell structure stacked certain number times depending size training desired running time ﬁnal model. although structured space simpliﬁes search process signiﬁcantly number possible cell structures still exponentially large. thus many opportunities exist searching space efﬁciently. approach similar algorithm search space models simple complex pruning unpromising models models ordered number blocks contain. start considering cells block. evaluate cells observed reward train heuristic function genetic algorithms sequential model-based optimization strategy search architectures order increasing complexity simultaneously learning surrogate function guide search similar search. cifar dataset method ﬁnds structure classiﬁcation accuracy method zoph times faster also outperforms method ﬁnds model worse performance takes times longer. finally show model learned cifar also works well task imagenet classiﬁcation. particular match state-of-the-art performance top- top- accuracy. deep neural networks demonstrated excellent predictive performance many tasks. particular ever since seminal work krizhevsky image classiﬁcation dominated convolutional neural networks various architectures widely-used neural network architectures designed people process laborious requires experience expertise. hence recent interest automatically learning good neural architectures. surrogate function) based predict reward model. using learned heuristic function decide cells blocks evaluate. evaluating them update heuristic function. repeat process good cells desired number blocks. progressive approach several advantages. first simple models train faster initial results train surrogate quickly. second surrogate predict quality models slightly different ones seen third factorize search space product smaller search spaces. summary present approach structure learning roughly times efﬁcient best previous method achieving quality results. believe approach allow scale architecture search complex search spaces larger datasets. zoph propose method based called neural architecture search particular reinforce algorithm estimate parameters recurrent neural network represents policy generates sequence symbols specifying structure cnn; reward function classiﬁcation accuracy validation generated sequence. call nas-rl method. zoph extend prior work ways. first replace reinforce proximal policy optimization second carefully designed search space consisting stacked cells rather completely unstructured cnns. call method nas-rl-cell distinguish original nas-rl method. nas-rl-cell method able learn cnns outperformed almost previous methods terms accuracy speed image classiﬁcation imagenet object detection datasets. several papers learn network topologies. zhong model search space nas-rl-cell replace policy gradient qlearning. baker also q-learning withexploiting cell structure. policy gradient train actions widen existing layer deepen network adding extra layer. requires specifying initial model gradually learning transform alternative genetic algorithms search space neural networks models early work used learn structure parameters network recent methods search structures estimate parameters. negrinho gordon monte carlo tree search search space architectures. mcts searches space models shallow-to-deep node search tree uses random selection choose branch expand inefﬁcient. sequential model based optimization improves mcts learning predictive model used decide nodes expand. technique used several papers none shown kinds gains large scale image classiﬁcation problems several papers learn surrogate function predict model performance without training methods applied ﬁxed sized models would work progressive search approach. furthermore learned models come close state results. brock train network predict weights another network thereby signiﬁcantly speeding evaluation candidate model. evaluate model using predicted weights call resulting quality estimate smash score model. technique combined random search good models. however models learn substantially better ones discover. believe several reasons this. first predicting quality model much easier task predicting parameters; second searching space progressively much efﬁcient random search; third searching space cells much efﬁcient searching space full cnns. several groups performed architecture search incrementally increasing complexity model stanley miikkulainen used similar approach context evolutionary algorithms zoph used schedule increasing number layers grosse used similar approach systematically search space latent factor models speciﬁed grammar. finally cortes huang goal architecture search procedure best transferable reusable cell topology. cell parameterized consisting blocks. block mapping input tensors output tensor. specify block cell -tuple speciﬁes inputs block speciﬁes operation apply input speciﬁes combine generate feature corresponding output block denote possible inputs previous plus ﬁnal block blocks cell previous cell plus ﬁnal block previous-previous cell operator space following functions operates single tensor depthwise-separable convolution depthwise-separable convolution depthwise-separable convolution identity average pooling pooling dilated convolution followed convolution figure provides diagram network architecture employed experiments parallel zoph given convolutional cell topology repeat cell variable number times control resulting model complexity. speciﬁcally insert stride cells stacked architecture network trained imagenet since spatial scale images much larger cifar-. employ common heuristic double number ﬁlter channels whenever spatial activation reduced stride ﬁnal output network obtained applying global average pooling last feature followed softmax transformation. thus network structure fully speciﬁed deﬁning cell topology initial ﬁlter count number cell repetitions remainder section devoted discussion construction convolutional cell. quantify size search space highlight magnitude search problem. space possible structures b’th block size |bb| |ib|×|o|×|c| |ib| ﬁnal outputs previous cells possible block structures. allow cells blocks total number cell structures given note smaller search space used zoph size since learn normal reduction cells whereas learn normal cells. nevertheless still extremely large space search. construct cell need combine blocks. unfortunately size search space grows exponentially shown above even possible cell structures. many previous approaches directly work ﬁnal large search space. example nas-rl-cell uses step controller generate model speciﬁcations. yuille ﬁxed-length binary string encoding architecture deﬁned used model evolution/mutation. direct approach argue difﬁcult directly navigate exponentially large search space especially beginning knowledge makes good model. alternative propose search space progressive order simplest models ﬁrst. particular start constructing possible cell structures queue. train evaluate models queue expand adding possible block structures gives candidate cells depth since cannot afford train evaluate child networks apply learned predictor function predict score; trained based measured performance cells visited far. predictor evaluate candidate cells pick promising ones. queue repeat process cells sufﬁcient number blocks. algorithm pseudocode figure illustration. note currently greedily pick models iteration. could instead follow technique used bayesian optimization literature acquisition function expected improvement upper conﬁdence bound rank candidate models; takes account uncertainty prediction model’s performance encourages models predictor unreliable. requires predictor able compute mean posterior predictive distribution reward model given data seen figure illustration pnas search procedure maximum number blocks represents candidate cells blocks. start consider cells block train evaluate cells update predictor. iteration expand cells cells blocks predict scores pick train evaluate them update predictor. iteration expand cells subset cells blocks predict scores pick train evaluate them return winner. blue horizontal lines denote sizes |bb| number possible blocks level beam size. explained above need mechanism predict ﬁnal classiﬁcation accuracy cell given speciﬁcation; known surrogate function. least three desired properties predictor handle variable-sized models need predictor work variable-length input strings. particular able predict performance cell blocks even trained subset cells blocks. algorithm progressive neural architecture search input ﬁlters ﬁrst layer times unroll cells max. blocks cell max. models evaluate iter. trainset training valset validation begin models block pred init-predictor surrogate predictor update predictor based data pred pred.update expand predict search beam store future candidate models store predicted scores expand-by-one-block children detail block input cell speciﬁed symbols corresponding unroll steps embeddings shared across blocks. lstm whose ﬁnal hidden state followed fully connected layer sigmoid nonlinearity regress validation accuracy cifar- training images test images. images training validation set. images whitened patches cropped images upsampled random horizontal also used. accuracy predictor lstm hidden state size embedding size bias term ﬁnal fully connected layer initialized account mean observed accuracy models. embeddings uniform initializer range adam optimizer learning rate level following levels. loss train surrogate predictor. training procedure cnns follows used zoph search evaluate networks stage maximum cell depth blocks ﬁlters ﬁrst convolutional cell unroll cells times child network trained epochs using initial learning rate cosine decay ﬁnding best model increase train epochs using initial learning rate cosine decay. training also used auxiliary classiﬁer located maximum depth weighted drop path probability regularization. discuss efﬁciency algorithm. iteration cells queue; cell generates |bb+| children added list; predict scores pick train evaluate survivors. thus total number models train evaluate experiments contrast method matches top- accuracy models sampled. algorithm exhibits stochastic variability pnas variability training cifar- model. addition algorithm exhibits additional signiﬁcant stochasticity sampling outputs. sampling permits greater diversity model exploration seen greater variance dots. given amount stochasticity algorithm focus comparison mean top- mean performances models sampled. models achieves mean top- accuracy mean top- accuracy numbers roughly comparable pnas support contention pnas requires roughly fewer models achieve similar performances. additional experiments however required understand relative efﬁciency algorithm larger search spaces. note figure corresponds training model scratch epochs cifar. could easily methods baker apply early stopping unpromising models. would reduce amount time takes evaluate dots. could also consider methods select fewer models iteration pnas. however leave extensions future work. best model pnasnet- achieves error rate nasnet-a model zoph however need evaluate models total model fewer models models evaluated nas-rl-cell method. note however search space zoph vastly larger search space work given results previous section indicating efﬁciency gains infer majority speed reduced search space. future plan pnas full space measure relative speed also ﬁnal model quality. figure comparing relative efﬁciency pnas algorithms. validation accuracy cifar- validation ﬁrst models visited pnas ﬁrst models visited nas-rl bottom zoomed section graph. pnas results come clusters reﬂecting stages algorithm. models pnas achieves top- accuracy mean top- accuracy mean top- accuracy models nasrl achieves top- accuracy mean top- accuracy mean top- accuracy note mean accuracies smooth stochastic ﬂuctuations architecture search algorithms. figure compare pnas method nas-rlcell method. methods search space namely size accuracy pnas models increases increases expected. search models method reaches peak accuracy mean top- accuracy mean top- accuracy note degree stochastable results cifar- recent structure learning methods; methods bottom block paper. algo type search algorithm used error top- misclassiﬁcation rate best model. params corresponding number parameters. models number models trained evaluated order best model. error rates form average validation performance random trials ﬁtting evaluating model standard deviation. uses evolutionary algorithms applied similar space cell structures. best model accurate best uses signiﬁcantly fewer parameters importantly also much efﬁcient achieve best result using model evaluations whereas need perform model evaluations more. also evaluated performance best smaller models discover search process denote pnasnet-{ train using hyperparameters pnasnet- evaluate test performance model. repeat process times using different random seeds initialization parameters. figure shows resulting performance pnasnet-{ models. test error rate decreases progress eventually achieves similar performance nasnet-a. show best learned cell structures figure figure architectures pnasnet nasnet similarities differences. pnasnets nasnets wide rather deep connections useful. notable difference pnasnet- maximum block depth whereas nasnet-a’s normal cell entirely ﬂat. figure boxplot test performance cifar- best model blocks found pnas. train test model times epochs; spread quality randomness parameter initialization optimization process etc. horizontal line performance best model found nas-rl-cell method also cell blocks. predictive accuracy surrogate function improves collect training data mean predictive error drops note since child networks trained epochs cifar- validation accuracy across models non-negligible variance making noisy training signal. train evaluate experiments rmsprop optimizer label smoothing value auxiliary classiﬁer located maximum depth weighted weight decay dropout ﬁnal softmax layer. mobile setting distributed synchronous workers gpu. worker batch size initial learning rate decayed every large setting epochs rate workers. worker batch size initial learning rate decayed every epochs rate training drop path probability results mobile setting summarized table pnasnet- achieves slightly better performance nasnet-a methods signiﬁcantly surpass previous state-of-the-art includes manually designed mobilenet shufﬂenet table shows large setting pnasnet- achieves comparable performance previous state-ofcandidate model times reduce variance.) furthermore noisy samples train nevertheless predictor seems provide useful steering signal search process. measure effectiveness model search process move compute three metrics highest validation accuracy mean validation accuracy number models validation accuracy among child networks. compare random baseline randomly selects models entire space level. figure regardless metric pnas algorithm shows signiﬁcant gains random baseline. particular pnas models much higher performance step search terms mean performance. addition fraction high performing models signiﬁcantly greater; useful want ensemble good figure measuring effectiveness pruning process. plot three metrics highest accuracy mean accuracy number models accuracy within models level pnas algorithm. rand means randomly selecting models main contribution work show accelerate search good structures using progressive search space increasingly complex graphs combined learned prediction function efﬁciently identify promising models explore. resulting architectures achieve level performance previous work fraction computational cost. future hope improve efﬁciency search scale technique applied complex graph structures used perform object detection. thank quoc inspiration discussion support; george dahl many fruitful discussions; gabriel bender vijay vasudevan development much critical infrastructure larger google brain team support discussions. ﬁrst author thanks lingxi support. cortes corinna gonzalvo xavier kuznetsov vitaly mohri mehryar yang scott. adanet adaptive structural learning artiﬁcial neural networks. icml howard andrew menglong chen kalenichenko dmitry wang weijun weyand tobias andreetto marco adam hartwig. mobilenets efﬁcient convolutional neural networks mobile vision applications. corr abs/. hutter frank hoos holger leyton-brown kevin. sequential model-based optimization general algorithm conﬁguration. intl. conf. learning intelligent optimization lecture notes computer science tsung-yi maire michael belongie serge hays james perona pietro ramanan deva doll´ar piotr zitnick lawrence. microsoft coco common objects context. eccv hanxiao simonyan karen vinyals oriol fernando chrisantha kavukcuoglu koray. hierarchical representations efﬁcient architecture search. corr abs/. mendoza hector klein aaron feurer matthias springenberg jost tobias hutter frank. towards automatically-tuned neural networks. icml workshop automl december miikkulainen risto liang jason meyerson elliot rawal aditya fink francon olivier raju bala shahrzad hormoz navruzyan arshak duffy nigel hodjat babak. evolving deep neural networks. corr abs/. szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. going deeper convolutions. cvpr", "year": 2017}