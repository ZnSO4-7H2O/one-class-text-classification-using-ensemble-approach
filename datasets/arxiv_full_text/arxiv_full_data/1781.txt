{"title": "Ask the GRU: Multi-Task Learning for Deep Text Recommendations", "tag": ["stat.ML", "cs.CL", "cs.LG", "I.2.7; I.2.6"], "abstract": "In a variety of application domains the content to be recommended to users is associated with text. This includes research papers, movies with associated plot summaries, news articles, blog posts, etc. Recommendation approaches based on latent factor models can be extended naturally to leverage text by employing an explicit mapping from text to factors. This enables recommendations for new, unseen content, and may generalize better, since the factors for all items are produced by a compactly-parametrized model. Previous work has used topic models or averages of word embeddings for this mapping. In this paper we present a method leveraging deep recurrent neural networks to encode the text sequence into a latent vector, specifically gated recurrent units (GRUs) trained end-to-end on the collaborative filtering task. For the task of scientific paper recommendation, this yields models with significantly higher accuracy. In cold-start scenarios, we beat the previous state-of-the-art, all of which ignore word order. Performance is further improved by multi-task learning, where the text encoder network is trained for a combination of content recommendation and item metadata prediction. This regularizes the collaborative filtering model, ameliorating the problem of sparsity of the observed rating matrix.", "text": "based similarity attributes ignoring data users. methods make recommendations items limited performance since cannot employ similarity user preferences hybrid recommendation systems seek best worlds leveraging item content user-item ratings hybrid recommendation methods consume item text recommendation often ignore word order either bags-of-words features linear model deﬁne unsupervised learning objective text topic model methods unable fully leverage text content limited bag-of-words suﬃcient statistics furthermore unsupervised learning unlikely focus aspects text relevant content recommendation. paper present method leveraging recurrent neural networks represent text items collaborative ﬁltering. recent years rnns provided substantial performance gains variety natural language processing applications language modeling machine translation rnns number noteworthy characteristics sensitive word order require hand-engineered features easy leverage large unlabeled datasets pretraining parameters unsupervised language modeling objectives computation parallelized applies naturally cold-start scenario feature extractor whenever text associated items. extreme data sparsity content recommendation datasets regularization also important consideration. particularly important deep models rnns since high-capacity models prone overﬁtting. existing hybrid methods used unsupervised learning objectives text content regularize parameters recommendation model however since consume text directly input prediction approach. instead provide regularization performing multi-task learning combining collaborative ﬁltering simple side task predicting item meta-data genres item tags. here network producing vector representations items directly text content shared prediction recommendation tasks. allows make predictions cold-start conditions providing regularization recommendation model. variety application domains content recommended users associated text. includes research papers movies associated plot summaries news articles blog posts etc. recommendation approaches based latent factor models extended naturally leverage text employing explicit mapping text factors. enables recommendations unseen content generalize better since factors items produced compactly-parametrized model. previous work used topic models averages word embeddings mapping. paper present method leveraging deep recurrent neural networks encode text sequence latent vector speciﬁcally gated recurrent units trained end-to-end collaborative ﬁltering task. task scientiﬁc paper recommendation yields models signiﬁcantly higher accuracy. cold-start scenarios beat previous state-of-the-art ignore word order. performance improved multi-task learning text encoder network trained combination content recommendation item metadata prediction. regularizes collaborative ﬁltering model ameliorating problem sparsity observed rating matrix. text recommendation important problem potential drive signiﬁcant proﬁts e-businesses through increased user engagement. examples text recommendations include recommending blogs social media posts news articles movies products research papers methods recommending text items broadly classiﬁed collaborative ﬁltering content-based hybrid methods. collaborative ﬁltering methods user-item rating matrix construct user item proﬁles past ratings. classical examples include matrix factorization methods completely ignore text information rely solely rating matrix. methods suﬀer cold-start problem rank unseen unrated items ubiquitous domains. content-based methods hand item text attributes make recommendations assigned gaussian priors leads regularization. implicit feedback recommendation systems ranking-based loss instead methods propose trained diﬀerentiable cost function. weighted squared loss experiments consistent baselines many applications factorization unusable since suﬀers cold-start problem unseen items recommended users associated embedding. lead increased interest hybrid methods leverage additional information item content make coldstart recommendations. cases also face cold-start problem users. though consider case techniques paper extended naturally accommodate whenever text content associated users. consider existing hybrid methods item metadata take form. cases linear function manually extracted item features. example agarwal chen gantner incorporate side information linear regression based formulation metadata like category user’s location etc. rendle proposed general framework incorporating higher order interactions among features factor model. refer references therein recent review hybrid methods. experiments compare collaborative topic regression state-of-the-art technique simultaneously factorizes item-word count matrix user-item rating matrix learning low-dimensional representations items able provide recommendations unseen items. typical datasets highly sparse thus important leverage available training signals many applications useful perform multi-task learning combines auxiliary tasks shared feature representation items used tasks. collective matrix factorization jointly factorizes multiple observation matrices shared entities relational learning. seek predict side information associated users. finally mcauley leskovec used topic models almahairi used language models review text. many applications text items associated tags including research papers keywords news articles user editor provided labels social media posts hashtags movies genres etc. used features however considerable drawbacks approach. first tags often assigned users lead cold-start problem since items annotation. moreover tags noisy especially user-assigned general available datasets items associated text abstracts rnn-based models yield relative-improvement recall cold-start recommendation collaborative topic regression approach wang blei word-embedding based model model giving competitive performance warm-start recommendation. also note simple linear model represents documents using average word embeddings trained completely supervised fashion obtains competitive results ctr. finally multi-task learning improves performance models signiﬁcantly including baselines. paper focuses task recommending items associated text content. j-th text item sequence word tokens token words vocabulary. additionally text items associated multiple tags setting observe whether person viewed liked item observe explicit ratings. user liked item otherwise. denote user-item matrix likes denote items liked user gradient descent. implicit feedback unobserved rating might indicate either user like item user never seen item. cases common weighted regularized squared loss annotation unreliable incomplete input features encouraging items’ representations predictive tags yield useful regularization problem. besides providing regularization multitask learning approach especially useful cold-start scenarios since tags used train time hence need available test time. section employ approach. neural networks received limited attention recommendation systems community. used restricted boltzmann machines component models tackle netﬂix challenge. recently proposed denoising auto-encoder based models collaborative ﬁltering trained denoise corrupted versions entire sparse vectors user-item likes item-user likes however models unable handle cold-start problem. wang addresses incorporating bag-of-words autoencoder model within bayesian framework. elkahky proposed neural networks manually extracted user item feature representations content based multidomain recommendation. dziugaite proposed neural network learn similarity function between user item latent factors. oord wang wang developed music recommender systems features extracted music audio using convolutional neural networks deep belief networks. however methods process user-item rating matrix isolation content information thus unable exploit direct interaction item content ratings weston proposed based model predict hashtags social media posts found learned representations also useful document recommendation. recently mcauley used image-features separately trained improve product recommendation tackle cold-start. almahairi used neural network based language models review text regularize latent factors product recommendation opposed using topic models mcauley leskovec found based language models perform poorly regularizers word embedding models mikolov perform better. section presents neural network-based encoders explicitly mapping item’s text content vector latent factors. allows perform cold-start prediction items. addition since vector representations items tied together shared parametric model able generalize better limited data. corresponds exactly linear model bag-of-words representation document. however using representation useful word embeddings pre-trained unsupervised manner large corpus note similar embedding-based model used weston hastag prediction. bag-of-words models limited capacity cannot distinguish sentences similar unigram statistics completely diﬀerent meanings example consider research paper abstracts this paper deep learning this paper deep learning. unigram statistics would interest diﬀerent sets users. powerful model exploit additional information inherent word order would expected recognize thus perform better recommendation. work reads text word time produces single vector representation. rnns provide impressive compression salient properties text. example accurate translation english sentence performed conditioning single vector encoding extracted item representation combined user embedding predicted rating useritem pair. model trained recommendation completely supervised manner using diﬀerentiable cost function note diﬀerence approach existing approaches item content apart sensitivity word order deﬁne unsupervised objective extracting text representation. however model beneﬁt unsupervised data pre-training word embeddings pre-training parameters using language models figure proposed architecture text item recommendation. rectangular boxes represent embeddings. layers used ﬁrst layer bi-directional rnn. output hidden units second layer pooled produce text encoding combined item-speciﬁc embedding produce ﬁnal representation users tags also represented embeddings combined item representation predection recommendation. items. therefore employ multi-task learning approach section tags associated papers considered summary topics items thus forcing encoder predictive tags provide useful inductive bias. consider example figure observing paper even though term present text force network attention sequence words order explain tags. worth noting diﬀerences approach almahairi language modeling text unsupervised multi-task objective item latent factors shared parameters. almahairi found increased ﬂexibility oﬀered makes strong regularizer leading worse performance simpler bag-of-words models. contrast trained fully supervised forces item representations discriminative recommendation prediction. furthermore using text input tion diﬃcult prohibiting learning long-term dependencies. several modiﬁcations proposed remedy problem popular long short-term memory units recent gated recurrent units grus simpler lstm fewer parameters give competitive performance lstms hidden vector output step input rkh×kh rkh× parameters dimension input word embeddings number hidden units rnn. denotes element-wise product. intuitively acts ‘forget’ gate decides parts previous hidden state consider ignore current step computes candidate state current time step using parts previous hidden state dictated acts output gate decides parts previous memory change candidate memory forget update operations diﬀerentiable allow learning backpropagation. ﬁnal architecture shown figure consists stacked layers hidden units. bi-directional ﬁrst layer feed concatenation forward backward hidden states input second layer. output hidden states second layer pooled obtain item content representation experiments mean pooling performs best. models ﬁnal state take much longer optimize. following ﬁnal item representation obtained combining representation item-speciﬁc embedding describe multi-task learning setup. maximum likelihood incomplete data algorithm broadly applicable algorithm computing maximum likelihood estimates incomplete data presented various levels generality theory showing monotone behaviour figure saliency word abstract paper size color words indicate leverage ﬁnal rating. model learns chunks word phrases important maximum likelihood iteratively reweighted least squares ignores punctutations stop words. citeulike. citeulike online platform allows registered users create personal libraries saving papers interest them. datasets consist papers users’ libraries user provided tags papers title abstract papers. similar wang blei remove users less ratings removed tags occur less articles. citeulike-a consists users papers tags total user-item likes. citeulike-t consists users papers tags total user-item likes. note citeulike-t much sparse citeulike-a warm-start case in-matrix prediction every test item least like training data. user -fold split papers like history. papers less likes always kept training data since cannot evaluated properly. learning predict ratings across active test items user ﬁlter items training ranked list. cold-start task predicting user interest paper existing likes based text content paper. papers split folds. again papers less likes always kept training set. fold remove likes papers fold forming test-set keep folds training-set. models training items fold form predictive per-user ranking items test set. plicit feedback often measured recall. precision reasonable since zero ratings mean user either like article know thus recallm average per-user metric compare proposed methods models item content using topic modeling. approach forth cannot perform tag-prediction thus fair comparison modify prediction. viewed probabilistic version collective matrix factorization deriving alternating least squares inference algorithm along line possible sigmoid loss. thus formulate prediction using weighted squared loss instead. learning model straightforward extension rather performing alternating updates blocks parameters rotate among three. call ctr-mtl. word embeddingbased model order-insensitive document encoder embed rnn-based model gru. corresponding models trained multi-task learning embed-mtl gru-mtl. follow wang blei setting hyperparameters. latent factor dimension regularization parameters cost weights parameters gave good results ctr-mtl. ctr-mtl trained using algorithm updates latent factors using alternating least squares full data sensitive good pre-processing text common topic modeling provided pre-processed text cant gain demonstrates model much better representing content items. improvements higher citulike-t dataset much sparse models utilize content appropriately give better recommendations. embed models perform competitively other. next observe multi-task learning uniformly improves performance models. model’s recall improves citulike-a citeuliket. leads overall improvement citeulike-a citeulike-t best baselines. comparatively improvement smaller. expected since bayesian topic model provides strong regularization model parameters. contrary this embed models also beneﬁts warm-start collaborative ﬁltering methods based matrix factorization often perform well hybrid methods warm-start scenario ﬂexibility item-speciﬁc embeddings consider models trained without mtl. model performs better either embed model relative improvement citeulike-a citeulike-t best models. multi-task learning improves performance models. improvements particularly signiﬁcant ctr-mtl since tags associated test items observed training provide strong inductive bias leading improved performance. interestingly gru-mtl model performs slightly better ctr-mtl model dataset slightly worse other. ﬁrst third plots figure demonstrate gru-mtl performs slightly better ctr-mtl smaller i.e. relevant articles ranked toward top. quantify this evaluate average reciprocal hit-rank given list ranked articles user denote ranks articles user actually liked. detests whether ranked articles correct. gru-mtl gives ctr-mtl gives conﬁrms list gru-mtl contains relevant recommendations. prediction although focus models recommendation evaluate performance multi-task models prediction. recall evaluate cold-start scenario tags present test article. embed models used word embeddings dimension order consistent ctr. models ﬁrst layer hidden state dimension second layer hidden state dimension pre-trained word embeddings using cbow corpus abstracts dropout used every layer network. probabilities dropping dimension embedding layer output ﬁrst layer output second layer respectively. also regularize user embeddings weight mild preprocessing text. replace numbers <num> token words total frequency less <unk>. note don’t remove stop words frequent words. leaves vocabulary words citeulike-a words citeulike-t. weight cost function diﬀerently positive negative samples. since total number negative examples much larger positive examples user stochastically sampling negative positive example implicitly down-weights negatives. used mini-batch size users used adam optimization. models maximum mini-batch updates early-stopping based recall validation training examples. table summarizes recall models citeulike datasets warm-start cold-start. figure shows variation recallm diﬀerent values multi-task learning models. cold-start recall cold-start recommendation item-speciﬁc embeddings identically equal zero thus items’ representations depend solely text content. ﬁrst note performance models without multi-task learning. model better best score either model embed model citeulike-a citeulike-t. signiﬁembed models perform similarly. ctr-mtl signiﬁcantly worse could squared loss training hyperparameters selected recommendation performance prediction. employ simple easy-to-implement tool analyzing predictions based denil produce heatmap every input word associated leverage output prediction. suppose recommended item user words suppose large. sequence word embeddings item since encoded neural network dˆrij dejt produce heatmap’s value word convert dˆrij dejt scalar. possible backpropagation dˆrij dxjt well-deﬁned since discrete index. instead compute dˆrij employ deep recurrent neural networks provide vector representations text content associated items collaborative ﬁltering. generic text-to-vector mapping useful trained directly gradient descent provides opportunities perform multi-task learning. scientiﬁc paper recommendation multi-task learning provide complementary performance improvements. encourage technique variety application domains. future work would like apply deep architectures users’ data explore additional objectives multi-task learning employ multiple modalities inputs movies’ images text descriptions. work supported part center intelligent information retrieval part allen institute artiﬁcial intelligence part grant part national science foundation grant number dmr- part darpa under agreement number fa---. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright notation thereon. opinions ﬁndings conclusions recommendations expressed material authors necessarily reﬂect sponsor.", "year": 2016}