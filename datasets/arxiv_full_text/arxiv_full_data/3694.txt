{"title": "Reinforcement Learning of POMDPs using Spectral Methods", "tag": ["cs.AI", "cs.LG", "cs.NA", "math.OC", "stat.ML"], "abstract": "We propose a new reinforcement learning algorithm for partially observable Markov decision processes (POMDP) based on spectral decomposition methods. While spectral methods have been previously employed for consistent learning of (passive) latent variable models such as hidden Markov models, POMDPs are more challenging since the learner interacts with the environment and possibly changes the future observations in the process. We devise a learning algorithm running through episodes, in each episode we employ spectral techniques to learn the POMDP parameters from a trajectory generated by a fixed policy. At the end of the episode, an optimization oracle returns the optimal memoryless planning policy which maximizes the expected reward based on the estimated POMDP model. We prove an order-optimal regret bound with respect to the optimal memoryless policy and efficient scaling with respect to the dimensionality of observation and action spaces.", "text": "propose reinforcement learning algorithm partially observable markov decision processes based spectral decomposition methods. spectral methods previously employed consistent learning latent variable models hidden markov models pomdps challenging since learner interacts environment possibly changes future observations process. devise learning algorithm running episodes episode employ spectral techniques learn pomdp parameters trajectory generated ﬁxed policy. episode optimization oracle returns optimal memoryless planning policy maximizes expected reward based estimated pomdp model. prove order-optimal regret bound respect optimal memoryless policy efﬁcient scaling respect dimensionality observation action spaces. keywords spectral methods method moments partially observable markov decision process latent variable model upper conﬁdence reinforcement learning. reinforcement learning effective approach solve problem sequential decision– making uncertainty. agents learn maximize long-term reward using experience obtained direct interaction stochastic environment since environment initially unknown agent balance between exploring environment estimate structure exploiting estimates compute policy maximizes long-term reward. result designing algorithm requires three different elements estimator environment’s structure planning algorithm compute optimal policy estimated environment strategy make trade exploration exploitation minimize regret i.e. difference performance exact optimal policy rewards accumulated agent time. azizzadenesheli supported part career award ccf- award n--- lazaric supported part grant cper nord-pas calais/feder data advanced data science technologies cristal french national research agency project extra-learn n.anr--ce--. literature assumes environment modeled markov decision process markovian state evolution fully observed. number exploration– exploitation strategies shown strong performance guarantees mdps either terms regret sample complexity however assumption full observability state evolution often violated practice agent noisy observations true state environment case appropriate partially-observable pomdp model. many challenges arise designing algorithms pomdps. unlike mdps estimation problem involves identifying parameters latent variable model agent directly observes state transitions estimation generative model straightforward empirical estimators. hand pomdp transition reward models must inferred noisy observations markovian state evolution hidden. planning problem i.e. computing optimal policy pomdp known parameters pspace-complete requires solving augmented built continuous belief space finally integrating estimation planning exploration–exploitation strategy guarantees non-trivial no-regret strategies currently known propose algorithm main contributions paper follows pomdps incorporates spectral parameter estimation within exploration-exploitation framework analyze regret bounds assuming access optimization oracle provides best memoryless planning policy learning episode prove order optimal regret efﬁcient scaling dimensions thereby providing ﬁrst guaranteed algorithm wide class pomdps. estimation pomdp carried spectral methods involve decomposition certain moment tensors computed data. learning algorithm interleaved optimization planning policy using exploration–exploitation strategy inspired ucrl method mdps resulting algorithm called sm-ucrl runs episodes variable length agent follows ﬁxed policy enough data collected updates current policy according estimates pomdp parameters accuracy. throughout paper focus estimation exploration–exploitation aspects algorithm assume access planning oracle class memoryless policies assumption common many works bandit literature linear bandit chen combinatorial bandit) focus exploration– exploitation strategy rather optimization problem. i.e. largest mean passage time state-action pairs pomdp using memoryless policy mapping observations actions. sm-ucrl steps using conﬁdence intervals thm. suitable assumptions pomdp space policies number samples another interesting aspect diameter pomdp natural extension case. dmdp measures mean passage time using state–based policies pomdps policies cannot deﬁned states rather observations naturally translates deﬁnition diameter details problemdependent terms bound discussed sect. derived regret bound respect best memoryless policy given pomdp. indeed general pomdp optimal policy need memoryless. however ﬁnding optimal policy uncomputable inﬁnite horizon regret minimization instead memoryless policies shown good performance practice moreover class so-called contextual special class pomdps optimal policy also memoryless analysis learning algorithm. learning results thm. based spectral tensor decomposition methods previously used consistent estimation wide class lvms contrast traditional learning methods expectation-maximization consistency guarantees converge local optimum arbitrarily bad. spectral methods previously employed sequence modeling hmms representing multiview model application pomdps trivial. fact unlike consecutive observations pomdp longer conditionally independent conditioned hidden state middle view. decision depends observations themselves. limiting memoryless policies control range dependence conditioning actions show obtain conditionally independent views. result starting samples collected along trajectory generated ﬁxed policy construct multi-view model tensor decomposition method action separately estimate parameters pomdp deﬁne conﬁdence intervals. proof follows similar steps previous works spectral methods extend concentration inequalities dependent random variables matrix valued functions combining results kontorovich matrix azuma’s inequality tropp allows remove usual assumption samples generated stationary distribution current policy. particularly important case since policy changes episode avoid discarding initial samples waiting corresponding markov chain converged condition pomdp observations states follows standard non-degeneracy conditions apply spectral method. corresponds considering pomdps underlying deﬁned number states produce large number noisy observations. common applications spoken-dialogue systems medical applications also show assumption relaxed result applied wider family pomdps. analysis exploration–exploitation strategy. sm-ucrl applies popular optimism-inface-of-uncertainty principle conﬁdence intervals estimated pomdp compute optimal policy optimistic pomdp admissible set. optimistic choice provides smooth combination exploration encouraged conﬁdence intervals exploitation estimates pomdp parameters. algorithmic integration rather simple analysis trivial. spectral method cannot samples generated different policies length episode carefully tuned guarantee estimators improve episode. furthermore analysis requires redeﬁning notion diameter pomdp. addition carefully bound various perturbation terms order obtain efﬁcient scaling terms dimensionality factors. principle successfully used wide number exploration–exploitation problems ranging multi-armed bandit linear contextual bandit linear quadratic control reinforcement learning also purely exploratory methods random sampling randomly chooses actions independent observations. sm-ucrl converges much faster better solution. solutions relying assumption directly work dimensional observation space perform poorly. fact even worse random sampling policy baseline. contrast method aims lower dimensional latent space derive policy allows ucrl much better memoryless policy vanishing regret. worth noting that general slight changes learning come algorithms learn different pomdp models with slightly upper conﬁdence bounds. moreover applying memoryless policy collecting sufﬁcient number samples model parameters learned well planing belief space memory dependent policy therefore improve performance even further. last decades widely studied different setting. even large state space classical approaches scalable kocsis szepesv´ari introduces monte-carlo planning tree viable approaches nearoptimal policy. addition special class mdps markov jump afﬁne model action space continuous proposes order optimal learning policy. mdps widely studied design effective exploration–exploration strategies pomdps still relatively unexplored. ross poupart vlassis propose integrate problem estimating belief state model-based bayesian approach distribution possible mdps updated time. proposed algorithms bayesian inference done accurately step pomdp sampled posterior corresponding optimal policy executed. resulting methods implicitly balance exploration exploitation theoretical guarantee provided regret algorithmic complexity requires introduction approximation schemes inference planning steps. alternative model-based approaches adapt model-free algorithms q-learning case pomdps. perkins proposes monte-carlo approach action-value estimation shows convergence locally optimal memoryless policies. algorithm advantage computationally efﬁcient local optimal policies arbitrarily suboptimal thus suffer linear regret. alternative approach solve pomdps policy search methods avoid estimating value functions directly optimize performance searching given policy space usually contains memoryless policies beside practical success ofﬂine problems policy search successfully integrated efﬁcient exploration–exploitation techniques shown achieve small regret nonetheless performance methods severely constrained choice policy space contain policies good performance. another approach solve pomdps proposed work agent randomly chooses actions independent observations rewards. agent executes random policy collects sufﬁcient number samples estimates model parameters given collected information. authors propose probably approximately correct framework pomdp setting shows polynomial sample complexity learning model parameters. learning phase deﬁnes induced hidden markov model applies random policy capture different aspects model planing phase given estimated model parameters compute optimum policy far. words proposed algorithm explores environment sufﬁciently enough exploits exploration come optimal policy given estimated model. contrast method considers pomdps episodic learning framework. matrix decomposition methods previously used general setting predictive state representation reconstruct structure dynamical system. despite generality psrs proposed model relies strong assumptions dynamics system theoretical guarantee performance. gheshlaghi azar used spectral tensor decomposition methods multi-armed bandit framework identify hidden generative model sequence bandit problems showed drastically reduce regret. recently introduced compressed method reduce computation cost exploiting advantages dimensionality reduction incremental matrix decomposition compressed sensing. work take ideas considering powerful tensor decomposition techniques. krishnamurthy recently analyzed problem learning contextual-mdps proved sample complexity bounds polynomial capacity policy space number states horizon. objective minimize regret ﬁnite horizon instead consider inﬁnite horizon problem. open question analyze modify spectral ucrl algorithm ﬁnite horizon problem. stated earlier contextual mdps special class pomdps memoryless policies optimal. assume samples drawn contextual handle much general class pomdps minimize regret respect best memoryless policy given pomdp. finally related problem considered ortner series possible representations based observation histories available agent actually markov. ucrl-like strategy adopted shown achieve near-optimal regret. paper focus learning problem consider access optimization oracle compute optimal memoryless policy. problem planning general pomdps intractable uncomputable inﬁnite horizon many exact approximate heuristic methods proposed compute optimal policy recent survey). alternative approach consider memoryless policies directly observations actions deterministic policies perform poorly stochastic memoryless policies shown near-optimal many domains even optimal speciﬁc case contextual mdps although computing optimal stochastic memoryless policy still np-hard several model-based model-free methods shown converge nearly-optimal policies polynomial complexity conditions pomdp work employ memoryless policies prove regret bounds reinforcement learning pomdps. works suggest focusing memoryless policies restrictive limitation practice. paper organized follows. sect. introduces notation technical assumptions concerning pomdp space memoryless policies consider. sect. introduces spectral method estimation pomdp parameters together thm. sect. outline sm-ucrl integrate spectral method exploration–exploitation strategy prove regret bound thm. sect. draws conclusions discuss possible directions future investigation. proofs reported appendix together preliminary empirical results showing effectiveness proposed method. pomdp tuple ﬁnite state space cardinality ﬁnite action space cardinality ﬁnite observation space cardinality ﬁnite reward space cardinality largest reward rmax. notation convenience vector notation elements indicator vectors entries equal except position corresponding speciﬁc element index states actions rewards observations. finally denotes transition density probability transition given state-action pair reward density probability receiving reward corresponding value indicator vector given state-action pair observation density probability receiving observation corresponding indicator vector given state whenever convenient tensor forms density functions focus stochastic memoryless policies observations actions policy denote density function. denote stochastic memoryless policies non-zero probability explore actions stochastic memoryless policy average reward. throughout paper assume access optimization oracle returning optimal policy pomdp need following assumptions pomdp characterize markov chains generated policies ergodic markov chain stationary distribution distribution states reached policy steps starting initial state inverse mixing time ρmixπ chain deﬁned assumption guarantees distribution state result linear combination distributions states. show later sufﬁcient condition recover since makes states distinguishable observations also implies notice pomdps often used opposite scenario applications robotics imprecise sensors prevents distinguishing different states. hand many domains number observations much larger states deﬁne dynamics system. typical example case spoken dialogue systems observations much larger state conversation similar scenario found medical applications state patient produce huge body different observations. problems crucial able reconstruct underlying small state space actual dynamics system observations. similar previous assumption means action distribution cannot obtained linear combination distributions states sufﬁcient condition able recover transition tensor. asm. strictly related assumptions introduced anandkumar tensor methods hmms. sect. discuss partially relaxed. section introduce novel spectral method estimate pomdp parameters stochastic policy used generate trajectory steps. need following assumption that together asm. guarantees states actions constantly visited. similar case hmms element apply spectral methods construct multi-view model hidden states. despite similarity spectral method developed anandkumar cannot directly employed here. fact hmms state transition observations depend current state. hand pomdps probability transition state depends also action since action chosen according memoryless policy based current observation creates indirect dependency observation makes model intricate. multi-view model estimate pomdp parameters action separately. step construct three views contain observable elements. seen fig. three views provide information hidden state careful analysis graph dependencies shows conditionally views independent. instance consider yt+. random variables clearly dependent since inﬂuences action triggers transition emits observation yt+. nonetheless sufﬁcient condition action break dependency make independent. similar arguments hold elements views used recover latent variable formally encode triple vector whenever suitable mapping index indices action observation reward. similarly proceed associated action deﬁned following denote notice asm. asm. imply view matrices full column rank. result construct multi-view model relates spectral decomposition second third moments views columns third view matrix. thus sufﬁcient invert equations obtain columns ﬁrst second view matrices. process could done order e.g. could ﬁrst estimate second view applying suitable symmetrization step recovering ﬁrst third views reversing similar equations hand cannot repeat symmetrization step multiple times estimate views independently fact estimates returned spectral method consistent suitable permutation indexes states. pose problem computing single view estimated views independently permutation different thus making non-consistent impossible recovering pomdp parameters. hand estimating ﬁrst view recovering others inverting guarantees consistency labeling hidden states. denote observation model recovered view related action exact case identical moving empirical version leads different estimates action view used compute among them select estimate better accuracy. empirical estimates pomdp parameters. practice available need estimated samples. given trajectory steps obtained executing policy steps action played collect triples construct corresponding views symmetrize views using empirical estimates covariance matrices build empirical version eqs. using samples thus obtaining process described lemma spectral methods indeed recover factor matrices permutation hidden states. case since separately carry spectral decompositions different actions recover permuted factor matrices. since observation matrix common actions align decompositions. let’s deﬁne actually minimum separability level matrix estimation error columns matrix less come permutation issue matching columns matrices. condition reﬂected condition number samples action larger number. showing consistency spectral method actions repeatedly tried time estimates converge true parameters pomdp. contrast em-based methods typically stuck local maxima return biased estimators thus preventing deriving conﬁdence intervals. bounds eqs. depend bound worse bounds eqs. factor seems unavoidable since columns estimating requires addition come upper bound complicated derivation procedure complicated compared adds term matrices smallest non-zero singular values inﬂuence accuracy inversion construction modiﬁed views computation second view third using similarly presence σmin justiﬁed pseudo-inversion used recover transition tensor finally dependency smallest singular values tensor decomposition method speciﬁc feature bounds depend state number times explored. indeed inverse dependency condition implies state poorly visited empirical estimate state negatively affected. striking contrast fully observable case accuracy estimating e.g. reward model state action simply depends number times state-action pair explored even states never explored all. difference intrinsic partial observable nature pomdp reconstruct information states indirect observations. result order accurate estimates pomdp structure need rely policy ergodicity corresponding markov chain guarantee whole state space covered. asm. markov chain ergodic since assumption made fact samples generated sampled stationary distribution condition depends fast chain converge characterized parameters policy deterministic actions would explored thus leading inaccurate estimations inverse dependency πmin accounts amount exploration assigned every actions determines accuracy estimates. furthermore notice also singular values depend distribution views turn partially determined policy notice ﬁrst terms basically bounds spectral methods applied dependency πmin speciﬁc pomdp case. hand analysis hmms usually dependency parameters samples assumed drawn stationary distribution chain. removing assumption required developing novel results tensor decomposition process using extensions matrix concentration inequalities case markov chain overall analysis reported app. worth note that kontorovich without stationary assumption proposes method learn transition matrix model given factor matrix provides theoretical bound estimation errors. interesting aspect estimation process illustrated previous section applied samples collected using policy result integrated exploration-exploitation strategy policy changes time attempt minimizing regret. algorithm. sm-ucrl algorithm illustrated alg. result integration spectral method structure similar ucrl designed optimize exploration-exploitation trade-off. learning process split episodes increasing length. beginning episode computed using spectral method alg. unlike ucrl sm-ucrl cannot samples past episodes. fact distribution views depends policy used generate samples. result whenever policy changes spectral method re-run using samples collected speciﬁc policy. nonetheless exploit fact spectral method applied action separately. sm-ucrl episode action samples coming past episode returned largest number samples action. number samples obtained episode action denote maxk′<k largest number samples available past episodes action separately feed spectral method compute estimated pomdpcm beginning episode given estimated pomdp result thm. construct admissible pomdps ayreft efrefoi whose transition reward observation models belong conﬁdence intervals −efok state computation optimal policy optimistic model trivial. nonetheless ﬁrst notice given horizon policy needs recomputed times furthermore optimization oracle given pomdp available sufﬁcient randomly sample multiple pomdps corresponding approximately optimistic procedure bounded performing purely explorative policy sm-ucrl still exploits current estimates construct admissible pomdps selects policy maximizes performance regret analysis. study regret sm-ucrl w.r.t. best policy general optimal πmin usually small value oftentimes optimal memoryless policy stochastic actually contained given horizon steps regret deﬁned random reward obtained time according reward model states traversed policies performed episodes actual pomdp. restate similar case complexity learning pomdp partially determined diameter deﬁned theorem consider pomdp states actions observations rewards characterized diameter observation matrix smallest non-zero singular value consider policy space worst smallest non-zero value worst smallest probability reach state ωmin. sm-ucrl steps conﬁdence intervals thm. used δ′/n bounds. means despite complexity pomdps sm-ucrl dependency number steps mdps vanishing per-step regret. furthermore dependency known minimax optimal. diameter general larger counterpart dmdp since takes account fact memoryless policy work observations cannot efﬁcient state-based policy moving state another. although lower bound available learning pomdps believe dependency unavoidable since strictly related partial observable nature pomdps. remark dependency number actions mdps pomdps. hand moving pomdps naturally brings dimensionality observation reward models bound. dependency directly inherited bounds thm. term indeed results terms; ﬁrst term mdps second comes fact transition tensor derived finally term summarizes series terms depend policy space pomdp structure. terms directly inherited spectral decomposition method used core sm-ucrl discussed sect. partial observability states fact states need visited often enough able compute accurate estimate observation reward transition models. remark common assumption dimensionality hidden state space known well number actions observations rewards often case terms appearing thm. actually available. pose problem descriptive bound thm. sm-ucrl actuexplicitly construct conﬁdence intervals. situation relatively common many exploration–exploitation algorithms require computing conﬁdence intervals containing range random variables parameters distributions case sub-gaussian variables. practice values often replaced parameters tuned hand much smaller values theoretical ones. result sm-ucrl terms replaced ﬁxed parameter. notice inaccurate choice setting would mostly translate bigger multiplicative constants ﬁnal regret bound similar bounds smaller probability. general computing conﬁdence bound hard problem even simpler cases markov chains therefore ﬁnding upper conﬁdence bounds pomdp challenging know mixing properties. mentioned another parameter needed compute upper conﬁdence bound described practice replace coefﬁcient constant causes bigger multiplicative constant ﬁnal regret bound. alternatively estimate data. case lower order term regret decays remark thm. rely observation matrix full column rank discussed sect. veriﬁed pomdps number states larger number observations nonetheless possible correctly estimate pomdp parameters full column-rank exploiting additional information coming reward action taken step particular triple redeﬁne third view replace asm. assumption view matrix full column-rank basically requires rewards jointly observations informative enough reconstruct hidden state. change affect observation reward models recovered lemma reconstruction transition tensor need write third view beside dependency multiplication fact larger matrix bound transitions triggered action scales number samples least visited action. fact matrix involves action computing transition model actions well. result actions poorly visited cannot accurately estimated parts negatively affect quality estimation transition model itself. directly propagates regret analysis since require actions repeatedly visited enough. immediate effect introduction different notion diameter. mean passage time steps action chosen according policy deﬁne diameter ratio deﬁnes ratio maximum mean passing time choosing action choosing again minimum. mentioned above order accurate estimate actions need repeatedly explored. dratio small action executed frequently enough large least action executed many others. finally obtain ﬁrst sight bound clearly worse case stronger assumptions notice contains smallest singular values newly deﬁned views. particular larger also covariance matrices kνν′ bigger larger singular values could signiﬁcantly alleviate inverse dependency result relaxing asm. necessarily worsen ﬁnal bound since bigger diameter compensated better dependencies terms. leave complete comparison conﬁgurations future work. introduced novel algorithm pomdps relies spectral method consistently identify parameters pomdp optimistic approach solution exploration–exploitation problem. resulting algorithm derive conﬁdence intervals parameters minimax optimal bound regret. work opens several interesting directions future development. sm-ucrl cannot accumulate samples episodes since thm. requires samples drawn ﬁxed policy. negative impact regret bound open question apply spectral method samples together still preserve theoretical guarantees. memoryless policies perform well domains important extend current approach bounded-memory policies. pomdp special case predictive state representation model littman allows representing sophisticated dynamical systems. given spectral method developed paper natural extension apply general model integrate exploration–exploitation algorithm achieve bounded regret. indicator vector pomdp model state space cardinality element indices observation space cardinality indicator element index action space cardinality element indices reward space cardinality element indicator element index largest value transition density state state given action transition tensor observation density indicator given state observation matrix reward density indicator given pair state-action reward tensor policy policy density action given observation indicator policy matrix smallest element policy matrix stochastic memoryless policies markov chain transition density policy pomdp transition density stationary distribution states given policy conditional action expected average reward policy pomdp best expected average reward policies index views view view matrix time given covariance matrix views smallest non-zero singular value given action second third order moments views given middle action estimates observation reward transition densities action total number samples number samples action numerical constants upper conﬁdence bound error estimated cumulative regret pomdp diameter index episode estimated parameters pomdp episode plausible pomdps episode number samples action episode maximum number samples action episodes optimistic policy executed episode min. number samples meet condition thm. policy action worst smallest non-zero singular value covariance smallest stationary probability actions states policies", "year": 2016}