{"title": "TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep  Reinforcement Learning", "tag": ["cs.AI", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Combining deep model-free reinforcement learning with on-line planning is a promising approach to building on the successes of deep RL. On-line planning with look-ahead trees has proven successful in environments where transition models are known a priori. However, in complex environments where transition models need to be learned from data, the deficiencies of learned models have limited their utility for planning. To address these challenges, we propose TreeQN, a differentiable, recursive, tree-structured model that serves as a drop-in replacement for any value function network in deep RL with discrete actions. TreeQN dynamically constructs a tree by recursively applying a transition model in a learned abstract state space and then aggregating predicted rewards and state-values using a tree backup to estimate Q-values. We also propose ATreeC, an actor-critic variant that augments TreeQN with a softmax layer to form a stochastic policy network. Both approaches are trained end-to-end, such that the learned model is optimised for its actual use in the tree. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a box-pushing task, as well as n-step DQN and value prediction networks (Oh et al. 2017) on multiple Atari games. Furthermore, we present ablation studies that demonstrate the effect of different auxiliary losses on learning transition models.", "text": "combining deep model-free reinforcement learning on-line planning promising approach building successes deep on-line planning look-ahead trees proven successful environments transition models known priori. however complex environments transition models need learned data deﬁciencies learned models limited utility planning. address challenges propose treeqn differentiable recursive tree-structured model serves drop-in replacement value function network deep discrete actions. treeqn dynamically constructs tree recursively applying transition model learned abstract state space aggregating predicted rewards state-values using tree backup estimate q-values. also propose atreec actor-critic variant augments treeqn softmax layer form stochastic policy network. approaches trained end-to-end learned model optimised actual tree. show treeqn atreec outperform n-step box-pushing task well n-step value prediction networks multiple atari games. furthermore present ablation studies demonstrate effect different auxiliary losses learning transition models. promising approach improving model-free deep reinforcement learning combine on-line planning. model-free value function viewed rough global estimate locally reﬁned current state on-line planner. crucially require samples environment additional computation often available. strategy on-line planning look-ahead tree search traditionally methods limited domains perfect environment simulators available board card games however general models complex environments high dimensional observation spaces complex dynamics must learned agent experience. unfortunately date proven difﬁcult learn models domains sufﬁcient ﬁdelity realise beneﬁts look-ahead planning simple approach learning environment models maximise similarity metric model predictions ground truth observation space. approach applied success cases model ﬁdelity less important e.g. improving exploration however objective causes signiﬁcant model capacity devoted predicting irrelevant aspects environment dynamics noisy backgrounds expense value-critical features occupy small part observation space consequently current state-of-the-art models still accumulate errors rapidly used look-ahead planning complex environments. another strategy train model that used predict value function error predictions minimised. encourage model focus features observations relevant control task. example predictron model used policy evaluation without addressing control. value prediction networks take similar approach model construct look-ahead tree constructing bootstrap targets selecting actions similarly td-search crucially model embedded planning algorithm optimisation. propose tree-structured neural network architecture address aforementioned problems. formulating tree look-ahead differentiable integrating directly qfunction policy train entire agent including learned transition model end-to-end. ensures model optimised correct goal suitable on-line planning execution policy. since transition model weakly grounded actual environment approach alternatively viewed model-free method fully connected layers replaced recursive network applies transition functions shared parameters tree node expansion. resulting architecture call treeqn encodes inductive bias based prior knowledge environment stationary markov process facilitates faster learning better policies. also present actor-critic variant atreec tree augmented softmax layer used policy network. show treeqn atreec outperform dqn-based counterparts box-pushing domain suite atari games deeper trees often outperforming shallower trees treeqn outperforming atari games. also present ablation studies investigating various auxiliary losses grounding transition model strongly environment could improve performance well lead interpretable internal plans. show grounding reward function valuable conclude learn strongly grounded transition models generate reliably interpretable plans without compromising performance remains open research question. consider agent learning markov decision process goal maxt= γtrt learning policy maps states actions state-action value function deﬁned optimal q-function maxπ bellman optimality equation writes recursively state transition function reward function simplicity assume deterministic. q-learning uses single-sample approximation contraction operator iteratively improve estimate deep q-learning represented deep neural network parameters improved regressing target maxa parameters target network periodically copied version n-step q-learning synchronous environment threads. particular starting timestep roll forward nenv threads timesteps each. bootstrap ﬁnal states gather nenv transitions single batch algorithms chosen simplicity reasonable wallclock speeds treeqn also used algorithms described section implementations based openai baselines canonical neural network architecture deep visual observations series convolutional layers followed fully connected layers ﬁnal layer produces output action-value. think network ﬁrst calculating encoding state evaluated ﬁnal layer estimate tree-search on-line planning look-ahead tree possible future states constructed recursively applying environment model. states typically evaluated heuristic learned value function monte-carlo rollouts. backups tree aggregate values along immediate rewards accumulated along path estimate value taking action current state. paper focuses simple tree-search deterministic transition function value uncertainty estimates approach extended tree-search variants like components remain differentiable. section propose treeqn novel end-to-end differentiable tree-structured architecture deep reinforcement learning. ﬁrst give overview architecture followed details model component training procedure. treeqn uses recursive tree-structured neural network encoded state predicted state-action values instead directly estimating state-action value current encoded state using fully connected layers speciﬁcally treeqn uses recursive model reﬁne estimate learned transition reward value functions tree backup learned components shared throughout tree treeqn implements inductive bias missing reﬂects prior knowledge q-values properties stationary markov process. also encode inductive bias q-values expressed scalar rewards values. speciﬁcally treeqn learns action-dependent transition function that given state representation l+|t action corresponding reward ˆrai zl|t predicts next state representation l|t. make distinction internal planning steps steps taken environment explicit write zl|t denote encoded state time internal transitions starting encoding treeqn applies transition function recursively construct tree containing state representations rewards received possible sequences actions predeﬁned depth value predicted state estimated value function module. using values predicted rewards treeqn performs tree backup mixing k-step returns along path tree using corresponds value prediction backup fig. formalized function recursively perform backup. value estimates intermediate states mixed ﬁnal q-estimate encourages intermediate nodes tree correspond meaningful states reduces impact outlier values. note even tree depth treeqn imposes signiﬁcant structure value function decomposing action-conditional reward next-state value using shared value function evaluate next-state representation. crucially training backpropagate ﬁnal q-estimate value prediction tree transitioning encoding layers tree i.e. entire network shown fig. learning components jointly ensures useful planning on-line. transition function. ﬁrst apply single fully connected layer current state embedding shared actions. generates intermediate representation could carry information action-agnostic changes environment. addition fully connected layer action applied intermediate representation calculate next-state representation carries information effect taking action residual connections layers rk×k benv learnable parameters. note next-state representation calculated every action independently using respective transition matrix transition function shared action throughout tree. caveat model still learn different parts latent state space different parts tree could undermine intended parameter sharing model structure. help treeqn learn useful transition functions maintain quality diversity latent states introduce unit-length projection state representations simply dividing state’s vector prevents magnitude representation growing shrinking encourages behaviour transition function consistent throughout tree. reward function. addition predicting next state also predict immediate reward every action state zl|t using using hard calculating backup would result gradient information used update parameters along maximal path tree. contrast softmax allows downstream gradient information update parameters along paths. furthermore potentially reduces impact outlier value predictions. learned temperature softmax function could represent hard arbitrarily closely. however empirical difference left temperature treeqn architecture fully differentiable directly place q-function deep algorithm discrete actions. differentiating entire tree ensures learned components useful planning on-line long planning performed training. however seems plausible auxiliary objectives based minimising error predicting rewards observations could improve performance helping ground transition reward functions environment. could also encourage treeqn perform model-based planning interpretable manner. principle objectives could give rise spectrum methods model-free fully model-based. extreme treeqn without auxiliary objectives seen model-free approach draws inspiration tree-search planning encode valuable inductive biases neural network architecture. extreme perfect grounded reward transition models could principle learned. using architecture would correspond standard model-based lookahead planning. sweet spot could intermediate level grounding maintains ﬂexibility end-to-end model-free learning beneﬁting additional supervision explicit model learning. investigate spectrum experiment auxiliary objectives. reward grounding. experiment loss regressing ˆratt+l− predicted reward level tree corresponding selected action sequence at+l−} true observed rewards. timesteps n-step q-learning gives hyperparameter weighting loss restricts rewards already observed true value. state grounding. experiment grounding latent space using loss regress predicted latent state zatt+l level tree z|t+l initial encoding true state corresponding actions actually taken employing additional decoder module could similar loss regress decoded observations true observations. informal experiments joint training decoder loss yield good performance also found intuitions guiding design treeqn applicable policy search valuebased policy tree planner improve estimates optimal action probabilities proposed architecture trained end-to-end easily adapted policy network. particular propose atreec actor-critic extension treeqn. architecture policy network identical treeqn additional softmax layer converts estimates probabilities stochastic policy. critic shares encoder parameters predicts scalar state value single fully connected layer bcr. used different parameters critic value function actor’s tree-value-function module found sharing parameters little effect performance. entire setup shown fig. trained described section addition auxiliary losses used treeqn. note treeqn could also used critic leave possibility future work. long history work combining model-based model-free early example dyna-q trains model-free algorithm samples drawn learned model. similarly seijen train sparse model environment samples used reﬁne model-free q-function. local linear models generate additional samples model-free algorithm. however approaches attempt model on-line improve value estimates. deep value iteration networks learned differentiable model plan require planning full state space must also possess spatial structure local dynamics convolution operations execute planning algorithm. predictron instead learns abstract-state transition functions order predict values. however restricted policy evaluation without control. value prediction networks take similar approach closely related work learned model components used tree planning. however work tree used construct targets choose actions compute value estimates training. estimates instead produced non-branching trajectories following on-policy action sequences. contrast treeqn uniﬁed architecture constructs tree dynamically every timestep differentiates eliminating mismatch model training test time. furthermore convolutional transition functions hence impose spatial structure latent state representations. differences simplify training allow model used ﬂexibly training regimes explain part substantially improved performance atari benchmark. donti propose differentiating stochastic programming optimisation using probabilistic model learn model parameters respect true objective rather maximum likelihood surrogate. however tackle full setting model repeatedly recursively reﬁne predictions. imagination-augmented agents learn improve policies aggregating rollouts predicted model. however rely pretraining observation-space model argue scale poorly complex environments. further aggregation rollout trajectories takes form generic rather value function tree backup inductive bias based structure explicitly present. class value gradient methods also differentiates models train policy. however approach model execution reﬁne policy requires continuous action spaces. chiappa propose methods learning observation-prediction models atari domain models improve exploration. variants scheduled sampling used improve robustness models scaling complex domains proven challenging evaluate treeqn atreec simple box-pushing environment well subset nine atari environments evaluate vpn. experiments designed determine whether treeqn atreec outperform whether scale complex domains. also investigate best ground transition function auxiliary losses. furthermore compare alternative ways increase number parameters computations standard architecture study impact tree depth. full details experimental setup well architecture training hyperparameters given appendix. grounding. perform hyperparameter search coefﬁcients reward state grounding auxiliary losses atari environment seaquest. experiments determine relevant trade-offs ﬂexibility model-free approach potential beneﬁts model-based algorithm. pushing. randomly place agent boxes goals obstacles center tiles grid. agent’s goal push boxes goals steps possible avoiding obstacles. boxes pushed other. obstacles however ‘soft’ block movement generate negative reward agent moves onto obstacle. rewards better planning without causing excessive gridlock. environment inspired sokoban used weber poor actions generate irreversibly conﬁgurations. however level generation process sokoban challenging reproduce exactly open-sourced. details environment rewards given appendix atari. demonstrate general applicability treeqn atreec complex environments evaluate atari suite following nine environments frameskip facilitate planning reasonable timescales. treeqn adds additional parameters standard architecture. compare treeqn baseline architectures increased computation numbers parameters verify beneﬁt additional structure grounding. dqn-wide doubles size embedding dimension dqn-deep inserts additional fully connected layers shared parameters residual connections fully-connected layers dqn. effect non-branching version treeqn architecture also lacks explicit reward prediction. fig. shows result hyperparameter search coefﬁcients auxiliary losses predicted rewards latent states. intermediate value helps performance beneﬁt using latent space loss. subsequent experiments predicted rewards reward-grounding objective encourages model learn appear q-value prediction target n-step q-learning. consequently expect auxiliary loss well aligned true objective. contrast state-grounding loss might help representation learning would explicitly learn part desired target. possible mismatch auxiliary primary objective leads degraded performance using form state grounding. potential route overcoming obstacle joint training would pre-training model done weber inside treeqn model could ﬁne-tuned perform well inside planner. leave possiblity future work. fig. shows results treeqn tree depths compared baseline. domain clear advantage treeqn architecture dqn. treeqn learns policies substantially better avoiding obstacles lining boxes goals easily pushed later. treeqn also substantially speeds learning. believe greater structure brought architecture regularises model encouraging appropriate state representations learned quickly. even depth- tree improves performance signiﬁcantly disentangling estimation rewards next-state values makes easier learn. facilitated sharing value-function parameters across branches. trained n-step q-learning deeper depth- depth- trees learn faster plateau higher shallow depth- tree. domain useful transition functions relatively easy learn extra computation time transition modules help reﬁne value estimates yielding advantages additional depth. fig. shows results atreec tree depths compared baseline. treeqn atreec substantially outperforms baseline. furthermore thanks stochastic policy substantially outperforms treeqn. whereas treeqn sometimes indecisively bounce back forth adjacent states atreec captures uncertainty policy probabilities thus acts decisively. however unlike treeqn atreec shows pronounced differences different tree depths. part ceiling effect domain. however atreec also gated quality critic’s value function experiments single linear layer state encoding described section nonetheless result demonstrates ease treeqn used drop-in replacement deep algorithm learns policies value functions discrete actions. table summarises atari results fig. shows learning curves depth. treeqn shows substantial beneﬁts many environments compared baseline often outperforms atreec always matches outperforms present mean performance random seeds results reported shown dashed lines fig. mean best seeds unspeciﬁed number trials. treeqn. environments except frostbite treeqn outperforms average signiﬁcant gains alien crazyclimber enduro krull seaquest. many environments seem well suited short horizon look-ahead planning simple dynamics generalise well tradeoffs actions become apparent several timesteps. example incorrect action alien trap agent corridor alien. seaquest looking ahead could help determine whether better deeper collect points surface oxygen. however even game mostly reactive decisions like racing game enduro treeqn shows signiﬁcant beneﬁts. treeqn also outperforms additional baselines dqn-wide dqn-deep indicating additional structure grounding architecture brings beneﬁts beyond simply adding model capacity computation. particular interesting dqn-deep often outperformed vanilla baseline optimisation difﬁculties grow depth. contrast additional table summary atari results. number best score throughout training calculated mean last episode rewards averaged exactly agents trained different random seeds. note report statistic average instead best unspeciﬁed number agents. structure auxiliary loss employed treeqn turn additional depth liability strength. atreec. atreec matches outperforms baseline environments. compared treeqn atreec’s performance better across environments particularly qbert reﬂecting overall advantage actor-critic also found mnih box-pushing experiments. however performance much worse seaquest revealing deﬁciency exploration policy entropy collapses rapidly consequently propensity policy gradient methods become trapped local optimum. krull frostbite algorithms poor performance high variance returns agents gated ability explore. games require completion sub-levels order accumulate large scores none agents reliably explore beyond initial stages game. mean performance appears favor treeqn atreec krull perhaps frostbite returns variable draw conclusions number random seeds. combining treeqn atreec smart exploration mechanisms interesting direction future work improve robustness training types environments. compared box-pushing domain less clear performance difference trees different depths. environments greater depth appear employed usefully treeqn small extent resulting best-performing individual agents. however atari domain embedding size transition function much larger dynamics much complex. consequently expect optimisation difﬁculties challenge learning abstract-state transition functions impede utility deeper trees cases. look future work reﬁne methods learning plan abstractly complex domains. however decomposition q-value reward next-state value employed ﬁrst tree expansion clearly utility broad range tasks. inspecting learned policies trees values sometimes correspond intuitive reasoning sensible policies scoring superior action sequences poorer ones. however actions corresponding branches tree scored highly frequently taken future timesteps. ﬂexibility treeqn atreec allows agents useful exploit computation tree reﬁne action-value estimates. found effective strongly ground model components without sacriﬁcing performance interpretability learned trees limited. presented treeqn atreec architectures deep reinforcement learning discreteaction domains integrate differentiable on-line tree planning action-value function policy. experiments box-pushing domain atari games show beneﬁt architectures counterparts well vpn. future work intend investigate enabling efﬁcient optimisation deeper trees encouraging transition functions produce interpretable plans integrating smart exploration. thank sasha salter luisa zintgraf wendelin b¨ohmer contributions valuable comments drafts paper. work supported epsrc autonomous intelligent machines systems. project received funding european research council european union’s horizon research innovation programme nvidia dgx- used research donated nvidia corporation. references marc bellemare yavar naddaf joel veness michael bowling. arcade learning environment evaluation platform general agents. artif. intell. res. samy bengio oriol vinyals navdeep jaitly noam shazeer. scheduled sampling sequence prediction recurrent neural networks. advances neural information processing systems cameron browne edward powley daniel whitehouse simon lucas peter cowling philipp rohlfshagen stephen tavener diego perez spyridon samothrakis simon colton. survey monte carlo tree search methods. ieee transactions computational intelligence games r´emi coulom. efﬁcient selectivity backup operators monte-carlo tree search. computers games international conference turin italy revised papers ./---- marc deisenroth carl rasmussen. pilco model-based data-efﬁcient approach policy search. proceedings international conference machine learning shixiang timothy lillicrap ilya sutskever sergey levine. continuous deep q-learning model-based acceleration. international conference machine learning nicolas heess gregory wayne david silver lillicrap erez yuval tassa. learning continuous control policies stochastic value gradients. advances neural information processing systems levente kocsis csaba szepesv´ari. bandit based monte-carlo planning. machine learning ecml european conference machine learning berlin germany september proceedings volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature ./nature. volodymyr mnih adri`a puigdom`enech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. proceedings international conference machine learning icml york city june vinod nair geoffrey hinton. rectiﬁed linear units improve restricted boltzmann machines. proceedings international conference machine learning june haifa israel junhyuk xiaoxiao honglak richard lewis satinder singh. action-conditional video prediction using deep networks atari games. advances neural information processing systems david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou vedavyas panneershelvam marc lanctot sander dieleman dominik grewe john nham kalchbrenner ilya sutskever timothy lillicrap madeleine leach koray kavukcuoglu thore graepel demis hassabis. mastering game deep neural networks tree search. nature ./nature. david silver julian schrittwieser karen simonyan ioannis antonoglou huang arthur guez thomas hubert lucas baker matthew adrian bolton mastering game without human knowledge. nature david silver hado hasselt matteo hessel schaul arthur guez harley gabriel dulac-arnold david reichert neil rabinowitz andr´e barreto thomas degris. predictron end-to-end learning planning. proceedings international conference machine learning icml sydney australia august richard sutton. integrated architectures learning planning reacting based approximating dynamic programming. machine learning proceedings seventh international conference machine learning austin texas june aviv tamar sergey levine pieter abbeel garrett thomas. value iteration networks. advances neural information processing systems annual conference neural information processing systems december barcelona spain tijmen tieleman geoffrey hinton. lecture .-rmsprop divide gradient running average recent magnitude. coursera neural networks machine learning harm seijen shimon whiteson hado hasselt marco wiering. exploiting best-match equations efﬁcient reinforcement learning. journal machine learning research theophane weber s´ebastien racani`ere david reichert lars buesing arthur guez danilo jimenez rezende adri`a puigdom`enech badia oriol vinyals nicolas heess yujia razvan pascanu peter battaglia david silver daan wierstra. imagination-augmented agents deep reinforcement learning. corr abs/. environment. episode level generated placing agent boxes goals obstacles center tiles grid sampling locations uniformly. outer tiles left empty prevent initial situations boxes cannot recovered. agent move four cardinal directions. agent steps grid episode ends agent receives penalty agent moves pushed direction movement. moving grid generates penalty moving another allowed trying generates penalty leaving positions unchanged. pushed goal removed agent receives reward obstacles generate penalty agent moved onto them. moving agent goals incurs penalty. lastly timestep agent receives penalty episodes terminate timesteps elapsed agent left grid boxes remain. observation given model tensor size ﬁrst four channels binary encodings position agent goals boxes obstacles respectively. ﬁnal channel ﬁlled number timesteps remaining architecture. encoder consists conv-wxh-s-n denotes convolution ﬁlters size stride fc-h denotes fully connected layer hidden units. layers separated relu nonlinearities. hidden layer reward function hidden units. preprocessing inputs follows procedure mnih including concatenation last four frames input although frameskip architecture. atari experiments architecture box-pushing except encoder architecture follows atreec value-function loss coefﬁcient entropy regularisation reward prediction loss scaled nsteps nenvs total batch size discount factor target networks updated every environment transitions.", "year": 2017}