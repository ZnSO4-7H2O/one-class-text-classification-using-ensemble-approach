{"title": "Finding Competitive Network Architectures Within a Day Using UCT", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "The design of neural network architectures for a new data set is a laborious task which requires human deep learning expertise. In order to make deep learning available for a broader audience, automated methods for finding a neural network architecture are vital. Recently proposed methods can already achieve human expert level performances. However, these methods have run times of months or even years of GPU computing time, ignoring hardware constraints as faced by many researchers and companies. We propose the use of Monte Carlo planning in combination with two different UCT (upper confidence bound applied to trees) derivations to search for network architectures. We adapt the UCT algorithm to the needs of network architecture search by proposing two ways of sharing information between different branches of the search tree. In an empirical study we are able to demonstrate that this method is able to find competitive networks for MNIST, SVHN and CIFAR-10 in just a single GPU day. Extending the search time to five GPU days, we are able to outperform human architectures and our competitors which consider the same types of layers.", "text": "design neural network architectures data laborious task requires human deep learning expertise. order make deep learning available broader audience automated methods ﬁnding neural network architecture vital. recently proposed methods already achieve human expert level performances. however methods times months even years computing time ignoring hardware constraints faced many researchers companies. propose monte carlo planning combination diﬀerent derivations search network architectures. adapt algorithm needs network architecture search proposing ways sharing information diﬀerent branches search tree. empirical study able demonstrate method able competitive networks mnist svhn cifar- single day. extending search time days able outperform human architectures competitors consider types layers. recently various approaches using reinforcement learning proposed. approaches based recurrent neural networks q-learning monte carlo planning learn select layer layer others learn change existing architecture improve results approaches require vast amount computational power. work propose method computational feasible still ﬁnds competitive networks. deﬁne search problem markov decision problem state-action graph tree. propose search network architectures maximizing expected reward using monte carlo planning. particular derive diﬀerent policies based compare method state-of-the-art empirical evaluation mnist svhn cifar-. show empirical evaluation proposed methods competitive network architectures minimal loss accuracy. deep learning methods successful various applications machine translation image speech recognition reinforcement learning general reason success ability learn extract features unstructured data. hence observe shift laborious manual feature engineering task audio images text engineering network components architectures. achieving overall better performances still involves laborious manual task requires experts. thus true end-to-end learning remains important active topic research. earliest work based neuro-evolution mutations cross-over operations used adapt network structure even learn network weights. computational expensive procedure. however constraining genetic algorithm mutations learning parameters gradient-based optimization methods possible discover good network architectures enough computational power algorithm selection hyperparameter optimization problem limited machine learning applications. bayesian optimization often considered state-of-the-art automated hyperparameter optimization. drawbacks expects ﬁxed length encoding algorithm hyperparameter choice validation score function smooth respect encoding. bayesian optimization able achieve state-of-theart results ﬁxed architecture encoding network architectures found satisﬁes assumption. nonetheless works apply bayesian optimization search network architectures recently various works shown reinforcement learning neuro-evolution neural networks achieve results similar human-engineered architectures idea using genetic algorithms order adapt neural networks relatively idea. early work evolved weights ﬁxed architecture later work also evolved architecture recent work based neuro-evolution able every action chosen every state order obtain legitimate network structures. soon termination action chosen ﬁnal state reached actions possible. actions adding convolutional layers possible point ﬁrst addition fully connected layer. pooling actions allowed immediately convolutional action. ﬁrst pooling action chosen soon ﬁrst convolutional action kernel size greater selected. computational reasons ﬁrst fully connected layer selected soon input dimension feature smaller eight. consecutive fully connected actions whereas second fully connected layer many units ﬁrst one. section brieﬂy reviewing monte carlo planning algorithm. then show methods applied task ﬁnding competitive neural network architectures describe necessary adaptions improvements. finally describe transfer learning method using order speed search. competitive convolutional neural networks cifar authors propose adapt network architecture random mutations. cross-over operations estimate network weights using gradient-based optimization methods. however optimization method remains computational expensive. several optimization methods based reinforcement learning proposed neural network architecture search. propose learn policy able improve existing network deepening widening others address task architectures scratch propose learn policy estimates ﬁnal architecture choosing layer layer. approaches common neuro-evolution computational expensive. negrinho gordon investigate various optimization methods problem among uct. order make feasible propose combine bisection method share information among similar actions state. contrast them propose diﬀerent variants uct. shares information action similar states sharing information similar actions predicting ﬁnal reward based previously chosen actions. main diﬀerence current work aiming ﬁnding competitive network architecture within time frame aﬀordable researchers. hence give method symbolic time budget single day. deﬁne search neural network architectures markov decision problem described tuple state state space describes current network architecture action action adds another layer. thus describe state actions taken state deﬁned subset action permutations ﬁnal states contain termination action. order allow fair comparison work baker deﬁning action space searching similar network architecture search space. twelve diﬀerent actions adding convolutional layers square kernel sizes ﬁlters stride zero padding. three pooling actions adding layers square pooling sizes strides respectively. another three actions adding fully connected layers monte carlo planning. monte carlo planning approach optimal policies large markov decision problems famous work domain algorithm advances artiﬁcial intelligence many games particular game algorithm designed state-action spaces tree structure. rollout-based algorithm works follows. part state-action tree stored starting root only. rollout algorithm follows tree policy long current state within stored tree. tree policy based formula cumulative reward received selecting action state number times action chosen state total number times action considered leads state rollout. constant controls trade-oﬀ exploration exploitation. architecture search. since state-action space described section tree algorithm principle applied problem without changes. however search space large rollout time-consuming since involves training testing neural network. aﬀord hundreds rollouts spend time exploring search space. reason according formula action state chosen least action considered second time. general problem ﬁrst rollouts. thus gelly proposed rapid action value estimation uct. make assumption matter action selected allows sharing information diﬀerent branches search tree limited domains action sequences transposed. obviously case neural network architectures since order layers important. thus propose alternative sharing information diﬀerent tree branches. combine information actions representing network part depth i.e. furthermore output dimension network part belongs representation bin. distinguish three bins representation size interval number chosen fully connected actions equal reaching states softmax actions additionally required action current state same contextual reward predictions. section propose share information among search tree branches means prediction model. prediction model forecasting reward taking action given state. contrast approach previous section enables predict rewards even cases ravenn cannot infer anything similar cases. furthermore also enables learn data sets investigated previous experiments data learn policies across problems. however work investigate possibility. deﬁne contextual reward prediction policy prediction model predicting reward based action state. number times state layers reached number times action chosen depth encode state reached taking action state representing network depth vector rd+. ﬁrst entries vector contain number parameters layer. entries number total parameters representation size. predictors convolutional actions another entry number ﬁlters deﬁned action exists predictors depth action. labeled examples collected search. convolutional actions share examples among actions kernel size allow predictors learn across diﬀerent number ﬁlters. gaussian process mat´ern kernel prediction model. gaining speed netnet. obviously rollouts conducted closer optimal policy. order speed rollouts using netnet netnet knowledge transfer approach widens layers increases kernel sizes deepens networks without changing predictions. changed network converge less epochs network initialized random weights. search store networks parameters whenever network needs initialized ﬁrst compute network edit distance networks already investigated. deﬁne network edit distance every edit operation supported netnet costs operation inﬁnity certain circumstances. distance eﬃciently computed dynamic programming. consider networks network edit distance two. candidate found initialize parameters random. section summarize results empirical study. ﬁrst describe direct competitor methods data sets used evaluation well post-processing. provide insights proposed policies ravenn develop time. furthermore compare found network architecture human proposed network architectures also using convolutions pooling layers. supplementary material provide additional experiment compare directly human performance recently released fashion-mnist data set. neural architecture search. zoph propose learn policy using recurrent neural network. trained reinforce rule williams recurrent neural network recommending number ﬁlters kernel size convolution layer layer. furthermore predicting skip connections layers. convolutional layers considered. training diﬀerent architectures trained concurrently gpus found best network architecture. ﬁnal grid search optimizes learning rate weight decay batchnorm epsilon epoch decay learning rate. report results achieved variant uses least human intervention optimization. large-scale evolution. real propose evolutionary algorithm order search neural network architectures. starting simplest network keeps improving mutation selection methods. selection achieved selecting individua random keeping better performing one. mutation selected uniformly random generate child. mutations alter learning rate remove convolutional layers skip connections alter properties kernel size number ﬁlters. similar neural architecture search convolutional layers considered. ﬁnal search less computational expensive using gpus metaqnn. baker propose learn policy using q-learning \u0001-greedy strategy experience replay order learn create convolutional neural network layer layer. order allow fair comparison selected action space deﬁned section reinforcement learning neural network transformation. reinforcement learning approach starts existing network learns policy learns widen deepen networks. start eight layer convolutional neural network reasonable architecture. authors argue network performing poor main reason parameters layer. widening starting layers result well performing network already. similar netnet knowledge transfer improve speed. contrast setup always proﬁt knowledge transfer. deeparchitect. negrinho gorden investigate performance random search bayesian optimization compares optimizing neural network architecture. also recognize using vanilla work task limitations discussed above. therefore propose combine bisection. method trade width search tree depth basically shares knowledge similar actions. however strict deﬁnition similar actions requiring network similar actions identical. conclusion approach still spends time exploring state space. experiments denote deeparchitect results obtained bisection best performing approach paper. implementation details. monte carlo planning approach uses following hyperparameters independently investigated data set. exploration-exploitation trade-oﬀ equation make better netnet knowledge transfer start maximum allowed network depth three increase every rollouts. case revisit network architecture evaluated already train previously computed classiﬁcation accuracy. proposed baker dropout layers added every second layer fully connected layer linear increasing dropout rate batch size decay. model signiﬁcantly better random predictor ﬁrst epoch decrease learning rate factor repeat times. possible initialize netnet knowledge transfer train single epoch. otherwise glorot uniform initialization train epochs. post-processing. searched architectures following image classiﬁcation data sets setup described above. since epochs training suﬃcient apply similar however simpler post-processing done baker example optimizing hyperparameters select chosen baker instead. network best score validation selected reinitialized trained according following policies. handwritten digits. subtracted global mean image trained ﬁnal model epochs using adam optimizer settings described above. however decrease learning rate factor every epochs. street view house numbers task svhn data similar mnist data identifying digits. however data larger diﬃcult since many images contain distracting digits sides digit interest. images training testing set. furthermore additional images. search training ﬁnal training make additional examples well. optimize parameters stochastic gradient descent initial learning rate momentum epochs. decrease learning rate factor epochs epochs another ﬁnally epochs weight decay preprocess image subtracting channel’s mean divide standard deviation. figure search progress three diﬀerent policies mnist svhn cifar-. clearly outperforming policies. report validation performance best network mean performance networks. cifar- figure report validation performance best found model well mean validation performance models quickly ﬁnds good performing network architectures clearly outperforming random policy. many cases performance even able outperform proposed ravenn policy. reason reward prediction based current network structure allows learn good combinations layers. methods tends start networks often similar layer combinations. many networks cifar- data start ravenn policy needs time converge good network architectures less information previous layers deciding next layer. however still outperforming random policy given enough time catch policy. figure gives insight whether proposed policies providing indeed useful improvements random policy. presents fraction neural networks achieves validation accuracy threshold provides insight policies’ regret. both ravenn better random policy able detect architectures focus promising ones. results data sets look similar moved supplementary material space constraints. cifar- data similar cifar- times classes. preprocess image using global contrast normalization. optimizer used svhn. however increased number epochs also weight decay reduce learning rate scheme epochs additionally moderately augment data using random horizontal ﬂips random translation pixels. table comparison diﬀerent automatic neural network architecture search methods. results reported accuracy percent duration days. method able competitive network architectures single day. results asterisk obtained using best architecture cifar-. according description section compare obtained results testing data diﬀerent state-of-the-art network architecture search methods table select search budget single demonstrate method used also case limited hardware. thus duration sometimes orders magnitudes smaller used competitor methods. reconduct experiments competitor methods report results corresponding papers. deeparchitect competitor method shortest searching time approximately time gave method. however ravenn clearly outperforming performance. test accuracy deeparchitect reported paper authors made information available website method method shortest running time outperforming results. however method start search scratch rather start network depth eight. well structured containing pooling convolutional fully connected layers. authors claim network poor performance want highlight parameters. since reinforcement approach aims actions ﬁlters layer actually good starting point. thus surprising method starts scratch provides slightly worse results. consider metaqnn closest competitor. method learning network architectures scratch designed action space match exactly setup. report test accuracy within days cifar- orders magnitudes time needed architectures achieve test accuracies mean test accuracy architectures standard deviation comparison architectures achieve test accuracies mean standard deviation thus method’s best solution good metaqnn networks provide consistent good test accuracies. reason ensemble architectures improves accuracy baker report loss accuracy metaqnn neural architecture search large-scale evolution achieve remarkable results. however computation time beyond anything researchers practitioners aﬀord. important question answer good competitor methods given less search time. real report test accuracy cifar- less similarly neural architecture search needs model evaluations show improvement random policy metaqnn uses search time random exploration means results days good random policy. however adapt schedule consider maximum running time instead hundred diﬀerent networks evaluated. network architectures chosen completely random architectures actions chosen according policy probability thus unlikely method ﬁnds useful network single day. report accuracy reinforcement learning method ﬁrst half networks sampled conclusion methods indeed outperforming competitors given time budget single day. conducted another experiment cifar- giving search method times higher time budget whether achieve equal performance time invested. experiment noticed search algorithm converges policy mainly chooses network architectures depth seven. reason learns choose many pooling layers representation size reduced quickly. order focus deeper networks applied following changes. first pooling layers constrained selected least three convolution layers. second slowly increase minimum number layers search. finally selected network architecture highest validation score twelve layers. network trained according previously described protocol full training data achieves accuracy thus achieve similar performance state-of-the-art fraction search time. addressed challenge automating neural network architecture engineering task order make deep learning accessible broader audience. particular aiming speeding procedure order make true end-to-end learning feasible everyone. solution proposed monte carlo planning combination diﬀerent derivations search network architectures. speed search adapted algorithm order share information similar network architectures made netnet knowledge transfer. empirical study demonstrated method able competitive networks mnist svhn cifar- single day. extending search time days outperform existing automated approaches", "year": 2017}