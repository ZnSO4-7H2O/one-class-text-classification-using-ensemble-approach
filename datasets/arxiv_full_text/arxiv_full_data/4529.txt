{"title": "Error-Correcting Tournaments", "tag": ["cs.AI", "cs.LG"], "abstract": "We present a family of pairwise tournaments reducing $k$-class classification to binary classification. These reductions are provably robust against a constant fraction of binary errors. The results improve on the PECOC construction \\cite{SECOC} with an exponential improvement in computation, from $O(k)$ to $O(\\log_2 k)$, and the removal of a square root in the regret dependence, matching the best possible computation and regret up to a constant.", "text": "text abstract present family pairwise tournaments reducing kclass classiﬁcation binary classiﬁcation. reductions provably robust constant fraction binary errors simultaneously matching best possible computation regret construction also works robustly selecting best k-choices tournament. strengthen previous results defeating powerful adversary previously addressed providing form analysis. setting error correcting tournament depth using comparators optimal small constant. technique analyzing reductions regret analysis bounds regret resulting multiclass classiﬁer terms average classiﬁcation regret induced binary problems. regret diﬀerence incurred loss smallest achievable loss problem i.e. excess loss suboptimal prediction. commonly applied reduction one-against-all creates binary classiﬁcation problem classes. classiﬁer class trained predict whether label not; predictions done evaluating binary classiﬁer randomizing predict labels answers simple reduction inconsistent sense given optimal binary classiﬁers reduction yield optimal multiclass classiﬁer presence noise. optimizing squared loss binary predictions instead loss makes approach consistent average squared loss regret induced problems. probing reduction upper bounds average binary classiﬁcation regret. composition gives consistent reduction binary classiﬁcation square root dependence binary regret probabilistic error-correcting output code approach reduces k-class classiﬁcation learning regressors interval creating binary examples multiclass example training test time test time computation resulting multiclass regret bounded removing dependence number classes fear associated pecoc approach creates binary problems form what probability label given random subset labels? hard solve. although fear addressed regret analysis overstated cases still concern especially larger values error-correcting tournament family presented answers questions aﬃrmative. provides exponentially faster method multiclass prediction resulting multiclass regret bounded average binary regret; every binary classiﬁer logically compares distinct class labels. result based basic observation non-leaf node fails predict binary label unavoidable noise distribution nodes node root preference class label prediction. utilizing observation construct reduction called ﬁlter tree uses computation multiclass example training test time whose multiclass regret bounded times average binary regret. decision process ﬁlter tree viewed bottom viewed single-elimination tournament players. using multiple independent single-elimination tournaments aﬀect average regret adversary controlling binary classiﬁers. somewhat surprisingly possible complete single-elimination tournaments players rounds player playing twice round. error-correcting tournament ﬁrst pairs labels simultaneous single-elimination tournaments followed ﬁnal carefully weighted single-elimination tournament decides among winners ﬁrst phase. ﬁlter tree test time evaluation start root proceed multiclass label computation. construction also useful problem robust search yielding ﬁrst algorithm allows adversary constant fraction time full setting comparator missort comparison. previous work either applied half case comparator fail sort actively missort full setting adversary ﬁxed known bound number lies ﬁxed budget fraction errors indeed might even appear impossible algorithm robust constant fraction full errors since error always reserved last comparison. repeating last comparison times defeats strategy. result also useful actual problem tournament construction games real players. analysis assume errors i.i.d. known noise distributions known outcome distributions given player skills consequently tournaments construct robust severe bias biased referee forms bribery collusion. furthermore tournaments construct shallow requiring fewer rounds m-elimination bracket tournaments satisfy guarantee provided here. m-elimination bracket tournament bracket single-elimination tournament players except structions smaller maximum depth bracketed -elimination. bracketed m-elimination tournament satisfy goal note second-best player could defeat ﬁrst player ﬁrst single elimination tournament ﬁnal elimination phase implying adversary need control matches. paper overview. begin deﬁning basic concepts introducing notation section section shows simple divideand-conquer tree approach inconsistent motivating filter tree algorithm described section section proves algorithm best possible computational dependence gives upper bounds regret returned multiclass classiﬁer. subsection presents experimental evidence filter tree indeed practical approach multiclass classiﬁcation. depth. setting gives regret ratio depth results provide nearly free generalization earlier work robust search setting powerful adversary missort well fail sort. section gives algorithm independent lower bound regret ratio large number calls binary classiﬁer independent label predicted strengthen lower bound large standard approach reducing multiclass learning binary learning split labels half learn binary classiﬁer distinguish subsets repeat recursively subset contains label. multiclass predictions made following chain classiﬁcations root leaves. following theorem gives example multiclass problem even optimal classiﬁer induced binary problem node tree reduction yield optimal multiclass predictor. proof find node subset corresponding labels subset corresponding single label. since freely rename labels without changing underlying problem ﬁrst labels third label choose property labels chance drawn given label drawn remaining probability distribution fraction examples label correct minimum error rate binary predictor must choose either label label choices error rate label suﬀers error rate classiﬁer based optimal binary classiﬁer greater tree labels. ﬁrst round labels paired according lowest level tree classiﬁer trained pair predict labels likely. winning labels ﬁrst round turn paired second round classiﬁer trained predict whether winner pair likely winner other. process training classiﬁers predict best pair winners previous round repeated root classiﬁer trained. trick training stage form right training interior node. denote subtree rooted node denote leaves tree training example node formed conditioned predictions classiﬁers round thus learned classiﬁers ﬁrst level tree used ﬁlter distribution examples reaching second level tree. given classiﬁers node every edge identiﬁed unique label. optimal decision non-leaf node choose input edge likely according true conditional probability. done using outputs classiﬁers round ﬁlter training process observation label left parent’s output matches multiclass label right parent’s output matches reject example otherwise. importance classiﬁer pays doesn’t predict importance weighted problem reduced binary classiﬁcation using costing reduction alters underlying distribution using rejection sampling importances. reduction here. algorithm requires computation multiclass example searching correct leaf time ﬁltering back toward root. matches information theoretic lower bound since simply algorithm transforms cost-sensitive multiclass example importance weighted binary labeled examples every nonleaf node tree. process implicitly transforms underlying distribution cost-sensitive multiclass examples distribution importance weighted binary examples reduce importance weighted binary classiﬁcation binary classiﬁcation using costing reduction alters using rejection sampling importance weights. composition transforms distribution core theorem relates regret resulting cost-sensitive classiﬁer again given test example classiﬁer returns unique label every path root prefers type analysis similar boosting round booster creates input distribution calls weak learning algorithm obtain classiﬁer error rate distribution depends classiﬁers returned oracle previous rounds. accuracy ﬁnal classiﬁer original distribution analyzed terms error rates. since importance weights either don’t need apply costing multiclass case. proof corollary given theorem simple since induced node level induced importance weight importance weights therefore thus choosing minimize error rate equivalent choosing minimize expected cost costing reduction uses rejection sampling according weights draw examples given examples drawn remainder section proves theorems conditioned value label distribution costs expected value ec∼d|x. zero regret cost-sensitive classiﬁer predicts according miny ec∼d|x. suppose predicts inducing cost-sensitive regret proof inequality follows induction result immediate assume claim holds subtrees providing respective inputs root outputs without loss generality. using inductive hypotheses consider ﬁlter tree evaluated using given binary classiﬁer before importances nodes importances nodes made mistake. recall regret denoted proof regt diﬀerence cost tree’s output smallest cost importance-weighted binary regret simply since expected importance upper bounded also bounds binary regret holds subtrees providing respective inputs root best cost left subtree suppose ﬁrst chooses regl regt regl left hand side inequality thus regt following simple example shows theorem essentially tight. power every label cost even otherwise. tree structure complete binary tree depth nodes paired order labels. suppose pairwise classiﬁcations correct except class wins games leading cost-sensitive multiclass regret regt leading regret ratio regt tree versus filter-tree all-pairs versus allpairs filter tree several diﬀerent datasets decision tree logistic regression classiﬁer. variant filter tree algorithm signiﬁcant diﬀerence performance practice. every classiﬁcation node essentially labels computed test time implying could simply learn classiﬁer every pair labels could reach test time. conditioning process tree structure gives better analysis achievable all-pairs approach variant uses computation requires data often maximizes performance form classiﬁer constrained. compared performance filter tree all-pairs variant described performance all-pairs tree reduction number publicly available multiclass datasets datasets came standard training/test split isolet optdigits pendigits satimage soybean. datasets reported average result random splits dataset used training testing. computation relatively unconstrained all-pairs all-pairs filter tree reasonable choices. comparison figure shows all-pairs filter tree yields similar prediction performance test error rates using decision trees logistic regression binary classiﬁer learners reported table using weka’s implementation default parameters lowest error rate shown bold although cases diﬀerence insigniﬁcant. tions understanding required reading section. simplicity work multiclass case. extension cost-sensitive multiclass problems possible using importance weighting techniques previous section. m-elimination tournament operates phases. ﬁrst phase consists single-elimination tournaments labels label paired another label round. consequently single elimination tournaments simple binary tree structure; example figure elimination tournament labels. substantial freedom pairings ﬁrst phase done; bounds depend depth mechanism pairs labels distinct single elimination tournaments. explicit mechanism given note example lost times eliminated longer inﬂuences training nodes closer root. second phase ﬁnal elimination phase select winner winners ﬁrst phase. consists redundant singleelimination tournament degree redundancy increases root approached. quantify redundancy every subtree charge equal number leaves subtree. first phase winners leaves ﬁnal elimination tournament charge non-leaf node comparing outputs subtrees importance weight binary example created node either depending whether label comes tournament applications importance weight expressed playing games repeatedly winner must beat winner times advance vice versa. labels compared same importance figure example -elimination tournament players. distinct single elimination tournaments ﬁrst phase—one black blue red. that ﬁnal elimination phase occurs three winners ﬁrst phase. ﬁnal elimination tournament extra weighting nodes detailed text. concept throughout section importance depth deﬁned worst-case length overall tournament importance-weighted matches ﬁnal elimination phase played repeated games. theorem prove bound importance depth. computational bound example essentially importance importance depth controls computation ﬁrst note importance depth bounds tournament depth since importance weights least training time example used tournament level starting leaves. testing time regret theorem analogue corollary error-correcting tournaments notation deﬁned there. previous section reduction transforms multiclass distribution induced distribution binary labeled examples. before denotes multiclass classiﬁer induced given binary classiﬁer tournament structure proof depth ﬁrst phase bounded classical problem robust minimum ﬁnding depth. ﬁrst three cases hold because construction upper bounds depth error-correcting tournament construction bounds fourth case construct depth bound analyzing continuous relaxation problem. relaxation allows number labels remaining single elimination tournament ﬁrst phase broken fractions. relative version actual problem important discretizations single-elimination tournament single label remaining enters next single elimination tournament. eﬀect decreasing depth compared continuous relaxation. single-elimination tournament number labels remaining label play round. thus number players quite halve potentially increasing depth compared continuous relaxation. labels continuous version tournament round ﬁrst tournament corresponds consequently number labels remaining tournaments estimate depth ﬁnding value number value found using chernoﬀ bound. probability lower bounds hold somewhat powerful adversary natural game playing tournament setting. particular disallow reductions importance weighting examples equivalently importance weights note modify upper bound obey constraint transforming ﬁnal elimination ﬁrst lower bound says reduction algorithm exists adversary average per-round regret make incur regret even knows advance. thus adversary corrupts half outcomes force maximally outcome. bounds below denotes multiclass classiﬁer induced reduction using binary classiﬁer proof adversary picks labels comparisons involving decided favor similarly outcome comparing determined parity number comparisons ﬁxed serialization algorithm. parity wins; otherwise wins. outcomes comparisons picked arbitrarily. assume without loss generality wins. depth tournament either least label appear round. depth since label involved query adversary probability label note number rounds bound depend next show algorithm taking number rounds adversary exists adversary regret roughly third make incur maximal loss even knows power adversary.", "year": 2009}