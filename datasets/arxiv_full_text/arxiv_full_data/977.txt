{"title": "Adaptive Normalized Risk-Averting Training For Deep Neural Networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "This paper proposes a set of new error criteria and learning approaches, Adaptive Normalized Risk-Averting Training (ANRAT), to attack the non-convex optimization problem in training deep neural networks (DNNs). Theoretically, we demonstrate its effectiveness on global and local convexity lower-bounded by the standard $L_p$-norm error. By analyzing the gradient on the convexity index $\\lambda$, we explain the reason why to learn $\\lambda$ adaptively using gradient descent works. In practice, we show how this method improves training of deep neural networks to solve visual recognition tasks on the MNIST and CIFAR-10 datasets. Without using pretraining or other tricks, we obtain results comparable or superior to those reported in recent literature on the same tasks using standard ConvNets + MSE/cross entropy. Performance on deep/shallow multilayer perceptrons and Denoised Auto-encoders is also explored. ANRAT can be combined with other quasi-Newton training methods, innovative network variants, regularization techniques and other specific tricks in DNNs. Other than unsupervised pretraining, it provides a new perspective to address the non-convex optimization problem in DNNs.", "text": "neural network models always lead non-convex optimization problem. optimization algorithm impacts quality local minimum hard global minimum estimate particular local minimum best possible solution. standard approach optimize dnns stochastic gradient descent many variants researchers practitioners typically choose particular variant empirically. nearly dnns optimization algorithms popular gradient-based recent work shown advanced second-order methods l-bfgs saddle-free newton approaches yield better results tasks second order derivatives addressed hardware extensions batch methods dealing massive data still provides robust default choice optimizing dnns. instead modifying network structure optimization techniques dnns focused designing error function convexify error space. convexiﬁcation approach studied optimization community decades never seriously applied within deep learning. well-known methods graduated nonconvexity method liufloudas convexiﬁcation method liufloudas convexiﬁcation applied optimization problems error criterion twice continuously differentiable although determining weight added quadratic function convexifying error criterion involves signiﬁcant computation dealing massive data parameters. following name employed deriving robust controllers ﬁlters type risk-averting error proposed theoretically solving non-convex optimization problems empirically proposal normalized riskaverting error gradual deconvexiﬁcation method error criterion proved competitive standard mean square error single layer two-layer neural networks solving data ﬁtpaper proposes error criteria learning approach called adaptive normalized riskaverting training attack non-convex optimization problem training deep neural networks without pretraining. theoretically demonstrate effectiveness based expansion convexity region. analyzing gradient convexity index explain reason learning method using gradient descent works. practice show training method successfully applied improved training deep neural networks solve visual recognition tasks mnist cifar datasets. using simple experimental settings withpretraining tricks obtain results comparable superior reported recent literature tasks using standard convnets mse/cross entropy. performance deep/shallow multilayer perceptron denoised auto-encoder also explored. anrat combined quasinewton training methods innovative network variants regularization techniques common tricks dnns. unsupervised pretraining provides perspective address non-convex optimization strategy training dnns. deep neural networks attracting attention largely impressive empirical performance image speech recognition tasks. convolutional networks facto state-of-the-art visual recognition deep belief networks deep boltzmann machines stacked auto-encoders provide insights generative models learn full generating distribution input data. recently researchers investigated various techniques improve learning capacity dnns. unsupervised pretraining using restrict boltzmann machines denoised autoencoders topographic proved helpful training dnns better weight initialization rectiﬁed linear unit variants proposed optimal activation functions copyright association advancement artiﬁcial intelligence rights reserved. theorem given risk-averting error criterion raepq twice continuous differentiable. corresponding jacobian hessian matrix. convexity region monotonically expands entire parameter space except subregion rn|rank) hpq}. please refer supplementary material proof. intuitively motivated emphasizing large individual deviations approximating functions optimizing parameters exponential manner thereby avoiding large individual deviations achieving robust performances. theoretically theorem states convexity index increases inﬁnity convexity region parameter space expands monotonically entire space except intersection ﬁnite number lower dimensional sets. number sets increases rapidly number training samples increases. roughly speaking larger cause size convexity region grow larger respectively error space rae. error space perfectly stretched strictly convex thus avoid local optimum guarantee global optimum. although works well theory bounded suffers exponential magnitude arithmetic overﬂow using gradient descent implementations normalized risk-averting error criterion ensures convexity error space global optimum. using nrae relax global optimum problem ﬁnding better local optimum meet theoretically practically reasonable trade-off real applications. function learning model parameters normalized risk-averting error criterion corresponding lp-norm error deﬁned proof provided supplemental materials. brieﬂy nrae bounded functions independent overﬂow occurs following theorem states quasi-convexity nrae. theorem given parameter space assume s.t. |λq| guarantee convexity raepq yi). then raepq ting classiﬁcation problems interestingly simnets generalization convnets recently proposed uses operator activation function generalize relu activation pooling. notice operator units exactly mathematical form nrae. however nrae still hard optimize practice plateaus unstable error space caused ﬁxed large convexity index. alleviates problems performance limited suffers slow learning speed. instead ﬁxing convexity index adaptive normalized risk-averting training optimizes nrae tuning adaptively using gradient descent. give theoretical proofs optimal properties standard lp-norm error. experiments mnist cifar- different deep/shallow neural nets demonstrate effectiveness empirically. optimization algorithm approach supposed deal speciﬁcally problem over-ﬁtting however show handled usual methods regularization weight decay dropout. begin deﬁnition norm theoretical justiﬁcations convexity property. suitable real applications since bounded. instead nrae bounded overcome register overﬂow real implementations. prove nrae quasiconvex thus shares global local optimum rae. moreover show lower-bound performance good lp-norm error convexity index satisﬁes constraint theoretically supports anrat method proposed next section. indicates raepq always larger convexity regions standard lp-norm error better enable escape local minima. raepq quasi-convex sharing local global optimum raepq conclusions still valid. roughly speaking nrae always larger convexity region standard lp-norm error terms hessian matrix property guarantees higher probability escape poor local optima using nrae. worst case nrae perform good standard lp-norm error convexity region shrinks decreases local search deviates tunnel convex regions. please refer supplemental materials proofs. rigid proofs generalized lp-norm error also given simnets authors also include quite similar discussions robustness respect lp-norm error propose novel learning method training dnns nrae called adaptive normalized risk-avering training approach. instead manually tuning like learn adaptively error backpropagation considering parameter instead hyperparameter. learning procedure standard batch sgd. show works quite well theory practice. together nrae also penalty term a||λ||−r control changing rate minimize nrae score small penalized regulate convexity region. hyperparameter control penalty index. ﬁrstorder derivatives weight convexity region nrae consistent rae. interpret statement another perspective function strictly monotone function. even strictly convex nrae still shares local global optimum rae. deﬁne mapping function easy bijective continuous. inverse also continuous open mapping. thus easy prove mapping function homeomorphism preserve topological properties given space. theorems state consistent relations among nrae mse. proven greater convexity index larger convex region intuitively increasing creates tunnels local-search minimization procedure travel good local optimum. however care justiﬁcation advantage nrae mse. theorem provides theoretical justiﬁcation performance lower-bound nrae. theorem given training samples model parameters raepq raepq always higher chance better local optimum standard lp-norm error expansion convexity region. convnets convnets mse/ce large convnets random feature convnets l-bfgs large convnets unsup pretraining convnets unsup pretraining convnets dropout large convnets unsup pretraining convnets anrat convnets anrat dropout loss function minimized batch without complex methods momentum adaptive/hand tuned learning rates tangent prop. learning rate penalty weight selected validation sets respectively. initial ﬁxed hold-out validation select best model used make predictions test set. experiments implemented quite easily python theano obtain acceleration mnist dataset consists hand written digits size. training images testing images total. images training validation select hyperparameters report performance test set. test method dataset without data augmentation. cifar- dataset composed classes natural images. training images total testing images. image image size dataset adapt pylearn apply global contrast normalization whitening used goodfellow last images training validation data hyperparameter selection report test accuracy. results convnets mnist dataset structure lenet convolutional max-pooling layers followed fully connected layer densely connected softmax layer. ﬁrst convolutional layer feature maps size max-pooled non-overlapping windows. second convolutional layer feature maps note becomes smaller expectation approaches standard lp-norm error. thus gradient approximately difference nrae standard lp-norm error. large incur plateaus prevent nrae ﬁnding better optima using batch need gradually deconvexify nrae make error space well shaped stable. eqn. anrat solve problem ﬂexible adaptive manner. nrae larger eqn. remains negative makes increase enlarge convexity region facilitating search error space better optima. nrae smaller learned parameters seemingly going optimal tunnel better optima. eqn. becomes positive decrease helps nrae deviate manifold standard lp-norm error make error space stable without large plateaus. thus anrat adaptively adjusts convexity index optimal trade-off between better solutions stability. training approach ﬂexibility. gradient weighted difference nrae standard -norm error enables nrae approach -norm error adjusting gradually. intuitively keeps searching error space near manifold lp-norm error better optima competing time relying standard lp-norm error space. eqn. penalty weight index control convergence speed penalizing small smaller emphasizes tuning allow faster convergence speed nrae lp-norm error. larger forces larger better chance better local optimum runs risk plateaus deviating stable error space. regulates magnitude derivatives gradient descent. present results series experiments designed mnist cifar- datasets test effectiveness anrat visual recognition dnns. explore full hyperparameters eqn. instead hyperparameters mainly compare mse. ﬁnal loss function anrat optimized convolutional max-pooling size. fully connected layer hidden units. prior used strength softmax layer. trained anrat obtain test error best result aware dropout pure convnets. summarize best published results standard mnist dataset table best performing neural networks pure convnets dropout unsupervised pretraining achieve error demonstrated performance l-bfgs. using dropout relu response normalization layer error reduces prior that jarrett showed increasing size network using unsupervised pretraining obtain better result previous state single model original mnist dataset. using batch optimize either convnets descried above error rate replacing training methods anrat using batch leads sharply decreased validation error test error dropout relu test error rate drops best results without averaging data augmentation standard convnets simple experimental settings. fig. shows progression training validation test errors training epochs. errors trained plateau train convnets sufﬁciently seems like underﬁt. using anrat validation test errors remain decreasing along training error. training sharply decrease regulating tunnel nrae approach manifold mse. afterward penalty term becomes signiﬁcant force grow gradually expanding convex region higher chance better optimum performed cifar- dataset. observed signiﬁcant overﬁtting using anrat ﬁxed learning rate batch dropout applied prevent co-adaption weights improve generalization. similar network convolutional max-pooling layers. ﬁrst convolutional layer convnets stochastic pooling dropout convnets dropout +bayesian hyperopt convnets maxout dropout convolutional dropout deeply supervised nets dropout feature maps size max-pooled non-overlapping windows. second convolutional layer feature maps convolutional max-pooling size. fully connected layer hidden units. dropout applied layers network probability retaining hidden unit different layers network. using batch optimize simple conﬁguration convnets dropout test accuracy achieved also reported performance instead similar network layout. replacing training methods anrat using batch gives test accuracy superior results obtained mse/ce unsupervised pretraining. table. result simple setting shown competitive achieved different convnet variants. dataset compare anrat shallow deep mlps mse/ce unsupervised pretraining. shallow mlps follow network layout hidden layer neurons. build stacked architecture deep network using architecture hidden units ﬁrst second third layers respectively. training approach purely batch momentum adaptive learning rate. weight decay regularization technique applied experiments. experiment results table. show deep classiﬁer trained anrat method lowest test error rate benchmark classiﬁers mse/ce settings. indicates anrat ability provide reasonable solutions different initial weight vectors. result also better deep supervised pretraining stacked logistic regression networks. note deep using unsupervised pretraining remains best test error unsupervised pretraining effective initializing weights obtain better local optimum. compared unsupervised pretraining tuning anrat sometimes still fall sightly worse local optima case. however anrat signiﬁcantly better mse/ce without unsupervised pretraining. interestingly observe signiﬁcant advantages anrat shallow mlps. although early literature error rate shallow mlps reported recent papers using experiments achieve error rate respectively. trained anrat test rate performance slightly better statistically identical performance obtained possible reason shallow networks trained quite well standard back propagation normalized initializations local optimum achieved mse/ce quite nearly global optimum good saddle point. result also corresponding conclusion dauphin extend previous ﬁndings networks single hidden layer show theoretically empirically badly suboptimal critical points saddle points. even better convexity property anrat good mse/ce shallow mlps. however problem poor local optimum becomes manifest deep networks. easier anrat towards better optimum near manifold mse. sake space please refer supplemental materials results shallow denoised auto-encoder. conclusion consistent anrat performs better attacking difﬁcult learning/ﬁtting problems. anrat slightly better ce/mse uniform paper introduce novel approach adaptive normalized risk-averting training help train deep neural networks. theoretically prove effectiveness normalized risk-averting error arithmetic bound global convexity local convexity lower-bounded standard lp-norm error convexity index analyzing gradient explained reason using back propagation works. experiments deep/shallow network layouts demonstrate comparable better performance experimental settings among pure convnets batch unsupervised pretraining provides perspective address nonconvex optimization strategy dnns. finally early results encouraging clearly research warranted address questions arise non-convex optimization deep neural networks. preliminarily showed order generalize wide array tasks unsupervised semi-supervised learning using unlabeled data crucial. interesting future work take advantage unsupervised/semisupervised pretraining non-convex optimization methods train deep neural networks ﬁnding nearly global optimum. another crucial question guarantee generalization capability preventing overﬁtting. finally quite interested generalizing approach recurrent neural networks. leave future work performance improvement benchmark datasets considering cutting-edge approach improve training generalization performance. t.-h.; peng overcoming local-minimum problem training multilayer perceptrons nrae training method. advances neural networks–isnn springer. t.-h. convexiﬁcation data ﬁtting. ranzato huang boureau y.l.; lecun unsupervised learning invariant feature hierarchies applications object recognicomputer vision pattern recognition tion. cvpr’. ieee conference ieee. speyer deyst jacobson optimization stochastic linear systems additive measurement process noise using exponential performance criteria. automatic control ieee transactions", "year": 2015}