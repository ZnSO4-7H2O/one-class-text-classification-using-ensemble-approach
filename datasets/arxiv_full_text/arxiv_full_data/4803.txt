{"title": "Incremental Truncated LSTD", "tag": ["cs.LG", "cs.AI"], "abstract": "Balancing between computational efficiency and sample efficiency is an important goal in reinforcement learning. Temporal difference (TD) learning algorithms stochastically update the value function, with a linear time complexity in the number of features, whereas least-squares temporal difference (LSTD) algorithms are sample efficient but can be quadratic in the number of features. In this work, we develop an efficient incremental low-rank LSTD({\\lambda}) algorithm that progresses towards the goal of better balancing computation and sample efficiency. The algorithm reduces the computation and storage complexity to the number of features times the chosen rank parameter while summarizing past samples efficiently to nearly obtain the sample complexity of LSTD. We derive a simulation bound on the solution given by truncated low-rank approximation, illustrating a bias- variance trade-off dependent on the choice of rank. We demonstrate that the algorithm effectively balances computational complexity and sample efficiency for policy evaluation in a benchmark task and a high-dimensional energy allocation domain.", "text": "balancing computational efﬁciency sample efﬁciency important goal reinforcement learning. temporal difference learning algorithms stochastically update value function linear time complexity number features whereas least-squares temporal difference algorithms sample efﬁcient quadratic number features. work develop efﬁcient incremental lowrank lstd algorithm progresses towards goal better balancing computation sample efﬁciency. algorithm reduces computation storage complexity number features times chosen rank parameter summarizing past samples efﬁciently nearly obtain sample efﬁciency lstd. derive simulation bound solution given truncated low-rank approximation illustrating biasvariance trade-off dependent choice rank. demonstrate algorithm effectively balances computational complexity sample efﬁciency policy evaluation benchmark task high-dimensional energy allocation domain. introduction value function approximation central goal reinforcement learning. common approach learn value function minimize mean-squared projected bellman error dominant approaches generally split stochastic temporal difference methods least squares temporal difference methods. learning requires computation storage step features sample inefﬁcient sample used stochastic update. nonetheless practical incremental updating particularly highdimensional features remains dominant approach. cost higher computational complexity storage complexity. several algorithms proposed tackle practical issues including ilstd ilstd sigma-point policy iteration random projections experience replay strategies forgetful lstd practical incremental lstd strategies typically consist using system model similar experience replay using random projections reduce size system date however none seem take advantage fact lstd system likely low-rank dependent features small numbers samples principal subspaces highways environment work propose t-lstd novel incremental lowrank lstd bridge computation sample efﬁciency. advantage using low-rank approximation direct approximation less signiﬁcant parts system. original linear system features corresponding matrix incrementally maintain truncated rank singular value decomposition reduces storage signiﬁcantly smaller matrices computation addition practical computational gains approach several beneﬁts. first exploits fact linear system likely redundancies reducing computation storage without sacriﬁcing much accuracy. second resulting solution better conditioned truncating singular values form regularization. regularization strategies proven effective stability however unlike previous approaches truncated also reduces size system. third like ilstd provides close approximation system storage complexity reduced instead intuitive toggle balance computation approximation. finally somewhat orthogonal strategy sub-select features applying lstd feature selection important topic own; therefore focus exploration direct approximations lstd system itself. iteratively update solve system maintaining rank approximation directly. matrix rd×d singular value decomposition rd×d diagonal matrix singular values rd×d orthonormal matrices decomposition full rank inverse simply computed inverting singular values vς−ub. many cases however rank smaller giving rank singular values zero. further approximate dropping number smallest singular values obtain rank approximation. correspondingly rows zeroed reducing size matrices reduce dimension practical efﬁcient incremental updating; however clearly trade-off terms accuracy solution. ﬁrst investigate theoretical properties using low-rank approximation present incremental t-lstd algorithm. characterizing low-rank approximation low-rank approximations provide efﬁcient approach obtaining stable solutions linear systems. approach particularly well motivated resource constrained setting classical eckart-young-mirsky theorem states optimal rank approximation matrix unitarily invariant norm truncated singular value decomposition. addition nice property facilitates development efﬁcient approximate lstd algorithm truncated viewed form regularization improving stability solution. ordered singular values σrank pseudo-inverse σrank+ rank ...) composed inverses non-zero singular values. small still non-zero outer product scaled large number; often correspond highly overﬁtting observed samples high variance estimate. common practice regularize regularization weight modifying multiplier ηi)−ab v−σub. regularization reduces variance introduces bias controlled obtain unbiased solution. similarly thresholding smallest singular values retain singular values better investigate merit low-rank approximations lstd ﬁrst derive simulation bound lowrank approximations highlighting bias-variance trade-off given form regularization. empirically investigate rank properties system benchmark task common feature representations explore validity using lowrank approximation reinforcement learning. finally demonstrate efﬁcacy t-lstd value function approximation domain well high-dimensional energy allocation domain. problem formulation assume agent interacts receives reward environment formalized markov decision process states |s|; actions; transition probability function; reward function probability transitioning state state taking action receiving reward discount rate. policy deﬁne πpr) vector average rewards state value state expected discounted future rewards assuming actions selected according rn×d corresponds features state; diagonal matrix stationary distribution diagonal; trace parameter λ-return. action-value function approximation system same state-action features matrices approximated using several strategies solve system incrementally. standard approach variants stochastically update samples w)zt. lstd algorithms instead incrementally approximate matrices corresponding system. example original lstd algorithm incrementally maintains using matrix inversion lemma step solution adding subtracting terms express error terms differences converges converges zero differences vjσp− converge zero. remark notice truncation bias term disappears ﬁrst term could large could small fact previous work ﬁnite sample analysis lstd uses unbiased estimate bound suffers inverse relationship smallest eigenvalue here avoid potentially large constant bound expense additional bias term determined choice lasso-td similarly avoids dependence using regularization; best knowledge however exist efﬁcient incremental lasso-td algorithm. future goal bound obtain ﬁnite sample bound t-lstd using up-to-date analysis tagorti scherrer general techniques linear system introduced pires szepesvari remark discrete picard condition could relaxed average discrete picard condition average similar bound variance ratio. assumption above however simpliﬁes analysis much clearly illustrates importance decay obtaining stable lstd solutions. shown low-rank approximation effective computing solution lstd samples. however computational complexity explicitly computing samples performing feasible settings. section propose algorithm incrementally computes low-rank singular value decomposition samples signiﬁcantly improved storage computational complexity reduced using mini-batches size maintain low-rank approximation incrementally need update samples. rank-one matrix consequently take advantage recent advances fast low-rank updates specialized computational improvements setting. algorithm summarizes generic incremental update t-lstd mini-batches update step depending choice mini-batch size space constraints detailed pseudo-code updates left detailed code explanations published on-line. basics update follow previous work implementation offers optimizations speciﬁc lstd case. characterize bias-variance tradeoff bound difference true solution approximate rank solution time wtr. similar analysis used regularized lstd previous bound easily extend regularized lstd singular values scaled maintaining information singular vectors bound loss incurred dropping singular vectors using insights work ill-posed systems. following simple realistic assumption illposed systems assumption states shrinks faster speciﬁes smoothness solution related smoothness parameter hilbert space setting assumption linear system deﬁned satisfy discrete picard condition write avoid cluttered notation explicitly subscript further though singular values unique space equivalent singular vectors sign changes multiplication rotation matrices. assume among space equivalent svds similar singular vectors singular value chosen avoids uniqueness issues without losing generality conceptually compare svds proof rely practically obtaining matching svd. theorem approximated samples truncated rank i.e. last singular values zeroed. rbt. assumption relative error rank-r weights true weights bounded follows detailed proof provided appendix posted paper. step split error terms approximation error ﬁnite number samples bias choice second part bounded using discrete picard condition ensure magnitude dominate error maintain computational complexity matrix vector multiplications need carefully ordered. example compute ﬁrst computed computed ﬁnally multiplied computation arises re-diagonalization multiplication resulting orthonormal matrices. mini-batches size computational improvements amortizing costs across steps obtain total amortized complexity losing term. multiplication reβt quires computation urσrv multiplying full matrix urσrv hand would require computation prohibitive. further selected obtain running average algorithm generally example improve tracking chosen constant weight recent samples highly value function estimate. lstd evaluate utility balancing sample computational complexity. common feature representations tile coding radial basis function coding. policy commonly used energy-pumping policy picks actions pushing along current velocity. true values estimated using rollouts states chosen uniform grid state-space. reported root mean squared error computed between estimated value functions rollout values. tile coding representation features using layers grids. representation features grid rbfs width equal times total range state space. purposefully total number features similar cases order keep results comparable. widths obtain good performance lstd. parameters optimized algorithm. mountain results mini-batch case discount results averaged runs. empirically observed large singular value rest small. observed mountain across wide range parameter choices tile coding rbfs hinting could reasonably approximated small rank. order investigate effect rank t-lstd vary t-lstd number samples. figure observe gracious decay quality estimated value function rank reduced achieving lstd level performance little rbfs tile coding given large enough rank numerical precision lstd t-lstd behave similarly. verify this figure plot learning curves t-lstd case small case large enough alongside lstd expected observe lstd t-lstd near identical learning curves while case smaller rank algorithm converge rapidly inferior solution. less sample efﬁcient converges slowly either. sample efﬁciency important property algorithm completely capture needs engineer attempting solve domain. many case requirements tend call balance runtime number samples. cases simulator available game playing samples readily available computational cost matters. reason explore performance lstd t-lstd given unlimited data limited time. figure plot accuracy methods respect computation time used. algorithms given access varying amounts samples samples t-lstd lstd. rmse time taken monitored which points averaged generate plots comparing runtime error learned solution. results show despite poor sample efﬁciency outperforms lstd given runtime computational efﬁciency update. supports trend preferring large problems lstd. observe figure impact rank rmse true discounted returns learned value function mountain car. large necessary performance levelling high values fewer samples error slightly increases likely instability incremental updating small singular values. figure rmse true discounted returns learned value function mountain rbfs signiﬁcantly reduced t-lstd match lstd outperforms best setting lstd computation restricted spend time processing samples. provide best scenario unlimited samples. again t-lstd almost match performance signiﬁcantly outperforms lstd. together graphs indicate t-lstd balance extremes. reported results best parameter settings t-lstd. t-lstd achieve comparable runtime even though t-lstd computationally costly superior sample efﬁciency compensates. furthermore inﬁnite sample stream case favorable scenario data obtain real-time sacriﬁcing sample efﬁciency computational gains might leave idling occasionally further reinforcing t-lstd good alternative. results indicate t-lstd offers effective approach balance sample efﬁciency computational efﬁciency match lstd respective cases offering good performance data plentiful still offering lstd-like sample efﬁciency. value function accuracy energy domain section demonstrate performance fully incremental algorithm large energy allocation domain focus experiment evaluate practical utility t-lstd important application versus realistic competitors ilstd. goal agent domain maximize revenue satisfy demand. action vector allocation decision. state four dimensional variable amount energy storage amounts renewable generation available market price energy demand needs satisﬁed. provided near-optimal policy approximate value function tile coding tilings tiling contains grids resulting features also included bias unit. choose representation ilstd computationally feasible high-dimensional sparse representations. before extensive rollouts computed subset states compute accurate estimate true value stored comparison computation rmse. results averaged runs. report results several values t-lstd. sweep additional parameters algorithms including step-sizes ilstd ilstd. sweep range divide number active features further ilstd unstable unless decayedwe sweep decay formula suggested geramifard bowling chosen focus parameter sweeps step-size much effect ilstd algorithms except tlstd choose restrict ilstd parameters small since many options even optimal stepsize would different choose different values. preliminary investigation indicated large enough ilstd. domain common strategy creating large number ﬁxed features t-lstd able signiﬁcantly take advantage rank structure learning efﬁciently without incurring much computational cost. figure shows t-lstd performs well figure rmse value function energy allocation domain. performance t-lstd improves rank increases; however even small algorithm still converges bias. smaller error signiﬁcantly worse. t-lstd converges almost level signiﬁcantly fewer steps. best parameters chosen algorithm ilstd tlstd. before plot rmse versus runtime selecting scenario extremes plotted figure number samples second restricted samples meaning sometimes idle waiting samples ilstd t-lstd could slow process samples. plot indicates advantages t-lstd particularly faster terms sample efﬁciency scales better ilstd converges better solution. highlight ilstd practical competitors introduced setting incremental learning computational constraints. even then ilstd restrictive feature representation must sparse storage requirements further though reasonably robust choice found ilstd quite sensitive choice step-size parameter. fact without careful decay still encountered divergence issues. goal investigate performance simplest version t-lstd fewest parameters withoptimizing thresholds kept ﬁxed reasonable heuristics across experiments. choice impact learning curves t-lstd. example though t-lstd signiﬁcantly faster early convergence less smooth either ilstd. lack smoothness could optimizing parameters solved step. beyond vanilla implementation t-lstd clear avenues explore smoothly update low-rank approximation nonetheless even simplest form t-lstd provides attractive alternative obtaining sample efﬁciency improvements without much additional computation without need tune step-size parameter. discussion conclusion paper introduced efﬁcient value function approximation algorithm called t-lstd maintains incremental truncated singular value decomposition lstd matrix. systematically explored validity using lowrank approximations lstd ﬁrst proving simulation error bound truncated low-rank lstd solutions then empirically examining incremental truncated lstd algorithm domains. demonstrated performance t-lstd benchmark domain mountain exploring runtime properties effect small rank approximation high-dimensional energy allocation domain illustrating t-lstd enables nice interpolation properties lstd out-performs ilstd. several potential beneﬁts t-lstd explore preliminary investigation. first clear advantages t-lstd tracking control. mentioned above unlike previous lstd algorithms past samples t-lstd efﬁciently down-weighted enabling down-weighting strongly inﬂuenced recent samples better adapt non-stationary environment control. another interesting avenue take advantage t-lstd early learning improve sample efﬁciency switch converge unbiased solution. even highly constrained systems terms storage computation aggressively small still useful early learning. empirical investigation could give insight switch could occur depending choice finally important avenue approach investigate convergence properties truncated incremental svds. algorithm derivation requires simple algebra clearly sound; however best knowledge question convergence numerical stability truncating non-zero singular values remains open. truncated incremental shown practically useful numerous occasions principal components analysis partial least squares moreover informal arguments even truncation re-orient open question important next step understanding t-lstd generally incremental singular value decomposition algorithms reinforcement learning. boyan. least-squares temporal difference learning. international conf. machine learning steven bradtke andrew barto. linear least-squares algorithms temporal difference learning. machine learning keller mannor precup. automatic basis function construction approximate dynamic programming reinforcement learning. international conference machine learning csaba szepesvari. statistical linear estimation penalized estimators application reinforcement learning. international conference machine learning prashanth nathaniel korda r´emi munos. fast lstd using stochastic approximation finite time analysis application trafﬁc control. arxiv.org roman vicente hernandez andres tomas. robust efﬁcient parallel solver based restarted lanczos bidiagonalization. electronic transactions numerical analysis salas powell. benchmarking scalable approximate dynamic programming algorithm stochastic control multidimensional energy storage problems. dept oper financial bruno scherrer. rate convergence error bounds lstd. inter. conf. machine learning roy. analysis temporal-difference learning ieee transactions function approximation. automatic control proof theorem theorem approximated samples truncated rank i.e. last singular values zeroed. rbt. assumption relative error rank-r weights true weights bounded follows function ﬁrst inequality follows using triangle inequality discrete picard condition. further know fact unit vectors. quantity second term including below better understand quickly term disappears. general even without explicit rate convergence know exists function rank follows know singular vectors correspond non-zero singular values rank. singular vectors must orthogonal rank. consequently implementation details experimental set-up experiments core intel xeon .ghz ram. trial thread running parallel. algorithms implemented python using numpy’s scipy’s math libraries. matrix vector operations performed numpy’s optimized subroutines. implementations lstd used. ﬁrst builds matrix incremental additions outer-products second batches operation faster matrix-matrix multiplication. note batch version allows solving least-squares problem lstd sparse dense case either numpy scipy’s subroutines used. dense case best knowledge numpy used subroutine used t-lstd. implementation t-lstd support sparse vectors lstd did. runtime experiment lstd used faster batch implementation. made sure enough memory available thread memory required stored disk. ensure minimal interaction different threads. note cost generating data counted runtime algorithm. mini-batch algorithm perform svd-update mini-batches size obtain computational complexity update given algorithm computational complexity call algorithm called every steps giving amortized complexity since fully incremental algorithm perform update compute current function approximation solution time step longer amortize costs across steps. setting however obtain nice efﬁciency improvements exploiting form update sample obtain algorithm. simplest implementation work using singular value decomposition diagonalize could improve efﬁciency using lanczos bi-diagonalization algorithm full orthogonalization however leave additional speed improvement future work focus vanilla t-lstd implementation work. note avoid computation update matrix saved next iteration ﬁrst applied vectors. additional aspect algorithm enable size approximation grow fully incremental setting information quickly thrown away truncation performed step. allowing subspace grow algorithm better track incorporate information; steps truncate. change computational complexity algorithms terms order runtime multiplied constants. further ensure maintain matrix multiplications perform computation step amortize periodically performing computation important numerical stability though remains future work fully understand frequently done well much subspace allowed grow. competitor algorithms highlight algorithms compare carefully justify match setting t-lstd designed. lstd included comparison energy domain computationally infeasible storage computational complexity billion. compare flstd-sa since algorithm designed streaming setting rather requires batch data upfront randomly subsample. fact much similar algorithm samples drawn uniformly randomly. similarly random projections lstd introduced analyzed batch setting. finally forgetful lstd also designed different purpose goal improve upon lstd linear dyna. consequently focus algorithm computational efﬁciency least terms memory storage. figure sorted singular values tile coding using layers grids. sorted singular values tile coding using grids rbfs widths equal range state space. number singular values required total weight singular values tile coding using varying number layers grids. number singular values required total weight singular values using grid rbfs varying widths reported fraction state space range. number singular values required accurately represent follows interesting trend. first figure change number features adding layers tile coding required number features plateaus. opens possibility using rich representations tile coding keeping representation compact. secondly figure observe rapid drop number singular value required width rbfs increases. shows form approximation effectively leverage dependencies features potentially giving designer ﬂexibility choosing features include letting algorithm extract relevant information. additional value function accuracy graphs learning curves benchmark domains main paper mountain rbfs. includes results tile coding pendulum. experiments combined singular values graphs highlight t-lstd well-suited problems large enough incorporate majority large singular values. singular values tile coding representation benchmark tasks drop quickly rbfs; clearly reﬂected performance t-lstd. fact expect t-lstd perform well settings large singular values. interestingly however even smaller needed system t-lstd still obtains early learning gains. suggests interesting avenues moving forward combining t-lstd strategies robustness t-lstd applied systems matrix large singular values. includes graphs already shown paper make easier look results together. finally figure demonstrates runtimes t-lstd ilstd increasing parameters corresponding parameters algorithms result runtime respectively. despite equivalence terms order t-lstdactually scales better ilstd stores matrix size accesses columns times. several steps ilstd implemented sparse operations matrix sparse. figure root mean squared error true discounted returns learned value function several different scenarios. rmse reported domains tile coding using layers grids. grid rbfs width equal times total range state space. versus chosen rank large necessary performance leveling high values fewer samples error slightly increases likely instability incremental updating small singular values.", "year": 2015}