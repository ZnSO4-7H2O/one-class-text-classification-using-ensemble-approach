{"title": "Sparse Neural Networks Topologies", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We propose Sparse Neural Network architectures that are based on random or structured bipartite graph topologies. Sparse architectures provide compression of the models learned and speed-ups of computations, they can also surpass their unstructured or fully connected counterparts. As we show, even more compact topologies of the so-called SNN (Sparse Neural Network) can be achieved with the use of structured graphs of connections between consecutive layers of neurons. In this paper, we investigate how the accuracy and training speed of the models depend on the topology and sparsity of the neural network. Previous approaches using sparcity are all based on fully connected neural network models and create sparcity during training phase, instead we explicitly define a sparse architectures of connections before the training. Building compact neural network models is coherent with empirical observations showing that there is much redundancy in learned neural network models. We show experimentally that the accuracy of the models learned with neural networks depends on expander-like properties of the underlying topologies such as the spectral gap and algebraic connectivity rather than the density of the graphs of connections.", "text": "propose sparse neural network architectures based random structured bipartite graph topologies. sparse architectures provide compression models learned speed-ups computations also surpass unstructured fully connected counterparts. show even compact topologies so-called achieved structured graphs connections consecutive layers neurons. paper investigate accuracy training speed models depend topology sparsity neural network. previous approaches using sparcity based fully connected neural network models create sparcity training phase instead explicitly deﬁne sparse architectures connections training. building compact neural network models coherent empirical observations showing much redundancy learned neural network models. show experimentally accuracy models learned neural networks depends expander-like properties underlying topologies spectral algebraic connectivity rather density graphs connections. last decade seen rapid development neural network methods widely considered best tools solving hard machine learning problems speech image video recognition. neural networks believed learn complex models iterating operations linear projections high-dimensional data pointwise nonlinear mappings. however process building good quality neural network models timeconsuming models’ space complexity usually high. paper investigate quality neural network model depends topology connections consecutive layers. propose sparse neural network architectures match exceed accuracy dense counterparts. first study random sparse architectures quite compressible study similar structured versions provide even compact description entire architecture. sparse structured graphs highly-compressible share properties sparse random counterparts. show good quality model well chosen topology equal supersede fully connected equivalent accuracy much smaller learned parameter space. also show results robust hold multiple layer neural network over-ﬁtting prevention techniques dropout also additional insight neural network inner workings limitations implementing snn’s viewing lens graphs. investigate algebraic connectivity impacts neural network’s power show strong correlation interconnected neural network resulting testing accuracy. recently observed imposing speciﬁc structure linear embedding part neural network computations leads signiﬁcant speed-ups memory usage reductions minimal loss quality model. approach graph connections consecutive neural network layers still dense reduction comes recycling compact vector learned weights across entire matrix. authors compact multilinear format called tensor-train format represent dense weight matrix fully-connected layers. even basic technique relies using low-rank representation weight matrices. empirically tested restricting rank matrix neurons connections rank affect much quality model setting restrictive presented since low-rank matrices considered papers particular displacement rank. sets techniques methods exploting so-called hashednets architectures incorporate speciﬁc hashing tricks based easily-computable hash functions group weights connections hash buckets. procedure signiﬁcantly reduces size entire model. methods involve applying certain clustering quantization techniques fully-connected layers that reported sparse neural network architectures studied before different context. good quality deep neural network/energy-based models might obtained imposing sparsity restrictions nonzero activations using linear rectiﬁer nonlinear mapping. sparsity also studied case dropout dropconnect features random vertices edges deactivated training batch prevent over-ﬁtting. however architectures based fully connected neural network sparsiﬁed training phase. goal deactivate many neurons connection training phase applying particular regularization cost function rather sparsify graph connections before. setting allows speed computations beginning training phase fact sparse matrix vector multiplication conducted much faster dense matrix vector multiplication counterpart. focus deﬁning adjacency matrices bipartite graphs consecutive layers ﬁxed neural network architecture. assuming layer consists neurons layer consists neurons consider standard unweighted adjacency matrix adjl rn×m where otherwise denotes edges corresponding bipartite graph stands edge neuron layer neuron layer. deﬁned adjacency matrix applied element-wise weight matrix respective layer update effectively zeroing non-desired connection. denote expected degree neuron layer i.e. adj. obtain satisfactory compression rates construction edge matrix created random binary vector length rotated circulant fashion. rotating vector ﬁlled assigning connection probability construction approximated bipartite expander graph created. vertex input ﬁxed degree meaning exactly connections output right layer. connections randomly assigned right layer giving random graph high probability expander. details probabilistic constant degree expanders refer construction particularly interesting rigid structure allowing better computing time smaller memory space used. edge matrix related graph presented ﬁgure below. construction deterministic vector length created ones followed ones regularly placed m−k/ positions left. motivation connectedness away neurons allows information mixing better accuracy simple regular rotating edge construction. construction deterministic vector length created using ﬁrst fibonacci numbers. fibonacci number smaller create connection index fibonacci list. also zero index fibonacci list. fibonacci number bigger normalize list multiplying ibo. transformation list ﬂoat numbers. certain exactly ones vector next element current value already one. vector rotated similar fashion previous deterministic constructions produce edge matrix. order investigate sparsity impacts performance canonical mnist database handwritten digits images training additional images test set. neural architecture hidden layer size conventional ﬂattened input output layer size. train many networks varying constructions described varying degree imposed input hidden layer connection hidden layer output ﬁtting total networks. train network epochs ﬁxed batch size mean square error loss function initialization function glorot gaussian standard stochastic gradient descent optimizer used nesterov momentum learning rate activation function neurons regular sigmoid. implementation achieved using python package keras theano backend. modify update steps impose sparsity step described previously. ﬁnding levels imposed sparsity perform well fully connected counterparts. figure precipitous drop accuracy well connections remain varying expected degree input layer hidden layer. result consistent varying expected degree hidden layer well seen contour plot figure experiments depicted hidden layer regular another result different sparse architectures degree connection don’t result accuracy. figure constructions vary performance sparse levels left well increasing levels connectedness right. cases structured regular fibonacci constructions outperforms fully connected counterpart indicating better ﬁtted model result reducing parameter space. reason might fully connected networks worse handling noise redundant connections. also subsequent section discuss algebraic connectivity could possible predict result. overall fibonacci seems best choice structure. indeed pseudo regular pseudo random regular good second choice since provides equivalently good results cases simpler implementation. previously discussed ﬁtting issue neural networks dropout methodology extensively used prevent problem. experimented sparsifying methods. augmented methodology architecture beginning methodology dropping ﬁxed number vertices learning batch avoid ﬁtting. dropout=. input layer dropout=. hidden layer show methods independent figure sparse fibonacci regular networks still better accuracy fully connected equivalent. neural networks rarely hidden layer becoming deeper increase computing power studied hidden layers. setting structured regular fibonacci still performs better fully connected network equivalent. figure also present results network size degrees hidden layers varying pace last layer fully connected. case regular fibonacci sparse neural network still perform better fully connected equivalent. connections network able increase accuracy number training epochs. training need measure causes differences between topologies graphical properties related resulting accuracy. critical measures identify spectral algebraic connectivity deﬁned ﬁrst second largest non-zero eigen value laplacian matrix respectively. connectivity varies constructions substantially particularly sparse regimes seen below. hand hand observation connectivity highly correlated accuracy. determining design topology connectivity critical parameter consider constructing also interesting look learned weights vary sparsity. construction doesnt particularly large impact magnitude distribution learned weights sparsity does. connections tempered distributions become ﬁnally leveling equilibrium reﬂects pattern overall accuracy follows. maximum minimum standard deviation connected weights lower respect shown constructions competitive sometimes outperform fully connected counterparts also shorter training time. outperform fully connected equivalent case structured regular fibonacci medium sparcity large hidden layers. overall fibonacci different regular accurate sparse setting regular also provide much simpler implementation. like properties matrix algebraic connectivity. much future work remains expand ﬁndings paper applying techniques neural network architectures paper opens orthogonal direction study promises interesting advances neural networks understanding adds tools neural network tool kit. could eventually employed better models provide tools resources constrained applications connected mobile devices.", "year": 2017}