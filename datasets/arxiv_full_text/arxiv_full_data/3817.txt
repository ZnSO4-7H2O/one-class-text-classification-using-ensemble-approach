{"title": "Semi-supervised Question Retrieval with Gated Convolutions", "tag": ["cs.CL", "cs.NE"], "abstract": "Question answering forums are rapidly growing in size with no effective automated ability to refer to and reuse answers already available for previous posted questions. In this paper, we develop a methodology for finding semantically related questions. The task is difficult since 1) key pieces of information are often buried in extraneous details in the question body and 2) available annotations on similar questions are scarce and fragmented. We design a recurrent and convolutional model (gated convolution) to effectively map questions to their semantic representations. The models are pre-trained within an encoder-decoder framework (from body to title) on the basis of the entire raw corpus, and fine-tuned discriminatively from limited annotations. Our evaluation demonstrates that our model yields substantial gains over a standard IR baseline and various neural network architectures (including CNNs, LSTMs and GRUs).", "text": "title boot ubuntu usb? body bought compaq windows months want install ubuntu still keep windows tried webi restarts read error know windows thing letting ubuntu still want without actually losing data title want install ubuntu laptop i’ll erase data. alonge side windows doesnt appear body want install ubuntu drive. says erase data want install along side windows install alongside windows option doesn’t appear. appear existing answers reducing response times unnecessary repeated work. unfortunately forums process identifying referring existing similar questions done manually forum participants limited scattered success. task automatically retrieving similar questions given user’s question recently attracted signiﬁcant attention become testbed various representation learning approaches however task proven quite challenging instance santos report classiﬁcation accuracy yielding percent gain simple word matching baseline. several factors make problem difﬁcult. first submitted questions often long contain extraneous information irrelevant main question asked. instance ﬁrst question figure pertains booting ubuntu using stick. large portion body contains tangential dequestion answering forums rapidly growing size effective automated ability refer reuse answers already available previous posted questions. paper develop methodology ﬁnding semantically related questions. task difﬁcult since pieces information often buried extraneous details question body available annotations similar questions scarce fragmented. design recurrent convolutional model effectively questions semantic representations. models pre-trained within encoder-decoder framework basis entire corpus ﬁne-tuned discriminatively limited annotations. evaluation demonstrates model yields substantial gains standard baseline various neural network architectures question answering forums stack exchange rapidly expanding already contain millions questions. expanding scope coverage forums often leads many duplicate interrelated questions resulting questions answered multiple times. identifying similar questions potentially reuse tails idiosyncratic user references compaq webi error message. surprisingly features repeated second question figure closely related topic. extraneous detail easily confuse simple word-matching algorithms. indeed reason existing methods question retrieval restrict attention question title only. titles succinctly summarize intent also sometimes lack crucial detail available question body. example title second question refer installation drive. second challenge arises noisy annotations. indeed pairs questions marked similar forum participants largely incomplete. manual inspection sample questions askubuntu shows similar pairs annotated users precision around paper design neural network model associated training paradigm address challenges. high level model used encoder title body combination vector representation. resulting question vector representation compared questions cosine similarity. introduce several departures typical architectures ﬁner level. particular incorporate adaptive gating non-consecutive cnns order focus temporal averaging models pieces questions. gating plays similar role lstms though lstms reach level performance setting. moreover counter scattered annotations available user-driven associations training model largely based entire unannotated corpus. encoder coupled decoder trained reproduce title noisy question body. methodology reminiscent recent encoder-decoder networks machine translation document summarization resulting encoder subsequently ﬁne-tuned discriminatively basis limited annotations yielding additional performance boost. evaluate model askubuntu corpus stack exchange used prior work training directly utilize noisy pairs readily available forum realistic evaluation system performance manually annotate pairs questions. clean data used splits development hyper parameter tuning another testing. evaluate model baselines using standard information retrieval measures mean average precision mean reciprocal rank precision full model achieves yielding absolute improvement standard baseline standard neural network architectures given growing popularity community forums question retrieval emerged important area research previous work question retrieval modeled task using machine translation topic modeling knowledge graph-based approaches recent work relies representation learning beyond word-based methods. instance zhou learn word embeddings using category-based metadata information questions. deﬁne question distribution generates word independently subsequently fisher kernel assess question similarities. santos propose approach combines convolutional neural network bagof-words representation comparing questions. contrast model treats question word sequence opposed words apply recurrent convolutional model opposed traditional model used santos questions meaning representations. further propose training paradigm utilizes entire corpus unannotated questions semi-supervised manner. recent work answer selection community forums similar task question retrieval also involved neural network architectures compared work approaches focus improving various aspects model. instance feng explore different similarity measures beyond cosine similarity adopt neural attention mechanism rnns generate better answer representations given questions context. begin introducing basic discriminative setting retrieving similar questions. query question generally consists title sentence body section. efﬁciency reasons compare queries data base. instead retrieve ﬁrst smaller candidate related questions using standard engine apply sophisticated models reduced set. goal rank candidate questions similar questions ranked dissimilar ones. deﬁne similarity score parameters similarity measures closely candidate related question method comparison make title body question. question similar question negative questions deemed similar training correct pairs similar questions obtained available user-marked pairs negative drawn randomly entire corpus idea likelihood positive match small given size corpus. candidate training testing candidate sets retrieved engine evaluate explicit manual annotations. problems remain. first deﬁne parameterize scoring function design recurrent neural network model purpose encoder question meaning representation. resulting similarity function cosine similarity corresponding representations shown figure parameters pertain neural network only. second order offset scarcity limited coverage training annotations pre-train parameters basis much larger unannotated corpus. resulting parameters subsequently ﬁne-tuned using discriminative setup described above. included much larger weights. achieve effect introduce neural gates similar lstms specify average observed signals. resulting architecture integrates recurrent networks non-consecutive convolutional models sigmoid function represents element-wise product. accumulator vectors store weighted averages -gram n-gram features. gate model represents traditional ﬁlter width however becomes exponential number terms enumerating possible n-grams within x··· note gate parametrized responds directly previous state token question. refer model rcnn pooling order model part discriminative question retrieval framework outlined earlier must condense state sequence single vector. simple alternative pooling strategies explored either averaging states simply taking last meaning representation. addition apply encoder question title body ﬁnal representation computed average resulting vectors. aggregation speciﬁed parameters gate ﬁlter matrices learned purely discriminative fashion. given available annotations limited user-guided instead discriminative training tuning already trained model. method pretraining model basis entire corpus questions discussed next. vector representation. approach inspired temporal convolutional neural networks particular recent reﬁnement tailored capture longerrange non-consecutive patterns weighted manner. models used effectively summarize occurrences patterns text aggregate vector representation. however summary produced selective since pattern occurrences counted weighted cohesive are. problem question body tends long full irrelevant words fragments. thus believe interpreting question body requires selective approach pattern extraction. model successively reads tokens question title body denoted {xi}l transforms sequence sequence states {hi}l resulting state sequence subsequently aggregated single ﬁnal vector representation text discussed below. approach builds thus begin brieﬂy outlining denote ﬁlter matrices pattern size generate sequence states response tokens according represents bigram pattern accumulates range patterns constant decay factor used down-weight patterns longer spans. operations cast recurrent manner evaluated dynamic programming. problem approach purposes however weighting factor triggered state observed token adaptive gated decay reﬁne model learning context dependent weights. example current input token provides relevant information model ignore incorporating token vanishing weight. contrast strong semantic content words ubuntu windows pre-training using entire corpus number questions askubuntu corpus exceeds user annotations pairs similar questions. make larger corpus different ways. first since models take word embeddings input tailor embeddings speciﬁc vocabulary expressions corpus. wordvec corpus addition wikipedia dump. second importantly individual questions training examples auto-encoder constructed pairing encoder model corresponding decoder illustrated figure resulting encoder-decoder architecture akin used machine translation summarization encoder-decoder pair represents conditional language model context original title itself question body title/body similar question. possible pairs used during training optimize likelihood words titles. question title target reasons. question body contains information title also many irrelevant details. result view title distilled summary noisy body encoder-decoder model trained denoising auto-encoder. moreover training decoder title also much faster since titles tend short lstms lstm cells used capture semantic information across wide range applications including machine translation entailment recognition success attributed neural gates adaptively read discard information to/from internal memory states. input forget output gates respectively. given visible state sequence {hi}l aggregate single vector exactly rcnns. lstm encoder pre-trained similar rcnn model. instance recently adopted pre-training text classiﬁcation task. comparison also train three alternative benchmark encoders mapping questions vector representations. lstm gru-based encoders pre-trained analogously rcnns ﬁne-tuned discriminatively. encoders hand trained discriminatively. plausible neither alternative reaches quite level performance unique questions length title length body unique questions user-marked pairs query questions annotated pairs positive pairs query query questions annotated pairs positive pairs query associated ﬁlters local chunks input feature representation. concretely denote ﬁlter width w··· corresponding ﬁlter matrices convolution operation applied window consecutive words follows sets output state vectors {ht} produced case typically referred feature maps. since vector feature pertains local information last vector sufﬁcient capture meaning entire sequence. instead consider max-pooling average-pooling obtain aggregate representation entire sequence. dataset stack exchange askubuntu dataset used prior work dataset contains unique questions consisting title body user-marked similar question pairs. provide various statistics dataset table gold standard evaluation user-marked similar question pairs sites often known incomplete. order evaluate dataset took sample questions paired candidate questions retrieved search engine trained askubuntu data. search engine used well-known model manual evaluation candidates showed similar questions marked users precision clearly recall would lead realistic evaluation used user marks gold standard. instead make expert annotations carried subset questions. training user-marked similar pairs positive pairs training since annotations high precision require additional manual annotations. allows much larger training set. random questions corpus paired query question negative pairs training. randomly sample questions negative examples epoch. development test sets re-constructed test sets consisting ﬁrst questions test sets provided santos questions retrieved similar candidates using manually annotated resulting pairs similar non-similar. annotation task initially carried expert annotators independently. initial reﬁned comparing annotations asking third judge make ﬁnal decision disagreements. consensus annotation guidelines reached overall annotation carried expert. table comparative results methods question similarity task. higher numbers better. neural network models show best average performance across independent runs corresponding pooling strategy. statistical signiﬁcance types model marked hyper-parameters performed extensive hyper-parameter search identify best model baselines neural network models. tf-idf baseline tried n-gram feature order without stop words pruning. baseline used default svmlight parameters whereas data used increase training size testing test set. also tried give higher weight instances result improvement. neural network models used adam optimization method default setting suggested authors. optimized hyper-parameters following range values learning rate dropout probability feature width also tuned pooling strategies ensured model comparable number parameters. default conﬁgurations lstms grus cnns rcnns shown table used identify best training epoch model conﬁguration. model conﬁguration report average performance across word vectors wordvec obtain -dimensional word embeddings using stack exchange data large wikipedia corpus. word vectors ﬁxed avoid over-ﬁtting across experiments. overall performance table shows performance baselines neural encoder models question retrieval results show full model rcnns pre-training achieves best performance across metrics test sets. instance full model gets test outperforming word matching-based method percent points. further rcnn model also outperforms neural encoder models baselines across metrics. superior performance indicates nonconsecutive ﬁlters varying decay effective improving traditional neural network models. table also demonstrates performance gain pre-training rcnn encoder. rcnn model pre-trained entire corpus consistently gets better results across metrics. cnns max-pooling cnns mean-pooling lstms pre-train mean-pooling lstms pre-train last state grus pre-train mean-pooling grus pre-train last state rcnns pre-train mean-pooling rcnns pre-train last state pooling strategy analyze effect various pooling strategies neural network encoders. shown table rcnn model outperforms neural models regardless pooling strategies explored. also observe simply using last hidden state ﬁnal representation achieves better results rcnn model. using question body table compares performance tf-idf baseline rcnn model using question titles using question titles along question bodies. tf-idf’s performance changes little question bodies included however inclusion question bodies improves performance rcnn model achieving improvement model variations. rcnn model’s greater improvement illustrates ability model pick components pertain directly question asked long descriptive question bodies. pre-training note that pre-training last hidden states generated neural encoder used decoder reproduce question titles. would interesting states capture meaning questions. evaluate using last hidden states question titles. also test encoder captures information question bodies produce distilled summary i.e. titles. evaluate perplexity trained encoderdecoder model heldout corpus contains questions. shown figure representations generated rcnn encoder perform quite well resulting perplexity without subsequent ﬁne-tuning. interestingly lstm networks obtain similar perplexity heldout achieve much worse similar question retrieval. instance encoder obtains worse rcnn model’s performance. result lstm encoder beneﬁt clearly pre-training suggested table inconsistent performance difference explained hypotheses. perplexity suitable measuring similarity encoded text thus power encoder illustrated terms perplexity. another hyfigure visualizations model several question pieces data set. scalar value make visualization simple. corresponding model simpliﬁed variant worse full model. adaptive decay finally analyze gated convolution model. figure demonstrates word position much input information taken model adaptive weights average weights vector decreases increments suggesting information encoded state vector saturates input processed. hand largest value weight vector remains high throughout input indicating least information stored also conduct case study analyzing neural gate. since directly inspecting dimensional decay vector difﬁcult train model uses scalar decay instead. shown figure model learns assign higher weights application names quoted error messages intuitively important pieces question askubuntu domain. task question retrieval community forums. architecture enables model glean pieces information lengthy detail-riddled user questions. pre-training within encoder-decoder framework basis entire corpus integral model’s success. thank zhang yoon danqi chen group reviewers helpful comments. work developed collaboration arabic language technologies group qatar computing research institute within iyas project. opinions ﬁndings conclusions recommendations expressed paper authors necessarily reﬂect views funding organizations. references dzmitry bahdanau kyunghyun yoshua bengio. neural machine translation jointly learning align translate. international conference learning representations. daniel b¨ar torsten zesch iryna gurevych. dkpro similarity open source framework text similarity. proceedings annual meeting association computational linguistics system demonstrations pages soﬁa bulgaria august. association computational linguistics. samuel bowman gabor angeli christopher potts christopher manning. large annotated corpus learning natural language inference. proceedings conference empirical methods natural language processing association computational linguistics. kyunghyun bart merri¨enboer dzmitry bahdanau yoshua bengio. properties neural machine translation encoder-decoder approaches. arxiv preprint arxiv.. kyunghyun bart merri¨enboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. proceedings conference empirical methods natural language processing junyoung chung caglar gulcehre kyunghyun yoshua bengio. empirical evaluation gated recurrent neural networks sequence modeling. arxiv preprint arxiv.. cicero santos luciano barbosa dasha bogdanova bianca zadrozny. learning hybrid representations retrieve semantically equivalent questions. proceedings annual meeting association computational linguistics international joint conference natural language processing pages beijing china july. association computational linguistics. geoffrey hinton nitish srivastava alex krizhevsky ilya sutskever ruslan salakhutdinov. improving neural networks preventing coarxiv preprint adaptation arxiv.. jiwoon jeon bruce croft joon lee. finding similar questions large question answer proceedings internaarchives. tional conference information knowledge management pages acm. kalchbrenner edward grefenstette phil blunsom. convolutional neural network modproceedings annual elling sentences. meeting association computational linguistics. yoon yacine jernite david sontag alexander rush. character-aware neural language models. twenty-ninth aaai conference artiﬁcial intelligence. regina barzilay tommi jaakkola. molding cnns text non-linear non-consecutive convolutions. proceedings conference empirical methods natural language processing pages lisbon portugal september. association computational linguistics. shuguang suresh manandhar. improving question recommendation exploiting information need. proceedings annual meeting association computational linguistics human language technologies-volume pages association computational linguistics. guangyou zhou tingting zhao learning continuous word embedding metadata question retrieval community question answering. proceedings annual meeting association computational linguistics international joint conference natural language processing pages beijing china july. association computational linguistics. preslav nakov llu´ıs m`arquez walid magdy alessandro moschitti glass bilal randeree. semeval- task answer selection commuproceedings nity question answering. international workshop semantic evaluation semeval preslav nakov llu´ıs m`arquez walid magdy alessandro moschitti glass bilal randeree. semeval- task community question answerproceedings international working. shop semantic evaluation semeval rockt¨aschel edward grefenstette karl moritz hermann tom´aˇs koˇcisk`y phil blunsom. reasoning entailment neural attention. international conference learning representations. alexander rush sumit chopra jason weston. neural attention model abstractive sentence summarization. proceedings conference empirical methods natural language processing. yikang shen wenge rong jiang baolin peng tang zhang xiong. word embedding based correlation model question/answer matching. arxiv preprint arxiv.. guangyou zhou yang fang daojian zeng zhao. improving question retrieval community question answering using world knowlproceedings twenty-third internaedge. tional joint conference artiﬁcial intelligence pages aaai press. re-processed dataset updated results accordingly. performance various neural network models reported version match numbers reported early version exactly relative observation change. snapshot askubuntu question. possible duplicate section shows title similar question marked forum user. section included original dump askubuntu removed early experiments.", "year": 2015}