{"title": "Efficient Multi-task Feature and Relationship Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "In this paper we propose a multi-convex framework for multi-task learning that improves predictions by learning relationships both between tasks and between features. Our framework is a generalization of related methods in multi-task learning, that either learn task relationships, or feature relationships, but not both. We start with a hierarchical Bayesian model, and use the empirical Bayes method to transform the underlying inference problem into a multi-convex optimization problem. We propose a coordinate-wise minimization algorithm that has a closed form solution for each block subproblem. Naively these solutions would be expensive to compute, but by using the theory of doubly stochastic matrices, we are able to reduce the underlying matrix optimization subproblem into a minimum weight perfect matching problem on a complete bipartite graph, and solve it analytically and efficiently. To solve the weight learning subproblem, we propose three different strategies, including a gradient descent method with linear convergence guarantee when the instances are not shared by multiple tasks, and a numerical solution based on Sylvester equation when instances are shared. We demonstrate the efficiency of our method on both synthetic datasets and real-world datasets. Experiments show that the proposed optimization method is orders of magnitude faster than an off-the-shelf projected gradient method, and our model is able to exploit the correlation structures among multiple tasks and features.", "text": "paper propose multi-convex framework multi-task learning improves predictions learning relationships tasks features. framework generalization related methods multi-task learning either learn task relationships feature relationships both. start hierarchical bayesian model empirical bayes method transform underlying inference problem multi-convex optimization problem. propose coordinate-wise minimization algorithm closed form solution block subproblem. naively solutions would expensive compute using theory doubly stochastic matrices able reduce underlying matrix optimization subproblem minimum weight perfect matching problem complete bipartite graph solve analytically efﬁciently. solve weight learning subproblem propose three different strategies including gradient descent method linear convergence guarantee instances shared multiple tasks numerical solution based sylvester equation instances shared. demonstrate efﬁciency method synthetic datasets real-world datasets. experiments show proposed optimization method orders magnitude faster off-the-shelf projected gradient method model able exploit correlation structures among multiple tasks features. multi-task learning received considerable interest past decades underlying assumptions behind many multi-task learning algorithms tasks related other. hence question deﬁne notion task relatedness capture learning formulation. common assumption tasks described weight vectors either ﬁnite dimensional space reproducing kernel hilbert space sampled shared prior distribution space another strand work assumes common feature representations shared among multiple tasks goal learn shared representation well task-speciﬁc parameters simultaneously moreover structure multiple tasks available e.g. task-speciﬁc descriptors task similarity graph regularizers often incorporated learning formulation explicitly penalize hypotheses consistent given structure. paper follow ﬁrst line work propose multi-convex framework multi-task learning. method improves predictions tabula rasa learning assuming task vectors sampled common shared prior. several attempts improve predictions along direction either learning relationships different tasks known multi-task relationship learning exploiting relationships different features known multi-task feature learning zhang schneider proposed multi-task learning framework task feature relationships inferred data assuming sparse matrix-normal penalty task feature representations. paper multi-task learning framework generalization mtrl mtfl learns relationships tasks features simultaneously. property favorable applications better generalization also seek clear understanding relationships among different tasks. compared sparse regularization approach zhang schneider framework free sparsity assumption result admits efﬁcient optimization scheme able derive analytic solutions subproblem whereas iterative methods used subproblem. term proposed framework feature task relationship learning main contributions summarized follows first formulate fetr hierarchical bayes model carefully chosen prior imposed task vectors order capture relatedness tasks features time. model free sparsity assumption usually made literature make algorithm scalable. apply empirical bayes method approximate exact posterior distribution leading learning formulation goal optimize model parameters i.e. task vectors well covariance matrices prior. second transform optimization problem fetr multi-convex structure design alternating direction algorithm using block coordinate-wise minimization solve problem efﬁciently. speciﬁcally achieve reducing underlying matrix optimization problem positive deﬁnite constraints minimum weight perfect matching problem complete bipartite graph able solve analytically using combinatorial techniques. solve weight learning subproblem propose three different strategies including closed form solution gradient descent method linear convergence guarantee instances shared multiple tasks numerical solution based sylvester equation instances shared. next demonstrate efﬁciency proposed optimization algorithm comparing off-the-shelf projected gradient descent algorithm synthetic data. experiments show proposed optimization method orders magnitude faster competitor often converges better local solution. lastly test statistical performance fetr modeling real-world related tasks comparing single task learning well mtfl mtrl. results show fetr able give better predictions also effectively exploit correlation structures among multiple tasks. also study fetr relationship existing multitask learning algorithms including mtfl mtrl general regularization framework showing viewed special cases fetr. notation setup denote m-dimensional symmetric positive semideﬁnite cone mdimensional symmetric positive deﬁnite cone respectively. write trace matrix finally denote weighted bipartite graph vertex sets edge weight function denotes consider following setup. suppose given learning tasks {ti}m learning task access training data instances focus supervised learning setting regression problem binary classiﬁcation problem. predictor/model parameter loss function task ease discussion follows assume model task approach also translated classiﬁcation linear regression i.e. problem e.g. logistic regression linear etc. refer interested readers appendix proofs claims theorems paper. multivariate normal distribution mean covariance matrix rd×m model parameter different tasks. applying hierarchical bayes approach specify prior distribution model parameter speciﬁcally deﬁne prior distribution denotes matrix-variate normal distribution mean rd×m covariance matrix later ﬁrst term interpreted regularizer penalize model complexity encodes structure task vectors intuitively imposing structure covariance column covariance matrices incorporate prior knowledge correlation among features well relationship among different tasks. worth pointing specify forms prior distributions instead authors laplacian prior generalized process respectively. shortly advantage probability density function covariance matrices compute exact posterior distribution follows computationally intractable case since analytic form posterior distribution. instead computing integration exactly take empirical bayes approach approximate intractable integration much changed replacing prior distribution point estimate representing peak distribution. substituting omitting constant terms depend maximize approximate posterior distribution optimizing leading following optimization scheme worth pointing optimal value achieved since constraint open. fact always decrease objective function setting make eigenvalues inﬁnitely close strictly greater case value approach hence lower bound. technical issue next section imposing boundedness constraints simplicity later discussion assume discussing details efﬁciently solve optimization program inspect equation discuss several special cases. optimization problem decomposed independent single task learning problems task corresponds ridge regression problem. generally block diagonal designs covariance matrices exhibit group/clustering properties features tasks. hand i.e. single task learning setting manually optimization problem reduces linear regression weighted linear smoother smoother speciﬁed optimal solution obtained closed form. case plays role stabilizer reweights relative importance different features. similarly models correlation different tasks relative strength. although empirical bayes framework appealing optimization posed easy solve directly. section ﬁrst show converted multi-convex optimization problem. multi-convex function generalization bi-convex function multiple variables blocks variables. multi-convex formulation hard optimization problem convex since |ω|+d concave function also unbounded below analyzed last section. handle technical issues introduce boundedness constraint constraint concretely instead constraining make constants. understand constraint putting implicit uniform prior distribution speciﬁed constraint. technically boundedness constraint make feasible sets compact hence extreme value theorem minimum guaranteed achieved since objective function continuous. next apply well known transformation optimization problem multi-convex terms transformed variables. deﬁne well-deﬁned constrained positive deﬁnite matrices. transformed optimization formulation based based multi-convex formulation developed last section propose alternating direction algorithm using block coordinate-wise minimization optimize objective given iteration alternatively minimize ﬁxed minimize ﬁxed lastly minimize ﬁxed. whole procedure repeated stationary point found decrease objective function less pre-speciﬁed threshold. follows assume ni∀i simplify notation. rn×m labeling matrix rn×d feature matrix shared tasks. using notation objective function equivalently expressed matrix form shown last section unconstrained convex optimization problem. present three different algorithms optimal solution subproblem. ﬁrst guarantees exact solution closed form time using isomorphism rd×m rdm. second gradient descent ﬁxed step size iteratively reﬁne solution show case linear convergence speed guaranteed. third ﬁnds optimal solution solving sylvester equation characterized ﬁrst-order optimality condition proper transformation. closed form solution. worth noting obvious obtain closed form solution directly formulation application ﬁrst order optimality condition lead following equation obtained simply reformatting matrix. computational bottleneck procedure solving system equations scales sparsity structure available. overall computational complexity gradient descent. closed form solution shown scales cubically requires explicitly form matrix size intractable even moderate cases instead computing exact solution gradient descent ﬁxed step size obtain approximate solution. objective function differentiable gradient obtained time follows note compute advance time cache need recompute gradient update step. largest eigenvalue real symmetric matrix provide linear convergence guarantee gradient method thm. proof technique adapted nesterov extend matrix function. theorem choose gradient descent step size converges optimal solution within steps. remark. computational complexity achieve approximate solution using gradient descent compared complexity exact solution gradient descent algorithm scales much better provided i.e. condition number λu/λl large. side note condition number large effectively reduce using conjugate gradient method sylvester equation. ﬁeld control theory sylvester equation matrix equation form goal solution matrix given problem efﬁcient numerical algorithms highly optimized implementations obtain solution within cubic time. example bartelsstewart algorithm solves sylvester equation ﬁrst transforming schur forms factorization solves resulting triangular system back-substitution. third approach based observation equivalently transform ﬁrst-order optimality equation given sylvester equation multiplying sides equation ﬁnding optimal solution subproblem amounts solving sylvester equation. speciﬁcally solution equation obtained using bartels-stewart algorithm within gradient descent bartels-stewart algorithm optimal solution cubic time. however gradient descent algorithm widely applicable bartels-stewart algorithm bartels-stewart algorithm applies case tasks share instances write matrix equation explicitly gradient descent applied case task different number inputs inputs shared among tasks. hand shortly experiments practice bartelsstewart algorithm faster gradient descent provides numerically stable solution. delve detailed analysis below ﬁrst list ﬁnal algorithms used optimize alg. alg. respectively. remarkably simple algorithm involves truncation matrix multiplications. computational complexity alg. bounded although convex optimization problem computationally expensive solve using offthe-shelf algorithms constraints well nonlinearity objective function. surprisingly closed form optimal solution problem well using tools theory doubly stochastic matrices perfect bipartite graph matching. without loss generality reparametrize eigenvectors corresponds eigenvalues increasing order rather decreasing order reasons become clear below. using representation realizing orthonormal matrix d-dimensional vector ones. ﬁrst glance seems form optimization complicated solve since even convex problem quadratic equality constraint. however shortly form helps decouple interaction inﬂuence ﬁrst term |λ|. implies ﬁrst partially optimize ﬁnding optimal solution function optimize mathematically means theorem consider minimization linear programming problem polyhedron suppose least extreme point exists optimal solution. exists optimal solution extreme point deﬁnition birkhoff polytope doubly stochastic matrices. convex polytope. given exists optimal solution permutation matrix reduce minimum-weight perfect matching problem complete bipartite graph. problem minimum-weight perfect matching deﬁned follows. deﬁnition undirected graph edge weight matching edges vertex common. matching called perfect every vertex occurs endpoint matching called minimum-weight perfect matching perfect matching minimum weight among perfect matchings construct vertex construct vertex pair construct edge edge wight theorem minimum value equal minimum weight perfect matching furthermore optimal solution constructed minimum-weight perfect matching even need standard graph matching algorithms solve matching problem. instead thm. gives closed form solution. theorem minimum-weight perfect matching minimum permutation matrix achieves minimum weight since note follows optimal also hence solve optimal matrix solving equation leads plug optimal optimization w.r.t. decomposes independent optimization problems simple scalar optimization problem depending whether value m/νi within range optimal solution scalar minimization problem take different forms. deﬁne soft-thresholding operator follows section analyze statistical performance fetr well efﬁciency proposed coordinate minimization algorithm solving underlying multi-convex optimization problem. ﬁrst investigate efﬁciency scalability three different algorithms minimizing w.r.t. synthetic data sets. experiment generate synthetic data consists instances shared among tasks. instances randomly sampled uniformly gradually increase dimension features number tasks test scalability. ﬁrst algorithm implements closed form solution explicitly computing tensor product matrix solving linear system. second proposed gradient descent whose stop condition either norm gradient less algorithm looped iterations. last uses bartels-stewart algorithm solve equivalent sylvester equation compute open source toolkit scipy whose backend implementation uses highly optimized fortran code. synthetic experiments corresponds condition number coefﬁcients since affect convergence speed. experimental results shown fig. expected closed form solution scale problems even moderate size large memory requirement. practice bartels-stewart algorithm order magnitude faster gradient descent method either large also worth pointing bartels-stewart algorithm numerically stable algorithm among three based observations. hence following experiments bartels-stewart solver applied case cannot applied gradient descent method. compare proposed coordinate minimization algorithm off-the-shelf projected gradient method solve optimization problem speciﬁcally projected gradient method updates based gradient direction iteration projects corresponding feasible regions. experiment number instances dimension feature vectors number tasks instances shared among tasks sylvester solver used optimize coordinate minimization. repeat experiments times report function values versus time used algorithms; fig. desktop twelve cores conduct experiment. clear synthetic experiment proposed algorithm converges much faster projected gradient descent also achieves better results. proceed experiments real-world data sets generate data conduct synthetic experiment show proposed model indeed learn relationships task vectors feature vectors simultaneously. randomly generate data points uniformly range consider following three regression tasks problem ﬁrst corresponds weight vector feature across tasks second corresponds weight vector column represents task vector. hence expect correlation also expect correlation ﬁrst third task correlations pairs tasks apply fetr problem setting algorithm converges estimated regression functions following feature task correlation matrices synthetic experiment shows learned feature correlation matrix task correlation matrix conform expectation demonstrating effectiveness fetr synthetic problem. evaluate fetr multi-task learning algorithms inverse dynamics problem seven degree-of-freedom sarcos anthropomorphic robot arm. goal task -dimensional input space corresponding joint torques. hence tasks inputs shared among tasks. training test contain examples respectively. partition training development validation contain instances. validation model selection. speciﬁcally train multiple models development different conﬁgurations hyperparameters ranging model select best mean-squared error validation set. experiments observe results sensitive hyperparameters. compare fetr multi-task feature learning multitask relationship learning treated different special cases model. controlled fair comparison methods implement mtfl mtrl experimental setting including model selection fetr. task ridge regression baseline model denote single task learning method best model validation used prediction report score test task. smaller score better predictive result. results summarized table expected size training data large multi-task learning methods perform least well baseline method. among three multi-task learning methods fetr achieves lowest test mean square error. fetr learn covariance matrices features tasks simultaneously methods estimate them. illustrate this show covariance matrices estimated mtfl mtrl fetr appendix. also test performance four methods size training increases. order summarize overall performance score better visualization total normalized mean squared error seven tasks original test data mean squared error divided variance ground truth. helps illustrate advantages multi-task learning methods single task learning data gets larger. training size repeat experiments times randomly sampling subset corresponding size whole training set. depict mean nmse four methods well standard deviation fig. also plot covariance matrices estimated mtfl mtrl fetr compare correlation structures found algorithms; fig. worth mentioning fetr learn correlation matrices also detects correlations features mtfl. example task correlation matrix learned fetr exhibits block diagonal structure meaning weight vectors ﬁrst tasks roughly uncorrelated. pattern shown task correlation matrix learned mtrl. hand fetr detects correlations among features mtfl shown fig. multi-task learning area active research machine learning received attention past years research multi-task learning carried several strands. early work focuses sharing knowledge separate tasks sharing hidden representations example hidden layer units neural networks anstream works constructs gaussian process prior regression functions different tasks transfer knowledge tasks enforced structure covariance functions/kernels gaussian process prior. bonilla presents kernel multi-task learning approach gaussian process prior task-speciﬁc features assumed available. concatenate feature input instance task feature deﬁne kernel covariance function regression functions among different tasks. learning reduced inferring hyperparameters kernel function maximizing marginal log-likelihood instances either gradient ascent. approach essentially decomposes joint kernel function regression functions separate kernels measures similarity among tasks measures similarity among instances. closely related approach strand consider nonparametric kernel matrix tasks rather parametrized kernel function however kernel input instances still restricted parametrized order avoid expensive computation semideﬁnite programming input features. contrast fetr optimizes feature task covariance matrices directly without assumption parametrized forms. argyriou develop mtfl mtrl viewed convex frameworks learning feature task-relatedness covariance matrices respectively. mtfl mtrl essentially convex regularization methods exploit feature task relatedness imposing trace constraints proper transformations. covariance matrices mtfl mtrl restricted speciﬁc parametrized forms perspective understood special cases mtfl mtrl covariance matrices constrained graph laplacians. however mtfl feature covariance matrix optimized task-relatedness covariance matrix assumed identity mtrl task-relatedness covariance matrix optimized feature covariance matrix again assumed identity. take inspiration previous works propose general regularization framework fetr models optimizes task feature relationships directly. worth emphasizing mtfl mtrl treated special cases fetr covariance matrices assumed identity corresponds left/right spherical matrix normal distributions. objective function fetr convex multi-convex. efﬁcient coordinate-wise minimization algorithm closed form solutions subproblem designed tackle multi-convex objective fetr. perhaps related work fetr zhang schneider work approach consider multi-task learning framework task feature covariance matrices inferred data. however authors make sparsity assumption covariance matrices approach general sense free assumption. secondly importantly work optimization w.r.t covariance matrices employs iterative method approximate optimal solution obtain efﬁcient non-iterative solution reduction graph matching problem. note order optimize covariance matrix using method enough zhang iteration requires svd. makes approach computationally efﬁcient used zhang schneider statistically approach makes less assumption framework proposed zhang schneider develop multi-convex framework multi-task learning improves predictions learning relationships tasks features. framework generalization related approaches multi-task learning either learn task relationships feature relationships. develop multi-convex formulation problem well algorithm block coordinatewise minimization. using theory doubly stochastic matrices able reduce underlying matrix optimization subproblem minimum weight perfect matching problem complete bipartite graph solve closed form. method orders magnitude faster off-the-shelf projected gradient descent method shows improved performance synthetic datasets well real-world dataset robotic modeling. current paper discusses approach context linear regression extended types prediction tasks logistic regression linear etc.", "year": 2017}