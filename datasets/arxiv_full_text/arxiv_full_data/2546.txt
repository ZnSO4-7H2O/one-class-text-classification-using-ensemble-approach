{"title": "Probably Approximately Correct Greedy Maximization", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Submodular function maximization finds application in a variety of real-world decision-making problems. However, most existing methods, based on greedy maximization, assume it is computationally feasible to evaluate F, the function being maximized. Unfortunately, in many realistic settings F is too expensive to evaluate exactly even once. We present probably approximately correct greedy maximization, which requires access only to cheap anytime confidence bounds on F and uses them to prune elements. We show that, with high probability, our method returns an approximately optimal set. We propose novel, cheap confidence bounds for conditional entropy, which appears in many common choices of F and for which it is difficult to find unbiased or bounded estimates. Finally, results on a real-world dataset from a multi-camera tracking system in a shopping mall demonstrate that our approach performs comparably to existing methods, but at a fraction of the computational cost.", "text": "submodular function maximization ﬁnds application variety real-world decision-making problems. however existing methods based greedy maximization assume computationally feasible evaluate function maximized. unfortunately many realistic settings expensive evaluate exactly even once. present probably approximately correct greedy maximization requires access cheap anytime conﬁdence bounds uses prune elements. show that high probability method returns approximately optimal set. propose novel cheap conﬁdence bounds conditional entropy appears many common choices difﬁcult unbiased bounded estimates. finally results real-world dataset multi-camera tracking system shopping mall demonstrate approach performs comparably existing methods fraction computational cost. introduction submodularity property functions formalizes notion diminishing returns i.e. adding element increases value function smaller equal amount adding element subset. many realworld problems involve maximizing submodular functions e.g. summarizing text selecting subsets training data classiﬁcation selecting sensors minimize uncertainty hidden variable formally given ground function submodular every maximization ﬁnds approximate solution faster iteratively adding partial solution element maximizes marginal gain. nemhauser showed value obtained greedy maximization close full maximization i.e. submodular non-negative monotone. lazy greedy maximization accelerates greedy maximization pruning elements whose marginal gain last iteration ensures marginal gain current iteration cannot maximal. lazier greedy maximization provides speedup evaluating marginal gain randomly sampled subset elements iteration. variations also minimize number marginal gain computations. however methods assume computationally feasible exactly compute thus marginal gain. many settings case. example consider surveillance task agent aims minimise uncertainty hidden state selecting subset sensors maximise information gain. computing information gain computationally expensive especially hidden variable take many values involves expectation entropy posterior beliefs hidden variable. surveilling large areas like shopping malls exactly computing entropy single posterior belief becomes infeasible alone expectation them. paper present algorithm called probably approximately correct greedy maximization. rather assuming access itself assume access conﬁdence bounds particular assume bounds cheaper compute anytime i.e. tighten spending computation time e.g. generating additional samples. inspired lazy greedy maximization method uses conﬁdence bounds prune elements thereby avoiding need tighten bounds. furthermore provide analysis shows that high probability method returns approximately optimal set. given unbiased estimator possible concentration inequalities like hoeffding’s inequality obtain conﬁdence bounds needed greedy maximization. unfortunately many applications sensor placement decision tree induction require information-theoretic deffurthermore assume anytime i.e. tighten procedure improves bounds exchange computation. speciﬁcally calling tighten expectation reduce increase assumptions easily satisﬁed many settings expensive compute exactly. example random variable xi’s i.i.d. samples unbiased estimator easily constructed using e.g. hoeffding’s inequality furthermore tighten need fold samples however speciﬁcally assume access unbiased estimator instead seek algorithm performs submodular function maximization given tighten. absence unbiased estimator arises many settings deﬁned using information-theoretic metrics information gain entropy. example consider sensor selection problem agent sensors giving information hidden state sensor denotes observation agent receive selects sensor selected. denotes complete observation vector generated sensors. upon selecting sensors observing agent unfortunately many possible states actions computing intractable also difﬁcult efﬁciently estimate fact paninski showed unbiased estimator entropy exists. therefore next section propose fundamentally different method requires tighten. solve sensor selection particular also need cheap anytime implementations conditional entropy propose section initions information gain depend computing entropy posterior beliefs impossible estimate unbiased absence unbiased estimator makes hard obtain computationally cheap conﬁdence bounds conditional entropy therefore paper propose novel cheap conﬁdence bounds conditional entropy. finally apply greedy maximization conﬁdence bounds real-life dataset collected agents controlling multi-camera tracking system employed shopping mall. empirical results demonstrate approach performs comparably greedy lazier greedy maximization fraction computational cost leading much better scalability. background given function greedy maximization computes subset approximates maxa∈a+ shown algorithm repeatedly adding element maximizes marginal gain greedy method much faster naive maximization. algorithm greedy-max nemhauser showed that certain conditions method bounded error. theorem non-negative monotone submodular lazy greedy maximization accelerates greedy maximization pruning elements whose marginal gain cannot maximal maintaining priority queue elements element’s priority marginal gain computed previous iteration. current iteration marginal gain element highest priority higher priority next element current iteration terminated since submodularity guarantees marginal gain remaining elements decrease. lazy greedy maximization computes greedy maximization much faster practice. problem setting paper consider variation submodular function maximization evaluating therefore marginal gain prohibitively expensive rendering greedy lazy greedy maximization inapplicable. instead assume access computationally cheap conﬁdence bounds particular functions probability probability method section propose probably approximately correct greedy maximization enables agent perform submodular function maximization without ever computing exactly. main idea prune elements high probability maximize marginal gain. approach inspired lazy greedy maximization. helpful view lazy greedy maximization pruning method terminating iteration priority queue empty effectively prunes element whose upper bound lower maximum lower bound greedy maximization generalizes idea ways. first accepts arbitrary upper lower bounds. makes possible replace bounds used lazy greedy maximization rely exact computation marginal gain cheaper ones. second uses conﬁdence bounds instead hard bounds. tolerating small probability error approach prune aggressively enabling large speedups maintaining bound. algorithm shows main loop simply adds iteration element selected pac-max subroutine. algorithm shows subroutine maintains queue unpruned elements prioritized upper bound. iteration outer loop pac-max examines elements prunes upper bound least greater lower bound found far. addition element lower bound never pruned. element pruned bounds tightened. algorithm terminates element remains improvement produced tightening falls threshold algorithm closely related best identiﬁcation algorithms multi-armed bandits. main difference directly estimate instead selects element using cases unbiased sample-based estimator available pac-max replaced off-the-shelf best identiﬁcation algorithm uses e.g. hoeffding bounds instead however paper focus setting estimator available thus bandit-based algorithms cannot directly applied. since greedy maximization assume oracle access instead works cheap anytime conﬁdence bounds prove bound greedy maximization. particular prove conditions greedy maximization ﬁnds solution that high probability close since greedy maximization assume oracle access marginal gain fundamentally different previous approaches greedy maximization typically analysed terms number evaluations analysis simply applicable greedy maximization makes queries instead repeatedly calls tighten. ﬁrst reaction might analyze algorithm terms calls argue would prove useful. number calls highly dependent nature tighten means possible quantify number calls without making strong assumptions. example assume that tighten calls converge number tighten calls since worst case need tighten elements true value. however analysis helpful since typically cannot quantify contrary greedy maximization would work much general setting assumptions would permit. fact greedy maximzation still employed even unbiased estimator available common assumption settings. show section conditional entropy unbiased estimator available even deﬁning tighten much less quantifying challenging. section present analysis greedy maximization without making speciﬁc assumptions tighten except following assumption pac-max always terminates ρ.length conditional entropy bounds many settings easily constructed using e.g. hoeffding’s inequality tighten need fold samples estimate however hoeffding’s inequality bounds error estimate expected value estimator. turn bounds error estimate true value estimator unbiased i.e. expected value estimator equals true value. interested settings sensor selection based conditional entropy computed approximating entropy posterior belief cannot estimated unbiased therefore section propose novel cheap conﬁdence bounds conditional entropy. start deﬁning maximum likelihood estimate entropy. given samples discrete distribution maximum likelihood estimator words assume tightened enough disambiguate converge sufﬁcient computation time available assumption easily satisﬁed. section show greedy maximization performs well even assumption violated. prove lemma shows high probability marginal gain element picked pac-max least nearly great average marginal gain elements lemma pac-max assumption holds probability theorem proves greedy maximization assuming access anytime conﬁdence bounds computes high probability bounded error respect greedy maximization requires access cheap upper lower conﬁdence bounds next section propose bounds conditional entropy. state agent wants track evolves time. every time step changes must selected. still requires iterating across values thus lower computational cost further estimates entropy lower bound practice larger value computing critical reducing furthermore negative bias lead intractability choosing small ensures belief updates performed. form many belief updates; computing high perform belief updates. yields cheap upper lower conﬁdence bound conditional entropy. following theorem ties together results presented paper. note that since deﬁned negative conditional entropy deﬁned using upper bound using lower bound. theorem log. assumption holds conditionally independent given then probability k|ω|δη proof. note |φ|δη. since cannot clustered clusters thus |ω|δη |ω|δη k|ω|. showed submodular conditionally independent given thus theorem implies stated result. performing belief update practice approximate using particle belief updates which given generate sample observation added samples approximating tightened increasing number samples used estimate and/or increasing number samples used estimate ˆbza using larger values however tightening practical computing involves posterior belief updates hence increases computational cost tightening upper conﬁdence bound since negatively biased ﬁnding upper conﬁdence bound difﬁcult. insight bound nonetheless obtained estimating posterior entropy using artiﬁcially coarsened observation function. group possible observations clusters pretend that instead observing agent observes cluster since observation contains less information conditional entropy higher yielding upper bound. furthermore since agent reason clusters instead observations also cheaper compute. generic clustering approach e.g. ignoring certain observation features used though cases domain expertise exploited select clustering yields tightest bounds. represent crude approximation every obtained clusters clusters deterministically denotes cluster belongs also note domain share lemma proof. using chain rule entropy since contains additional since conditioning never increase entropy stated result holds. reha quires belief updates instead |ω|. starting small tightened increasing number clusters thus |φ|. requires note computing computed marginalizing computationally expensive operation. marginalization needs done reused performing greedy maximization various occurs naturally e.g. sensor selection hidden related work work submodular function maximization focuses algorithms approximate greedy maximization minimize number evaluations particular randomly sample subset iteration select element subset maximizes marginal gain. badanidiyuru vondr´ak selects element iteration whose marginal gain exceeds certain threshold. proposed methods maximize surrogate submodular functions address streaming distributed settings also assume access exact contrast approach assumes expensive compute even works instead conﬁdence bounds krause guestrin propose approximating conditional entropy submodular function maximization still assuming compute exact posterior entropies. case computing exact posterior entropy prohibitively expensive. streeter golovin radlinski propose conceptually related methods also assume never computed exactly. however online setting fundamentally different system must ﬁrst select entire subset receives estimate well estimates marginal gain elements since system learns time maximize variation multi-armed bandit setting. contrast assume feedback given element’s marginal gain available before committing element. mentioned earlier algorithm closely related best identiﬁcation algorithms however methods assume unbiased estimator available hence concentration inequalities like hoeffding’s inequality applicable. exception work nowozin bounds difference between entropy estimate estimate’s expected value. however since entropy estimator biased yield conﬁdence bounds respect true entropy. propose using bounds best identiﬁcation guarantees provided would hard obtain since bias estimating entropy addressed. however bounds could used place theorem work proposes accurate estimators entropy computationally efﬁcient thus directly useful setting. instead selecting maxi∈x\\ag selects ∆|ag) total error bounded exploit property method conﬁdence bounds introduce probabilistic element high probability evaluated greedy maximization problem tracking multiple people using multi-camera system. problem extracted real-world dataset collected shopping mall. dataset gathered hours using cctv cameras located shopping mall. camera uses fpdw pedestrian detector detect people camera image in-camera tracking generate tracks detected people’s movement time. dataset thus consists trajectories specifying person’s position. evaluate given algorithm trajectory sampled randomly. timestep trajectory subset cameras selected algorithm. using resulting observations person tracked using unweighted particle ﬁlter starting random initial belief. timestep prediction maxs person’s location compared person’s true location. performance total number correct predictions made multiple trajectories. baselines greedy maximization lazier greedy maximization. since cannot compute exactly greedy maximization simply uses approximation based estimates conditional entropy ignoring resulting bias conlazier ﬁdence bounds. greedy maximization iteration samples subset size selects subset estimated maximizes marginal gain. neither greedy lazier greedy figure multi-person trackmaximization employ lazy evaluations means pruning longer justiﬁed. addition since lazy greedy maximization’s pruning based marginal gain instead bias exacerbated presence entropy approximation instead one. figure shows number correct predictions runtime method various settings thus left desirable region. general greedy maximization performs nearly well best-performing algorithm lower computational cost. naively decreasing number samples worsens performance scale computational cost even performing greedy maximization nominal samples huge bottom plot. greedy maximization hand performs consistently three settings scales much better increases making suitable real-world problems. baan sander landsmeer chris kruszynski gert antwerpen judith dijk. real-time tracking fast retrieval persons multiple surveillance cameras shopping mall. baharan mirzasoleiman amin karbasi sarkar andreas krause. distributed submodular maximization identifying representative elements massive data. nips baharan mirzasoleiman ashwinkumar badanidiyuru amin karbasi vondr´ak andreas krause. lazier lazy greedy. aaai mow. tight upper bound discrete", "year": 2016}