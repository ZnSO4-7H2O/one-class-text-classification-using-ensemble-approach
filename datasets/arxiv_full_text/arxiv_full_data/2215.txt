{"title": "Prediction-Constrained Training for Semi-Supervised Mixture and Topic  Models", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Supervisory signals have the potential to make low-dimensional data representations, like those learned by mixture and topic models, more interpretable and useful. We propose a framework for training latent variable models that explicitly balances two goals: recovery of faithful generative explanations of high-dimensional data, and accurate prediction of associated semantic labels. Existing approaches fail to achieve these goals due to an incomplete treatment of a fundamental asymmetry: the intended application is always predicting labels from data, not data from labels. Our prediction-constrained objective for training generative models coherently integrates loss-based supervisory signals while enabling effective semi-supervised learning from partially labeled data. We derive learning algorithms for semi-supervised mixture and topic models using stochastic gradient descent with automatic differentiation. We demonstrate improved prediction quality compared to several previous supervised topic models, achieving predictions competitive with high-dimensional logistic regression on text sentiment analysis and electronic health records tasks while simultaneously learning interpretable topics.", "text": "supervisory signals potential make low-dimensional data representations like learned mixture topic models interpretable useful. propose framework training latent variable models explicitly balances goals recovery faithful generative explanations high-dimensional data accurate prediction associated semantic labels. existing approaches fail achieve goals incomplete treatment fundamental asymmetry intended application always predicting labels data data labels. prediction-constrained objective training generative models coherently integrates loss-based supervisory signals enabling effective semisupervised learning partially labeled data. derive learning algorithms semi-supervised mixture topic models using stochastic gradient descent automatic differentiation. demonstrate improved prediction quality compared several previous supervised topic models achieving predictions competitive high-dimensional logistic regression text sentiment analysis electronic health records tasks simultaneously learning interpretable topics. latent variable models widely used explain high-dimensional data learning appropriate low-dimensional structure. example model online restaurant reviews might describe single user’s long plain text blend terms describing customer service terms related italian cuisine. modeling electronic health records single patient’s high-dimensional medical history results diagnostic reports might described classic instance juvenile diabetes. crucially often wish discover faithful low-dimensional representation rather rely restrictive predeﬁned representations. latent variable models including mixture models topic models like latent dirichlet allocation widely used unsupervised learning high-dimensional data. many efforts generalize methods supervised applications observations accompanied target values especially seek predict targets future examples. example paul dredze topics twitter model trends jiang topics image captions make travel recommendations. smartly capturing joint distribution input data targets supervised lvms lead predictions better generalize limited training data. unfortunately many previous methods supervised learning lvms fail deliver promise—in work ﬁrst contribution provide theoretical empirical explanation exposes fundamental problems prior formulations. naïve application lvms like topic models supervised tasks uses two-stage training ﬁrst train unsupervised model train supervised predictor given ﬁxed latent representation stage one. unfortunately two-stage pipeline often fails produce high-quality predictions especially data features carefully engineered contain structure irrelevant prediction. example applying clinical records might topics common conditions like diabetes heart disease irrelevant ultimate supervised task predicting sleep therapy outcomes. two-stage approach often unsatisfactory many attempts made directly incorporate supervised labels observations single generative model. mixture models examples supervised training numerous similarly many topic models proposed jointly generate word counts document labels however survey halpern ﬁnds approaches little beneﬁt standard unsupervised clinical prediction tasks. furthermore often quality supervised topic models signiﬁcantly improve model capacity increases even large training datasets available. work expose correct several deﬁciencies previous formulations supervised topic models. introduce learning objective directly enforces intuitive goal representing data enables accurate downstream predictions. objective acknowledges inherent asymmetry prediction tasks clinician interested predicting sleep outcomes given medical records medical records given sleep outcomes. approaches like supervised optimize joint likelihood labels words ignore crucial asymmetry. prediction-constrained latent variable models tuned maximize marginal likelihood observed data subject constraint prediction accuracy exceeds target threshold. emphasize approach seeks compromise distinct goals build reasonable density model observed data making high-quality predictions target values given data. cared modeling data well could simply ignore target values adapt standard frequentist bayesian training objectives. cared prediction performance host discriminative regression classiﬁcation methods. however many applications beneﬁt representations lvms provide including ability explain target predictions high-dimensional data interpretable low-dimensional representation. many cases introducing supervision enhances interpretability generative model well task forces modeling effort focus relevant parts high-dimensional data. finally many applications beneﬁcial ability learn observed data target labels unavailable. especially semi-supervised domain prediction-constrained training objectives provides clear wins existing methods. section develop prediction-constrained training objective applicable broad family latent variable models. later sections provide concrete learning algorithms supervised variants mixture models topic models however emphasize framework could applied much broadly allow supervised training well-known generative models like probabilistic dynamic topic models latent feature models hidden markov models sequences trees linear dynamical system models stochastic block models relational data many more. broad family latent variable model consider illustrated fig. assume observed dataset paired observations yd}d refer data labels targets understanding intended applications easily access data often need predict example pairs text documents accompanying class labels images accompanying scene categories patient medical histories accompanying diagnoses. often refer observation document since motivated part topic models emphasize work directly applicable many lvms data types. assume exchangeable data pairs generated independently model hidden variable simple mixture model integer indicating associated data cluster. complex members family like topic models several document-speciﬁc hidden variables. generative process random variables local document unfolds three steps generate prior generate given according distribution ﬁnally generate given distribution joint density document factorizes assume generating distributions parameterized probability density functions easily evaluated differentiated. global parameters specify density. training model treat global parameters random variables associated prior density chosen model family example downstream core assumption generative process produces observed data targets conditioned hidden variable contast upstream models dirichlet-multinomial regression disclda labeled assume observed labels generated ﬁrst combined hidden variables produce data upstream models inference challenging labels missing. example downstream models computed omitting factors containing upstream models must explicitly integrate possible similarly upstream prediction labels data complex downstream models. said predictively constrained framework could also used produce novel learning algorithms upstream lvms. given general model family core problems interest. ﬁrst global parameter learning estimating values approximate posteriors given training data yd}. second local prediction estimating target given data model parameters classical approach estimating would maximize marginal likelihood training data targets integrating hidden variables equivalent minimizing following objective function here denotes regularizer global parameters. prior density function equivalent maximum posteriori estimation problem standard training inputs targets modeled perfectly symmetric fashion. could equivalently concatenate form larger variable standard unsupervised learning methods joint representation. however practical models typically misspeciﬁed approximate generative process real-world data solving objective lead solutions matched practitioner’s goals. care much predicting patient mortality rates estimating past incidences routine checkups. especially inputs usually higher-dimensional targets conventionally trained lvms poor predictive performance. alternative maximizing joint likelihood consider prediction-constrained objective wish best possible generative model data meets quality threshold prediction targets given natural quality threshold probabilistic model require conditional probabilities must exceed scalar value leads following constrained optimization problem marginalization allows make predictions correctly account uncertainty given importantly given goal predict given cannot train model assuming informed scalar lagrange multiplier distinct value solution also solves constrained problem particular threshold mapping monotonic constructive lacks simple parametric form. deﬁne optimization problem prediction-constrained training objective. objective directly encodes asymmetric relationship data labels prioritizing prediction contrasts joint maximum likelihood objective treats variables symmetrically accurately model predictive density special case objective reduces objective penalizing aggregate predictive probability sensible many problems applications loss functions appropriate. generally penalize expected loss true labels predicted labels posterior general approach allows incorporate classic non-probabilistic loss functions like hinge loss epsilon-insensitive loss penalize errors asymmetrically classiﬁcation problems measuring quality predictions. however paper algorithms experiments focus probabilistic loss formulation deﬁned prediction quality constraint using document-speciﬁc losses alternative stringent training object would enforce separate prediction constraints document modiﬁed optimization problem would generalize allocating distinct lagrange multiplier weight observation tuning weights would require sophisticated optimization algorithms topic leave future research. extension semi-supervised prediction constraints data missing labels many applications dataset observations {xd}d subset observed labels remaining labels unobserved. semi-supervised learning problems like this generalize enforce label prediction constraint documents objective becomes |dy| fraction labeled data gets smaller need much larger lagrange multiplier uphold average quality predictive performance. occurs simply gets smaller data likelihood term continue larger relative magnitude compared label prediction term deﬁnition training objective straightforward desirable features shared supervised training objectives downstream lvms. section contrast objective several approaches often comparing methods topic modeling literature give concrete alternatives. chosen family supervised downstream lvms standard training method point estimate global parameters maximizes joint log-likelihood related bayesian methods approximate posterior distribution variational methods markov chain monte carlo methods estimate moments joint likelihood relating hidden variables data labels example supervised learns latent topic assignments optimizing joint probability bag-of-words document representations document labels several problems joint likelihood objective cardinality mismatch relative sizes random variables reduce predictive performance. particular one-dimensional binary label high-dimensional word count vector optimal solution often indistinguishable solution unsupervised problem modeling data alone. low-dimensional labels neglible impact joint density compared high-dimensional words causing learning ignore subtle features motivated similar concerns joint likelihood training jebara pentland introduce method explicitly optimize conditional likelihood particular gaussian mixture model. replace conditional likelihood tractable lower bound monotonically increase bound coordinate ascent algorithm call conditional expectation maximization chen instead variant backpropagation optimize conditional likelihood supervised topic model. concern conditional likelihood objective exclusively focuses prediction task; need lead good models data cannot incorporate unlabeled data. contrast prediction-constrained approach allows principled tradeoff optimizing marginal likelihood data conditional likelihood labels given data. ﬁrst notice high-dimensional data swamp inﬂuence lowdimensional labels among practitioners common workaround imbalance retain symmetric maximum likelihood objective replicate label observed times document yd}. applied supervised label replication leads alternative power slda topic model label replication still leads nearly per-document joint density except likelihood density raised r-th power label replication better balance relative sizes performance gains standard supervised often negligible approach address assymmetry issue. examine label-replicated training objective objective contain direct penalty predictive density fundamental idea prediction-constrained approach core term objective instead symmetric joint density maximized training assuming data replicated labels present. easy examples optimal solution objective performs poorly target task predicting given training directly prioritized asymmetric prediction. later sections case study fig. provide intuition-building examples maximum likelihood joint training label replication fails give good prediction performance value replication weight approach better sufﬁciently large. example label replication lead poor predictions. even number replicated labels optimal solution label-replicated training objective suboptimal prediction given demonstrate this consider example involving two-component gaussian mixture models. consider one-dimensional data consisting evenly spaced points three points positive labels rest negative labels suppose goal mixture model gaussian components data assuming minimal regularization indicate mixture component term dominate term optimized setting probability close depending cluster. particular choose computes maximum likelihood solution remaining parameters given assignments points however exists alternative two-component mixture model yields better labels-given-data likelihood makes fewer mistakes. cluster centers cluster variances model labels-given-data likelihood point misclassiﬁed. solution achieves lower misclassiﬁcation rate choosing narrow gaussian cluster model adjacent positive points correctly making attempt capture positive point therefore solution suboptimal making predictions given counter-example also illustrates intuition behind replicated objective fails increasing replicates forces take value predictive training close possible. however guarantees necessary predicting given fig. additional in-depth example. posterior regularization framework introduced graça later reﬁned ganchev notable early work applied explicit performance constraints latent variable model objective functions. work focused models local random variables data hidden variables without explicit labels mindful this naturally express objective notation explaining data explicitly objective function incorporating labels later performance constraints. approach begins overall goals expectation-maximization treatment maximum likelihood inference frame problem estimating approximate posterior latent variable approximation close possible divergence real posterior generally select density tractable parametric family free parameters restricted parameter space makes valid density. leads objective here function strict lower bound data likelihood popular algorithm optimizes objective coordinate descent steps alternately update variational parameters model parameters framework graça adds additional constraints approximate posterior additional loss function interest observed latent variables bounded value distribution purposes possible loss function could negative likelihood label loss) informative directly compare constraint objective approach directly constrains expected loss true hidden-variable-from-data posterior contrast approach constrains expectation approximate posterior posterior stay close true hidden-variable-from-data posterior indeed write objective unconstrained form lagrange multiplier assume loss negative label log-likelihood have shown reach surprising conclusion objective reduces lower bound symmetric joint likelihood labels replicated times. thus inherit problems label replication discussed above optimal training update incorporates information data labels however train model good approximation show critical good predictive performance. another thread related work putting constraints approximate posteriors known maximum entropy discrimination ﬁrst published jaakkola details followup work approach developed training discriminative models without hidden variables primary innovation showing manage uncertainty parameter estimation max-margin-like objectives. context lvms work differs standard optimization important separable ways. first estimates posterior global parameters instead simple point estimate. second enforces margin constraint label prediction rather maximizing probability labels. note brieﬂy jaakkola consider objective unsupervised latent variable models constraint directly expectation lower-bound data likelihood. choice constrain data likelihood fundamentally different constraining labels-given-data loss done lvms original work aligned focus high-quality predictions. application supervised lvms med-lda extension topic model based med-inspired training objective. later work developed similar objectives lvms broad name regularized bayesian inference understand objectives focus original unconstrained training objectives med-lda regression classiﬁcation notation follows scalar emphasizing important loss function relative unsupervised problem prior distribution global parameters lower bound make objective comparable earlier objectives performing point estimation instead posterior approximation reasonable moderate large data regimes posterior global parameters concentrate. choice allows focus core question deﬁne objective balances data labels rather separate question managing uncertainty training. making simpliﬁcation substituting point estimates expectations divergence regularization term reducing med-lda objective becomes objective graça framework consider expectations approximate posterior rather choice data-only posterior however difference med-lda objectives med-lda objective computes loss expected prediction earlier objective penalizes full expectation loss earlier work also suggests using expectation loss q)]. decision theory argues latter choice preferable possible since lead decisions better minimize loss uncertainty. suspect med-lda chooses former leads tractable algorithms chosen loss functions. motivated decision-theoretic view consider modifying med-lda objective take full expectation loss. swap also justiﬁed assuming loss function convex epsilon-insensitive loss hinge loss used med-lda jensen’s inequality used bound objective above. resulting note irregularity classiﬁcation regression formulation med-lda published classiﬁcation-med-lda included labels loss term regression-med-lda included terms objective penalize reconstruction inside likelihood bound term using gaussian likelihood well inside separate epsilon-insensitive loss term. here assume loss term used simplicity. form recovered symmetric maximum likelihood objective label replication replicated times. thus even effort fails properly handle asymmetry issue raised possibly leading poor generalization performance. relationship semi-supervised learning frameworks often semi-supervised training performed optimization joint likelihood using algorithm impute missing data work falls thread self-training model trained labeled data used label additional data retrained accordingly. chang incorporated constraints semi-supervised self-training upstream hidden markov model starting small labeled dataset iterate steps train model parameters maximum likelihood estimation fully labeled expand revise fully labeled constraint-driven approach. given several candidate labelings example step reranks prefer obey soft constraints importantly however work’s subprocedure training fully labeled data symmetric maximum likelihood objective approach directly encodes asymmetric structure prediction tasks. work deliberately speciﬁes prior domain knowledge label distributions penalizes models deviate prior predicting unlabeled data. mann mccallum propose generalized expectation constraints extend earlier expectation regularization approach objective terms conditional likelihood objective regularization term comparing model predictions weak domain knowledge here indicates expected domain knowledge overall labels-given-data distribution predicted labels-given-data distribution current model. distance function weighted penalizes predictions deviate domain knowledge. unlike approach objective focuses exclusively label prediction task incorporate notion generative modeling. present simple case study applying prediction-constrained training supervised mixture models. goal illustrate beneﬁts prediction-constrained approach situation marginalization computed exactly closed form. allows direct comparison proposed training objective alternatives like maximum likelihood without worry approximations needed make inference tractable affect either objective. consider simple supervised mixture model generates data pairs illustrated fig. mixture model assumes possible discrete hidden states hidden variable data point indicator variable {zd} indicates clusters point assigned mixture model parameterize densities follows parameter latent variable prior simple vector positive numbers representing prior probability cluster. emphasize data likelihood label likelihood left generic form since relatively modular could apply mixture model objectives many different data label distributions long valid densities easy evaluate optimize parameters fig. happens show particular likelihood choices used data experiments develop training general case. assumption make clusters separate parameter related work supervised mixtures. knowledge prediction-constrained optimization objective novel large related literature applying mixtures supervised problems practioner observes pairs data covariates targets line work uses generative models factorization structure like fig. cluster parameters generating data example ghahramani jordan consider nearly model experiments derive expectation maximization algorithm maximize lower bound symmetric joint likelihood later applied work sometimes called models bayesian proﬁle regression targets real-valued efforts seen broad extensions generalized linear models especially context bayesian nonparametric priors like dirichlet process mcmc sampling procedures however none efforts correct assymmetry issues raised instead simply using symmetric joint likelihood. work takes discriminative view clustering task. krause develop objective called regularized information maximization learns conditional distribution preserves information data efforts estimate probability densities supervised clustering many applications paradigm exist parameter estimation gradient descent. original unconstrained optimization problem thus formulated mixture models using closed form marginal probability functions appropriate regularization terms practically solve optimization objective gradient descent. however parameters live constrained spaces like k−dimensional simplex. handle this apply invertible one-to-one transformations constrained spaces unconstrained real spaces apply standard gradient methods easily. practice training supervised mixtures adam gradient descent procedure requires specifying baseline learning rate adaptively scaled parameter dimension improve convergence rates. initialize parameters random draws reasonable ranges several thousand gradient update steps achieve convergence local optima. sure best possible fig. example sec. asymmetric prediction constrained training predicts labels better symmetric joint maximum likelihood training label replication rows estimated -cluster gaussian mixture model training procedure different weight values taking best many initializations using relevant training objective function. curves show estimated gaussian distribution cluster. upper left text panel gives estimated probability cluster emit positive label. colors assigned cluster higher probability emitting positive labels. stacked histograms -dimensional training dataset overlaid background bottom area-under-the-roc-curve error rate scores predicting labels data training data using best solution across different weight values final panel shows negative likelihood data across values. consider small example illustrate fundamental contributions training often superior symmetric maximum likelihood training label replication terms ﬁnding models accurately predict labels given data apply supervised mixture models simple dataset data real line binary labels observed training dataset shown rows fig. stacked histogram. construct data drawing data three different uniform distributions distinct intervals real line label order left right later reference interval contains data points roughly even distribution positive negative labels; interval contains points purely positive labels; interval contains points purely negative labels. stacked histograms data distribution colored assigned label found fig. wish train supervised mixture model dataset. fully specify model must deﬁne concrete densities parameter spaces. data likelihood gaussian distribution parameters cluster mean parameter take real value standard deviation positive small minimum value avoid degeneracy label likelihood select bernoulli likelihood {ρk} deﬁnes probability bern parameter cluster labels produced cluster positive. example model structure exactly total clusters simplicity. choices ensure estimates unique always exist numerically valid ranges helpful closed-form maximization step algorithm ml+rep objective. using model explain dataset fundamental tension explaining data labels parameters outrank parameters objectives. example standard joint maximum likelihood training happens prefer mixture model well-separated gaussian clusters means around gives reasonable coverage data density quite poor predictive performance left cluster centered interval right cluster explains training objective allows prioritizing prediction increasing lagrange multiplier weight fig. shows objective prefers solution cluster exclusively explaining interval positive labels. cluster wider variance cover remaining data points. solution much lower error rate higher values basic solution. course tradeoff visibly lower likelihood training data since higher-variance blue cluster less well explaining empirical distribution increases beyond quality label prediction improves slightly decision boundaries even sharper requires blue background cluster drift away data reduce data likelihood even more. total example illustrates training enables practitioner explore range possible models tradeoff data likelihood prediction quality. contrast amount label replication standard maximum likelihood training reach prediction quality obtained approach. show trained models replication weights values equal fig. values symmetric joint ml+rep training ﬁnds solution gaussian clusters exclusively dedicated either purely positive purely negative labels. occurs training time fully observed thus replicated presence strongly cues cluster assign allows completely perfect label classiﬁcation. however asymmetric prediction given training data performance much worse error rate roughly method achieved near important stress amount label replication would this asymmetric task predicting given focus symmetric joint likelihood objective. next study training objective enables useful analysis semi-supervised datasets contain many unlabeled examples labeled examples. again illustrate clear advantages approach standard maximum likelihood training prediction quality. dataset generated stages. first generate data vectors drawn mixture well-separated gaussians diagonal covariance matrices full data vectors -dimensional visualize ﬁrst dimensions scatterplot fig. point annotated binary label -labeled data points grey markers -labeled points black markers. finally make problem semi-supervised selecting percentage data points keep labeled training. example train using labeled pairs randomly selected full dataset well remaining unlabeled data points. model speciﬁcation fig. example sec. estimated supervised mixture models produced training ml+rep semi-supervised tasks labeled examples. panel shows elliptical contours estimated cluster gaussian mixture model scored best training objective using indicated weight percentage examples observed labels training varies upper text panel gives estimated probability cluster emit positive label. colors assigned cluster higher probability emitting positive labels. background panel scatter plot ﬁrst dimensions data point colored binary label previous example gaussian diagonal covariance bernoulli likelihood light regularization allow closed-form numerically-valid m-steps optimizing ml+rep objective fig. example sec. panel shows line plots performance metrics replication weight increases particular percentage data labeled. shows label prediction error rate bottom shows negative data likelihood visualizations corresponding parameters fig. deliberately constructed dataset supervised mixture model misspeciﬁed. either model well capturing data density covering well-separated blobs equal-covariance gaussians model predictive density well using thin horizontal gaussian model black points well much larger background gaussian capture rest. clusters single model well both. approach provides range possible models consider value tradeoff objectives. line plots showing overall performance trends data likelihood prediction quality shown fig. corresponding parameter visualizations shown fig. overall training equivalent standard training yields solution explains data well poor label prediction. tested fractions labeled data increase exists critical point solution longer prefered objective instead favors solution near-zero error rate label prediction. solution near zero error rate takes contrast test symmetric training label replication across many replication weights differences plentiful labels scarce labels enough labeled examples available high replication weights favor near-zero error rate solution found approach. however critical value solution longer favored instead prefered solution label replication pathological well-separated clusters explain data well extreme label probabilities consider solution ml+rep fig. cluster explains left blob unlabeled data well positive labels observed training occur left right blobs symmetric joint objective weighs data point whether labeled unlabeled equally updating parameters control matter much replication occurs. thus enough unlabeled points exert strong inﬂuence particular well-separated blob conﬁguration data density labeled points easily explained outliers blobs. contrast objective construction allows upweighting inﬂuence asymmetric prediction task parameters including thus even replication happens yield good predictions labels observed yield pathologies labels easily avoids. present much thorough case-study prediction-constrained topic models building latent dirichlet allocation downstream supervised extension slda unsupervised topic model takes observed data collection documents generally groups discrete data. document represented counts discrete word types features explain observations latent clusters topics document exhibits mixed-membership across topics. speciﬁcally terms general downstream model family model assumes here hidden variable prior density chosen symmetric dirichlet parameters scalar. similarly data likelihood parameters deﬁned {φk}k topic parameter vector positive numbers sums one. value deﬁnes probability generating word topic finally assume size document observed supervised setting assume document also observed target value applications we’ll assume binary labels emphasize types values easily possible generalized linear models standard supervised topic models like slda assume labels word counts conditionally independent given topic probabilities label likelihood logit function vector real-valued regression parameters. model large positive values imply high usage topic given document lead predictions positive label large negative values imply high topic usage leads negative label prediction original slda model represents count likelihood independent assignments word tokens topics generates labels bern) vector k−dimensional probability simplex given empirical distribution token-to-topic assignments ¯zdk πdk. enable efﬁcient inference algorithms analytically marginalize topic assignments away however objective simply version maximum likelihood label-replication sec. albeit hidden variables instantiated rather marginalized. poor prediction quality issues arise inherent symmetry. instead wish train assymetric conditions needed test time instantiate free variable deterministic mapping words topic simplex. speciﬁcally maximum a-posteriori solution argmaxπ∈∆k write deterministic function map. show sec. deterministic embedding document’s data onto topic simplex easy compute. chosen embedding seen feasible approximation full posterior needed choice respects need embedding observed words low-dimensional training test scenarios. previous training objectives slda. originally slda model trained variational algorithm optimizes lower bound marginal likelihood observed words labels mcmc sampling posterior estimation also possible. treatment ignores cardinality mismatch assymetry issues making difﬁcult make good predictions given conditions model mismatch. alternatives like med-lda offered alternative objectives enforce constraints loss function given expectations approximate posterior objective still ignores crucial asymmetry issue. also showed earlier sec. objectives reduced ineffective maximum likelihood label-replication. recently chen developed backpropagation methods called bp-lda bp-slda unsupervised supervised versions lda. train using extreme cases endto-end weighted objective supervised bp-slda entire data likelihood term omitted completely unsupervised bp-lda entire label likelihood omitted. contrast overriding goal guaranteeing minimum prediction quality objective leads lagrange multiplier allows systematically balance generative discriminative objectives. bp-slda offers tradeoff later experiments label predictions sometimes good underlying topic model quite terrible explaining heldout data yields difﬁcult-to-interpret topic-word distributions. fitting slda model given dataset using optimization objective requires concrete procedures per-document inference compute hidden variable global parameter estimation topic-word parameters logistic regression weight vector first show embedding computed several iterations exponentiated gradient procedure convex structure. second show differentiate entire objective perform gradient descent parameters interest experiments assume prior concentration parameter ﬁxed constant could easily optimized well procedure. problem convex non-convex otherwise. convex case suggest iterative exponentiated gradient algorithm procedure begins uniform probability vector iteratively performs elementwise multiplication exponentiated gradient convergence using scalar stepsize small enough steps ﬁnal result iterations converges solution. thus deﬁne embedding function outcome iterations procedure. iterations step size work well. line search could reduce number iterations needed importantly taddy points general non-convex case single solution simplex multimodal sparsity-promoting dirichlet prior simple reparameterization softmax basis leads unimodal posterior thus unique reparameterized space. elegantly softmax basis solution particular estimate simplex estimate posterior thus exponentiated gradient procedure reliably perform natural parameter estimation even trick. global parameter estimation stochastic gradient descent. optimize objective realize ﬁrst iterative estimation function fully differentiable respect parameters probability density functions means entire objective differentiable modern gradient descent methods applied easily. course requires standard transformations constrained parameters like topic-word distributions simplex unrestricted real vectors. loss function speciﬁed unconstrained parameters perform automatic differentiation compute gradients perform gradient descent adam algorithm easily allows stochastically sampling minibatches data gradient update. practice developed python implementations based autograd tensorﬂow plan release public. earlier work chen optimized fully discriminative objective mirror descent algorithm directly constrained parameters using manually-derived gradient computations within heroically complex implementation language. approach advantage easily extending supervised loss functions without need derive implement gradient calculations although automatic differentation slow. hyperparameter selection. hyperparameter prediction-constrained algorithm lagrange multiplier generally topic models text data needs order number tokens average document though need much larger depending much tension exists unsupervised supervised terms objective. possible suggest trying range logarithmically spaced values selecting best validation data although requires expensive retraining value. somewhat mitigated using ﬁnal parameters value initial parameters next value although escape preferred basins attraction overall non-convex objective. assess well proposed training slda hereafter abbreviate pc-lda achieves simultaneous goals solid heldout prediction labels given maintaining reasonably interpretable explanations words test ﬁrst goal comparing discriminative methods like logistic regression supervised topic models latter comparing unsupervised topic models. full descriptions datasets protocols well results please appendix. baselines. discriminative baselines include logistic regression fully supervised bp-slda algorithm chen supervised medlda gibbs sampler improve earlier variational methods original med-lda variational algorithm also consider implementation standard coordinate-ascent variational inference unsupervised supervised topic models. finally consider vanilla gibbs sampler using mallet toolbox third-party public code possible single-label-perdocument experiments pc-lda implementations support multiple binary labels document occur later yelp review label prediction electronic health record drug prediction tasks. datasets only method call bp-slda special case pclda implementation veriﬁed comparable single-target-only public implementation allows multiple binary targets. fig. vowels-from-consonants task heldout prediction error rates negative data probabilities enough topics good unsupervised topic models classify well. however numbers topics consonants outnumber vowels many methods explain better vowels. pclda slda good cleanly separating vowels capacity models offers much better heldout data predictions slda all). fig. vowels-from-consonants task rows example documents true generative topics task. rows -end heldout error rates learned topic-word parameters best model method. unsupervised gibbs supervised medlda pclda weight high error rates indicating little inﬂuence supervision task. slda achieves error rate expense messy topic-word parameters tuned predict however pclda reaches similar error rates interpretable topics separate initializations record point estimates topic-word parameters logistic regression weights deﬁned intervals throughout training select best pair validation bayesian methods like gibbslda select dirichlet concentration hyperparameters small grid search validation data pc-lda bp-slda recommended chen methods given snapshot parameters evaluate prediction quality area-under-the-roc-curve error rates using prediction rule evaluate data model quality computing fig. movie reviews task area-under-roc curve binary sentiment prediction negative heldout probability tokens pclda makes competitive label predictions maintaining better data models bpslda fig. yelp reviews task area-under-roc curve label prediction negative heldout probability tokens here report average across possible review labels table tasks. apply pc-lda approach following tasks vowels-from-consonants. study tradeoffs models built vowels-from-consonants task document sparse count vector pixels square grid illustrated fig. data generated model total topics letter topics well common background topics. documents labeled least vowel appears. several letters easily confused pairs share many pixels. design even unsupervised well here regime assesses well supervised methods label cues form topics targeted vowels rather plentiful consonants. movie yelp reviews. movie review task contains documents documents drawn published reviews professional movie critics. document binary label yelp task contains documents aggregating reviews single restaurant words possible vocabulary terms. document also possible binary attributes predicting successful antidepressants health records. finally consider predicting subset common antidepressants successful patient major depressive disorder given sparse bag-of-codewords summary electronic health record real deidentiﬁed data patients tertiary care hospital related outpatient centers. table contains results label prediction fig. visualizes word lists learned topics. prediction-constrained match discriminative baselines like logistic regression predicting labels. fig. shows pc-lda high values competitive logistic regression well bpslda med-lda. numbers table show method worst within competitor scores cases better logistic regression. yelp task fig. pc-lda various methods. drug prediction task uses common topic-word parameters independent regression weights. prediction-constrained lda’s gains predictive performance harm heldout data predictions nearly much slda. right panels ﬁgures show performance pc-lda approach baselines generative task modeling data. expected gibbs variational bayes inference procedures applied unsupervised objective best trying simultaneously optimize task. vbslda also models well noted earlier lack weighted objective traditional slda formulation means also essentially focuses entirely generative task. approaches predict well bp-slda consistently worst performer signiﬁcantly test error pc-lda. example difference well nats token topics yelp dataset fig. gold line dark line results show can’t expect solely discriminative approach explain data well. however prediction-constrained approach model capacity wisely capture variation data getting high quality discriminative performance. prediction-constrained topic-word parameters qualitatively interpretable. fig. fig. show learned topics letters task antidepressant recommendation task. letters task pc-lda bp-slda achieve error rates discriminative task; pc-lda features look like letters—and particular vowels—while bpslda’s topics less sparse harder interpret. antidepressant recommendation task drugs interest. fig. shows medical codewords topics predictive success non-success drug bupropion. again pc-lda topics clinically coherent predictor success contains words related migraines whereas predictor nonsuccess concerned testicular function. contrast topics found bp-slda little clinical coherence especially data highbp slda nortriptyline nonspecific_abnormal_find other_specified_local_inf embryonic_cyst_of_fallopi supraspinatus__ learned topics selected largest negative positive logistic regression coefﬁcients drug bupropion slda pclda pclda topics appear interpretable guide conversations clinicians hypotheses explore e.g. drugs better patients history migraines?. contrast slda’s exclusive focus makes topics hard interpret. panel shows topic’s medical codewords ranked computable bayes rule learn topic-word probabilities dimensional coherent topics dimensionality reduction—as well high predictive performance—enables conversations domain experts factors predictive treatment success. arriving proposed prediction-constrained training objective required many false starts. below comment advantages approach designing inference around modern gradient descent methods focusing asymmetry prediction task. also discuss limitations computational scalability local optima offer possible remedies. building modern gradient descent. designed inference around modern stochastic gradient descent methods using automatic differentiation compute gradients. choice stands contrast prior work supervised topic models using hand-designed coordinate descent mirror descent gibbs sampling methods automatic differentiation easy practitioners extend efforts custom loss functions supervised task without time-consuming derivations. example easily able handle multiple binary labels case yelp reviews prediction task antidepressant prediction task. focus asymmetry. focus asymmetry among supervised topic model work aware broadly authors molitor describe asymmetric inference strategies result principled probability distributions posterior graphical model towards better local optima. found even modern gradient methods training models objective quite challenging requiring many hours computation many random restarts avoid local optima. example even taking best restarts pc-lda curve fig. shows poor performance isolated local optima expect combination better initialization procedures annealing objective intelligent proposal moves could lead better local optima. approach requires repeatedly solving objective different values corduneanu jaakkola look continuation homotopy methods balance multiple objectives tradeoff scalar parameter starting unsupervised solution gradually increase re-optimize. approach later applied semi-supervised training hmms. unfortunately found non-convexity objectives caused even small changes induce solutions parameters appear connected previous optima recommend practical forward. similar concern non-convexity occurs using intelligent initialization strategies based purely unsupervised objective topic model methods using order word cooccurance moments including anchor words spectral methods thorough comparison needed brief tests using unsupervised methods initializaitons suggest pc-lda optimization landscape beneﬁt using initializations gradient descent. found parameter estimates often remain trapped near unsupervised optima unable parameters produce better label predictions random initializations. towards scalable training. within topic model case study taking derivatives embedding procedure signiﬁcant runtime bottleneck. scalable possibility would amortize cost recognition network variational auto-encoder brieﬂy explored recognition network pc-training supervised topic models previous workshop paper found predictions general lower quality simply embedding inference hidden variables within gradient descent. hope report inspires future work proposed objective easily applied many lvms. conclusion. presented optimization objective prediction-constrained framework training latent variable models. previous methods appropriate either fully discriminative fully generative goals objective unique simultaneously balancing goals allowing practitioner best possible generative model meets minimum prediction performance. approach also applied semi-supervised setting demonstrated mixtures case study. semi-supervised setting expect latent variable models show strongest advantages prediction tasks. references abadi agarwal barham brevdo chen citro corrado davis dean devin ghemawat goodfellow harp irving isard jozefowicz kaiser kudlur levenberg mané monga moore murray olah schuster shlens steiner sutskever talwar tucker vanhoucke vasudevan viégas vinyals warden wattenberg wicke zheng. tensorflow large-scale machine learning heterogeneous systems http//tensorflow.org/. software available tensorﬂow.org. jaakkola meila jebara. maximum entropy discrimination. technical report aitr- artiﬁcial intelligence laboratory massachusetts institute technology http//people. csail.mit.edu/tommi/papers/maxent.ps. liverani hastie azizi papathomas richardson. premium package proﬁle regression mixture models using dirichlet processes. journal statistical software https//www.jstatsoft.org/v/i. n.-t. molitor best jackson richardson. using bayesian graphical models model biases observational studies combine multiple sources data application birth weight water disinfection by-products. journal royal statistical society series ramage hall nallapati manning. labeled supervised topic model credit attribution multi-labeled corpora. proceedings conference empirical methods natural language processing volume -volume pages association computational linguistics dataset document count vector vocabulary symbol pixel square grid. data generated model total topics letter topics displaying either vowel consonant well background topics dispersed distributions. letter’s active pixels conﬁned small region entire square grid thus several letters easily confused example share many pixels \"m\". document least topics active. documents labeled least vowel appears. ﬁnal corpus includes training documents validation documents test-set documents. document tokens. text movie reviews four critics comes scaledata dataset released pang given plain text ﬁles movie reviews tokenized stemmed using snowball stemmer nltk python package words similar roots become token. removed tokens mallet’s list common english stop words well token included common ﬁrst names census. added step seeing many common ﬁrst names like michael jennifer appear meaninglessly many top-word lists trained topics. manually whitelisted \"oscar\" \"tony\" saliency movie reviews sentiment. performed counts remaining tokens across full corpus documents discarding tokens appear least documents less distinct documents. ﬁnal vocabulary list terms. original documents reduced vocabulary set. discarded documents short leaving documents. document binary label indicates negative review indicates positive review threshold matches threshold previously used data’s -category scale separate star reviews star reviews. data pairs split training validation test. validation test used documents evenly balancing positive negative labeled documents. remaining documents allocated training set. text online yelp reviews yelp dataset challenge construct multi-label binary dataset. dataset includes text reviews businesses. businesses associated meta data. consider businesses values seven interesting binary attributes reservations accepted deliver offered alcohol served good kids price range outdoor seating offered. construct documents concatenate reviews single business. thus business represented single document. also prune vocabulary removing rare words occur fewer documents removing common words occur documents. finally sort remaining words tf-idf score keep scoring words ﬁnal vocabulary. resulting corpus includes documents total observations. study deidentiﬁed cohort hundreds thousands patients drawn large academic medical centers afﬁliated outpatient networks period several years. patient least diagnostic code major depressive disorder included patient identiﬁed successful treatment included possible common anti-depressants marked primary treatments major depressive disorder clinical collaborators. labeled interval patient’s record successful prescription events interval used subset primary drugs interval lasted least days encounters occurred least every months. applying criteria identiﬁed patients deﬁnition success. patient extracted bag-of-codewords possible codewords binary label vector marking prevalent anti-depressants used known successful treatment. irbs harvard university massachusetts general hospital approved study. extracting data patient known successful treatment build data vector summarize facts known patient successful treatment given. thus must conﬁne records interval patient’s ﬁrst encounter last encounter drugs successful list ﬁrst prescribed. summarize patient’s interval pre-successful treatment built sparse count vector procedures diagnoses labs medications within interval deﬁnition none anti-depressant medications patient’s eventual success list appear simplify reduced ﬁnal vocabulary codewords occurred least distinct patients. discard patients fewer tokens extracting labels among primary drugs identiﬁed smaller antidepressants used successful treatment least patients. remaining primary drugs occur commonly enough thought could accurately access prediction quality. chosen list drugs predict nortriptyline amitriptyline bupropion fluoxetine sertraline paroxetine venlafaxine mirtazapine citalopram escitalopram drugs given combination multiple binary label problem. future work could look structured prediction tasks. pclda requires choice step size adam optimizer. select among using validation set. generally larger rates like preferred. bp-slda also requires step size choose among consider possible ways initialize topics first drawing topics low-variance random noise initial topic extreme symmetry breaking occurs. second using anchor words procedure given ﬁnds vocabulary words whose empirical distributions nicely cover span observed word-cooccurance distributions.", "year": 2017}