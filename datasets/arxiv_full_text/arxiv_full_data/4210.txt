{"title": "Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks", "tag": ["cs.AI", "cs.CV"], "abstract": "Recurrent Neural Networks (RNNs) continue to show outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code is publicly available at https://imatge-upc.github.io/skiprnn-2017-telecombcn/ .", "text": "v´ıctor campos∗† brendan jou‡ xavier gir´o-i-nieto§ jordi torres† shih-fu changγ †barcelona supercomputing center ‡google §universitat polit`ecnica catalunya γcolumbia university {victor.campos jordi.torres}bsc.es bjougoogle.com xavier.giroupc.edu shih.fu.changcolumbia.edu recurrent neural networks continue show outstanding performance sequence modeling tasks. however training rnns long sequences often face challenges like slow inference vanishing gradients difﬁculty capturing long term dependencies. backpropagation time settings issues tightly coupled large sequential computational graph resulting unfolding time. introduce skip model extends existing models learning skip state updates shortens effective size computational graph. model also encouraged perform fewer state updates budget constraint. evaluate proposed model various tasks show reduce number required updates preserving sometimes even improving performance baseline models. source code publicly available https//imatge-upc. github.io/skiprnn--telecombcn/. recurrent neural networks become standard approach practitioners addressing machine learning tasks involving sequential data. success enabled appearance larger datasets powerful computing resources improved architectures training algorithms. gated units long short-term memory gated recurrent unit designed deal vanishing gradients problem commonly found rnns architectures popularized part impressive results variety tasks machine translation language modeling speech recognition main challenges rnns training deployment dealing long sequences inherently sequential behaviour. challenges include throughput degradation slower convergence training memory leakage even gated architectures sequence shortening techniques seen sort conditional computation time alleviate issues. common approaches cropping discrete signals reducing sampling rate continuous signals based heuristics suboptimal. contrast propose model able learn samples need used order solve target task. consider video understanding task example scenes large motion beneﬁt high frame rates whereas frames needed capture semantics mostly static scene. main contribution work novel modiﬁcation existing architectures allows skip state updates decreasing number sequential operations performed without requiring additional supervision signal. model called skip adaptively determines whether state needs updated copied next time step. show network encouraged perform fewer state updates adding penalization term training allowing train models different computation budgets. proposed modiﬁcation generally integrated show paper implementations well-known rnns namely lstm gru. resulting models show promising results series sequence modeling tasks. particular evaluate proposed skip architecture sequence learning problems adding task sine wave frequency discrimination digit classiﬁcation sentiment analysis movie reviews action classiﬁcation video temporal action localization video. conditional computation shown gradually increase model capacity without proportional increases computational cost exploiting certain computation paths input idea extended temporal domain learning many times input needs pondered moving next designing architectures whose number layers depend input data works addressed time-dependent computation rnns updating fraction hidden states based current hidden state input following periodic patterns however inherently sequential nature rnns parallel computation capabilities modern hardware reducing size matrices involved computations performed time step generally accelerated inference dramatically hoped. proposed skip model seen form conditional computation time computation associated updates executed every time step. idea related update copy operations hierarchical multiscale rnns applied whole stack layers time. difference allowing approach skip input samples effectively reducing sequential computation shielding hidden state longer time lags. learning whether update copy hidden state time steps seen learnable zoneout mask shared units hidden state. similarly interpreted input-dependent recurrent version stochastic depth selecting parts input signal similar spirit hard attention mechanisms applied image regions patches input image attended order generate captions detect objects model understood generating hard temporal attention mask on-the-ﬂy given previously seen samples deciding time steps attended operating subset input samples. subsampling input sequences explored visual storylines generation although jointly optimizing weights subsampling mechanism often computationally infeasible expectation-maximization algorithm instead. similar research conducted video analysis tasks discovering minimally needed evidence event recognition training agents decide frames need observed order localize actions time motivated advantages training recurrent models shorter subsequences efforts conducted learning differentiable subsampling mechanisms although computational complexity proposed method precludes application long input sequences. contrast proposed method trained backpropagation degrade complexity baseline rnns. accelerating inference rnns difﬁcult inherently sequential nature leading design quasi-recurrent neural networks simple recurrent units relax temporal dependency consecutive steps. goal speeding inference lstm-jump augments lstm cell classiﬁcation layer decide many steps jump updates. despite promising results text tasks model needs trained reinforce requires deﬁning reasonable reward signal. determining rewards non-trivial necessarily generalize across tasks. moreover number tokens read jumps maximum jump distance number jumps allowed need chosen advance. hyperparameters deﬁne reduced subsequences model sample instead allowing network learn arbitrary sampling scheme. unlike lstm-jump proposed approach differentiable thus requiring modiﬁcations loss function simplifying optimization process limited predeﬁned sample selection patterns. augment network binary state update gate selecting whether state updated copied previous time step every time step probability ˜ut+ performing state update emitted. resulting architecture depicted figure characterized follows weights vector scalar bias sigmoid function fbinarize binarizes input value. network composed several layers columns ﬁxed ∆˜ut depends states subset layers implement fbinarize deterministic step function round although stochastic sampling bernoulli distribution bernoulli would possible well. model formulation encodes observation likelihood requesting input update state increases number consecutively skipped samples. whenever state update omitted pre-activation state update gate following time step ˜ut+ incremented ∆˜ut. hand state update performed accumulated value ﬂushed ˜ut+ ∆˜ut. number skipped time steps computed ahead time. particular formulation used work fbinarize implemented means rounding function number skipped samples performing state update time step given enables efﬁcient implementations computation performed whenever computational savings possible ∆˜ut ∆˜ut− need evaluate again depicted figure several advantages reducing number updates. computational standpoint fewer updates translates fewer required sequential operations process input signal leading faster inference reduced energy consumption. unlike models reduce average number operations step enables skipping steps completely. replacing updates copy operations increases memory network ability model long term dependencies even gated units since exponential memory decay observed lstm alleviated. training gradients propagated fewer updating time steps providing faster convergence tasks involving long sequences. moreover proposed model orthogonal recent advances rnns could used conjunction techniques e.g. normalization regularization variable computation even external memory figure model architecture proposed skip rnn. complete skip architecture computation graph time step conditioned architecture state updated i.e. architecture update step skipped previous state copied i.e. practice redundant computation avoided propagating ∆˜ut time steps whole model differentiable except fbinarize outputs binary values. common method optimizing functions involving discrete variables reinforce although several estimators proposed particular case neurons binary outputs select straight-through estimator consists approximating step function identity computing gradients backward pass yields biased estimator proven efﬁcient unbiased high-variance estimators reinforce successfully applied different works using straight-through estimator backward pass fbinarize model parameters trained minimize target loss function standard backpropagation without deﬁning additional supervision reward signal. skip able learn update copy state without explicit information samples useful solve task hand. however different operating point trade-off performance number processed samples required depending application e.g. willing sacriﬁce accuracy points order faster machines computational power reduce energy impact portable devices. proposed model encouraged perform fewer state updates additional loss terms common practice neural networks dynamically allocated computation particular consider cost sample condition lbudget cost associated single sequence cost sample sequence length. formulation bears similarity weight decay regularization network encouraged slowly converge toward solution norm weights small. similarly case network encouraged converge toward solution fewer state updates required. although budget formulation extensively studied experiments budget loss terms used depending application. instance speciﬁc number samples encouraged applying loss target value number updates following section investigate advantages adding state skipping common architectures lstm variety tasks. addition evaluation metric task report number state updates number ﬂoating point operations measures computational load model. since skipping update results ignoring corresponding input refer number updates number used samples interchangeably. goal studying effect skipping state updates learning capability networks also introduce baseline skips state update probability pskip. tune skipping probability obtain models perform similar number state updates skip models. training performed adam learning rate batches gradient clipping threshold applied trainable variables. bias equation initialized samples used beginning training. initial hidden state learned training whereas constant value order force ﬁrst update experiments implemented tensorflow single nvidia gpu. revisit original lstm tasks network given sequence tuples. desired output addition values marked whereas marked need ignored. follow experimental setup neil ﬁrst marker randomly placed among ﬁrst samples second placed among last half samples marker distribution yields sequences least samples distractors provide useful information all. however worth noting task risk missing marker large compared beneﬁts working shorter subsequences. practice forcing network samples beginning training improves robustness random initializations weights increases reproducibility presented experiments. similar behavior observed augmented architectures neural stacks train models units sequences length values uniformly drawn ﬁnal state fully connected layer regresses scalar output. model trained minimize mean squared error output ground truth. consider model able solve task held-out examples least orders magnitude variance output distribution. criterion stricter version followed hochreiter schmidhuber models learn solve task results table show skip models able roughly half updates corresponding counterparts. observed models using fewer updates never miss marker since penalization terms would large conﬁrmed poor performance baselines randomly skip state updates able solve tasks even skipping probability low. skip models learn skip samples sequence markers. moreover updates skipped second marker found since relevant information sequence already seen. last pattern provides evidence proposed models effectively learn whether update copy hidden state based input sequence opposed learning biases dataset only. downside skip models show difﬁculties skipping large number updates once probably cumulative nature ˜ut. mnist handwritten digits classiﬁcation benchmark traditionally addressed convolutional neural networks efﬁciently exploit spatial dependencies weight sharing. ﬂattening images vectors however reformulated challenging task rnns long term dependencies need leveraged follow standard data split aside training samples validation purposes. processing pixels units last hidden state linear classiﬁer predicting digit class. models trained epochs minimize cross-entropy loss. table summarizes classiﬁcation results test epochs training. skip rnns able solve task using fewer updates counterparts also show lower variation among runs train faster hypothesize skipping updates make skip rnns work shorter subsequences simplifying optimization process allowing networks capture long term dependencies easily. similar behavior observed phased lstm increasing sparsity cell updates accelerates training long sequences however drop performance observed models state updates skipped randomly suggests learning samples component performance skip rnn. figure accuracy evolution training validation mnist. skip exhibits lower variance faster convergence baseline gru. similar behavior observed lstm skip lstm omitted clarity. shading shows maximum minimum runs dark lines indicate mean. performance models task boosted techniques like recurrent batch normalization recurrent skip coefﬁcients cooijmans show lstm speciﬁc weight initialization schemes improved gradient reach accuracy rates note techniques orthogonal skipping state updates skip models could beneﬁt well. sequences pixels reshaped back images allowing visualize samples used rnns sort hard visual attention model examples ones depicted figure show model learns skip pixels discriminative padding regions bottom images. similarly qualitative results adding task attended samples vary depending particular input given network. popular approach video analysis tasks today extract frame-level features modeling temporal dynamics videos commonly recorded high sampling rates generating long sequences strong temporal redundancies challenging rnns. moreover processing frames computationally expensive become prohibitive high frame rates. issues alleviated previous works using short clips downsampling original data order cover long temporal spans without increasing sequence length excessively instead addressing long sequence problem input data level network learn frames need used. charades dataset containing videos annotated action classes per-frame fashion. frames encoded using features stream two-stream provided organizers challenge extracted fps. encoded frames stacked layers units hidden state last layer used compute update probability skip models. since frame annotated zero classes networks trained minimize element-wise binary cross-entropy every time step. unlike previous sequence tagging tasks setup allows evaluate performance skip task output sequence well. evaluation performed following setup sigurdsson evaluating equally spaced frames instead results reported table surprising baselines randomly skip state updates perform skip counterparts skipping probabilities. hypothesize several reasons behavior observed previous experiments supervision signal every time step inputs outputs strongly correlated consecutive frames. hand skip models clearly outperform random methods fewer updates allowed. note setup challenging longer time spans updates properly distributing state updates along sequence performance models. interestingly skip models learn frames need attended data without access explicit motion information. skip tends perform fewer state updates skip lstm cost sample none. behavior opposite observed adding task related observation determining best performing gated unit depends task hand chung indeed models consistently outperform lstm ones task. mismatch number used samples observed large values skip lstm skip converge comparable number used samples. previous work reports better action localization performance integrating optical information input lstm reaching boost performance comes cost roughly doubling number flops memory footprint encoder plus requiring extraction information preprocessing step. interestingly model learns frames need attended data without access explicit motion information. table mean average precision used samples average flops sequence inference validation charades. number state updates displayed mean videos validation set. presented skip rnns extension existing recurrent architectures enabling skip state updates thereby reducing number sequential operations computation graph. unlike approaches parameters skip trained backpropagation. experiments conducted lstms grus showed skip rnns match cases even outperform baseline models relaxing computational requirements. skip rnns provide faster stable training long sequences complex models owing gradients backpropagated fewer time steps resulting simpler optimization task. moreover introduced computational savings better suited modern hardware methods reduce amount computation required time step work partially supported spanish ministry economy competitivity european regional development fund contracts tec--r tin-p bsc-cns severo ochoa program sev-- grant -sgr- catalan government. v´ıctor campos supported obra social caixa caixa-severo ochoa international doctoral fellowship program. would also like thank technical support team barcelona supercomputing center. yoshua bengio nicholas l´eonard aaron courville. estimating propagating gradients stochastic neurons conditional computation. arxiv preprint arxiv. kyunghyun bart merri¨enboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. emnlp matthieu courbariaux itay hubara daniel soudry el-yaniv yoshua bengio. binarized neural networks training deep neural networks weights activations constrained or-. arxiv preprint arxiv. jeffrey donahue lisa anne hendricks sergio guadarrama marcus rohrbach subhashini venugopalan kate saenko trevor darrell. long-term recurrent convolutional networks visual recognition description. cvpr david krueger tegan maharaj j´anos kram´ar mohammad pezeshki nicolas ballas rosemary anirudh goyal yoshua bengio hugo larochelle aaron courville zoneout regularizing rnns randomly preserving hidden activations. iclr noam shazeer azalia mirhoseini krzysztof maziarz andy davis quoc geoffrey hinton jeff dean. outrageously large neural networks sparsely-gated mixture-of-experts layer. iclr kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhudinov rich zemel yoshua bengio. show attend tell neural image caption generation visual attention. icml yue-hei matthew hausknecht sudheendra vijayanarasimhan oriol vinyals rajat monga george toderici. beyond short snippets deep networks video classiﬁcation. cvpr saizheng zhang yuhuai tong zhouhan roland memisevic ruslan salakhutdinov yoshua bengio. architectural complexity measures recurrent neural networks. nips experiment network trained classify sinusoids whose period range milliseconds whose period range milliseconds every sine wave period random phase shift drawn every time step input network single scalar representing amplitude signal. since sinusoid continuous signals tasks allows study whether skip rnns converge solutions parameters ﬁxed sampling period changed. study different sampling periods milliseconds hyperparameters. train rnns units input signals milliseconds. batches stratiﬁed containing number samples class yielding chance accuracy. last state -way classiﬁer trained cross-entropy loss. consider model able solve task achieves accuracy held-out examples. table summarizes results task. cost sample number updates differ different sampling conditions. attribute behavior potentially large number local minima cost function since numerous subsampling patterns task successfully solved explicitly encouraging network converge particular solution. hand skip models cost sample roughly number input samples even sampling frequency doubled. desirable property since solutions robust oversampled input signals. qualitative results found section table results frequency discrimination task displayed mean four different runs. task considered solved classiﬁcation accuracy models cost sample converge similar number used samples different sampling conditions. imdb dataset contains training testing movie reviews annotated classes positive negative sentiment approximate average length words review. aside training data validation purposes. words embedded vector representations units. embedding matrix initialized using pre-trained wordvec embeddings available random vectors drawn otherwise dropout rate applied last state classiﬁcation layer order reduce overﬁtting. evaluate models sequences length cropping longer sequences padding shorter ones models reducing number required updates. results highlight trade-off accuracy available computational budget since larger cost sample results lower accuracies. however allowing network select samples instead cropping sequences given length boosts performance observed skip lstm achieves higher accuracy baseline lstm seeing roughly number words review. similar behavior seen skip models allowing select words longer reviews boosts classiﬁcation accuracy using comparable number tokens sequence. order reduce overﬁtting large models miyato leverage additional unlabeled data adversarial training achieve state accuracy imdb. extended analysis different experimental setups affect performance rnns task refer reader ucf- dataset containing trimmed videos belonging different action categories. seconds video sampled cropping longer ones padding shorter examples empty frames. activations global average pooling layer resnet- pretrained imagenet dataset used frame-level features stacked layers units each. weights tuned training reduce overﬁtting. hidden state last layer used compute update probability skip models. evaluate different models ﬁrst split ucf- report results table skip models improve classiﬁcation accuracy respect baseline require updates possibly motion consecutive frames resulting frame-level features high temporal redundancy moreover figure shows models performing fewer updates converge faster thanks gradients preserved longer spans training backpropagation time. recurrent architectures video action recognition achieved high performance ucf- comprise cnns spatiotemporal kernels two-stream cnns carreira zisserman show beneﬁts expanding ﬁlters pretraining larger datasets obtaining accuracy using data incorporating optical information. figure sample usage examples skip lstm frequency discrimination task .ms. dots indicate used samples whereas blue ones skipped. network learns using ﬁrst samples enough classify frequency sine waves contrast uniform downsampling result aliasing.", "year": 2017}