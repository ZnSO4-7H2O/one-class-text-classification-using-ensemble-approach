{"title": "Exploiting Cyclic Symmetry in Convolutional Neural Networks", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Many classes of images exhibit rotational symmetry. Convolutional neural networks are sometimes trained using data augmentation to exploit this, but they are still required to learn the rotation equivariance properties from the data. Encoding these properties into the network architecture, as we are already used to doing for translation equivariance by using convolutional layers, could result in a more efficient use of the parameter budget by relieving the model from learning them. We introduce four operations which can be inserted into neural network models as layers, and which can be combined to make these models partially equivariant to rotations. They also enable parameter sharing across different orientations. We evaluate the effect of these architectural modifications on three datasets which exhibit rotational symmetry and demonstrate improved performance with smaller models.", "text": "manifest different spatial positions input encoded similarly feature representations extracted layers. pooling layers provide local translation invariance combining feature representations extracted convolutional layers position-independent within local region space. together layers allow cnns learn hierarchy feature detectors successive layer sees larger part input progressively robust local variations layer below. cnns seen widespread computer vision tasks hierarchical structure provides strong prior especially effective images. success largely scalability parameter sharing implied convolution operation allows capacity model used much effectively would case fully connected neural network similar number parameters signiﬁcantly reduces overﬁtting. also enables used large images without dramatically increasing number parameters. paper investigate idea also applied rotation invariance equivariance. many types data exhibit properties exploiting increase parameter sharing allow regularise scale neural networks. propose four neural network layers represented figure used together build cnns partially fully rotation equivariant. resulting framework scalable easy implement practice. many classes images exhibit rotational symmetry. convolutional neural networks sometimes trained using data augmentation exploit this still required learn rotation equivariance properties data. encoding properties network architecture already used translation equivariance using convolutional layers could result efﬁcient parameter budget relieving model learning them. introduce four operations inserted neural network models layers combined make models partially equivariant rotations. also enable parameter sharing across different orientations. evaluate effect architectural modiﬁcations three datasets exhibit rotational symmetry demonstrate improved performance smaller models. many machine learning applications involving sensory data neural networks largely displaced traditional approaches based handcrafted features. features require considerable engineering effort prior knowledge design. neural networks however able automatically extract knowledge data previously incorporated models using feature engineering. nevertheless evolution exempted machine learning practitioners getting know datasets problem domains prior knowledge encoded architecture neural networks instead. prominent example standard architecture convolutional neural networks typically consist alternating convolutional layers pooling layers. convolutional layers endow models property translation equivariance patterns filters layers learn detect particular patterns multiple spatial locations input. patterns often occur different orientations example edges images arbitrarily oriented. result cnns often learn multiple copies ﬁlter different orientations. especially apparent input data exhibits rotational symmetry. therefore useful encode form rotational symmetry architecture neural network like parameter sharing resulting convolution operation encodes translational symmetry. could reduce redundancy learning detect patterns different orientations free model capacity. alternatively allow reduce number model parameters risk overﬁtting. four possible orientations input allow application ﬁlter without interpolation rotations angles sampling grid input rotated angles aligns original true angle. would like avoid interpolation because adds complexity relatively expensive operation. input represented matrix rotated versions obtained using transposition ﬂipping rows columns operations cheap computationally. group four rotations isomorphic cyclic group order refer restricted form rotational symmetry cyclic symmetry henceforth. addition four rotations apply horizontal ﬂipping operation total eight possible orientations obtainable without interpolation. refer dihedral symmetry dihedral group encode cyclic symmetry cnns parameter sharing ﬁlter operate four transformed copies input resulting four feature maps. crucially feature maps rotated versions other relative orientation input ﬁlter different them. equivalent applying four transformed copies ﬁlter unchanged input. demonstrated figure paper primarily former interpretation. discuss practical implications choice section many classes images exhibit partial full rotational symmetry particularly biology astronomy medicine aerial photography. types data speciﬁcally exhibit dihedral symmetry board conﬁgurations figure convolving image rotated ﬁlter inversely rotating result convolving inversely rotated image unrotated ﬁlter follows fact rotation distributive w.r.t. convolution. game tasks wish perform data neural networks usually require equivariance rotations input rotated learnt representations change predictable formally function equivariant class transformations transformations input corresponding transformation output found often representations change input rotated i.e. invariant. follows invariant representation also equivariant case identity also consider special case i.e. transformations input output same. refer same-equivariance. simplest achieve invariance class transformations input train neural network data augmentation during training examples randomly perturbed transformations class encourage network produce correct result regardless input transformed. provided network enough capacity able learn invariances data many cases even perfectly learns invariance training guarantee generalise. obtain guarantee might want encode desired invariance properties network architecture allow additional freed learning capacity learn concepts. forcing network rediscover prior knowledge data rather wasteful. number different transformations input simply combine together averaging them. requires modiﬁcations network architecture training procedure comes expense requiring computation generate predictions approach also applied case partial rotation invariance part broader test-time augmentation strategy includes transformations besides rotation predictions averaged. however discussed section provide guarantees invariance properties resulting model solve problem principled way. introduce four operations cast layers neural network constitute framework easily build networks equivariant cyclic rotations share parameters across different orientations. overview provided table visualisation figure operations changes size minibatch number feature maps beyond this operations affect behaviour surrounding layers principle compatible recent architectural innovations inception residual learning cyclic pooling layer combines predictions different rotated copies example using permutation-invariant pooling function reducing size minibatch process. formally tensor representing minibatch input examples feature maps clockwise rotation cyclic slicing operation represented column vector indicate rotated feature maps stacked across batch dimension practice. result layers following slicing layer process data pooling operation applied dense layers point feature maps longer spatial structure. case inverse rotations realign feature maps omitted. resulting network invariant cyclic rotations. problems however spatial structure preserved output case realignment important. resulting network same-equivariant input rotates output rotate way. using layers straightforward modify existing network architecture invariant sameequivariant inserting slicing layer input side pooling layer output. important note effective batch size layers become larger also reduce input batch size compensate this. otherwise modiﬁcation would signiﬁcantly slow training. need necessarily insert pooling layer could also position network layers after pooling operation convolutional spatial pooling layers. otherwise would relinquish equivariance point pooled feature maps would rotate input relative orientation w.r.t. ﬁlters following layers would change. next introduce cyclic rolling operation. observe minibatch intermediate activations network cyclic slicing pooling contains four sets feature maps example. rotations other correspond different relative orientations ﬁlters inputs realigning stacking along feature dimension increase number feature maps within pathway fourfold simple copy operation means next convolutional layer receives richer representation input. this reduce number ﬁlters convolutional layers still retaining rich enough representation. practice amounts -way parameter sharing ﬁlter produces four feature maps resulting different relative orientations w.r.t. input. figure schematic representation effect cyclic slice roll pool operations feature maps cnn. arrows represent network layers. square represents minibatch feature maps. letter used clearly distinguish orientations. different colours used indicate feature maps qualitatively different i.e. rotations other. feature maps column stacked along batch dimension practice; feature maps stacked along feature dimension. table four operations form framework building neural networks equivariant cyclic rotations. clockwise rotation cyclic permutation shifting elements backward permutation-invariant pooling function. equivariance properties slicing operation applied rotated input obtain denotes cyclic permutation shifting elements backward along batch dimension. roll operation simply consists applying possible cyclic permutations input stacking results along batch dimension equivalently figure shows effect cyclic slice roll pool operations feature maps cnn. stacking feature maps right order able preserve equivariance across layers sameequivariant w.r.t. cyclic permutation σr). resulting increase parameter sharing used either signiﬁcantly reduce number parameters better capacity model number parameters kept level. network rolling layer introduced every convolutional layer keep number parameters approximately constant reducing number ﬁlters half. turn increase number produced feature maps factor ends balancing number parameters layer proportional number input feature maps number ﬁlters. also want achieve parameter sharing rolling networks required fully equivariant mentioned section even networks trained natural images often exhibit redundancy ﬁlters learned ﬁrst couple layers. accommodate case simply stack feature maps obtained different orientations along feature dimension point network instead pooling together would otherwise. corresponds stacking operation introduced previously. mentioned section equivalently rotate either ﬁlters feature maps operate achieve -way parameter sharing relative orientation affects result convolution. implies possible practical implementations advantages disadvantages. rotating feature maps seem natural choice easiest implement many modern deep learning software frameworks roll operation isolated viewed separate layer neural network model. stacking feature maps corresponding different orientations batch dimension becomes easier exploit data parallelism feature maps must necessarily square otherwise would possible stack different orientations together single batch. ﬁlters need however. also implies roll operation make four copies feature increases memory requirements. rotating ﬁlters hand means feature maps need square ﬁlters must operation cannot isolated convolutional layers model affects parameters rather input activations. many frameworks complicates implementation require partial reimplementation convolutional layer abstraction. requires copying ﬁlters generally smaller feature maps operate memory requirements reduced. training straightforward produce version model require special operations beyond slicing pooling stacking different orientations ﬁlters convolutional layer. version used perform inference inputs size. previous discussion readily generalises dihedral case changing slice operation include ﬂipped addition rotated copies input adapting operations accordingly. complication equivariance properties dihedral slicing less straightforward resulting permutation longer cyclic. also important take account ﬂipping rotation commute. convolutional structure become accepted approach encoding translational equivariance image representations consensus classes transformations. many architectural modiﬁcations proposed encode rotation equivariance. schmidt roth modify markov random ﬁelds learn rotation-invariant image priors. kivinen williams sohn schmidt roth propose modiﬁed restricted boltzmann machines fasel gatica-perez create multiple rotated versions images feed ﬁlters shared across different orientations. representations gradually pooled together across different layers yielding fully invariant representation output. approach rotating input rather ﬁlters identical ours strategy achieving invariance using single pooling layer allows intermediate layers accurate orientation information. dieleman also create multiple rotated ﬂipped versions images feed stack convolutional layers. resulting representations concatenated stack dense layers. yield invariant predictions enable parameter sharing across orientations. framework approach reproduced using slicing layer input stacking layer convolutional dense parts network. similar approach investigated teney hebert ﬁlters individual convolutional layers constrained rotations other. apply rotated ﬂipped copies ﬁlter convolutional layer max-pool across resulting activations. concatenate instead prefer pool output side network able achieve global equivariance possible multiple pooling stages network. indeed useful apply approach higher layers network subset ﬁlters orientation information preserved. clark storkey force ﬁlters convolutional layers exhibit dihedral symmetry weight sharing means resulting feature maps necessarily invariant. however network able accurately detect fully symmetric patterns input restrictive many problems. sifre mallat propose model resembling ﬁxed rather learned ﬁlters scalingrotation-invariant addition translation-invariant. gens domingos propose deep symmetry networks generalisation cnns form feature maps arbitrary transformation groups. also modify architecture facilitate learning equivariance properties data rather directly encode them. approach ﬂexible requires training data. model kavukcuoglu able learn local invariance arbitrary transformations grouping ﬁlters overlapping neighbourhoods whose activations pooled together. liao describe template-based approach successfully learns representations invariant afﬁne nonafﬁne transformations cohen welling propose probabilistic framework model transformation group given dataset exhibits equivariance. tiled cnns weight sharing reduced able approximate complex local invariances regular cnns. third alternative encoding learning equivariance properties involves explicitly estimating transformation applied input separately example transforming auto-encoders spatial transformer networks approach also investigated face detection rowley goodfellow lenc vedaldi discuss equivariance properties representations measured. latter also show representations learnt lower layers cnns approximately linearly transformed input rotated ﬂipped implies equivariance. concurrently work cohen welling present group-equivariant convolutional neural networks. provide theoretically grounded formalism exploiting symmetries cnns describes type models also built framework. plankton plankton dataset consists grayscale images varying size divided unevenly classes correspond different species plankton. rescaled based length longest side. split separate validation training sets images respectively. dataset used national data science bowl data science competition hosted kaggle platform. although images provided testing labels never made public. images used evaluate competition results. able obtain test scores submitting predictions kaggle even though competition already ended. nature images acquired class organism fully invariant rotation ignoring minor effects gravity organisms arbitrarily oriented photographed. example image training shown figure galaxies galaxies dataset consists colour images size training. downscaled factor cropped images display galaxies various morphological properties form subset image used galaxy project image classiﬁed according taxonomy consisting questions varying numbers answers. answers total. questions pertain e.g. smoothness depicted galaxy whether spiral pattern many spiral arms are. image vector probability values corresponding answers provided estimated votes users galaxy crowd-sourcing platform. split dataset validation images training images. since dataset also used competition kaggle labels test containing images provided. obtained test scores submitting predictions kaggle dataset well. canonical orientation galaxies space absence ﬁxed reference frame. follows morphological properties galaxy independent orientation observe earth. means answer probabilities describing properties invariant rotation images. example image shown figure massachusetts buildings massachusetts buildings dataset consists aerial images boston area image covering area square kilometers. dataset features images pixelwise annotations buildings. following split training images validation images test images. training randomly sample smaller tiles predict labels square center. annotations dataset pixelwise rotating input image result identical rotation corresponding output task labeling buildings satellite images same-equivariant. example image corresponding label information dataset shown figure baseline cnns dataset designed following today’s common practices achieve competitive performance. plankton galaxies datasets architectures inspired architectures using ‘same’ convolutions throughout combination pooling. networks would ranked kaggle respectively competition entries quite reasonable taking account participants used extensive model averaging best results. massachusetts dataset stack ‘valid’ convolutional layers withpooling followed convolutions ensure enough contextual information available pixel. shown figure adam optimisation method experiments allows avoid retuning learning rates cyclic layers inserted. discrete learning rate schedules tenfold decreases near training following krizhevsky plankton dataset also weight decay additional regularisation. data augmentation reduce overﬁtting including random rotation made sure training baselines well would opportunity learn desired invariance properties. focus cyclic symmetry preliminary experiments showed usually practical beneﬁt dihedral symmetry. addition eightfold increase parameter sharing makes difﬁcult compare models equal footing. plankton dataset report cross-entropy. galaxies dataset report prediction root-meansquare error massachusetts buildings dataset report area curve report mean standard deviation metrics across training runs. provide train test sets give idea level overﬁtting. figure baseline architectures plankton galaxies massachusetts buildings conv. layers shown pooling layers blue dense layers orange. numbers units indicated left ﬁlter sizes right. relus used throughout. dropout applied dense layers. first modiﬁed plankton baseline architecture adding cyclic slicing layer input side cyclic pooling layer output layer. reduced batch size used training factor compared three different pooling functions mean maximum root-mean-square also evaluated whether apply relu nonlinearity features pooling not. gives total conﬁgurations listed table along approximate number parameters results. noted choice function typically depend dataset size model. anecdotally found alternative pooling functions sometimes result better regularization. datasets pooling also gives modest improvement baseline. next investigated effect inserting rolling layers networks addition slicing pooling layers. considered approaches case insert rolling layers convolutional layers well ﬁrst dense layer reduce number units layers. case insert rolling layer ﬁrst dense layer keep number feature maps constant layers rolling operations inserted reducing number ﬁlters factor layers times fewer parameters. halving number ﬁlters instead number feature maps double relative original model. implies decrease parameters layer rolling operation increase next layer. three datasets observe limited effect performance number parameters signiﬁcantly reduced dataset also report performance version baseline network half number ﬁlters layers except last comparable number parameters demonstrate models rolling layers make efﬁcient parameter budget. especially interesting models take roughly amount computation train; additional cost roll operations minimal compared plankton galaxies datasets baseline models several dense layers. ﬁrst parameters input consists ﬂattened stack feature maps topmost convolutional layer. reduce number parameters layer halving number units adding rolling layer doubles number parameters next dense layer plankton network result reduction layer fewer parameters begin with. performance slightly improved compared baseline networks. massachusetts buildings dataset baseline model fully convolutional. evaluated version network roll layers number ﬁlters layer reduced factor resulting network roughly number parameters baseline better performance. note datasets limited size models would heavily overﬁt. introduced framework building rotation equivariant neural networks using four layers easily inserted existing network architectures. beyond adapting minibatch size used training modiﬁcations required. demonstrated improved performance resulting equivariant networks datasets exhibit full rotational symmetry reducing number parameters. fast implementation rolling operation theano available https//github.com/ benanne/kaggle-ndsb. future work would like apply approach types data exhibit rotational symmetry particularly domains data scarcity often issue additional parameter sharing would valuable reduce overﬁtting. also explore extension approach groups transformations including rotations angles multiples investigate strategies manage additional complexity arising required interpolation realignment feature maps. finally would like extend work volumetric data reducing number parameters even important larger number symmetries exploited without requiring costly interpolation. references clark christopher storkey amos. training deep convolutional neural networks play proceedings international conference machine learning cowen robert sponaugle robinson k.l. planktonset plankton imagery data collected f.g. walton smith straits ﬂorida used national data science bowl noaa national centers environmental information. dataset. doi./vdvjd. dieleman sander willett kyle dambre joni. rotation-invariant convolutional neural networks galaxy morphology prediction. monthly notices royal astronomical society kavukcuoglu koray ranzato marc aurelio fergus le-cun yann. learning invariant features topographic ﬁlter maps. computer vision pattern recognition cvpr ieee conference ieee liao qianli leibo joel poggio tomaso. learning invariant representations applications face veriﬁadvances neural information processing cation. systems rowley henry baluja shumeet kanade takeo. rotation invariant neural network-based face detection. computer vision pattern recognition proceedings. ieee computer society conference ieee schmidt roth stefan. learning rotation-aware features invariant priors equivariant descripcomputer vision pattern recognition tors. ieee conference ieee sifre laurent mallat st´ephane. rotation scaling deformation invariant scattering texture discrimcomputer vision pattern recognition ination. ieee conference ieee szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. arxiv preprint going deeper convolutions. arxiv. willett kyle lintott chris bamford steven masters karen simmons brooke casteels kevin edmondson edward fortson lucy kaviraj sugata keel william galaxy detailed morphological classiﬁcations galaxies sloan digital survey. monthly notices royal astronomical society", "year": 2016}