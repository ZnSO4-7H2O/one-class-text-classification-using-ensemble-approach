{"title": "Deep Learning Approximation for Stochastic Control Problems", "tag": ["cs.LG", "cs.AI", "cs.NE", "math.OC", "stat.ML"], "abstract": "Many real world stochastic control problems suffer from the \"curse of dimensionality\". To overcome this difficulty, we develop a deep learning approach that directly solves high-dimensional stochastic control problems based on Monte-Carlo sampling. We approximate the time-dependent controls as feedforward neural networks and stack these networks together through model dynamics. The objective function for the control problem plays the role of the loss function for the deep neural network. We test this approach using examples from the areas of optimal trading and energy storage. Our results suggest that the algorithm presented here achieves satisfactory accuracy and at the same time, can handle rather high dimensional problems.", "text": "many real world stochastic control problems suffer curse dimensionality. overcome difﬁculty develop deep learning approach directly solves high-dimensional stochastic control problems based monte-carlo sampling. approximate time-dependent controls feedforward neural networks stack networks together model dynamics. objective function control problem plays role loss function deep neural network. test approach using examples areas optimal trading energy storage. results suggest algorithm presented achieves satisfactory accuracy time handle rather high dimensional problems. traditional solving stochastic control problems principle dynamic programming. mathematically elegant high-dimensional problems approach runs technical difﬁculty associated curse dimensionality. fact precisely context term ﬁrst introduced richard bellman turns problem also heart many subjects machine learning quantum many-body physics. recent years deep learning shown impressive results variety hard problems machine learning suggesting deep neural networks might effective tool dealing curse dimensionality problem. emphasized although partial analytical results reason deep neural networks performed well still largely remains mystery. nevertheless motivates using deep neural network approximation contexts curse dimensionality essential obstacle. paper develop deep neural network approximation context stochastic control problems. even though unexpected idea fact already explored context reinforcement learning subject overlaps substantially control theory formulation problem still merits. first framework propose much simpler corresponding work reinforcement learning. secondly study problem ﬁnite horizon. makes optimal controls time dependent. thirdly instead formulating approximations value function commonly done formulation terms approximating optimal control time. fact control time step approximated feedforward subnetwork. stack subnetworks together form deep network train simultaneously. numerical examples section suggest approximation achieve near-optimality time handle high-dimensional problems relative ease. note passing research similar stochastic control problems evolved name deep reinforcement learning artiﬁcial intelligence community stressed papers deal inﬁnite horizon problem time-independent policy. contrast algorithm involves single deep network obtained stacking together model dynamics different subnetwork approximating time-dependent controls. dealing high-dimensional stochastic control problems conventional approach taken operations research community approximate dynamic programming essential steps adp. ﬁrst replacing true value function using function approximation. second advancing forward time sample path backward sweep update value function. unlike deal value function all. deal directly controls. addition approximation scheme appears generally applicable. mathematical formulation consider stochastic control problem ﬁnite time horizon probability space ﬁltration throughout paper adopt convention variable indexed ft-measurable. denote state variable potential states. control variable denoted setting model-based. assume evolution system described stochastic model deterministic drift term given model. ft+-measurable random variable contains noisy information arriving time view discretized version stochastic differential equations. ensure generality model allow state-dependent constraints control task approximate functional dependence control state i.e. function assumed memory effects necessary memory effects also taken account difference principle. represent dependence multilayer feedforward neural network clarity ignore conditional dependence initial distribution. observation derivation algorithm given sample stochastic process {ξt}t total cost regarded output deep neural network. general architecture network illustrated figure note three types connections network multilayer feedforward neural network approximating control time weights subnetwork parameters optimize. direct contribution ﬁnal output network. functional form determined cost function parameters optimized type connection. figure illustration network architecture stochastic control problems hidden layers subnetwork. column corresponds subnetwork hidden variables subnetwork time penalty functions equality inequality constraints penalty coefﬁcients. speciﬁc examples found below. stress testing stage project optimal controls learned admissible ensure strictly satisfy constraints. training algorithm training sample {ξt}t input data compute neural network. standard stochastic gradient descent method backpropagation easily adapted situation. training algorithm easily implemented using common libraries without modifying sgd-type optimizers. also adopted technique batch normalization subnetworks right linear transformation activation. method accelerates training allowing larger step size easier parameter initialization. brieﬂy mention details implementation. numerical examples dell desktop .ghz intel core without accerleration. tensorflow implement algorithm adam optimizer optimize parameters. adam variant algorithm based adaptive estimates lower-order moments. default values corresponding hyper-parameters recommended deal constraints choose quadratic function penalty functions architecture subnetworks number layers input layer hidden layers output layer choose rectiﬁed linear unit activation function hidden variables. weights network initialized using normal distribution without pre-training. numerical results reported below facilitate comparison benchmark initial state deterministic value rather random distribution. therefore optimal control also deterministic batch normalization skipped ﬁrst example area ﬁnance. concerned minimizing expected cost trading blocks stocks ﬁxed time horizon. portfolio requires frequent rebalancing large orders across many stocks appear must executed within relatively short time horizon. execution costs associated tradings often substantial calls smart trading strategies. consider linear percentage price-impact model based work reason choose example analytic solution facilitates evaluation numerical solutions. denote number shares stock bought period price investor’s objective diag captures potential inﬂuence market conditions rn×n rn×m. complete model speciﬁcation dynamics simple multivariate autoregressive process solved analytically using dynamic programming analytic expression optimal execution strategy corresponding optimal cost. implementation parameters model assigned realistic values. choose gives generic high-dimensional problem control space number hidden units hidden layers initial learning rate batch size iteration steps learning curves different random seeds different time horizons plotted figure figure relative trading cost relative error controls function number iterations validation samples. shaded area depicts mean standard deviation different random seeds. average relative trading cost relative error controls test samples average running time respectively. dashed line figure represents analytical optimal trading cost deﬁned optimal execution cost cents/share no-impact cost problem objective function achieves near-optimality good accuracy average relative trading cost exact solution figure also observe computed optimal strategy approximates exact solution well. note layers total. practical applications usually constraints execution strategies. example feasible buying strategy might require nonnegative. constraints imposed easily storage wind energy recently received signiﬁcant attention increase efﬁciency electrical grid. practical adaptive methods optimal energy allocation power systems critical operate reliably economically. consider allocation problem aims optimizing revenues storage device renewable wind energy source satisfying stochastic demand. model follows. state variable amount energy storage device amount energy produced wind source price electricity spot market demand satisﬁed. maximum rates charging discharging storage device respectively rmax capacity storage device. control variable given amount energy transferred time superscript stands wind demand storage spot market. figure illustrates meaning control components network diagram. modeled ﬁrst-order markov chains bounded domains independent control exact speciﬁcation). maximize total reward need optimal control space since components control negative relu activation ﬁnal layer subnetwork. number hidden units hidden layers batch size penalty coefﬁcients iteration steps learning rate ﬁrst half iterations second half. literature many algorithms multidimensional stochastic control problems e.g. ones proceed discretizing state variable control variable ﬁnite present optimal control form lookup table. contrast algorithm handle continuous variables directly. however ease comparison optimal lookup table obtained backward dynamic programing figure relative reward function number iterations validation samples optimal lookup table obtained backward dynamic programming benchmark. shaded area depicts mean standard deviation different random seeds. average relative reward test samples average running time artiﬁcially conﬁne values lookup table. relative reward different random seeds plotted figure despite presence multiple constraints algorithm still gives near-optimal reward. neural-network policy gives even higher expected reward lookup table policy. noted relax discretization constraint imposed method achieve better reward lookup table cases learning curves figure display clearly feature algorithm problem time horizon increases variance becomes larger batch size iteration steps required. also learning curves rougher ﬁrst example. might presence multiple constraints result nonlinearity optimal control policy. extend previous example case devices test algorithm’s performance rather high dimensional problems available solution comparison. consider situation pure arbitrage i.e. allow buying electricity spot market store device. state variable resource vector denoting storage device. control variable characterized case function number iterations validation samples. shaded area depicts mean standard deviation different random seeds. average relative reward test samples average running time three cases parameters distributed bounded domains. number devices increases look reﬁned allocation policy expected reward larger. learning parameters case single device except reduce penalty coefﬁcients batch size learning curves plotted figure conﬁrms expectation reward increases number devises increases. learning curves behave similarly case single device different random initializations still give similar expected reward. note function space control policy grows increases algorithm still ﬁnds near-optimal solution slightly increased computational time. paper formulate deep learning approach directly solve high-dimensional stochastic control problems ﬁnite horizon. feedforward neural networks approximate timedependent control time stack together model dynamics. objective function control problem plays role loss function deep learning. numerical results suggest different problems even presence multiple constraints algorithm ﬁnds near-optimal solutions great extendability high-dimensional case. approach presented applicable wide variety problems different areas including dynamic resource allocation many resources demands dynamic game theory many agents wealth management large portfolios. literature problems treated different assumptions separability mean-ﬁeld approximation. suggested results paper deep neural network approximation provide general setting give better results. alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. pereira burges bottou weinberger editors advances neural information processing systems pages curran associates inc. john schulman sergey levine pieter abbeel michael jordan philipp moritz. trust region policy optimization. proceedings international conference machine learning june timothy lillicrap jonathan hunt alexander pritzel nicolas heess david silver daan wierstra erez yuval tassa. continuous control deep reinforcement learning. proceedings international conference learning representations john schulman philipp moritz sergey levine michael jordan pieter abbeel. high-dimensional continuous control using generalized advantage estimation. proceedings international conference learning representations duan chen rein houthooft john schulman pieter abbeel. benchmarking deep reinforcement learning continuous control. proceedings international conference machine learning june sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. proceedings international conference machine learning june daniel salas warren powell. benchmarking scalable approximation dynamic programming algorithm stochastic control multidimensional energy storage problems. technical report princeton university", "year": 2016}