{"title": "OpenAI Gym", "tag": ["cs.LG", "cs.AI"], "abstract": "OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.", "text": "openai toolkit reinforcement learning research. includes growing collection benchmark problems expose common interface website people share results compare performance algorithms. whitepaper discusses components openai design decisions went software. reinforcement learning branch machine learning concerned making sequences decisions. rich mathematical theory found variety practical applications recent advances combine deep learning reinforcement learning great deal excitement ﬁeld become evident general algorithms policy gradients q-learning achieve good performance difﬁcult problems without problem-speciﬁc engineering build recent progress reinforcement learning research community needs good benchmarks compare algorithms. variety benchmarks released arcade learning environment exposed collection atari games reinforcement learning problems recently rllab benchmark continuous control refer reader survey benchmarks including openai aims combine best elements previous benchmark collections software package maximally convenient accessible. includes diverse collection tasks common interface collection grow time. environments versioned ensure results remain meaningful reproducible software updated. alongside software library openai website scoreboards environments showcasing results submitted users. users encouraged provide links source code detailed instructions reproduce results. reinforcement learning assumes agent situated environment. step agent takes action receives observation reward environment. algorithm seeks maximize measure agent’s total reward agent interacts environment. literature environment formalized partially observable markov decision process openai focuses episodic setting reinforcement learning agent’s experience broken series episodes. episode agent’s initial state randomly sampled distribution interaction proceeds environment reaches terminal state. goal episodic reinforcement learning maximize expectation total reward episode achieve high level performance episodes possible. env.reset sample environment state return first observation agent.act agent chooses first action done info env.step environment returns observation reward boolean flag indicating episode complete. agent.act done info env.step agent.act done info env.step done true terminal design openai based authors’ experience developing comparing reinforcement learning algorithms experience using previous benchmark collections. below summarize design decisions. environments agents. core concepts agent environment. chosen provide abstraction environment agent. choice maximize convenience users allow implement different styles agent interface. first could imagine online learning style agent takes input timestep performs learning updates incrementally. alternative batch update style agent called observation input reward information collected separately algorithm later used compute update. specifying agent interface allow users write agents either styles. emphasize sample complexity ﬁnal performance. performance algorithm environment measured along axes ﬁrst ﬁnal performance; second amount time takes learn—the sample complexity. speciﬁc ﬁnal performance refers average reward episode learning complete. learning time measured multiple ways simple scheme count number episodes threshold level average performance exceeded. threshold chosen per-environment ad-hoc example maximum performance achievable heavily trained agent. ﬁnal performance sample complexity interesting however arbitrary amounts computation used boost ﬁnal performance making comparison computational resources rather algorithm quality. encourage peer review competition. openai website allows users compare performance algorithms. inspiration kaggle hosts machine learning contests leaderboards. however openai scoreboards create competition rather stimulate sharing code ideas meaningful benchmark assessing different methods. presents challenges benchmarking. supervised learning setting performance measured prediction accuracy test correct outputs hidden contestants. it’s less straightforward measure generalization performance except running users’ code collection unseen environments would computationally expensive. without hidden test must check algorithm overﬁt problems tested would like encourage peer review process interpreting results submitted users. thus openai asks users create writeup describing algorithm parameters used linking code. writeups allow users reproduce results. source code available possible make nuanced judgement whether algorithm overﬁt task hand. strict versioning environments. environment changes results change would incomparable. avoid problem guarantee changes environment accompanied increase version number. example initial version cartpole task named cartpole-v functionality changes name updated cartpole-v. monitoring default. default environments instrumented monitor keeps track every time step reset called. monitor’s behavior conﬁgurable record video periodically. also sufﬁcient produce learning curves. videos learning curve data easily posted openai website.", "year": 2016}