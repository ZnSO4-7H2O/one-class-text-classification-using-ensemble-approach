{"title": "Small-footprint Deep Neural Networks with Highway Connections for Speech  Recognition", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "For speech recognition, deep neural networks (DNNs) have significantly improved the recognition accuracy in most of benchmark datasets and application domains. However, compared to the conventional Gaussian mixture models, DNN-based acoustic models usually have much larger number of model parameters, making it challenging for their applications in resource constrained platforms, e.g., mobile devices. In this paper, we study the application of the recently proposed highway network to train small-footprint DNNs, which are {\\it thinner} and {\\it deeper}, and have significantly smaller number of model parameters compared to conventional DNNs. We investigated this approach on the AMI meeting speech transcription corpus which has around 70 hours of audio data. The highway neural networks constantly outperformed their plain DNN counterparts, and the number of model parameters can be reduced significantly without sacrificing the recognition accuracy.", "text": "approach teacher large-size network ensemble several different models used predict soft targets training student model much smaller. discussed soft targets provided teacher encode generalisation power teacher student model trained using labels observed perform better small model trained usual recently investigated rank displacement structured matrices small-footprint neural networks. work line argument neural networks dense connections over-parameterised linear layer replaced structured efﬁcient linear layers paper investigate thin deep architectures small-footprint neural network acoustic models. however depth increases training dnns stochastic gradient decent becomes increasingly difﬁcult highly non-convexity error surface. approach pre-train neural network unsupervised greedy layer-wise fashion however approach cannot circumvent difﬁculty arises tuning stage. another approach rely teacher-student architecture e.g. fitnet requires additional effort train teacher model beforehand. work paper builds recently proposed highway networks transform gate used scale output hidden layer carry gate used pass input directly elementwise rescaling. similar idea also studied long short-term memory recurrent neural networks speech recognition work observe highway connections successfully applied training thinner deeper networks still retraining recognition accuracy. experiments performed meeting speech transcription corpus contains around hours training data. using highway neural networks managed number model parameters marginal accuracy loss compared baseline acoustic models. feed-forward neural network multiple hidden layers performs cascaded layer-wise nonlinear transformations input. network hidden layers model represented speech recognition deep neural networks signiﬁcantly improved recognition accuracy benchmark datasets application domains. however compared conventional gaussian mixture models dnn-based acoustic models usually much larger number model parameters making challenging applications resource constrained platforms e.g. mobile devices. paper study application recently proposed highway network train small-footprint dnns thinner deeper signiﬁcantly smaller number model parameters compared conventional dnns. investigated approach meeting speech transcription corpus around hours audio data. highway neural networks constantly outperformed plain counterparts number model parameters reduced signiﬁcantly without sacriﬁcing recognition accuracy. index terms speech recognition highway network smallfootprint deep learning. modern state-of-the-art speech recognition systems based neural network acoustic models typical architecture deep neural network feedforward neural network multiple hidden layers layer large number hidden units compared conventional gaussian mixture models acoustic models usually much larger number model parameters explains large statistical modelling capacities high recognition accuracies. however becomes challenging applications dnn-based speech recognition systems resource constrained scenarios. instance highly desirable speech recognition system still function wearable computing mobile devices internet connection unavailable. requires smaller size acoustic models still achieve high recognition accuracy. number works small footprint dnns purpose. instance sainath approximate weight matrix hidden layers product low-rank matrices equivalent insert bottleneck layer without nonlinear activation. another branch studies based teacher-student architecture also referred model compression knowledge distillation funded epsrc programme grant ep/i/ natural speech technology research data collection accessed http//datashare.is.ed.ac.uk/handle//. thank zhang dong helpful discussions using cntk toolkit. input vector network; denotes transformation input parameter followed nonlinear activation function output function parameterised output layer. given ground truth target network usually trained gradient decent minimise loss function however number hidden layers increases error surface becomes increasingly non-convex possible poor local minima using gradient-based optimisation algorithms random initialisation furthermore showed variance back-propagated gradients become small lower layers model parameters initialised properly. numerous studies overcoming difﬁculties training deep neural networks including pre-training normalised initialisation deeplysupervised networks etc. recently srivastav proposed highway network demonstrated good results train deep networks highway network hidden layers augmented gating functions represented transform gate scales original hidden activations; carry gate scales input passing directly next hidden layer; denotes elementwise product; outputs constrained sigmoid functions gates parameterised respectively. unlike work bias vector gate functions. carry gate constrained work evaluate generalisation ability highway networks without constraint. without transform gate i.e. highway network similar network skip connections main difference input ﬁrstly scaled carry gate. without carry gate i.e. hidden layer bernoulli distribution element originally proposed shown later using continuous distribution well designed mean variance works well better perspective transform gate work regulariser difference deterministic function drawn stochastically predeﬁned distribution dropout. nevertheless empirical results indicate transform gate carry gate speed convergence rate. addition highway networks also generalise better measured terms recognition accuracy presumably regularisation effect gating functions. paper train small-footprint neural networks resource constrained speech recognition. highway network directly suitable purpose because introduces additional computational cost model parameters gating functions. rationale computational complexity number model parameters layer densely connected network order size hidden units. increasing depth network linearly increases computational cost model size reducing width yield quadratic reduction metrics. highway connections make feasible train thin deep networks therefore overall model sizes much smaller. save model parameters work shared gates hidden layers additional number model parameters relatively small. residual network type deep network using skip connections achieved state-of-the-art results image recognition building block residual networks shown figure fact residual networks similar highway networks without additional gate functions signiﬁcantly reduce computational cost. also reduces number model parameters albeit reduction marginal gating functions tied hidden layers conﬁguration. however without gating functions training residual networks difﬁcult compared highway networks empirically studied following experimental section. experiments performed individual headset microphone subset meeting speech transcription corpus amount training data around hours corresponding roughly million frames. dataset much larger datasets types thin deep networks evaluated used -dimensional fmllr adapted features vectors normalised per-speaker level spliced context window frames systems. number tied states table comparison depth width plain dnns hdnns. ∗indicates models trained using kaldi toolkit networks initialised restricted boltzmann machine based pre-training random initialisation yield convergence. systems trained alignment. results reported paper obtained using cntk toolkit kaldi decoder networks trained using cross-entropy criterion without pre-training unless speciﬁed otherwise. momentum epoch used sigmoid activation hidden layers. weights hidden layer randomly initialised uniform distribution range bias parameters initialised cntk systems. used trigram language model decoding. table shows word error rates plain dnns highway networks different conﬁgurations. number hidden units decreases accuracy plain dnns degrade rapidly accuracy loss cannot compensated increasing depth network. faced difﬁculty train thin deep networks directly without pre-training however highway connections difﬁculty. hdnns achieved consistent lower wers compared plain counterparts margin gain also increases number hidden units becomes smaller shown figure highway connections number model parameters around marginal accuracy loss less million model parameters trained hdnn achieve comparable slight higher accuracy compared strong baseline speaker adaptive training bmmi-based discriminative training. accuracy smaller-size hdnn models improved teacher-student style training investigated future. evaluated speciﬁc role transform carry gate highway architectures. results shown table disabled gates. observed using gates hdnn still achieved lower compared plain best results obtained gates active indicates gating functions complementary other. figure shows convergence curve training hdnns without transform carry gate. observed converged faster gates turned transform gate convergence rate much slower. discussed before carry gate viewed particular type skip connection important speed convergence compared transform gate experiments. also evaluated using constrained carry gate experiments studied approach computational cost reduced since matrixvector multiplication carry gate required. evaluated conﬁguration -layer neural networks results shown table contrary expectations constrained carry gate obtained worse results networks relatively wide accuracy reduced number hidden units smaller. reason constrained setting transform gate learns scaling function input output time. regularisation expected important training wide deep networks achieved using single gating function. instance input output hidden layer require larger smaller scaling weights time impossible constrained setting. future shall look regularisation generalisation properties gating functions closely. finally compare highway networks residual networks results given table experiments showed without gating functions training residual networks comparably challenging. instance hidden layers using sigmoid activations residual networks achieved higher compared highway networks. however differences terms accuracy smaller using relu activations residual networks training relu networks relatively less difﬁcult. furthermore experienced difﬁculty train residual networks hidden layers using sigmoid activations instead relu although relu activations residual networks slightly outperformed highway networks case. note that residual networks still performed better compared plain networks pre-training e.g. depth experiments draw conclusion residual networks powerful train deeper networks compared plain dnns particular relu activation functions reduce optimisation difﬁculty. however highway networks ﬂexible activation functions gating functions control follow information. paper investigate thin deep neural networks small-footprint acoustic models. study build recently proposed highway neural network introduces additional transform carry gate hidden layer. experiments indicate highway connections facilitate information mitigate difﬁculty training deep feedforward networks. thin deep architecture highway connections achieved consistently lower wers compared plain dnns reducing number hidden units signiﬁcantly total number model parameters negligible accuracy loss. also evaluated speciﬁc role transform carry gate found carry gate important speed convergence experiment. small-footprint highway networks improved teacher-student style training investigated future work. erhan p.-a. manzagol bengio bengio vincent difﬁculty training deep architectures effect unsupervised pre-training international conference artiﬁcial intelligence statistics hinton srivastava krizhevsky sutskever salakhutdinov improving neural networks preventing co-adaptation feature detectors arxiv preprint arxiv. srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overﬁtting journal machine learning research vol. zhang deep residimage recognition arxiv preprint eversole seltzer huang guenter kuchaiev zhang seide wang introduction computational networks computational network toolkit tech. rep. microsoft research tech. rep. povey ghoshal boulianne burget glembek goel hannemann motlıcek qian schwarz silovsk´y semmer vesel´y kaldi speech recognition toolkit proc. asru hinton deng dahl a.-r. mohamed jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition shared views four research groups signal processing magazine ieee vol. dahl deng acero contextdependent pre-trained deep neural networks largevocabulary speech recognition ieee transactions audio speech language processing vol. sainath kingsbury sindhwani arisoy ramabhadran low-rank matrix factorization deep neural network training high-dimensional output targets proc. icassp.", "year": 2015}