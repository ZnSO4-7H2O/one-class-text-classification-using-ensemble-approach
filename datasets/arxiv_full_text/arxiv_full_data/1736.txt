{"title": "On the accuracy of self-normalized log-linear models", "tag": ["stat.ML", "cs.CL", "cs.LG", "stat.ME"], "abstract": "Calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces. The problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature. In this paper, we analyze a recently proposed technique known as \"self-normalization\", which introduces a regularization term in training to penalize log normalizers for deviating from zero. This makes it possible to use unnormalized model scores as approximate probabilities. Empirical evidence suggests that self-normalization is extremely effective, but a theoretical understanding of why it should work, and how generally it can be applied, is largely lacking. We prove generalization bounds on the estimated variance of normalizers and upper bounds on the loss in accuracy due to self-normalization, describe classes of input distributions that self-normalize easily, and construct explicit examples of high-variance input distributions. Our theoretical results make predictions about the difficulty of fitting self-normalized models to several classes of distributions, and we conclude with empirical validation of these predictions.", "text": "calculation log-normalizer major computational obstacle applications log-linear models large output spaces. problem fast normalizer computation therefore attracted signiﬁcant attention theoretical applied machine learning literature. paper analyze recently proposed technique known self-normalization introduces regularization term training penalize normalizers deviating zero. makes possible unnormalized model scores approximate probabilities. empirical evidence suggests self-normalization extremely effective theoretical understanding work generally applied largely lacking. prove generalization bounds estimated variance normalizers upper bounds loss accuracy self-normalization describe classes input distributions self-normalize easily construct explicit examples high-variance input distributions. theoretical results make predictions difﬁculty ﬁtting self-normalized models several classes distributions conclude empirical validation predictions. log-linear models general class includes conditional random ﬁelds generalized linear models offer ﬂexible tractable approach modeling conditional probability distributions possible values large however computational cost computing normalizing constant prohibitive—involving summation many terms high-dimensional integral expensive dynamic program. machine translation community recently described several procedures training selfnormalized log-linear models goal self-normalization choose model parameters simultaneously yield accurate predictions produce normalizers clustered around unity. model scores used approximate surrogates probabilities obviating computation normalizer computation. paper aims understand theoretical properties self-normalization. empirical results already demonstrated efﬁcacy approach—for discrete models many output classes appears normalizer values made nearly constant without sacriﬁcing much predictive accuracy providing dramatic efﬁciency increases minimal performance cost. broad applicability self-normalization makes likely spread large-scale applications log-linear models including structured prediction regression obvious expect approaches successful number inputs order millions geometry resulting input vectors highly complex class functions associated different inputs quite rich. nontrivial parameter setting roughly constant seems challenging enough; require corresponding also lead good classiﬁcation results seems much. many input distributions arise practice appears possible choose make nearly constant without sacriﬁce classiﬁcation accuracy. goal bridge theoretical intuition practical experience. previous work bounds sample complexity self-normalizing training procedures restricted class models leaves open question self-normalization interacts predictive power learned model. paper seeks answer question. begin generalizing previously-studied model much general class distributions including distributions continuous support next provide believe ﬁrst characterization interaction self-normalization model accuracy section characterization given perspectives figure present empirical evidence bounds correctly characterize difﬁculty self-normalization conclusion survey open problems believe merit investigation. immediate motivation work procedure proposed speed decoding machine translation system neural-network language model language model used standard feed-forward neural network softmax output layer turns network’s predictions distribution vocabulary probability log-proportional output activation. observed sufﬁciently large vocabulary becomes prohibitive obtain probabilities model this language model trained following objective response output neural weights given input lagrangian perspective extra penalty term simply conﬁnes empirically normalizing parameters log-normalizers close origin. suitable choice observed trained network simultaneously accurate enough produce good translations close enough self-normalized scores used place log-probabilities without substantial degradation quality. seek understand observed success models ﬁnding accurate normalizing parameter settings. possible derive bounds kind interested general neural networks paper work simpler linear parameterization believe captures interesting aspects problem. possible view log-linear model single-layer network softmax output. usefully results presented apply directly trained neural nets last layer retrained self-normalize approach described beginning section closely related alternative selfnormalization trick described based noise-contrastive estimation alternative direct optimization likelihood instead training classiﬁer distinguish true samples model noise samples distribution. structure training objective makes possible replace explicit computation log-normalizer estimate. traditional values treated part parameter space estimated simultaneously model parameters; exist guarantees normalizer estimates eventually converge true values. instead possible estimates one. case empirical evidence suggests resulting model also exhibit self-normalizing behavior host techniques exist solving computational problem posed log-normalizer. many involve approximating associated integral using quadrature herding monte carlo methods special case discrete ﬁnite output spaces alternative approach—the hierarchical softmax—is replace large normalizer series binary decisions output classes arranged binary tree probability generating particular output product probabilities along edges leading reduces cost computing normalizer limits distributions learned still requires greater-than-constant time compute normalizers appears work well practice. cannot however applied problems continuous output spaces. begin providing slightly formal characterization general log-linear model deﬁnition given space inputs space outputs measure nonnegative function function µ-measurable respect second argument deﬁne log-linear model indexed parameters form next formalize notion self-normalized model. deﬁnition log-linear model self-normalized respect case self-normalizable self-normalizing w.r.t. some readers familiar generalized linear models also describe exponential family distributions linear dependence input. presentation strictly general notational advantages makes explicit dependence lets avoid tedious bookkeeping involving natural mean parameterizations. sets approximately normalizing parameters ﬁxed solutions uniform given upper bound normalizer variance feasible parameters nonconvex grows increases. previously motivated downstream uses models robust small errors resulting improper normalization would useful generalize deﬁnition normalizable distributions distributions approximately normalizable. exact normalizability conditional distribution deterministic statement—there either exist violates constraint. figure example sufﬁces single indicated surface make non-normalizable. approximate normalizability contrast inherently probabilistic statement involving distribution inputs. note carefully attempting represent representation approximate normalizability depends informally input violates self-normalization constraint large margin occurs infrequently problem; instead concerned expected deviation. also stage distinction penalization normalizer log-normalizer becomes important. normalizer necessarily bounded zero log-normalizer unbounded directions. applications concerned probabilities log-odds ratios expected normalizer close zero close inﬁnity. thus log-normalizer natural choice quantity penalize. deﬁnition log-linear distribution δapproximately normalized respect distribution case δ-approximately self-normalizable δ-approximately self-normalizing. sets δ-approximately self-normalizing parameters ﬁxed input distribution feature function depicted figure unlike self-normalizable sets inputs self-normalizing approximately self-normalizing sets parameters complex geometry. throughout paper assume vectors sufﬁcient statistics bounded norm natural parameter vectors norm vectors kinds finally assume input vectors constant feature—in particular every ﬁrst question must answer whether problem training self-normalized models feasible all—that whether exist exactly self-normalizable data distributions least δ-approximately self-normalizable distributions small section already gave example exactly normalizable distribution. fact large classes exactly approximately normalizable distributions. observation. given ﬁxed consider distribution supported normalizable. additionally every self-normalizable distribution characterized least deﬁnition provides simple geometric characterization self-normalizable distributions. example solution shown figure generally discrete consists repetitions ﬁxed feature function write provided convex level sets function form boundaries convex sets. particular exactly normalizable sets always boundaries convex regions simple example figure general expect real-world datasets supported precise class selfnormalizable surfaces. nevertheless often observed data practical interest low-dimensional manifolds within embedding feature spaces. thus whether sufﬁcient target distribution well-approximated self-normalizing one. begin constructing appropriate measurement quality approximation. deﬁnition input distribution d-close occasionally instructive consider special case boolean hypercube explicitly note assumption made. otherwise results apply general distributions continuous discrete. discussion concerned problem ﬁnding conditional distributions selfnormalize without concern well actually perform modeling data. relationship approximately self-normalized distribution true distribution essential. indeed concerned making good model always trivial make normalized one—simply take scale appropriately ultimately desire good self-normalization good data likelihood section characterize tradeoff maximizing data likelihood satisfying self-normalization constraint. achieve characterization measuring likelihood classical maximum likelihood estimator subject self-normalization constraint. speciﬁcally given pairs result lower-bounds likelihood explicitly constructing scaled version satisﬁes self-normalization constraint. speciﬁcally chosen normalizers penalized distance increase along span data guaranteed increase penalty. possible choose satisﬁes constraint. likelihood necessarily less used obtain desired lower bound. thus extreme distributions close uniform self-normalized little loss likelihood. extreme—distributions uniform possible? suitable assumptions form construction self-normalizing parameter achieve alternative characterization distributions close deterministic proposition suppose subset boolean hypercube ﬁnite conjunction element indicator output class. suppose additionally every input makes unique best prediction—that exists unique whenever result obtained representing constrained likelihood second-order taylor expansion true mle. terms likelihood vanish except remainder; upper-bounded ||ˆηδ|| times largest eigenvalue feature covariance matrix turn bounded e−cδ/r. favorable rate obtain case indicates all-nonuniform distributions also easy class self-normalization. together theorem suggests hard distributions must mixture uniform nonuniform predictions different inputs. supported results section next question whether corresponding lower bound; whether exist conditional distributions nearby distributions provably hard self-normalize. existence direct analog theorem remains open problem make progress developing general framework analyzing normalizer variance. issue likelihoods invariant certain changes natural parameters normalizers invariant. therefore focus equivalence classes natural parameters deﬁned below. throughout assume ﬁxed distribution inputs deﬁnition natural parameter values said equivalent denoted deﬁne optimal normalizer variance distribution associated natural parameter value. deﬁnition deﬁne optimal normalizer variance log-linear model associated natural parameter value important special case arises example multi-way logistic regression. setting show despite fundamental non-identiﬁability model variance still shown high parameterization distribution. theorem input distribution uniform exists high-level intuition behind results preceding section summarized follows predictive distributions expectation high-entropy low-entropy self-normalization results relatively small likelihood gap; mixtures highlow-entropy distributions self-normalization result large likelihood gap. generally expect increased tolerance normalizer variance associated decreased likelihood gap. section provide experimental conﬁrmation predictions. begin generating random sparse feature vectors initial weight vector order produce sequence label distributions smoothly interpolate low-entropy high-entropy introduce temperature parameter various settings draw labels selfnormalized model training pairs. addition synthetic data compare results empirical data self-normalized language model. likelihood function expected divergence uniform distribution. predicted theory likelihood increases decreases predictive distributions become peaked. model accuracy seen—as normalization constraint relaxed likelihood decreases. figure shows likelihood varies function quantity kl||unif). predicted seen extremes quantity result small likelihood gaps intermediate values result large likelihood gaps. motivated empirical success self-normalizing parameter estimation procedures log-linear models attempted establish theoretical basis understanding procedures. characterized self-normalizable distributions constructing provably easy examples self-normalizing training procedures bounding loss likelihood associated selfnormalization. addressed many important ﬁrst-line theoretical questions around selfnormalization study problem means complete. hope family problems attract study larger machine learning community; toward provide following list open questions else approximately self-normalizable distributions characterized? class approximately normalizable distributions described unlikely correspond perfectly real-world data. expect proposition generalized parametric classes relaxed accommodate spectral sparsity conditions. upper bounds theorem proposition tight? constructions involve relating normalization constraint norm general parameters large norm still give rise almost-normalized distributions. corresponding lower bounds exist? easy construct exactly selfnormalizable distributions empirical evidence hard distributions also exist. would useful lower-bound loss likelihood terms simple property target distribution. hard distribution theorem stable? related previous question. existence high-variance distributions less worrisome distributions comparatively rare. variance lower bound falls quickly given construction perturbed associated distribution still approximately self-normalizable good rate. already seen theoretical insights domain translate directly practical applications. thus addition inherent theoretical interest answers questions might applied directly training approximately self-normalized models practice. expect self-normalization increasingly many applications hope results paper provide ﬁrst step toward complete theoretical empirical understanding self-normalization log-linear models. devlin zbib huang lamar schwartz makhoul fast robust neural network joint models statistical machine translation. proceedings annual meeting association computational linguistics. vaswani zhao fossum chiang decoding large-scale neural language models improves translation. proceedings conference empirical methods natural language processing. gutmann hyv¨arinen noise-contrastive estimation estimation principle unnormalized statistical models. proceedings international conference artiﬁcial intelligence statistics. proof. either associated class associated zero element associated feature identically zero. thus assume associated correspond nonzero elements proof. lemma entry covariance matrix e−c||η|| e−cβ||ˆη||. features nonzero active matrix. thus gershgorin’s theorem maximum eigenvalue term equation qe−cβ||ˆη|| also upper bound sum.", "year": 2015}