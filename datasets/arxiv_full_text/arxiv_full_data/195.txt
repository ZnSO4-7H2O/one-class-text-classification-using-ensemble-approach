{"title": "WRPN: Training and Inference using Wide Reduced-Precision Networks", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "abstract": "For computer vision applications, prior works have shown the efficacy of reducing the numeric precision of model parameters (network weights) in deep neural networks but also that reducing the precision of activations hurts model accuracy much more than reducing the precision of model parameters. We study schemes to train networks from scratch using reduced-precision activations without hurting the model accuracy. We reduce the precision of activation maps (along with model parameters) using a novel quantization scheme and increase the number of filter maps in a layer, and find that this scheme compensates or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly reduce the dynamic memory footprint, memory bandwidth, computational energy and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN - wide reduced-precision networks. We report results using our proposed schemes and show that our results are better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks.", "text": "rameters) increase number ﬁlter maps layer. call networks using scheme wide reduced-precision networks scheme compensates surpasses accuracy baseline full-precision network. although number compute operations increase increase number ﬁlter maps layer compute bits required operation fraction required using full-precision operations. present results using scheme alexnet resnet ilsvrc- dataset. results show proposed scheme offer better accuracies ilsvrc- dataset computationally less expensive compared previously reported reduced-precision networks. further reduced-precision quantization scheme hardware friendly allowing efﬁcient hardware implementations networks servers deeply-embedded real-time deployments. prior works proposing reduced-precision networks work precision weights activation maps occupy larger memory footprint using mini-batches inputs. using minibatches inputs typical training dnns multi-modal inference figure shows memory footprint activation maps ﬁlter maps batch size changes different networks training inference steps. batch-size increases ﬁlter reuse aspect across batches inputs activation maps occupy signiﬁcantly larger fraction memory compared ﬁlter weights. based observation reduce precision activation maps dnns speed training inference steps well memory requirements. however straightforward reduction precision activation maps leads signiﬁcant reduction model accuracy. reported prior work well conduct sensitivity study reduce precision activation maps model weights alexnet network using ilsvrc- dataset. table reports ﬁndings. reducing precision activation maps hurts model accuracy much reducing precision ﬁlter parameters. re-gain model accuracy working reduced-precision operands increase number ﬁlter maps layer. increasing number ﬁlter maps increases layer compute complexity linearly. another knob tried increase size ﬁlter maps. however increasing ﬁlter spatial dimension incomputer vision applications prior works shown efﬁcacy reducing numeric precision model parameters deep neural networks also reducing precision activations hurts model accuracy much reducing precision model parameters. study schemes train networks scratch using reduced-precision activations without hurting model accuracy. reduce precision activation maps using novel quantization scheme increase number ﬁlter maps layer scheme compensates surpasses accuracy baseline full-precision network. result signiﬁcantly reduce dynamic memory footprint memory bandwidth computational energy speed training inference process appropriate hardware support. call scheme wrpn wide reduced-precision networks. report results using proposed schemes show results better previously reported accuracies ilsvrc- dataset computationally less expensive compared previously reported reduced-precision networks. deep learning robotics vision demands highly efﬁcient real-time solutions. promising approach deliver extremely efﬁcient solution numeric precision deep learning algorithms. operating lower precision mode reduces computation well data movement storage requirements. efﬁciency beneﬁts many existing works proposed low-precision deep neural networks even -bit ternary mode -bit mode however majority existing works low-precision dnns sacriﬁce accuracy baseline full-precision networks. further prior works target reducing precision model parameters primarily beneﬁts inference step batch sizes small. observe activation maps occupy memory compared model parameters batch sizes typical training. observation holds even inference batch size modern networks. based observation study schemes training inference using low-precision dnns reduce precision activation maps well model parameters without sacriﬁcing network accuracy. reduce precision activation maps reduced-precision resnet compute cost full-precision resnet. quantizing tensor values ﬁrst constrain weight values within range {-+} activation values within range followed k-bit quantization values within respective interval. since k-bit number represent numbers −)∗w quantized activation values reserved sign-bit case weight values hence quantized values. appropriate afﬁne transformations convolution operations done using quantized values followed scaling ﬂoatingpoint constants. makes scheme hardware friendly used embedded systems. quantization scheme results reported alexnet resnet. vision speech based applications seen tremendous success dnns. however dnns compute intensive workloads. paper present wrpn scheme reduce compute requirements dnns. prior works look reducing precision weights activations contribute signiﬁcantly memory footprint weight parameters using mini-batches thus aggressively reduce precision activation values. further increase number ﬁlter maps layer show widening ﬁlters reducing precision network parameters sacriﬁce model accuracy. scheme hardware friendly making viable deeply embedded system deployments useful robotics applications.", "year": 2017}