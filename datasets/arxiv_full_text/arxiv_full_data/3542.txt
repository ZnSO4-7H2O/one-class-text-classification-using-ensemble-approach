{"title": "Virtual Adversarial Ladder Networks For Semi-supervised Learning", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Semi-supervised learning (SSL) partially circumvents the high cost of labeling data by augmenting a small labeled dataset with a large and relatively cheap unlabeled dataset drawn from the same distribution. This paper offers a novel interpretation of two deep learning-based SSL approaches, ladder networks and virtual adversarial training (VAT), as applying distributional smoothing to their respective latent spaces. We propose a class of models that fuse these approaches. We achieve near-supervised accuracy with high consistency on the MNIST dataset using just 5 labels per class: our best model, ladder with layer-wise virtual adversarial noise (LVAN-LW), achieves 1.42% +/- 0.12 average error rate on the MNIST test set, in comparison with 1.62% +/- 0.65 reported for the ladder network. On adversarial examples generated with L2-normalized fast gradient method, LVAN-LW trained with 5 examples per class achieves average error rate 2.4% +/- 0.3 compared to 68.6% +/- 6.5 for the ladder network and 9.9% +/- 7.5 for VAT.", "text": "semi-supervised learning partially circumvents high cost labeling data augmenting small labeled dataset large relatively cheap unlabeled dataset drawn distribution. paper offers novel interpretation deep learning-based approaches ladder networks virtual adversarial training applying distributional smoothing respective latent spaces. propose class models fuse approaches. achieve near-supervised accuracy high consistency mnist dataset using labels class best model ladder layer–wise virtual adversarial noise achieves average error rate mnist test comparison reported ladder network. adversarial examples generated l-normalized fast gradient method lvan-lw trained examples class achieves average error rate compared ladder network vat. ladder networks virtual adversarial training seemingly unrelated deep learning methods successfully applied semi-supervised learning present view methods share statistical information labeled unlabeled examples smoothing probability distributions respective latent spaces. interpretation propose class deep models apply spatially-varying anisotropic smoothing latent spaces direction greatest curvature unsupervised loss function. models investigate apply virtual adversarial training cost addition ladder classiﬁcation denoising costs; models inject virtual adversarial noise encoder path. train models labeled examples class mnist dataset evaluating performance standard test adversarial examples generated using fast gradient method found models achieve state-of-the-art accuracy high stability labels class setting normal adversarial examples mnist. section outline ladder network virtual adversarial training present ladder network representing data hierarchy nested latent spaces. performed smoothing labeled unlabeled data distributions hierarchy thus sharing distributional information labeled unlabeled distributions coarse-to-ﬁne regime. latent space hierarchy instead particularly clever choice smoothing operator. illustration ladder network architecture. left noisy encoder activations additive gaussian noise layer outputs centre decoder; input layer corresponding layer noisy encoder combined denoising function form reconstructions right clean encoder weights shared noisy encoder; activations denoising targets. conceptual illustration virtual adversarial perturbation rvadv surface representing divergence pr]. indicate eigenvectors hessian ∇∇rd|r=. rvadv lies parallel dominant eigenvector magnitude equal hyperparameter ladder network supervised unsupervised tasks ladder network uses single autoencoder-like architecture added skip connections encoder decoder. labeled examples encoder used feed-forward classiﬁer unsupervised task full architecture used denoising autoencoder extra reconstruction costs intermediate representations denoising autoencoder additive spherical gaussian noise applied encoder activations interpret applying isotropic smoothing hierarchy latent spaces modelled ladder network. denoted smoothed activations mathematically output previous layer nonlinearity weights bias. ladderi) normal distribution injected noise given mean variance ladder network architecture illustrated figure ladder network layers. ladder network trained simultaneously minimize negative log-likelihood labeled examples denoising reconstruction cost layer unlabeled examples. speciﬁcally reconstruction cost squared error decoder activation noiseless encoder activation forward passes training time model seen monte carlo sampling equation virtual adversarial training virtual adversarial perturbations ﬁrst presented extending adversarial perturbations case labels. adversarial perturbations computed parameter dictating size perturbation; target distribution i.e. onehot vector true labels; output probabilities model parameters statistical divergence kullback–leibler divergence vaps generated approximating perturbation deﬁned practice detailed direction rvadv approximated second-order taylor expansion dominant eigenvector hessian matrix divergence respect perturbation eigenvector computed ﬁnite difference power method training time virtual adversarial perturbed examples rvadv added training virtual adversarial training loss added current loss. virtual adversarial training loss deﬁned average input data points minimization term seen smoothing operation since penalizes differences direction greatest curvature. range smoothing hence strength regularization controlled parameter magnitude vap. found ﬁxing coefﬁcient lvadv relative supervised cost tuning sufﬁcient achieve good results rather tuning both. ladder virtual adversarial costs approach applying anisotropic smoothing output space ladder network virtual adversarial cost term supervised cross-entropy cost unsupervised activation reconstruction cost optimized train ladder network. general formulation ladder loss term written activation layer corrupted encoder path gives ladder virtual adversarial costs layer-wise illustrated conceptually applying input images rather intermediate activations encoder gives ladder virtual adversarial cost illustrated figure ladder virtual adversarial noise alternatively smoothing applied ladder network injecting virtual adversarial perturbations activations layer corrupted encoder addition isotropic gaussian noise. layer computed form equation layer models proposed above addition noise input images only giving ladder virtual adversarial noise illustrated figure alternative case adding noise layer encoder ladder virtual adversarial noise layer-wise shown figure symmetric encoder. models trained epochs using adam optimizer initial learning rate linear learning rate decay epochs. models trained unlabeled batch size labeled batch size training labeled examples otherwise. vap’s generated l∞-norm. hyperparameters roughly tuned using bayesian optimization values used given appendix labeled data table shows average error rates mnist labeled examples proposed models benchmark implementations ladder network vat. mean standard deviation labels computed across training runs different random seeds selecting labeled data initializing weights; mean standard deviations labels computed training runs. expect high variability training runs labeled examples performance depends signiﬁcantly particular examples chosen. setting lvan lvan-lw notably highly stable achieving good performance. adversarial examples tested performance models adversarial examples generated using cleverhans implementation fast gradient method norms adversarial examples generated norm outperformed ladder network proposed models labeled examples. models showed improvement number labeled examples increased ladder network appear improve substantially. adversarial examples lvan-lw followed closely lvan outperformed models including ladder labels lvac lvac-lw performed better lvan lvan-lw ladder labels. ladder network performed poorly labels labels appears limited relatively high non-adversarial examples. relative strength lvac models examples compared lvan models performed better could relative strengths regularization cost based perturbations denoising cost. full results presented table introspection measuring smoothness virtual adversarial loss given equation measure lack local smoothness output distribution respect input images. expect ladder lvan models though explicitly minimize cost still perform smoothing reﬂected metric. cost computed models epochs training expected directly minimizes cost smooth metric. benchmark ladder network signiﬁcantly smoother fully supervised baseline despite explicitly minimising virtual adversarial cost. measure lvan lvac smoother lvan-lw lvac-lw respectively. suggests smoothing respect input image metric measures traded layer-wise models smoothing intermediate latent spaces. work conducted analysis ladder network virtual adversarial training semi-supervised learning proposed four variants model applying virtual adversarial training ladder network ladder virtual adversarial cost ladder layer-wise virtual adversarial cost ladder virtual adversarial noise ladder layer-wise virtual adversarial noise based manifold cluster assumptions semi-supervised learning hypothesised virtual adversarial training could improve classiﬁcation accuracy ladder network trained semi-supervised context. tested hypothesis mnist dataset training models training sets consisting labeled examples augmented full images mnist training unlabeled examples. measured performance error rate held-out test examples. found models signiﬁcantly lvan-lw improved performance ladder labels labels achieving state-of-the-art error rates. labels ladder baselines outperformed models. leads believe additional regularization provided vap’s ladder network useful task sufﬁciently challenging suggesting test models complex datasets svhn cifar-. additionally found models performed better ladder network adversarial examples. outperformed models adversarial examples models especially lvan-lw model achieved best performance few-label cases l-normalized adversarial examples. best-performing models overall based adding virtual adversarial noise corrupted encoder path ladder additional advantages lvac models proposed faster require fewer passes network produce stable consistent results. natural extension work would extend interpretation deep methods recently introduced temporal ensembling random data augmentation mean teacher method", "year": 2017}