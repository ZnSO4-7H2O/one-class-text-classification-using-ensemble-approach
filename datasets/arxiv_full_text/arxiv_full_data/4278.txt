{"title": "A Hebbian/Anti-Hebbian Network for Online Sparse Dictionary Learning  Derived from Symmetric Matrix Factorization", "tag": ["q-bio.NC", "cs.NE", "stat.ML"], "abstract": "Olshausen and Field (OF) proposed that neural computations in the primary visual cortex (V1) can be partially modeled by sparse dictionary learning. By minimizing the regularized representation error they derived an online algorithm, which learns Gabor-filter receptive fields from a natural image ensemble in agreement with physiological experiments. Whereas the OF algorithm can be mapped onto the dynamics and synaptic plasticity in a single-layer neural network, the derived learning rule is nonlocal - the synaptic weight update depends on the activity of neurons other than just pre- and postsynaptic ones - and hence biologically implausible. Here, to overcome this problem, we derive sparse dictionary learning from a novel cost-function - a regularized error of the symmetric factorization of the input's similarity matrix. Our algorithm maps onto a neural network of the same architecture as OF but using only biologically plausible local learning rules. When trained on natural images our network learns Gabor-filter receptive fields and reproduces the correlation among synaptic weights hard-wired in the OF network. Therefore, online symmetric matrix factorization may serve as an algorithmic theory of neural computation.", "text": "abstract—olshausen field proposed neural computations primary visual cortex partially modelled sparse dictionary learning. minimizing regularized representation error derived online algorithm learns gabor-filter receptive fields natural agreement physiological experiments. whereas algorithm mapped onto dynamics synaptic plasticity single-layer neural network derived learning rule nonlocal synaptic weight update depends activity neurons prepostsynaptic ones hence biologically implausible. here overcome problem derive sparse dictionary learning novel cost-function regularized error symmetric factorization input’s similarity matrix. algorithm maps onto neural network architecture using biologically plausible local learning rules. trained natural images network learns gabor-filter receptive fields reproduces correlation among synaptic weights hard-wired network. therefore online symmetric matrix factorization serve algorithmic theory neural computation. mammals primary visual cortex attractive well-studied target system major tasks computing orientationally selective responses gabor-filter orientationally nonselective computation successfully modeled olshausen field proposed neural network learns gabor-filter receptive fields ensemble natural images unsupervised fashion network appeals model rigorously derived principled cost function captures several salient anatomical physiological features networks implementation algorithm model sensory periphery feedback connections observed time algorithm appropriate learning rule derived cost function nonlocal synaptic weight update depends activity neurons prepostsynaptic ones therefore biologically implausible. paper propose novel cost function demonstrate derive neuronal dynamics local learning rules hebbian feedforward anti-hebbian lateral synaptic connections. demonstrate training network natural image ensemble yields gabor-filter receptive fields. also demonstrate application rules yields lateral connection weights obey relationship feedforward weights framework. addition framework accounts several salient properties biological networks predicts learning rate decays time activitydependent fashion agreeing experiments. therefore make step towards understanding mammalian neural computation general. paper organized follows. next section neural network summarize implementation. results present cost function regularized error squared input’s output’s similarity matrices derivation online algorithm sparse dictionary learning local learning rules; report results numerical simulations showing network performs similarly derive analytically relationship feedforward lateral synaptic connection weights network reproduces hard-wired constraint network; show online symmetric matrix factorization algorithm discover independent components whitened input data. discussion compare model biology; suggest matrix factorization generic model neural computation. neural network implementation algorithm. soft thresholding function. single-layer network. neuron applies inputs weighted feedforward connections minus outputs weighted lateral connections w’w. connection weights updated using nonlocal learning rules. model successfully reproduces several salient features primary visual cortex anatomy physiology overcompleteness cortical representation sparsity neural activity nonlinearity neural responses perhaps impressively receptive fields learned network ensemble whitened natural images gabor-filter patches resembling receptive fields neurons however major problem modeling algorithm single-layer network implementation learning rules nonlocal. specifically proposed learning rule requires synapse knows weights synapses belonging neurons prepostsynaptic neuron. mechanism exists communication brain clear model describe learning addition lateral connection weights model learned directly computed feedforward connection weights i.e. lateral connection matrix satisfies previously problem addressed network architecture local learning rules hebbian feedfoward anti-hebbian lateral connections however local learning rules postulated rather derived cost function. here derive single-layer network sparse overcomplete representation minimizing cost function comprising squared difference similarity matrices input output data sparsityinducing regularizer. next demonstrate network learns gabor patch receptive fields trained natural image ensemble. furthermore show relationship lateral feedforward connection weights agrees hard-wired network. interestingly motivate work briefly review model point biologically implausible aspect single-layer implementation. starting point model assumption vectorized image patches columns overcomplete dictionary weighted sparse vector neuronal activities obtain representation model minimizes squared representation error regularized -norm activity derive neural network algorithm minimized response sequentially presented natural image patches socalled online setting. specifically presented image find optimal value fixed fixed perform stochastic gradient descent respect feature vectors next discuss steps detail. component-wise soft-threshold function fig. equation viewed dynamics activity lateral connections fig. then represents total input currents neurons soft thresholding models rectifying nonlinearity biological neuron. network lateral connections implement explaining away competition neurons representing input signal. expressions lead natural single-layer network implementation algorithm matrices correspond feedforward lateral synaptic connection weights correspondingly. interestingly although synaptic weights appear explicitly cost function arise naturally online minimization algorithm sparse matrix factorization algorithm. single-layer network local learning rules. neuron applies soft thresholding inputs weighted feedforward connections minus outputs weighted lateral connections connection weights updated using hebbian anti-hebbian learning rules correspondingly. receptive fields learned whitened natural image ensemble. i-th matrix activity i-th output channel. loss term without regularizer used previously offline setting multidimensional scaling symmetric nonnegative matrix factorization constrained element-wise nonnegative whereas regularizer look familiar induces sparsity outer product rows hence activity output channels. motivation choosing particular form inducing regularizer become clear below. neuron. therefore simulate neural dynamics total iterations. initialize network connection weights gaussian random variables output activity zeros. initial synaptic learning rate result training network learns feedforward weight matrix plot neural filters acting natural image patches right-multiply whitening matrix plot rows fig. receptive fields appearance gabors filters varying orientation spatial frequency. receptive fields gabor functions )sinθ rotation angle equation amplitude represent widths gaussian envelope spatial frequency sinusoidal grating phase offset. present measured distribution spatial frequencies orientation fig. respectively. statistics similar network physiological measurements distribution output activity strong peak zero heavy tail fig. lateral connection weights strongly correlated fig. whereas network correlation predetermined algorithm network appeared result independently acting learning rules. lateral connections section present analytical derivation relationship connection matrices sparse symmetric matrix steady state solution factorization cost overcomplete regularization constant large steady state solution satisfies approximately thus feedforward synaptic weights updated according oja’s modification hebb rule activity dependent learning rate. best knowledge single-neuron learning rule previously derived multi-neuron case. moreover first time able derive oja-like version anti-hebbian rule also including regularizer cost function alters derivation instead derivative needs take sub-derivative affect learning rules adds soft thresholding inputs dynamics motivation choice regularizer become clear regularizer chosen order preserve magnitude threshold time. output activity binary spiking neuron threshold stays exactly same. output activity real corresponding firing rate model graded potential neurons constancy approximate confirmed numerical simulations. thus derived online algorithm implemented single-layer network architecture relying local learning rules. next simulate algorithm numerically training ensemble natural images. applied algorithm natural image ensemble. specifically pixel patches randomly extracted natural images whitened. extracted principal components presented sequentially network neuron feedforward lateral connections fig. patch presented coordinate descent update repeated times contains share also contains nonzero diagonal values diagonal. therefore identical establishing relationship feedforward lateral connection weights. statistics receptive fields neuronal activity computed network matches model mammalian physiology spatial frequencies gabor fits distribution orientation preference distribution activity among output units sparse heavy-tailed. extended straightforwardly offline problem. symmetric matrix factorization whitened input suitably chosen sparsity inducing regularizer used cost function. paper introducing novel cost-function derived online algorithm reproduces many features model implemented single-layer neural network relying local learning rules. therefore proposed biologically plausible implementation sparse coding hypothesis. weighted summation inputs soft thresholding. online algorithm maps onto neural network unit performs soft thresholding weighted inputs computation corresponds commonly used basic model biological neurons. although two-sided thresholding algorithm requires encountered biological neurons implemented pair neurons responsible positive negative inputs. neurons exist peripheral visual system vertebrates inveretebrates local hebbian anti-hebbian synaptic learning rules. learning rules derived consistent previously abstracted biological observations synaptic plasticity. crucially learning rules require synapse keep track activity neurons prepostsynaptic pair connects. anti-hebbian learning could implemented indirectly hebbian update synaptic weights inhibitory interneurons. dependence learning rate cumulative activity. learning rate synaptic weight update inversely proportional cumulative activity postsynaptic neuron variation plasticity time corresponds reports decaying activity dependent manner neural computation believe significance online symmetric matrix factorization goes beyond deriving sparse dictionary learning local hebbian anti-hebbian learning rules. speculate serves powerful versatile independent components argue symmetric matrix factorization used discover independent components whitened mixture i.e. perform independent component analysis successful recovering gabor filters natural images argument given seen alternative explanation numerical simulation results given section mixing matrix assumed invertible random source vector statistically independent elements. source assumed zero mean sparse e.g. laplace distributed. establish connection sparse symmetric matrix factorization first show whitened input orthogonal rotation original sources this rewrite whitened input terms assumed known whitening matrix substituting find druckmann chklovskii. mechanistic model early sensory processing based subtracting sparse representations. inadvances neural information processing systems carroll chang. idioscal generalization indscal allowing idiosyncratic reference systems well analytic approximation indscal. psychometric meeting princeton pehlevan d.b. chklovskii. hebbian/anti-hebbian network linear subspace tracking derivation form multidimensional scaling streaming data\" neural computation submitted. pehlevan d.b. chklovskii. hebbian/anti-hebbian network dervied online nonnegative matrix factorization cluster discover sparse features\" asilomar conference signals systems computers nov. elementary building block neural computation. indeed symmetric matrix factorization various constraints solve multiple computational objectives. argued formulated symmetric matrix factorization problem. furthermore unconstrained symmetric matrix factorization compute principal subspace streamed data nonnegative symmetric matrix factorization viewed clustering algorithm capable nonlinear feature discovery jointly tools represent formidable arsenal modeling neural computation. authors would like thank sanjeev arora alex genkin bruno olshausen eftychios pnevmatikakis christopher rozell zaid towfic helpful discussions. zylberberg murphy deweese sparse coding model synaptically local plasticity spiking neurons account diverse shapes simple cell receptive fields. plos computational biology", "year": 2015}