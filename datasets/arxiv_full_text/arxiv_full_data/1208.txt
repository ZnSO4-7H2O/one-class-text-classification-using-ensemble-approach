{"title": "Stereo Matching by Training a Convolutional Neural Network to Compare  Image Patches", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "We present a method for extracting depth information from a rectified image pair. Our approach focuses on the first stage of many stereo algorithms: the matching cost computation. We approach the problem by learning a similarity measure on small image patches using a convolutional neural network. Training is carried out in a supervised manner by constructing a binary classification data set with examples of similar and dissimilar pairs of patches. We examine two network architectures for this task: one tuned for speed, the other for accuracy. The output of the convolutional neural network is used to initialize the stereo matching cost. A series of post-processing steps follow: cross-based cost aggregation, semiglobal matching, a left-right consistency check, subpixel enhancement, a median filter, and a bilateral filter. We evaluate our method on the KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it outperforms other approaches on all three data sets.", "text": "present method extracting depth information rectiﬁed image pair. approach focuses ﬁrst stage many stereo algorithms matching cost computation. approach problem learning similarity measure small image patches using convolutional neural network. training carried supervised manner constructing binary classiﬁcation data examples similar dissimilar pairs patches. examine network architectures task tuned speed accuracy. output convolutional neural network used initialize stereo matching cost. series post-processing steps follow cross-based cost aggregation semiglobal matching left-right consistency check subpixel enhancement median ﬁlter bilateral ﬁlter. evaluate method kitti kitti middlebury stereo data sets show outperforms approaches three data sets. consider following problem given images taken cameras diﬀerent horizontal positions wish compute disparity pixel left image. disparity refers diﬀerence horizontal location object left right image—an figure input pair images left right camera. input images diﬀer mostly horizontal locations objects note objects closer camera larger disparities objects farther away. output dense disparity shown right warmer colors representing larger values disparity described problem stereo matching important many ﬁelds autonomous driving robotics intermediate view generation scene reconstruction. according taxonomy scharstein szeliski typical stereo algorithm consists four steps matching cost computation cost aggregation optimization disparity reﬁnement. following hirschm¨uller scharstein refer ﬁrst steps computing matching cost last steps stereo method. focus work computing good matching cost. propose training convolutional neural network pairs small image patches true disparity known output network used initialize matching cost. proceed number post-processing steps novel necessary achieve good results. matching costs combined neighboring pixels similar image intensities using cross-based cost aggregation. smoothness constraints enforced semiglobal matching left-right consistency check used detect eliminate errors occluded regions. perform subpixel enhancement apply median ﬁlter bilateral ﬁlter obtain ﬁnal disparity map. introduction large stereo data sets like kitti middlebury relatively stereo algorithms used ground truth information learn parameters models; section review ones did. general overview stereo algorithms scharstein szeliski kong used squared distances compute initial matching cost. trained model predict probability distribution three classes initial disparity correct initial disparity incorrect fattening foreground object initial disparity incorrect reasons. predicted probabilities used adjust initial matching cost. kong later extend work combining predictions obtained computing normalized cross-correlation diﬀerent window sizes centers. peris initialized matching cost ad-census used multiclass linear discriminant analysis learn mapping computed matching cost ﬁnal disparity. ground-truth data also used learn parameters probabilistic graphical models. zhang seitz used alternative optimization algorithm estimate optimal values markov random ﬁeld hyperparameters. scharstein constructed data stereo pairs used learn parameters conditional random ﬁeld. huttenlocher presented conditional random ﬁeld model nonparametric cost function used structured support vector machine learn model parameters. recent work focused estimating conﬁdence computed matching cost. haeusler used random forest classiﬁer combine several conﬁdence measures. similarly spyropoulos trained random forest classiﬁer predict conﬁdence matching cost used predictions soft constraints markov random ﬁeld decrease error stereo method. related problem computing matching cost learning local image descriptors problems share common subtask measure similarity image patches. brown introduced general framework learning image descriptors used powell’s method select good hyperparameters. several methods suggested solving problem learning local image descriptors boosting convex optimization hierarchical moving-quadrant similarity convolutional kernel networks convolutional neural networks works zagoruyko komodakis particular similar diﬀering mostly architecture network; concretely inclusion pooling subsampling account larger patch sizes larger variation viewpoint. typical stereo algorithm begins computing matching cost position disparities consideration. simple method computing matching cost absolute diﬀerences bold lowercase letters denote image locations. bold lowercase denotes disparity cast vector typewriter font names hyperparameters. example would patch size denote size since examples good matches constructed publicly available data sets attempt solve matching problem supervised learning approach. inspired successful application convolutional neural networks vision problems used assess well small image patches match. ground truth disparity maps either kitti middlebury stereo data sets construct binary classiﬁcation data set. image position true disparity known extract negative positive training example. ensures data contains equal number positive negative examples. positive example pair patches left right image whose center pixels images point negative example pair patches case. following section describes data construction step detail. figure fast architecture siamese network. sub-networks consist number convolutional layers followed rectiﬁed linear units similarity score obtained extracting vector input patches computing cosine similarity them. diagram well implementation cosine similarity computation split steps normalization product. reduces running time normalization needs performed position including opos instead setting zero stereo method used later particular found cross-based cost aggregation performs better network assigns matching costs good matches well near matches. experiments hyperparameter dataset never larger pixel. describe network architectures learning similarity measure image patches. ﬁrst architecture faster second produces disparity maps slightly less accurate. cases input network pair small image patches output measure similarity them. architectures contain trainable feature extractor represents image patch feature vector. similarity patches measured feature vectors instead image intensity values. fast architecture uses ﬁxed similarity measure compare feature vectors accurate architecture attempts learn good similarity measure feature vectors. ﬁrst architecture siamese network shared-weight sub-networks joined head sub-networks composed number convolutional layers rectiﬁed linear units following last layer. sub-networks output vector capturing properties input patch. resulting vectors figure accurate architecture begins convolutional feature extractors. extracted feature vectors concatenated compared number fullyconnected layers. inputs image patches output single real number interpret measure similarity input images. network trained minimizing hinge loss. loss computed considering pairs examples centered around image position example belongs positive negative class. output network positive example output network negative example margin positive real number. hinge loss pair examples deﬁned max. loss zero similarity positive example greater second architecture derived ﬁrst replacing cosine similarity measure number fully-connected layers architectural change increased running time decreased error rate. sub-networks comprise number convolutional layers rectiﬁed linear unit following layer. resulting vectors concatenated forward-propagated number fully-connected layers followed rectiﬁed linear units. last fully-connected layer produces single number which transformed sigmoid nonlinearity interpreted similarity score input patches. binary cross-entropy loss training. denote output network training example denote class training example; example belongs positive class example belongs negative class. decision diﬀerent loss functions architecture based empirical evidence. would preferred loss function architectures experiments showed binary cross-entropy loss performed better hinge loss accurate architecture. hand since last step fast architecture cosine similarity computation cross-entropy loss directly applicable. hyperparameters accurate architecture number convolutional layers sub-network number feature maps layer size convolution kernels size input patch number units fully-connected layer number fully-connected layers compute entire matching cost tensor ccnn would naively perform forward pass image location disparity consideration. following three implementation details kept running time manageable forward pass disparity consideration; maximum disparity kitti data middlebury data set. result fullyconnected part network needs times bottleneck accurate architecture. compute matching cost pair images sub-networks image fully-connected layers times maximum disparity consideration. insight important designing architecture network. could chosen architecture images concatenated presented network would imply large cost runtime whole network would need times. insight also development fast architecture layer times product feature vectors. outputs convolutional neural network enough produce accurate disparity maps errors particularly apparent low-texture regions occluded areas. quality disparity maps improved applying series post-processing steps referred stereo method. stereo method used inﬂuenced comprises cross-based cost aggregation semiglobal matching left-right consistency check subpixel enhancement median bilateral ﬁlter. information neighboring pixels combined averaging matching cost ﬁxed window. approach fails near depth discontinuities assumption constant depth within window violated. might prefer method adaptively selects neighborhood pixel support collected pixels physical object. cross-based cost aggregation build local neighborhood around location comprising pixels similar image intensity values hope pixels belong object. method begins constructing upright cross position; cross used deﬁne local support region. left position extends left long following conditions hold zhang suggest aggregation consider support regions images stereo pair. denote support regions left right image. deﬁne combined support region iteration number. repeat averaging number times. since support regions overlapping results change iteration. skip crossbased cost aggregation fast architecture crucial achieving error rate relatively expensive compute. denotes indicator function. ﬁrst term penalizes disparities high matching costs. second term adds penalty disparity neighboring pixels diﬀer one. third term adds larger penalty neighboring disparities diﬀer one. rather minimizing directions simultaneously could perform minimization single direction dynamic programming. solution would introduce unwanted streaking eﬀects since would incentive make disparity image smooth directions optimizing over. semiglobal matching minimize energy single direction repeat several directions average obtain ﬁnal result. although hirschm¨uller suggested choosing sixteen direction optimized along horizontal vertical directions; adding diagonal directions improve accuracy system. minimize direction deﬁne matching cost following recurrence relation hyperparameters base penalty discontinuities disparity map. base penalty reduced factor indicate strong image gradient larger factor indicate strong image gradient. value reduced factor considering vertical directions; ground truth small changes disparity much frequent vertical directions horizontal directions penalised less. semiglobal matching repeat cross-based cost aggregation described previous section. hyperparameters cbca iterations cbca iterations determine number cross-based cost aggregation iterations semiglobal matching. interpolation steps attempt resolve conﬂicts disparity predicted left image disparity predicted right image. denote disparity obtained treating left image reference image—this case d—and denote disparity obtained treating right image reference image. sometimes disagree correct disparity particular position detect conﬂicts performing left-right consistency check. label position applying following rules turn positions marked occlusion want disparity value come background. interpolate moving left position labeled correct value. positions marked mismatch nearest correct pixels diﬀerent directions median disparities interpolation. refer interpolated disparity dint. mc-cnn-acrt accurate architecture g¨uney geiger zbontar lecun vogel fast architecture yamaguchi vogel chen unpublished work menze geiger chakrabarti table highest ranking methods kitti data october setting column provides insight disparity computed indicates optical indicates temporally adjacent images indicates epipolar geometry computing optical ﬂow. error column reports percentage misclassiﬁed pixels runtime column measures time seconds required process pair images. kitti stereo data collection rectiﬁed image pairs taken video cameras mounted roof roughly centimeters apart. images recorded driving around city karlsruhe sunny cloudy weather daytime. images taken resolution ground truth disparities test withheld online leaderboard provided researchers evaluate method test set. submissions allowed every three days. error measured percentage pixels true disparity predicted disparity diﬀer three pixels. translated distance means that example error tolerance centimeters objects meters camera centimeters objects meters camera. kitti stereo data sets exist kitti newer kitti task computing stereo nearly identical newer data improving aspects optical task. data contains training testing images data contains training testing images. subtle important diﬀerence introduced newer data vehicles motion densely labeled glass included evaluation. emphasizes method’s performance reﬂective surfaces. best performing methods kitti data listed table accurate architecture ranks ﬁrst error rate third place leaderboard held previous work error rate changes reduced error augmenting data doubling number convolution layers reducing kernel size method second place uses matching cost computed previous work test error rate fast architecture would enough ﬁfth place method allowed appear public leaderboard. running time processing single image pair seconds accurate architecture seconds fast architecture. figure contains pair examples kitti data together predictions method. table presents frontrunners kitti data sets. error rates methods accurate architecture fast architecture occupying ﬁrst second place leaderboard. since submission paper figure examples predicted disparity maps kitti data set. note regions image cause problems census transform. fast accurate architecture perform better accurate architecture making fewer mistakes average. table summary middlebury stereo data sets. column number image pairs counts image pairs ground truth available. data sets additionally contain number image pairs ground truth disparities withheld; image pairs constitute test set. image pairs middlebury stereo data indoor scenes taken controlled lighting conditions. structured light used measure true disparities higher density precision kitti data set. data sets published separate works years paper refer middlebury data concatenation data sets; summary presented table scene data sets taken number lighting conditions shutter exposures typical image pair taken four lighting conditions seven exposure settings total images scene. online leaderboard similar provided kitti displays ranked list submitted methods. participants opportunity submit results test public leaderboard. rule stricter kitti data submissions allowed every three days. test contains images borrowed data sets. data provided full half quarter resolution. error computed full resolution; method outputs half quarter resolution disparity maps upsampled error computed. chose method half resolution images limited size graphic card’s memory available. rectifying pair images using standard calibration procedures like ones present opencv library results vertical disparity errors nine pixels middlebury data stereo pair data rectiﬁed twice using standard imperfect approach using precise correspondences perfect rectiﬁcation train network imperfectly rectiﬁed table methods middlebury stereo data october error column weighted average error upsampling full resolution runtime time seconds required process pair images. error measured percentage pixels true disparity predicted disparity diﬀer pixels; corresponds error tolerance pixel half resolution. error evaluation server default computed non-occluded pixels. ﬁnal error reported online weighted average ﬁfteen test images weights authors data set. table contains snapshot third newest version middlebury leaderboard. method ranks ﬁrst error rate substantial lead second placed meshstereo method whose error rate figure disparity maps produced method image pair middlebury data set. construct binary classiﬁcation data available image pairs training set. data contains million examples kitti million examples kitti million examples middlebury data set. training time input network batch pairs image patches. test time input entire left right image. could used entire images training well would allow implement speed optimizations described section several reasons preferred train image patches easier control batch size examples could shuﬄed batch contained patches several diﬀerent images easier maintain number positive negative examples within batch. minimized loss using mini-batch gradient descent momentum term trained epochs learning rate initially accurate architecture fast architecture. learning rate decreased figure example particularly diﬃcult image pair middlebury data set; white wall background practically textureless. accurate architecture able classify correctly. fast architecture doesn’t well still performs better census. input patch size conv layers conv feature maps conv kernel size layers units dataset dataset high dataset cbca intensity cbca distance cbca iterations cbca iterations blur sigma blur threshold table hyperparameter values used fast accurate architectures note hyperparameters concerning image intensity values apply preprocessed images images intensity values range factor epoch. number epochs initial learning rate learning rate decrease schedule treated hyperparameters optimized cross-validation. image preprocessed subtracting mean dividing standard deviation pixel intensity values. left right image stereo pair preprocessed separately. initial experiments suggested using color information improve quality disparity maps; therefore converted color images grayscale. post-processing steps stereo method implemented cuda network training done torch environment using convolution routines cudnn library opencv library used aﬃne transformation data augmentation step. augmenting data repeatedly transforming training examples commonly employed technique reduce network’s generalization error. transformations applied training time aﬀect runtime performance. randomly rotate scale shear training patches; also change brightness contrast. since transformations applied patches extracted images data augmentation step alter ground truth disparity ruin rectiﬁcation. parameters transformation chosen randomly pair patches epoch training example presented network second time random parameters selected. choose slightly diﬀerent transformation parameters left right image; example would rotate left patch degrees right diﬀerent data sets beneﬁted diﬀerent types transformations cases using wrong transformations increased error. middlebury data took advantage fact images taken diﬀerent lighting conditions diﬀerent shutter exposures training available images. data augmentation parameters used kitti kitti data sets. middlebury test data sets contains images worth mentioning classroom right image underexposed therefore darker left; djembe left right images taken diﬀerent light conditions. handle cases train time images either shutter exposure arrangements lights diﬀerent left right image. describing steps data augmentation introduce notation following word typewriter used denote name hyperparameter deﬁning word italic used denote number drawn randomly set. example rotate hyperparameter deﬁning possible rotations rotate number drawn randomly set. steps data augmentation presented following list rotate scale horizontal scale horizontal shear brightness contrast vertical disparity rotate diff horizontal scale diff horizontal shear diff brightness diff contrast diff table hyperparameters governing data augmentation aﬀect validation error. error column reports validation error particular data augmentation step used. last rows report validation errors without data augmentation. example validation error kitti data augmentation used steps except rotation used data augmentation steps used. measure runtime implementation computer nvidia titan graphics processor unit. table contains runtime measurements across range hyperparameter settings three data sets kitti middlebury half resolution table reveals fast architecture times faster accurate architecture. furthermore running times fast architecture seconds kitti seconds middlebury seconds tiny data set. also fully-connected layers responsible runtime accurate architecture hyperparameters controlling number convolutional layer number feature maps small eﬀect runtime. argue error rate method convolutional neural network superior stereo method. verify claim replacing convolutional neural network three standard approaches computing matching cost around position interest comparing intensity values pixel patch intensity value pixel center. center pixel brighter corresponding set. matching cost computed hamming distance census transformed vectors. normalized cross-correlation matching cost computes cosine similarity between left right image patch left right image patches viewed vectors instead matrices. function computed last layers fast architecture cens columns table contain results absolute diﬀerences census transform normalized cross-correlation kitti kitti middlebury data sets. validation errors last rows table table time seconds required compute matching cost time spent convolutional neural network without post-processing steps. time include computing matching cost twice left image taken reference image right image taken reference image. measure runtime function four hyperparameters controlling network architecture; example ﬁrst rows contain runtime number convolutional layers network increases six. last table contains running time entire method including post-processing steps. before abbreviate fast accurate architectures acrt. used compare methods. three data sets accurate architecture performs best followed fast architecture turn followed census transform. three best performing methods three data sets. error rates kitti kitti middlebury. absolute diﬀerences normalized cross-correlation matching costs produce disparity maps larger errors. visual comparison method census transform figures stereo method includes number post-processing steps cross-based cost aggregation semiglobal matching interpolation subpixel enhancement median bilateral ﬁlter. experiments excluded aforementioned steps recorded validation error last rows table allude importance post-processing steps stereo method. that post-processing steps removed validation error accurate architecture increases kitti kitti middlebury. post-processing steps stereo method semiglobal matching aﬀects validation error strongest. remove validation error increases kitti kitti middlebury. left-right consistency check eliminate errors occluded regions middlebury data set. error rate increased using left-right consistency check accurate architecture decided remove used supervised learning approach measure similarity image patches. therefore natural size data aﬀect quality disparity maps. answer question retrain networks smaller training sets obtained selecting random examples observe validation error decreases increase number training examples. experiments suggest simple strategy improving results stereo method collect larger data set. point training validation sets created stereo data either kitti kitti middlebury. evaluate performance method transfer learning setting experiments validation error computed diﬀerent data used training. example would middlebury data train matching cost neural network evaluate performance kitti data set. experiments give idea expected performance real-world application isn’t possible train specialized table numbers measure validation error particular post-processing step excluded stereo method. last rows tables interpreted diﬀerently contain validation error convolutional neural network validation error complete stereo method. example exclude semiglobal matching fast architecture achieves error rate kitti data error rate applying full stereo method. abbreviate method names fast architecture acrt accurate architecture absolute diﬀerences cens census transform normalized crosscorrelation matching cost. table validation error training test sets diﬀer. example validation error middlebury data used training fast architecture trained network tested kitti data set. results table unexpected. example validation error kitti lower using middlebury training compared kitti training even though kitti data obviously similar kitti middlebury. furthermore validation error kitti lower using fast architecture instead accurate architecture training kitti matching cost neural network trained middlebury data transfers well kitti data sets. validation error similar validation errors obtained networks trained kitti data sets. searching good hyperparameters daunting task—with search space growing exponentially number hyperparameters gradient guide better understand eﬀect hyperparameter validation error conduct series experiments vary value hyperparameter keeping others ﬁxed default values. results shown table summarized observing increasing size network improves generalization source code implementation available https//github.com/jzbontar/ mc-cnn. online repository contains procedures computing disparity training network well post-processing steps stereo method. accurate architecture produces disparity maps lower error rates previously published method kitti kitti middlebury data sets. fast architecture computes disparity maps times faster accurate architecture small increase error. results suggest convolutional neural networks well suited computing stereo matching cost even applications require real-time performance. fact relatively simple convolutional neural network outperformed previous methods well-studied problem stereo rather important demonstration power modern machine learning approaches.", "year": 2015}