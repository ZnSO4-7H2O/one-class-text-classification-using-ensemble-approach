{"title": "Deep Learning Sparse Ternary Projections for Compressed Sensing of  Images", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Compressed sensing (CS) is a sampling theory that allows reconstruction of sparse (or compressible) signals from an incomplete number of measurements, using of a sensing mechanism implemented by an appropriate projection matrix. The CS theory is based on random Gaussian projection matrices, which satisfy recovery guarantees with high probability; however, sparse ternary {0, -1, +1} projections are more suitable for hardware implementation. In this paper, we present a deep learning approach to obtain very sparse ternary projections for compressed sensing. Our deep learning architecture jointly learns a pair of a projection matrix and a reconstruction operator in an end-to-end fashion. The experimental results on real images demonstrate the effectiveness of the proposed approach compared to state-of-the-art methods, with significant advantage in terms of complexity.", "text": "compressed sensing sampling theory allows reconstruction sparse signals incomplete number measurements using sensing mechanism implemented appropriate projection matrix. theory based random gaussian projection matrices satisfy recovery guarantees high probability; however sparse ternary projections suitable hardware implementation. paper present deep learning approach obtain sparse ternary projections compressed sensing. deep learning architecture jointly learns pair projection matrix reconstruction operator end-to-end fashion. experimental results real images demonstrate effectiveness proposed approach compared state-of-the-art methods signiﬁcant advantage terms complexity. compressed sensing compressive sampling theory merges compression acquisition exploiting sparsity recover signals sampled drastically lower rate shannon/nyquist theorem imposes. results important impact numerous signal processing applications including efﬁcient processing analysis high-dimensional data audio image video assume ﬁnite-length real-valued signal yields compressed representation treated signal using sensing mechanism realized sensing projection matrix. linear measurement process described rm×n projection matrix vector containing obtained measurements. assume either sparse signal sparse representation respect suitable basis rn×n quasi-norm counting nonvanishing coefﬁcients treated signal. therefore obtain underdetermined linear system conventional theory based random gaussian random bernoulli matrices used recover n-dimensional s-sparse signal provided number measurements important issue considering random matrices matrices typically difﬁcult build hardware. difﬁculty storing matrices certain physical constraints measurement process makes challenging realize practice. moreover multiplying arbitrary matrices signal vectors high dimension lack fast matrix multiplication algorithm results high computational cost. deep learning emerging ﬁeld learns multiple levels representation data used successfully image processing tasks. existing work presented image superresolution image denoising compressed sensing deep learning also applied distributed quantized video paper adopt deep learning approach learn optimized projection matrix non-linear reconstruction mapping measurements original signal. order design projections suitable hardware implementation focus sparse matrices composed imposing sparsity binary constraints proposed network architecture. network trained image patches learned projection matrix used acquire images block-based manner. experimental results show high quality reconstruction achieved projections containing nonzero entries. rest paper organized follows. section review related work deep learning. section includes proposed approach section experimental results. conclusions drawn section compressed sensing introduced utilizing random projection matrices results concern matrices efﬁcient random projections leading fewer necessary measurements improvement reconstruction performance alternative studies proposed designs leverage prior knowledge signal akin theory compressed sensing prior information important research directions focus hardware implementation order achieve efﬁcient storage fast encoding decoding structured matrices proposed unfortunately constructions come cost poor recovery conditions. binary ternary matrices network architecture illustrated fig. consists sensing reconstruction module. network takes vectorized image patches size input. sensing layer projects n-dimensional input signal m-dimensional domain; thus number units layer sensing rate. sensing layer weights +}n×m corresponding projection matrix order learn simple linear projection matrix bias non-linearity activation function layer. ﬁrst layer reconstruction module scaling layer linearly scales outputs sensing layer learned factors layer consists hidden units connected hidden units sensing layer employed compensate loss induced binarizing sensing weights explained sec. nevertheless deployment projection matrix implemented sensing devices learned scaling factors kept reconstruction module. scaling layer followed hidden layers. hidden layers employ rectiﬁed linear unit activation function output layer linear fully connected layer size equal input dimension. layers reconstruction module except scaling layer fully connected followed batch-normalization layer general network training follows standard mini-batch gradient descent method. denote input reconstructed patches respectively employ mean squared error input reconstruction loss function number sample patches. constrast conventional mini-batch gradient descent method introduce sparsifying binarization step training sensing layer. particularly ﬁrst sparsify continuous-valued sensing weights rn×m sparse weights rn×m. step propose retain entries correspond top-k largest absolute values rest entries zero. refer procedure select function. selection weights applied column-wise row-wise whole matrix. nevertheless since update scaling layer presented below involves column-wise operator sensing weights perform select column-wise manner. neuron sensing layer connected elements input signal sparsity ratio. implementation-wise construct sparse binary mask }n×m entries equal corresponding largest weights sparse sensing weights updated according represents hadamard product. binarization step involves mapping sparse continuous valued weights rn×m sparse binary weights +}n×m. nevertheless sparse binarization step introduce loss want recover reconstruction. reason sensing layer followed scaling layer similar proposed layer yield fast computations; however proposed constructions deterministic impose restrictions matrix dimensions. moreover proven ldpc like matrices entries perform well long sparse. approach addresses hardware limitations recovery requirements. proposed projections extremely sparse allowing highly efﬁcient storage nonzero entries yielding fast computations acquisition sparse projection matrix would lead unacceptable recovery performance combined conventional reconstruction algorithms joint optimization sensing mechanism reconstruction process yields state-of-the-art results image recovery. ﬁrst work concerning recovery compressed measurements deep learning presented authors stacked denoising autoencoders jointly train non-linear sensing operator non-linear reconstruction mapping. reconstruction quality comparable state-of-the-art algorithms gain reconstruction time considerable. deep learning also employed approach follows similar principles focuses simplifying projection matrix. projection matrices dense matrices real-valued entries enforce projection matrix sparse matrix elements efﬁciently implemented. proposed algorithm based recent work simplifying deep neural networks applied image classiﬁcation tasks deep networks achieved great progress. neural networks trained binary weights study extends full binary neural networks binary weights binary hidden unit activations. different technique adds scaling factors compensate loss introduced weight binarization. another direction simplifying deep neural networks compress pre-trained networks. method proposed learns weights also connections producing sparsely-connected networks. extending connection pruning weight quantization huffman coding employed compress deep neural networks. algorithm incorporates ideas directions. nevertheless compared existing methods novelty fold propose sparsifying technique weights implementing sensing layer; combined binarization techniques method yields highly sparse ternary projection matrix. learned projections stored efﬁciently allow fast computations during acquisition therefore suitable hardware implementation. simplify ﬁrst layer network corresponds linear projection matrix allow reconstruction module non-linear order achieve high performance. next present proposed method efﬁcient compressed sensing images. section starts description network architecture followed proposed training algorithm. fig. network architecture left block corresponds sensing module right block corresponds reconstruction module. stands sparse binary sensing weights. denotes number units layer white blocks denote linear layers shaded blocks denote non-linear layers. algorithm proposed training algorithm sparse ternary projection matrix reconstruction weights step input patches weights learning rate output loss updated weights sparse binary weights procedure sparsify binarize sensing weights still lies continuous domain. denoting continuous sparse sparse binary sensing weights respectively scaling layer’s weights reconstruction weights column step training summarized algorithm order evaluate proposed algorithm carried experiments image recovery. employed ilsvrc validation including images training tested model testing sets images resolution ﬁrst testing consists images taken ilsvcr dataset provided authors second testing composed images randomly selected labelme dataset images converted grayscale experiments. reduce computational overhead experiments small image patches size pixels. total randomly sampled millions patches form training set. training input patches preprocessed subtracting mean dividing standard deviation. trained network using algorithm adam parameter update weights serves inverse mapping sparse binarized weights continuous weights thus output scaling layer approximation measurements corresponding continuous projections columns respectively. corresponds dense continuous weights hidden unit sensing layer. approximate αjθsb scale factor corresponding entry scaling weights values determined minimizing following mean square error respect column supp denotes positions nonzero entries treated vector. solution vector containing signs obtaining optimal solve optimal making derivative equal zero. considering sbθsb optimal given following existing training algorithms networks binary weights sparse binary weights forward backward propagation. high precision weights hand used parameter update accommodate small changes weights update step. training updated using gradient loss function respect θsb. noted even though contains discrete weights gradient loss respect batch size epochs learning rate decaying factor every epochs. training samples randomly shufﬂed epoch. avoid over-ﬁtting employ regularization reconstruction modules weight equal testing phase sampled overlapping patches test image stride pixels determined ﬁnal image reconstruction average patches’ reconstructions. methods evaluated using psnr values expressed concerning network architecture empirically number non-linear hidden layers hidden units since conﬁguration produces good trade-off training time reconstruction quality. sensing rate first experiment different sensing rates. vary mean psnr values ﬁrst testing shown table shown table overall reconstruction quality gets better larger sensing rates since information signal retained measurements. sparsity ratio next experiment different sparsity ratios using varying mean psnr values ﬁrst testing presented table dimension input obtain +}×. number nonzero entries column respectively. seen acceptable reconstruction performance achieved using extremely sparse projection matrices nonzero entries varying considerably improves performance difference negligible. network reaches peak performance performs slightly worse pointed nonzero entries projection matrix respectively. former enough fully cover −dimensional input signal. argue reason noticeable performance jump increasing training network experiences over-ﬁtting explains gives better performance result proposed sparse binary constraint considered extra regularizer network. comparison state proposed algorithm implements deep learning next experiment involves comparison method employs stacked denoising autoencoder jointly learn sensing layer reconstructor. select best algorithm referred o-nl-sda comparison. algorithm uses non-linear sensing mechanism overlapping image patches size results o-nl-sda ﬁrst testing taken obtain results second testing train o-nl-sda model training using proposed conﬁgurations results obtained conventional reconstruction algorithm namely basis pursuit using random ternary projections also presented. sparse binary ternary constructions like ones proposed could employed experiments constraints impose matrix dimensions. fair comparison sensing rate choose proposed method since yields best performance producing highly sparse projection matrix. comparison between selected methods ﬁrst testing shown table second testing mean psnr values o-nl-sda proposed algorithm respectively. seen proposed algorithm yields signiﬁcantly better results conventional reconstruction ternary sparse projections. despite sparse ternary matrix nonzero entries method outperforms o-nl-sda terms recovery performance. concerning speed reconstructor reported reconstructor implemented using feed-forward neural network perform orders magnitude faster convex optimization solver. clearly method provide convenient hardware implementation sensing mechanism fast reconstructor better reconstruction quality state art. paper propose novel algorithm train pair highly sparse ternary projection matrix reconstruction operator compressed sensing images. sparse ternary structure learned projection matrix exploited efﬁcient hardware implementations. experimental results real images show achieved reconstruction performance projection matrix nonzero binary entries corresponding reconstructor trained end-to-end proposed algorithm outperforms stateof-the-art methods. extremely sparse projection matrices nonzero entries learned using algorithm yield acceptable performance well. cand`es romberg robust uncertainty principles exact signal reconstruction highly incomplete frequency information ieee trans. inf. theory vol. mota deligiannis sankaranarayanan cevher rodrigues adaptive-rate reconstruction time-varying signals application compressive foreground extraction ieee trans. signal process. vol. image superresolution using deep convolutional networks ieee trans. pattern anal. mach. intell. vol. vincent larochelle lajoie bengio manzagol stacked denoising autoencoders learning useful representations deep network local denoising criterion mach. learn. res. vol. dec. exploiting correlations among channels distributed compressive sensing ieee internaconvolutional deep stacking networks tional conference acoustics speech signal processing song mota deligiannis rodrigues measurement matrix design compressive sensing side information encoder statistical signal processing workshop ieee applebaum howard searle calderbank chirp sensing codes deterministic compressed sensing measurements fast recovery app. comp. harm. anal. vol. binaryconnect training deep neural networks binary weights during propagations international conference neural information processing systems courbariaux hubara soudry el-yaniv bengio binarized neural networks training deep neural networks weights activations constrained arxiv e-prints feb. rastegari ordonez redmon farhadi xnor-net imagenet classiﬁcation using binary convolutional neural networks european conference computer vision dally deep compression compressing deep neural networks pruning trained quantization huffman coding international conference learning representations ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift international conference machine learning fei-fei imagenet large scale visual recognition challenge int. comput. vis. vol. russell torralba murphy freeman labelme database web-based tool image annotation int. comput. vis. vol. kingma adam method stochastic optimization international conference learning representations", "year": 2017}