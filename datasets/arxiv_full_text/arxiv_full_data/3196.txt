{"title": "Active Clustering with Model-Based Uncertainty Reduction", "tag": ["cs.LG", "cs.CV", "stat.ML", "62H30", "H.2.8; H.3.3; I.5.3; I.4.9"], "abstract": "Semi-supervised clustering seeks to augment traditional clustering methods by incorporating side information provided via human expertise in order to increase the semantic meaningfulness of the resulting clusters. However, most current methods are \\emph{passive} in the sense that the side information is provided beforehand and selected randomly. This may require a large number of constraints, some of which could be redundant, unnecessary, or even detrimental to the clustering results. Thus in order to scale such semi-supervised algorithms to larger problems it is desirable to pursue an \\emph{active} clustering method---i.e. an algorithm that maximizes the effectiveness of the available human labor by only requesting human input where it will have the greatest impact. Here, we propose a novel online framework for active semi-supervised spectral clustering that selects pairwise constraints as clustering proceeds, based on the principle of uncertainty reduction. Using a first-order Taylor expansion, we decompose the expected uncertainty reduction problem into a gradient and a step-scale, computed via an application of matrix perturbation theory and cluster-assignment entropy, respectively. The resulting model is used to estimate the uncertainty reduction potential of each sample in the dataset. We then present the human user with pairwise queries with respect to only the best candidate sample. We evaluate our method using three different image datasets (faces, leaves and dogs), a set of common UCI machine learning datasets and a gene dataset. The results validate our decomposition formulation and show that our method is consistently superior to existing state-of-the-art techniques, as well as being robust to noise and to unknown numbers of clusters.", "text": "abstract—semi-supervised clustering seeks augment traditional clustering methods incorporating side information provided human expertise order increase semantic meaningfulness resulting clusters. however current methods passive sense side information provided beforehand selected randomly. require large number constraints could redundant unnecessary even detrimental clustering results. thus order scale semi-supervised algorithms larger problems desirable pursue active clustering method— i.e. algorithm maximizes effectiveness available human labor requesting human input greatest impact. here propose novel online framework active semi-supervised spectral clustering selects pairwise constraints clustering proceeds based principle uncertainty reduction. using ﬁrst-order taylor expansion decompose expected uncertainty reduction problem gradient step-scale computed application matrix perturbation theory cluster-assignment entropy respectively. resulting model used estimate uncertainty reduction potential sample dataset. present human user pairwise queries respect best candidate sample. evaluate method using three different image datasets common machine learning datasets gene dataset. results validate decomposition formulation show method consistently superior existing state-of-the-art techniques well robust noise unknown numbers clusters. introduction semi-supervised clustering plays crucial role machine learning computer vision ability enforce top-down structure clustering methods user allowed provide external semantic knowledge— generally form constraints individual pairs elements data—as side information clustering process. efforts shown that constraints selected well incorporating pairwise constraints signiﬁcantly improve clustering results. computer vision variety domains semi-supervised clustering potential powerful tool including example facial recognition plant categorization first surveillance videos signiﬁcant demand automated grouping faces actions instance recognizing person appears different times different places someone performs particular action particular location tasks problematic traditional supervised recognition strategies difﬁculty obtaining training data—expecting humans label large strangers’ faces categorize every possible action might occur video realistic. however human probably reliably determine whether face images person recorded actions similar making quite feasible obtain pairwise constraints contexts. problem plant identiﬁcation similar even untrained non-expert humans probably generally determine plants species even expert could actually provide semantic label images. thus non-expert labor conjunction semi-supervised clustering reduce large uncategorized images small clusters quickly labeled expert. pattern holds true variety visual domains identifying animals speciﬁc classes man-made objects well nonvisual tasks document clustering however even using relatively inexpensive human labor attempt apply semi-supervised clustering methods large-scale problems must still consider cost obtaining large numbers pairwise constraints. number possible constraints quadratically related number data elements number possible user queries rapidly approaches point small proportion constraints feasibly queried. simply querying random constraint pairs space likely generate large amount redundant information lead slow improvement clustering results. worse davidson demonstrated poorly chosen constraints circumstances lead worse performance constraints all. overcome problems community begun exploring active constraint selection methods allow semi-supervised clustering algorithms intelligently select constraints based structure data and/or intermediate clustering results. active clustering methods divided categories sample-based samplepair-based. sample-based methods ﬁrst select samples interest query pairwise constraints based selected sample basu propose ofﬂine active k-means clustering based twostage process ﬁrst explores problem space performs user queries initialize grow sets samples known cluster assignments extracts large constraint known sample sets semi-supervised clustering. mallapragada present another active k-means method based min-max criterion also utilizes initial exploration phase determine basic cluster structure. also previously proposed different sample-based active clustering methods paper represents improvement extension works. contrast sample-pair-based methods directly seek pair constraints query. provide min-max framework identify informative pairs non-parametric kernel learning provide encouraging results. however complexity method problem) high limiting size data number constraints processed. wang davidson propose active spectral clustering methods designed twoclass problems poorly suited multiclass case. recently biswas jacobs propose method seeks pair constraints maximize expected change clustering result. proves meaningful useful criterion proposed method requires recomputing potential clustering results many times sample-pair selected thus slow. types current approaches suffer drawbacks current sample-based methods ofﬂine algorithms select constraints single selection phase clustering thus cannot incorporate information actual clustering results decisions. pair-based methods online high computational complexity nature pair selection problem candidate pairs every iteration) thus severely limited scalability. paper overcome limitations existing methods propose novel sample-based active spectral clustering framework using certain-sample sets performs efﬁcient effective sample-based constraint selection online iterative manner iteration algorithm sample yield greatest predicted reduction clustering uncertainty generate pairwise queries based sample pass human user update certain-sample sets clustering next iteration. usefully framework number clusters need known outset clustering instead discovered naturally human interaction clustering proceeds framework refer sample yield greatest expected uncertainty reduction informative sample active clustering algorithm revolves around identifying querying sample iteration. order estimate uncertainty reduction sample propose novel approximated ﬁrst-order model decomposes expected uncertainty reduction components gradient step-scale factor. estimate gradient adopt matrix perturbation theory approximate ﬁrst-order derivative eigenvectors current similarity matrix respect current sample. step-scale factor entropy-based models current cluster assignment ambiguity sample. describe framework uncertainty reduction formulation fully section compare method baseline stateof-the active clustering techniques three image datasets leaf images images common machine learning datasets gene dataset sample images seen figure results show given number pairs queried method performs signiﬁcantly better existing state-of-the-art techniques. background related work clustering uncertainty? clustering methods ultimately built relationships pairs samples. thus clustering method data perfectly reﬂects true relationship sample-pair method always achieve perfect result. practice however data imperfect noisy—the relationship pairs samples clear others highly ambiguous. moreover samples predominantly clear relationships samples data others predominantly ambiguous relationships. since goal clustering make decision assignment samples cluster despite inevitable ambiguity view overall sample-relationship ambiguity data uncertainty clustering result. semisupervised clustering eliminates amount uncertainty removing ambiguity pair relationships constraint. thus follows goal active clustering choose constraints maximally reduce total sample-assignment uncertainty. order achieve this however must somehow measure uncertainty contribution sample/sample-pair order choose expect yield greatest reduction. paper propose novel ﬁrst-order model matrix perturbation theory concept local entropy contribution selected sample details section sample-based uncertainty reduction? main reasons proposing sample-based approach rather sample-pair-based one. first uncertain pair uncertain either contains uncertain sample contains uncertain samples. latter case constraint samples extrapolate well beyond them yields limited information. second presence pair constraints every samples pair selection inherently higher complexity limits scalability pair-based approach. relation active learning. active query selection previously seen extensive ﬁeld active learning huang jain kapoor example offer methods similar select query uncertain samples. however active learning algorithms oracle needs know class label queried data point. approach applicable many semi-supervised clustering problems oracle give reliable feedback relationship pairs samples though implicitly label queried samples comparing exemplar samples representing cluster strictly pairwise queries. additionally sake comparison begin experiments exploration phase identiﬁes least member cluster real data reliable option. simply many clusters fully explore initially clusters appear additional data acquired certain clusters rare thus encountered time. cases active clustering framework adapt simply increasing number clusters. contrast active learning methods must initialized least sample class data allow online modiﬁcation class structure. recall certain-sample sets sets samples certain-sample constrained reside cluster samples different certain-sample sets guaranteed different clusters. groundtruth used experiments class corresponds speciﬁc certain-sample set. framework concept certain-sample sets translate sample selection pairwise constraint queries. given data x··· denote corresponding pairwise similarity matrix {wij} cannot-link initialize algorithm randomly select single sample {xi} {xi} change time notation indicate values iteration. noted that aside ability collect maximally useful constraint information human algorithm signiﬁcant advantage number clusters problem need known outset clustering instead discovered naturally human interaction algorithm proceeds. whenever queried pairwise constraints result creation certainsample increment account allows algorithm naturally overcome problem faced active clustering methods clustering methods general typically require parameter controlling either size number clusters generate. particularly useful image clustering domain true number output clusters unlikely initially available real-world application. conducted experiments evaluate method model selection; results encouraging presented section recalling steps framework proceed iteratively three main computational steps clustering pairwise constraints informative sample selection querying pairwise constraints. describe them. thus seek samples greatest impact clustering solution. strategy ﬁnding constraints though sample-pairs rather samples) estimate likely value constraint simulate effect constraint clustering solution. however approach unrealistic computationally expensive thus adopt indirect method estimating impact sample query based matrix perturbation theory local entropy selected sample. present details method section sample-based pairwise constraint queries presenting model informative sample selection brieﬂy describe selected sample. active selection system samplebased constraints pair-based selected informative sample must generate pairwise queries related sample. goal queries obtain enough information sample correct certainsample set. generate queries follows. second since certain sets recorded sample similarity values. sort samples based corresponding similarity then order descending similarity query oracle relation selected sample must-link connection. certain-sample containing relations cannot-link create certain-sample certain added regardless correspondingly updated adding value querying greater also update reﬂect newly discovered ground-truth cluster. since relation sample certain sets known generate pairwise constraints selected sample samples without submitting queries human. uncertainty reduction model informative sample selection described section spectral learning clustering algorithm. spectral learning spectral clustering pairwise constraints spectral clustering well-known unsupervised clustering method given symmetric similarity matrix denote laplacian matrix degree matrix otherwise. spectral clustering partitions samples groups performing k-means ﬁrst eigenvectors eigenvectors found incorporate pairwise constraints spectral clustering adopt simple effective method called spectral learning whenever obtain pairwise constraints directly modify current similarity matrix producing matrix wt+. speciﬁcally afﬁnity matrix determined informative sample selection section formulate problem ﬁnding informative sample uncertainty reduction. ultimately develop discuss model uncertainty reduction section deﬁne uncertainty dataset iteration conditioned current updated similarity matrix current certain-sample thus uncertainty expressed therefore objective function sample selection follows best knowledge direct computing uncertainty data. order optimize objective function consider querying pairs make chosen sample certain remove ambiguity clustering solution thus reduce uncertainty dataset whole. expected change clustering solution results making chosen sample certain certain decide query oracle relation sample become known corresponding updated spectral learning. therefore estimate inﬂuence sample gradient eigenvectors simply inﬂuences relevant values based thus deﬁne approximate model derivative uncertainty reduction note operate subset certain samples order save computation avoid redundancy. could simply entirety place would likely distort results. intuitively effect must-link constraint shift eigenspace representations constrained samples together. samples certain thus similar eigenspace representations expect additional constraints diminishing returns. second component uncertainty reduction estimation ∆h—the change ambiguity sample result querying sample. component serves step scale factor gradient according assumptions section sample queried ambiguity resulting sample reduced leads conclusion therefore problem estimating change ambiguity sample reduces problem estimating current ambiguity sample. problem still cannot solved precisely present reasonable heuristics estimating ambiguity sample. based concept entropy—speciﬁcally entropy probability distributions local cluster labels nonparametric structure model cluster probability first consider current clustering result cnc} cluster number clusters. deﬁne simple nonparametric model based similarity matrix determining probability belonging cluster clustering result arises values ﬁrst eigenvectors current similarity matrix. therefore impact sample query clustering result approximately measured estimating impact represents assignment-ambiguity represents reduction ambiguity querying ﬁrst-order derivative changes eigenvectors result ambiguity reduction. describe estimate gradient ambiguity reduction sections respectively. estimating gradient uncertainty reduction order solve must ﬁrst evaluate since know spectral learning information obtained oracle queries expressed changes similarity values queried point contained given this changes ambiguity always mediated changes approximate propose method based matrix perturbation theory first note graph laplacian iteration fully reconstructed eigenvectors corresponding eigenvalues then given small constant ﬁrst-order change fig. qualitative results small subset dataset cluster ground-truth; initial unconstrained clustering result; result pairwise constraints queried; result pairwise constraints queried. best viewed color. local nearest neighbors large similarity values relation given sample k-nearest neighbors point efﬁciently approximate entropy. neighbors need computed once ambiguity estimation process fast scalable. experiments parametric model cluster probability alternately simply eigenspace representation data produced recent semisupervised spectral clustering operation compute probabilistic clustering solution. elect learn mixture model embedded eigenspace current similarity matrix purpose value approximately represent combination approximate uncertainty gradient computed section allows evaluate effectively estimate uncertainty reduction every point there solving sample selection objective simple argmin operation. figure show qualitative example small subset dataset top-left ground-truth clusters shown dog-breed labels. three panes show clustering increasing numbers constraints selected method. notice clustering initially emphasizes image appearance violates many breed boundaries whereas constraints clusters increasingly correct. complexity analysis iteration must select query sample among possibilities applying uncertainty reduction estimation model potential sample. computing gradient component uncertainty model takes time sample number certain sets number clusters/eigenvectors. complexity uncertainty gradient evaluation iteration computing step scale factors costs nonparametric method used parametric method. regardless total complexity active selection process iteration compute gradient samples largest step scales. assuming yields overall complexity note results method shown paper obtained using fast approximation. also note large data cost method generally dominated spectral clustering itself worst case depending eigendecomposition method used sparseness similarity matrix). evaluate proposed active framework selection measures three image datasets gene dataset machine learning datasets seek demonstrate method generally workable different types data/applications wide range cluster numbers. face dataset face images extracted face dataset called pubfig large real-world face dataset consisting images people collected internet. unlike existing face datasets images taken completely uncontrolled settings noncooperative subjects. thus large variation pose lighting expression scene camera imaging conditions. subsets face- face- leaf dataset leaf images iphone photographs leaves monochrome background acquired leafsnap subset feature representations resulting similarity matrices leaf face datasets dataset images stanford dogs dataset contains images breeds dogs. extract subset containing images different breeds compute features used afﬁnity measured kernel. v-measure alternate metric determining cluster correspondence ground-truth classes clusters deﬁnes entropy-based measures completeness homogeneity clustering results computes harmonic mean two. homogeneity baseline state-of-the-art methods evaluate active clustering framework proposed active constraint selection strategies test following methods including number variations proposed method well baseline multiple state-of-the-art active clustering learning techniques. point forward refer proposed method uncertainty reducing active spectral clustering variants urasc quire binary-only active learning method computes sample uncertainty based informativeness representativeness sample. certain-sample framework generate requested sample labels pairwise queries. variant methods baseline figure compare parametric nonparametric methods well three partial urasc procedures three image sets sets varying numbers constraints. show results terms jaccard coefﬁcient v-measure witness similar patterns each. cases parametric nonparametric methods perform relatively similarly nonparametric modest lead most constraint counts. importantly methods consistently outperform random baseline particularly number constraints increases. methods always show notable improvement constraints provided—in contrast random baseline which best yields minor improvement. even relatively simple wine dataset clear randomly selected constraints yield little information. finally note complete methods consistently meet exceed performance corresponding partial methods. neither step-scale-only methods gradient-only method consistently yield better results every case combined method performs least on-par better cases signiﬁcantly better either results validate theoretical conception method showing combination gradient stepscale indeed correct represent active selection problem method’s performance driven combined information terms. comparison state-of-the-art active learning methods next compare methods active learning methods representatives pair-based techniques test three binary datasets order provide reasonable evaluation quire method binary-only. least methods outperforms quire pknn+al cases deﬁnitively losing constraint level sonar dataset. random baseline before methods competition generally increases number constraints. results suggests simply plugging active learning methods clustering also notable fact that active learning methods quire clearly superior signiﬁcant because like method quire seeks measure global impact given constraint pknn+al models local uncertainty reduction. lends support idea effect given query considered within context entire clustering problem terms local statistics. finally test methods existing active clustering techniques represent results visually figure methods appear charts applicable binary data. again methods present clear overall advantage competing algorithms many cases parametric nonparametric methods exceed performance others seeks estimate expected change clustering result potential query. however method much expensive fails dataset. also somewhat competitive methods binary nature greatly limits usefulness solving real-world semi-supervised clustering problems. methods still appears clear winner though nonparametric approach appears reliable given relative failure parametric approach leaf diabetes sets. comparison noisy input. previous experiments based assumption oracle reliably returns correct ground-truth response every time queried. previous works active clustering also relied assumption obviously general rule realistic—human oracles make errors problems ground-truth ambiguous subjective. speciﬁcally face leaf datasets used here amazon mechanical turk experiments shown human error face queries leaf queries. order evaluate active clustering method realistic setting performed experiments simulated query error rate face- datasets. plot results figure that improvement noticeably slower noisier previous experiments algorithms still demonstrate signiﬁcant overall advantage active passive clustering techniques. results also emphasize importance active query selection general noise added effect random queries actually negative. comparison unknown numbers clusters since advantage method ability dynamically discover number clusters based query results analyze approach effects performance time. thus method face- leaf datasets number clusters initially increasing certain-sample sets discovered. results shown figure results promising unknown-k results initially much lower converging time towards known-k results cluster structure discovered. datasets tested results appear eventually become indistinguishable. conclusion paper present novel sample-based online active spectral clustering framework actively selects pairwise constraint queries goal minimizing uncertainty clustering problem. order estimate uncertainty reduction according ﬁrst-order taylor expansion decompose gradient step-scale pairwise queries disambiguate sample largest estimated uncertainty reduction. experimental results validate decomposed model uncertainty support theoretical conception problem well demonstrating performance signiﬁcantly superior existing state-of-the-art algorithms. moreover experiments show method robust noise query responses functions well even number clusters problem initially unknown. avenue future research involves reducing computational burden active selection process adjusting algorithm select multiple query samples iteration. naive approach problem—selecting uncertain points— yield highly redundant information nuanced technique necessary. adjustment active spectral clustering method could become powerful tool large-scale online problems particularly increasingly popular crowdsourcing domain. acknowledgements grateful support part provided following grants career wnf--- darpa minds wnf--- darpa cssg n---. findings authors reﬂect views funding agencies. xing jordan russell distance metric learning application clustering side-information advances neural information processing systems learning nonparametric kernel matrices pairwise constraints international conference machine learning. chen zhang semi-supervised variable weighting clustering siam international conference data mining davidson wagstaff basu measuring constraintset utility partitional clustering algorithms european conference machine learning principles practice knowledge discovery databases. springer http//leafsnap.com/. khosla jayadevaprakash novel dataset ﬁne-grained image categorization first workshop fine-grained visual categorization conference computer vision pattern recognition kumar belhumeur biswas jacobs kress lopez soares leafsnap computer vision system automatic plant species identiﬁcation european conference computer vision. springer xiong johnson corso online active constraint selection semi-supervised clustering european conference artiﬁcial intelligence active incremental learning workshop xiong johnson corso uncertainty reduction active image clustering hybrid global-local uncertainty model aaai conference artiﬁcial intelligence biswas jacobs large scale image clustering active pairwise constraints international conference machine learning workshop combining learning strategies reduce label cost campbell winzeler steinmetz conway wodicka wolfsberg gabrielian landsman lockhart genome-wide transcriptional analysis mitotic cell cycle molecular cell vol. rosenberg hirschberg v-measure conditional entropy-based external cluster evaluation measure. conference empirical methods natural language processingconference computational natural language learning", "year": 2014}