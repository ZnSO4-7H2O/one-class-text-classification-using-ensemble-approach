{"title": "Batch Normalized Recurrent Neural Networks", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Recurrent Neural Networks (RNNs) are powerful models for sequential data that have the potential to learn long-term dependencies. However, they are computationally expensive to train and difficult to parallelize. Recent work has shown that normalizing intermediate representations of neural networks can significantly improve convergence rates in feedforward neural networks . In particular, batch normalization, which uses mini-batch statistics to standardize features, was shown to significantly reduce training time. In this paper, we show that applying batch normalization to the hidden-to-hidden transitions of our RNNs doesn't help the training procedure. We also show that when applied to the input-to-hidden transitions, batch normalization can lead to a faster convergence of the training criterion but doesn't seem to improve the generalization performance on both our language modelling and speech recognition tasks. All in all, applying batch normalization to RNNs turns out to be more challenging than applying it to feedforward networks, but certain variants of it can still be beneficial.", "text": "recurrent neural networks powerful models sequential data potential learn long-term dependencies. however computationally expensive train difﬁcult parallelize. recent work shown normalizing intermediate representations neural networks signiﬁcantly improve convergence rates feedforward neural networks particular batch normalization uses mini-batch statistics standardize features shown signiﬁcantly reduce training time. paper show applying batch normalization hidden-to-hidden transitions rnns doesn’t help training procedure. also show applied input-to-hidden transitions batch normalization lead faster convergence training criterion doesn’t seem improve generalization performance language modelling speech recognition tasks. applying batch normalization rnns turns challenging applying feedforward networks certain variants still beneﬁcial. recurrent neural networks received renewed interest recent success various domains including speech recognition machine translation language modelling so-called long short-term memory type particularly successful. often seems beneﬁcial train deep architectures multiple rnns stacked unfortunately training cost large datasets deep architectures stacked rnns prohibitively high often times order magnitude greater simpler models like n-grams this recent work explored methods parallelizing rnns across multiple graphics cards lstm type distributed layer-wise across multiple gpus bidirectional distributed across time. however sequential nature rnns difﬁcult achieve linear speed relative number gpus. another reduce training times better conditioned optimization procedure. standardizing whitening input data long known improve convergence gradientbased optimization methods extending idea multi-layered networks suggests normalizing whitening intermediate representations similarly improve convergence. however applying transforms would extremely costly. batch normalization used standardize intermediate representations approximating population statistics using sample-based approximations obtained small subsets data often called mini-batches also used obtain gradient approximations stochastic gradient descent commonly used optimization method neural network training. also shown convergence improved even whitening intermediate representations instead simply standardizing methods reduced training time convolutional neural networks order magnitude additionallly provided regularization effect leading state-of-the-art results object recognition imagenet dataset paper explore leverage normalization rnns show training time reduced. optimization feature standardization whitening common procedure shown reduce convergence rates extending idea deep neural networks think arbitrary layer receiving samples distribution shaped layer below. distribution changes course training making layer ﬁrst responsible learning good representation also adapting changing input distribution. distribution variation termed internal covariate shift reducing hypothesized help training procedure reduce internal covariate shift could whiten layer network. however often turns computationally demanding. batch normalization approximates whitening standardizing intermediate representations using statistics current minibatch. given mini-batch calculate sample mean sample variance feature along mini-batch axis however standardizing intermediate activations reduces representational power layer. account this batch normalization introduces additional learnable parameters respectively scale shift data leading layer form note bias vector removed since effect cancelled standardization. since normalization part network back propagation procedure needs adapted propagate gradients mean variance computations well. test time can’t statistics mini-batch. instead estimate either forwarding several training mini-batches network averaging statistics maintaining running average calculated mini-batch seen training. recurrent neural networks extend neural networks sequential data. given input sequence vectors produce sequence hidden states computed time step follows vanilla rnns activation function usually sigmoid function hyperbolic tangent. training networks known particularly difﬁcult vanishing exploding gradients commonly used recurrent structure long short-term memory addresses vanishing gradient problem commonly found vanilla rnns incorporating gating functions state dynamics time step lstm maintains hidden vector cell vector responsible controlling state updates outputs. concretely deﬁne computation time step follows sigmoid logistic sigmoid function tanh hyperbolic tangent function recurrent weight matrices input-to-hiddent weight matrices. respectively input forget output gates cell. however experiments batch normalization applied fashion didn’t help training procedure instead propose apply batch normalization input-to-hidden transition i.e. follows idea similar dropout applied rnns batch normalization applied vertical connections horizontal connections principle lstms batch normalization applied multiplication input-to-hidden weight matrices wx·. applications like speech recognition usually access entire sequences. however sequences variable length. usually using mini-batches smaller sequences padded zeroes match size longest sequence mini-batch. setups can’t frame-wise normalization number unpadded frames decreases along time axis leading increasingly poorer statistics estimates. solve problem apply sequence-wise normalization compute mean variance feature along time batch axis using speech task used wall street journal speech corpus. used split training evaluated models development set. audio transformed dimensional ﬁlter-banks deltas delta-deltas. forced alignments generated kaldi recipe trib leading clustered triphone states. memory issues removed training sequences longer frames leading training sequences. baseline model stack bidirectional lstm layers hidden units each followed size softmax output layer. weights initialized using glorot scheme biases zero. batch normalized model applied sequence-wise normalization lstm baseline model. networks trained using standard momentum ﬁxed learning rate ﬁxed momentum factor mini-batch size small lstm layers memory cells parameters initialized uniform distribution range back propagate across time steps gradients scaled according maximum norm gradients whenever norm greater train epochs halve learning rate every epoch medium lstm hidden size layers parameters initialized uniform distribution range apply dropout probability layers. back propagate across time steps gradients scaled according maximum norm gradients whenever norm greater train epochs divide learning rate every epoch large lstm layers memory cells parameters initialized uniform distribution range apply dropout layers. back propagate across time steps gradients scaled according maximum norm gradients whenever norm greater train epochs divide learning rate every epoch figure shows training development framewise cross entropy curves networks speech experiments. batch normalized networks trains faster overﬁts more. best results reported table comparable ones obtained experiments observed faster training greater overﬁtting using version batch normalization. last effect less prevalent speech experiment perhaps training bigger perhaps frame-wise normalization less effective sequence-wise one. language modeling task predict character time whereas predict whole sequence speech experiment. batch normalization also allows higher learning rates feedforward networks however since applied batch normalization parts network higher learning rates didn’t work well affected un-normalized parts well. experiments suggest applying batch normalization input-to-hidden connections rnns improve conditioning optimization problem. future directions include whitening input-to-hidden connections normalizing hidden state instead portion network.", "year": 2015}