{"title": "Sum-Product-Quotient Networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We present a novel tractable generative model that extends Sum-Product Networks (SPNs) and significantly boosts their power. We call it Sum-Product-Quotient Networks (SPQNs), whose core concept is to incorporate conditional distributions into the model by direct computation using quotient nodes, e.g. $P(A|B) = \\frac{P(A,B)}{P(B)}$. We provide sufficient conditions for the tractability of SPQNs that generalize and relax the decomposable and complete tractability conditions of SPNs. These relaxed conditions give rise to an exponential boost to the expressive efficiency of our model, i.e. we prove that there are distributions which SPQNs can compute efficiently but require SPNs to be of exponential size. Thus, we narrow the gap in expressivity between tractable graphical models and other Neural Network-based generative models.", "text": "present novel tractable generative model extends sum-product networks signiﬁcantly boosts power. call sum-product-quotient networks whose core concept incorporate conditional distributions model direct computation using quotient nodes provide suﬃcient conditions tractability spqns generalize relax decomposable complete tractability conditions spns. relaxed conditions give rise exponential boost expressive eﬃciency model i.e. prove distributions spqns compute eﬃciently require spns exponential size. thus narrow expressivity between tractable graphical models neural network-based generative models. sum-product networks class generative models capable exact tractable inference probability function directly modelled simple computational graph composed weighted product nodes known also arithmetic circuits following strict constraints connectivity. spns applied solve wide range tasks e.g. image classiﬁcation activity recognition missing data spns certain advantages areas cases expressiveness limiting factor fall behind contemporary generative models leveraging neural networks inference engine spns ﬁrst introduced hypothesized perhaps tractable distributions could represented eﬃciently spns. however hypothesis later proven false martens medabalimi speciﬁcally shown uniform distribution spanning trees complete graph vertices known tractable methods cannot realized spns unless size exponential reason behind limitation simple operations built efﬁciently computable function could approximated arbitrarily well polynomially-sized arithmetic circuits rather strict structural constraints spns required tractability. paper introduce extension spns call sum-product-quotient networks addresses limited expressivity spns. underlying concept behind extension incorporate conditional probabilities model direct computation i.e. repeatedly applying formula speciﬁcally show adding quotient node i.e. node inputs computes division relax structural constraints spns still model capable tractable inference internal node represents conditional probability input variables. moreover prove spqns represent distribution spns virtue extension exists distributions eﬃcient spqns require spns exponential size proving spqns exponentially expressively eﬃcient. rest article organized follows. sec. brieﬂy describe spns model basic concepts. followed sec. present spqns extension prove resulting model indeed tractable. sec. analyze expressive eﬃciency spqns respect spns. finally discuss implications model prior work plans future research sec. section give brief description sumproduct networks simplicity limit description probability models binary variables extension higher-dimensional continuous variables quite straightforward. computes unnormalized marginal visible variables. leaves univariate indicators binary variables i.e. special property respective indicators equal internal nodes compute either positive weighted product i.e. arithmetic circuit indicator variables deﬁned above. denote nodes product nodes indicator complete represents unnormalized distribution positive constraints weights computing normalization term typically tractable. generative model said possess tractable inference computing normalized probability function tractable. though general spns posses tractable inference limiting decomposable complete suﬃcient condition tractability undercomputing normalization term i.e. comx...xn∈{} equivalent evaluating thus normalized probability given also valid probability function shown peharz simply normalizing weights node ensures already normalized probability function need compute normalization factor furthermore restriction aﬀect expressiveness model namely unnormalized nodes could converted size normalized nodes. hence remainder article simply assume nodes normalized weights. important understand leads tractability. decomposability condition ensures children product node shared variables product distributions diﬀerent sets variables also normalized distribution product node decomposable represents normalized distribution long children represent normalized distributions. similarly completeness condition ensures children nodes exact scope weighted average distributions variables normalized weights also normalized distribution variables node represents normalized distribution children well. employing induction argument conditions combined together guarantee every node represent valid distribution. lastly learning model given structure typically carried simply according maximum likelihood principle several methods proposed ranging specialized expectation maximization algorithms gradient based methods e.g. simply performing stochastic gradient ascent. discussed sec. tractable distributions represented reasonable size limitation stems connectivity constraints imposed computational graphs spns achieve tractable inference. section describe extension spns relax constraints thus dramatically increase capacity eﬃciently represent tractable distributions. heart model introduction quotient node i.e. node inputs numerator denominator outputs division. quotient nodes natural interpretation hence call model sum-product-quotient networks spqns short. spns computational graph made product quotient nodes results model possessing tractable inference. ensure tractability spqns introduce restrictions generalizing conditions deﬁned sec. formally accordance notations sec. denote quotient nodes sentially represents conditional distribution variables scope give rise natural partition scope disjoint sets conditioning scope denoted cond eﬀective scope denoted partition tractable spqns node computes condideﬁnitions place ready present generalization conditions spqns. discussed sec. intuition behind conditions allow rather basic combine distributions deﬁned children given node respective scope form valid distribution scope parent node. broad terms simply carry idea spqns apply conditional distributions instead. product nodes translates essence applying conditions respect eﬀective scope node instead general scope formalize variables conditional scope cond treat conditional distributions children simply distributions variables eﬀective scope. complete respect eﬀective scope following arguments sec. represents distribution long children well. logic also applied product nodes restrictive form conditional case depends probability eﬀective scope conditioned variables eﬀective scope representing dependencies children directed graph child represents valid conditional distribution scope graph acyclic eﬀectively deﬁnes bayesian network factorization conditional probability scope hence valid conditional distribution. point important note conditional actually relaxed versions unconditional counterparts. first notice conditional scope empty i.e. sub-graph rooted contains product nodes words sub-graph conditional equivalent d&c. second importantly notice conditional scope nonempty conditional decomposability allows taking product nodes overlapping scopes forbidden stricter decomposability constraint. entails conditional spqns allow richer structures spns. last ensure tractability spqns must also introduce condition quotient nodes equivalent classical spns. following condition captures motivation quotient node compute conditional distributions stricter restriction leads concrete construction tractable spqn. speciﬁcally deﬁne building block operator composed product quotient nodes guarantees resulting model tractable call conditional mixing operator motivation behind construction connection conditional probability mixture model. notice numerator essentially represents mixture model decomposable mixing components divided sets according denominator represents marginalization variables relating deﬁnition strong conditional soundness required tractability weaker conditional soundness ensure eﬃcient sampling discussed sec. conclude formally proving spqn meets conditions henceforth referred tractable spqn results tractable generative model described following theorem theorem conditionally decomposable conditionally complete conditionally sound spqn random binary variables values variables found cond holds normalized probability given spqn ﬁxed structure meets tractability conditions theorem output diﬀerential probability function data learn parameters simply maximizing likelihood data gradient ascent methods commonly employed spns deep learning methods. adjusting methods typically used learn spns e.g. em-type algorithms parameter learning various suggested structure learning algorithms deferred future works. though theorem provides suﬃcient conditions spqns tractable prescriptive exactly models must structured. speciﬁcally conditionally decomposable conditionally complete conditions quite simple follow generally clear adhere conditionally sound condition. address next section. discussed previous section tractable spqns must comply conditionally sound condition verifying given model adheres nontrivial. section suggest instead follow left show also conditionally sound. achieved induction argument depth spqn composed valid cmos assume nodes given depth strictly positive conditionally sound hence also represent valid distributions according theorem assumption internal product nodes depth valid node also represent valid distributions already conditionally d&c. hence directly compute marginalization variables eﬀective scope b-type children conclude proof conditional soundness. strong conditional soundness follows conditional soundunlike conditional soundness practical validate nodes given spqn valid. simply start root recursively validate children given node valid base case nodes connected indicator nodes govern ﬁrst condition def. proceed verifying internal product nodes follow conditional constraints simply testing eﬀective conditional scopes according def. def. though valid cmos pave tractable spqns raise question lost process. indeed conditional soundness allows richer valid structures valid cmos e.g. allow distribution denominator numerator quotient node deﬁned completely diﬀerent sub-graphs unlike cmos share children. determine signiﬁcant expressivity cases important property spqn composed valid cmos eﬀectively represented model hence restriction least expressive spn. sec. show fact signiﬁcantly expressive spns. edge case spns demands unique treatment exists node connected both valid must positive weights indicator leaves. scenario instead arbitrarily approxiscope. section leverage relations describe generative process spqns showing sampling spqn eﬃcient inference strongly conditional soundness constraint. ability eﬃciently draw samples probabilistic model highly desirable trait many applications e.g. completing missing values introspection learned models. sampling tractable spqn model follow general steps sampling spn. begin root node graph stochastically traverse nodes according parameters model reach indicator nodes representing sampled value respective random variable. spns traversal follow simple rules encounter product node decomposable child distribution separate sets variables hence recursively sample child separately; encounter node sample children according categorical distribution deﬁned respective weights. given spqns extensions spns generative process seen simply generalization traversal rules spns. however distinctive property nodes represent conditional distributions calls adjustments. namely required traverse graph also keep track values already sampled process pass along nodes depend algo. follow traversal workﬂow spns described above following adjustments quotient nodes directly traverse numerator child denominator serve normalization factor. product nodes though eﬀective scopes children disjoint sets could processed separately spns dependencies induced conditional scopes child require sampling according topological order dependencies graph. additionally possibility eﬀective scope child nodes already sampled case simply skip nodes probability sampling child longer given weights also marginal probability already sampled take account topological sort applied children product nodes time operation required every sampling. conclusion sampling tractable spqn also strongly conditionally sound e.g. composition valid cmos eﬃcient spns. distribution spqns realize spns cannot unless size exponential length input size spqn deﬁned number internal nodes. speciﬁcally show tractable spqns represent strictly positive distribution sampling undirected triangle-free graph vertices edge represented random binary number spns polynomial size cannot represent even approximate distributions. deﬁnition falsely appears lead efﬁcient realization spns strictly positive distribution triangle-free graphs simply deﬁne node potential triangle positive legal i.e. least edges part graph take product nodes guarantee triangles legal. speciﬁcally deﬁne node triplet product nodes modify weights output normalized probability function. however because node meet completeness condition children diﬀerent scopes product node nodes meet decomposability condition edge present multiple triplets i.e. multiple child nodes resulting non-disjoint scopes. computing normalization factor practice intractable. generally show approximating strictly positive distribution triangle-free graphs must exponentially large proof sketch. modiﬁed proof similar theorem martens medabalimi showed approximate arbitrarily well probability function uniform distribution spanning trees complete graph contrast spns tractable spqns eﬃciently realize least strictly positive distributions triangle-free graphs size polynomial case spqns built cmos exact realization replaced arbitrarily good approximation without size increase. formalized following theorem theorem exists tractable spqn exactly realizing probability function strictly positive distribution triangle-free graphs vertices size spqn case spqns composed strictly cmos instead exact realization approximate said distribution arbitrarily well size proof sketch. taking inspiration failed attempt realize distribution spns construct tractable spqn realize distribution eﬃciently. before begin examining potential triangles instead directly modelling constraints individually group largest edge edge respective group triangles deﬁne conditional probability edge conditioned edges participating triangles conditional probability non-zero triangles include edge part graph. edges part triangle largest edge simply deﬁne node represent equal probability including edge not. finally simply take product conditional distributions edge giving rise normalized probability function edges non-zero edges represent triangle free graph. app. complete proof. work address limited expressive eﬃciency spns martens medabalimi proven incapable approximating even simple tractable distributions unless size exponential number variables. mitigate limitation spns presented novel extension spns call sum-product-quotient networks spqns short. spqns introduce node type computes quotient inputs part enabled relax strict structural conditions commonly used ensure tractability spns. requiring less strict conditions tractability proven spqns strict superset spns moreover spqns exponentially expressive eﬃcient spqns. vast literature analyzing expressivity arithmetic circuits particularly spns notable amongst work sharir shashua compared expressive eﬃciency convolutional overlapping receptive ﬁelds equivalent sub-class spns following tree-structure partitioning scopes convac overlaps equivalent spn. found simply introducing overlaps i.e. breaking decomposability condition eﬀect exponentially increasing expressive eﬃciency model. closer examination overlapping convac reveals shares construct numerator cmos nodes without denominator thus results could trivially adapted spqns following similar architecture. entails distributions spqns represent eﬃciently spns cannot showed sec. almost distributions realized spqns cannot realized tree-like spns known also latent tree models unless exponential size. nevertheless important stress importance results separate spqns spns conceivable structure small sub-class spns. recently telgarsky examined relations neural networks rational functions i.e. quotient polynomials well model called rational networks neural network activation functions limited rational functions. found neural network relu activations could approximated arbitrarily well similarly size rational network reverse true well. though might seem suggest spqns could neural networks hoover confused sum-product trees restricted sub-class spns every product nodes single parent opposed limiting nodes single parent non-overlapping convacs. broader literature proposal introducing quotient node previously considered deemed unnecessary. argument based proof circuits compute polynomial functions quotient nodes could replaced single negation node words quotient node power acs. despite negative outcome apply case accounts assumes output circuit identically polynomial function instead rational function since proof relies structural properties polynomial namely degree homogenous decomposition cannot adapted case. apply monotone weights restricted non-negative case spns negation allowed. context proven even single negation gate leads exponential separation monotone circuits quotient nodes could replaced negation reverse generally true hence last result trivialize own. overall given results might suggest role quotient nodes reexamined acs. prove spqn model exponentially expressive spns increase expressive eﬃciency come without cost. great advantages spns possess tractable inference also tractable marginalization uncommon ability amongst generative models many uses e.g. missing data however relax decomposability conditional decomposability means spqns eﬀectively induce partial ordering input variables limit tractable marginalization subsets variables agree ordering. appear fewer tasks beneﬁt signiﬁcantly tractable marginalization compared tractable inference cases required spns even limited expressivity still advantage spqns. limitation address future works detailed below. additionally martens medabalimi shown mild assumptions suﬃcient also necessary tractable marginalization entails possible relaxation would result losing general tractable marginalization hence speciﬁc case spqns. models based neural networks notable amongst nade pixelrnn pixelcnn despite signiﬁcant diﬀerences underlying operations spqns models also similarities shared concepts. speciﬁcally model based inducing partial ordering input variables modelling conditional probabilities subsets them main diﬀerence probabilities represented. employ neural networks black model them leverage interpretable spns compose conditional distributions hierarchy. conjecture embedded hierarchy conditional distributions used model leads advantage terms expressive capacity while addition interpretable nature inner-workings model many real-world applications. lastly conducted preliminary experiments demonstrating practical advantages spqns spns app. nevertheless still remains veriﬁed superior expressive power translates real-world applications task tackle future works. beyond that spqns give rise many straightforward extensions marginalization naturally supported spqns induce normalized distributions subset input variables generally consistent other. joint training spqns random subsets variables could suﬃcient ensuring consistency induced marginal distributions. convolutional spqns model natural formulation convnet-like generative model following theoretical architecture convacs overlaps unparalleled success convnets theoretical advantages spqns potential rivalling neural tractable generative models. benigno uria marc-alexandre cote karol gregor iain murray hugo larochelle. neural autoregressive distribution estimation. journal machine learning research aaron oord kalchbrenner lasse espeholt koray kavukcuoglu oriol vinyals alex graves. conditional image generation pixelcnn decoders. advances neural information processing systems", "year": 2017}