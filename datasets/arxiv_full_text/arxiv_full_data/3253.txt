{"title": "Generalized Ambiguity Decomposition for Understanding Ensemble Diversity", "tag": ["stat.ML", "cs.CV", "cs.LG", "I.5"], "abstract": "Diversity or complementarity of experts in ensemble pattern recognition and information processing systems is widely-observed by researchers to be crucial for achieving performance improvement upon fusion. Understanding this link between ensemble diversity and fusion performance is thus an important research question. However, prior works have theoretically characterized ensemble diversity and have linked it with ensemble performance in very restricted settings. We present a generalized ambiguity decomposition (GAD) theorem as a broad framework for answering these questions. The GAD theorem applies to a generic convex ensemble of experts for any arbitrary twice-differentiable loss function. It shows that the ensemble performance approximately decomposes into a difference of the average expert performance and the diversity of the ensemble. It thus provides a theoretical explanation for the empirically-observed benefit of fusing outputs from diverse classifiers and regressors. It also provides a loss function-dependent, ensemble-dependent, and data-dependent definition of diversity. We present extensions of this decomposition to common regression and classification loss functions, and report a simulation-based analysis of the diversity term and the accuracy of the decomposition. We finally present experiments on standard pattern recognition data sets which indicate the accuracy of the decomposition for real-world classification and regression problems.", "text": "ﬁrst term right hand side square bias difference target expected prediction distribution second term measures variance ensemble. reduces experts functional form training drawn convex mixture training sets {dk}k mixture weights {wk}k classiﬁcation regression. assume classiﬁer estimating posterior distribution label case classiﬁcation. often encountered practice example case support vector machines. note prior work done expert. hence limiting domain twice-differentiability continuity second derivative reasonable assumption. next lemma presents ambiguity decomposition squared error loss function denote diversity measures spread expert predictions ensemble’s predictions f∀k. diversity non-negative convex loss function jensen’s inequality furthermore diversity depends loss function true target need compute maximum minimum second derivative function monotonically increasing achieves maxima monotonically decreases note deﬁnition hence maximum governs spread predictions around mean. varied around true label picked regression loss functions depend distance prediction target. analysis also extends easily classiﬁcation loss functions. generated monte carlo samples theorem reduces ambiguity decomposition. diversity term also remains nearly constant maximum likelihood estimator variance diversity corrects bias weighted expert loss actual used theorem’s proof accurate expert predictions close true label. also note bound approximation error lgad| follows general trend error tight. ensemble becomes since gradient boosting estimates given estimates fk}k− assumes vkfk close words assumes base learner weak contributes information learned vkfkl′ maximizing correlation vkfk negative loss function gradient −l′. central idea used training ensemble using gradient boosting. taylor series expansion highlights differences gradient boosting nential loss function. observe gradient boosting minimum error ensemble mean near decision boundary however approximation becomes poor move away decision boundary. lgad provides good approximation", "year": 2013}