{"title": "Model compression as constrained optimization, with application to  neural nets. Part I: general framework", "tag": ["cs.LG", "cs.NE", "math.OC", "stat.ML"], "abstract": "Compressing neural nets is an active research problem, given the large size of state-of-the-art nets for tasks such as object recognition, and the computational limits imposed by mobile devices. We give a general formulation of model compression as constrained optimization. This includes many types of compression: quantization, low-rank decomposition, pruning, lossless compression and others. Then, we give a general algorithm to optimize this nonconvex problem based on the augmented Lagrangian and alternating optimization. This results in a \"learning-compression\" algorithm, which alternates a learning step of the uncompressed model, independent of the compression type, with a compression step of the model parameters, independent of the learning task. This simple, efficient algorithm is guaranteed to find the best compressed model for the task in a local sense under standard assumptions.  We present separately in several companion papers the development of this general framework into specific algorithms for model compression based on quantization, pruning and other variations, including experimental results on compressing neural nets and other models.", "text": "compressing neural nets active research problem given large size state-of-the-art nets tasks object recognition computational limits imposed mobile devices. give general formulation model compression constrained optimization. includes many types compression quantization low-rank decomposition pruning lossless compression others. then give general algorithm optimize nonconvex problem based augmented lagrangian alternating optimization. results learning-compression algorithm alternates learning step uncompressed model independent compression type compression step model parameters independent learning task. simple eﬃcient algorithm guaranteed best compressed model task local sense standard assumptions. present separately several companion papers development general framework speciﬁc algorithms model compression based quantization pruning variations including experimental results compressing neural nets models. large neural network models become central component state-of-the-art practical implementations solution various machine learning artiﬁcial intelligence problems. include example classiﬁcation problems involving images audio text reinforcement learning problems involving game playing robot manipulation navigation. also resulted enormous increase interest deep neural models researchers academy industry even non-experts public general evidenced amount scientiﬁc papers blog entries mainstream media articles published practical successes diﬃcult problems ocurred thanks availability large-scale datasets massive computational power provided gpus particularly well suited kind linear algebra operations involved training neural notable characteristic deep neural nets seems distinguish machine learning models ability grow data. size training grows continue increase classiﬁcation accuracy increasing size neural unlike example linear model whose classiﬁcation accuracy quickly stagnate data keeps growing. hence continue improve accuracy neural making bigger training data. means expect ever larger neural nets future practical applications. indeed models reported literature computer vision gone less million weights millions recent works models exceeding billions weights large size resulting model cause important practical problem intends deploy resource-constrained target device mobile phone embedded systems. large neural trained resource-rich setting e.g. gpus multicore architecture large memory disk model designer explore diﬀerent model architectures hyperparameter values etc. model best accuracy validation found. ﬁnal large model ready deployed practice. however target device typically restrictive computation constraints memory size speed arithmetic operations clock rate energy consumption etc. make impossible accommodate large model. words deploy models certain size. download bandwidth also signiﬁcantly limited apps mobile phones software cars. problem attracted considerable attention recently. important facts weigh upon known time among researchers. ﬁrst large neural nets currently know train optimal accuracy contain signiﬁcant redundancy makes possible smaller neural nets comparable accuracy. second that reasons entirely understood typically achieves accurate model training large model ﬁrst somehow transforming smaller training small model ﬁrst place. leads problem compressing neural focus. compressing neural nets recognized recent years important problem various academic industrial research groups shown indeed signiﬁcantly compress neural nets without appreciable losses accuracy however solutions proposed somewhat ad-hoc senses. first deﬁne speciﬁc compression technique work well types models others. second solutions guaranteed optimal sense achieving highest classiﬁcation accuracy compression technique considered. paper provide general formulation model compression training algorithm solve formulation accommodate compression technique long certain mathematical form includes existing techniques. compression mechanism appears black algorithm simply iterates learning large model compressing guaranteed converge locally optimal compressed model standard assumptions. separate papers develop speciﬁc algorithms various compression forms quantization pruning evaluate experimentally. rest paper discuss related work give generic formulation generic algorithm give conditions convergence discuss relation algorithm algorithms relation generalization model selection general sense understand model compression replacing large model small model within context given task done diﬀerent ways. discuss setting problem motivates need compression. assume deﬁne machine learning task classiﬁcation object recognition images consider large model deep neural inputs outputs real-valued weights order train model loss e.g. cross-entropy large training input-output pairs also assume trained large reference neural i.e. minw happy performance terms accuracy size large. want smaller model apply task deﬁne smaller model? possibility small model type reference model reparameterized terms low-dimensional parameter vector. construct weights low-dimensional parameters transformation size smaller size i.e. example before would deep weight values quantized using codebook entries second possibility direct learning minθ small model best loss regardless reference. simply train small model minimize loss directly possibly using chain rule compute gradient approximates reference model indirectly trying solve task. small model reparameterized version completely diﬀerent model. direct learning best thing sometimes always noted later. direct compression minθ closest approximation parameters reference model using low-dimensional parameterization forces models type. direct compression simply done compression generally optimal loss since latter ignored learning discuss later detail. erence function norm. norm domain approximated sample here reference model teaches student model small model reparameterized version completely diﬀerent model. existing compression approaches fall categories. particular traditional compression techniques applied using techniques related direct training direct compression using low-precision weight representations form rounding even single-bit values ternary values powers quantization weight values soft hard zeroing weights achieve sparse model low-rank factorization weight matrices hashing lossless compression huﬀman codes papers combine several techniques produce impressive results e.g. pruning trained quantization huﬀman coding. although comment works paper defer detailed discussions companion papers speciﬁc compression techniques teacher-student approach seems arisen ensemble learning literature inspired desire replace large ensemble model requires large storage slow test time smaller faster model similar classiﬁcation accuracy smaller model type ensemble members diﬀerent type model altogether basic idea trained large ensemble labeled training ensemble label larger unlabeled dataset larger labeled dataset used train smaller model hope knowledge captured teacher adequately represented synthetically created training student learn well even though smaller model. generally approach applied teacher necessarily ensemble model teacher’s labels transformed improve student’s learning e.g. log-outputs manipulations output class probabilities however compression point view results unimpressive modest compression ratios even failure compress practical problem teacher-student approach really construct artiﬁcial training leaves design student user diﬃcult model selection problem. unlike compression approaches cited above teacher’s model architecture compress parameters. work understand compression mathematically speciﬁc sense involves learning task want solve large model sets reference meet. related direct training direct compression concepts introduced above. assume large reference model parameters trained loss solve task minw abuse notation write loss directly weights rather minimizer local. deﬁne compression ﬁnding low-dimensional parameterization terms parameters deﬁne compressed model seek corresponding model optimal loss. denote thisoptimal compressed write ordinarily could solve problem directly minθ direct learning option previous section. instead equivalently write model compression constrained optimization problem figure schematic representation idea model compression constrained optimization. plots illustrate uncompressed model space contour lines loss compressed models grayed areas) generic compression technique θ-space shown. optimizes infeasible direct compression feasible optimal compressed optimal compressed. plot shows local optima loss respective points plot shows several feasible sets corresponding diﬀerent compression levels eliminating formulation equivalent direct learning minθ ﬁrst place rather training large model compressing fact direct learning using gradient-based methods sometimes good option. always convenient possible. firstly decompression mapping diﬀerentiable chain rule apply. second using gradient-based methods lead slow convergence prone local optima compared training large uncompressed model third direct learning beneﬁt availability large well-trained model w-space since operates exclusively low-dimensional θ-space. finally direct learning learning task aspects intimately linked compression ones design direct learning algorithm speciﬁc combination loss compression technique well deﬁned call compression mapping. behaves inverse decompression mapping since generally unique inverse given compressing need algorithm simple formula usually satisfy i.e. decompressing compressing gives back decompression mapping appears explicitly problem deﬁnition compression mapping appears algorithm see. feasible contains high-dimensional models obtained decompressing low-dimensional model framework compression equivalent orthogonal projection feasible set. indeed minθ equivalent minθw′ s.t. problem ﬁnding closest feasible point euclidean distance i.e. orthogonal projection feasible set. low-rank compression deﬁnes write weights matrix form min. learn compression mapping given singular value decomposition also ﬁxed dictionary basis learn either only. compression mapping given solving linear system. study paper preparation. quantization uses discrete mapping given assigning weight codebook values. learn assignments codebook compression done k-means. also ﬁxed codebook compression mapping given form rounding. study separate paper low-precision approximation deﬁnes constraint weight real single-precision ﬂoat. compression mapping sets truncation particular case binarization compression mapping sets seen quantization using ﬁxed codebook. lossless compression takes many forms huﬀman coding arithmetic coding run-length encoding special bijection. hence direct compression solves problem need algorithm. however lossless compression aﬀords limited compression power. lossy compression technique user choose compression level obviously interested highest compression level retains acceptable accuracy task. note accounting size compressed model need costs storing weights compressed model storing decompression algorithm data ﬂexible deﬁnition constraints problem need compress weights simply means constraint weight diﬀerent types compression diﬀerent parts model means sets constraints separate parallel step also additional constraints problem example quantization binary assignment vectors whose must equal variables must belong original minimization loss also constrained e.g. weights nonnegative. decompression mapping take diﬀerent forms. function entire shared compression parameters. happens low-rank compression instead shared parameters private parameters happens quantization codebook shared weights index codebook assigned here {ϑi}p earlier deﬁned feasible contains highdimensional models obtained decompressing low-dimensional model good compression schemes satisfy desiderata achieve compression error. obviously depends compression level optimization e.g. algorithm form compression mapping matters. form every uncompressed model interest near part feasible i.e. decompression mapping space-ﬁlling extent. applying penalty method alternating optimization learning part separates compression part however formulation generally less preferable constrained formulation reason penalty guarantee optimum exactly compressed model close compressed model ﬁnal suboptimal compression step required. example penalizing deviations weights given codebook encourage weights cluster around codebook values actually equal them upon termination must round weights makes result suboptimal. types compression penalty formulation produce exactly compressed models. pruning want weight vector contain many zeros using penalty sparsity-inducing norm result sparse weight vector. still penalty form number nonzeros implicitly related value constraint form allows number nonzeros directly convenient practice. directly terms well-deﬁned compression mapping rather terms decompression mapping advantage problem simply solved setting minw without need iterative algorithm indeed constraint actually constrain however formulation rarely useful resulting compressed model arbitrarily large loss exception lossless compression satisﬁes optimal compressed solution indeed achieved compressing reference model directly. although constrained problem solved number nonconvex optimization algorithms proﬁt parameter separability achieve penalty methods alternating optimization described next. optimizing penalized function alternating optimization applying alternating optimization penalized function gives learning-compression algorithm. steps follows reusing existing code steps step gradually pulls towards model obtained decompressing step compresses current steps done reusing existing code rather writing algorithm makes algorithm easy implement. step needs additive term gradient e.g. stochastic gradient descent neural nets. step depends compression type generally correspond well-known compression algorithm diﬀerent types compression used simply calling diﬀerent compression routine step change algorithm. facilitates model designer’s trying diﬀerent types compression suitable task hand. runtime runtime step typically negligible compared step although depends type compression loss. hence pays step exactly possible. overall runtime algorithm dominated steps training uncompressed model longer time. schedule penalty parameter practice usual penalty methods multiplicative schedule slightly larger also section noted pseudocode single step value keeps initialization termination always initialize i.e. reference model direct compression exact solution show next section. stop algorithm smaller tolerance happen large enough. point ﬁnal iterate satisﬁes compressed model hence solution optimal) comments derivation algorithm used quadratic penalty penalize violations equality constraint convenient makes step easy gradientbased optimization step also easy compression forms however non-quadratic penalty forms convenient situations. note step also seen trying predict weights low-dimensional parameters mapping sense compression machine learning problem modeling data using low-dimensional space noted denil context low-rank models applies generally framework. also discussion parametric embeddings section fact model ﬁtting machine learning seen compressing training model parameters. algorithm parameters trace path solution obtained constraints satisﬁed achieve optimal compressed model. beginning path special meaning corresponds direct compression show next. i.e. orthogonal projection feasible recalling discussion section hence path starts corresponds direct compression training large reference model compressing weights i.e. compressed model). optimal sense problem compression ignores learning task; best compression weights need best compressed model task. constrained optimization view shows that optimal uncompressed model feasible i.e. optimal compressed since compression zero error case generally compression increase loss larger compression level model). therefore expect that compression levels direct compression near-optimal compress more—which goal critical actual deployment mobile devices—it become worse worse loss optimal compressed model hence high compression rates require better optimization. plot ﬁgure illustrates this. indeed suboptimality direct compression compared result algorithm becomes practically evident experiments compressing neural nets push compression quadratic penalty augmented lagrangian methods belong family homotopy algorithms minima deﬁne path solution want give theorem based similar results possible assume loss decompression mapping continuously diﬀerentiable arguments loss lower bounded. theorem consider constrained problem quadratic-penalty function given positive increasing sequence nonnegative sequence starting point suppose method ﬁnds approximate minimizer satisﬁes straint gradients linearly independent. prove statements turn. first limit sequence exists loss hence function lower bounded continuous derivatives. second constraint gradients linearly independent point thus particular limit this note jacobian constraint function matrix whose rank obviously full-rank. stated otherwise algorithm deﬁnes continuous path which mild assumptions increasingly accurately converges stationary point constrained problem convex problems unique path leading solution. nonconvex problems multiple paths leading local optimum. nonconvex continuous optimization problem convergence occur pathological cases stationary point constrained problem minimizer cases rare practice. computationally better approach solution following path small become progressively ill-conditioned ideally would follow path closely increasing slowly practice follow path loosely reduce runtime typically using multiplicative schedule penalty parameter ﬁrst iteration iterates stuck direct compression value usually sign increased fast. smaller closely follow path. theorem applies number common losses used machine learning various compression techniques low-rank factorization continuously diﬀerentiable. however apply popular compression forms speciﬁcally quantization pruning generally give rise npcomplete problems. least-squares linear regression deﬁnes quadratic loss weights whose solution given linear system. forcing weights either deﬁnes binary quadratic problem weights np-complete forcing given proportion weights zero ℓ-constrained problem also np-complete cannot expect algorithm global optimum problems expect reasonably good results following sense. algorithm still guaranteed converge weight vector satisﬁes constraints hence always converge validly compressed model. step minimizes loss convergence likely low-loss point theorem states convergence occur steps must solved increasingly accurately. generally problem step usually solved existing compression algorithm. step needs consideration. objective function step form original loss plus simple term separable quadratic function intuitively optimizing diﬀerent optimizing loss indeed gradient-based optimization straightforward since gradient simply adds gradient loss. many optimization algorithms used solve depending form modiﬁed newton method line searches even solving linear system quadratic. however large-scale problems particularly interested gradient-based optimization without line searches gradient descent ﬁxed step size stochastic gradient descent learning rates satisfying robbins-monro schedule. convergence without line searches requires certain conditions step size must corrected account fact quadratic µ-term increases increases since gradient term also increases cause problems consider cases optimizing step using gradient descent ﬁxed step size using sgd. well known convex optimization arguments loss convex diﬀerentiable lipschitz continuous gradient lipschitz constant training reference model done gradient descent ﬁxed step size step objective function strictly convex therefore unique minimizer. gradient descent ﬁxed step size initial point. hence simply need adjust step size step. although step size becomes smaller increases convergence becomes faster. reason objective function becomes closer separable quadratic function whose optimization easier. neural nets involve nonconvex loss large dataset usual optimization procen= loss training point large costly evaluate gradient exactly. practically preferable take approximate gradient steps based evaluating gradient step minibatch hence step seen gradient step using noisy gradient convergence theorems stochastic setting diﬀerent assuming exact gradients notably requirement step sizes must tend zero certain speed call robbins-monro schedule appendix gives detailed theorems suﬃcient conditions convergence case noise deterministic particular incremental gradient algorithm noise stochastic conditions include lipschitz continuity loss gradient condition noise small enough learning rates satisfy robbins-monro schedule. convergence rate sublinear much slower exact gradients practice schedule typically form determined trial-and-error subset data. unfortunately convergence theory practical apart conditions loss noise theory tells using robbins-monro schedule lead convergence. however performance sensitive schedule selecting well great importance. indeed learning neural nets notoriously tricky considerable trial error setting hyperparameters unavoidable practice. concern given good robbins-monro schedule {ηt}∞ reference model alone) schedule modiﬁed optimizing how? theory change needed condition convergence theorems require schedule robbins-monro. practice requires consideration. addition µ-term loss eﬀects. first since exact gradient fast compute noise gradient smaller good thing convergence purposes. second practical problem since increases towards inﬁnity gradient becomes progressively large. hence early weight updates step considerably perturb sending away initial convergence minimizer still guaranteed robbins-monro schedule much slower occur diﬀerent local minimizer. makes overall optimization unstable avoided. theorem guarantees clipping robbins-monro schedule remains robbins-monro. hence clipped schedule makes sure initial larger updates exceed otherwise leaves unchanged. ensures ﬁrst steps unduly perturb initial convergence minimum guaranteed since schedule robbins-monro lipschitz continuous gradient nutshell practical recommendation follows ﬁrst determine trial error good schedule reference model then clipped schedule step done experiments various compression forms found eﬀective. emphasize speciﬁc form algorithm follows necessarily deﬁnition compression technique constraints problem work neural compression based modifying usual training procedure manipulating weights ad-hoc binarizing pruning them usually guarantee solve problem converge all. framework algorithm follows necessarily constraints deﬁne form compression example quantization low-rank compression step results k-means respectively step optimization minθ takes form quadratic distortion problem cases. pruning using norm optimization step results form weight magnitude thresholding. need ad-hoc decisions algorithm. direct compression consists training reference model compressing weights. shown section corresponds beginning iterates’ path algorithm suboptimal produce compressed model lowest loss. said direct compression appealing approach obvious thing simple fast require training reference model indeed particular instances corresponding particular compression techniques recently applied compress neural nets include quantizing weights reference k-means pruning weights reference zeroing small values reducing rank weight matrices reference using however algorithm nearly simple direct compression seen iterating direct compression crucial term. practically much slower given link steps train reference model anyway. since steps usually negligible compared steps algorithm behaves like training reference model longer time. standard approach neural pruning here ﬁrst train reference prune weights e.g. thresholding small-magnitude weights. gives direct compression model using sparsity-inducing norms finally optimize loss remaining unpruned weights. reduces loss often considerably. however loses advantage retrain still suboptimal since generally weights pruned would give lowest loss. algorithm consistently beats retraining pruning particularly higher compression levels imagine iterate direct compression procedure. optimize obtain compress θdc. next optimize initializing compress etc. argument section implies nothing would change ﬁrst would simply cycle forever. fact several iterations needed happen reasons. local optima might converge diﬀerent optimum compression however sooner rather later cycling local optimum compressed model. still improves optimum. likely reason practice inexact compression learning steps. implies iterates never fully reach either both keep oscillating forever between. particularly case training neural nets stochastic gradient descent converging high accuracy requires long training times. call procedure iterated direct compression version quantization proposed recently although without context constrained optimization framework provides. experiments elsewhere verify neither converge local optimum problem algorithm does. overall derive algorithm applying following design pattern solve compression problem introducing auxiliary variables handling constraints penalty methods eqs. optimizing penalized function using alternating optimization original variables auxiliary variables eqs. design pattern similar used method auxiliary coordinates optimizing nested systems deep nets i.e. involving functions form fk+; wk+) input data point trainable parameters here introduces auxiliary coordinates data point form then handling constraints penalty method applying alternating optimization yields ﬁnal algorithm. alternates maximization step optimizes single-layer functions independently coordination step optimizes auxiliary coordinates independently data point. hence auxiliary coordinates data point capture intermediate function values within nested function algorithm auxiliary variables duplicate parameters model order separate learning compression. algorithm become identical interesting case parametric embeddings. nonlinear embedding algorithm seeks collection high-dimensional data points collection low-dimensional projections distances similarities pairs points approximately preserved corresponding pairs projections examples nonlinear embeddings spectral methods multidimensional scaling laplacian eigenmaps truly nonlinear methods stochastic neighbor embedding t-sne elastic embedding example elastic embedding optimizes deﬁnes similarity therefore ﬁrst term attracts similar points second term repels points optimal embedding balances forces parametric embedding wish learn projection mapping rather projections elastic embedding means optimizing following algorithm model compression regard uncompressed model reference model) compressed model. algorithm coincide parametric embedding data point associated parameter vector decompression mapping recovers uncompressed model applying projection mapping high-dimensional dataset. compression step ﬁnds optimally parameters regression learning step learns regularized embedding direct compression directly reference embedding suboptimal corresponds beginning path algorithm. hence view parametric embeddings seen compressed nonlinear embeddings. paper focus exclusively compression mechanism model minimal loss belonging compressed models precisely formulated problem however generalization important aspect compression brieﬂy discuss this. compression also seen prevent overﬁtting since aims obtaining smaller model similar loss well-trained reference model. noted early literature neural nets particular pruning weights neurons seen explore diﬀerent network architectures soft weight-sharing form soft quantization weights neural proposed regularizer make network generalize better. recently weight binarization schemes also seen regularizers many recent papers including work report experimentally training and/or test error compressed models lower reference papers interpret improvement generalization ability compressed net. extent true simpler reason surely accounts part error reduction reference model trained well enough continued training happens compressing reduces error. generally unavoidable practice large neural nets diﬃculty training accurately mean reference model close optimal never exactly optimal. model selection consists determining model type size achieves best generalization given task. diﬃcult problem general much neural nets many factors determine architecture number layers number hidden units type activation function type connectivity etc. results exponential number possible architectures. compression seen shortcut model selection follows. instead ﬁnding approximately optimal architecture problem hand expensive trial-and-error search train reference overestimates necessary size architecture gives good estimate best performance achieved problem. then compresses reference using suitable compression scheme desired compression level pruning weights quantizing weights using bits. then algorithm automatically search subset models given size example ℓ-based pruning mechanism carreira-perpi˜n´an idelbayev uses single parameter implicitly considers possible pruning levels layer net. much easier network designer test multiple combinations number hidden units layer. running algorithm diﬀerent compression levels determine smallest model achieves target loss good enough application hand. summary good approximate strategy model selection neural nets train large enough reference model compress much possible. described general framework obtain compressed models optimal task performance based casting compression usually understood procedure constrained optimization model parameter space. accommodates many existing compression techniques also sheds light previous approaches derived procedurally converge optimal compressed model even eﬀective practice. also given general learning-compression algorithm provably convergent standard assumptions. algorithm reuses kinds existing algorithms black-box independently other step learning algorithm task loss requires training whose form independent compression technique; step compression algorithm model parameters whose form depends compression technique independent loss training set. steps follow mathematically deﬁnition constrained problem; example step quantization low-rank compression results k-means respectively step takes form quadratic distortion problem cases. model designer diﬀerent losses compression techniques simply calling appropriate routine step. companion papers develop framework speciﬁc compression mechanisms report experimental results often exceed comparable published state additional advantages generality simplicity convergence guarantees. this think penalty parameter. assume given sequence nonnegative sequence tolerances starting point quadratic-penalty method works ﬁnding iterate approximate minimizer starting terminating k∇xqk theorem suppose tolerances penalty parameters satisfy then limit point sequence infeasible stationary point function hand limit point feasible constraint gradients linearly independent point problem points inﬁnite note method lagrange multipliers way; fact tends lagrange multiplier constraint subproduct fact iterates converge point. method improves precisely capitalizing availability estimates lagrange multipliers. function convex convex diﬀerentiable. strongly convex function constant lipschitz continuous lipschitz constant norms euclidean section. statements apply convex function deﬁned convex subset details standard reference nesterov theorem strongly convex constant continuously diﬀerentiable gradient lipschitz continuous lipschitz constant given deﬁne sequence unique global minimizer remark theorem shows that strongly convex constant gradient lipschitz continuous constant gradient descent constant step size converges linearly rate apply results case. train reference model minimize loss assume convex diﬀerentiable lipschitz continuous gradient lipschitz constant could gradient descent ﬁxed step size guaranteed convergence. step strongly convex minimize objective function constant unique minimizer gradient lipschitz continuous lipschitz constant hence gradient descent ﬁxed step size initial point. large literature convergence sgd-type algorithms although basic conditions convergence similar quote theorems bertsekas tsitsiklis simple consider cases deterministic errors incremental gradient algorithm stochastic errors. continuously diﬀerentiable function lipschitz continuous gradient consider minimization using approximate descent method search direction error vector. step sizes robbins-monro positive satisfy continuously diﬀerentiable lipschitz continuous gradient. incremental gradient algorithm cycle time updating using gradient corresponds using minibatch size without reshuﬄing dataset epoch. theorem sequence generated method deterministic positive step size descent direction random noise term. increasing sequence σ-ﬁelds. assume following many practical cases. tempting think condition hold step objective function holds loss since gradient µ-term error true generally. this assume error condition holds function consider function error error comes could pick adversarially error small although could solved placing assumptions well assume error condition error consequently theorems hold minimization satisﬁes step sizes robbins-monro. hence neither simpliﬁcation complication minimizing minimizing convergence theory leaves choice step sizes user long robbins-monro.", "year": 2017}