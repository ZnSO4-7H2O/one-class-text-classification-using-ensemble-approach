{"title": "Quantifying the probable approximation error of probabilistic inference  programs", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "This paper introduces a new technique for quantifying the approximation error of a broad class of probabilistic inference programs, including ones based on both variational and Monte Carlo approaches. The key idea is to derive a subjective bound on the symmetrized KL divergence between the distribution achieved by an approximate inference program and its true target distribution. The bound's validity (and subjectivity) rests on the accuracy of two auxiliary probabilistic programs: (i) a \"reference\" inference program that defines a gold standard of accuracy and (ii) a \"meta-inference\" program that answers the question \"what internal random choices did the original approximate inference program probably make given that it produced a particular result?\" The paper includes empirical results on inference problems drawn from linear regression, Dirichlet process mixture modeling, HMMs, and Bayesian networks. The experiments show that the technique is robust to the quality of the reference inference program and that it can detect implementation bugs that are not apparent from predictive performance.", "text": "paper introduces technique quantifying approximation error broad class probabilistic inference programs including ones based variational monte carlo approaches. idea derive subjective bound symmetrized divergence distribution achieved approximate inference program true target distribution. bound’s validity rests accuracy auxiliary probabilistic programs reference inference program deﬁnes gold standard accuracy meta-inference program answers question what internal random choices original approximate inference program probably make given produced particular result? paper includes empirical results inference problems drawn linear regression dirichlet process mixture modeling hmms bayesian networks. experiments show technique robust quality reference inference program detect implementation bugs apparent predictive performance. challenge practitioners probabilistic modeling approximation error introduced variational monte carlo inference techniques. kullback-leibler divergence result approximate inference i.e. variational approximation distribution induced sampler true target distribution typically unknown. predictive performance held-out test sometimes used proxy need track posterior convergence. paper introduces technique quantifying approximation error broad class probabilistic inference programs including variational monte carlo approaches. idea derive subjective bound symmetrized divergence distribution achieved approximate inference program true target distribution. bound’s validity rests beliefs accuracy auxiliary probabilistic program. ﬁrst reference inference program deﬁnes gold standard accuracy might diﬃcult compute. original approximate inference program tractable output probability density suﬃcient. output density approximate inference program available technique also depends accuracy meta-inference program answers question what internal random choices approximate inference program interest probably make assuming produced particular result actually produced reference? section relate technique recent work. technique implemented probabilistic meta-program venture probabilistic programming platform written venturescript language. paper includes empirical results inference problems drawn linear regression dirichlet process mixture modeling hmms bayesian networks. experiments show technique robust quality reference inference program detect implementation bugs apparent predictive performance. figure estimating subjective divergences probabilistic inference programs. relates subjective divergence kullback-leibler divergence. show estimated subjective divergence proﬁles sampling-based optimization-based inference respectively small bayesian linear regression problem using oracle reference program shows model program implemented venturescript show venturescript programs used estimate subjective divergences sampling importance resampling inference program. shows schematic probabilistic programs process used algorithm estimate subjective divergence. kullback-leibler divergences inference program’s approximating distribution target distribution objective model-independent measures approximation error. however tractable techniques estimating divergences approximate inference lacking. paper deﬁnes quantity subjective divergence terms following elements data speciﬁc dataset induces posterior distribution approximate inference program samples output approximates also returns history inference program execution generated approximate inference program induces weight function p/q. ˆqis ˆqhm denote random variables. realized estimate induces realized weight estimate. cases expectations without subscript respect ˆqis ˆqhm. mators ˆqis ˆqhm make meta-inference program tional distribution given inference program output ratio densities eﬃciently computed given baseline ˆqis estimator samples produces single sample importance sampling estimate q/m. baseline ˆqhm estimator obtains history inference program execution generated produces single sample harmonic mean estimate q/m. procedure estimating subjective divergence using baseline meta-inference based estimators shown algorithm require elements meta-inference program number reference replicates independent replicates using reference inference program gold standard sample reference inference program plausible inference program execution history producing estimate weight figure estimated weight data timing data individual runs venturescript implementation algorithm black variational inference program. weights colored source timing data broken stages estimation procedure. shows schematic illustration relationship between divergences subjective divergence estimated algorithm case reference program exact inference oracle. produced venturescript inference programming library implements algorithm applications weight estimate computation performed incrementally within inference program meta-inference program obviating need explicit representation inference program execution history separate weight computation illustrated figure subjective divergence based estimating symmetrized divergence order handle fact posterior density available unnormalized form reference sampler proxy posterior sampler address challenge monte carlo estimation respect posterior term dkl||q) symmetrized inference programs output density computed eﬃciently mean-ﬁeld variational families weight estimate algorithm replaced true weight subjective divergence equivalent symmetrized divergence oracle reference used. inference programs large number internal random choices densities outputs intractable compute algorithm uses meta-inference construct marginal density estimators ˆqis ˆqhm proposition holds. subjective divergence interpreted approximately comparing samples inference program interest gold standard samples lens log-weight terest /ˆqhm)]]) expected estimated weight taken under reference program /ˆqis)]]). expectation inference program interest less normalizing constant least dkl||p) figure subjective divergence proﬁles several inference programs bayesian linear regression problem figure using oracle reference high quality approximate reference quality approximate reference oracle implements collapsed sampling posterior. lw-sir references particles respectively subjective divergence estimates using subjective divergence diﬀerence expectation reference program expectation inference program interest taking diﬀerence bounds cancels terms proves proposition relationships quantities proof illustrated figure proposition oracle reference program available estimate upper bound symmetrized divergence estimating subjective divergence. eﬀect quality reference inference program reference inference program oracle still possible retain validity subjective divergence upper bound depending accuracy reference program eﬀect quality meta-inference program setting oracle reference program baseline marginal density estimators ˆqis ˆqhm used algorithm quality meta-inference determines tightness upper bound proposition particular true symmetrized divergence subjective divergence symmetrized conditional relative entropy measures closely meta-inference approximates conditional distribution inference execution histories given inference output note exact meta-inference could compute density weight estimate simpliﬁes authors point unbiased estimators like ˆqis unbiased reciprocal estimators like ˆqhm estimate lower upper bounds log-estimand respectively estimate lower upper bounds also suggests combining stochastic upper bounds obtained running reversed versions sequential monte carlo algorithms starting exact sample obtained simulating data model lower bounds elbo upper bound divergences. authors introduce general auxiliary variable formalism estimating lower bounds elbo markov chain inference equivalent estimation expected estimated weight inference program baseline ˆqhm estimator applied markov chains. used venturescript implementation algorithm estimate subjective divergence proﬁles diverse approximate inference programs applied several probabilistic models. addition applying technique mean-ﬁeld variational inference output density available derived meta-inference programs classes inference programs whose density generally intractable sequential inference utilizing markov chain detailedbalance transition operators particle ﬁltering state space models. sequential inference coarse-grained representation inference execution history suppresses internal random choices made within segments markov chain satisfy detailed balance respect single distribution. meta-inference program also sequential detailed-balance inference order transition operators reversed. reversed markov chain instance formalism used construct annealed importance sampling sampled weight estimate corresponds marginal likelihood estimate. subjective divergence standard non-sequential mcmc analyzed using construction results trivial upper bound divergence failure approximating assumptions used derive meta-inference program. particle ﬁltering state space models conditional update weight estimate marginal likelihood estimate particle ﬁlter. intuitive csmc answer might particle ﬁlter produced given particle? special case particle ﬁlter sampling importance resampling meta-inference program ﬁrst considered small bayesian linear regression problem unknown intercept slope latent variables generated subjective divergence proﬁles sampling-based variational inference programs using oracle reference. estimated proﬁles black mean-ﬁeld programs diﬀered choice variational family—each family diﬀerent ﬁxed variance latents. varied number iterations stochastic gradient descent generate proﬁles exhibited distinct nonzero asymptotes. also estimated proﬁles sequential inference figure shows subset noisy-or network shows subjective divergence proﬁles sequential inference programs based single-site block transition operators network using block gibbs-based scheme reference. shows subset shows subjective divergence proﬁles particle ﬁlters diﬀerent proposals likelihood-weighted resampling applied smoothing problem using exact oracle reference. points. program used repeated application metropolis-hastings transitions resimulation proposal within used applications random-walk transition. varied number applications within primitive transition operator. proﬁle based resimulation converged rapidly. finally produced subjective divergence proﬁle likelihood-weighting sampling importance resampling inference program varying number particles. lw-sir algorithm applied problem whose subjective divergence proﬁle converged zero. estimated subjective divergence proﬁles approximate inference programs applied noisybayesian network network contained latent causes ﬁndings prior cause probabilities transmission probabilities spontaneous ﬁnding activation probabilities edges sampled uniformly probability presence. ﬁndings active. compared four sequential inference programs advanced sequence target distributions deﬁned gradually lowering ﬁnding spontaneous activation probability true model value across equallength steps applied distinct types transition operators step. compared single-site resimulation operator block resimulation operator single-site gibbs operator block gibbs operator primitive operators within varied number applications primitive operator within generate proﬁles shown figure reference program used sequential inference four applications block gibbs target distribution step. inference problem hard singlesite gibbs operators explaining away eﬀects hard resimulation-based operators probability data prior. resimulation based proﬁles exhibited much slower convergence gibbs operators. next applied technique hidden markov model discrete state observation space produced subjective divergence proﬁles particle ﬁlter inference programs prior conditional proposals. particle ﬁlters used independent resampling. used exact forwardﬁltering backwards sampling reference inference program. proﬁles respect number particles shown figure conditional proposal proﬁle exhibits faster convergence expected. note single particle case latent random choices particle ﬁlters subjective divergence symmetrized divergence. distributions. reference used relatively trusted sequential inference program based venture’s built-in single-site resimulation implementation. estimated subjective divergence proﬁles inference based single-site resimulation operator inference based cycle operator consisting single-site gibbs steps latent cluster assignments resimulation global parameters. subjective divergence gibbs/mh operator exhibited anomalous behavior degraded additional inference quickly becoming worse resimulation operator. identify gibbs/mh operator inference performed within-cluster variance parameter. proﬁle corrected operator exhibited markedly faster convergence resimulation proﬁle. produced termination inference programs. expected likelihood proﬁle gibbs/mh operator signiﬁcantly higher proﬁle resimulation despite signiﬁcantly poorer proﬁle corrected version. note unlike subjective divergence proﬁles expected likelihood proﬁles operator seemed anomalous. paper introduced technique quantifying approximation error broad class probabilistic inference programs. ideas assess error relative subjective beliefs quality reference inference program symmetrized divergences meta-inference program ﬁnds probable executions original inference program output density cannot directly assessed. approach implemented probabilistic meta-program venturescript uses ancillary probabilistic meta-programs reference meta-inference schemes. much empirical theoretical development needed. speciﬁc directions include better characterizing impact reference meta-inference quality identifying contexts theoretical bounds predictably tight loose. applying technique broad corpus venturescript programs seems like useful ﬁrst step. empirically studying behavior subjective divergence broader sample buggy inference programs also informative. also important connect approach results theoretical computer science including computability complexity probabilistic inference. example asymptotic scaling probabilistic program runtime analyzed using standard random access memory model suitable assumptions implementation. includes model program; inference program; reference program; meta-inference program; probabilistic meta-program implementing algorithm thus possible align computational tractability approximate inference varying qualities standard results algorithmic computational complexity theory combining asymptotic analysis technique opens research opportunities. example possible predict probable performance approximate inference building probabilistic models characteristics problem instances predict subjective divergences. also possible technique justify inference heuristics stochastic bayesian relaxations finally seems fruitful technique study query sensitivity approximate inference practitioners probabilistic modeling inference familiar diﬃculties come dependence approximation algorithms especially stochastic ones. diagnosing convergence sampling schemes known diﬃcult theory practice many practitioners respond restricting class models queries consider. deﬁnition tractable sometimes even taken synonymous admits polynomial time algorithms exactly calculating marginal probabilities probabilistic programming throws diﬃculties sharp relief making easy explore unbounded space possible models queries inference strategies. hardly probabilistic inference programs come certiﬁcates give exact answers polynomial time. understandable many practitioners wary expressive probabilistic languages. techniques paper make possible pursue alternative approach expressive languages modeling potentially even also stochastic inference strategies also build quantitative models time-accuracy proﬁles approximate inference practice empirical data. inherently subjective process involving qualitative quantitative assumptions meta-level. however note probabilistic programming potentially help manage meta-modeling process providing probabilistic—or sense meta-probabilistic— tools studying probable convergence proﬁles probabilistic inference programs. authors would like thank ulrich schaechtle anthony testing technique david wingate alexey radul feras saad taylor campbell helpful feedback discussions. research supported darpa iarpa oﬃce naval research army research oﬃce gifts analog devices google.", "year": 2016}