{"title": "The Role of Conversation Context for Sarcasm Detection in Online  Interactions", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Computational models for sarcasm detection have often relied on the content of utterances in isolation. However, speaker's sarcastic intent is not always obvious without additional context. Focusing on social media discussions, we investigate two issues: (1) does modeling of conversation context help in sarcasm detection and (2) can we understand what part of conversation context triggered the sarcastic reply. To address the first issue, we investigate several types of Long Short-Term Memory (LSTM) networks that can model both the conversation context and the sarcastic response. We show that the conditional LSTM network (Rocktaschel et al., 2015) and LSTM networks with sentence level attention on context and response outperform the LSTM model that reads only the response. To address the second issue, we present a qualitative analysis of attention weights produced by the LSTM models with attention and discuss the results compared with human performance on the task.", "text": "debanjan ghosh§ alexander richard fabbri† smaranda muresan‡ §school communication information rutgers university †department computer science columbia university usera plane window shades open people ﬁre. userb usera reason feel really great. userc yourselves. fact remains caribbean poverty crime everyone near nil. contented self-sufﬁcient standard life. huge social gaps. userd kidding think caribbean countries content? maybe wander beach sometime yourself. computational models sarcasm detection considered utterances isolation many instances even humans difﬁculty recognizing sarcastic intent considering utterance isolation paper investigate role conversation context detecting sarcasm social media discussions table shows examples sarcastic replies taken media platforms modeling conversation context help sarcasm detection understand part conversation context triggered sarcastic reply. address ﬁrst issue investigate several types long short-term memory networks model conversation context sarcastic response. show conditional lstm network lstm networks sentence level attention context response outperform lstm model reads response. address second issue present qualitative analysis attention weights produced lstm models attention discuss results compared human performance task. argued sarcasm verbal irony type interactional phenomenon speciﬁc perlocutionary effects hearer break pattern expectation. thus able detect speakers’ sarcastic intent necessary consider utterances larger conversation context. consider twitter conversation example table without context usera’s address speciﬁc issues modeling conversation context help sarcasm detection understand part conversation context triggered sarcastic reply userc’s comment triggered userd’s sarcastic reply). address ﬁrst issue investigate models linguistically-motivated discrete features several types long short-term memory networks model context sarcastic reply show conditional lstm network lstm networks sentence level attention context reply outperform lstm model reads reply address second issue present qualitative analysis attention weights produced lstm models attention discuss results compared human performance task make datasets code available. goal investigation comparatively study types social media platforms considered individually sarcasm detection discussion forums twitter. ﬁrst discuss datasets point differences could impact results modeling choices. discussion forums. oraby introduced sarcasm corpus subset internet argument corpus consists discussion forum data. corpus consists sarcastic responses context annotation sarcastic non-sarcastic replies done using crowdsourcing annotators asked label reply sarcastic part reply contained sarcasm ﬁnal gold sarcastic label assigned majority annotators labeled reply sarcastic. although dataset described oraby consists post twitter collect sarcastic non-sarcastic tweets adopt methodology proposed related work sarcastic tweets collected using hashtags sarcasm sarcastic irony non-sarcastic tweets ones contain hashtags might contain sentiment hashtags happy love hate. exclude retweets duplicates quotes tweets contain hashtags urls shorter three words. also eliminate tweets hashtags interest positioned message. thus removed utterances sarcasm something love. built conversation context sarcastic nonsarcastic utterance used reply status parameter tweet determine whether reply previous tweet downloaded last tweet original tweet replying addition also collected entire threaded conversation available although collected tweets ﬁrst step around reply another tweet thus ﬁnal twitter conversations contains instances observe tweets tweet conversation context. main differences datasets need acknowledged. first discussion forum posts much longer twitter messages. second gold labels sarcastic class obtained different. discussion forum dataset gold label obtained crowdsourcing thus gold label emphasizes whether sarcastic intent perceived hearers twitter dataset gold label given directly hashtag speaker used signaling clearly speaker’s sarcastic intent. third difference made size forum dataset much smaller size twitter dataset. assess effect conversation context labeling reply sarcastic sarcastic consider binary classiﬁcation tasks. refer sarcastic instances non-sarcastic instances ﬁrst task classiﬁcation performed using reply isolation second classiﬁcation considers reply context experiment types computational models support vector machines linguistically-motivated discrete features approaches using distributed representations. latter long short-term memory networks shown successful various tasks constituency parsing language modeling machine translation textual entailment present models next subsections. discrete features features used n-grams lexicon-based features sarcasm indicators commonly used existing sarcasm detection approaches short description features. sentiment pragmatic features linguistic inquiry word count lexicon identify pragmatic features. category dictionary treated separate feature deﬁne boolean feature indicates context reply contains liwc category. sentiment lexicons also used model utterance sentiment mpqa opinion lexicon capture sentiment count number positive negative sentiment tokens negations boolean feature represents whether reply contains positive negative sentiment tokens. sc+r sc+r classiﬁcation task check whether reply different sentiment context given sarcastic utterances often contain positive sentiment towards negative situation hypothesize feature capture type sentiment incongruity. sarcasm indicators burgers introduce sarcasm indicators explicitly signal utterance sarcastic. morpho-syntactic features interjections questions exclatypographic mation marks features capitalization words quotation marks emoticons; tropes superlative intensiﬁers words often occur sarcastic utterances building features lowercased utterances except words characters uppercased tokenization conducted cmu’s tweeboparser discussion forum dataset nltk tool sentence boundary detection tokenization. used libsvm toolkit linear kernel weights inversely proportional number instances class. long short-term memory networks lstms type recurrent neural networks able learn long-term dependencies recently lstms shown effective natural language inference research task establish relationship multiple inputs since goal explore role contextual information recognizing whether reply sarcastic argue using lstm networks read context reply natural modeling choice. attention-based lstm networks attentive neural networks shown perform well variety tasks using attentionbased lstm accomplish goals test whether achieve higher performance simple lstm models attention weights produced lstm models perform qualitative analysis determine portions context triggers sarcastic reply. although yang included levels attention mechanisms word level another sentence level primarily focus sentence level attention speciﬁc reasons. first sentence level attentions show exact sentence context informative trigger sarcasm. discussion forum dataset context posts usually three four sentences long could helpful identify exact text triggers sarcastic reply. second attention words sentences seek learn large number model parameters given moderate size discussion forum corpus might overﬁt. tweets treat individual tweet sentence. majority tweets consist single sentence even multiple sentences tweet often sentence contains hashtags urls emoticons making uninformative treated isolation. context contain sentences sentence contain words. similar notation yang ﬁrst feed sentence annotation layer hidden representation weight sentence measuring similarity sentence level context vector ucs. gives normalized importance weight softmax function. vector summarize information sentences context also experiment word sentence level attentions hierarchical fashion similarly approach proposed yang show section however achieve best performance datasets using sentence-level attention. conditional lstm networks also experiment conditional encoding model introduced rockt¨aschel task recognizing textual entailment. architecture separate lstms used similar previous architecture without attention memory state initialized last cell state words conditioned representation built context. parameters pre-trained word vectors. discussion forum twitter split randomly corpus training development test maintaining distribution sarcastic non-sarcastic data training development test. twitter used skip-gram word-embeddings used built using million tweets. discussion forums standard google n-gram wordvec pre-trained model optimize word embedding training. out-ofvocabulary words training randomly initialized sampling values uniformly development data tune parameters selected dropout rate regularization strength evaluate conﬁguration test set. datasets mini-batch size employed. report precision recall scores classes. svmr respectively represent performance model using discrete features using reply reply together context. lstmca lstmra attention-based lstm models context reply subscripts denote word-level sentence-level word sentence level attentions. lstmconditional conditional encoding model discussion forums table shows classiﬁcation results discussion forum dataset. although vast majority context posts contain sentences around context posts sentences thus cutoff maximum sentences context modeling. reply considered entire reply. models based discrete features perform well adding context actually hurt performance. regarding performance neural network models observe modeling context improves performance using types lstm architectures read context reply highest performance considering classes achieved lstmconditional model lstm model sentence-level attentions context reply gives best score class. class notice improvement precision notice drop recall compared lstm model sentence level attention reply remember sentence-level attentions based average word embeddings. also experimented hierarchical attention model sentence represented weighted average word embeddings. case attentions based words sentences follow architecture hierarchical attention network observe performance deteriorates probably lack enough training data. since attention words sentences seek learn model parameters adding training data helpful. full release sarcasm corpus used oraby expect achieve better accuracy models. twitter table shows results twitter dataset. discussion forums adding context using models show statistically signiﬁcant neural networks model similar results discussion forums lstm models read context reply outperform lstm model reads reply best performing architectures lstmconditional lstm sentence-level attentions lstmconditional model shows improvement class class compared lstmr. attentionbased models improvement using context smaller kept maximum length context last tweets conversation context available.we also conducted experiments word-level attentions however obtain lower accuracy comparison sentence level attention models. wallace showed providing contextual information humans able identify sarcastic utterances unable without context. however useful understand whether speciﬁc part context triggers sarcastic reply. begin address issue conducted qualitative study understand whether human annotators able identify parts context trigger sarcastic reply attention weights able signal similar information. designed crowdsourcing experiment looked attention weights lstm networks. short description crowdsourcing task. crowdsourcing experiment. designed amazon mechanical turk task framed follow given pair context sarcastic reply discussion forum dataset identify sentences trigger sarcastic reply turkers could select sentences context including entire context. test data select examples context length three seven sentences since longer posts task complicated turkers. provided deﬁnition sarcasm examples turkers. also explained carry task help context/reply pairs. contains task turkers allowed attempt turkers reasonable quality selected paid seven cents task. visualize compare sentence-level attention weights lstm models context turkers’ annotations ﬁrst measure overlap turkers choice attention weights. sentence-based attention model selected sentence highest attention weight matched sentence selected turkers using majority voting. found times sentence highest attention weight also picked turkers. figure shows side side heat maps attention weights lstm models turkers’ choices picking sentences context thought triggered sarcastic reply semantic coherence context reply. figure depicts case context contains three sentences attention weights given sentences similar turkers’ choice. looking example seems model pays attention output vectors semantically coherent sarcastic response example contains single sentence hold tongue support anti-gay argument. context contains sentence i’ve held tongue long can. attention-based lstm architecture learning attention weights simultaneously context response thus model showing contextual understanding setting high weights semantically coherent parts figure attention weights given informative sentence –rationally explain creatures existence recently human history extinct millions years?. here sarcastic reply mocks claiming author context reading religious script also observe similar behavior tweets incongruity context reply meaning incongruity inherent characteristic irony sarcasm extensively studied linguistics philosophy communication science well recently instance riloff pointed identifying incongruity positive sentiment towards negative situation characteristic sarcasm detection social media. observe discussion forums tweets attention-based models frequently identiﬁed sentences words semantically incongruous instance figure attention model chosen sentence contains strong negative sentiment word interestingly contrast attention model reply given highest weight sentence contain opposite sentiment thus model seems learn context incongruity opposite sentiment detecting sarcasm. however seems turkers prefer second sentence instructive sentence instead ﬁrst sentence. looking sarcastic reply observe reply contains remarks mothers apparently commonality assisted turkers chose second sentence. twitter dataset observe often attention models selected utterance context opposite sentiment here word sentence-level attention model chosen particular utterance context words high attention .these words show examples meaning incongruity useful sarcasm detection. word-models seem also work well words context/reply semantically incongruous connected deeper semantics attention weights sarcasm markers looking attention weights reply notice models giving highest weight sentences contain sarcasm markers emoticons interjections sarcasm markers explicit indicators sarcasm signal utterance sarcastic emoticons uppercase spelling words interjections. markers social media extensive. started understand semantic attention weights task studies need carry out. rockt¨aschel argued interpretations based attentions weights taken care since classiﬁcation task forced solely rely attentions weights. thus future work plan analyze utterances subtle consist sarcasm markers explicit incongruence opposite sentiment context response. computational models sarcasm detection considered utterances isolation however even humans difﬁculty sometimes recognizing sarcastic intent considering utterance isolation thus recent work sarcasm irony detection started exploit contextual information. particular analyzed authors’ prior sentiment towards certain entities tweet deviates author’s estimated sentiment tweet predicted sarcastic. similar approach several models introduced; relied extensive feature engineering capture contextual information authors topics conversation context whereas rest using deep learning techniques embed authors’ information studies considered conversation context among contextual information shown minimal improvement modeling conversation context using twitter data work show using better models lstm networks show clear beneﬁt using context sarcasm detection. stated earlier section lstm’s shown effective tasks especially task establish relationship multiple inputs observe lstmconditional model sentence level attention-based models using context reply present best results. research makes complementary contribution existing work modeling context sarcasm/irony detection looking particular type context conversation context. addressed issues modeling conversation context help sarcasm detection determine part conversation context triggered sarcastic reply. answer ﬁrst question show long short-term memory networks model context sarcastic reply achieve better performance lstm networks read reply. particular conditional lstm networks lstm networks sentence level attention achieved signiﬁcant improvement address second issue presented qualitative analysis attention weights produced lstm models attention discussed results compared human annotators. also showed attention-based models able identify inherent characteristics sarcasm future plan study larger context full thread discussion forum consider also responses sarcastic comment available. also interested analyzing sarcastic replies contain sarcasm markers explicit incongruence paper based work supported darpa-deft program. views expressed authors reﬂect ofﬁcial policy position department defense u.s. government. authors thank christopher hidey discussions resources lstm anonymous reviewers helpful comments.", "year": 2017}