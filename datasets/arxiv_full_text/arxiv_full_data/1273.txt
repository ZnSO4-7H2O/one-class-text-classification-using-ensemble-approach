{"title": "Universal Deep Neural Network Compression", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Compression of deep neural networks (DNNs) for memory- and computation-efficient compact feature representations becomes a critical problem particularly for deployment of DNNs on resource-limited platforms. In this paper, we investigate lossy compression of DNNs by weight quantization and lossless source coding for memory-efficient inference. Whereas the previous work addressed non-universal scalar quantization and entropy coding of DNN weights, we for the first time introduce universal DNN compression by universal vector quantization and universal source coding. In particular, we examine universal randomized lattice quantization of DNNs, which randomizes DNN weights by uniform random dithering before lattice quantization and can perform near-optimally on any source without relying on knowledge of its probability distribution. Entropy coding schemes such as Huffman codes require prior calculation of source statistics, which is computationally consuming. Instead, we propose universal lossless source coding schemes such as variants of Lempel-Ziv-Welch or the Burrows-Wheeler transform. Finally, we present the methods of fine-tuning vector quantized DNNs to recover the performance loss after quantization. Our experimental results show that the proposed universal DNN compression scheme achieves compression ratios of 124.80, 47.10 and 42.46 for LeNet5, 32-layer ResNet and AlexNet, respectively.", "text": "abstract—compression deep neural networks memorycomputation-efﬁcient compact feature representations becomes critical problem particularly deployment dnns resource-limited platforms. paper investigate lossy compression dnns weight quantization lossless source coding memory-efﬁcient inference. whereas previous work addressed non-universal scalar quantization entropy coding weights ﬁrst time introduce universal compression universal vector quantization universal source coding. particular examine universal randomized lattice quantization dnns randomizes weights uniform random dithering lattice quantization perform near-optimally source without relying knowledge probability distribution. entropy coding schemes huffman codes require prior calculation source statistics computationally consuming. instead propose universal lossless source coding schemes variants lempel–ziv–welch burrows–wheeler transform. finally present methods ﬁne-tuning vector quantized dnns recover performance loss quantization. experimental results show proposed universal compression scheme achieves compression ratios lenet -layer resnet alexnet respectively. state-of-the-art performance many computer vision applications image classiﬁcation object detection semantic segmentation super resolution recent impressive progress dnns comes deep over-parameterized multi-layer architectures enable dnns extract useful feature representations data automatically trained large datasets. although over-parameterization advantageous training large datasets always necessary accurate inference. compression dnns develop compact models memorycomputation-efﬁcient feature representations inference. compression lowers memory requirements also reduces computational costs. hence desirable compress dnns deployment particularly resource-limited devices e.g. battery-powered mobile wearable devices. paper focus ﬁnding memory-efﬁcient models. particular investigate model size compression weight quantization lossless source coding. compression dnns weight quantization lossless coding ﬁrst examined kmeans clustering used scalar quantization weights followed huffman coding entropy source coding. explored weight quantization problem compression previous work hessian-weighted k-means clustering proposed analytically shown minimize approximate quantization loss dnns. experimental results conﬁrmed hessian-weighted k-means clustering yields better accuracy-compression trade-off vanilla kmeans clustering. paper investigate compression entropy coded scalar quantization particular elaborate ecsq solutions weight quantization i.e. uniform quantization iterative solution derived interesting simple uniform quantization efﬁcient combined entropy coding. however generally impossible achieve shannon’s rate-distortion lower bound scalar quantization therefore extend focus vector quantization. vector quantization reduces rate-distortion bound jointly quantizing multiple symbols. using vector quantization shannon’s limit achievable asymptotically dimension increases computational complexity searching optimal vector quantizer becomes prohibitive dimension increases iterative algorithms successful ﬁnding good unstructured vector quantizers reasonable computational complexity. example linde–buzo–gray algorithm provides empirical extension lloyd’s algorithm vector quantization chou altered algorithm entropy constraint best entropy coded vector quantization iteratively. although iterative vector quantization algorithms demonstrated improved compression efﬁciency scalar quantization computational complexity required associate input vector best reconstruction vector codebook still signiﬁcant unstructured quantizers dimension large. different designing simple efﬁcient vector quantizers suggested impose structural constraints quantization codebook. examples structured quantization tree quantization trellis coded quantization lattice quantization since conjectured gersho lattice quantization presumed efﬁcient ecvq high resolution regime asymptotically rate goes inﬁnity distortion diminishes therefore examine lattice quantization compression. another reason choice lattice quantization fast simple memory efﬁcient algorithms proposed e.g. encoding taking advantage structure lattice codes. though lattice quantizers simple empirically shown perform well even ﬁnite rates efﬁciency depends source statistics. hence moreover consider universal quantization provides near-optimal performance sources particular interest randomized lattice quantization uniform random dithering makes distortion independent source rate rate-distortion bound distortion level provably bits sample ﬁnite dimension bits sample asymptotically dimension grows sources establish universal compression framework combining universal quantization universal lossless source code lempel–ziv–welch burrows–wheeler transform hand huffman codes achieve entropy bound within source symbols independent identically distributed hand huffman coding requires knowledge true source statistics. given speciﬁc neural network empirical approximation source statistics obtained computing weight histograms time-consuming inaccurate inapplicable networks even network weight distribution changes ﬁne-tuning. contrast universal source codes asymptotically achieve optimal performance source belonging broad class sources without needing know statistics. another method consider improve compression weight pruning. weight pruning curtails redundant parameters completely neural networks even skip computations pruned weights. number highly successful weight pruning schemes proposed recently moreover shown weight pruning weight quantization achieved together training procedure soft weight sharing originated paper ﬁrst reduce model size weight pruning compress remaining unpruned weights proposed universal compression algorithm. order recover performance loss resulting weight quantization propose algorithms ﬁne-tuning shared quantized values neural networks. present ﬁne-tuning algorithms vector quantized dnns cases without randomization quantization. gain ﬁne-tuning gets larger dimension vector quantization increases fact number shared quantized values tunable quantized model increases dimension increases. note matrix/tensor decomposition low-rank approximation studied compact representation weight matrices/tensors neural networks consequently save computations well. moreover efﬁcient hardware implementation low-precision dnns examined extremes low-precision dnns consisting binary ternary weights found rest paper organized follows. ﬁrst review lossy data compression entropy coded quantization following section. then propose entropy coded quantization schemes section particular focus scalar quantization section explore vector quantization section universal compression framework discussed section furthermore present methods ﬁne-tuning quantized dnns section finally experimental results conclusion found section section respectively. lossy compression fundamental problems well studied information theory. comparing lossless compression trades competing costs rate distortion. rate-distortion theory establishes information theoretic bounds lossy compression theoretical foundation practical compression techniques extensively investigated last decades particularly multimedia data speech images high-quality audio video practical widely-used lossy compression techniques entropy coded scalar/vector quantization optimal codes achieve minimum average codeword length given source. entropy theoretical limit average codeword length symbol achieve lossless data compression proved shannon known optimal codes achieve limit overhead less integer-length codewords allowed. optimal coding also called entropy coding. huffman coding entropy coding schemes commonly used source distribution provided estimated arithmetic coding another well-known entropy coding scheme similar characteristics huffman coding i.e. explicitly utilizes source statistics suffers mismatch source statistics present. assuming entropy coding quantization scalar/vector quantizers optimized minimizing distortion ﬁxed entropy rate achievable entropy coding scalar/vector quantization followed entropy coding called entropy coded scalar/vector quantization three main thrusts investigation ecsq/ecvq iterative solution finding optimal quantizer given entropy constraint general intractable. however attempts solve optimization problem directly iterative algorithms particular chou employed method lagrangian multipliers provided compression ratio deﬁned ratio total number bits original network parameters number bits compressed parameters codebook i.e. compression ratio given number bits network parameter quantization average binary codeword length network parameter compression denotes codebook storage size additional overhead compressed models need take account. number clusters quantization network parameters cluster number bits codeword assigned parameters cluster codebook stores binary codewords bits corresponding quantized values hence assuming entropy coding follows quantization compression ratio approximately maximized entropy quantization output minimized. quantization scheme designed minimize output entropy called entropy coded scalar/vector quantization subsection efﬁcient heuristic solutions ecsq proposed quantization i.e. uniform quantization iterative solution similar lloyd’s algorithm k-means clustering. iterative algorithm ecsq/ecvq similar lloyd’s algorithm k-means clustering distortion measure altered include quantization error entropy cost. high-resolution approximation high-resolution quantization theory originated optimal quantizers analytically derived asymptotic regime quantization resolution goes inﬁnite i.e. rate goes inﬁnite. ecsq/ecvq uniform scalar quantization ﬁrst proved optimal highresolution ecsq result later extended vector quantization. optimal high-resolution ecvq conjectured lattice codebook lattices shown efﬁcient others efﬁcient lattice quantization cell shapes determined dimension values randomization ﬁnite resolutions order guarantee universally good performance close optimum within ﬁxed sources rates proposed randomize source uniformly distributed random dithers particular shown expected distortion randomized lattice quantization determined quantization voronoi cell size given random dithers drawn uniformly polytope congruent quantization voronoi cells. universal property established randomized lattice quantization performs near optimal ecvq within ﬁxed sources distortion levels. universal performance bound randomized quantization named universal quantization. paper interest compress network parameters given pretrained dnn. following entropy coded quantization methodology compression achieved steps weight quantization entropy coding. furthermore performance loss quantization compensated ﬁne-tuning shared quantized values end. consider general non-linear neural network consisting network parameters denoted values network parameters source data compress. model deﬁnition number layers layer types number parameters layer assumed stored separately. size dominated network parameters information model deﬁnition ignored counting total size. weight quantization partitions network parameters clusters parameters cluster share quantized value representative value cluster belong quantization entropy coding follows compress quantized parameters binary codewords stored instead actual parameter values. decoding additionally need store index network parameters cluster shared quantized value parameters cluster quantization. assumption quantized parameters independent identically distributed entropy given |ci|/n ratio number network parameters cluster number network parameters optimize ecsq minimizing quantization distortion subject entropy constraint constant deﬁne lagrangian cost function follows optimization problem note hyperparameters i.e. lagrangian multiplier number clusters multiplier controls entropy constraint solving optimization different values results quantization solutions different entropy constraints. example using larger value effectively give penalty entropy consequently leads minimize distortion smaller entropy constraint. recall entropy constraint related compression ratio distortion determines performance loss. hence solving optimization problem different values obtain trade-off curve compression ratio performance. number clusters problem equal number remaining non-empty clusters optimization since clusters empty entropy constraint. note long enough optimization result impacted much value since solving problem entropy constraint automatically optimizes number non-empty clusters well. shown uniform quantizer optimal high-resolution entropy coded scalar quantizer regardless source statistics mean square error criterion implying asymptotically optimal minimizing mean square quantization error quantization resolution becomes inﬁnite i.e. rate goes inﬁnite random source reasonably smooth density function. asymptotic result leads come simple efﬁcient quantization scheme follows cell size uniform quantization design parameter select given compression ratio accuracy requirements. meet requirements search minimum cell size experiments validation dataset. note compression ratio increases accuracy degrades uniform quantization cell size grows. uniform quantization straightforward method never shown literature actually efﬁcient scalar quantization schemes compression entropy coding follows. recall k-means clustering combined huffman coding proposed however paper identiﬁed simple uniform quantization outperforms k-means clustering output encoded entropy codes huffman arithmetic codes. uniform quantization always good. inefﬁcient ﬁxed-rate coding also ﬁrst shown paper dnns. although uniform quantization known optimal high-resolution ecsq continuous random sources observed experiments still yields good performance employed weight quantization source discrete quantization resolution ﬁnite. hand pointed section iterative solutions also successful ﬁnding good entropy coded quantizers reasonable complexity. thus investigate iterative ecsq algorithm compression next subsection. iterative algorithms better ecsq solutions uniform quantization long stuck local optima since solve ecsq problem directly high-resolution approximation. iterative algorithm solves general ecvq problem provided derive similar iterative algorithm solve ecsq problem quantization. although iterative solution complicated uniform quantization discussed section ﬁnds local optima given discrete source data. propose compressing parameters layers together once rather layer-by-layer compression layer-by-layer compression maximize overall compression ratio need search optimal compression ratios jointly across individual layers requires exponential time complexity respect number layers total number possible combinations compression ratios individual layers increases exponentially number layers increases. recent dnns getting deeper deeper e.g. dnns layer-by-layer compression optimization generally intractable. instead quantizing parameters layers together straightforward practical since avoid layer-by-layer compression rate optimization. heuristic iterative algorithm solves presented algorithm similar lloyd’s algorithm k-means clustering. difference partition network parameters assignment step. lloyd’s algorithm euclidean distance minimized. algorithm ecsq individual lagrangian cost function i.e. minimized instead includes quantization error expected codeword length entropy coding. focused scalar quantization network parameters section however well known rate-distortion theory vector quantizers require lower code rates scalar quantizers data compression subsection investigate vector quantization deployed weight quantization. particular generalize ecsq solutions previous subsection ecvq. ⌈n/n⌉ ⌈n/n⌉n. vector quantization partitions n-dimensional vectors ﬁnite number clusters. vectors cluster share quantized value i.e. cluster center. quantization quantized vectors encoded binary codewords quantized vector treated symbol. shared quantized vectors stored codebook. decoding shared quantized vector corresponding received symbol retrieved codebook. remark although vector quantization theoretically provide better rate-distortion trade-off vector quantizers generally complex implement scalar quantizers. moreover practice compression ﬁnite number data gain vector quantization limited codebook overhead becomes considerable dimension increases becomes dominant factor degrades compression ratio point. example using huffman coding vector quantization quantized vector encoded binary codeword variable length. number bits codeword assigned vectors cluster store binary codewords bits corresponding n-dimensional shared quantized vectors codebook. codebook storage size increases dimension grows. hence vector quantization always beneﬁcial scalar quantization compression size small hold. reviewed section ﬁrst conjectured optimal high-resolution ecvq lattice codebook asymptotic regime quantization resolution goes inﬁnite. conjecture proved mostly accepted true literature e.g. high-resolution ecvq result propose following simple lattice quantization scheme dnns. straightforward extension uniform quantizer section vector quantization. although multidimensional uniform quantizer efﬁcient lattice quantizer found propose quantization since simple implement value dimension finally cancel random dithers lattice quantization output obtain ﬁnal randomized lattice quantization output. ﬁnal quantization output represented iterative solution ecvq follows algorithm replacing scalar network parameters n-dimensional vectors. generalization straightforward thus omit details here. section propose universal compression scheme consisting universal quantization universal lossless source coding. major advantage proposed scheme need compute estimate statistics network parameters loss compression independent statistics network parameters. thus proposed scheme universally applicable models compression rates achieving universally good performance close optimal entropy coded vector quantization. figure illustrates proposed universal compression scheme. observe dithering values change vector change every vector. dimension dithering values independent identically distributed uniform random variables sufﬁcient make quantization errors dimension independent source statistics. entropy coding randomized lattice quantization assumed encoder decoder share information random dithers i.e. u⌈n/n⌉. assumption encoder adds dithering vectors back ﬁnal output randomized lattice quantization encodes lattice quantization output i.e. hand decoder decodes lattice quantization output cancels dithering vectors decompress randomized lattice quantization output. practice avoid sharing random dithers using pseudo-random number generator known encoder decoder. randomized lattice quantization achieves universally good performance regardless source statistics rates dithering source uniformly quantization. quantization loss becomes independent source statistics uniform random dithering results universally good rate-distortion trade-off close optimum within ﬁxed sources distortion levels. result propose following universal quantization method universally applicable models compression rates optimization. randomize n-dimensional vectors adding uniform random dithers. ⌈n/n⌉ vectors consisting random dithers. randomized n-dimensional vectors follow u⌈n/n⌉ independent identically distributed uniform random variables. support uniform distribution cell size dimension following ndimensional uniform quantization. huffman codes known optimal encoding individual source symbols binary codewords. however optimality comes assumptions source symbols independent statistics known. exploiting dependency source symbols exists improve compression efﬁciency. example could encode blocks source symbols instead individual symbols. extension huffman coding called extended huffman coding block huffman coding. however extended huffman coding even difﬁcult since require know accurately estimate joint distribution source symbols. therefore explore existence entropy coding schemes less hopefully sensitive source statistics. class algorithms known universal lossless source coding popular ones variants lempel–ziv named authors seminal papers describe basic algorithms underlie class. codes class universal sense universally optimal i.e. asymptotic compression rate approaches entropy rate source stationary ergodic source. practical algorithms universal source coding lempel–ziv–welch gzip bzip deploys burrows– wheeler transform universal source coding algorithms convenient practice huffman coding since require knowledge source statistics. furthermore utilize dictionary-based coding codebook built source symbols encoding decoding therefore codebook overhead smaller huffman coding. method ﬁne-tuning shared quantized values presented dnns quantized kmeans clustering. similarly ﬁne-tune shared quantized values scalar quantizers. particular obtain average gradient network loss function network parameters cluster. cluster center updated using average gradient gradient descent manner parameters cluster follow change cluster center. change parameters cluster amount gradient descent update step using average gradient. note clustering change ﬁne-tuning cluster centers number distinct shared quantized values remains same. ﬁne-tune shared quantized vectors vector quantization network parameters. dimensional element shared quantized vector ﬁned-tuned separately. clusters n-dimensional vectors effectively divide network parameters nkvq groups ﬁne-tune shared quantized values separately. similar scalar quantization average gradient network loss function respect network parameters computed group used update shared quantized value. shared quantized vector cluster note network parameters quantized value cij. then element shared quantized vector ﬁne-tuned e.g. using gradient descent given vector consisting network parameters network loss function given training dataset here note gradient evaluated consists quantized parameters satisfying example suppose sequence network parameter values want quantize neural network. given uniform cell boundaries scalar quantization partitions clusters cluster centers respectively. scalar quantized model scalar centers ﬁne-tuned. hand vector quantization ﬁrst construct sequence vectors then given uniform boundaries dimension lattice quantization yields clusters cluster centers respectively. case vector cluster centers ﬁne-tuned implying actually four groups network parameters shared scalar centers ﬁne-tuned. remark vector quantization produces larger number shared quantized values trainable quantized model dimension increases therefore beneﬁt ﬁne-tuning becomes considerable dimension increases. particular numbers non-empty clusters produced scalar vector quantization respectively assumption cell boundaries dimension. then shown satisfy ksq/n dimension vectors implies number shared quantized values vector quantization least equal larger scalar quantization i.e. nkvq ksq. however codebook overhead also increases dimension increases becomes dominant factor degrades compression ratio large dimension using randomized lattice quantization ﬁne-tuning quantized parameters straightforward random dithers added quantization subtracted back quantization. randomized quantization scheme shared values cluster actually intermediate quantization output canceling random dithers i.e. lattice quantization output convenient assume ﬁne-tune shared values random dithers stay same. similar case without dithering average gradient respect network parameters computed every dimension every cluster used update shared values. here note average gradients computed network loss function evaluated quantized parameters canceling random dithers ﬁne-tune shared values obtained canceling random dithers. shared vector cluster obtained subtracting random dithers. ﬁne-tune element shared vector similar average gradient obtained gradient evaluated consists quantized parameters satisfying information compute index differences adjacent unpruned network parameters original model compress huffman coding also compare performance quantized dnns ﬁne-tuning shared quantized values described section first evaluate quantization schemes lenet model consisting convolutional layers fully-connected layers followed soft-max layer. total parameters achieves accuracy. pruned model prune original lenet parameters. second evaluate quantization schemes pre-trained -layer resnet model cifar- dataset. -layer resnet consists parameters total achieves top- accuracy. pruned model prune parameters. third consider alexnet imagenet ilsvrc- dataset. obtain pre-trained alexnet caffe model achieves top- accuracy. pruned model prune parameters. parameters original model assumed -bit ﬂoating-point numbers. original model size simply computed product total number parameters number bits parameter i.e. compressed model size consists total number coded bits compressed parameters codebook. proposed compression techniques compress parameters together once rather layer-by-layer compression avoid layer-by-layer compression rate optimization. evaluate performance weight quantization without weight pruning. magnitude-based weight pruning scheme. namely choose parameters prune selecting ones whose magnitude less threshold value. pruning unpruned parameters ﬁne-tuned original accuracy recovered. incrementally perform weight pruning unpruned weight ﬁne-tuning long accuracy loss negligible obtain ﬁnal pruned models. figure presents experimental results compression scalar weight quantization huffman coding -layer resnet model. weight pruning considered now. observe ecsq solutions proposed section perform better k-means clustering based algorithms huffman coding follows. performance difference uniform quantization iterative algorithm ecsq. methods heuristic difﬁcult expected better. iterative algorithm better uniform quantization seen figure since converges local optima discrete source data situation quantization uniform quantization asymptotically optimal high resolution regime rate goes inﬁnite continuous sources. figure present experimental results -layer resnet ﬁxed-rate coding follows scalar quantization. fixed-rate coding produces binary codewords length source data whereas entropy coding generally yields variable-length codewords. contrary previous case using huffman coding ecsq solutions underperform k-means clustering algorithms ﬁxed-rate coding follows expected since optimized entropy coding. subsection ﬁrst evaluate lattice quantization scheme presented section -layer resnet compare performance scalar uniform quantization. particular consider cases lattice quantization whose uniform boundaries dimension either respectively integers. quantization cell size cases former case origin boundary quantization cells latter case origin middle quantization cell. lattice quantization huffman coding used encode quantized vectors entropy coding. figure figure accuracy plotted compression ratio cases lattice quantization respectively. cases lattice quantization outperforms scalar uniform quantization particularly compression ratio large performance degradation. gain lattice quantization becomes signiﬁcant ﬁne-tuning fact number shared quantized values trainable vector quantized models pointed remark however increasing dimension lattice quantization given additional gain even hurts performance point i.e. without ﬁne-tuning ﬁne-tuned since codebook overhead surges signiﬁcantly becomes dominant factor degrades compression ratio noted remark furthermore unpruned resnet model high volume network parameters concentrated around zero thus quantization output entropy smaller latter case. utilizing information latter case similarly evaluate randomized lattice quantization followed huffman coding plot accuracy compression ratio figure figure cases uniform boundaries dimension either respectively. results show randomized lattice quantization performs better lattice quantization former case reversed latter case. interesting randomized lattice quantization provides universally good performance cases beneﬁt randomizing source uniform dithering quantization. emphasize randomized lattice quantization applicable models optimization regardless statistics network parameters. codebook overhead vector quantization plot proportion codebook storage size compressed -layer resnet model different values dimension lattice randomized lattice quantization figure observe codebook overhead small however increases signiﬁcantly dominates model size dimension becomes larger. fig. accuracy versus compression ratio -layer resnet compressed lattice quantization huffman coding different values dimension uniform boundaries dimension fig. accuracy versus compression ratio -layer resnet compressed lattice quantization huffman coding different values dimension uniform boundaries dimension subsection experiment compression using universal lossless source codes. consider scalar quantization network parameters uniform randomized uniform quantization. quantization universal lossless source coding follows compression quantized parameters distinct quantized value treated distinct alphabet source coding. universal source coding algorithms evaluated lempel–ziv–welch bzip figure compare compression performance bzip huffman coding. huffman coding also examine extended huffman codes different values block length observe extended huffman codes outperform plain huffman codes universal lossless source codes perform similar even better extended huffman codes. largest compression ratios achieved proposed methods i.e. compressed model sizes original sizes lenet -layer resnet alexnet respectively. using universal compression scheme consisting randomized uniform quantization universal source coding e.g. bzip achieve comparable performance compression ratios lenet -layer resnet alexnet respectively. vector quantization provides gain scalar quantization lenet alexnet gain vector quantization observed resnet. universal source coding algorithms outperform huffman coding alexnet huffman coding better lenet resnet. summarize compression ratios achieve different compression methods pruned models table also compare ones recall layer-by-layer quantization k-means clustering fig. accuracy versus compression ratio -layer resnet compressed randomized lattice quantization huffman coding different values dimension uniform boundaries dimension fig. accuracy versus compression ratio -layer resnet compressed randomized lattice quantization huffman coding different values dimension uniform boundaries dimension fig. proportion codebook storage size compressed model -layer resnet compressed lattice randomized lattice quantization different values dimension uniform boundaries dimension randomized lattice quantization randomized lattice quantization yields universally good rate-distortion trade-off sources rates. combining universal quantization scheme universal lossless source coding established universal compression framework dnns. experimental results showed proposed entropy coded network quantization schemes provide considerable gain fig. accuracy versus compression ratio -layer resnet compressed uniform randomized uniform quantization followed either huffman coding universal source coding uniform boundaries dimension conventional quantization methods using k-means clustering entropy coding follows. furthermore identiﬁed vector quantization provides additional gain scalar quantization particularly compression ratio large performance loss. finally conﬁrmed proposed universal compression scheme consisting universal quantization universal lossless source coding provides comparable performance best entropy coded vector quantization. szegedy sermanet reed anguelov erhan vanhoucke rabinovich going deeper convolutions proceedings ieee conference computer vision pattern recognition sermanet eigen zhang mathieu fergus lecun overfeat integrated recognition localization detection using convolutional networks arxiv preprint arxiv. girshick donahue darrell malik rich feature hierarchies accurate object detection semantic segmentation proceedings ieee conference computer vision pattern recognition conference computer vision girshick faster r-cnn towards real-time object detection region proposal networks advances neural information processing systems l.-c. chen papandreou kokkinos murphy yuille semantic image segmentation deep convolutional nets fully connected crfs arxiv preprint arxiv. ledig theis husz´ar caballero cunningham acosta aitken tejani totz wang photorealistic single image super-resolution using generative adversarial network arxiv preprint arxiv. el-khamy image super resolution based fusing multiple convolution neural networks ieee conference computer vision pattern recognition workshops gersho asymptotically optimal block quantization ieee transactions information theory vol. conway sloane fast quantizing decoding algorithms lattice quantizers codes ieee transactions information theory vol. huffman coding huffman coding extended huffman coding bzip gzip huffman coding extended huffman coding bzip gzip huffman coding huffman coding huffman coding huffman coding extended huffman coding bzip gzip huffman coding extended huffman coding bzip gzip huffman coding huffman coding huffman coding huffman coding extended huffman coding bzip gzip huffman coding extended huffman coding bzip gzip huffman coding huffman coding russakovsky deng krause satheesh huang karpathy khosla bernstein imagenet large scale visual recognition challenge international journal computer vision vol. convolutional neural networks proceedings ieee conference computer vision pattern recognition y.-d. park choi yang shin compression deep convolutional neural networks fast power mobile applications arxiv preprint arxiv. xiao wang convolutional neural networks courbariaux bengio j.-p. david binaryconnect training deep neural networks binary weights propagations advances neural information processing systems berger optimum quantizers permutation codes ieee transactions information theory vol. minimum entropy quantizers permutation codes ieee transactions information theory vol. gish pierce asymptotically efﬁcient quantizing ieee transactions information theory vol. conway sloane voronoi regions lattices second moments polytopes quantization ieee transactions information theory vol.", "year": 2018}