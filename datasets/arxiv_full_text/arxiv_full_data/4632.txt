{"title": "Large-Margin kNN Classification Using a Deep Encoder Network", "tag": ["cs.LG", "cs.AI"], "abstract": "KNN is one of the most popular classification methods, but it often fails to work well with inappropriate choice of distance metric or due to the presence of numerous class-irrelevant features. Linear feature transformation methods have been widely applied to extract class-relevant information to improve kNN classification, which is very limited in many applications. Kernels have been used to learn powerful non-linear feature transformations, but these methods fail to scale to large datasets. In this paper, we present a scalable non-linear feature mapping method based on a deep neural network pretrained with restricted boltzmann machines for improving kNN classification in a large-margin framework, which we call DNet-kNN. DNet-kNN can be used for both classification and for supervised dimensionality reduction. The experimental results on two benchmark handwritten digit datasets show that DNet-kNN has much better performance than large-margin kNN using a linear mapping and kNN based on a deep autoencoder pretrained with retricted boltzmann machines.", "text": "popular classiﬁcation methods often fails work well inappropriate choice distance metric presence numerous class-irrelevant features. linear feature transformation methods widely applied extract class-relevant information improve classiﬁcation limited many applications. kernels used learn powerful non-linear feature transformations methods fail scale large datasets. paper present scalable non-linear feature mapping method based deep neural network pretrained restricted boltzmann machines improving classiﬁcation large-margin framework call dnet-knn. dnet-knn used classiﬁcation supervised dimensionality reduction. experimental results benchmark handwritten digit datasets show dnet-knn much better performance largemargin using linear mapping based deep autoencoder pretrained retricted boltzmann machines. popular classiﬁcation methods simplicity reasonable effectiveness doesn’t require ﬁtting model shown good performance classifying many types data. however good classiﬁcation performance highly dependent metric used computing pairwise distances data points. practice often euclidean distances similarity metric calculate nearest neighbors data points interest. classify high-dimensional data real applications often need learn choose good distance metric. previous work metric learning learns global linear transformation matrix original feature space data points make similar data points stay closer making dissimilar data points move farther apart using additional similarity label information. global linear transformation applied original feature space data points learn mahalanobis metrics requires data points class collapse point. making data points class collapse point unnecessary classiﬁcation. produce poor performance data points cannot essentially collapsed points often true class containing multiple patterns. information-theoretic based approach used learn linear transformations global linear transformation learned directly improve classiﬁcation achieve goal large margin. method shown yield signiﬁcant improvement classiﬁcation linear transformation often fails give good performance high-dimensional space pre-processing dimensionality reduction step often required success. many situations linear transformation powerful enough capture underlying classspeciﬁc data manifold; thus need resort powerful non-linear transformations data point stay closer nearest neighbors class data non-linearly transformed feature space. kernel tricks used kernelize methods order improve classiﬁcation method extends work perform linear dimensionality reduction improve large-margin classiﬁcation kernelized method however kernel-based approaches behave almost like template-based approaches. chosen kernel cannot well reﬂect true class-related structure data resulting performance bad. besides kernel-based approaches often difﬁculty handling large datasets. might want achieve non-linear mappings learning directed multi-layer belief deep autoencoder perform classiﬁcation using hidden distributed representations original input data. however multi-layer belief often suffers explaining away effect hidden units become dependent conditional bottom visible units makes inference intractable; learning deep autoencoder backpropagation amost impossible gradient backpropagated lower layers output often becomes noisy meaningless. fortunately recent research shown training deep generative model called deep belief feasible pretraining deep using type undirected graphical model called restricted boltzmann machine rbms produce complementary priors make inference process deep belief much easier deep trained greedily layer layer using simple efﬁcient learning rule rbm. greedy layerwise pretraining strategy made learning models deep architures possible moreover greedy pretraining idea also successfully applied initialize weights deep autoencoder learn powerful non-linear mapping dimensionality reduction illustrated fig. besides idea deep learning motivated researchers powerful generative models deep architectures learn better discriminative models paper combining idea deep learning large-margin discriminative learning propose classiﬁcation supervised dimensionality reduction method called dnetknn. learns non-linear feature transformation directly achieve goal large-margin classiﬁcation based deep encoder network pretrained rbms shown approach mainly inspired work given labels training data allows learn non-linear feature mapping minimize invasions data point’s genuine neighborhood impostor nearest neighbors favours classiﬁcation directly. previous researchers used autoencoder deep autoencoder non-linear dimensionality reduction improve none approaches used objective function direct improving classiﬁcation. approach discussed uses convolution learn similarity metric discriminatively handcrafted. approach based general deep neural networks ﬂexible connection weight matrices layers automatically learned data. applied dnet-knn usps mnist handwritten digit datasets classiﬁcation. test error obtained mnist benchmark dataset better obtained deep belief deep autoencoder addition ﬁne-tuning process fast converges good local minimum within several iterations conjugate-gradient update. experimental results show that good generative model used pretraining stage improve discriminative learning; pretraining generative models layerwise greedy makes possible learn good discriminative model deep architecture; pretraining rbms makes discriminative learning process much faster without pretraining; organize paper follows section introduce classiﬁcation using linear transformations large-margin framework. section describe previous work training models deep architectures. section present dnet-knn trains deep encoder network improving large-margin classiﬁcation. section present experimental results usps mnist handwritten digit datasets. section conclude paper discussions propose possible extensions current method. section review large-margin framework classiﬁcation described given data points additional neighborhood information labeled data points total number classes target neighbors seek distance function pairwise data points given neighborhood information preserved transformed feature space corresponding distance function. based mahanalobis distances admits following form linear transformation matrix. based goal margin maximization learn parameters distance function that data point distance data point another class least plus largest distance target neighbors. using binary matrix represent class otherwise labeled data points formulate problem optimization problem penalty coefﬁcient penalizing constraint violations hinge loss function max. matrix problem corresponds work matrix problem corresponds work non-square matrix learned dimensionality reduction resulting problem non-convex stochastic gradient descent conjugate gradient descent often used solve problem. constrained full-rank square matrix solve directly resulting problem convex. alternating projection simple gradient-based methods applied large datasets rich information existing data features often enables build powerful generative models learn constraints structures underlying given data. learned information often reveals characteristics data points belonging different classes. shown deep belief composed stacked restricted boltzmann machines perform handwritten digit classiﬁcation remarkably well undirected graphical model visible layer hidden layer symmetric connections hidden layer visible layer within-layer connections. stochastic binary visible units stochastic binary hidden units joint probability distribution conﬁguration deﬁned based energy follows +exp beneﬁcial property allows unbiased samples posterior distribution hidden units given input data vector. minimizing negative loglikelihood observed input data vectors using gradient descent update rule weight turns learning rate >data denotes expectation respect data distribution denotes expectation respect model distribution. practice sample equilibrium distribution model even one-step reconstruction samples work well although update rule follow gradient log-likelihood data exactly approximately follows gradient another objective function shown deep belief based stacked rbms trained greedily layer layer. given observed input data train hidden representations data. view learned hidden representations data train another rbm. repeat procedure many times. shown greedy training strategy always better hidden representations original input data number features added layer decrease varational lower bound log-likelihood observed input data never decreases. greedy training strategy used initialize weights deep autoencoder shown fig. back-propagation used tuning weights network shown time lower-bound guarantee longer holds greedy pre-training still works well practice work made full capabilities generative models label information weakly used. following describe dnet-knn stacked rbms initialize weights encoder hidden layers shown fig. ﬁne-tune weights encoder minimizing following objective function impostor neighbour discussed details later. deﬁnition discussed section )||. function continuous non-linear mapping component continuous function input vector parameters connection weight matrices deep neural network. example fig. equation differs ways. first distance data points computed euclidean distance using feature vectors output layer encoder fig. secondly objective function focuses maximizing margin neglects term reducing distance nearest neighbors. additionally unlike hinton’s deep autoencoder longer minimize reconstruction error since found criterion reduced ability code vectors accurately describe subtle differences classes practice. reduce complexity backpropagation training simpliﬁed versions objective function compared described section index nearest neighbors among data points class label contrast selected impostor nearest neighbors union nearest neighbors every class class example case digit recognition classes total impostor data point method choosing impostor nearest neighbors optimal classiﬁcation because selecting impostor neighbors every class help ensure potential competitors removed. dimensional code vector generated deep encoder network. time complexity computing okmn) signiﬁcant improvement time complexity purposes calculating nearest neighbors impostor nearest neighbors euclidean distances pixel space. means need recalculated time code vectors updated. unfortunately non-linear mapping mean ordinary data points pixel space become impostors code space taken account objective function. however likely mapping quasi-linear. therefore taking large value captures impostors code space evidenced classiﬁcation errors. experiments improve computation time calculating objective function gradient kmn) matrix triples generated. triples represent sets allowed indices non-zero. therefore triples matrix entries column represent inclass nearest neighbors relative ﬁrst column entries column represent impostor nearest neighbors relateive ﬁrst column. triples matrix used calculating gradient objective function value objective function itself. equation unwieldy matlab implementation triples matrix makes computation much easier. calculate individually using triples matrix determine appropriate indicies combined later. example determine value ﬁrst summation term simply search triples matrix identify triples yield margin violation then choose index ﬁrst column. thus speciﬁc triples tells appropriate indices ﬁrst sum. speciﬁcally second column triples matrix becomes index values third column becomes index values. likewise strategy repeated second third summations. test model dnet-knn classiﬁcation dimensionality reduction handwritten digit dataset usps mnist. demonstrate different types classiﬁcation standard minimum energy classiﬁcation. standard ﬁnish learning nonlinear mapping discriminative ﬁne-tuning directly compute pairwise euclidean disitances classication used dnet-knn. alternatively minimum energy classiﬁcation calculate feature vectors training data test data also predict class label test data point class test data point assigned lowest energy deﬁned minimum energy classiﬁcation denoted experimental results. usps minist experiments downloaded usps digit dataset public link. dataset several different preparations used. ﬁrst preparation usps-ﬁxed takes ﬁrst data points digit classes create point training set. test usps-ﬁxed consists data points data class. figures observe training errors test errors different dimensionality codes. cases dnet-knn classiﬁcation outperforms deep autoencoder furthermore dimensionality codes increases classiﬁcation accuracy increases. trend coninues till levels off. figures compare dnet-knn dimensionality reduction deep autoencoder. dnet-knn clearly produces superior clustering data point classes two-dimensional space. still class overlaps however backpropagation algorithm optimize classiﬁcation best choice improve visualization. objective function chooses data points considers impostor nearest neighbours using pixel space rather code space however visualization requires reduction low-dimensional spaces mapping pixel space code space must become highly non-linear dimensionality reduced. therefore pixel space becomes poorer representation spatial relationships code space correct choice impostor nearest neighbours becomes less reliable visualization. section deals mnist dataset another digit available online. dataset contains training samples test samples. usps dataset possible pre-training backpropagation single batch data. however given size mnist dataset training data broken smaller batches randomly selected datapoints. then training backpropagation could applied iteratively batch. experiments batch size figure show mapping mnist dataset onto reduced space using dnet-knn deep autoencoder. usps dataset shows signiﬁcant improvement deep autoencoder. ﬁnal table shows classiﬁcation error dnet-knn compared common classiﬁcation techniques mnist dataset. despite fact must batches dnet-knn still produces best classiﬁcations. indicates dnet-knn classiﬁer highly robust since perform well limited seeing part dataset time. finally worth noting that unlike deep autoencoder tuning dnet-knn classiﬁer backpropagation displays extremely fast convergence. often error reaches minimum three epochs. fact pretraining provided ideal starting point also using supervised learning algorithm opposed unsupervised algorithm deep autoencoder. paper present non-linear feature mapping method called dnet-knn uses deep encoder network pretrained rbms achieve goal large-margin classiﬁcation. experimental resuls usps mnist handwritten digits show dnet-knn powerful classiﬁcation non-linear embedding. results suggest that pretraining good generative model helpful learning good discriminative model pretraining makes discriminative learning much faster often help much better local minimum especially deep architecture without pretraining. ﬁndings consistent idea discussed huge dataset current implemention method works using mini-batches. essentially compute genuine nearest neighbors impostor nearest neighbors mini-batch might optimum whole dataset. future plan develop dynamic version mini-batches change dynamically training dynamically update true nearest neighbors impostor nearest neighbors data point. additionally plan label information training data constrain distances pairwise data points class. example penalty term using supervised stochastic neighbor embedding t-sne constrain within-class distances.", "year": 2009}