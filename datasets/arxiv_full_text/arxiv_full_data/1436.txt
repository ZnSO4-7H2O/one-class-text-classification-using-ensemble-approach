{"title": "Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "We propose a novel deep learning-based framework to tackle the challenge of semantic segmentation of large-scale point clouds of millions of points. We argue that the organization of 3D point clouds can be efficiently captured by a structure called superpoint graph (SPG), derived from a partition of the scanned scene into geometrically homogeneous elements. SPGs offer a compact yet rich representation of contextual relationships between object parts, which is then exploited by a graph convolutional network. Our framework sets a new state of the art for segmenting outdoor LiDAR scans (+11.9 and +8.8 mIoU points for both Semantic3D test sets), as well as indoor scans (+5.8 mIoU points for the S3DIS dataset).", "text": "propose representation large point clouds collection interconnected simple shapes coined superpoints spirit similar superpixel methods image segmentation illustrated figure structure captured attributed directed graph called superpoint graph nodes represent simple shapes edges describe adjacency relationship characterized rich edge features. representation several compelling advantages. first instead classifying individual points voxels considers entire object parts whole easier identify. second able describe detail relationship adjacent objects crucial contextual classiﬁcation cars generally roads ceilings surrounded walls etc. third size deﬁned number simple structures scene rather total number points typically several order magnitude smaller. allows model long-range interaction would intractable otherwise without strong assumptions nature pairwise connections. contributions follows based representation able apply deep learning large-scale point clouds without major sacriﬁce details. architecture consists pointnets superpoint embedding graph convolutions contextual segmentation. latter introduce novel efﬁcient version edgeconditioned convolutions well form input gating gated recurrent units propose novel deep learning-based framework tackle challenge semantic segmentation largescale point clouds millions points. argue organization point clouds efﬁciently captured structure called superpoint graph derived partition scanned scene geometrically homogeneous elements. spgs offer compact rich representation contextual relationships object parts exploited graph convolutional network. framework sets state segmenting outdoor lidar scans well indoor scans semantic segmentation large point clouds presents numerous challenges obvious scale data. another hurdle lack clear structure akin regular grid arrangement images. obstacles likely prevented convolutional neural networks achieving irregular data impressive performances attained speech processing images. previous attempts using deep learning large data trying replicate successful architectures used image segmentation. example snapnet converts point cloud virtual rgbd snapshots semantic segmentation projected original data. segcloud uses convolutions regular voxel grid. however argue methods capture inherent structure point clouds results limited discrimination performance. indeed converting point clouds format comes loss information requires perform surface reconstruction problem arguably hard semantic figure visualization individual steps pipeline. input point cloud partitioned geometrically simple shapes called superpoints based preprocessing superpoints graph constructed linking nearby superpoints superedges rich attributes finally superpoints transformed compact embeddings processed graph convolutions make contextual information classiﬁed semantic labels. datasets semanticd sdis particular improve mean per-class intersection union points semanticd reduced test points semanticd full test points sdis dataset. classic approach large-scale point cloud segmentation classify point voxel independently using handcrafted features derived local neighborhood solution spatially regularized using graphical models structured optimization clustering preprocessing postprocessing used several frameworks improve accuracy classiﬁcation. deep learning point clouds. several different approaches going beyond naive volumetric processing point clouds proposed recently notably setbased tree-based graph-based however methods deep learning components demonstrated able segment large-scale point clouds. pointnet segment large clouds sliding window approach therefore constraining contextual information within small area only. engelmann improves increasing context scope multi-scale windows considering directly neighboring window positions voxel grid. segcloud handles large clouds subsampling volumetric processing followed interpolation back original resolution post-processing conditional random ﬁeld none approaches able consider details long-range contextual information simultaneously. contrast pipeline partitions point clouds adaptive according geometric complexity allows deep learning architecture detail interactions long distance. formulations able deal graphs variable sizes seen form message passing graph edges particular interest models supporting continuous edge attributes represent interactions. image segmentation convolutions graphs built superpixels used postprocessing liang traverses graphs sequential node order based unary conﬁdences improve ﬁnal labels. update graph nodes parallel exploit edge attributes informative context modeling. convolves information graphs object detections infer contextual relationships. work infers relationships implicitly improve segmentation results. finally note graph convolutions also bear functional similarity deep learning formulations crfs discuss section main obstacle framework tries overcome size lidar scans. indeed reach hundreds millions points making direct deep learning approaches intractable. proposed representation allows split semantic segmentation problem three distinct problems different scales shown figure turn solved methods corresponding complexity geometrically homogeneous partition ﬁrst step algorithm partition point cloud geometrically simple meaningful shapes called superpoints. unsupervised step takes whole point cloud input therefore must computationally efﬁcient. easily computed partition. superpoint embedding node corresponds small part point cloud corresponding geometrically simple object expected semantically homogeneous. primitives reliably represented downsampling figure illustration framework scan table chair. perform geometric partitioning point cloud allows build superpoint graph superpoint embedded pointnet network. embeddings reﬁned grus message passing along superedges produce ﬁnal labeling neighborhood. paper three dimensionality values proposed linearity planarity scattering well verticality feature introduced adaptive neighborhoods compensate variable sampling density. also compute elevation point deﬁned coordinate normalized whole input cloud. global energy proposed deﬁned respect -nearest neighbor adjacency graph point cloud geometrically homogeneous partition deﬁned constant connected components solution following optimization problem problem deﬁned equation known generalized minimal partition problem seen continuous-space version potts energy model variant graph total variation. minimized functional nonconvex noncontinuous implies problem cannot realistically solved exactly large point clouds. however -cut pursuit algorithm introduced able quickly approximate solution graph-cut iterations. contrast optimization methods α-expansion -cut pursuit algorithm require selecting size partition advance. constant connected components {s··· solution equation deﬁne geometrically simple elements referred superpoints rest paper. contextual segmentation graph superpoints orders magnitude smaller graph built original point cloud. deep learning algorithms based graph convolutions used classify nodes using rich edge features facilitating longrange interactions. subsection describe method partitioning input point cloud parts simple shape. objective retrieve individual objects cars chairs rather break objects simple parts seen figure however clusters geometrically simple expect semantically homogeneous well i.e. cover objects different classes. note step pipeline purely unsupervised makes class labels beyond validation. follow global energy model described computational efﬁciency. another advantage segmentation adaptive local geometric complexity. words segments obtained large simple shapes roads walls well much smaller components parts chair. consider input point cloud points. point deﬁned position available observations color intensity. point compute geometric features characterizing shape local subsection describe compute well features. structured representation point cloud deﬁned oriented attributed graph whose nodes superpoints edges represent adjacency superpoints. superedges annotated features re×df characterizing adjacency relationship superpoints. superedge features also derived comparing shape size adjacent superpoints. compute number points comprised superpoint well shape features length surface volume derived eigenvalues covariance positions points comprised superpoint sorted decreasing value. table describe list different superedge features used paper. note break symmetry edge features makes directed graph. superpoint embedding goal stage compute descriptor every superpoint embedding vector ﬁxed-size dimensionality note superpoint embedded isolation; contextual information required reliable classiﬁcation provided following stage means graph convolutions. several deep learning-based methods proposed purpose recently. choose pointnet remarkable simplicity efﬁciency robustness. pointnet input points ﬁrst aligned spatial transformer network independently processed multilayer perceptrons ﬁnally max-pooled summarize shape. case input shapes geometrically simple objects reliably represented small amount points embedded rather compact pointnet. important limit memory needed evaluating many superpoints current gpus. particular subsample superpoints on-the-ﬂy points maintain efﬁcient computation batches facilitate data augmentation. superpoints less points sampled replacement principle affect evaluation pointnet max-pooling. however observed including small superpoints less nminp points training harms overall performance. thus embedding superpoints zero classiﬁcation relies solely contextual information. order pointnet learn spatial distribution different shapes superpoint rescaled unit sphere before embedding. points represented normalized position observations geometric features furthermore original metric diameter superpoint concatenated additional feature pointnet max-pooling order stay covariant shape sizes. ﬁnal stage pipeline classify superpoint based embedding local surroundings within spg. graph convolutions naturally suited task. section explain propagation model system. approach builds ideas gated graph neural networks edge-conditioned convolutions general idea superpoints reﬁne embedding according pieces information passed along superedges. concretely superpoint maintains state hidden gated recurrent unit hidden state initialized embedding processed several iterations iteration takes hidden state incoming message input computes hidden state superpoint computed weighted hidden states neighboring superpoints actual weighting superedge depends attributes fji· listed element-wise multiplication sigmoid function trainable parameters shared among grus. equation lists standard rules update gate improve stability training equation apply layer normalization deﬁned )/+\u0001) separately linearly transformed input transformed hidden state small constant. finally model includes three interesting extensions equations detail below. input gating. argue possess ability down-weight input vector based hidden state. example might learn ignore context class state highly certain direct attention speciﬁc feature channels. equation achieves gating message hidden state using input edge-conditioned convolution. plays crucial role model dynamically generate ﬁltering weights value continuous attributes fji· processing multi-layer perceptron original formulation regresses weight matrix perform matrix-vector multiplication edge. shown generalize regular convolution grids also brings increased runtime elevated memory usage high number parameters. work propose regress edge-speciﬁc weight vector perform element-wise multiplication equation efﬁcient combination update rules reached similar level performance initial experiments. although network explicitly ﬁnal sigmoid activation function element-wise multiplication gives option learn edge-conditioned gate. finally remark shared time iterations self-loops necessary existence hidden states grus. state concatenation. inspired densenet concatenate hidden states time steps linearly transform produce segmentation logits equation allows exploit dynamics hidden states increasing receptive ﬁeld ﬁnal classiﬁcation. relation crfs. postprocessing convolutional outputs using conditional random fields widely popular. several inference algorithms formulated network layers amendable end-to-end learning possibly general pairwise potentials method information propagation shares characteristics grus operate dz-dimensional intermediate feature space richer less constrained low-dimensional vectors representing beliefs classes also discussed enhanced access information motivated desire learn powerful representation context goes beyond belief compatibilities well desire able discriminate often relatively weak unaries empirically evaluate claims section details adjacency graphs. paper different adjacency graphs points input clouds section gvor section indeed different deﬁnitions adjacency different advantages. voronoi adjacency suited capture long-range relationships superpoints beneﬁcial spg. nearest neighbors adjacency tends connect objects separated small empty gap. desirable global energy tends produce many small connected components decreasing embedding quality. fixed radius adjacency avoided general handles variable density lidar scans poorly. also clip edges longer dataset-speciﬁc distance gvor discard meaningless superedges. training. geometric partitioning step unsupervised superpoint embedding contextual segmentation trained jointly supervised cross entropy loss. superpoints assumed semantically homogeneous hypothesis semantic homogeneity consequently assigned hard ground truth label corresponding majority label among contained points. also considered using soft labels corresponding normalized histograms point labels training kullback-leibler divergence loss. performed slightly worse initial experiments though. naive training large spgs approach memory limits current gpus. circumvent issue randomly subsampling sets superpoints iteration training induced subgraphs i.e. graphs composed subsets nodes original edges connecting them. speciﬁcally graph neighborhoods order sampled select superpoints nminp points smaller superpoints embedded. note induced graph union small neighborhoods relationships many hops still formed learned. strategy also doubles data augmentation strong regularization together randomized sampling point clouds described section additional data augmentation performed randomly rotating superpoints around vertical axis jittering point features gaussian noise truncated testing. modern deep learning frameworks testing made memory-efﬁcient discarding layer activations soon follow-up layers computed. practice able label full spgs once. compensate randomness subsampling point clouds pointnets average logits obtained runs different seeds. evaluate pipeline currently largest point cloud segmentation benchmarks semanticd stanford large-scale indoor spaces state art. furthermore perform ablation study pipeline section even though data sets quite different nature nearly model both. deep model rather compact memory enough testing training. refer appendix precise details hyperparameter selection architecture conﬁguration training procedure. performance evaluated using three metrics per-class intersection union per-class accuracy overall accuracy deﬁned proportion correctly classiﬁed points. stress metrics computed original point clouds superpoints. semanticd rural scenes. point intensity values dataset consists training scans test scans withheld labels. also evaluate reduced subsampled scans common past work. table provide results algorithm compared state-of-the-art recent algorithms figure provide qualitative results framework. framework improves signiﬁcantly state semantic segmentation data i.e. nearly miou points reduced nearly miou points full set. particular observe steep gain artefact class. explained ability partitioning algorithm detect artifacts singular shape hard capture using snapshots suggested furthermore small object often merged road performing spatial regularization. sdis dataset consists point clouds ﬂoors three different buildings split individual rooms. evaluate framework following dominant strategies found previous works. advocated perform -fold cross validation microaveraging i.e. computing metrics merged predictions test folds. following also report performance ﬁfth fold corresponding building present folds. since classes data cannot partitioned purely using geometric features concatenate color information geometric features partitioning step. quantitative results displayed table qualitative results figure appendix sdis difﬁcult dataset hard retrieve classes white boards white walls columns within walls. quantitative results framework performs better methods average. notably doors able correctly classiﬁed higher rate approaches long open illustrated figure indeed doors geometrically similar walls position respect door frame allows network retrieve correctly. hand partition merges white boards walls depriving network opportunity even learn classify them boards theoretical perfect classiﬁcation superpoints computation time. table report computation time different steps framework inference area. partition step performed deep model runs titan black. partition input clouds takes computation time allows inference step faster operates much smaller spgs only. note feature computation partitioning step fully optimized would beneﬁt greatly parallelization. better understand inﬂuence various design choices made framework compare several baselines perform ablation study. lack public ground truth test sets semanticd evaluate performance limits. contribution contextual segmentation bounded above. lower bound estimated training pointnet otherwise architecture denoted pointnet directly predict class logits without grus. upper bound corresponds assigning superpoint ground truth label thus sets limit performance geometric partition. contextual segmentation able roughly miou points unaries conﬁrming importance. nevertheless learned model still room miou points improvement miou points forfeited semantic inhomogeneity superpoints. crfs. compare effect gru+eccbased network crf-based regularization. baseline post-process unary outputs inference connectivity scalar transition matrix described next adapt crf-rnn framework zheng general graphs edgeconditioned convolutions train pointnet end-to-end. finally modify best pointnet. observe icrf barely improves accuracy expected since partitioning step already encourages spatial regularity. better end-to-end learning edge attributes though still performs complex operations enforce normalization embedding. nevertheless channels used best instead used provide even freedom feature representation ablation. explore advantages several design choices individually removing best order compare framework’s performance without them. noinputgate remove input gating gru; noconcat consider last hidden state output instead concatenation steps; nogeomfeat geometric features input pointnet; noedgefeat perform homogeneous regularization setting superedge features scalar original formulation matrix-vector multiplication equation ﬁrst three choices accounts miou points. next without edge features method falls back even icrf level unary validates design overall motivation spg. finally original formulation brings marginal increase performance paid higher demand resources. appendix perform similar analysis superedge features. presented deep learning framework performing semantic segmentation large point clouds based partition simple shapes. showed spgs allow effective deep learning tools wouldn’t able handle data volume otherwise. method signiﬁcantly improves state-of-the-art publicly available datasets. experimental analysis suggested future improvements made partitioning learning deep contextual classiﬁers. former plan optimize computational bottleneck geometric partitioning step well obtaining homogeneous clusters.", "year": 2017}