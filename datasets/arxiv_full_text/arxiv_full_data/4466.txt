{"title": "Depth CNNs for RGB-D scene recognition: learning from scratch better  than transferring from RGB-CNNs", "tag": ["cs.CV", "stat.ML"], "abstract": "Scene recognition with RGB images has been extensively studied and has reached very remarkable recognition levels, thanks to convolutional neural networks (CNN) and large scene datasets. In contrast, current RGB-D scene data is much more limited, so often leverages RGB large datasets, by transferring pretrained RGB CNN models and fine-tuning with the target RGB-D dataset. However, we show that this approach has the limitation of hardly reaching bottom layers, which is key to learn modality-specific features. In contrast, we focus on the bottom layers, and propose an alternative strategy to learn depth features combining local weakly supervised training from patches followed by global fine tuning with images. This strategy is capable of learning very discriminative depth-specific features with limited depth images, without resorting to Places-CNN. In addition we propose a modified CNN architecture to further match the complexity of the model and the amount of data available. For RGB-D scene recognition, depth and RGB features are combined by projecting them in a common space and further leaning a multilayer classifier, which is jointly optimized in an end-to-end network. Our framework achieves state-of-the-art accuracy on NYU2 and SUN RGB-D in both depth only and combined RGB-D data.", "text": "trast data crowdsourced crawling rgb-d data needs captured specialized relatively complex setup. reason rgbdatasets orders magnitude smaller largest datasets also much fewer categories. prevents training deep cnns properly handcrafted features still better choice modality. however recent rgb-d dataset signiﬁcantly larger previous rgb-d scene datasets still large enough train scratch deep cnns comparable size counterparts least provides enough data tuning deep models without signiﬁcant overﬁtting. approach typically exploit encoding depth data since also three channel representation fine tuning typically used target dataset limited data another large dataset covers similar domain exploited ﬁrst train deep model. thus transferring features tuning depth data common practice learn deep representations depth data. however although images resemble images shapes objects identiﬁed really reasonable reusing features inter-modal scenario? paper focus low-level differences data show large number low-level ﬁlters either useless ignored tuning network hha. figure shows average activation ratio ﬁlters layer conv places-cnn different input data network properly designed trained tends show balanced activation rate curve meaning ﬁlters contributing almost equally build discriminative representations. transferred scene datasets scene recognition images extensively studied reached remarkable recognition levels thanks convolutional neural networks large scene datasets. contrast current rgb-d scene data much limited often leverages large datasets transferring pretrained models ﬁne-tuning target rgb-d dataset. however show approach limitation hardly reaching bottom layers learn modality-speciﬁc features. contrast focus bottom layers propose alternative strategy learn depth features combining local weakly supervised training patches followed global tuning images. strategy capable learning discriminative depthspeciﬁc features limited depth images without resorting places-cnn. addition propose modiﬁed architecture match complexity model amount data available. rgb-d scene recognition depth features combined projecting common space leaning multilayer classiﬁer jointly optimized end-to-end network. framework achieves state-of-the-art accuracy rgb-d depth combined rgb-d data. success visual recognition mainly depends feature representing input data. scene recognition particular beneﬁted recent developments ﬁeld. notably massive image datasets provide necessary amount data train complex convolutional neural networks millions parameters without falling overﬁtting. features extracted models pretrained datasets shown generic powerful enough obtain state-of-the-art performance smaller datasets sun) using ﬁne-tuning outperforming earlier handcrafted paradigms cost depth sensors capture depth information addition data. depth provide valuable information model object boundaries understand global layout objects scene. thus rgb-d models improve recognition mere models. however conuse quantized feature segmentation scene classiﬁcation. recently multi-layered networks learn features directly large amounts data. socher single layer trained unsupervisedly patches combined recurrent convolutional network gupta r-cnn depth images detect objects indoor scenes. since training data limited augment training rendering additional synthetic scenes. current state-of-the-art relies transferring tuning places-cnn depth data. wang extract deep features local regions whole images combine features depth patches images component aware fusion method. approaches propose incorporating architectures ﬁne-tune jointly depth image pairs. jointly ﬁne-tune depth models including multi-model fusion layer simultaneously considering inter intra-modality correlations meanwhile regularizing learned features compact discriminative. alternatively gupta propose transfer model depth data according depth image pairs. paper avoid relying large biased models obtain depth features train depth cnns using weak supervision directly depth data learning truly depth-speciﬁc discriminative features compared transferred adapted biased models. weakly-supervised cnns recently several works propose weakly supervised frameworks specially object detection oquab propose object detection framework tune pretrained cnns multiple regions global max-pooling layer selects regions used tuning. durand extend idea selecting useful useless regions maximum minimum mixed pooling layer. weakly supervised detection network uses region proposal method select regions. works rely cnns already pretrained large datasets weak supervision used subsequent tuning adaptation stage improve ﬁnal features particular task. contrast motivation training data scarce weakly supervised rely pretrained cnns. fact used pretrain convolutional layers prior tuning full images. figure depth modalities. examples scenes captured depth middle average nonzero activations ﬁlter conv layer places-cnn different datasets. bottom conv ﬁlters ordered mean activation rgb-d hha. curve still similar showing majority conv ﬁlters still useful scenes indoor however curve shows completely different behavior subset ﬁlters relevant large number rarely activated useful edges smooth gradients common while gabor-like patterns high frequency patterns seldom found data. thus observe ﬁlters bottom layers crucial. however conventional full tuning cnns hardly reach explore ways make better limited data focusing bottom layers. particular compare strategies tuning bottom layers propose weakly supervised strategy learn ﬁlters directly data. addition combine pretrained depth networks network tuned rgb-d image pairs. show experimentally features lead state-of-the-art performance depth rgb-d provide insights evidences. code available https//github.com/ songxinhang/d-cnn rgb-d scene recognition earlier works handcrafted features engineered expert capture speciﬁc properties considered representative. gupta propose method detect contours depth images segmentation further quantize segmentation outputs local features scene classiﬁcation. banica quantize local features second order pooling figure strategies tune places-cnn withdepth data layers bottom layers frozen bottom layers layers frozen; bottom layers column represents particular setting. transfer) well studied. general low-level ﬁlters bottom layers capture generic patterns real visual world reused effectively datasets modality similar characteristics. thus tuning layers often enough. layers dataset-speciﬁc need rewired target categories. contrast depth images signiﬁcantly different low-level visual patterns regularities since bottom convolutional layers essential capture modality-speciﬁc visual appearances regularities tuning layers seem insufﬁcient adapt properly depth data. since want focus bottom layers compare conventional tuning strategies reach bottom layers better strategies differ basically layers tuned trained remain unaltered. using alexnet architecture pretrained places-cnn ﬁrst compare three strategies ft-top conventional method layers tuned ft-bottom bottom layers frozen ft-keep layers directly removed. note always trained since must resized according target number categories. classiﬁcation accuracy depth data different strategies shown fig. fine tuning layers help signiﬁcantly including bottom convolutional layers opposite tuning tuning layers almost enough reach maximum gain extending bottom layers helps marginally. contrast tuning three bottom layers achieves higher tuning layers. furthermore tuning removing layers also comparable tuning layers. results support intuition bottom layers much important layers transferring depth convenfigure comparisons different learning strategies depth images accuracy obtained softmax tuning strategies illustrated fig. training strategies fig. figure visualizing ﬁrst convolutional layer places-cnn; full tuned places-cnn; ftbottom ft-keep conv; train-alex-cnn train-alex-cnn training patches wsp-cnn kernel size pixels training patches train-d-cnn methods trained/ﬁne tuned using depth data rgb-d. insight conv layer provide complementary insight bottom layers important focus ﬁlters ﬁrst convolutional layer i.e. conv shown fig. although gain accuracy observed particular ﬁlters noticeable changes tuning process. suggest reusing ﬁlters thus trying rgb-like patterns depth data. additionally fig. middle shows large number ﬁlters placescnn signiﬁcantly underused depth data observations suggest reusing places-cnn ﬁlters conv bottom layers good idea. moreover since ﬁlters also represent tunable parameters results model many parameters difﬁcult train limited data. extract multiple patches single image increasing amount training data. second include weakly supervised training. patches typically cover objects parts whole scene. however since local labels scene category weak label since know patches given image belong scene. refer network weakly supervised patchcnn weak supervision patches weakly-supervised strategy used alex-cnn training. ﬁrst sample grid patches pixels weakly-supervised pretraining tune full images. switching architecture wsp-cnn full alex-cnn amount connections changes. thus weights convolutional layers pretrained wsp-cnn copied tuning similarly weakly supervised methods. fig. shows using weak supervision patches significantly outperforms training full image train-alex-cnn furthermore conv ﬁlters shown fig. depth speciﬁc-patterns much evident fig. nevertheless still show signiﬁcant amount noise suggests alexnet still complex perhaps kernels depth data. since complexity depth images signiﬁcantly lower images reduced size kernels layer train wspcnn consists convolutional layers shallow deep. visualization conv illustrated fig. however cannot observe visual regularities visualizations even though shallow network. order adapt amount training data complexity model modify slightly training procedure. first reduce support working patches rather whole image. thus networks simpler architectures fewer trainable parameters capable. additionally depth-cnn bottom shows full architecture depth-cnn training wsp-cnn transfer weights convolutional layers. output conv d-cnn almost times larger output pool alex-cnn leads times parameters part. order reduce parameters include three spatial pyramid pooling layers size also captures spatial information allows train model end-to-end. proposed train-d-cnn outperforms tuning weakly-supervised training alex-cnn comparing visualizations fig. proposed wsp-cnn d-cnn learn representative kernels supporting better performance. also suggests smaller kernel size suitable depth data. previous works independent networks depth optimizing different independent stages fusion parameters tuning classiﬁcation. contrast integrate rgb-cnn depth-cnn fusion procedure integrated rgb-d-cnn trained end-to-end jointly learning fusion parameters tuning layers depth layers branch. fusion mechanism network fully connected layers followed loss concatenated feature frgb rdr× fdepth rdd× depth features respectively. ﬁrst layer fusion network learns modality-speciﬁc projections common feature space recent works exploit metric learning fisher vector correlation analysis reduce redundancy joint rgb-d representation. important note step particularly effective depth features signiﬁcantly correlated. likely case recent works depth feature extractors tuned versions model case depth models learned directly data independently already much less correlated even without explicit multimodal analysis. dataset evaluate approach datasets depth dataset second version rgb-d. former relatively small dataset indoor categories well represented. following split categories reorganized categories including common categories ’other’ category consisting remaining categories. training/test split images. rgb-d contains categories rgbimages. following publicly available split common categories selected consisting images training images test. classiﬁer features since found training linear classiﬁers output fully connected layer increases slightly performance following results svms speciﬁed. variant uses category-speciﬁc weights during training compensate imbalance training data. weight {w...wk} egory computed number training images category. select cross validation. depth rgb-d places-cnn ft-places-cnn ft-places-cnn ft-places-cnn ft-places-cnn ft-places-cnn ft-places-cnn ft-places-cnn ft-places-cnn ft-places-cnn ft-places-cnn ft-places-cnn r-cnn used features train classiﬁers. select different models comparison table alexcnn architecture bottom convolutional layers training scratch. alex-cnn architecture bottom layers trained scratch perform better transferred places-cnn even though layers worse. using weakly-supervised training patches performance comparable tuned places-cnn layers better bottom layers smaller model without relying places data. d-cnn consistently achieves best performance. comparisons depth data compare related methods depth recognition table fair comparison also implement places-cnn tuning. d-cnn outperforms ft-places-cnn+spp accuracy. models using weighted training d-cnn works even better. rgb-d fusion compare state-of-the-art works rgb-d indoor recognition table discriminative rgb-d fusion proposed joint feature fusion rgb-d scene objects proposed proposed rgb-d-cnn outperforms rgb-d fusion method comparisons nyud compare rgb-d-cnn state-of-the-art nyud table gupta propose encode segmentation responses features scene recognition. rgb-d-cnn largely outperforms work segmentation based handcrafted features. comparing rgb-d fusion achieves gain accuracy without including r-cnn models work. transferring deep representations within modality works well since low-level patterns similar distributions bottom layers reused adjusting datasetspeciﬁc layers. however tuning effective inter-modal transfer places-cnn depth space low-level features require modalityspeciﬁc ﬁlters. paper focus bottom layers critical represent depth data properly. reducing number parameters network using weakly supervised learning patches complexity model matches better amount data available. depth representation discriminative tuned places-cnn also combined features gain higher showing complementary. notice also depend large datasets places. work supported part national natural science foundation china grant grant part national high technology research development program china grant part beijing municipal commission science technology grant part lenovo outstanding young scientists program part national program special support eminent professionals national program support top-notch young professionals. zhou lapedriza xiao torralba oliva learning deep features scene recognition using places database. ghahramani welling cortes lawrence weinberger eds. nips", "year": 2018}