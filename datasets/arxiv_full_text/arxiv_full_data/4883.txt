{"title": "Extrapolation and learning equations", "tag": ["cs.LG", "cs.AI", "68T05, 68T30, 68T40, 62J02, 65D15", "I.2.6; I.2.8"], "abstract": "In classical machine learning, regression is treated as a black box process of identifying a suitable function from a hypothesis set without attempting to gain insight into the mechanism connecting inputs and outputs. In the natural sciences, however, finding an interpretable function for a phenomenon is the prime goal as it allows to understand and generalize results. This paper proposes a novel type of function learning network, called equation learner (EQL), that can learn analytical expressions and is able to extrapolate to unseen domains. It is implemented as an end-to-end differentiable feed-forward network and allows for efficient gradient based training. Due to sparsity regularization concise interpretable expressions can be obtained. Often the true underlying source expression is identified.", "text": "classical machine learning regression treated black process identifying suitable function hypothesis without attempting gain insight mechanism connecting inputs outputs. natural sciences however ﬁnding interpretable function phenomenon prime goal allows understand generalize results. paper proposes novel type function learning network called equation learner learn analytical expressions able extrapolate unseen domains. implemented end-to-end differentiable feed-forward network allows efﬁcient gradient based training. sparsity regularization concise interpretable expressions obtained. often true underlying source expression identiﬁed. quality model typically measured ability generalize training previously unseen data distribution. regression tasks generalization essentially boils interpolation training data sufﬁciently dense. long models selected correctly overﬁt data regression problem well understood least conceptually considered solved. however working data real-world devices controlling robotic interpolation might sufﬁcient. could happen future data lies outside training domain temporarily operated outside speciﬁcations. sake robustness safety desirable case regression model continues make good predictions least fail catastrophically. setting call extrapolation generalization topic present paper. particularly interested regression tasks systems described real-valued analytic expression mechanical systems pendulum robotic arm. typically governed highly nonlinear function nevertheless possible principle infer behavior extrapolation domain behavior elsewhere. make main contributions type network learn analytical expressions able extrapolate unseen domains model selection strategy tailored extrapolation setting. following section describes setting regression extrapolation. afterwards introduce method discuss architecture training relation prior art. present results section experimental evaluation close conclusions. regression extrapolation consider multivariate regression problem training assume data originates unknown analytical function additive zero-mean noise function instance reﬂect system ordinary differential equations govern movements robot like. general task learn function approximates true functional relation well possible squared loss sense achieves minimal expected error practice particular examples training test data sampled distribution speak interpolation problem. extrapolation setting training data assumed cover limited range data domain. example robot instance training restricted certain joint angle range maximal velocity. testing want make predictions unseen domains higher velocities. usual split data available training time part model training part validation model selection. main model propose multi-layered feed-forward network computational units speciﬁcally designed extrapolation regression tasks. l-layer network hidden layers consisting linear mapping followed non-linear transformations. simplicity notation explain network hidden layer structure inputs outputs). practice layer designed independently others course long input/output dimensions match. output previous layer convention weight matrix rd×k bias vector free parameters learned training. non-linear transformation contains unary units binary units total nonlinear stage outputs inputs. unary units receive respective component inputs unit following base functions speciﬁed ﬁxed type parameter sigm +e−z standard sigmoid function. binary units receive remaining component zu+v input pairs two. multiplication units compute product input values proposed network architecture differs main aspects typical feed-forward networks existence multiplication units possibility sine cosine nonlinearities unary units. design choices motivated objective learning system equations govern physical system extrapolate parts input space. sigmoid nonlinearities canonical choice activation function artiﬁcial neural networks proved successful. fact include sigmoids architecture making super class anns. however typically disabled training procedure corresponding absence considered physical equations. other predominantly local nonlinearities particular radial basis functions include since cannot expect extrapolate all. nonlinearities roots logarithms could principle useful learning physical equations pose problems domains deﬁnition restricted positive inputs. leave task incorporating principled future work. ability multiply values second crucial component network architecture. again inspired typical form physical equations multiplication components arguably second common basic operation addition multiplication introduced neural networks long product-units pi-sigma-unit product-units large fan-in compute products inputs potentiated respective weights. result typically behavior high order polynomial powerful function approximators rarely occur physical equations. polynomials also known require careful ﬁne-tuning order overﬁt makes risky choice purpose extrapolation. pi-sigma units multiplication units ﬁxed number factors multiplication units special factors. multiplying values time well adjusted task allows control maximal degree learned polynomial depth network. denotes current mini-batch stepsize parameter. choice adam critical standard stochastic gradient descent also works. numerical experiments mini-batch size role regularization encourage networks sparse connections matching intuition typical formula describing physical system contains small number terms operating variables. however non-convex setting local minima likely occur type regularization undesirable side-effect course optimization weights hardly ever change sign. reason regularization leads constant rate weight decay whereas counteracting derivative respect square loss proportional backpropagated error signal input unit. latter contributions often smaller along paths small weights many weights zero stay there. additionally non-zero regularization term causes learned weights reﬂect trade-off minimizing loss regularizer. although lead improved generalization also results systematic underestimation function values. therefore follow hybrid regularization strategy beginning training procedure regularization parameters vary freely reach reasonable starting points. afterwards switch regularization setting nonzero value effect sparse network structure emerges. finally last steps training disable regularization enforce norm weights. achieved keeping weights ...l close remaining epochs. ensures learned model ﬁnds function right parametric form also observed values closely possible. observed exact choice breakpoints critical. practice total number update steps. selected large enough ensure convergence. note convergence sparse structure important here early stopping disadvantageous. networks number hyper-parameters number layers number units regularization constant. unfortunately standard techniques model selection evaluation hold-out cross-validation optimal purpose since rely interpolation quality. order extrapolate network right formula. tell? using occams razor principle simplest formula likely right one. intuitively choice truncated power series approximation ﬁrst preferred. number active hidden units network proxy complexity formula appendix details. could also think differentiating unit types. case argumentation correct model explains data well validation error. dual objective minimize solve ranking instances validation error sparsity select smallest norm ﬁeld machine learning regression often treated black process identifying suitable realvalued function hypothesis reproducing kernel hilbert space gaussian processes regression support vector regression multi-layer network suitable expressive power goal prediction function leads small expected error future data necessarily gain insight mechanism output values derive inputs. goal ﬁnding interpretable function rather common natural sciences biology high noise levels strong inter-system variability often make important rely external prior knowledge ﬁnding biologically plausible model often preferable ﬁnding makes highest prediction accuracy. consequence model classes often highly constrained allowing sparse linear models. task learning true nonlinear functional dependence observing physical system received little attention machine learning literature forms basis ﬁeld system identiﬁcation. there typically functional form system known parameters identiﬁed. another approach model time evolution autoregressive models higher order convolution integrals learning analytic formulas common. causal learning area recent research aims identifying causal relation multiple observables typically result physical process. classically tasks reduces ﬁnding minimal graphical model based tests conditional independence although successful ﬁelds classical approach provides factorization problem separating causes effects leaves exact functional dependency unexplained. recent extensions causal learning take functional view typically constrain regression functions physically plausible ones rather constrain noise distributions topic learning regression function emphasis extrapolation performance studied much literature far. existing work time series prediction deals extrapolation temporal domain predict next value nomenclature typically rather interpolation task prediction based behaviour series earlier time steps similar value distribution extrapolating data domain implies data distribution prediction time differ data distribution training time. traditionally called domain adaptation setting. particular since assume common labeling function setting would fall covariate shift setting unfortunately connection particularly useful problem. domain adaptation typically make additional assumptions data distribution change existing methods need access unlabeled data test distribution already training time setting possible obtain. technical level networks instance general feed-forward networks function approximation contrast recent trends towards deep learning goal learn data representation learn function compactly represents input-output relation generalizes different regions data space like physical formula. structurally networks resemble sum-product networks pi-sigma networks sense based directed acyclic graphs computational units allows summation multiplication. otherwise spns different efﬁcient alternative probabilistic graphical models representing probability distributions whereas networks meant classical task function approximation. psns output needs passed multiplicative units whereas multiplication optional. finding equations observations also known symbolic regression search performed certain function space typically done evolutionary computation. techniques possible discover physical laws invariants conserved quantities unfortunately computational complexity/search time explodes larger expressions high-dimensional problems. attempt circumvent modeling gradient based optimization problem. related symbolic regression ﬁnding mathematical identities instance computationally efﬁcient expressions. done using machine learning overcome potentially exponential search space. demonstrate ability learn physically inspired models good extrapolation quality experiments synthetic real data. this implemented network training evaluation procedure python based theano framework make code training evaluation public acceptance manuscript. second value angular velocity. physics literature usually denoted purposes call order keep notation consistent experiments. pendulum’s dynamic behavior governed following ordinary differential equations training data sample points uniformly hypercube note domain contains half sine period sufﬁcient identify analytic expression. target values disturbed gaussian noise standard derivation also deﬁne three test sets points. interpolation test sampled data distribution training set. extrapolation test contains data sampled uniformly data domain relatively near training region extrapolation test extends region outside ×\\×. train -layer perform model selection among hyper-parameters regularization strength {−−.−−.−−.−−.−} weights randomly initialized normal distribution number nodes chose epochs. compare algorithm standard multilayer perceptron tanh activation functions possible hyperparameters number layers number neurons second baseline given epsilon support vector regression hyperparameters {−−−.} {−−−} using radial basis function kernel width numeric results reported tab. expected models able interpolate well test error order noise level extrapolation however performance differ approaches. prediction quality decreases quickly leaving training domain. remains better near extrapolation also fails catastrophically extrapolation data. hand extrapolates well near away training domain. reasons seen figure simply learns function interpolates training values ﬁnds correct functional expression therefore predicts correct values input data. table numeric results pendulum dataset. reported mean standard deviation root mean squares error different test sets random initializations. extrapol. figure learning pendulum dynamics. slices outputs inputs true system equation instances. shaded area marks training region vertical bars show size near extrapolation domain. learned networks. numbers edges correspond entries numbers inside nodes show bias values weights orphan nodes omitted. learned formulas correct symmetry double pendulum kinematics. second system consider real double pendulum forward kinematics learned. recorded trajectories real double pendulum task learn position tips double pendulum segments given joint angles positions measured supply following formula correspond x-y-coordinates ﬁrst second end-point respectively. dataset contains short trajectories. ﬁrst covers part domain consists samples used validation fig. second trajectory corresponds behavior several spins pendulum segments much larger domain covered. nevertheless angle values conﬁned trajectory extrapolation test set. trajectory outputs method shown fig. prediction unseen domains perfect also illustrated systematic sweep fig. performance already near training domain. better still give usable predictions test data also root means square error fig. figure double pendulum kinematics. training trajectory extrapolation test trajectory output learned instance. slices output inputs true system instances. numeric results tab. details. note predicting would yield mean error table extrapolation performance kinematic robotic arms. tab. details. standard deviations random initializations. interpolation error methods around kin--all robotic arms. complicated task learn forward kinematics multi-segment robotic arms. consider planar arms joints segment units long. training controlled sinusoidal joint target angles amplitude joint different frequency. number data points segment arms respectively added noise above. testing extrapolation performance amplitude used. note extrapolation space much larger training space. task predict coordinates end-effector arms coordinates segment positions kin--all. numerical results tab. shows method able extrapolate cases. model selection layer number illustrate dependence amount noise number available training points provide quantiﬁcation appendix short increasing noise compensated increasing amount data keep performance. ﬁrst equation requires hidden layer represented. second equation third equation requires hidden layers. particular contains product contains product three terms test restriction pairwise product units causes problems complex target functions. follow procedure pendulum case building training test sets though input data range. points training validation points test sets. model selection performed using number layers number units select well table shows numerical results. again methods able interpolate achieves good extrapolation results except equation settles cases local minimum ﬁnds approximating equation deviates outside training domain. interestingly restrict base functions contain cosine algorithm ﬁnds right formula. note sparsity correct formula lower approximation selected found. figure fig. illustrates performance learned networks visually. shows model-selected instances case. correct formula identiﬁed correct predictions made even outside training region network provided surprise yields good extrapolation performance hidden layer implement cos? apparently uses good approximation sparsity solution whereas true solution needs least explains selection. suboptimal local minima uses strange approximating using deviates fast however true solution would sparser found. remove cosine base functions always correct formula fig. x-ray transition energies. example consider data measured atomic physics. shooting electron beams onto atoms excite consequently emit x-ray radiation characteristic peak energies. element/isotope energies different correspond potential difference electron shells identify elements probe way. data taken consider speciﬁc transition called line measured elements. true relationship atomic number transition energies complicated involves many body interactions closed-form solution exists. nevertheless relationships system proposes. known main relationship according moseley’s law. correction figure formula learning analysis. single input space true system equation instance mlp. shows learned networks correspondingly fig. details. formula representations extracted networks. algorithm fails overcomplete base typically ends local minima. less base function right formula found. results presented. text discussion. figure x-ray transition energies. measured data predicted values visualized prediction error methods train/validation splitting. solutions model selection validation error sparsity space appendix details. numeric results. reported errors standard deviation independent train/validation splits. real units error well difference neighboring high-z elements. learned formulas different sparsities terms elements larger potentially higher order. data elements split training/validation sets range extrapolation test interval since little data evaluate performance independent training/validation splits. data scaled kα/. model selection based validation error only. selection sparsity validation error yields relationship. mini-batch size used. figure presents data predictions learned formulas numerical results. achieve similar performance signiﬁcantly worse. however also yields interpretable formulas fig. used gain insights potential relationship. consider pendulum attached cart move horizontally along rail attached spring damper system fig. system parametrized unknowns position cart velocity cart angle pendulum angular velocity pendulum. combine four-dimensional vector formulas contain divisions included architecture singularities. incorporate principled manner left future work. thus cart-pendulum dynamics outside hypothesis class. case cannot expect great extrapolation performance conﬁrmed experiments. fig. extrapolation performance illustrated slicing input space. near extrapolation performance still acceptable soon training region left even best instances differ considerably true values also numeric results tab. performing poorly also near extrapolation range. inspecting learned expressions sigmoid functions rarely used. conclusions presented network architecture called learn analytic expressions typically occur equations governing physical particular mechanical systems. network fully differentiable allows figure cart-pendulum system. sketch system. lengths masses gravitation constant friction constant slices outputs inputs true system equation best instances. end-to-end training using backpropagation. sequencing regularization ﬁxing norm achieve sparse representations unbiased estimation factors within learned equations. also introduce model selection procedure speciﬁcally designed select good extrapolation quality multiobjective criterion based validation error sparsity. proposed method able learn functional relations extrapolate unseen parts data space demonstrate experiments synthetic well real data. approach learns concise functional forms provide insights relationships within data show physical measurements x-ray transition energies. optimization problem nontrivial many local minima. shown cases algorithm reliably ﬁnding right equation instead ﬁnds approximation only case extrapolation poor. origin data hypothesis class underlying expression cannot represented network good extrapolation performance cannot achieved. thus important increase model class incorporating base functions address future work alongside application larger examples. expect good scaling capabilities larger systems gradient based optimization. apart extrapolation also expect improved interpolation results high-dimensional spaces data less dense.", "year": 2016}