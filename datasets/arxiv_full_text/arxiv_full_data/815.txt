{"title": "Trial without Error: Towards Safe Reinforcement Learning via Human  Intervention", "tag": ["cs.AI", "cs.LG", "cs.NE"], "abstract": "AI systems are increasingly applied to complex tasks that involve interaction with humans. During training, such systems are potentially dangerous, as they haven't yet learned to avoid actions that could cause serious harm. How can an AI system explore and learn without making a single mistake that harms humans or otherwise causes serious damage? For model-free reinforcement learning, having a human \"in the loop\" and ready to intervene is currently the only way to prevent all catastrophes. We formalize human intervention for RL and show how to reduce the human labor required by training a supervised learner to imitate the human's intervention decisions. We evaluate this scheme on Atari games, with a Deep RL agent being overseen by a human for four hours. When the class of catastrophes is simple, we are able to prevent all catastrophes without affecting the agent's learning (whereas an RL baseline fails due to catastrophic forgetting). However, this scheme is less successful when catastrophes are more complex: it reduces but does not eliminate catastrophes and the supervised learner fails on adversarial examples found by the agent. Extrapolating to more challenging environments, we show that our implementation would not scale (due to the infeasible amount of human labor required). We outline extensions of the scheme that are necessary if we are to train model-free agents without a single catastrophe.", "text": "systems increasingly applied complex tasks involve interaction humans. training systems potentially dangerous haven’t learned avoid actions could cause serious harm. system explore learn without making single mistake harms humans otherwise causes serious damage? model-free reinforcement learning human loop ready intervene currently prevent catastrophes. formalize human intervention show reduce human labor required training supervised learner imitate human’s intervention decisions. evaluate scheme atari games deep agent overseen human four hours. class catastrophes simple able prevent catastrophes without affecting agent’s learning however scheme less successful catastrophes complex reduces eliminate catastrophes supervised learner fails adversarial examples found agent. extrapolating challenging environments show implementation would scale outline extensions scheme necessary train model-free agents without single catastrophe. systems increasingly applied complex tasks involve interaction humans. training systems potentially dangerous haven’t learned avoid actions would cause serious harm. system explore learn without making single mistake harms humans destroys property damages environment? crucial safeguard danger human intervention. self-driving cars overseen human drivers take control predict system perform badly. overseers frequently intervene especially self-driving systems early stage development safeguard used human learners overseen licensed driver. many systems pose physical danger humans. web-based systems still cause unintended harm. microsoft’s chatbot reproduced thousands offensive tweets taken facebook’s algorithms sharing news stories inadvertently provided platform malicious false stories disinformation election human operators monitored systems real-time outcomes could avoided. human oversight currently means avoiding accidents complex real-world domains. human intervention safety together deep learning reinforcement learning likely components future applied systems? present scheme human intervention systems test scheme atari games. document serious scalability problems human intervention applied outline potential remedies. provide formal scheme applying human oversight agents. scheme makes easy train supervised learner imitate human’s intervention policy take human. human oversees particular agent supervised learner re-used safety-harness different agents. goal hirl enabling agent learn real-world task without single catastrophe. investigated scalability hirl atari games challenging environments current hirl applied deep agents playing three games pong space invaders road runner ﬁrst hours training human watched every frame intervened block agent taking catastrophic actions. pong space invaders class catastrophes chosen simple learn supervised learner succeeded blocking catastrophes. road runner class catastrophes diverse complex hirl reduced number catastrophes factor reduce zero. compared hirl baseline agent gets large negative reward causing catastrophic outcomes blocked causing them. baseline can’t avoid catastrophes could become reliably safe small number catastrophes. baseline agent never stopped causing catastrophes. pong show catastrophic forgetting agent periodically cause catastrophes re-learn shows hirl succeed only approach safety fails. describe challenges hirl. first supervised learner imitates human oversight must robust adversarial distribution shift second additional techniques needed reduce amount time human spend overseeing agent. show implementation hirl would feasible atari games they’d require years human time. suggest range techniques reducing human time-cost. agents learn safely real-world environments? existing literature contains variety deﬁnitions safe paper agent safe never takes catastrophic actions training. deﬁne catastrophic actions actions human overseer deems unacceptable circumstances avoid formalizing concept catastrophes human supervisor specify overseer typically distinguish sub-optimal actions catastrophic actions. tolerable drive slowly learning; hitting pedestrians catastrophic must avoided start training. reinforcement learning alone insufﬁcient achieve kind safety. fundamental problem learns trial error. without prior knowledge model-free agent avoid catastrophic action unless tried action learned negative experience. problem could potentially side-stepped training simulation agent explores dangerous actions simulation transfers knowledge real world work reliably figure hirl scheme. human overseer block/intercept unsafe actions replace safe actions overseer deliver negative reward penalty agent choosing unsafe action. would require advances transfer learning simulation. simulating humans accurately infeasible many tasks tasks involving human interaction safety-critical. imitation learning used learn safe initial policy human demonstrations initial policy much safer random initialization deviation human learned policy result unsafe actions subsequent ﬁne-tuning policy using introduce catastrophic behavior. imitation learning sufﬁcient could valuable combined hirl. model agent’s environment markov decision process environment speciﬁed tuple state space action space transition function reward function discount factor. agent learn never taking single catastrophic action? scheme hirl simple. human controls interface agent environment constantly watching agent blocking catastrophic actions happen. precisely timestep human observes current state agent’s proposed action catastrophic human sends safe action environment instead. human also replaces reward penalty period human blocks agent called human oversight phase hirl. phase store state-action binary label whether human blocked dataset used train blocker classiﬁer trained supervised learning imitate human’s blocking decisions. human oversight phase lasts blocker performs well held-out subset training data. point human retires blocker takes rest time. blocker never stops overseeing agent prevents catastrophes even agent exhibits random exploration catastrophic forgetting hirl agnostic inner workings algorithm works q-learning policy gradient algorithms like model-based moreover blocker imitates human overseer modular. trained data agent blocker safeguard completely different agent. scheme hirl presented skips important challenges avoiding catastrophes. blocker’s task standard classiﬁcation task human need spend time providing safety interventions whenever agent architecture. makes possible typical work-ﬂow researchers explore variety different algorithms task. distribution state-action pairs shifts address multiple human oversight phases human provides additional training data blocker distribution starts shift. section elaborations hirl. learn zero catastrophes blocker needs achieve near-perfect reliability recognizing catastrophic actions. require huge labeled examples might costly terms human labor. discuss challenge section requirement environment proceeds slowly enough human intervene. rules real-world tasks intrinsically high-speed. environments speed controllable parameter slowing environment might make agent’s learning slow hirl work. figure pong it’s catastrophe agent enters catastrophe zone. space invaders it’s catastrophe agent shoots defensive barriers road runner it’s catastrophe road runner touches coyote. experiments used openai implementation atari learning environment modiﬁed allow interactive blocking actions human. used open-source implementations lstm policy double rewards clipped using double blocker used convolutional neural network trained atari images pooling layers. architectures hyperparameters neural networks section appendix. code available github. goal blocker never misclassiﬁes catastrophe false-negative rate extremely low. trained training human interventions minimize standard cross-entropy loss. achieve false-negative rate selected threshold cnn’s sigmoid output blocked actions exceeded threshold. threshold initially gradually raised becomes possible agent learn task. experiments simple approach sufﬁced. well deciding actions block blocker replaces catastrophic actions safe actions implementation action replacement described section summarize application hirl involved following sequence steps atari catastrophic actions human researchers running atari agents don’t care agents millions times process mastering game. experiments stipulate certain outcomes catastrophic require agent maximize reward without causing catastrophes example agent learn road runner without losing single life level outcomes stipulate catastrophic outcome regular pong provides example avoiding catastrophes.) space invaders it’s catastrophe agent shoots defensive barriers. road runner it’s catastrophe agent dies level choose outcomes catastrophic? catastrophes avoided adjusting course catastrophe would happened. call locally avoidable catastrophes. example pong agent move upwards would entered catastrophe zone catastrophes cannot avoided happen. example losing point pong it’s often impossible agent salvage situation agent’s critical error came hundreds frames earlier. compared locally avoidable catastrophes preventing non-local catastrophes requires much understanding environment. experiments used locally avoidable catastrophes. human overseer needs recognize catastrophe imminent provide action averts don’t need skill game. human-trained reward shaping baseline shares hirl modiﬁes agent still receives reward penalty taking catastrophic action blocked. reward shaping baseline cannot achieve zero catastrophes must catastrophic actions learn negative reward however negative rewards large agent would rate catastrophes quickly falls zero. pong road runner authors took role human overseer. possible strategy space invaders shoot slit barriers attack behind slit. experiments appear strategy blocking hirl harm performance. driving catastrophes locally avoidable others not. expect hirl negative reward much larger maximum total discounted reward episode. it’s never rational cause catastrophe means achieving greater reward catastrophe. space invaders used reward clipping rewards either makes impossible negative reward catastrophic actions larger total discounted return. space invaders baseline slightly different pong road runner. objective avoid catastrophes achieving good performance. must achieved feasible amount human oversight. figure shows objective trivially satisﬁed agent human oversight thousand catastrophes game. pong incentive regular game avoid catastrophe zone. space invaders road runner incentive avoid catastrophes agents become good enough learn this. hirl mixed success overall. pong space invaders agent zero catastrophes still able achieve impressive performance game. road runner achieve zero catastrophes able reduce rate deaths frame figure shows reward shaping agent total number catastrophes compared oversight setting games catastrophe rate appear converging zero. section shows persistence catastrophes pong caused catastrophic forgetting. frequently blocking agent hirl essentially changes game’s transition function. it’s conceivable added complexity makes game harder deep learn. however don’t negative effects learning hirl compared reward shaping baseline. indeed hirl appears improve faster achieves much better reward performance overall. hirl successful pong agent mastered pong incurring catastrophes. would blocker work well different agents? reward shaping agent fail keep trying catastrophic actions? blocker trained examples human overseeing agent. figure shows performance blocker agent. virtue hirl blocker modular trained data agent applied another. would blocker equally reliable another agent? applied blocker variety agents always blocked catastrophes without preventing agent mastering pong. agents were argued section regular agents catastrophe-safe. avoid catastrophic actions they’ve already tried them; can’t learn task zero catastrophes. figure demonstrated second current deep agents unsafe never stop taking catastrophic actions. reward-shaping agent initially trained human overseer blocks catastrophes. this agent receives negative rewards catastrophes blocked. agent learns mostly avoid catastrophes catastrophe rate seems converge non-zero level. reward shaping agent keep taking actions received negative reward? investigate examining frequently hirl agent attempts catastrophic actions. hirl agent blocked actually taking catastrophic actions. measuring often attempts catastrophic actions learn many catastrophes would caused blocking turned pong hirl agent attempts catastrophes rate frame episode. ac’s stochastic policy catastrophic forgetting? tested turning learning rate zero making agent deterministic table shows four possibilities. ﬁrst regular hirl shown pong results figure catastrophe rate goes learning rate zero. strongly suggests catastrophic forgetting problem. also examined probability taking catastrophic action regular agent’s policy network agent mastered pong probability continues vary road runner side-scrolling game based warner bros cartoon. player controls road runner constantly pursued coyote road runner gets points eating birdseed knocking coyote. stipulate losing life level catastrophe. main lose life touching coyote easily avoided level running away. deep agents known discover score exploit road runner agent learns intentionally kill earns greater reward. dying precise time causes agent repeat part level earns points level local optimum policy space human gamer would never stuck ideally blocker would prevent deaths level hence eliminate score exploit. however random exploration agent upon ways dying fool blocker hence learn version score exploit. words agent performing random search adversarial examples blocker cnn. ﬁrst attempt prevent catastrophes road runner instructive failure. early stages training rate deaths/catastrophes low. however much later training death rate rises reaches almost level baseline oversight agent inspecting videos hirl agent found although usual score exploit blocked million frames agent found alternative score exploit. agent moved along screen right corner waited coyote kill there. position screen presumably fooled blocker cnn. figure reward/catastropherate hirl agent failed blocker. blue line indicates agent learned score exploit. before point catastropherate spikes times indicating additional failures blocker; spikes anti-correlated reward indicate score exploit. results successful blocker fig. blocker failed examined frames used training data blocker looked mistakes labels. spent minutes correcting mistakes re-trained blocker. reduced average death rate factor rate deaths frame oversight baseline rate experiments human oversight phase short number examples catastrophes used train blocker small. pong space invaders training sufﬁced train blocker blocked catastrophes. road runner training insufﬁcient. three games catastrophes occur start game. contrasts games certain catastrophes occur higher levels. human overseer oversee agent reached level road runner would increase amount human labor orders magnitude. assess feasibility agents learning zero catastrophes it’s crucial estimate amount human labor required. present simple formula computing human time-cost extrapolations. want estimate amount wall-clock time human spends overseeing agent. time takes generate training sufﬁcient train blocker. training contains agent’s observations whether catastrophic. nall size training set. formula formula thuman average time takes human process observation. since humans intrinsically slow we’re stuck bound thuman seconds. main reduce reduce nall. blocker extremely false-negative rate needs substantial number positive negative examples training bounding much nall reduced. however many environments catastrophes rare training consists mostly safe observations. increasing proportion attempted catastrophes therefore reduce nall without harming blocker’s performance. denote ratio observations catastrophe observations re-write formula terms training blocker requires ncat observations catastrophes. many observed catastrophes agent encounters greater number safe observations have number ncat small catastrophe simple blocker didn’t need much data. ratio also small agent frequently tries catastrophic actions. agent learns avoid catastrophes increases around suppose experiment used agent pre-trained similar environment avoid catastrophes pre-trained agent start total time human labeling would days huge amount human labor learn simple concept ratio would also much higher catastrophe zone hard reach. consider atari game montezuma’s revenge suppose treat catastrophe agent ever walks ledge dies. current deep algorithms might take million frames reach distinct rooms game contain ledges overseeing agent million frames would take human least year. suggests implementation hirl paper would scale atari games alone environments variety visual complexity currently guarantee safety systems training human watch system’s actions ready intervene else automated overseer reliable preventing catastrophes. investigated whether human oversight could allow deep agents learn without single catastrophic event. hirl succeeded preventing simplest catastrophes partial success blocking complex catastrophes. moreover extrapolations suggest hirl implementation would scale complex environments; human time-cost would infeasible. make human time-cost hirl feasible complex environments techniques required. conclude outlining promising techniques make blockers data-efﬁcient classiﬁer would learn imitate human smaller training make agents data-efﬁcient deep tends require millions observations successful learning. data-efﬁcient human would need wait long agent observe full range catastrophes seek catastrophes even agent slow master whole environment could quick catastrophes. means higher ratio catastrophes safe events lower human time-cost note agents data-efﬁcient sometimes increase human time-costs. quickly learn avoid catastrophes catastrophes become rare blocker’s training suggests role agents initially explore systematically aggressively encounter many catastrophes early selectively query human environments agent spends long time states away dangerous regions. human oversight necessary times; principle human could take break agent gets close dangerous region. similarly blocker might reliably block catastrophes region state space novel region hasn’t visited yet. human could take break agent already-visited region come back agent gets close novel region. montezuma’s revenge example human could come back agent enter room. techniques active learning anomaly detection used detect unfamiliar states related approaches pursued recent work safe exploration example suppose agent already trained environment similar pong. might still want train blocker it’s uncertain whether agent generalize perfectly environment pong. explaining action catastrophic could augment binary catastrophe/safe labels additional information explanations exactly caused catastrophe. introduce additional labeling cost could make easier learn robust imitator small training set. model-based safe learning model-based agents could potentially learn actions catastrophic without ever trying them. could achieve learning good world model exploration safe regions state space. environments human need ready take control times. algorithm suspects action leads novel state blocks action. action sent human evaluates whether action safe. work supported future life institute grant future humanity institute oxford. thank vlad firoiu early contributions leike david abel helpful comments. special thanks david krueger detailed comments draft. references david abel john salvatier andreas stuhlmüller owain evans. agent-agnostic humanin-the-loop reinforcement learning. corr abs/. http//arxiv.org/ abs/.. intelligence community assessment. background assessing russian activities intentions recent elections analytic process cyber incident attribution. https//web-beta.archive.org/web//https/www.dni.gov/ files/documents/ica__.pdf. accessed april--. marc bellemare sriram srinivasan georg ostrovski schaul david saxton remi munos. unifying count-based exploration intrinsic motivation. advances neural information processing systems pages charles blundell benigno uria alexander pritzel yazhe avraham ruderman joel leibo jack daan wierstra demis hassabis. model-free episodic control. arxiv preprint arxiv. paul christiano zain shah igor mordatch jonas schneider trevor blackwell joshua tobin pieter abbeel wojciech zaremba. transfer simulation real world learning deep inverse dynamics model. arxiv preprint arxiv. xiaoxiao satinder singh richard lewis honglak lee. deep learning reward design improve monte carlo tree search atari games. arxiv preprint arxiv. zachary lipton abhishek kumar jianfeng lihong deng. combating deep reinforcement learning’s sisyphean curse reinforcement learning. arxiv preprint arxiv. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature http//dx.doi.org/./ nature. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. international conference machine learning pages yanan alkis gotovos joel burdick andreas krause. safe exploration optimization gaussian processes. francis bach david blei editors proceedings international conference machine learning volume proceedings machine learning research pages lille france pmlr. http//proceedings. mlr.press/v/sui.html. hado hasselt arthur guez matteo hessel volodymyr mnih david silver. learning values across many orders magnitude. advances neural information processing systems agent network architecture based openai’s universe starter agent input format grayscale convolutional layers ﬁlters applied stride last convolutional layer lstm hidden units lstm output linear layers produce value function estimate policy logits activation learning rate adam optimizer entropy bonus discount factor steps policy gradient updates based openai’s baseline implementation using double input format grayscale convolutional layer ﬁlters stride convolutional layer ﬁlters stride convolutional layer ﬁlters hidden layer units output layer relu activation adam optimizer steps exploration schedule exploration rate step linearly decreased gradient norm clipping target network update frequency learning starts frame history length replay buffer size discount factor batch size frameskip episode ended life convolutional layers ﬁlters size stride -unit hidden layers dropout probability discarding examples reweighted give positive negative examples equal weight labels manually cleaned collection would makes supervised learning problem training blocker complex binary classiﬁcation task. experiments avoid dealing complex learning problem seems unlikely change conclusions. instead following techniques action pruning action blocked sent environment. agent choose action ensure agent always least action available action lowest logit score never blocked. space invaders agent starts left side screen. human blocks shooting left barrier responds staying left left barrier means many episodes never goes middle right barriers. training includes shooting barriers human would label long time. ﬁxed problem including episodes agent initially placed center right screen. alternated episodes three different initializations starting center starting right). human oversight phase complete reverted normal initialization every episode", "year": 2017}