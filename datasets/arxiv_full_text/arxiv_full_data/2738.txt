{"title": "Cross-modal Recurrent Models for Weight Objective Prediction from  Multimodal Time-series Data", "tag": ["stat.ML", "cs.AI", "cs.LG", "q-bio.QM"], "abstract": "We analyse multimodal time-series data corresponding to weight, sleep and steps measurements. We focus on predicting whether a user will successfully achieve his/her weight objective. For this, we design several deep long short-term memory (LSTM) architectures, including a novel cross-modal LSTM (X-LSTM), and demonstrate their superiority over baseline approaches. The X-LSTM improves parameter efficiency by processing each modality separately and allowing for information flow between them by way of recurrent cross-connections. We present a general hyperparameter optimisation technique for X-LSTMs, which allows us to significantly improve on the LSTM and a prior state-of-the-art cross-modal approach, using a comparable number of parameters. Finally, we visualise the model's predictions, revealing implications about latent variables in this task.", "text": "petar veliˇckovi´c laurynas karazija nicholas lane sourav bhattacharya edgar liberis pietro angela chieh otmane bellahsen matthieu vegreville analyse multimodal time-series data corresponding weight sleep steps measurements. focus predicting whether user successfully achieve his/her weight objective. this design several deep long short-term memory architectures including novel cross-modal lstm demonstrate superiority baseline approaches. x-lstm improves parameter efﬁciency processing modality separately allowing information recurrent cross-connections. present general hyperparameter optimisation technique x-lstms allows signiﬁcantly improve lstm prior state-of-the-art cross-modal approach using comparable number parameters. finally visualise model’s predictions revealing implications latent variables task. recently consumer-grade health devices wearables smart home appliances became widespread presents data modelling opportunities. here investigate task—predicting users’ future body weight relation weight goal given historical weight sleep steps measurements. study enabled ﬁrst-of-its-kind dataset ﬁtness measurements users. data captured different sources smartwatches wristhip-mounted wearables smartphone applications smart bathroom scales. work show deep long short-term memory models able produce accurate predictions setting signiﬁcantly outperforming baseline approaches even though factors observed latently. also discover interesting patterns input sequences push network’s conﬁdence success failure extremes. hypothesise patterns affect latent variables link hypotheses existing research sleep. importantly improve parameter efﬁciency lstm models multimodal input proposing cross-modal lstms x-lstms extract features modality separately still allowing information different modalities cross-connections. ﬁndings supported general data-driven methodology exploits unimodal predictive power vastly simplify ﬁnding appropriate hyperparameters x-lstms also compare model previous state-of-the-art cross-modal sequential data technique outlining limitations successfully outperforming task. performed investigation anonymised data obtained bathroom scales wearables nokia digital health withings range gathered using withings smartphone application. data pre-processed remove outliers users sporadic data observations. consider weight objective achieved exists weight measurement future reaches exceeds failed user stops recording weights sets conservative objective. following best practices data normalised mean zero standard deviation per-feature. derived dataset spans sequences associated weight objectives. sequences comprised user-related features height gender category weight objective whether achieved; along sequential features—for duration light deep sleep time fall asleep time spent awake; number times awoken night; time required wake bed-in/bed-out times; steps weights day. consider sequences span least contiguous days. dataset contains successful unsuccessful examples. compared deep recurrent models several common baseline approaches time-series classiﬁcation outlined considered support vector machines kernel random forests gaussian hidden markov models deep neural networks hyperparameters optimised using thorough sweep. here correspond weights biases lstm layer respectively corresponds element-wise vector multiplication. tanh hyperbolic tangent hard sigmoid function. remainder description compress eqn-s lstmx) primary architecture -layer lstm model processing sequential data. features computed ﬁnal lstm layer concatenated height gender category weight objective providing following feature representation input features featurewise concatenation length initial sequence. result processed -layer fully-connected network logistic sigmoid activation end. task also propose novel cross-modal lstm architecture exploits multimodality input data explicitly using number parameters traditional lstm. partition input sequence three parts pass separate three-layer lstm stream. also allow information streams second layer cross-connections features single sequence stream passed concatenated features another sequence stream equation form outputs three streams used h{xyz} lstm. finally ﬁnal lstm frames across three streams concatenated passed fully-connected classiﬁer illustration entire construction process individual building blocks shown fig. similar techniques already successfully applied handling sparsity within convolutional neural networks audiovisual data integration evaluate three crossconnecting strategies given eqn-s cross-connections intra-layer lstms without cross-connections latter corresponds prior work multimodal deep learning allows computing largest number features within parameter budget three variants—no parameters spent cross-connections. finally consider recent state-of-the-art approach processing multimodal sequential data imposes cross-modality weight sharing refer method sh-lstm. hinders expressivity—in order share weights matrices size requiring modality streams compute number features depth level. keeping parameter count comparable baseline lstm evaluate three strategies weight sharing sharing across modalities sharing across weight sleep without steps data. informed fact weight sleep data have found signiﬁcantly inﬂuential steps data. figure diagram -layer x-lstm model cross-connection second layer. left single lstm block middle lstm layer right -layer cross-modal lstm model streams. second layer hidden sequences passed separate lstm layer feature-wise concatenated main stream sequence facilitate sharing. practice anticipate x-lstms derived baseline lstm order redistribute parameters efﬁciently. however x-lstms might introduce overwhelming amount hyperparameters tune. make process less taxing focus meaning feature counts—their comparative values supposed track relative signiﬁcance modality. first attempt solve task basic lstm architecture using modalities. scores obtained three modalities redistribute intra-layer feature counts x-lstm according ratio sst. enforce larger discrepancies raise obtained scores power controls tendency network favour predictive modality redistributing features. ﬁxed choice solve system equations order derive feature counts intra-layer lstm layers x-lstm. thus effort amounts ﬁnding hyperparameter—k. performed stratiﬁed -fold crossvalidation baseline classiﬁers proposed lstm models. curves evaluation metric also report accuracy precision recall score threshold maximises score. construct competitive x-lstms computed aucs individual unimodal lstms validation dataset. results similar reliably generate non-uniform x-lstms searched parameter x-lstm performed best cross-connections compare directly lstm sh-lstms. conﬁrm advantages methodology statistically signiﬁcant performed paired t-testing metrics individual cross-validation folds choosing signiﬁcance threshold sh-lstm performed best variant even unable outperform baseline lstm—highlighting essential ability accurately specify relative importances modalities. results summarised table table comparative evaluation results baseline models lstms -fold crossvalidation. reported x-lstm sh-lstm reported p-values x-lstm baseline roc-auc metric. hard interpet parameters network directly instead focus generating artiﬁcial sequences maximise network’s conﬁdence success failure iteratively produce input maximises network’s conﬁdence starting argmaxi λ||i|| network’s output l-regularisation parameter found works best. generated sequences spanning days shown fig. expected observe user likely weight objective downwards trend weight upwards trend steps vice-versa failing sequence. interestingly model also uncovered higher conﬁdence success important user fall asleep quicker going bed. likely encoding important latent variables directly access dataset—for example person takes time fall asleep likely snack evening known detrimental weight loss", "year": 2017}