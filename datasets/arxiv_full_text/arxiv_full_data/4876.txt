{"title": "Towards Deep Symbolic Reinforcement Learning", "tag": ["cs.AI", "cs.LG"], "abstract": "Deep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-of-concept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system -- though just a prototype -- learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game.", "text": "deep reinforcement learning brings power deep neural networks bear generic task trial-and-error learning effectiveness convincingly demonstrated tasks atari video games game however contemporary systems inherit number shortcomings current generation deep learning techniques. example require large datasets work effectively entailing slow learn even datasets available. moreover lack ability reason abstract level makes difﬁcult implement high-level cognitive functions transfer learning analogical reasoning hypothesis-based reasoning. finally operation largely opaque humans rendering unsuitable domains veriﬁability important. paper propose end-to-end reinforcement learning architecture comprising neural back symbolic front potential overcome shortcomings. proof-ofconcept present preliminary implementation architecture apply several variants simple video game. show resulting system though prototype learns effectively acquiring symbolic rules easily comprehensible humans dramatically outperforms conventional fully neural system stochastic variant game. deep reinforcement learning wherein deep neural network used function approximator within reinforcement learning system recently shown effective number domains including atari video games robotics game thought step towards instantiating formal characterisation universal artiﬁcial intelligence presented hutter theoretical framework artiﬁcial general intelligence founded reinforcement learning. however contemporary systems suffer number shortcomings. first inherit deep learning need large training sets entails learn slowly. second brittle sense trained network performs well task often performs poorly task even task similar originally trained third strictly reactive meaning high-level processes planning causal reasoning analogical reasoning fully exploit statistical regularities present training data. fourth opaque. typically difﬁcult extract humanly-comprehensible chain reasons action choice system figure proposed reinforcement learning architecture. neural back learns sensor data symbolic representation used symbolic front learn effective policy. take different approach. propose novel reinforcement learning architecture addresses issues principled combining neural network learning aspects classical symbolic gaining advantages methodologies without respective disadvantages. central classical language-like propositional representations encode knowledge. thanks compositional structure representations amenable endless extension recombination essential feature acquisition deployment high-level abstract concepts general intelligence moreover knowledge expressed propositional form exploited multiple high-level reasoning processes general-purpose application across multiple tasks domains. features these derived beneﬁts human language motivated several decades research symbolic approach general intelligence classical symbolic disappointing. major obstacle symbol grounding problem symbolic elements representation classical constants functions predicates typically hand-crafted rather grounded data real world. philosophically speaking means semantics parasitic meanings heads designers rather deriving direct connection world. pragmatically hand-crafted representations cannot capture rich statistics realworld perceptual data cannot support ongoing adaptation unknown environment obvious barrier full autonomy. contrast none problems afﬂict machine learning. deep neural networks particular proven remarkably effective supervised learning large datasets using backpropagation deep learning therefore already viable solution symbol grounding problem supervised case unsupervised case essential full solution rapid progress made hybrid neuralsymbolic reinforcement learning architecture propose relies deep learning solution symbol grounding problem. level deep symbolic reinforcement learning architecture propose simple comprises deep neural network back whose transform perceptual data symbolic representation symbolic front whose task action selection. function computed half architecture shaped machine learning system whole learns end-to-end minimal assumptions made nature environment. neural back must learn compositionally-structured compressed representation paper present instantiation architecture proof-of-concept illustrate effectiveness several variants simple video game. demonstrator system many limitations makes numerous simplifying assumptions inherent larger proposal illustrates four fundamental principles architectural manifesto. conceptual abstraction. determining situation similar analogous encountered previously operation fundamental general intelligence reinforcement learning particular. conventional system achieved generalising capabilities neural network approximates function however low-level approach establishing similarity relationships requires gradual buildstatistical picture state space. upshot novice human player rapidly spot high-level similarity between paddle ball pong paddle ball breakout conventional system blind this. contrast present architecture maps high-dimensional input lower-dimensional conceptual state space within possible establish similarity states using symbolic methods operate higher level abstraction. facilitates data efﬁcient learning transfer learning well providing foundation high-level cognitive processes planning innovative problem solving communication agents compositional structure. enable sort conceptual abstraction representational medium required compositional structure. comprise elements combined recombined open-ended way. classically theoretical foundation representational medium ﬁrst-order logic underlying language comprises predicates quantiﬁers constant symbols function symbols boolean operators binary nature classical logic makes less well suited dealing uncertainty inherent real data bayesian approach. handle uncertainty propose probabilistic ﬁrst-order logic semantic underpinnings low-dimensional conceptual state space representation neural front must system’s high-dimensional input common sense priors. although target general intelligence meaning ability achieve goals perform tasks wide variety domains unrealistic expect end-to-end reinforcement learning system succeed prior assumptions domain. example systems take visual input spatial priors likelihood similar patterns appear different locations visual ﬁeld implicit convolutional structure network everyday physical world structured according many common sense priors consisting mostly empty space contains variety objects tend persist time various attributes shape colour texture objects frequently move typically continuous trajectories. objects participate number stereotypical events starting move coming halt appearing disappearing coming contact objects. minimal assumptions expectations built system grafting suitable ontology onto underlying representational language greatly reducing learning workload facilitating various forms common sense reasoning. causal reasoning. current generation architectures eschews model-based reinforcement learning ensuring resulting systems purely reactive. contrast architecture propose attempts discover causal structure domain encode symbolic causal rules expressed terms common sense ontology described above. causal rules enable conceptual abstraction. already mentioned general intelligence ability ongoing situation similar analogous previously encountered situation situations. deep neural network approximates function reinforcement learning thought carrying analogical inference kind superﬁcial statistical level. carry analogical inference abstract level thereby facilitate transfer expertise domain another narrative structure ongoing situation needs mapped causal structure previously encountered situations. well maximising beneﬁt past experience enables high-level causal reasoning processes deployed action selection planning lookahead off-line exploration implemented proof-of-concept system embodies principles albeit restricted form. back system learns construct symbolic representations sequences game states pixel data encoded conceptually abstract form deﬁned terms objects types locations interactions. representational elements joined arbitrary combinations yielding compositional structure. ontology representational medium reﬂects common sense priors tendency objects persist time default assumption objects look alike behave similar ways. finally symbolic front system learns effective policies because rudimentary sense nevertheless inapplicable conventional systems understands causal relations actions interactions among objects acquisition reward. however present prototype neural back fact relatively shallow symbolic front carries little high-level reasoning. consequently barely scratches surface believe possible architecture proposing regarded ﬁrst step towards general purpose system fully exploits combined power deep learning symbolic reasoning. benchmark prototype system implemented several variants simple game agent learn either avoid collect objects depending shape. agent reaches object using four possible move actions object disappears agent obtains either positive negative reward. encountering circle results negative reward collecting cross yields positive reward. applied system four variants game wherein type objects involved initial positions follows variant environment objects return negative rewards positioned grid across screen. layout every game. encountering object returns score beginning game player located middle board. variant layout version types objects. before circles give points introduce crosses return points. variant version game contains objects return negative reward. order increase difﬁculty learning process however position objects determined random changes every game. variant version combines randomness environment different object types version goal ﬁrst stage generate unsupervised manner symbols used represent objects scene. convolutional neural network this since networks well-suited feature extraction especially images. speciﬁcally train convolutional autoencoder randomly generated images varying numbers game objects scattered across screen. given simplicity images consist objects three different geometric shapes uniform background don’t need train deep network detect different features current game benchmark. network consists convolutional layer followed pooling layer plus corresponding decoding layers. activations across features middle layer used directly detection objects scene. object detection characterisation. next step symbol generation stage uses salient regions convoluted image shown salient areas image result higher activations throughout layers convolutional network. given games geometrically simple property enough enable extraction individual objects given frame. this ﬁrst select pixel feature highest activation. threshold activation values forming list sufﬁciently salient. ideally member list representative pixel single object. objects identiﬁed assigned symbolic type according geometric properties computed autoencoder. done comparing activation spectra salient pixels across features. current implementation comparison carried using squared distances involves setting threshold maximal distance objects type. future work richer environments anticipate using sophisticated clustering algorithms state unsupervised learning advances. extracted low-level symbols single snapshot game need able track across frames order observe learn dynamics. this need take account ﬁrst common sense prior object persistence across time. concept persistence deploy based three measures combined single value. value identiﬁed captures likelihood object next frame function type transitions. given types objects consecutive frames determine probability object changed type learning transition matrix previously observed frames. given transition matrix already normalised probability. figure unsupervised extraction low-level symbols information provided convolutional autoencoder. spectrum activations across features obtained selecting pixels highest activations across features. second step symbol assigned existing types comparing spectra. match found type created. order able describe transitions including ones correspond objects appearing disappearing introduce object type type corresponds ’non-existent’. object type appears frame thus carried transition object disappears later would transition addition carried tracking step every object assigned corresponding type previous time steps. neighbourhood. neighbourhood object typically similar frame next. allows discriminate objects spatially close approaching different directions. consider difference number neighbours objects deﬁne neighbour object within distance dmax another object future improvements could include considering types neighbours rather number. result tracking process acquire additional attribute object scene unique identiﬁer labels across time. identiﬁer added symbolic representation ongoing game state. figure overview symbolic representations extracted three main steps. images left show consecutive frames example. ﬁrst column corresponds information obtained frames low-level symbol extraction convolutional autoencoder. tracking unique label assigned persistent object associated type position recorded. last column right lists interactions objects used reinforcement learning stage. information expressed terms relative distance rather absolute position corresponds difference frames. symbolic interactions dynamics. information game expressed terms static frame-by-frame representations ﬁnal reinforcement learning stage algorithm require information dynamics objects spatial interactions. obtained static representations constructed using principles. first consider difference frames rather working single frames thus moving temporally extended representation. second represent positions objects relative objects rather using absolute coordinates. moreover record relative positions objects within certain maximum distance other. approach justiﬁed common sense prior local relations multiple objects relevant global properties single objects. course assumption always valid even game. result present system arrive locally optimal policy inferior global optimum. future implementations handle question circumscribing relevance nuanced general representational sparseness results adopting locality assumption compensates loss global optimality allowing much faster training. concise spatio-temporal representation game situation captures objects scene along locations types also doing. particular represents frame-to-frame interactions objects changes type relative position result. input reinforcement learning third ﬁnal stage pipeline. spatio-temporal representation constructed stages system pipeline used learn effective policy game play. point advantage using locality heuristic becomes clear. representation included possible relations objects would result large state space long training times states visited infrequently. however interactions objects independent other real world particular game. instead representing relations global state representation comprises localised representations. results signiﬁcantly reduced state space enables fast generalisation across object types. order implement independence train separate function interaction object types. main idea learn several functions different interactions query relevant current situation. given simplicity game reduced state space results sparse symbolic representation approximate optimal policy using tabular q-learning. update rule interaction objects types therefore learning rate temporal discount factor state represents interaction object types time step case interactions changes relative distance objects question. state space thus describes different possible relations objects types given limited interactions certain radius proximity state space bounded learning guaranteed converge global optimum. finally order choose next action values obtained currently relevant functions time step pick return highest reward overall. note that present benchmark games moving object agent acts directly ensures every currently relevant function pertains possible agent action. general implementation necessary automatically identify objects world agent controls directly restrict function accordingly. agents trained epochs time steps maximum epochs. trained agents separately tested time steps games every tenth epoch. resulting average score plotted ﬁgure four games. four cases score increases within ﬁrst hundred epochs remains approximately constant remaining time. plotting average score common visualising successful learning type experiment measure produce incomplete characterisation learning process environments positive negative objects. reasons this. first given scores returned objects cancel achieve given ﬁnal score. example collecting positive nine negative objects result ﬁnal score collecting positive object. ﬁrst case number positive objects agent collects second scenario therefore although score percentage measure reveals agent latter case learned react different objects correctly agent former case collects items without regard type. second agent limited radius view therefore stuck location surrounded negative objects. case agent forced spend test time avoiding negative objects won’t obtain high score. reasons introduce second measure percentage positive objects collected total amount objects encountered. rather measuring well agent performs results game variants feature objects different types shown ﬁgure expected objects agent initially collects positive. training continues percentage increases approximately cases. finally tested transfer learning capabilities algorithm training agent games grid variant testing games random variant. setup similar experiments random game sense grid setup seen among possible random initialisations difference lies fact agent exposed type environment training whereas random experiments agent experiences numerous random variations. shown ﬁgure learning curve comparable random case albeit showing slightly inferior performance. finally compare approach dqn. environments suited best types objects initial score independent speed agent beginning given cancel out. figure shows performance time. it’s important note system’s convolutional network pre-trained images corresponds epochs worth frames. don’t include plots pre-training applicable games carried once. thanks geometrical simplicity grid scenario agent quickly learns move diagonally collect positive objects avoid negative ones. result relative number objects positive reward game variant reaches hundred epochs training agent achieve hand objects positioned random agent able learn effective policy within epochs also case ﬁnal experiment agent trained grid variant tested random variant agent rapidly attains percentage approximately ﬂuctuates around value unable better chance epochs. although haven’t experiment long enough conﬁrm this hypothesise that might eventually learn play random game effectively trained game never achieve competence random game trained grid setup. proposed hybrid neural-symbolic end-to-end reinforcement learning architecture claimed addresses number drawbacks inherent current generation systems. support claim presented simple prototype system conforming architecture demonstrated several variants basic video game. although present system cannot learn globally optimal policy games learns effectively them. moreover even though system preliminary proof-of-concept many limitations dramatically outperforms difﬁcult game variant initial placement objects random. game dqn’s performance didn’t exceed chance level epochs training system acquired effective policy conjecture struggles game form statistical picture possible object placements would require much larger number games. contrast thanks conceptual abstraction made possible symbolic front system quickly gets game forms general rules covers every possible initial conﬁguration. demonstration merely hints potential symbolic front promote data efﬁcient learning potential exploit fully future work. proof-of-concept system also illustrates aspect architecture’s inherent capacity transfer learning. training unsupervised neural back system able form symbolic representation given frame within micro-world game. effect acquired ontology micro-world capability applied game within micro-world irrespective speciﬁc rules. present case re-training back required system applied variants game. however form transfer learning operates superﬁcial level. abstract level architecture supports powerful forms transfer learning. system understand situation analogous previously encountered potently hypothesise situation contains elements several previously encountered situations combined novel way. present system capability barely exploited. future work explore full potential analogical reasoning made possible symbolic representation. finally fundamental mode operation front system carry inference symbolic representations humanly-comprehensible chain justiﬁcations decisions makes. benchmark example every action choice analysed terms functions involved decision. given functions describe types objects involved interaction well relations track back reasons certain decision. agent chooses move upwards towards positive object example function describing interaction positive object agent assigns positive reward action going step towards systems greater transparency. several avenues work explore move complex games richer environments building complete implementation deep symbolic reinforcement learning architecture. neural back concerned intend make extensive state deep learning. particular sophisticated deep network capable unsupervised learning disentangled representations required handle realistic images occur benchmark games similarly symbolic back draw heavily achievements classical present. three potential elaborations architectural blueprint particularly promising. first incorporation inductive logic programming would enable powerful form generalisation applied function recall claim made introduction general intelligence rests ability determine newly encountered situation similar situations encountered past. much deep neural network used function approximator achieve conventional inductive logic programming used symbolic context. second amplify system’s ability determine similarity current past situations formal techniques analogical reasoning deployed structure mapping engine relatives technique especially appropriate challenge much generalise large numbers past scenarios rather ongoing situation shares features single recorded past episode. third elaborate architecture build planning component exploits knowledge causal structure domain acquired learning process. domains sparse reward it’s often possible agent discover sequence actions leading reward state off-line search rather on-line exploration. contemporary logic-based planning methods capable efﬁciently ﬁnding large plans complex domains would rash exploit potential techniques. finally means ruling possibility symbolic components proposed architecture implemented using neural networks. success deep learning inspired good deal novel research much involves innovative neural networks novel architectures. research could well produce neurally-based implementations symbolic reasoning functions advocating here advantages approach. mean time architecture combines deep neural networks directly implemented symbolic reasoning seems like promising research direction. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature sergey levine chelsea finn trevor darrell pieter abbeel. end-to-end training deep david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature john-alexander assael niklas wahlström thomas schön marc peter deisenroth. data-efﬁcient learning feedback policies image pixels using deep dynamical models. arxiv. andrei rusu neil rabinowitz guillaume desjardins hubert soyer james kirkpatrick koray kavukcuoglu razvan pascanu raia hadsell. progressive neural networks. arxiv. xiaoxiao satinder singh honglak richard lewis xiaoshi wang. deep learning real-time atari game play using ofﬂine monte-carlo tree search planning. advances neural information processing systems pages chen duan rein houthooft john schulman ilya sutskever pieter abbeel. infogan interpretable representation learning information maximizing generative adversarial nets. arxiv. irina higgins loic matthey xavier glorot arka benigno uria charles blundell shakir mohamed alexander lerchner. early visual concept learning unsupervised deep learning. arxiv. patrick hayes. second naive physics manifesto. ernest davis. representations commonsense knowledge. morgan kaufmann erik mueller. commonsense reasoning event calculus based approach. morgan kauf guiying junlong chunhui jiang tang. relief impression image detection unsupervised extracting objects directly feature arrangements deep cnn. arxiv. sašo džeroski raedt hendrik blockeel. relational reinforcement learning. inductive logic programming international conference ilp- madison wisconsin pages", "year": 2016}