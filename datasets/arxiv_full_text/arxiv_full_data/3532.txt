{"title": "CUR Decompositions, Similarity Matrices, and Subspace Clustering", "tag": ["cs.LG", "cs.CV", "math.NA", "stat.ML"], "abstract": "A general framework for solving the subspace clustering problem using the CUR decomposition is presented. The CUR decomposition provides a natural way to construct similarity matrices for data that come from a union of unknown subspaces $\\mathscr{U}=\\underset{i=1}{\\overset{M}\\bigcup}S_i$. The similarity matrices thus constructed give the exact clustering in the noise-free case. A simple adaptation of the technique also allows clustering of noisy data. Two known methods for subspace clustering can be derived from the CUR technique. Experiments on synthetic and real data are presented to test the method.", "text": "subspace clustering problem stated follows suppose collected data vectors comes union linear subspaces denoted however know priori subspaces even many are. consequently desires determine number subspaces represented data dimension subspace basis subspace ﬁnally cluster data data ordered particular clustering data means determine data belong subspace. images face illuminated differently image represents vector lying approximately low-dimensional linear subspace higher dimensional space experimentally shown images given subject approximately dimensions {di}m decomposition used construct similarity matrix particular element-wise binary absolute value version qdmax similarity matrix i.e. columns come subspace columns come different subspaces. paper extends previous framework ﬁnding similarity matrices clustering data comes union independent subspaces. showed factorization columns come form basis column space used produce similarity matrix work shows need limit factorization bases extend frames. also utility analysis rank matrix so-called skinny form urσrv∗ comprises ﬁrst left singular vectors comprises ﬁrst right singular vectors diag kr×r. note case rank skinny simply low-rank approximation zamarashkin begin decomposition corollary study error a−cu−r case rank larger whereby decomposition cu−r approximate. additionally allow ﬂexibility choice since computing inverse moreover mahoney drineas give another algorithm based selecting columns provides nearly optimal error bounds curf also note decomposition bounds form a−curf a−urσrv favored analyzing real data dimensional matrices maintain theorem km×n rank matrix whose columns drawn factorized km×k ks×n ks×k theorem either binary absolute value version then qdmax similarity matrix corollary km×n rank matrix whose columns drawn factorized ww†w. either binary absolute value version w†w. qdmax similarity matrix moreover skinny singular value decomposition urσrv∗ pioneering work factorization methods motion tracking costeira kanade introduced shape interaction matrix sim. given data matrix whose skinny urσrv∗ deﬁned vrv∗ following work found wide utility theory practice. observation often provides similarity matrix data coming independent subspaces. noted shown examples data matrices found vrv∗ similarity matrix however noted almost surely similarity matrix. qdmax similarity matrix also provides theoretical justiﬁcation idea robust shape interaction matrix authors took powers better similarity matrix data. note general solving minimization problem np–hard since equivalent minimizing vector singular values number nonzero entries vector. additionally solution equation generally unique typically rank function replaced norm produce convex optimization problem. based upon intuition compressed sensing literature natural consider replacing deﬁnition nuclear norm denoted particular following considered lemma suppose submatrix whose columns come independent subspaces columns generic suppose kr×m rank selection matrix rank rank. subspaces independent. next facts needed come basic graph theory. suppose ﬁnite undirected weighted graph vertices edges geodesic distance vertices length shortest path connecting diameter graph maximum pairwise geodesic distances vertices. lemma generic vectors represent data subspace dimension theorem case finally graph whose vertices whose edges connected diameter moreover adjacency matrix fully connected graph. selection matrix particular note maps structure write columns belong subspace note also columns generic subspace account lemma subspaces independent lemma additionally since consists certain columns rank rank rank assumption columns ˜si. above. ﬁrst demonstrate block rank columns generic. since uiyi rank rank rank since rank product minimum ranks. hand since rank rank whence rank columns generic columns suppose constants follows block diagonal structure consider block adjacency matrix graph prescribed lemma thus conclusion there block gives connected graph diameter qdmax give rise graph distinct fully connected components graph arising corresponds data drawn thus qdmax indeed similarity matrix columns basis row-selection matrix gives i.e. pbz. columns linearly independent whence z†b† z††. moreover linear independence columns also implies identity matrices appropriate size sort algorithm extracts features moving objects tracks positions features time. video obtains data matrix size number frames video number features tracked. vector corresponds trajectory ﬁxed feature i.e. form position feature frame even though trajectories vectors high dimensional ambient space known trajectories feature belonging rigid body subspace dimension similar fashion. particularly synthetic experiments data comes union independent dimensional subspaces corresponds feature tracked seconds video stream. cases similar ones hopkins dataset investigated increasing levels ﬁrst case data comes union subspaces whereas second case number subspaces cases level noise gradually increased initial noiseless state maximum noise level. entries noise i.i.d. random variables variance increases case noise level data matrices randomly generated. data matrix formed similarity matrix generated using sim. similarity matrices spectral clustering results evaluated based clustering error percentages. found using skinny expected rank data matrix found using columns expected rank number rows therefore matrix theorem r†r. rows chosen uniformly random ensured expected rank proceeding. given random nature decomposition simple improvement performance achieved calculating similarity matrix using single decomposition calculating different decompositions data matrix. decomposition similarity matrix found ﬁnal similarity matrix given taking median entrywise family mentioned clustering performance using decompositions tested using hopkins dataset. here calculated case synthetic data section; i.e. still aggregate result different similarity matrices explained above. turns real takes columns exactly expected rank number rows exhibits best performance. decomposition form wr†r performs better average form fact choosing columns performs better matrix noisy makes sense representation form representation terms frame provides greater robustness noise. additionally noticed experimentally choosing exactly rows decomposition wr†r exhibits best performance. clear case. aldroubi zaringhalam nonlinear least squares acta applicandae mathematicae aldroubi cabrelli molter optimal non-linear models sparsity sampling journal fourier analysis", "year": 2017}