{"title": "Explaining Recurrent Neural Network Predictions in Sentiment Analysis", "tag": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "abstract": "Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.", "text": "recently technique called layer-wise relevance propagation shown deliver insightful explanations form input space relevances understanding feed-forward neural network classiﬁcation decisions. present work extend usage recurrent neural networks. propose speciﬁc propagation rule applicable multiplicative connections arise recurrent network architectures lstms grus. apply technique word-based bi-directional lstm model ﬁve-class sentiment prediction task evaluate resulting relevances qualitatively quantitatively obtaining better results gradient-based related method used previous work. semantic composition plays important role sentiment analysis phrases sentences. includes detecting scope impact negation reversing sentiment’s polarity well quantifying inﬂuence modiﬁers degree adverbs intensiﬁers rescaling sentiment’s intensity recently trend emerged tackling challenges deep learning models convolutional recurrent neural networks observed e.g. semeval- task sentiment analysis twitter another type analysis seeks determine input features important reaching ﬁnal top-layer prediction. recent work direction focused bringing measures feature importance state-of-the-art models deep convolutional neural networks vision general deep neural networks text techniques based model’s local gradient information methods seek redistribute function’s value input variables typically reverse propagation neural network graph refer reader overview methods understanding interpreting deep neural network predictions. bach proposed speciﬁc propagation rules neural networks rules shown produce better explanations e.g. gradient-based techniques also successfully transferred neural networks text data paper extend rule handles multiplicative interactions lstm model particularly suitable model modeling long-range interactions texts occurring sentiment analysis. given trained neural network models scalar-valued prediction function class classiﬁcation problem given input vector interested computing input dimension relevance score quantifying relevance w.r.t considered target class interest others words want analyze features important classiﬁer’s decision toward class neural network classiﬁer derivatives obtained standard gradient backpropagation made available neural network toolboxes. refer deﬁnition relevance sensitivity analysis similar technique previously used computer vision layer-wise relevance conservation principle given input redistributes quantity starting output layer network backpropagating quantity input layer. relevance propagation procedure described layer-by-layer type layer occurring deep convolutional neural network consists deﬁning rules attributing relevance lower-layer neurons given relevances upper-layer neurons. hereby intermediate layer neuron gets attributed relevance score input layer neurons. case recurrent neural network architectures lstms grus types neural connections involved manyto-one weighted linear connections two-toone multiplicative interactions. hence restrict deﬁnition procedure types connections. note that simpliﬁcation refrain explicitly introducing notation non-linear activation functions; activation present neuron always take account activated lower-layer neuron’s value subsequent formulas. order compute input space relevances start setting relevance output layer neuron corresponding target class interest value simply ignore output layer neurons then compute layer-by-layer relevance score intermediate lower-layer neuron accordingly subsequent formulas depending type connection involved. weighted connections. upper-layer neuron whose value forward pass comzi lower-layer neurons connection weights biases. given relevances upper-layer neurons goal compute lower-layer relevances neurons start procedure.) relevance redistribution onto lower-layer neurons performed steps. first computing relevance messages ri←j going upper-layer neurons lowerlayer neurons then summing incoming messages lower-layer neuron obtain relevance messages ri←j computed fraction relevance accordingly following rule even local propagation rule seems ignore respective values redistribute relevance indeed taken account computing value relevances next upper-layer neurons connected weighted connections. total number lower-layer neurons connected small positive number serves stabilizer sign deﬁned sign relevance subsequently computed ri←j. moreover multiplicative factor either case total relevance neurons layer conserved else implies part total relevance absorbed biases relevance propagation rule approximately conservative. default last variant refer denote explicitly lrpcons results experiments. multiplicative interactions. another type connection two-way multiplicative interaction lower-layer neurons. upperlayer neuron whose value forward pass computed multiplication lowerlayer neuron values i.e. multiplicative interactions occur e.g. lstms grus always lower-layer neurons constitutes gate whose value ranges output sigmoid activation function call gate neuron refer remaining source neuron given conﬁguration denoting relevance upper-layer neuron propose redistribute relevance onto lower-layer neurons following intuition behind reallocation rule gate neuron decides already forward pass much information contained source neuron retained make overall classiﬁcation decision. thereby value controls much relevance attributed upper-layer neurons. thus recurrent neural network model employ hidden-layer bi-directional lstm trained ﬁve-class sentiment prediction phrases sentences stanford sentiment treebank movie reviews dataset used previous work neural network interpretability made available authors. model takes input sequence words word represented word embedding dimension hidden layer size thorough model description found appendix details training refer experiments input tokenized sentences stanford sentiment treebank test preprocessing lowercasing done ﬁve-class sentiment prediction full sentences model achieves accuracy binary classiﬁcation test accuracy using trained bi-lstm compare relevance decomposition methods sensitivity analysis layer-wise relevance propagation former similar firstderivative saliency used besides work authors aggregate relevance single input variables obtain word-level relevance value moreover employ absolute value partial derivatives compute relevance single input variables. section present qualitative well quantitative results obtained performing test sentences. outcome relevance decomposition chosen target class ﬁrst word embedding input sentence vector relevance values. order obtain scalar word-level relevance remind simply relevances contained vector. also note that definition relevances positive relevances signed. order illustrate differences provide fig. heatmaps exemplary test sentences. heatmaps obtained mapping positive word-level relevance values negative relevances blue. exemplary sentences belong either class very negative class very positive target class relevance decomposition always true class. left figures indicate true sentence class well bi-lstm’s predicted class whereby upper examples correctly classiﬁed bottom examples falsely classiﬁed. inspection heatmaps notice clearly distinguish words speaking target class. indeed sometimes attributes comparatively high relevance words expressing positive appreciation like thrilling master must-see target class very negative; word difﬁcult expressing negative judgment target class very positive. contrary discern reliably words addressing negative sentiment waste horrible disaster repetitive difﬁcult words indicating positive opinion like funny suspenseful romantic thrilling worthy entertaining furthermore explains well sentences mistakenly classiﬁed very positive positive accentuating negative relevance terms speaking target class i.e. class very negative must-see list remember future whereas understanding provided heatmaps. holds misclassiﬁed very positive sentence word fails gets attributed deep negatively signed relevance similar limitation gradient-based relevance visualization explaining predictions recurrent models also observed previous work moreover interesting property observe sentiment negation modulated sentiment subsequent words sentence. hence e.g. heatmaps target class very negative negators like followed words indicating negative sentiment like waste horrible marked negatively signed relevance subsequent words express positive impression like worth surprises funny good positively signed relevance heatmap visualizations provide insights sentiment single words composed bi-lstm model indicate sentiment attributed words static depends context sentence. nevertheless would like point explanations delivered relevance decomposition highly depend quality underlying classiﬁer good neural network itself hence carefully tuned model might deliver even better explanations. another qualitative analysis conduct datasetwide consists building list resp. least relevant words class. ﬁrst perform test sentences speciﬁc target class example take class very positive. secondly order words appearing test sentences decreasing resp. increasing order relevance value retrieve table least relevant words obtain. figure heatmaps exemplary test sentences using target class true sentence class. relevances positive mapped color intensity normalized maximum relevance sentence. true sentence class classiﬁer’s predicted class indicated left. figure heatmaps exemplary test sentences using target class true sentence class. positive relevance mapped negative blue color intensity normalized maximum absolute relevance sentence. true sentence class classiﬁer’s predicted class indicated left. word lists observe highest relevances mainly point words strong semantic meaning necessarily expressing positive sentiment e.g. broken-down lackadaisical mournfully lowest relevances correspond stop words. contrary extremal relevances reliable highest relevances indicate words expressing positive sentiment lowest relevances attributed words deﬁning negative sentiment hence extremal relevances related meaningful target class interest i.e. class very positive. order quantitatively validate word-level relevances obtained perform word deleting experiments. experiments consider test sentences length greater equal words delete sentence words accordingly resp. relevance value re-predict bi-lstm sentiment sentence missing words track impact deletions classiﬁer’s decision. idea behind experiment relevance decomposition method pertinently reveals words important classiﬁer’s decision impact decision deleting words accordingly relevance value. prior deletions ﬁrst compute resp. word-level relevances original sentences using true sentence sentiment target class relevance decomposition. then conduct types deletions. initially correctly classiﬁed sentences delete words decreasing order relevance value initially falsely classiﬁed sentences delete words increasing order relevance. additionally perform random word deletion uninformative variant comparison. results terms tracking classiﬁcation accuracy number word deletions sentence reported fig. results show that considered cases deleting words decreasing increasing order relevance pertinent effect suggesting relevance decomposition method appropriate detecting words speaking classiﬁer’s decision. variant relevance conservation lrpcons performs almost good standard latter yields slightly superior results thus preferred. finally deleting words increasing order relevance value starting initially falsely classiﬁed sentences observe performs even worse random deletion. indicates lowest relevances point essentially words inﬂuence classiﬁer’s decision rather signalizing words inhibiting it’s decision speaking true class indeed able identify. similar conclusions drawn comparing convolutional network document classiﬁcation idea words sentence length attributed relevance compute word relevance statistic performing test sentences length greater equal words then divide sentence length equal intervals word relevances interval absolute value word-level relevance values finally distribution normalize results one. compute statistic considering particularity relevance distribution notice relevances left encoder tend smooth right encoder surprising result might expect unidirectional model constituents behave similarly mechanism model make distinction text read original reversed order. work introduced simple effective strategy extending procedure recurrent architectures lstms proposing rule backpropagate relevance multiplicative interactions. applied extended version bi-directional lstm model sentiment prediction sentences demonstrating resulting word relevances trustworthy reveal words supporting classiﬁer’s decision speciﬁc class perform better obtained gradient-based decomposition. technique helps understanding verifying correct behavior recurrent classiﬁers detect important patterns text datasets. compared non-gradient based explanation methods rely e.g. random sampling iterative representation occlusion technique deterministic computed pass network. moreover method self-contained require train external classiﬁer deliver explanations obtained directly original classiﬁer. future work would include applying proposed technique recurrent architectures character-level models grus well extractive summarization. besides method restricted domain might also useful applications relying recurrent architectures. thank rico raber many insightful discussions. work partly supported bmbf also institute information communications technology promotion grant funded korea government figure impact word deleting initially correctly falsely classiﬁed test sentences using either relevance decomposition method relevance target class true sentence class words deleted decreasing increasing order relevance. random deletion averaged runs steep decline incline indicate informative word relevance. either total word relevance obtained bi-lstm model considering part relevance comes unidirectional model constituents i.e. relevance contributed lstm takes input sentence words original order contributed lstm takes input sentence words reversed order resulting distributions different relevance target classes reported fig. interestingly relevance distributions symmetric w.r.t. sentence middle major part relevance attributed second half sentences except target class neutral relevance attributed last computational time steps left right encoder resulting almost symmetric distribution total relevance class. maybe explained fact that least longer movie reviews strong judgments movie’s quality tend appear sentences beginning sentences serves introduction review’s topic describing e.g. movie’s subject genre. another figure word relevance distribution sentence length relevance target class obtained performing test sentences length greater equal words absolute value word-level relevances used compute statistics. ﬁrst corresponds total relevance second resp. third contain relevance bi-lstm’s left right encoder. references leila arras franziska horn gr´egoire montavon klaus-robert m¨uller wojciech samek. explaining predictions non-linear classiﬁers nlp. proceedings workshop representation learning nlp. association computational linguistics pages leila arras franziska horn gr´egoire montavon klaus-robert m¨uller wojciech samek. what relevant text document? inarxiv terpretable machine learning approach. sebastian bach alexander binder gr´egoire montavon frederick klauschen klaus-robert m¨uller wojciech samek. pixel-wise explanations non-linear classiﬁer decisions layer-wise relevance propagation. plos kyunghyun bart merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoderdecoder statistical machine translation. proceedings conference empirical methods natural language processing association computational linguistics pages muriel gevrey ioannis dimopoulos sovan lek. review comparison methods study contribution variables artiﬁcial neural network models. ecological modelling landecker michael thomure lu´ıs bettencourt melanie mitchell garrett kenyon steven brumby. interpreting individual ieee classiﬁcations hierarchical networks. symposium computational intelligence data mining pages sebastian lapuschkin alexander binder gr´egoire montavon klaus-robert m¨uller wojciech samek. layer-wise relevance propagation toolbox artiﬁcial neural networks. journal machine learning research saif mohammad. challenges sentiment erik cambria dipankar sivaji analysis. bandyopadhyay antonio feraco editors practical guide sentiment analysis. springer international publishing pages gr´egoire montavon sebastian lapuschkin alexander binder wojciech samek klaus-robert m¨uller. explaining nonlinear classiﬁcation decisions deep taylor decomposition. pattern recognition james murdoch arthur szlam. automatic rule extraction long short term memory networks. international conference learning representations conference track preslav nakov alan ritter sara rosenthal fabrizio sebastiani veselin stoyanov. semeval task sentiment analysis twitter. proceedings international workshop semantic evaluation association computational linguistics pages marco tulio ribeiro sameer singh carlos guestrin. trust you? explaining predictions classiﬁer. proceedings sigkdd international conference knowledge discovery data mining. pages wojciech samek alexander binder gr´egoire montavon sebastian lapuschkin klaus-robert m¨uller. evaluating visualization ieee transdeep neural network learned. actions neural networks learning systems pp–. karen simonyan andrea vedaldi andrew zisserman. deep inside convolutional networks visualising image classiﬁcation models international conference saliency maps. learning representations workshop track richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment proceedings conference treebank. empirical methods natural language processing association computational linguistics pages input lstm gets sequence vectors representing word embeddings input sentence’s words. matrices vectors connection weights biases initial states zero. last hidden state eventually attached fully-connected linear layer yielding prediction score vector entry class used sentiment prediction. bi-directional lstm bi-directional lstm present work concatenation separate lstm models described above taking different sequence word embeddings input.", "year": 2017}