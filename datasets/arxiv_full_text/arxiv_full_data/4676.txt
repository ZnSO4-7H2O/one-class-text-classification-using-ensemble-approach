{"title": "Hybrid Batch Bayesian Optimization", "tag": ["cs.AI", "cs.LG"], "abstract": "Bayesian Optimization aims at optimizing an unknown non-convex/concave function that is costly to evaluate. We are interested in application scenarios where concurrent function evaluations are possible. Under such a setting, BO could choose to either sequentially evaluate the function, one input at a time and wait for the output of the function before making the next selection, or evaluate the function at a batch of multiple inputs at once. These two different settings are commonly referred to as the sequential and batch settings of Bayesian Optimization. In general, the sequential setting leads to better optimization performance as each function evaluation is selected with more information, whereas the batch setting has an advantage in terms of the total experimental time (the number of iterations). In this work, our goal is to combine the strength of both settings. Specifically, we systematically analyze Bayesian optimization using Gaussian process as the posterior estimator and provide a hybrid algorithm that, based on the current state, dynamically switches between a sequential policy and a batch policy with variable batch sizes. We provide theoretical justification for our algorithm and present experimental results on eight benchmark BO problems. The results show that our method achieves substantial speedup (up to %78) compared to a pure sequential policy, without suffering any significant performance loss.", "text": "bayesian optimization aims optimizing unknown function costly evaluate. focus applications concurrent function evaluations possible. cases could choose either sequentially evaluate function evaluate function batch multiple inputs sequential mode generally leads better optimization performance function evaluation selected information whereas batch mode time efﬁcient goal combine strength settings. systematically analyze using gaussian process posterior estimator provide hybrid algorithm dynamically switches sequential batch variable batch sizes. theoretically justify algorithm present experimental results eight benchmark problems. results show method achieves substantial speedup compared sequential without suffering signiﬁcant performance loss. d-dimensional compact input space non-concave underlying function multiple local optima. function might performance black device characterized input example motivating application optimize power output nano-enhanced microbial fuel cells mfcs micro-organisms generate electricity. shown efﬁciency generated electricity power signiﬁcantly depends surface properties anode problem involves optimizing surface properties anodes order maximize output power. goal develop efﬁcient algorithm application since running experiment expensive time consuming. focusing task function maximization consists main steps estimating values unknown function probabilistic model selecting best next experiment according probabilistic model selection criterion. results experiment added update probabilistic model cycle repeated meet stopping criterion. proposed selection criteria sequential experiment selected iteration sequential policies usually perform well practice since optimize experiment selection iteration using maximum available information experiment. however time efﬁcient many applications running experiment takes long time capability multiple experiments parallel. motivates batch algorithms experiment selected iteration. recently azimi introduced batch approach selects batch experiments iteration approximates behavior given sequential heuristic. ginsbourger introduced constant liar heuristic algorithm select batch experiments based expected improvement policy. speciﬁcally selecting experiment output selected point constant value. experiment added prior procedure repeated experiments selected. although batch algorithms speedup experiment selection factor results show batch selection general performs worse sequential policy especially total number experiments small. observation motivates introduce hybrid approach dynamically alternates sequential batch selection achieve improved time efﬁciency sequential without degrading optimization performance. paper focus class batch policies based simulating sequential policy provide systematic approach analyze batch policies. analytically connect mismatch bo’s probabilistic model underlying true function performance batch policy. provide full characterization simulated-based batch policies batch size purpose illustration consider batch policy selects experiments. ﬁrst experiment matches sequential policy. choice second experiment however depend simulated outcome ﬁrst experiment. show distance second experiment picked simulation-based batch policy picked sequential policy upper-bounded quantity proportional square root estimation error analysis naturally gives rise hybrid batch/sequential algorithm. algorithm works follows step given sequential policy best next single experiment estimate possible outcome bo’s probabilistic model then update prior point choose next best single experiment analytically show process continued certain stopping criterion met. stopping criterion measures much simulated experiment going bias probabilistic model bias small continue examples batch; large stop. proposed algorithm appealing property behaves like sequential policy early stages number observed experiments small naturally transits batch mode later stages experiments available. stopping criterion tends stringent early stages bias prior potentially large forcing algorithm sequentially. beauty algorithm evolves sequential algorithm batch algorithm optimal manner characterized theoretical results. experimental results show proposed algorithm achieve speedup sequential policy without degrading performance even small number experiments. also show that increasing number experiments speedup rate increased signiﬁcantly consistent theoretical results presented paper. paper organized follows. introduce gaussian process used model section proposed dynamic batch algorithm described section section presents experimental results paper concluded section algorithm main ingredients probabilistic model unknown function selection criterion choosing next best experiment based model. select probabilistic model selection criterion. study properties section postpone analysis next section. build posterior outcome values given observation where inputs outcomes underlying unknown function. input point models unknown output normal random variable kk−k where practical point view theorem enables update variance computing difference previous value. scheme much faster recalculating variance directly. computational bottleneck update matrix inversion complexity considering fact computed before complexity direct variance computation estimationµz|ox accurate hence make decision point without knowing i.e. remark want minimize estimation error ofµz|ox expectation sety µx|o. best choice fory since increases expected value around optimal point model. previous theorem provides performance bound based estimation error however practical point view bound cannot computed since know exact values practical measure would like focus expected value estimation error opposed error itself. next corollary provides upper-bound expected error simply taking expectation result theorem since computable without knowledge observation motivates stopping criterion algorithm determine current estimation bias large continue selecting examples batch. nutshell want query batch samples criterion sure estimation accurate hence need wait label selected examples making next selection. sequential approach query experiment time using selection criterion mainly selection criterion requires output previous query next best one. suppose capability running experiments parallel limited total number possible experiments iteration question whether query sample speed experimental procedure without losing performance comparing sequential approach. proposed algorithm selects batch samples iteration based policy batch size dynamically determined step. particular algorithm continue select experiments condition satisﬁed select point explain algorithm suppose beginning ﬁrst round algorithm. thus observed randomly chosen sample points form batch query start empty samples gradually next best sample time. ﬁrst sample pick identical simply maximize objective i.e. ﬁrst sample sequential picks value estimation changes function unobserved remark theorem shows sample estimation error proportional square root estimation means sample estimation sensitive output estimation error functions taking line analysis extended next samples. results show algorithm based estimation µy|o) condition satisﬁed batch query move algorithm summarizes proposed method hybrid batch bayesian optimization. early stages algorithm behaves like sequential policy since criterion building batch hard satisfy mainly large samples collecting enough samples term starts decreasing gets closer closer zero select larger larger batch sizes. thus algorithm gradually transits batch policy maintaining close match performance pure sequential policy. benchmarks. consider well-known synthetic benchmark functions cosines rosenbrock hartman hartman shekel michalewicz analytic expression functions shown table real benchmarks fuel cell hydrogen. fuel cell goal maximize generated electricity microbial fuel cells changing nano structure properties anodes. regression model data build function evaluation. hydrogen benchmark data collected part study hydrogen production particular bacteria goal maximize amount hydrogen production optimizing nitrogen levels growth medium. fuel cell hydrogen data contour plots shown figure hyper sphere whose expected values affected algorithm benchmark independent times average simple regret reported result. simple regret difference maximum value denoted ymax ﬁnishing experimental procedure. algorithm starts initial random points -dimensional benchmarks initial random points higher dimensional benchmarks. total number experiments -dimensional higher dimensional benchmarks. maximum batch size iteration parameter -dimensional higher dimensional benchmarks. note that experimental setup designed match typical scenarios encountered real applications typically start small number random experiments restricted total budget. hope conﬁrm comparing different possible estimations particular consider different estimations including means expect observe best possible output experiment selected demonstrate effectiveness algorithm consider state-of-the-art batch algorithms literature simulation matching constant liar approach output selected samples batch mean order select next experiment methods batch size also reported performance sequential pure random selection policies. speedup proposed approach calculated percentage samples whole experiment selected batch mode. speciﬁcally ﬁnish samples steps speedup calculated clearly maximum speedup setting achieved select experiments time steps. example speedup proposed baseline batch approaches matching table shows result. interestingly considered estimators achieved similar performance terms regrets. difference different estimators level speedup achieve. particular inspection speedup rates reveal setting large value example ymax ymax generally leads less speedup choices. explained noting large value lead higher chance violating condition required making next experiment selection algorithm stated equation particular large next point selected likely close since large. consequently easy violate condition thus stop selection process early contrast away since mean variance points close small. considering terms jointly expect achieve higher speedup setting ymin comparing setting large value mean points close high. lead large further quantity µx|o likely ymin although µx|o large expect small next point selected likely exactly observe experiments. finally setting µx|o µx|o stopping decreases variances variance input space signiﬁcantly comparing case observation points. therefore stopping criteria algorithm less likely early stages experimental procedure observation points. µ-constant batch approach. part experiments motivated theoretical analysis goal shed lights batch method recently proposed ginsbourger selects batch experiments jointly maximize objective. show ﬁnding batch experiments practically intractable. therefore introduced heuristic approach called constant liar select batch experiments. selecting ﬁrst experiment constant liar sets output selected experiment constant value experiment added observations next experiment selected. procedure repeated experiments selected. introduced several possible ways setting including theoretical analysis particular corollary indicates setting toµx|o condition justiﬁes choice setting µx|o constant liar approach. call approach µ-constant batch. ymin. empirically demonstrated setting provided good result particular test functions. however theoretical justiﬁcation guidance toward best continued experiment selection easily comparing settings i.e. γzθx thus batch experiments requested iterations without degrading performance. theoretical result also algorithm proposed benchmarks different batch sizes figures show performance µ-constant along competitive approaches sequential constant liar constant liar ymax; constant liar ymin; matching recently proposed approach azimi experiments experimental setup used table results show µ-constant batch approach performs competitively compared matching approach best existing batch bayesian optimization approach literature. addition practical matching approach high dimensional applications since computational complexity signiﬁcantly less matching algorithm. note performance µ-constant also shown table worth emphasizing µ-constant achieves highly competitive batch performance consistently worse sequential proposed hybrid batch algorithm. result suggests stopping criterion used algorithm fact effective toward identifying condition must stop increasing batch size avoid signiﬁcant performance degradation compared sequential bayesian optimization framework investigated problem batch query selection goal maintaining performance sequential policy using fewer iterations. although results general problems sake clarity focused task maximizing unknown non-convex/concave function. firstly introduce systematic analyze performance limits simulation-based batch methods proving universal bounds bias caused simulation error; analyzing selection second experiment estimate outcome ﬁrst experiment. cases provide theoretical bounds error relating simulation error prediction error next best experiment. secondly based analysis above proposed algorithm behaves optimally expectation. algorithm step decides whether pick another query current batch dynamically determines appropriate batch size step. early iterations algorithm behaves similar sequential policy gradually moves toward batch policy variable batch sizes. empirical evaluation synthetic real data shows substantial speedup compared corresponding sequential policy little nothing loss optimization performance. theoretical results also shed interesting light constant-liar approach recently proposed batch selection method based objective.", "year": 2012}