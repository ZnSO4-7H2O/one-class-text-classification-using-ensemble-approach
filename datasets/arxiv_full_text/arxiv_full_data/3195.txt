{"title": "The Informed Sampler: A Discriminative Approach to Bayesian Inference in  Generative Computer Vision Models", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Computer vision is hard because of a large variability in lighting, shape, and texture; in addition the image signal is non-additive due to occlusion. Generative models promised to account for this variability by accurately modelling the image formation process as a function of latent variables with prior beliefs. Bayesian posterior inference could then, in principle, explain the observation. While intuitively appealing, generative models for computer vision have largely failed to deliver on that promise due to the difficulty of posterior inference. As a result the community has favoured efficient discriminative approaches. We still believe in the usefulness of generative models in computer vision, but argue that we need to leverage existing discriminative or even heuristic computer vision methods. We implement this idea in a principled way with an \"informed sampler\" and in careful experiments demonstrate it on challenging generative models which contain renderer programs as their components. We concentrate on the problem of inverting an existing graphics rendering engine, an approach that can be understood as \"Inverse Graphics\". The informed sampler, using simple discriminative proposals based on existing computer vision technology, achieves significant improvements of inference.", "text": "varun jampani planck institute intelligent systems spemannstraße t¨ubingen germany sebastian nowozin microsoft research cambridge station road cambridge united kingdom matthew loper planck institute intelligent systems spemannstraße t¨ubingen germany peter gehler planck institute intelligent systems spemannstraße t¨ubingen germany computer vision hard large variability lighting shape texture; addition image signal non-additive occlusion. generative models promised account variability accurately modelling image formation process function latent variables prior beliefs. bayesian posterior inference could then principle explain observation. intuitively appealing generative models computer vision largely failed deliver promise difﬁculty posterior inference. result community favoured efﬁcient discriminative approaches. still believe usefulness generative models computer vision argue need leverage existing discriminative even heuristic computer vision methods. implement idea principled informed sampler careful experiments demonstrate challenging generative models contain renderer programs components. concentrate problem inverting existing graphics rendering engine approach understood inverse graphics. informed sampler using simple discriminative proposals based existing computer vision technology achieves signiﬁcant improvements inference. conceptually elegant view computer vision consider generative model physical image formation process. observed image becomes function unobserved variables interest nuisance variables building generative model think scene description produces image using deterministic rendering engine generally results distribution images given image observation prior scenes perform bayesian inference obtain updated beliefs view advocated since late ’ies years later would argue generative approach largely failed deliver promise. successes idea limited settings. successful examples either generative model restricted high-level latent variables e.g. restricted image transformations ﬁxed reference frame e.g. modelled limited aspect object shape masks worst case generative model merely used generate training data discriminative model intuitive appeal beauty simplicity fair track record generative models computer vision poor. result ﬁeld computer vision dominated efﬁcient data-hungry discriminative models empirical risk minimization learning energy minimization heuristic objective functions sampling informed sampler leverages computer vision features algorithms make informed proposals state latent variables proposals accepted rejected based generative model. informed sampler simple easy implement enables inference generative models reach current uninformed samplers. demonstrate claim challenging models incorporate rendering engines object occlusion ill-posedness multi-modality. carefully assess convergence statistics samplers investigate truthfulness probabilistic estimates. experiments existing computer vision technology informed sampler uses standard histogramof-gradients features opencv library produce informed proposals. likewise models existing computer vision model blendscape model parametric model human bodies section discuss related work explain informed sampler approach section section presents baseline methods experimental setup. present experimental analysis informed sampler three diverse problems estimating camera extrinsics occlusion reasoning estimating body shape conclude discussion future work section vast literature approaches solve computer vision applications means generative models. mention works also accurate graphics process generative model. includes applications indoor scene understanding human pose estimation hand pose estimation many more. works however interested inferring solutions rather full posterior distribution. method similar spirit data driven markov chain monte carlo methods bottomapproach help convergence mcmc sampling. ddmcmc methods used image segmentation object recognition human pose estimation idea making markov samplers data dependent general works mentioned above lead highly problem speciﬁc implementations mostly using approximate likelihood functions. specialization figure example inverse graphics problem. graphics engine renders body mesh depth image using artiﬁcial camera. inverse graphics refer process estimating posterior probability possible bodies given depth image. generative models succeed? problems need addressed design accurate generative model inference therein. modern computer graphic systems leverage dedicated hardware setups produce stunning level realism high frame rates. believe systems design generative models open exciting modelling opportunities. observation motivates research question paper design general inference technique efﬁcient posterior inference accurate computer graphics systems. understood instance inverse graphics illustrated figure applications. problem generative world view difﬁculty posterior inference test-time. difﬁculty stems number reasons ﬁrst parameter typically high-dimensional posterior. second given image formation process realizes complex dynamic dependency structures example objects occlude self-occlude other. intrinsic ambiguities result multi-modal posterior distributions. third renderers real-time simulation forward process expensive prevents exhaustive enumeration. believe usefulness generative models computer vision tasks argue order overcome substantial inference challenges devise techniques general allow reuse several different models novel scenarios. hand want maintain correctness terms probabilistic estimates produce. improve inference efﬁciency leverage existing computer vision features discriminative models order inference generative model. paper propose informed sampler markov chain monte carlo method discriminative proposal distributions. understood instance data-driven mcmc method design method general enough applied across different problems problem domain proposed samplers easily transferable problems. focus work provide simple efﬁcient general inference technique problems accurate forward process exists. method general believe easy adapt variety models tasks. idea invert graphics order understand scenes also roots computer graphics community term inverse rendering. goal inverse rendering however derive direct mathematical model forward light transport process analytically invert work falls category. authors formulate light reﬂection problem convolution understand inverse light transport problem deconvolution. elegant pose problem require speciﬁcation inverse process requirement generative modelling approaches circumvent. approach also viewed instance probabilistic programming approach. recent work authors combine graphics modules probabilistic programming language formulate approximate bayesian computation. inference implemented using metropolis-hastings sampling. approach appealing generality elegance however show graphics problems plain sampling approach sufﬁcient achieve reliable inference proposed informed sampler achieve robust convergence challenging models. another piece work similar proposed inference method knowledge forward process learned stochastic inverses applied mcmc sampling bayesian network. present work devise mcmc sampler show works multi-modal problem well inverting existing piece image rendering code. summary method understood similar context above-mentioned papers including general inference posterior distribution challenging complex model closed-form simpliﬁcations made. especially true case consider corresponds graphics engine rendering images. despite apparent complexity observe following many computer vision applications exist well performing discriminative approaches that given image predict parameters distributions thereof. correspond posterior distribution interested intuitively availability discriminative inference methods make task inferring easier. furtherphysically accurate generative model used ofﬂine stage prior inference generate many samples would like afford computationally. again intuitively allow prepare summarize useful information distribution order accelerate test-time inference. concretely case discriminative method provide global density valid mcmc inference method. remainder section ﬁrst review metropolis-hastings markov chain monte carlo discuss proposed informed samplers. goal sampler realize independent identically distributed samples given probability distribution. mcmc sampling metropolis particular instance generates sequence random variables simulating markov chain. sampling target distribution consists repeating following steps ordinary local proposal distribution example multivariate normal distribution centered around current sample global proposal distribution independent current state. inject knowledge conditioning global proposal distribution image observation. learn informed proposal discriminatively ofﬂine training stage using nonparametric density estimator described below. mixture parameter controls contribution proposal recover proposal would identical resulting metropolis sampler would valid metropolized independence sampler call baseline method informed independent intermediate values combine local global moves valid markov chain. call method informed metropolis hastings step construction include discriminative information sample ideally would hope propose global moves improve mixing even allow mixing multiple modes whereas local proposal responsible exploring density locally. principle possible consider case perfect global proposal case would independent samples every proposal accepted. practice approximation approximation good enough mixture local global proposals high acceptance rate explore density rapidly. principle conditional density estimation technique learning proposal samples. typically high-dimensional density estimation difﬁcult even conditional case; however case true generating process available provide example pairs therefore simple scalable non-parametric density estimation method based clustering feature representation observed image cluster estimate unconditional density using kernel density estimation chose simple setup since easily reused many different scenarios experiments solve diverse problems using method. method yields valid transition kernel detailed balance holds. addition estimate global transition kernel also experimented random forest approach maps observations transition kernels details given section feature representation leverage successful discriminative features heuristics developed computer vision community. different task speciﬁc feature representations used order provide invariance construct cluster relatively small kernel bandwidth order accurately represent high probability regions posterior. similar spirit using high probability regions darts darting monte carlo sampling technique sminchisescu welling summarize ofﬂine training algorithm test time method advantage given image need identify corresponding cluster using order sample efﬁciently kernel density show full procedure algorithm method yields transition kernel mixture kernel reversible symmetric metropolis-hastings kernel metropolized independence sampler. combined transition kernel hence also reversible. measure kernel dominates support posterior kernel ergodic correct stationary distribution ensures correctness inference experiments investigate efﬁciency different methods terms convergence statistics. remainder paper demonstrate proposed method three different experimental setups. experiments four parallel chains initialized different random locations sampled prior. reported numbers median statistics multiple test images except noted otherwise. metropolis hastings within gibbs metropolis hastings scheme gibbs sampler draw one-dimensional conditional distributions proposing moves markov chain updated along dimension time. blocked variant mhwg sampler update blocks dimensions time denote bmhwg. parallel tempering parallel tempering address problem sampling multi-modal distributions technique also known replica exchange mcmc sampling different parallel chains different temperatures sampling sampling step propose exchange randomly chosen chains. experiments three chains temperature levels found best working combinations experiments individually. highest temperature levels corresponds almost distribution. regeneration sampler implemented regenerative mcmc method performs adaption proposal distribution sampling. mixture kernel proposal distribution adapt global part initialized prior times regeneration already drawn samples. comparison used mixture coefﬁcient inf-mh established methods monitoring convergence mcmc method particular report different diagnostics. compare different samplers respect number iterations instead time. forward graphics process signiﬁcantly dominates runtime therefore iterations experiments correspond linearly runtime. acceptance rate ratio accepted samples total markov chain length. higher acceptance rate fewer samples need approximate posterior. acceptance rate indicates well proposal distribution approximates true distribution locally. derived comparing within-chain variances between-chain variances sample statistics. this requires independent runs multiple chains parallel. sample multi-dimensional estimate psrf parameter dimension separately take maximum psrf value ﬁnal psrf value. value close indicates chains characterize distribution. imply convergence chains collectively miss mode. however psrf value much larger certain sign lack convergence chain. psrf also indicates well sampler visits different modes multi-modal distribution. root mean square error experiments access input parameters generated image. assess whether posterior distribution covers correct value report rmse value posterior expectation generating input. since noise added observation access ground truth posterior expectation therefore measure indicator. convergence samplers would agree correct value. sampler individually selected hyper-parameters gave best psrf value iterations. case psrf differ multiple values chose highest acceptance rate. include detailed analysis baseline samplers parameter selection supplementary material. implement following simple graphics scenario create challenging multi-modal problem. render cubical room edge length point light source center room viewpoint camera somewhere inside room. camera parameters described -position orientation speciﬁed pitch roll angles. inference process consists estimating posterior camera parameters figure example renderings. posterior inference highly multi-modal problem room cubical thus symmetric. different camera parameters result image. also shown figure plot position orientation camera parameters create image. rendering image resolution using single core intel xeon .ghz machine takes average. discovers different modes quicker compared baseline samplers. sampling global proposal distribution inf-indmh initially visiting modes dominated inf-mh range. indicates mixture kernel takes advantage local global moves either exploring slower. also examples samplers miss modes deﬁnition average number discovered modes inf-mh even lower figure shows effect mixture coefﬁcient informed sampling inf-mh. since signiﬁcant difference psrf values chose high acceptance rate. likewise parameters baseline samplers chosen based psrf acceptance rate metrics. supplementary material analysis baseline samplers parameter selection. figure role mixture coefﬁcient. prsfs acceptance rates corresponding various mixture coefﬁcients inf-mh sampling ‘estimating camera extrinsics’ experiment. also tested mhwg sampler found converge even iterations psrf value around expected since single variable updates traverse multi-modal posterior distributions fast enough high correlation camera parameters. figure plot median auto-correlation samples obtained different sampling techniques separately extrinsic camera parameters. informed sampling approach appears produce samples independent compared baseline samplers. expected knowledge multi-modal structure posterior needs available sampler perform well. methods inf-indmh inf-mh information perform better baseline methods reg-mh. figure rendered room images possible camera positions headings produce image. shown orientations; left example headings rolled degrees image. posterior distribution infer reads uniform. uniform prior location parameters ranges prior angle parameters modelled wrapped uniform distribution learn informed part proposal distribution data computed histogram oriented gradients descriptor image using gradient orientations cells size yielding feature vector generated training images using uniform prior camera extrinsic parameters performed k-means using cluster centers based feature vector. cluster cell computed stored dimensional camera parameters following steps algorithm test data create images using extrinsic parameters sampled uniform random range. show results figure observe yield acceptance rate compared methods. however parallel tempering appears overcome multi-modality better improves terms convergence. holds regeneration technique observe many regenerations good convergence inf-indmh inf-mh converge quickly. experimental setup access different exact modes different ones. analyze quickly samplers visit modes whether capture them. ever different instance pairwise distances modes changes therefore chose deﬁne visiting mode following way. compute voronoi tesselation modes centers. mode visited sample falls corresponding voronoi cell closer mode. sampling uniform random would quickly modes valid sampler. also experimented balls different radii around modes found similar behaviour report here. figure shows results various samplers. inf-mh figure results ‘estimating camera extrinsics’ experiment. acceptance rates psrfs average number modes visited different sampling methods. plot median/average statistics test examples. blur image gaussian random noise example depicted figure note tiles size farther away tiles look smaller. rendering image takes average. here prior uniform distribution cube tile location parameters wrapped uniform distribution tile orientation angle. avoid label switching issues tile given ﬁxed color changed inference. chose experiment resembles dead leaves model properties commonplace computer vision. scene composed several objects independent except occlusion complicates problem. occlusion exist task readily solved using standard opencv rectangle ﬁnding algorithm output algorithm seen figure algorithm discriminative source information. problem higher dimensional previous inference becomes challenging higher dimension approach without modiﬁcation scale well increasing dimensionality. approach problem factorize joint distribution blocks learn informed proposals separately. present experiment observed baseline samplers plain informed sampling fail proposing parameters jointly. since tiles independent except occlusion approximate full joint distribution product block distributions block corresponds parameters single tile. estimate full posterior distribution learn global proposal distributions block separately block-gibbs like scheme sampler propose changes tile time alternating tiles. experimental protocol before render images apply opencv algorithm rectangles take found four parameters features clustering distributions cluster test time assign observed image corresponding cluster. chosen cluster determines global sampler tile. propose update parameters tile. refer procedure inf-bmhwg. empirically optimal inf-bmhwg sampling. example result shown figure found inf-mh samplers fail entirely problem. proposal distribution entire state high dimensions almost acceptance sample rendered image ground truth squares probable estimates samples obtained mhwg sampler inf-bmhwg sampler. posterior expectation square boundaries obtained inf-bmhwg sampling. thus reach convergence. mhwg sampler updating dimension time found best among baseline samplers acceptance rate around followed block sampler samples tile separately. opencv algorithm produces reasonable initial guess fails occlusion cases. block wise informed sampler inf-bmhwg converges quicker higher acceptance rates lower reconstruction error. median curves test examples shown figure inf-bmhwg produces lower reconstruction errors. also posterior distribution visualized fully visible tiles localized position orientation occluded tiles uncertain. figure appendix shows visual results. although model relatively simple baseline samplers perform poorly discriminative information crucial enable accurate inference. discriminative information provided readily available heuristic opencv library. experiment illustrates variation informed sampling strategy applied sampling highdimensional distributions. inference methods general high-dimensional distributions active area research intrinsically difﬁcult. occluding tiles experiment simple illustrates point namely non-block baseline samplers fail. block sampling common strategy scenarios many computer vision problems block-structure. informed sampler improves convergence speed baseline method. techniques produce better conditional marginals give faster convergence. last experiment motivated real world problem estimating body shape person single static depth image. recent availability cheap active depth sensors rgbd data become ubiquitous computer vision model updates originally proposed scape model better training blend weights. model produces mesh human body shown figure function shape pose parameters. shape parameters allow represent bodies many builds sizes includes statistical characterization parameters control directions deformation space learned corpus roughly mesh models registered scans human bodies pca. pose parameters joint angles indirectly control local orientations predeﬁned parts model. model uses pose parameters number shape parameters produce mesh vertices. ﬁrst scape components represent shape person. camera viewpoint orientation pose person held ﬁxed. thus rendering process takes generates mesh representation projects virtual depth camera create depth image person. done various resolutions chose depth values represented numbers interval average full render path takes gaussian noise standard deviation created depth image. fig. example. used simple level features feature representation. order learn global proposal distribution compute depth histogram features grid image. cell record mean variance depth values. additionally height width body silhouette features resulting feature vector normalization feature dimension divided maximum value training set. used training images sampled standard normal prior distribution clusters learn proposal distributions cluster cell. experiment also experimented different conditional density estimation approach using forest random regression trees previous experiments utilizing estimates discriminative information entered feature representation. then suppose relation observed features variables trying infer would require large number samples reliably estimate densities different clusters. regression forest adaptively partition parameter space based observed features able ignore uninformative features thus lead better conditional densities. thus understood adaptive version k-means clustering technique solely relies used metric particular features k-means clustering grow regression trees using mean square error criterion scoring split functions. forest binary trees depth grown constraint minimum training points leaf node. leaf nodes trained before. test time regression forest yields mixture kdes global proposal distribution. denote method inf-rfmh experiments. instead placing using model cluster could also explore regression approach example using discriminative linear regression model observations proposal distributions. using informative covariates regression model able overcome curse dimensionality. semi-parametric approach would allow capture explicit parametric dependencies variables combine non-parametric estimates residuals. exploring technique future work. again chose parameters samplers individually based empirical mixing rates. informed samplers chose local proposal standard deviation full analysis samplers included supplementary material. tested different approaches test images generated parameters drawn standard normal prior distribution. figure summarizes results sampling methods. make following observations. baselines methods mhwg show inferior convergence results also suffer lower acceptance rates. sampling distribution discriminative step enough acceptance rate indicates global proposals represent correct posterior distribution. however combined local proposal mixture kernel achieve higher acceptance rate faster convergence decrease rmse. regression forest approach slower convergence inf-mh. example regeneration sampler reg-mh improve simpler baseline methods. attribute rare regenerations improved specialized methods. believe simple choice depth image representation also signiﬁcantly improved example features computed identiﬁed body parts something simple histogram features taken account. computer vision literature discriminative approaches pose estimation exist prominent inﬂuential work pose recovery parts kinect xbox system future work plan similar methods deal pose variation complicated dependencies parameters observations. figure show sample body mesh reconstruction result using inf-mh sampler iterations. visualized difference mean posterior ground truth mesh terms mesh edge directions. figure inference body shape depth image. sample test result showing result mesh reconstruction ﬁrst samples obtained using inf-mh sampling method. visualize angular error estimated ground truth edge project onto mesh. figure results ‘body shape’ experiment. acceptance rates psrfs rmses different sampling methods body shape experiment. median results test examples. observe differences belly region feet person. retrieved posterior distribution allows assess model uncertainty. visualize posterior variance record standard deviation edge directions mesh edges. backprojected achieve visualization figure posterior variance higher regions higher error model predicts uncertainty correctly real-world body scanning scenario information beneﬁcial; example scanning multiple viewpoints experimental design scenario helps selecting next best pose viewpoint record. figure shows mesh reconstruction results using sampling approach. predicting body measurements many applications including clothing sizing ergonomic design. given pixel observations wish infer distribution measurements fortunately original shape training corpus includes host different per-subject measurements obtained professional anthropometrists; allows relate shape parameters measurements. among many possible forms regression regularized linear regression found best predict measurements shape parameters. linear relationship allows transform posterior distribution scape parameters posterior measurements shown figure report three randomly chosen subjects’ results three measurements. dashed lines corresponds ground truth values. estimate faithfully recovers true value also yields characterization full conditional posterior. figure body measurements quantiﬁed uncertainty. plots three body measurements three test subjects computed ﬁrst samples obtained inf-mh sampler. dotted lines indicate measurements corresponding ground truth scape parameters. experiment occluding portion observed depth image. inference learning codes parametrization features non-occlusion case retrain model account changes forward process. result inf-mh computed ﬁrst samples shown fig. reconstruction reasonable even large occlusion; error edge direction variance increase expected. work proposes method incorporate discriminative methods bayesian inference principled way. augment sampling technique discriminative information enable inference global accurate generative models. empirical results three challenging diverse computer vision experiments discussed. carefully analyse convergence behaviour several different baselines informed sampler performs well across different scenarios. sampler applicable general scenarios work leverage accurate forward process ofﬂine training setting frequently found computer vision applications. main focus generality approach inference technique applicable many different problems tailored particular problem. show even simple scenarios baseline samplers perform poorly fail completely. including global image-conditioned proposal distribution informed discriminative inference improve sampling performance. deliberately simple learning technique enable easy reuse applications. using stronger tailored discriminative models lead better performance. top-down inference combined bottom-up proposals probabilistic setting. figure inference incomplete evidence. mean mesh corresponding errors uncertainties mesh edge directions test case ﬁgure computed ﬁrst samples inf-mh sampling method occlusion mask image evidence. method initial step direction general inference techniques accurate generative computer vision models. identifying conditional dependence structure improve results e.g. recently stuhlm¨uller used structure bayesian networks identify dependencies. assumption work accurate generative model. relaxing assumption allow general scenarios generative model known approximately important future work. particular high-level computer vision problems scene object understanding accurate generative models available clear trend towards physically accurate representations world. general setting different consider paper believe ideas carried over. example could create informed proposal distributions manually annotated data readily available many computer vision data sets. anproblem domain trans-dimensional models require different sampling techniques like reversible jump mcmc methods investigating general techniques inform sampler similar ways described manuscript. believe generative models useful many computer vision scenarios interplay computer graphics computer vision prime candidate studying probabilistic inference probabilistic programming however current inference techniques need improved many fronts efﬁciency ease usability generality. method step towards direction informed sampler leverages power existing discriminative heuristic techniques enable principled bayesian treatment rich generative models. emphasis generality; aimed create method easily reused scenarios existing code bases. presented results successful example inversion involved rendering pass. future plan investigate ways combine existing computer vision techniques principled generative adapting proposal distribution existing mcmc samples straight-forward would potentially violate markov property chain approach identify times regeneration chain restarted proposal distribution adapted using samples drawn previously. several approaches identify good regeneration times general markov chain proposed build proposed splitting methods ﬁnding regeneration times. here brieﬂy describe method implemented study. arbitrary constant value maximize regeneration probability. every sampling step sample independent proposal distribution accepted compute regeneration probability using equation regeneration occurs present sample discarded replaced independent proposal distribution mixture proposal distribution informed sampling approach initialize global proposal prior distribution times regeneration existing samples. becomes adapted distribution refer details regeneration technique. work regeneration technique used success darting monte carlo sampler. figure qualitative results occluding tiles experiment shown. informed sampling approach better best baseline still challenging problem since parameters occluded tiles large region. posterior variance occluded tiles already captured references chen welling. distributed adaptive darting monte carlo regenerations. proceedings international conference artiﬁcial intelligence statistics mansinghka kulkarni perov tenenbaum. approximate bayesian image interpretation using generative probabilistic graphics programs. advances neural information processing systems pages shotton fitzgibbon cook sharp finocchio moore kipman blake. real-time human pose recognition parts single depth images. computer vision pattern recognition s.-c. zhang integrating bottom-up/topobject recognition data driven markov chain monte carlo. computer vision pattern recognition volume pages figure qualitative results occluding tiles experiment. left right given image ground truth tiles probable estimates samples obtained mhwg sampler inf-bmhwg sampler. posterior expectation tiles boundaries obtained inf-bmhwg sampling. figure qualitative results body shape experiment. shown mesh reconstruction results ﬁrst samples obtained using inf-mh informed sampling method. varun jampani planck institute intelligent systems spemannstraße t¨ubingen germany sebastian nowozin microsoft research cambridge station road cambridge united kingdom matthew loper planck institute intelligent systems spemannstraße t¨ubingen germany peter gehler planck institute intelligent systems spemannstraße t¨ubingen germany next pages supplementary material give in-depth performance analysis various samplers effect hyperparameters. choose hyperparameters lowest psrf value iterations sampler individually. differences psrf signiﬁcantly different among multiple values choose highest acceptance rate. metropolis hastings figure shows median acceptance rates psrf values corresponding various proposal standard deviations plain sampling. mixing gets better acceptance rate gets worse standard deviation increases. value selected standard deviation sampler. metropolis hastings within gibbs mentioned main paper mhwg sampler onedimensional updates converge value proposal standard deviation. problem high correlation camera parameters multi-modal nature sampler problems with. parallel tempering sampling took best performing sampler used different temperature chains improve mixing sampler. figure shows results corresponding different combination effect mixture coefﬁcient informed sampling figure shows effect mixture coefﬁcient informed sampling inf-mh. since signiﬁcant different psrf values chose high acceptance rate. metropolis hastings figure shows results sampling. results show poor convergence proposal standard deviations rapid decrease increasing standard deviation. high-dimensional nature problem. selected standard deviation blocked metropolis hastings within gibbs results bmhwg shown figure sampler update block tile variables sampling step. results show much better performance compared plain optimal proposal standard deviation sampler metropolis hastings within gibbs figure shows result mhwg sampling. sampler better bmhwg converges much quickly. standard deviation found best. figure results ‘estimating camera extrinsics’ experiment. prsfs acceptance rates corresponding various standard deviations various temperature level combinations sampling various mixture coefﬁcients inf-mh sampling. parallel tempering figure shows results sampling various temperature combinations. results show improvement plain sampling temperature levels found optimal. effect mixture coefﬁcient informed sampling figure shows effect mixture coefﬁcient blocked informed sampling infbmhwg. since signiﬁcant different psrf values chose high acceptance rate. figure results ‘occluding tiles’ experiment. prsf acceptance rates corresponding various standard deviations bmhwg mhwg; various temperature level combinations sampling and; various mixture coefﬁcients informed inf-bmhwg sampling. figure results ‘body shape estimation’ experiment. prsfs acceptance rates corresponding various standard deviations mhwg; various temperature level combinations sampling and; various mixture coefﬁcients informed inf-mh sampling. figure summary statistics three experiments. shown several baseline methods informed samplers acceptance rates psrfs rmse values results median results multiple test examples.", "year": 2014}