{"title": "Explaining Predictions of Non-Linear Classifiers in NLP", "tag": ["cs.CL", "cs.IR", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Layer-wise relevance propagation (LRP) is a recently proposed technique for explaining predictions of complex non-linear classifiers in terms of input variables. In this paper, we apply LRP for the first time to natural language processing (NLP). More precisely, we use it to explain the predictions of a convolutional neural network (CNN) trained on a topic categorization task. Our analysis highlights which words are relevant for a specific prediction of the CNN. We compare our technique to standard sensitivity analysis, both qualitatively and quantitatively, using a \"word deleting\" perturbation experiment, a PCA analysis, and various visualizations. All experiments validate the suitability of LRP for explaining the CNN predictions, which is also in line with results reported in recent image classification studies.", "text": "layer-wise relevance propagation recently proposed technique explaining predictions complex non-linear classiﬁers terms input variables. paper apply ﬁrst time natural language processing precisely explain predictions convolutional neural network trained topic categorization task. analysis highlights words relevant speciﬁc prediction cnn. compare technique standard sensitivity analysis qualitatively quantitatively using word deleting perturbation experiment analysis various visualizations. experiments validate suitability explaining predictions also line results reported recent image classiﬁcation studies. following seminal work bengio collobert deep learning models natural language processing applications received increasing attention recent years. parallel initiated computer vision domain also trend toward understanding deep learning models visualization techniques decision tree extraction work dedicated understanding neural network classiﬁers tasks gradientbased approaches. recently technique called layer-wise relevance propagation shown produce meaningful explanations context image classiﬁcations paper apply technique task neural network maps sequence wordvec vectors representing text document category evaluate whether similar beneﬁts terms explanation quality observed. present work contribute applying method domain proposing technique quantitative evaluation explanation methods classiﬁers qualitatively quantitatively comparing different explanation methods namely gradient-based approach topic categorization task using newsgroups dataset. consider problem explaining prediction associated input assigning input variable score determining relevant input variable explaining prediction. scores pooled groups input variables visualized heatmaps highlighted texts images. layer-wise relevance propagation newly introduced technique obtaining explanations. applied various machine learning classiﬁers deep convolutional neural networks. technique produces decomposition function value input variables satisﬁes conservadecomposition obtained performing backward pass network neuron relevance associated redistributed predecessors. considering neurons mapping inputs neuron activation sequence functions convenience neuron bias distributed equally input neuron monotonously increasing activation function. denoting relevance associated relevance redistributed layer deﬁning messages ri←j indicating much relevance must propagated neuron input neuron lower layer. messages deﬁned stabilizing term handles near-zero denominators intuition behind local relevance redistribution formula input assigned relevance proportionally contribution forward pass pooling ensures layer-wise conservation max-pooling layer relevance output layer redistributed pooled neuron maximum activation implementation found downloaded www.heatmapping.org. sensitivity analysis alternative procedure called sensitivity analysis produces explanations scoring input variables based affect decision output locally sensitivity input variable given squared partial derivative quantity however directly related amount evidence category detect. similar gradient-based analyses recently applied domain also used simonyan context image classiﬁcation. recent work uses different relevance deﬁnitions group input variables max-norm absolute value simple derivatives simonyan present work employ squared l-norm gradients allowing decomposition relevances input variables. following experiments newsbydate version newsgroups dataset consisting train/test documents evenly distributed among twenty ﬁne-grained categories. -max-pool denote max-pooling layer pooling regions span whole text length introduced conv relu denote convolutional layer rectiﬁed linear units activation fully-connected linear layer. building experiments term target class identify function analyze relevance decomposition. function maps neural network input neural network output variable corresponding target class. evaluating word-level relevances order evaluate different relevance models perform sequence word deletions track impact deletions classiﬁcation performance. carry deletion experiments starting either test documents initially classiﬁed correctly initially classiﬁed wrongly. estimate lrp/sa word relevances using target class true document class. subsequently delete words decreasing resp. increasing order obtained word relevances. fig. summarizes results. yields best results deletion experiments. thereby provide evidence positive relevance targeted words support classiﬁcation decision negative relevance tuned upon words inhibit decision. ﬁrst experiment classiﬁcation accuracy curve decreases signiﬁcantly faster random curve representing performance change randomly deleting words indicating able identify relevant words. however curve clearly curve indicating provides better explanations predictions. similar results reported image classiﬁcation tasks second experiment indicates classiﬁcation performance increases numerical input concatenate horizontally -dimensional pre-trained wordvec vectors order corresponding words appear pre-processed document keep input representation ﬁxed training. convolutional operation apply ﬁrst neural network layer one-dimensional along text sequence direction receptive ﬁeld convolutional layer neurons spans entire word embedding space vertical direction covers consecutive words horizontal direction. convolutional layer ﬁlter bank contains ﬁlters. experimental setup pre-processing remove document headers tokenize text nltk ﬁlter punctuation numbers ﬁnally truncate document ﬁrst tokens. train stochastic mini-batch gradient descent momentum trained classiﬁer achieves classiﬁcation accuracy input representation applying neural classiﬁer yields relevance value word-embedding dimension. single input variable relevances obtain wordlevel relevances relevances word embedding space case take squared l-norm corresponding word gradient case precisely given input document consisting sequence words word represented ddimensional word embedding compute relevance word input document summation best published newsgroups accuracy however notice simpliﬁcation ﬁxed-length document representation main focus explaining classiﬁer decisions improving classiﬁcation state-of-the-art. document visualization wordvec embeddings known exhibit linear regularities representing semantic relationships words explore regularities transferred document representation using document vector linear combination wordvec embeddings. weighting scheme employ scores classiﬁer’s predicted class target class relevance estimation. comparison perform uniform weighting simply word embeddings document words either l-norm squared lnorm pooling word gradient values along wordvec dimensions i.e. addition standard word relevance deﬁned alternative ∇wtf denote relevance model employ different variations weighting scheme. precisely given input document composed sequence d-dimensional wordvec embeddings build document representations either using wordlevel relevances element-wise multiplication word embeddings single input variable relevances formally element-wise multiplication. finally normalize document vectors resp. e.w. unit l-norm perform projection. fig. label resulting d-projected test documents using top-level document categories. figure word deletion initially correct false classiﬁed test documents using either target class true document class words deleted decreasing increasing order relevance. random deletion averaged runs steep decline incline indicate informative word relevances. deleting words lowest relevance small values points words less inﬂuence classiﬁcation performance random word selection. result partly explained fact contrast provides signed explanations. generally different quality explanations provided attributed different objectives aims decomposing global amount evidence class build solely upon derivatives describes effect local variations input variables classiﬁer decision. detailed view well interpretation propagation rules deep taylor decomposition montavon word-level relevances used highlighting purposes. fig. provide visualizations test document different relevance target classes using either relevance models. observe word ride highly negative-relevant target class rec.motorcycles positively highlighted suggests clearly discriminate words speaking speciﬁc classiﬁer decision figure heatmaps test document sci.space using either layerwise relevance propagation sensitivity analysis highlighting words. positive relevance mapped negative blue. target class lrp/sa explanation indicated left. figure projection newsgroups test documents formed linearly combining wordvec embeddings. weighting scheme based word-level relevances single input variable relevances uniform target class relevance estimation predicted document class. corresponds variant simple l-norm pooling word gradient values. visualizations provided equal axis scale. visualization quality variant simple l-norm yields partly overlapping dense clusters still schemes better uniform weighting. case note that even though power word gradient norms raised affects present visualization experiment inﬂuence earlier described word deletion analysis. word deleting quantitatively evaluated compared classiﬁer explanation models pinpointed effective investigated application word-level relevance information document highlighting visualization. derive empirical analysis superiority stems fact reliably links determinant words support speciﬁc classiﬁcation decision distinguishes within preeminent words opposed decision. future work would include applying neural network architectures tasks well exploring relevance information could taken account improve classiﬁer’s training procedure prediction performance. work supported german ministry education research berlin data center bbdc brain korea plus program national research foundation korea funded ministry education.", "year": 2016}