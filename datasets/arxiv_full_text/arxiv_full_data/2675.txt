{"title": "Shallow Updates for Deep Reinforcement Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Deep reinforcement learning (DRL) methods such as the Deep Q-Network (DQN) have achieved state-of-the-art results in a variety of challenging, high-dimensional domains. This success is mainly attributed to the power of deep neural networks to learn rich domain representations for approximating the value function or policy. Batch reinforcement learning methods with linear representations, on the other hand, are more stable and require less hyper parameter tuning. Yet, substantial feature engineering is necessary to achieve good results. In this work we propose a hybrid approach -- the Least Squares Deep Q-Network (LS-DQN), which combines rich feature representations learned by a DRL algorithm with the stability of a linear least squares method. We do this by periodically re-training the last hidden layer of a DRL network with a batch least squares update. Key to our approach is a Bayesian regularization term for the least squares update, which prevents over-fitting to the more recent data. We tested LS-DQN on five Atari games and demonstrate significant improvement over vanilla DQN and Double-DQN. We also investigated the reasons for the superior performance of our method. Interestingly, we found that the performance improvement can be attributed to the large batch size used by the LS method when optimizing the last layer.", "text": "deep reinforcement learning methods deep q-network achieved state-of-the-art results variety challenging high-dimensional domains. success mainly attributed power deep neural networks learn rich domain representations approximating value function policy. batch reinforcement learning methods linear representations hand stable require less hyper parameter tuning. substantial feature engineering necessary achieve good results. work propose hybrid approach least squares deep q-network combines rich feature representations learned algorithm stability linear least squares method. periodically re-training last hidden layer network batch least squares update. approach bayesian regularization term least squares update prevents over-ﬁtting recent data. tested ls-dqn atari games demonstrate signiﬁcant improvement vanilla double-dqn. also investigated reasons superior performance method. interestingly found performance improvement attributed large batch size used method optimizing last layer. reinforcement learning ﬁeld research uses dynamic programing among approaches solve sequential decision making problems. main challenge applying real world problems exponential growth computational requirements problem size increases known curse dimensionality tackles curse dimensionality approximating terms calculation value function policy. popular function approximators task include deep neural networks henceforth termed deep linear architectures henceforth termed shallow methods enjoyed wide popularity years particular batch algorithms based least squares approach least squares temporal difference fitted-q iteration known stable data efﬁcient. however success algorithms crucially depends quality feature representation. ideally representation encodes rich expressive features accurately represent value function. however practice ﬁnding good features difﬁcult often hampers usage linear function approximation methods. hand features learned together value function deep architecture. recent advancements using convolutional neural networks demonstrated learning expressive features state-of-the-art performance challenging tasks video games date impressive results obtained using online algorithms based stochastic gradient descent procedure. hand stable data efﬁcient. hand learns powerful representations. motivates combine leverage beneﬁts both? work develop hybrid approach combines batch algorithms online drl. main insight last layer deep architecture seen linear representation preceding layers encoding features. therefore last layer learned using standard algorithms. following insight propose method repeatedly re-trains last hidden layer network batch algorithm using data collected throughout run. focus value-based algorithms based methods propose least squares algorithm approach novel regularization term least squares method uses solution prior bayesian least squares formulation. experiments demonstrate hybrid approach signiﬁcantly improves performance atari benchmark several combinations methods. support results performed in-depth analysis tease factors make hybrid approach outperform drl. interestingly found improved performance mainly large batch size methods compared small batch size typical drl. section describe framework several shallow deep algorithms used throughout paper. framework consider standard formulation based markov decision process tuple ﬁnite states ﬁnite actions discount factor. transition probability function maps states actions probability distribution next states. finally denotes reward. goal learn policy γtrt| value based γtrt|st represents expected discounted return executing action state following policy thereafter. optimal action value function obeys fundamental recursion known bellman equation q∗)| least squares temporal difference q-learning lstd lstd-q batch algorithms. lstd-q learns control policy batch samples estimating linear approximation action value function r|s||a| weights r|s||a|×k feature matrix. represents feature vector state-action pair weights learned enforcing satisfy ﬁxed point equation w.r.t. projected bellman operator resulting system linear equations φtr. here r|s||a| reward vector r|s||a|×|s| transition matrix r|s|×|s||a| matrix describing policy. given nsrl samples si+}nsrl approximate following empirical averages weights calculated using least squares minimization minw calculating pseudo-inverse ˜a†˜b. lstd-q off-policy algorithm samples used train policy long deﬁned every set. fitted iteration algorithm batch q-function using regression. algorithm computes deﬁned approximation iteration algorithm iteration used generate supervised learning targets previous nsrl. targets used supervised maxa method compute next function sequence minimizing qn−))). linmse loss argminq function approximation used give solution minw deep q-network algorithm learns function minimizing mean squared error bellman equation deﬁned estatrtst+qθ maxa qθtarget. maintains separate networks namely current network weights target network weights θtarget. fixing target network makes algorithm equivalent regression algorithm chosen off-policy learning algorithm. therefore tuples used optimize network weights ﬁrst collected agent’s experience stored experience replay buffer providing improved stability performance. double ddqn modiﬁcation algorithm addresses overly optimistic estimates value function. achieved performing action selection current network evaluating action target network θtarget yielding ddqn target update terminal otherwise γqθtarget). present hybrid approach updates. algorithm ls-dqn algorithm periodically switches training network re-training last hidden layer using method. assume algorithm uses deep network representing function last layer linear fully connected. networks used extensively deep recently representation last layer approximates function seen linear combination features propose learn accurate weights using srl. explicitly ls-dqn algorithm begins training weights network using value-based algorithm ndrl steps ls-dqn updates last hidden layer weights wlast executing ls-update retraining weights using algorithm nsrl samples ls-update consists following steps. first data trajectories batch update gathered using current network weights practice current experience replay used additional samples need collected. algorithm next generates features data trajectories using current network weights step guarantees samples inconsistent features contains features ’old’ networks weights. computationally step requires running forward pass deep network every sample performed quickly using parallelization. features generated ls-dqn uses algorithm re-calculate weights last hidden layer wlast ls-dqn algorithm conceptually straightforward found naively running off-the-shelf algorithms lstd resulted instability degradation performance. reason ‘slow’ computation essentially retains information older training epochs batch method ‘forgets’ data recent batch. following propose novel regularization method addressing issue. goal improve performance value-based agent using batch algorithm. batch algorithms however leverage knowledge agent gained recent batch. observed issue prevents off-the-shelf implementations methods hybrid ls-dqn algorithm. code available online https//github.com/shallow-updates-for-deep-rl refer reader appendix diagram algorithm. features last layer action dependent. generate action-dependent features enjoy beneﬁts worlds batch algorithm accumulated knowledge gained network introduce novel bayesian regularization method lstd-q bayesian prior uses last hidden layer weights network wlast algorithm bayesian prior formulation interested learning weights last hidden layer using least squares algorithm. pursue bayesian approach prior weights distribution iteration ls-dqn given wprior recall last hidden layer weights network iteration srliter bayesian wlast solution regression problem algorithm given section present experiments showcasing improved performance attained lsdqn algorithm compared state-of-the-art methods. experiments divided three sections. section start investigating behavior algorithms high dimensional environments. show results ls-dqn atari domains section compare resulting performance regular ddqn agents. finally section present ablative analysis ls-dqn algorithm clariﬁes reasons behind algorithm’s success. ﬁrst experiments explore least squares algorithms perform domains high dimensional observations. important step applying method within ls-dqn algorithm. particular focused answering following questions regularization method use? generate data algorithm? many policy improvement iterations perform? answer questions performed following procedure trained agents games arcade learning environment namely breakout qbert using vanilla implementation periodically save current network weights algorithm re-learn weights last layer evaluate resulting network temporarily replacing weights solution weights. evaluation replace back original weights continue training. evaluation entails roll-outs \u0001-greedy policy periodic evaluation setup allowed effectively experiment algorithms obtain clear comparisons without waiting full runs complete. regularization experiments standard methods without regularization yielded poor results. found main reason matrices used solutions ill-conditioned resulting instability. possible explanation stems sparseness features. uses relu activations causes network learn sparse feature representations. example completed training breakout features zero. added regularization term found performance algorithms improved. reader referred ghavamzadeh overview using bayesian methods every three million steps referred epoch each roll-out starts game follows policy agent loses lives. sensitive scale regularization coefﬁcient. regularizers range performed well across domains. table average scores different coefﬁcients found appendix note expect much improvement replace back original weights evaluation. data gathering experimented mechanisms generating data generating data current policy using found data generation mechanism signiﬁcant impact performance algorithms. data generated current policy solution resulted poor performance compared solution using believe main reason works well contains data sampled multiple policies therefore exhibits exploration state space. policy improvement lstd-q off-policy algorithms applied iteratively dataset however practice found performing multiple iterations improve results. possible explanation improving policy policy reaches areas state space represented well current therefore approximated well solution current network. next full ls-dqn algorithm atari domains asterix space invaders breakout q-bert bowling. ls-dqn using ddqn algorithm using lstd-q algorithms. chose ls-update every ndrl steps total steps used current buffer ‘generated’ data ls-update function regularization coefﬁcient bayesian prior solution emphasize additional samples beyond samples already obtained algorithm. figure presents learning curves network ls-dqn lstd-q ls-dqn three domains asterix space invaders breakout. note evaluation process described mnih also interested test measure differences learning curves maximal score. hence chose perform wilcoxon signed-rank test average scores three variants. non-parametric statistical test measures whether related samples differ means found learning curves ls-dqnlstd-q ls-dqnfqi statistically signiﬁcantly better p-values smaller three domains. table presents maximum average scores along learning curves domains algorithms incorporated agents ddqn agents algorithm ls-dqn attained better performance compared vanilla agents seen higher scores table figure observe interesting phenomenon game asterix figure dqn’s score crashes zero ls-dqnlstd-q manage resolve issue even though achieved signiﬁcantly higher score dqn. ls-dqnfqi however maintained steady performance crash zero. found that general incorporating algorithm agents resulted improved performance. previous section ls-dqn algorithm improved performance compared agents across number domains. goal section understand reasons behind ls-dqn’s improved performance conducting ablative analysis algorithm. analysis used agent trained game breakout manner described section focus analyzing ls-dqnfqi algorithm optimization objective postulate following conjectures improved performance experiments started analyzing learning method last layer optimizing last layer ls-update epoch using bayesian prior solution adam optimizer without additional bayesian prior regularization term loss function. compared approaches different mini-batch sizes data points used experiments. relating conjecture note algorithm hyper-parameter tune produces explicit solution using whole dataset simultaneously. adam hand hyper-parameters tune works different mini-batch sizes. experimental setup experiments done periodic fashion similar section i.e. testing behavior different epochs vanilla run. adam ﬁrst collected data samples epoch. adam performed iterations data iteration consisted randomly permuting data dividing mini-batches optimizing using adam mini-batches. simulate agent report average scores across trajectories. results figure depicts difference average scores baseline scores. larger mini-batches result improved performance. moreover solution outperforms adam solutions mini-batch sizes epochs even slightly outperforms best addition solution prior performs better solution without prior. summary ablative analysis experiments strongly support conjectures above explaining ls-dqn’s improved performance. large-batch methods perform better small-batch methods combining explained above; algorithms focus training last layer easier optimize optimizing last layer improved score across epochs. figure differences average scores different learning methods compared vanilla dqn. positive values represent improvement vanilla dqn. example mini-batch epoch out-performed vanilla adam prior adam without prior under-performed respectively. note that mini-batch size increases improvement adam becomes closer improvement optimizing last layer improves performance. ﬁnish section interesting observation. solution improves performance agents found solution weights close baseline solution. appendix full results. moreover distance inversely proportional performance solution. solution performed best closest norm) solution vice versa. orders magnitude differences norms solutions performed well not. similar results i.e. large-batch solutions solutions close baseline reported compare results ﬁndings keskar section follow. review recent works related paper. regularization general idea applying regularization feature selection avoid overﬁtting common theme machine learning. however applying algorithms challenging fact algorithms based ﬁnding ﬁxed-point rather optimizing loss function .value-based approaches regularization layers popular deep learning methods. example relatively shallow architecture without regularization layers. recently regularization introduced problems combine value-based learning objectives. example hester combine supervised learning expert demonstration introduce regularization avoid over-ﬁtting expert data; kirkpatrick introduces regularization avoid catastrophic forgetting transfer learning. methods hand perform well regularization shown converge farahmand batch size results suggest large batch solution last layer value-based network signiﬁcantly improve it’s performance. result somewhat surprising observed practitioners using larger batches deep learning degrades quality model measured ability generalize however method differs experiments performed keskar therefore contradict them following reasons ls-dqn algorithm uses large batch solution last layer. lower layers network affected large batch solution therefore converge sharp minimum. experiments performed classiﬁcation tasks whereas algorithm minimizing loss. keskar showed large-batch solutions work well piggy-backing small-batch solution. similarly algorithm mixes small large batch solutions switches periodically. moreover recently observed minima practical deep learning model classes turned sharp minima re-parameterization without changing generalization hence requires investigation dinh addition hoffer showed large-batch training generalize well small-batch training adapting number iterations hoffer thus strongly believe ﬁndings combining large small batches agreement recent results deep learning research groups. deep shallow using last-hidden layer feature extractor learning last layer different algorithm addressed literature e.g. context transfer learning competitive attempts unsupervised features play atari best knowledge ﬁrst attempt successfully combines algorithms. work presented ls-dqn hybrid approach combines least-squares updates within online deep ls-dqn obtains best worlds rich representations deep networks well stability data efﬁciency least squares methods. experiments deep methods least squares methods revealed hybrid approach consistently improves vanilla deep atari domain. ablative analysis indicates success ls-dqn algorithm large batch updates made possible using least squares. work focused value-based however hybrid linear/deep approach extended methods actor critic broadly decades research linear methods provided methods strong guarantees approximate linear programming modiﬁed policy iteration approach shows correct modiﬁcations bayesian regularization term linear methods combined deep opens door future combinations well-understood linear deep representation learning. acknowledgement research supported european community’s seventh framework program grant agreement tamar supported part siemens viterbi scholarship technion. donahue jeff yangqing vinyals oriol hoffman judy zhang ning tzeng eric darrell trevor. decaf deep convolutional activation feature generic visual recognition. proceedings international conference machine learning hester todd vecerik matej pietquin olivier lanctot marc schaul piot bilal sendonaris andrew dulac-arnold gabriel osband agapiou john learning demonstrations real world reinforcement learning. arxiv preprint arxiv. jarrett kevin kavukcuoglu koray lecun yann best multi-stage architecture object recognition? computer vision ieee international conference ieee keskar nitish shirish mudigere dheevatsa nocedal jorge smelyanskiy mikhail tang ping peter. large-batch training deep learning generalization sharp minima. arxiv preprint arxiv. kirkpatrick james pascanu razvan rabinowitz neil veness joel desjardins guillaume rusu andrei milan kieran quan john ramalho tiago grabska-barwinska agnieszka overcoming catastrophic forgetting neural networks. proceedings national academy sciences liang yitao machado marlos talvitie erik bowling michael. state control atari games using shallow reinforcement learning. proceedings international conference autonomous agents multiagent systems mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg human-level control deep reinforcement learning. nature mnih volodymyr badia adria puigdomenech mirza mehdi graves alex lillicrap timothy harley silver david kavukcuoglu koray. asynchronous methods deep reinforcement learning. international conference machine learning scherrer bruno ghavamzadeh mohammad gabillon victor lesner boris geist matthieu. approximate modiﬁed policy iteration application game tetris. journal machine learning research silver david huang maddison chris guez arthur sifre laurent driessche george schrittwieser julian antonoglou ioannis panneershelvam veda lanctot marc mastering game deep neural networks tree search. nature tessler chen givony shahar zahavy mankowitz daniel mannor shie. deep hierarchical approach lifelong learning minecraft. proceedings national conference artiﬁcial intelligence wang ziyu schaul hessel matteo hasselt hado lanctot marc freitas nando. dueling network architectures deep reinforcement learning. proceedings international conference machine learning lstd-q regularization cannot applied directly since algorithm ﬁnding ﬁxed-point solving problem. overcome obstacle augment ﬁxed point function lstd-q algorithm include regularization term based stands linear projection bellman optimality operator regularization function. augmented problem solved solution regularized lstd-q problem given derivation results solution lstd-q obtained special case regularized solution kolter figure provides overview ls-dqn algorithm described main paper. agent trained ndrl steps weights last hidden layer denoted data gathered agent’s experience replay features generated srl-algorithm applied generated features includes regularized bayesian prior weight update note weights used prior. weights last hidden layer replaced output wlast process repeated. used implementation adam optim package torch found https// github.com/torch/optim/blob/master/adam.lua. used default hyperparameters learningrate= learningratedecay= beta= beta= epsilon= weightdecay= solutions prior table shows norm difference different solution weights original last layer weights averaged epochs. note stands mini-batch sizes used adam solver. ls-dqn algorithm requires function creates features dataset using current value-based network. notice value-based networks features function state function action. hand lstdq algorithms require features function state action. therefore augment features function action following manner. denote output last hidden layer network deﬁne rf|a| subset indices belongs action zero otherwise refers size action space. note practice ddqn maintain create features states computationally efﬁcient approach would store features agent visits them makes forward propagation store however algorithms work features ﬁxed time. therefore generate features current network.", "year": 2017}