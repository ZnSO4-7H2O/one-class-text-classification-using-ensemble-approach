{"title": "SenGen: Sentence Generating Neural Variational Topic Model", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "We present a new topic model that generates documents by sampling a topic for one whole sentence at a time, and generating the words in the sentence using an RNN decoder that is conditioned on the topic of the sentence. We argue that this novel formalism will help us not only visualize and model the topical discourse structure in a document better, but also potentially lead to more interpretable topics since we can now illustrate topics by sampling representative sentences instead of bag of words or phrases. We present a variational auto-encoder approach for learning in which we use a factorized variational encoder that independently models the posterior over topical mixture vectors of documents using a feed-forward network, and the posterior over topic assignments to sentences using an RNN. Our preliminary experiments on two different datasets indicate early promise, but also expose many challenges that remain to be addressed.", "text": "present topic model generates documents sampling topic whole sentence time generating words sentence using decoder conditioned topic sentence. argue novel formalism help visualize model topical discourse structure document better also potentially lead interpretable topics since illustrate topics sampling representative sentences instead words phrases. present variational auto-encoder approach learning factorized variational encoder independently models posterior topical mixture vectors documents using feedforward network posterior topic assignments sentences using rnn. preliminary experiments different datasets indicate early promise also expose many challenges remain addressed. popular approaches fully generative modeling documents latent dirichlet allocation model. model assumes discrete mixture distribution topics document sampled dirichlet prior shared documents. topic sampled word position document mixture word generated another multinomial indexed corresponding topic. although successful various tasks shortcomings model bag-of-words approach dependencies words explicitly modeled. several extensions proposed relax bag-of-words assumption capture longer term relationships words based approach learning complex generative distributions generative model well approximate variational posterior based deep neural networks approach recently applied topic modeling documents several researchers. ﬁrst vae-based approaches document modeling called neural variational document model reports impressive gains models perplexity. however topic mixture vector model real-valued vector generated multivariate gaussian interpretable unlike multinomial mixture standard model. motivated weakness nvdm authors propose nvlda model employs logistic normal distribution replace dirichlet prior variational logistic normal posterior bring vector multinomial space. however perplexity values nvlda model unseen data worse nvdm model. although models mentioned employ sophisticated approach still bag-of-words formalism modeling document. further approach focuses modeling posteriors document-level topic mixtures vector ignores modeling posteriors local topic assignment words sentences. advent neural networks rnn-based language models emerged facto choice capture short long range dependencies words used language modeling speech dialogue however models capture topical structure larger document context. recent work integrates topic modeling rnns gaussian based topic vector similar used nvdm used model topic strengths document used generate words conditioned topic vector. topic model marginalizes topic assignments words without explicitly modeling posteriors. steps algorithm describe generative process model detail. graphical representation model also presented figure note separate decoder topic share parameters except word-generating softmax layer. parameters word-generating decoder softmax operator maps topic strengths vector multinomial simplex hyperparameter indicating number topics model number sentences document similar work interested modeling dependencies words document also capturing larger topical context jointly. addition also interested capturing topical discourse structure document including notions topical drift topical switch. capture phenomena argue sentences ideal smallest units modeling instead individual words phrases since sentences tend topically cohesive topical drift switch usually occur across sentence paragraph boundaries. therefore make topical assignments whole sentences unlike traditional topic models assign individual word position document. rnns generate words sentence conditioned assigned topic capture within-sentence dependencies words. believe modeling choice allows better visualize topical discourse structure document also potentially lend topics better interpretability since visualize generating representative sentences learned topic-speciﬁc word generators. work also present framework model posteriors topic assignment variables sentence-level explicitly encoder based another rnn. previous work vae-based learning approach topic models focus modeling posterior topical mixture document-level ignore issue modeling posterior topic assignments. hope work explicit modeling posteriors topic assignment variables pave future work sophisticated posteriors also capture topical correlations across neighboring sentences. sengen model ﬁrst samples document-level topic strengths k-dimensional multivariate gaussian topic indices sampled mixture distribution softmax sentence document. conditioned topic-id topic-speciﬁc gru-rnn based decoder generate words sentence. conditioning topic done topic-embedding vector embz also learned automatically. sample topic mult) initialize hidden state embeddings zeroth word zeros select context vector topic embeddings embz word position sentence {··· number words document parameters encoder. k-dimensional gaussian noise vector generated last equation above used reparametrization trick sample encoder’s posterior maintaining end-to-end differentiability model. thus encoder consumes words sentence input every time step emits posterior probabilities topics last time step graphical representation variational encoders displayed figure vector words document vector words sentence words posteriors topic indicators sentence assumed independent posteriors topic vector entire document. clearly simplifying assumption makes inference tractable need relaxed future. note encoders amortized documents unlike mean-ﬁeld approaches latent variable assumed independent posterior document-level encoder simple feed forward network estimates mean covariance posterior given following series steps training validation sets validation loss could used early stopping training process. training documents validation documents test documents. sentences document words sentence average. pruned vocabulary frequent words close reported experiments however clear vocabulary used experiments identical vocabulary preprocessed versions since many clean-up operations removing email-headers signatures documents reduce noise. cnn/daily mail corpus large corpus consisting documents. documents corpus well formatted sentence boundaries required model. randomly subsampled documents training documents validation documents testing. average dataset sentences document words sentence. vocabulary size pruned frequent words. baselines corpus using open source code. runs datasets used word embeddings dimension pre-trained using wordvec full cnn/daily mail corpus. hidden state encoder decoder rnns dimension readout layer topic decoder share parameters except softmax layer parameters distinct. since softmax layer size training vocabulary size size readout layer training model challenging terms space time computational requirements. therefore limited number topics also ﬁxed batch size save memory implemented variant large vocabulary trick batch sampled subset words training corpus distribution used vocabulary softmax layer addition words occurred figure graphical representation encoder architecture posteriors sentence-level topic assignments document level topical strengths modeled independent other. ﬁrst term above involving kl-divergence between gaussians computed analytically described second term above consisting expectation computed using sample based estimate. computing terms took advantage clear separation posterior compute expectations exactly involves summation topics. experiments used ‘by-date’ version newsgroups dataset downloadable http// qwone.com/˜jason/newsgroups/ well cnn/daily mail corpus available http//cs.nyu. edu/˜kcho/dmqa/. although preprocessed versions newsgroups datasets available text tokenized words converted integer preprocessed text since needed preserve sentence boundary information. remove stopwords since want model produce meaningful sentences. used ofﬁcial training test splits deﬁned by-date version dataset. sub-divided training pect main reason differences preprocessing work others noticed many non-dictionary terms vocabulary originated email signatures headers. another potential reason model overﬁtting training extremely large number parameters. advantage assigning topics whole sentences decoder learns generate sentences topic could potentially interpretable representing topics merely ranking words. table displayed best sequences generated beam search width decoder’s softmax layer three randomly chosen topics daily mail data set. also displayed different stochastic samples topics greedily sample words distribution deﬁned softmax layer decoder time-step. table shows best sequences tend generic non-informative sentences. although grammatically well formed beginning tend repeat generated phrases time-steps. stochastic samples hand grammatically well formed contain topical words. however learned topics certainly coherent learned bag-of-words approaches lda. clearly work needs done models learn interpretable topics. address issue non-informative best sequences need handle stop words frequent words special manner done work used separate class words mixed topics. also since sengen model large number parameters desirable initialize model’s parameters learned bag-of-words model less chance gets stuck arbitrary local minima. batch. despite this training still slow since need compute softmax probabilities sentence decoders. newsgroups dataset total number parameters model epoch took hours average single gpu. cnn/daily mail corpus parameter size epoch took hours. although cnn/daily mail corpus smaller number parameters owing smaller vocabulary size longer documents average newsgroups explains longer training time. used adadelta variant automatically adapts learning rate used gradient clipping training stability. regularization objective function. reduce model size also experimented variant model even softmax layer shared topic-speciﬁc decoders except bias vector topic-speciﬁc. model resulted considerable memory savings also sped training time smaller number parameters. however failed learn distinct topics forced abandon variant. probability computed using lowerbound estimate equation number test documents number words document compared perplexity model models could compare results since reported numbers different datasets code publicly released either. results table indicate sengen model achieve better perplexity models compared newsgroups dataset. cnn/daily mail model achieves better perplexity well prodlda variant good vae-based models. newsgroups datasets sussaid made case made case bestsellers positives hollywood-walk-of-fame dribbled association wilmington- horyn timberlake united-lincolnshire-hospitals-trust enrolled snowballed helipad advertiser people able able make catania ralph-lauren some impressionable re-interview texas-department-of-public-safety characters lucy-jones breakwater chats david-laws fanciful dyke gustafson said table example sequences words generated model trained daily mail corpus conditioned various topics. best sequence obtained performing beam search softmax layer decoder rnn. stochastic samples obtained greedy sampling softmax layer word time. main contributions work assigning topics whole sentences instead words resulting topics potential interpretable since generate representative sentences topic; presenting approach models posteriors topic mixtures document-level also topic assignments sentence-level. preliminary qualitative quantitative results indicate promise deeper investigation needs conducted overcome existing deﬁciencies current model handling frequent words preventing overﬁtting learning better topics improving computational efﬁciency. although motivations capture topical discourse structure including phenomena topic drift topic switch work addresses issue partially posteriors topics sentence visualized graphically. believe framework proposed work extended construct sophisticated models capture dependencies topics adjacent sentences. another direction interested exploring provide decoder topical context also context previous sentences document. finally also need relax assumption fully factorized posteriors document’s topic vector sentence topics. chung junyoung g¨ulc¸ehre aglar kyunghyun bengio yoshua. empirical evaluation gated recurrent neural networks sequence modeling. corr abs/. http//arxiv.org/ abs/.. chung junyoung kastner kyle dinh laurent goel kratarth courville aaron bengio yoshua. recurrent latent variable model sequential data. corr abs/. http//arxiv. org/abs/.. dieng adji wang chong jianfeng paisley john william. topicrnn recurrent neural network long-range semantic dependency. corr abs/. http//arxiv.org/ abs/.. jean s´ebastien kyunghyun memisevic roland using large target bengio yoshua. corr vocabulary neural machine translation. abs/. http//arxiv.org/ abs/.. mikolov karaﬁt martin burget ernock khudanpur sanjeev. recurrent neural network based language model. proceedings annual conference international speech communication association serban iulian vlad sordoni alessandro lowe ryan charlin laurent pineau joelle courville aaron bengio yoshua. hierarchical latent variable encoder-decoder model generating dialogues. corr abs/. http//arxiv.org/ abs/..", "year": 2017}