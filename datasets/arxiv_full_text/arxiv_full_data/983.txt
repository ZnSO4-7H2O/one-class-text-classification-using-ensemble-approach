{"title": "Semi-Supervised Learning with Ladder Networks", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pre-training. Our work builds on the Ladder network proposed by Valpola (2015), which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in semi-supervised MNIST and CIFAR-10 classification, in addition to permutation-invariant MNIST classification with all labels.", "text": "combine supervised learning unsupervised learning deep neural networks. proposed model trained simultaneously minimize supervised unsupervised cost functions backpropagation avoiding need layer-wise pre-training. work builds ladder network proposed valpola extend combining model supervision. show resulting model reaches state-of-the-art performance semi-supervised mnist cifar- classiﬁcation addition permutationinvariant mnist classiﬁcation labels. paper introduce unsupervised learning method well supervised learning. idea using unsupervised learning complement supervision new. combining auxiliary task help train neural network proposed suddarth kergosien sharing hidden representations among task network generalizes better. multiple choices unsupervised task example reconstruction inputs every level model classiﬁcation input sample class although methods able simultaneously apply supervised unsupervised learning often unsupervised auxiliary tasks applied pre-training followed normal supervised learning complex tasks often much structure inputs represented unsupervised learning cannot deﬁnition know useful task hand. consider instance autoencoder approach applied natural images auxiliary decoder network tries reconstruct original input internal representation. autoencoder preserve details needed reconstructing image pixel level even though classiﬁcation typically invariant kinds transformations preserve pixel values. information required pixel-level reconstruction irrelevant takes space relevant invariant features which almost deﬁnition cannot alone used reconstruction. approach follows valpola proposed ladder network auxiliary task denoise representations every level model. model structure autoencoder skip connections encoder decoder learning task similar denoising autoencoders applied every layer inputs. skip connections relieve pressure represent details higher layers model because skip connections aspects approach follows compatibility supervised methods. unsupervised part focuses relevant details found supervised learning. furthermore added existing feedforward neural networks example multi-layer perceptrons convolutional neural networks show take state-of-the-art supervised learning method starting point improve network adding simultaneous unsupervised learning scalability resulting local learning. addition supervised learning target layer model local unsupervised learning targets every layer making suitable deep neural networks. demonstrate deep supervised network architectures. computational efﬁciency. encoder part model corresponds normal supervised learning. adding decoder proposed paper approximately triples computation training necessarily training time since result achieved faster better utilization available information. overall computation update scales similarly whichever supervised learning approach used small multiplicative factor. explained section skip connections layer-wise unsupervised targets effectively turn autoencoders hierarchical latent variable models known well suited semisupervised learning. indeed obtain state-of-the-art results semi-supervised learning mnist permutation invariant mnist cifar- classiﬁcation tasks however improvements limited semi-supervised settings permutation invariant mnist task also achieve record normal full-labeled setting. latent variable models attractive approach semi-supervised learning combine supervised unsupervised learning principled way. difference whether class labels observed not. approach taken instance goodfellow multi-prediction deep boltzmann machine. particularly attractive property hierarchical latent variable models general leave details lower levels represent allowing higher levels focus invariant abstract features turn relevant task hand. training process latent variable models typically split inference learning ﬁnding posterior probability unobserved latent variables updating underlying probability model observations better. instance expectation-maximization algorithm e-step corresponds ﬁnding expectation latent variables posterior distribution assuming model ﬁxed m-step maximizes underlying probability model assuming expectation ﬁxed. main problem latent variable models make inference learning efﬁcient. suppose layers latent variables latent variable models often represent probability distribution variables explicitly product terms directed graphical models. inference process model updates derived bayes’ rule typically kind approximation. inference often iterative generally impossible solve resulting equations closed form function observed variables. close connection denoising probabilistic modeling. hand given probabilistic model compute optimal denoising. want reconstruct latent using prior observation noise. ﬁrst compute posterior distribution center gravity reconstruction show minimizes expected denoising cost hand given denoising function draw preliminary results full-labeled setting permutation invariant mnist task reported short early version paper compared that added noise layers model simpliﬁed denoising function improved results. valpola proposed ladder network inference process learned using principle denoising used supervised learning denoising autoencoders denoising source separation complementary tasks. autoencoder trained reconstruct original observation corrupted version learning based simply minimizing norm difference original reconstruction corrupted cost daes normally trained denoise observations framework based idea using denoising functions latent variables train mapping models likelihood latent variables function observations. cost function identical used except latent variables replace observations cost thing keep mind needs normalized somehow otherwise model trivial solution constant. cannot happen model cannot change input figure depicts optimal denoising function one-dimensional bimodal distribution could distribution latent variable inside larger model. shape denoising function depends distribution properties corruption noise. noise optimal denoising function would identity function. general denoising function pushes values towards higher probabilities shown green arrows. figure shows structure ladder network. every layer contributes cost function term trains layers learn denoising function maps corrupted onto denoised estimate estimate incorporates prior knowledge cost function term also trains encoder layers cleaner features better match prior expectation. since cost function needs clean corrupted training encoder twice clean pass corrupted pass another feature differentiates ladder network regular daes layer skip connection encoder decoder. feature mimics inference structure latent variable models makes possible higher levels network leave details lower levels represent. rasmus showed skip connections allow daes focus abstract invariant features higher levels making ladder network good supervised learning select information relevant task hand. picture ladder network consider collection nested denoising autoencoders share parts denoising machinery other. viewpoint autoencoder layer representations higher layers treated hidden neurons. words particular reason produced decoder resemble corresponding representations produced encoder. cost function ties together forces inference proceed reverse order decoder. sharing helps deep denoising autoencoder learn denoising process splits task meaningful sub-tasks denoising intermediate representations. steps involved implementing ladder network typically follows take feedforward model serves supervised learning encoder decoder invert mappings layer encoder supports unsupervised learning train whole ladder network minimizing cost function terms. figure depiction optimal denoising function bimodal distribution. input function corrupted value target clean value denoising function moves values towards higher probabilities show green arrows. figure conceptual illustration ladder network feedforward path shares mappings corrupted feedforward path encoder ˜y). decoder consists denoising functions cost functions layer trying minimize difference output encoder also trained match available labels consider training classiﬁer mapping input output targets training pairs semi-supervised learning studies auxiliary unlabeled data help training classiﬁer. often case labeled data scarce whereas unlabeled data plentiful ladder network improve results even without auxiliary unlabeled data original motivation make possible take well-performing feedforward classiﬁers augment auxiliary decoder follows train standard feedforward neural network. network type limited standard mlps approach applied example convolutional recurrent networks. encoder part ladder network. layer analyze conditional distribution representations given layer above observed distributions could resemble example gaussian distributions mean variance depend values bimodal distributions relative probability masses modes depend values deﬁne function approximate optimal denoising function family observed distributions. function therefore expected form reconstruction resembles clean given corrupted higher-level reconstruction starting point fully connected network rectiﬁed linear units. follow ioffe szegedy apply batch normalization preactivation including topmost layer l-layer network. serves purposes. first improves convergence result reduced covariate shift originally proposed ioffe szegedy second explained section dss-type cost functions input layer require type normalization prevent denoising cost encouraging trivial solution encoder outputs constant values easiest denoise. batch normalization conveniently serves purpose too. formally batch normalization layers implemented component-wise batch normalization /ˆσxi ˆµxi ˆσxi estimates calculated minibatch trainable parameters activation function rectiﬁed linear unit max. outputs always softmax activation. activation functions scaling parameter bias redundant apply non-redundant cases. example rectiﬁed linear unit need scaling linear activation function needs neither scaling bias softmax requires both. explained section shown figure ladder network requires forward passes clean corrupted produce clean corrupted respectively. implemented corruption adding isotropic gaussian noise inputs batch normalization networks structure reach close state-of-the-art results purely supervised learning makes good starting points improvement semi-supervised learning adding auxiliary unsupervised task. designing suitable decoder support unsupervised learning make choice kinds distributions latent variables decoder would optimally able denoise. ultimately ended choosing parametrization supports optimal denoising gaussian latent variables. also experimented alternative denoising functions details found appendix analysis different denoising functions recently published pezeshki order derive chosen parametrization justify supports gaussian latent variables begin assumption noisy value latent variable want denoise form clean latent variable value gaussian distribution variance want estimate denoised version estimate minimizes squared error difference clean latent variable values shown functional form linear order minimize denoising cost assumption could trainable parameters model model would learn estimate optimal weighting prior problem formulation supports optimal denoising latent variables gaussian distribution function linear wrt. relax assumption making model require distribution layer gaussian conditional values latent variables layer above. similar vein layer multiple latent variables assume latent variables independent conditional latent variables layer above. distribution latent variables therefore assumed follow distribution interpretation formulation modeling distribution mixture gaussians diagonal covariance matrices value layer modulates form gaussian distributed practice implement dependence batch normalized projection followed expressive nonlinearity trainable parameters. ﬁnal formulation denoising function therefore trainable parameters nonlinearity neuron layer worth noting parametrization denoised value full means model optimally denoise conditionally independent distributions. nonlinearity makes number parameters decoder slightly higher encoder difference insigniﬁcant parameters vertical projection mappings dimensions note slight abuse notation since rather full vectors given parametrization linear respect slope bias depend nonlinearly hoped. side note values truly independently distributed gaussians nothing left layer above model. case mixture gaussians needed model diagonal gaussian modeled linear denoising function constant values equation would sufﬁce. parametrization correlations nonlinearities non-gaussianities latent variables represented modulations layers optimal denoising. parametrization allows distribution modulated encourages decoder representations high mutual information crucial allows supervised learning indirect inﬂuence representations learned unsupervised decoder abstractions selected supervised learning bias lower levels representations carry information abstractions. cost function unsupervised path mean squared reconstruction error neuron slight twist found important. batch normalization useful properties noted section also introduces noise affects clean corrupted encoder pass. noise highly correlated noise derives statistics samples happen minibatch. highly correlated noise biases denoising functions simple copies solution found implicitly projections target denoising scale cost function term appearing error term batch normalized instead. moment works scalar case model parameters trained simply using backpropagation algorithm optimize total cost feedforward pass full ladder network listed algorithm classiﬁcation results read clean feedforward path. section detailed build decoder ladder network match fully connected encoder described section easy extend approach encoders instance convolutional neural networks decoder fully connected networks used vertical mappings whose shape transpose encoder mapping. treatment works convolution operations networks tested paper decoder convolutions whose parametrization mirrors encoder effectively reverses whole point using denoising autoencoders rather regular autoencoders prevent skip connections short-circuiting decoder force decoder learn meaningful abstractions help denoising. many convolutional networks pooling operations stride; downsample spatial feature maps. decoder needs compensate corresponding upsampling. several alternative ways implement paper chose following options encoder side pooling operations treated separate layers batch normalization linear activation function downsampling pooling encoder side compensated upsampling copying decoder side. provides multiple targets decoder match helping decoder recover information lost encoder side. worth noting simple special case decoder model corresponds denoising cost layer means decoder omitted. model call γ-model shape graph useful easily plugged feedforward network without decoder implementation. addition γ-model mlps convolutional neural networks. encoder γ-model still includes clean corrupted paths full ladder. experiments mnist cifar- datasets wanted compare method semi-supervised methods also show attach decoder fully connected network convolutional neural network described section also wanted compare performance simpler γ-model full ladder network experimented cost function input layer. cifar- tested γ-model. also measured performance supervised baseline models included encoder supervised cost function. cases compared directly ladder networks best optimize hyperparameters regularization baseline supervised learning models improvements could explained example lack suitable regularization would provided denoising costs. convolutional networks focus exclusively semi-supervised learning. supervised baselines labels intend show performance selected network architectures line ones reported literature. make claims neither optimality statistical signiﬁcance baseline results. used adam optimization algorithm weight updates. learning rate ﬁrst part learning followed annealing phase learning rate linearly reduced zero. minibatch size source code experiments available https//github.com/arasmus/ladder unless explicitly noted text. evaluating semi-supervised learning used standard test samples held-out test randomly split standard training samples -sample validation used samples training set. training randomly chose labels supervised cost. samples used decoder need labels. validation used evaluating model structure hyperparameters. also balanced classes ensure particular class over-represented. repeated training times varying random seed used splits. experiments careful optimize parameters hyperparameters model choices basis results held-out test samples. customary used labeled validation samples even settings used labeled samples training. obviously something could done real case labeled samples. however mnist classiﬁcation easy task even permutation invariant case labeled samples correspond greater number labeled samples many datasets. table collection previously reported mnist test errors permutation invariant setting followed results ladder network. svm. standard deviation parentheses. test error used labels semi-sup. embedding transductive pseudo-label atlasrbf dropout adversarial virtual adversarial baseline gaussian noise γ-model ladder bottom-level cost ladder full optimizing hyperparameters performed ﬁnal test runs using training samples different random initializations weight matrices data splits. trained models epochs followed epochs annealing. minibatch size amounts weight updates validation runs ﬁnal test runs. useful test general learning algorithms permutation invariant mnist classiﬁcation task. permutation invariance means results need invariant respect permutation elements input vector. words allowed prior information spatial arrangement input pixels. excludes among others convolutional networks geometric distortions input images. hyperparameters tuned model noise level added inputs layer denoising cost multipliers also supervised baseline model various noise levels. models cost multiplier optimized search grid ladder networks cost function layers much larger search space explored much sparsely. instance optimal model found labels good value gaussian corruption noise mostly labels better value. complete selected denoising cost multipliers hyperparameters please refer code. results presented table show proposed method outperforms previously reported results. encouraged good results also tested labels test error simple γ-model also performed surprisingly well particularly labels. labels models sometimes failed converge properly. bottom level full costs ladder around runs result test error order able estimate average test error reliably presence random outliers instead test runs random initializations. tested convolutional networks general mnist classiﬁcation task omitted data augmentation geometric distortions. focused -label case since labels results already good even difﬁcult permutation invariant task. ﬁrst network straightforward extension fully connected network tested permutation invariant case. turned ﬁrst fully connected layer convolution -by- ﬁlters resulting -by- spatial features. nine spatial locations processed independently network structure previous section ﬁnally resulting -by- spatial features. pooled global mean-pooling layer. essentially thus convolved image complete fully connected network. depooling topmost layer deconvolutions layers implemented described section since internal structure nine almost independent processing paths permutation invariant task used hyperparameters optimal permutation invariant task. table model referred conv-fc. second network inspired convpool-cnn-c springenberg tested γ-model. mnist classiﬁcation task typically solved smaller number parameters cifar- topology originally developed modiﬁed network removing layers reducing number parameters remaining layers. addition observed adding small fully connected layer neurons global mean pooling layer improved results semi-supervised task. tune parameters noise level chosen using validation set. exact architecture network detailed table appendix referred conv-small since smaller version network used forthe cifar- dataset. results table conﬁrm even single convolution bottom level improves results fully connected network. convolutions improve γ-model signiﬁcantly although high variance results suggests model still suffers conﬁrmation bias. ladder network denoising targets every level converges much reliably. taken together results suggest combining generalization ability convolutional networks efﬁcient unsupervised learning full ladder network would resulted even better performance left future work. cifar- dataset consists small -by- images classes. labeled samples training testing. like mnist dataset used testing semi-supervised learning decided test simple γ-model convolutional network reported perform well standard supervised setting labels. tested model architectures selected convpool-cnn-c springenberg also evaluated strided convolutional version springenberg performed well labels found max-pooling version overﬁtted less fewer labels thus used general convolutional networks excel mnist classiﬁcation task. performance fully supervised conv-small labels line literature provided rough reference main differences convpool-cnn-c gaussian noise instead dropout convolutional per-channel batch normalization following ioffe szegedy dropout useful labels seem offer advantage additive gaussian noise fewer labels. detailed description model please refer model conv-large table testing purely supervised model performance limited number labeled samples found model overﬁtted quite severely training error samples decreased much network effectively learned nothing network already conﬁdent classiﬁcation. network equally conﬁdent validation samples even misclassiﬁed. noticed could regularize network stripping away scaling parameter last layer. means variance input softmax restricted unity. also used setting corresponding γ-model although denoising target already regularizes network signiﬁcantly improvement pronounced. hyperparameters models optimized using samples training remaining samples validation. best hyperparameters selected ﬁnal model trained settings samples. experiments four different random initializations weight matrices data splits. applied global contrast normalization whitening following goodfellow data augmentation used. results shown table supervised reference obtained model closer original convpool-cnn-c sense dropout rather additive gaussian noise used regularization. spent time tuning regularization fully supervised baseline model labels indeed results exceed previous state art. tuning important make sure improvement offered denoising target γ-model sign poorly regularized baseline model. although improvement dramatic mnist experiments came simple addition standard supervised training. early works semi-supervised learning proposed approach inputs ﬁrst assigned clusters cluster class label. unlabeled data would affect shapes sizes clusters thus alter classiﬁcation result. approach reinterpreted input vectors corrupted copies ideal input vectors classiﬁcation mapping split parts ﬁrst denoising labeling well known probabilistic model directly estimates trained unlabeled data cannot help. study assign probabilistic labels unlabeled inputs train using labels shown gradient vanish. different ways circumventing phenomenon adjusting assigned labels related γ-model. label propagation methods estimate adjust probabilistic labels basis assumption nearest neighbors likely label. labels start propagate regions high-density γ-model implicitly assumes labels uniform vicinity clean input since corrupted inputs need produce label. produces similar effect labels start propagate regions high density weston explored deep versions label propagation. co-training assumes multiple views train classiﬁers different views know even unlabeled data true label view. view produces probabilistic labeling combination train individual classiﬁers. interpret several corrupted copies input different views relationship proposed method. adjusts assigned labels rounding probability likely class others zero. training starts trusting true labels gradually increasing weight so-called pseudo-labels. similar scheduling could tested γ-model seems suffer conﬁrmation bias. well denoising cost optimal beginning learning smaller optimal later stages learning. dosovitskiy pre-train convolutional network unlabeled data treating clean image class. training image corrupted transforming location scaling rotation contrast color. helps features invariant transformations used. discarding last classiﬁcation layer replacing classiﬁer trained real labeled data leads surprisingly good experimental results. interesting connection γ-model contractive cost used rifai linear denoising function parameters turns denoising cost stochastic estimate contractive cost. recently miyato achieved impressive results regularization method similar idea contractive cost. required output network change little possible close input samples. requires labels able unlabeled samples regularization. semi-supervised results good denoising target input layer results full labels come close. cost function last layer suggests approaches complementary could combined potentially improving results further. reviewed semi-supervised methods unsupervised cost function output layer therefore related γ-model. move semisupervised methods concentrate modeling joint distribution inputs labels. multi-prediction deep boltzmann machine train backpropagation variational inference. targets inference include supervised targets unsupervised targets used training simultaneously. connections inference network somewhat analogous lateral connections. speciﬁcally inference paths observed inputs reconstructed inputs highest layers. compared approach mp-dbm requires iterative inference initialization hidden activations whereas case inference simple single-pass feedforward procedure. deep autoregressive network unsupervised method learning representations also uses lateral connections hidden representations. connectivity within layer rather different ours though unit receives input preceding units whereas case unit receives input learning algorithm based approximating gradient description length measure whereas gradient simple loss function. kingma proposed deep generative models semi-supervised learning based variational autoencoders. models trained variational algorithm stochastic gradient variational bayes stochastic backpropagation. also experimented stacked version bottom autoencoder reconstructs input data autoencoder concentrate classiﬁcation reconstructing hidden representation stacked version performed best hinting might important carry information highest layers. compared ladder network interesting point variational autoencoder computes posterior estimate latent variables encoder alone ladder network uses decoder compute implicit posterior approximate interesting whether approaches combined. ladder-style decoder might provide posterior another decoder could generative model variational autoencoders. zeiler train deep convolutional autoencoders manner comparable ours. deﬁne max-pooling operations encoder feed function upwards next layer argmax function laterally decoder. network trained layer time using cost function includes pixel-level reconstruction error regularization term promote sparsity. zhao similar structure call stacked what-where autoencoder network trained simultaneously minimize combination supervised cost reconstruction errors level like ours. recently bengio proposed target propagation alternative backpropagation. idea base learning errors gradients expectations. similar idea denoising source separation therefore resembles propagation expectations decoder ladder network. ladder network additional lateral connections encoder decoder play important role remains seen whether lateral connections compatible target propagation. nevertheless interesting possibility ladder network includes mechanisms propagating information backpropagation gradients forward propagation expectations decoder possible rely solely latter thus avoiding problems related propagation gradients many layers exploding gradients. showed simultaneous unsupervised learning task improves networks reaching state various semi-supervised learning tasks. particular performance obtained small numbers labels much better previous published results shows method capable making good unsupervised learning. however model also achieves state-of-the-art results signiﬁcant improvement baseline model full labels permutation invariant mnist classiﬁcation suggests unsupervised task disturb supervised learning. proposed model simple easy implement many existing feedforward architectures training based backpropagation simple cost function. quick train convergence fast thanks batch normalization. surprisingly largest improvements performance observed models large number parameters relative number available labeled samples. cifar- started model originally developed fully supervised task. beneﬁt building existing experience well best results obtained models parameters fully supervised approaches could handle. obvious future line research therefore study kind encoders decoders best suited ladder network. work made small modiﬁcations encoders whose structure optimized supervised learning designed parametrization vertical mappings decoder mirror encoder information reversed. nothing preventing decoder different structure encoder. interesting future line research extension ladder networks temporal domain. datasets millions labeled samples still images exist prohibitively costly label thousands hours video streams. ladder networks scaled easily therefore offer attractive approach semi-supervised learning large-scale problems. received comments help number colleagues deserve mentioned wish thank especially yann lecun diederik kingma aaron courville goodfellow søren sønderby hugo larochelle helpful comments suggestions. software simulations paper based theano blocks also acknowledge computational resources provided aalto science-it project. academy finland supported tapani raiko. references bastien lamblin pascanu bergstra goodfellow bergeron bouchard bengio theano features speed improvements. deep learning unsupervised feature learning nips workshop. bergstra breuleux bastien lamblin pascanu desjardins turian wardefarley bengio theano math expression compiler. proceedings python scientiﬁc computing conference oral presentation. proc. eleventh annual conference computational learning theory pages chapelle sch¨olkopf zien semi-supervised learning. press. dosovitskiy springenberg riedmiller brox discriminative unsupervised feature learning convolutional neural networks. advances neural information processing systems pages mclachlan iterative reclassiﬁcation procedure constructing asymptotically optimal rule allocation discriminant analysis. american statistical association miyato maeda koyama nakae ishii distributional smoothing suddarth kergosien rule-injection hints means improving network performance learning time. proceedings eurasip workshop neural networks pages springer. merri¨enboer bahdanau dumoulin serdyuk warde-farley chorowski bengio blocks fuel frameworks deep learning. corr abs/.. vincent larochelle lajoie bengio manzagol p.-a. stacked denoising autoencoders learning useful representations deep network local denoising criterion. jmlr convpool-cnn-c conv. relu conv. relu conv. relu max-pooling stride conv. relu conv. relu conv. relu max-pooling stride conv. relu conv. relu conv. relu global meanpool conv-large conv. leakyrelu conv. leakyrelu conv. leakyrelu max-pooling stride conv. leakyrelu conv. relu conv. leakyrelu conv. relu conv. leakyrelu max-pooling stride conv. leakyrelu conv. relu conv. leakyrelu conv. leakyrelu conv. relu global meanpool global meanpool fully connected describe model structures conv-small conv-large used mnist cifar- datasets respectively. inspired convpool-cnn-c springenberg table details model architectures differences models work convpool-cnn-c. noteworthy architecture fully connected layers replaces global mean pooling layer softmax function. main differences models convpool-cnn-c gaussian noise instead dropout convolutional per-channel batch normalization following ioffe szegedy also used stride max-pooling instead stride max-pooling. leakyrelu used speed training mentioned springenberg utilized batch normalization layers including pooling layers. gaussian noise also added layers instead applying dropout layers convpool-cnn-c. denoising function tries clean reconstructed ˆz). reconstruction therefore based corrupted value reconstruction layer above. optimal functional form depends conditional distribution want model able denoise. example distribution gaussian optimal function function achieves lowest reconstruction error going linear respect parametrization chose basis preliminary comparisons different denoising function parametrizations. modeled expressive nonlinearity asigmoid+ asigmoid left superscript subscript order clutter equations. given parametrization linear respect slope bias depended nonlinearly order test whether elements proposed function necessary systematically removed components replaced altogether compared resulting performance results obtained original parametrization. tuned hyperparameters comparison model separately using grid search relevant hyperparameters. however standard deviation additive gaussian corruption noise means comparison include best-performing models reported table achieved best validation errors careful hyperparameter tuning. proposed function comparison denoising functions mapped neuron-wise corrupted hidden layer pre-activation reconstructed hidden layer activation given projection reconstruction layer above table semi-supervised results mnist dataset. proposed function compared alternative parametrizations. note hyperparameter search exhaustive ﬁnal results means results proposed model deviate slightly ﬁnal results presented table augmented input trainable weight vectors trainable scalar weight. parametrization capable learning denoising several different distributions including subsuper-gaussian bimodal distributions. differs linear sigmoid term. formulation linear supports gaussian distributions. although parametrization augmented term lets modulate slope shift distribution scope possible denoising functions still fairly limited. differs inputs allowed modulate terms depend effect additive. means parametrization supports optimal denoising functions conditional distribution shifts mean distribution otherwise leaves shape distribution intact. results models tested similar setting semi-supervised fully connected mnist task using labeled samples. also reran best comparison model labels. results analyses presented table results support ﬁnding rasmus modulation lateral connection critical encouraging development invariant representations higher layers model. comparison function lacked modulation clearly performed worse denoising function listed table even linear performed well long term ˜zu. leaving nonlinearity removing hurt performance much more. addition alternative parametrizations g-function experiments using standard autoencoder structure. structure attached additional decoder standard using hidden layer input decoder reconstruction clean input target. structure decoder encoder number size layers input hidden layer decoder attached number size layers decoder. ﬁnal activation function decoder sigmoid nonlinearity. training target weighted reconstruction cost classiﬁcation cost. tested autoencoder structure labeled samples. experiments possible decoder lengths tried attaching decoder hidden layers. however manage signiﬁcantly better performance standard supervised model without decoder experiments.", "year": 2015}