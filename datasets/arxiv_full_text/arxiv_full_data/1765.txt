{"title": "Model Interpolation with Trans-dimensional Random Field Language Models  for Speech Recognition", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "The dominant language models (LMs) such as n-gram and neural network (NN) models represent sentence probabilities in terms of conditionals. In contrast, a new trans-dimensional random field (TRF) LM has been recently introduced to show superior performances, where the whole sentence is modeled as a random field. In this paper, we examine how the TRF models can be interpolated with the NN models, and obtain 12.1\\% and 17.9\\% relative error rate reductions over 6-gram LMs for English and Chinese speech recognition respectively through log-linear combination.", "text": "throughout denote sentence length ranging element corresponds single word. denotes whole training corpus denotes collection length training corpus. denotes size nl/n empirical probability length fd)t feature vector usually deﬁned position-independent length-independent e.g. n-grams. dimension feature vector corresponding parameter vector normalization constant length making explicit role length model deﬁnition clear model mixture random ﬁelds sentences different lengths hence called trans-dimensional random ﬁeld hypothesized value ratio respect namely chosen reference value calculated exactly. important observation equal true ratios marginal probability length distribution equals nl/n. property used construct joint algorithm jointly estimates model parameters normalization constants. recent development models related algorithms examine models interpolated models improve performance. linear combination done either word-level sentence-level contrast log-linear combination differentiation. three schemes detailed tab.. models output sentence probabilities scheme applicable combining models models. dominant language models n-gram neural network models represent sentence probabilities terms conditionals. contrast trans-dimensional random ﬁeld recently introduced show superior performances whole sentence modeled random ﬁeld. paper examine models interpolated models obtain relative error rate reductions -gram english chinese speech recognition respectively log-linear combination. index terms language modeling random ﬁelds stochastic approximation language modeling involves determining joint probability words sentence. conditional approach dominant representing joint probability terms conditionals. examples include n-gram neural network recently trans-dimensional random ﬁeld introduced whole sentence modeled random ﬁeld. random ﬁeld approach avoids local normalization required conditional approach computationally efﬁcient computing sentence probabilities potential advantage able ﬂexibly integrate richer features. effective training algorithm using joint stochastic approximation transdimensional mixture sampling developed found models signiﬁcantly outperformed modiﬁed kneser-ney smoothed -gram relative reduction speech recognition word error rates performed slightly better recurrent neural network faster speed re-scoring n-best lists hypothesized sentences. knowledge result represents ﬁrst strong empirical evidence supporting power using whole-sentence random ﬁeld approach paper examine models interpolated models improve performance. speech recognition experiments conducted english chinese. three kinds interpolation schemes namely linear interpolation log-linear interpolation evaluated model combinations n-gram lms. loglinearly combining obtain relative error rate reductions -gram english chinese speech recognition respectively. table model interpolation schemes. combined score sentence conditional probabilities given history estimated lms; joint probabilities whole sentence estimated lms. interpolation weight tuned development set. table wers ppls wsj’ test data. feat denotes feature size denote different model combination schemes deﬁned tab.. trf* denotes features w+c+ws+cs+wsh+csh+tied. tied represents tied long-skip-bigram features introduced skip-bigrams skipping distances share parameter. section speech recognition -best list rescoring experiments conducted conﬁgured maximum length trfs equal maximum length training sentences. conﬁgurations tmax regularization constant used avoid over-ﬁtting. cores used parallelize algorithm. word error rates perplexities wsj’ test shown tab.. combining provides reduction. different schemes give close wers combining rnn. log-linear interpolation performs stable considering english chinese experiments english obtained indicates relative reductions compared result using best result combining respectively. section report results using large vocabulary mandarin speech recognition experiment. different evaluated rescoring -best list toshiba’s internal test oracle character error rate -best lists generated dnn-based acoustic model. corpus toshiba contains words. randomly select corpus development others training set. vocabulary contains words special token <unk>. used feedforward neural network trained cslm toolkit. number hidden units projection layer units models trained using feature w+c+ws+cs+cpw different numbers classes conﬁgurations tmax sample number increased improvements development observed ﬁnally cores used parallelize algorithm. cers ppls test shown tab.. results demonstrate signiﬁcantly outperform n-gram able match log-linear combination reduces cer. obtained indicates relative reductions compared result using best result combining respectively. currently attention research attracted using neural networks shown surpass classic n-gram lms. basic classes based recent extensions involve sumproduct networks deep recurrent neural networks feedforward sequential memory networks perplexity results reported studies. crucially matter form networks take various follow conditional approach thus suffer expensive softmax computations requirement local normalization. lots studies alleviate deﬁciency. initial efforts include using hierarchical output layer structure word clustering converting n-gram recently number studies make noise contrastive estimation build unnormalized variants trickily avoiding local normalization training heuristically ﬁxing normalizing term testing. contrast eliminate local normalization root thus much efﬁcient testing theoretical guarantee. empirically found average time costs re-ranking -best list sentence based respectively equally importantly evaluations paper also shown able perform good variety tasks. encouragingly random ﬁeld approach open door language modeling addition dominant conditional approach envisioned integrating richer features introducing hidden variables worthwhile future works. arisoy chen ramabhadran sethy converting neural network language models back-off language models efﬁcient decoding automatic speech recognition ieee/acm transactions audio speech language processing vol. gutmann hyv¨arinen noise-contrastive estimation unnormalized statistical models application natural image statistics journal machine learning research vol.", "year": 2016}