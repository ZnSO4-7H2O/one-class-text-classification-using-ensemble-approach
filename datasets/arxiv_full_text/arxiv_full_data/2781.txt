{"title": "Quantitative CBA: Small and Comprehensible Association Rule  Classification Models", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Quantitative CBA is a postprocessing algorithm for association rule classification algorithm CBA (Liu et al, 1998). QCBA uses original, undiscretized numerical attributes to optimize the discovered association rules, refining the boundaries of literals in the antecedent of the rules produced by CBA. Some rules as well as literals from the rules can consequently be removed, which makes the resulting classifier smaller. One-rule classification and crisp rules make CBA classification models possibly most comprehensible among all association rule classification algorithms. These viable properties are retained by QCBA. The postprocessing is conceptually fast, because it is performed on a relatively small number of rules that passed data coverage pruning in CBA. Benchmark of our QCBA approach on 22 UCI datasets shows average 53% decrease in the total size of the model as measured by the total number of conditions in all rules. Model accuracy remains on the same level as for CBA.", "text": "quantitative postprocessing algorithm association rule classiﬁcation algorithm qcba uses original undiscretized numerical attributes optimize discovered association rules reﬁning boundaries literals antecedent rules produced cba. rules well literals rules consequently removed makes resulting classiﬁer smaller. one-rule classiﬁcation crisp rules make classiﬁcation models possibly comprehensible among association rule classiﬁcation algorithms. viable properties retained qcba. postprocessing conceptually fast performed relatively small number rules passed data coverage pruning cba. benchmark qcba approach datasets shows average decrease total size model measured total number conditions rules. model accuracy remains level cba. current rule learning approaches divided categories depending learn rules process numerical attributes inductive rule learning typically based variation separate-and-conquer approach natively supporting numerical attributes association rule-based classiﬁcation approaches work nominal data. largely owing restriction association rule-based algorithms fast datasets many instances high dimensions. paper focus association rule-based classiﬁcation quantitative data. current mainstream approaches applied data numerical attributes discretized prior mining. disconnection discretization model building source ineﬃciencies resulting classiﬁer rule boundaries original continuous data resulting loss accuracy redundancies. recently several approaches support numerical data proposed produce fuzzy association rules containing fuzzy item sets deteriorate comprehensibility model. paper propose postprocessing algorithm classiﬁcation algorithm reverts original attribute space edit discovered association rules reﬁning scope literals antecedent rules. consequence individual rules data improves rendering rules attributes redundant. removed making resulting classiﬁer smaller. models ordered rule lists several properties make comprehensible one-rule classiﬁcation crisp rules. preprocessing retains these. postprocessing conceptually fast performed relatively small number rules passed data coverage pruning cba. paper organized follows. following related work section discusses association rule classiﬁcation quantitative association rule learning current limitations. section presents quantitative framework. section presents experimental validation. conclusions summarize contributions approach discuss ideas future work point publicly available implementation framework. separate-and-conquer strategy likely commonly approach rule learning. provides basis seminal ripper algorithm well state-of-the-art furia algorithm association rule learning algorithmically diﬀerent approach originally designed discover interesting patterns large sparse instance spaces yields conjunctive rules correspond high density regions data. unlike typical separate-and-conquer approach cardinal features need discretized prior execution association rule learning converted along nominal attributes binary-valued features. resulting rules correspond hypercubes boundaries aligned discretization breakpoints. impaired precision oﬀset computationally eﬃciency high-dimensional data allowing association rule learning succeed approaches fail association rule learning adopted also classiﬁcation several years after conception early ﬁrst association rule classiﬁcation algorithm dubbed introduced multiple follow-up algorithms providing marginal improvements classiﬁcation performance cmar ﬁrst step framework standard association rule learning algorithms apriori fp-growth used learn conjunctive classiﬁcation rules data. mining setup constrained algorithms adapted target class occur consequent rules. output association rule learning algorithms determined typically parameters minimum conﬁdence support thresholds. brieﬂy remind deﬁnition metrics. conﬁdence rule deﬁned number correctly classiﬁed objects i.e. matching rule antecedent well rule consequent number misclassiﬁed objects i.e. matching antecedent consequent. support rule deﬁned number objects simply main obstacles straightforward discovered association rules classiﬁer excessive number rules discovered even small datasets fact contradicting rules generated. algorithms contain rule pruning step signiﬁcantly reduces number rules deﬁne situation object matched multiple rules handled. qualitative review rule pruning algorithms used presented commonly used method according survey papers data coverage pruning. type pruning processes rules order strength removing transactions rule matches database. rule correctly cover least instance deleted data coverage pruning combined default rule pruning algorithm replaces rules current rule default rule inserted place would reduce number errors. default rule rule empty antecedent ensures query instance always classiﬁed even matched rule classiﬁer. eﬀect pruning size rule list reported presents evaluation datasets. illustrate eﬀect data coverage pruning algorithm average number rules dataset without pruning pruning average number rules reduced without eﬀectively impacting accuracy. instance assigned class consequent ﬁrst rule antecedent matching instance ordered list rules. advantage rule classiﬁcation easily understandable provides advantages applications eﬀort improve classiﬁer accuracy successors cpar combine multiple rules perform classiﬁcation. main beneﬁt using rule-based classiﬁer opposed state-of-the-art sub-symbolic method deep neural network comprehensibility rule-based model combined fast execution large sparse datasets accuracy comparable state-of-the-art black-box classiﬁcation models. individual algorithms meet aspirations diﬀerent degree. table presents comparison well known algorithms terms comprehensibility metrics accuracy performance. selected basis work since follows table produces comprehensible models successors maintaining high accuracy fast execution times. diﬀerence accuracy model accuracy state-of-the-art algorithms farc-hd small. terms accuracy outperformed farc-hd cpar however cpar times rules output less comprehensible multi-rule classiﬁcation. farc-hd outperforms terms accuracy even evolved version farc-hd-ovo however algorithm slower produces less comprehensible fuzzy rules. addition criteria table also advantage uses standard association rule learning ﬁrst step. makes work postprocessing output future-proof since performance improved replacing apriori another association rule learning algorithm fp-growth report faster problems. proposed qcba algorithm focuses support quantitative attributes association rule classiﬁcation. follow fuzzy approach because noted earlier results impaired comprehensibility model. knowledge prior work creating algorithms composed crisp rules support numerical attributes. however work learning standard association rules numerical data. note diﬀerence association rule mining exploratory data mining task aimed discovering interesting patterns data association rule classiﬁcation aims building understandable classiﬁcation models. table comparison association rule classiﬁers. single refers single rule classiﬁcation crisp whether rules comprising classiﬁer crisp det. whether algorithm deterministic random element genetic optimization assoc corresponds whether method based association rules rules time average accuracy average rule count average time across datasets reported indicates algorithm process datasets earlier proposed quantminer evolutionary algorithm optimizes multi-objective ﬁtness function combines support conﬁdence. essence quantminer number seed rules evolved using standard evolutionary operators mutation corresponds increase decrease lower/upper bound rule. nar-discovery takes diﬀerent stage approach. similarly qcba coarse association rules generated prediscretized data standard association rule generation algorithms ﬁrst stage. second stage coarse grained rule number reﬁned-rules generated using bins. granularity well coarse bins parameter algorithm. feature nar-discovery produces least order magnitude rules quantminer. comparison qcba framework nar-discovery quantminer summarized table additional justiﬁcations individual values table classiﬁcation models neither quantminer nar-discovery designed classiﬁcation deterministic quantminer evolutionary algorithm number rules many generated rules biggest issues facing association rule generation algorithms neither nar-discovery quantminer contain procedures limit number rules qcba contains several iterations rule pruning precision intervals quantminer precision intervals depends setting evolutionary process nar-discovery discretization setting qcba generates interval boundaries exactly corresponding values input continuous data. externally parameters nar-discovery requires granularity settings quantminer requires number parameters population size mutation crossover rate evolutionary process. qcba require externally parameters finally noted comparison described completely fair quantitative association rule learning algorithms since address diﬀerent task qcba. unlike qcba unsupervised algorithms class information available. exploited qcba among others perform rule pruning allows reduce number rules informed way. quantitative association rule classiﬁcation framework designed extension cba. classiﬁcation workﬂow involving qcba consists main components. ﬁrst component implementation. core framework second component post-processes discovered rules comprising classiﬁer phases. ﬁrst phase individual rules training data improved. increases coverage individual rules reduces length removing redundant attributes. second phase three types rule pruning performed reduce number rules. discretization numeric ﬁelds dataset. discretization technique used. typically used discretization based minimum description length principle selects number cut-points highest entropy gains. reﬁtting rules value grid. literals originally aligned borders discretized regions reﬁt ﬁner grid steps corresponding unique attribute values appearing training data. pruning optimized rule list. rules resorted since regions cover could change another iteration pruning performed remove rules made newly redundant data coverage pruning. rules extended match objects make rules redundant therefore data coverage pruning performed remove newly redundant rules. default rule pruning. rules current rule replaced default rule reduces number errors training data. note preserve rules qcba work with pruning skipped performed qcba tuned rule list. default rule overlap pruning. rules classify class default rule classiﬁer removed rule removed rule default rule would change classiﬁcation instances originally classiﬁed removed rule. algorithm depicts succession tuning steps qcba provides pointers algorithms described detail following subsections. please note interactive tutorial visually demonstrating qcba tuning steps referenced referenced https//github.com/kliegr/qcba. ﬁrst step qcba reﬁts boundaries rule ﬁner grid corresponds unique attribute values actually appearing data reﬁt operation inspired decision tree learning algorithm selects splitting points numerical attributes literal pruning step removes redundant literals rules. literal considered redundant removal decrease rule conﬁdence. literal pruning depicted algorithm corrcovbyl training instances covered literal distvalsl distinct values training instances attribute size {intervals restricted single value permitted} extension process depicted algorithm ranges literals body rule attempted enlarged. range literal increased literal boundary time. extension generally accepted improves rule conﬁdence. overcome local minima extension process provisionally accept drop conﬁdence compared seed rule. deﬁnition cardinal literal value range. direct extension literals derived higher direct extension lower direct extension. higher direct extension literal xk+i. lower direct extension literal hxi− xki. higher lower extension exist elements exists element none extensions exists empty. tension. extension meet conditions still conditionally accepted line conditional accept sets direction algorithm locks beam extension lines veriﬁes whether direction yield unconditional accept not. getbeamextension procedure depicted algorithm note extension accepted also extension cover additional training object i.e. conﬁdence remains unchanged well support. default extend accepted deteriorate rule conﬁdence corresponds minimprovement value increased user desires reduce number extensions. conditional accept process default extensions given direction tried values exhausted corresponds mincondimprovement user wish decrease value obtain faster failure conditional extension process improving performance datasets many distinct values previous steps aﬀected individual rules changing coverage. number rules reduced using adaptation cba’s data coverage default rule pruning also default rule rule list. refer second iteration postpruning. rule matched training data. rule correctly classify object discarded. otherwise rule kept. case objects matching rule discarded. data coverage pruning combined default rule pruning determines rule lowest number errors training data rules replaced default rule performs replace. default rule overlap pruning iterates rules classifying class default rule. rules overlap default rule terms coverage class assigned thus candidates pruning. removed removal change classiﬁcation instances training data/in entire instance space correctly classify rules default rule. consider versions drop transaction-based range-based. result ﬁrst iteration data coverage pruning rule list input qcba. obtained better results default rule pruning performed ﬁrst iteration since qcba left rules optimize. diﬀerent class) pruning conﬁrmed potentially clashing rules cover diﬀerent geometric regions range-based pruning thus guarantees solution generalizes beyond training data. potential disadvantage removes less rules since stronger transaction-based pruning. section present evaluation qcba framework number standard datasets. evaluation focuses comparison terms accuracy classiﬁer size runtime. results obtained using open source qcba implementations available https//cran.r-project.org/web/packages/arc/ https//github.com/kliegr/qcba. experiments directly replicated using evaluation framework published https//github.com/kliegr/arcbench. order verify correctness implementation last subsection devoted comparison results obtained reported authors. cutoﬀrule cutoﬀclass frequent class lowesttotalerror class cutoﬀclass| totalerrorswithoutdefault rules sort rules according criteria fig. defclass frequent class {data coverage pruning} rules covered instances matched antecedent corrcovered instances matched antecedent consequent covered {remove instances covered training data} corrcovered=∅ misclassiﬁed covered corrcovered totalerrorswithoutdefault totalerrorswithoutdefault misclassiﬁed defclass frequent class defaultruleerror class defclass| totalerrorwithdefault defaultruleerror totalerrorswithoutdefault totalerrorwithdefault <lowesttotalerror university california provides https//archive.ics.uci.edu publicly available datasets commonly used benchmarking machine learning algorithms. chose datasets perform evaluation. selection criteria least numerical predictor attribute dataset previously used evaluation symbolic learning algorithms following seminal papers details selected datasets given table several datasets come visual information processing signal processing domains second strongly represented domain medical datasets eleven datasets binary classiﬁcation problems nine datasets multinominal datasets ordinal class attribute datasets contain cardinal attributes needed pre-discretized qcba. numeric explanatory attributes three distinct values subject discretization using mdlp algorithm wrapped package. algorithms involved benchmark recba algorithm three hyperparameters minimum conﬁdence minimum support thresholds total number candidate rules. recommended minimum conﬁdence minimum support. experiments used thresholds. total number rules used however noted performance starts stabilize already around rules. according experiments virtually diﬀerence threshold apart higher computation time former therefore used also limited maximum number items itemset experiments performed using implementation package available cran github. preferably qcba obtain input model built without default rule pruning performed. variation reported separately described prior research. implementation adapted allow deactivation default rule pruning. table overview datasets involved benchmark. att. denotes number attributes inst. number instances miss. whether dataset contains missing observations. anneal australian autos breast-w colic credit-a credit-g diabetes glass heart-statlog hepatitis hypothyroid ionosphere iris labor letter lymph segment sonar spambase vehicle vowel nominal binary ordinal binary binary binary binary binary nominal binary binary nominal binary nominal ordinal nominal nominal nominal binary binary nominal nominal credit card applications riskiness second hand cars breast cancer horse colic credit approval credit risk diabetes types glass diagnosis heart disease hepatitis prognosis radar data types irises employer’s contribution health plan letter recognition lymphography domain image segment classiﬁcation determine object based sonar signal spam detection object type based silhouette classiﬁcation performance measured accuracy computed correct/n correct number correct predictions total number objects. results reported using fold cross validation macro averaging. average accuracy datasets reported indicative comparison measure. reliable comparison included won-tie-loss matrix compares classiﬁers reporting number datasets reference classiﬁer wins loses classiﬁers perform equally well. include p-value wilcoxon signed test recommended comparison classiﬁers multiple datasets authoritative work three metrics measure size model average antecedent length number rules model average number conditions model computed number rules times average antecedent length. include benchmark indicating much processing power postprocessing qcba requires. build times reported table computed average classiﬁer learning time models addition absolute time volatile across software hardware platforms include relative execution time baseline assigned score reported time includes discovery candidate association rules data coverage default rule pruning. summary results presented table includes baseline results seven diﬀerent conﬁgurations qcba. allows demonstrate eﬀect individual postprocessing steps comprising qcba. conﬁguration corresponds reﬁt tuning step performed conﬁguration reﬁt tuning step literal pruning etc. conﬁguration correspond full qcba diﬀerence whether transaction-based range-based default rule overlap pruning performed. qcba setup produces highest accuracy achieving maximum reduction size classiﬁer conﬁguration includes tuning steps closely followed excludes default overlap pruning. conﬁgurations average accuracy surpass concerns won-tie-loss metric datasets wins datasets draw dataset. p-value wilcoxon signed rank test indicates change won-tie-loss matrix signiﬁcantly diﬀerent compared neither qcba conﬁgurations. noted p-value close level signiﬁcance level. qcba conﬁgurations thus marginally improve results. conﬁguration number performs exactly equally well winning datasets loosing also datasets. models produced best-performing qcba conﬁguration smaller models reduction number rules average number conditions. reductions combined amount reduction model size terms total number conditions. reduction model size achieved transaction variant default overlap pruning reduces size model average conditions compared original model incurring drop average accuracy terms won-tie-loss record. results runtime reported last rows table seen reﬁt literal pruning trimming tuning take together roughly much time average learning model. computationally intensive operation extension. look median build times qcba prolongs execution factor discrepancy median average build times qcba explained several datasets qcba extension step takes excessive time complete increases average runtime leaves median time unaﬀected. extension particularly slow large number distinct values dataset. slowest datasets segment letter spambase. segment letter datasets contain various image metrics spambase word frequency attributes. datasets typical representatives cases interpretable machine learning models required. nevertheless evaluation runtime indicates computational optimization extension algorithm important areas work. oﬃcial implementation authors publicly available. used implementation obtain baseline results support central assertion marc reduces model size classiﬁers keeping accuracy unaﬀected. order verify implementation detailed results present table small variations individual datasets overall average accuracy implementation oﬃcial equal regarding number rules original average rules also exactly number baseline implementation. precise match results came surprise implementation include optional pessimistic pruning step used used ﬁrst phase candidate association rules generated. according results reported absence pessimistic pruning eﬀect classiﬁer accuracy congruent results. results also indicate pruning eﬀect number rules classiﬁer account eﬀects suggests could eﬀective technique reducing time required build model. conceptually association rule classiﬁcation approach building rule-based classiﬁers eﬃcient large sparse datasets traditionally used separate-and-conquer approaches possibility highlyoptimized frequent pattern mining algorithms generate initial rule list. presented framework ameliorates major drawbacks association rules adherence rules comprising classiﬁer multidimensional grid created discretization numerical attributes. ﬁrst association rule classiﬁcation approach still best rule-based classiﬁcation algorithms concerns balance comprehensibility model predictive power scalability. numerous enhancements proposed since seminal paper according review modiﬁcations succeeding association rule classiﬁcation approaches negatively aﬀect comprehensibility resulting rule-based model yielding none small improvement accuracy. contrast qcba reduces number rules datasets evaluation retaining favourable properties making models smaller thus comprehensible. accuracy resulting models unimpaired even improves datasets. mechanism qcba works largely based recovering information lost prediscretizing quantitative attributes. previous adaptations association rule classiﬁcation numerical data known author fuzzy approaches. example state-of-the-art farc-hd association rule classiﬁer outputs rules fuzzy regions makes rules less comprehensible crisp rules output cba. qcba author’s knowledge ﬁrst non-fuzzy association rule classiﬁcation algorithm supporting quantitative attributes. qcba reuses central concepts data coverage pruning introduces several enhancements pruning steps similar algorithms used symbolic learning algorithms context association rule learning classiﬁcation author’s knowledge novel. qcba design avoids introduction data-speciﬁc thresholds user optimize constitutes certain advancement previous quantitative association rule learning approaches quantminer nar-discovery. qcba does however contain several parameters changed speed model building. imminent future work improvement proposed algorithms terms scalability. include incorporation pessimistic pruning using version data coverage pruning proposed instead version optimization extension algorithm according results runtime benchmark biggest bottleneck datasets. implementation qcba algorithm environment code replication results paper interactive tutorial additional resources including available open license http//github.com/kliegr/qcba.", "year": 2017}