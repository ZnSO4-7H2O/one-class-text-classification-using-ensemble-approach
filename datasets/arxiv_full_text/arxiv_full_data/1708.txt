{"title": "Improved Bayesian Logistic Supervised Topic Models with Data  Augmentation", "tag": ["cs.LG", "cs.CL", "stat.AP", "stat.ML"], "abstract": "Supervised topic models with a logistic likelihood have two issues that potentially limit their practical use: 1) response variables are usually over-weighted by document word counts; and 2) existing variational inference methods make strict mean-field assumptions. We address these issues by: 1) introducing a regularization constant to better balance the two parts based on an optimization formulation of Bayesian inference; and 2) developing a simple Gibbs sampling algorithm by introducing auxiliary Polya-Gamma variables and collapsing out Dirichlet variables. Our augment-and-collapse sampling algorithm has analytical forms of each conditional distribution without making any restricting assumptions and can be easily parallelized. Empirical results demonstrate significant improvements on prediction performance and time efficiency.", "text": "supervised topic models logistic likelihood issues potentially limit practical response variables usually over-weighted document word counts; existing variational inference methods make strict mean-ﬁeld assumptions. address issues introducing regularization constant better balance parts based optimization formulation bayesian inference; developing simple gibbs sampling algorithm introducing auxiliary polya-gamma variables collapsing dirichlet variables. augment-and-collapse sampling algorithm analytical forms conditional distribution without making restricting assumptions easily parallelized. empirical results demonstrate signiﬁcant improvements prediction performance time efﬁciency. widely latent models improve predictive power deﬁne likelihood model widely available document-level response variables addition likelihood model document words. example logistic likelihood model commonly used binary multinomial responses. imposing priors posterior inference done bayes’ rule. though powerful issue could limit existing logistic supervised models treat document-level response variable additional word normalized likelihood model. although special treatment carried deﬁning likelihood single response variable normally much smaller scale likelihood usually tens hundreds words document. noted observed experiments model imbalance could result weak inﬂuence response variables topic representations thus non-satisfactory prediction performance. another difﬁculty arises dealing categorical response variables commonly used normal priors longer conjugate logistic likelihood thus lead hard inference problems. existing approaches rely variational approximation techniques normally make strict mean-ﬁeld assumptions. address issues present improvements. first present general framework bayesian logistic supervised topic models regularization parameter better balance response variables words. technically instead standard bayesian inference bayes’ rule requires normalized likelihood model propose regularized bayesian inference solving optimization problem posterior regularization deﬁned expectation logistic loss surrogate loss expected misclassiﬁcation error; regularization parameter introduced balance surrogate classiﬁcation loss word likelihood. general formulation subsumes standard slda special case. second solve intractable posterior inference problem generalized bayesian logistic supervised topic models present simple gibbs sampling algorithm exploring ideas data augmentation speciﬁcally extend polson’s method bayesian logistic regression generalized logistic supervised topic models much challenging presence non-trivial latent variables. technically introduce polya-gamma variables document reformulate generalized logistic pseudo-likelihood model scale mixture mixture component conditionally normal classiﬁer parameters. then develop simple efﬁcient gibbs sampling algorithms analytic conditional distributions without metropolishastings accept/reject steps. bayesian models also explore conjugacy dirichlet-multinomial prior-likelihood pairs collapse dirichlet variables collapsed gibbs sampling better mixing rates finally empirical results real data sets demonstrate signiﬁcant improvements time efﬁciency. classiﬁcation performance also signiﬁcantly improved using appropriate regularization parameters. also provide parallel implementation graphlab shows great promise preliminary studies. paper structured follows. sec. introduces logistic supervised topic models general optimization problem. sec. presents gibbs sampling algorithms data augmentation. sec. presents experiments. sec. concludes. consider binary classiﬁcation training response variset able takes values output space logistic supervised topic model consists parts model describing words {wd}d {wdn}nd denote words within document logistic classiﬁer considering below introsupervising signal {yd}d duce turn. dirichlet distribution; mult multinomial distribution; φzdn denotes topic selected non-zero entry zdn. fully-bayesian topics random samples dirichlet prior dir. denote topic asd= signments document {zd}d denote topic assignments {θd}d mixing proportions entire corpus. infers posterior distribution joint distribution deﬁned model. noticed posterior distribution bayes’ rule equivalent solution information theoretical optimization problem logistic classiﬁer consider binary supervising information logistic supervised topic model builds logistic classiﬁer using topic representations input features posterior distribution makes predictions fact choice motivated observation logistic loss widely used convex surrogate loss misclassiﬁcation loss task fully observed binary classiﬁcation. also note logistic classiﬁer likelihood coupled sharing latent topic assignments strong coupling makes possible learn posterior distribution describe observed words well make accurate predictions. un-normalized shall un-normalization affect subsequent inference. then generalized inference problem logistic supervised topic models written standard bayesian inference form normalization constant make distribution. model reduces standard slda practice imbalance issue response variable usually dominated words. imbalance formulation logistic supervised topic models instance regularized bayesian inference provides direct comparison topic model max-margin form optimization problems. difference lies posterior regularization medlda uses hinge loss expected classiﬁer logistic supervised topic model uses expected log-logistic loss. another max-margin model adopts expected hinge loss posterior regularization. shall experiments using appropriate logistic supervised topic models achieve comparable performance max-margin methods. note relationship logistic loss hinge loss discussed extensively various settings presence latent variables poses additional challenges carrying formal theoretical analysis surrogate losses topic model setting. variational approximation algorithms commonly used normal prior nonconjugate logistic likelihood makes posterior inference hard. moreover latent variables make inference problem harder bayesian logistic regression models previous algorithms solve problem rely variational approximation techniques. easy show variational method coordinate descent algorithm solve problem additional fully-factorized constraint variational approximation expectation log-logistic likelihood intractable compute directly. nonbayesian treatment unknown parameters results algorithm still needs make strict mean-ﬁeld assumptions together variational bound expectation log-logistic likelihood. paper consider full bayesian treatment principally consider prior distributions although gibbs sampling infer complete posterior distribution thus ignoring mixing rate would slow large sample space. effectively improve mixing rates integrate intermediate variables build markov chain whose equilibrium distribution marginal distribution propose collapsed gibbs sampling successfully used model collapsed posterior distribution qdim number times term assigned topic numwhole corpus times terms associated topic within d-th document then conditional distributions used collapsed gibbs sampling follows. posterior mean covariance easily draw sample k-dimensional multivariate gaussian distribution. inverse robustly done using cholesky decomposition procedure. since normally large inversion done efﬁciently. conditional distribution since logistic pseudo-likelihood conjugate normal priors easy derive sampling algorithms directly. instead develop algorithms introducing auxiliary variables lead scale mixture gaussian components analytic conditional distributions automatical bayesian inference without accept/reject ratio. algorithm represents ﬁrst attempt extend polson’s approach deal highly non-trivial bayesian latent variable models. ﬁrst introduce polya-gamma variables. pdλd polya-gamma variable parameters result indicates posterior distribution generalized bayesian logistic supervised topic models i.e. expressed marginal higher dimensional distribution includes augmented variables complete posterior distribution indicates term excluded corresponding document topic; discriminant function value without word ﬁrst term model observed word counts second term supervising signal equality achieved using construction deﬁnition general class exponential tilting density draw samples polya-gamma distribution adopt efﬁcient method proposed draws samples drawing samples closely related exponentially tilted jacobi distribution. conditional distributions construct markov chain iteratively draws samples using using using initial condition. experiments initially randomly draw uniform distribution. training markov chain iterations outlined algorithm then draw sample ﬁnal classiﬁer make predictions testing data. shall markov chain converges stable prediction performance burn-in iterations. apply classiﬁer testing data need infer topic assignments. take approach uses point estimate topics training data makes prediction based them. speciﬁcally estimate replace probability distribution gibbs sampler estimate using samples ˆφkt then given testing document infer latent components using ˆφkwn times terms document assigned topic n-th term excluded. present empirical results sensitivity analysis demonstrate efﬁciency prediction performance generalized logistic supervised topic models newsgroups data contains postings within news groups. follow setting remove standard list stop words binary multi-class classiﬁcation. experiments standard normal prior symmetric dirichlet priors vector entries setting report average performance standard deviation randomly initialized runs. setting following task distinguish postings newsgroup alt.atheism group training contains talk.religion.misc. documents test contains documents. compare generalized logistic supervised using gibbs sampling various competislda using tors standard variational mean-ﬁeld methods medlda model using variational mean-ﬁeld methods medlda model using collapsed gibbs sampling algorithms also include unsupervised using collapsed gibbs sampling baseline denoted glda. glda learn binary linear topic representations using svmlight results disclda linear bag-of-words features reported gslda compare versions standard slda slda well-tuned value. distinguish denote latter gslda+. gslda+ gslda gslda+. shall gslda insensitive wide range. fig. shows performance different methods various numbers topics. accuracy draw conclusions without making restricting assumptions posterior distributions gslda achieves higher accuracy vslda uses strict variational mean-ﬁeld approximation; using regularization constant improve inﬂuence supervision information gslda+ achieves much better classiﬁcation results fact comparable medlda models since similar mechanism improve inﬂuence supervision tuning regularization constant. fact glda+svm performs better standard gslda reason since part glda+svm well capture supervision information learn classiﬁer good prediction standard slda can’t well-balance inﬂuence supervision. contrast well-balanced gslda+ model successfully outperforms two-stage approach glda+svm performing topic discovery prediction jointly. efﬁcient e.g. orders magnitudes faster vslda order magnitude faster vmedlda. testing time gslda gslda+ comparable gmedlda unsupervised glda faster variational vmedlda vslda especially large. perform multi-class classiﬁcation data categories. multiclass classiﬁcation possible extension multinomial logistic regression model categorical variables using topic representations input features. however nontrivial develop gibbs sampling algorithm using similar data augmentation idea presence latent variables nonlinearity soft-max function. fact harder multinomial bayesian logistic regression done coordinate strategy here apply binary gslda multi-class classiﬁcation following one-vs-all strategy shown effective provide preliminary analysis. namely learn binary gslda models aggregate predictions taking likely ones ﬁnal predictions. evaluate versions gslda standard gslda improved gslda+ well-tuned value. since gslda also insensitive multi-class task gslda gslda+ gslda+. number burn-in sufﬁciently large stable results shall see. fig. shows accuracy training time. that using gibbs sampling withrestricting assumptions gslda performs better variational vslda uses strict mean-ﬁeld approximation; imbalplied several machine learning algorithms including gibbs sampling choose preliminary attempt parallelize gibbs sampling algorithm. systematical investigation parallel computation various architectures interesting beyond scope paper. task since coupling among binary gslda classiﬁers learn parallel. suggests efﬁcient hybrid multi-core/multi-machine implementation avoid time consumption namely experiments cluster nodes node equipped -core cpus node responsible learning binary gslda classiﬁer parallel implementation -cores. binary gslda model construct bipartite graph connecting train documents corresponding terms. graph works follows edges contain token counts topic assignments; vertices contain individual topic counts augmented variables global topic counts aggregated vertices periodically topic assignments sampled asynchronously phases. started sampling signaling propagate graph. thing note since cannot directly measure number iterations asynchronous model estimate total number topic samplings aggregated periodically divided number tokens. denote parallel models parallel-gslda parallel-gslda+ fig. parallel gslda models orders magnitudes faster sequential counterpart models promising. also prediction performance sacriﬁced shall fig. ance single supervision large word counts gslda doesn’t outperform decoupled approach glda+svm; increase value regularization constant supervision information better captured infer predictive topic representations gslda+ performs much better gslda. fact gslda+ even better medlda uses mean-ﬁeld approximation comparable medlda using collapsed gibbs sampling. finally note improvement accuracy might different strategies building multi-class classiﬁers. given performance gain binary task believe gibbs sampling algorithm without factorization assumptions main factor improved performance. training time gslda models times faster variational vslda. table shows detail percentages training time spent sampling step gslda+. that sampling global variables efﬁcient sampling local variables much expensive; sampling relatively stable increases sampling takes time becomes larger. good news gibbs sampling algorithm easily parallelized speedup sampling local variables following similar architectures lda. parallel implementation graphlab graph-based programming framework parallel computing provides high-level abstraction parallel tasks expressing data dependencies distributed graph. graphlab implements model data required compute vertex gathered along neighboring components modiﬁcation vertex trigger adjacent components recompute values. since successfully apperformance gslda binary classiﬁcation task different values. wide range e.g. performance quite stable three values. standard slda model i.e. training accuracy test accuracy indicates slda doesn’t supervision data well. becomes larger training accuracy gets higher doesn’t seem over-ﬁt generalization performance stable. experiments multiclass classiﬁcation similar observations previous experiments. dirichlet prior fig. shows performance gslda binary task different values. report cases performance quite stable wide range values e.g. also noted change affect training time much. present improvements bayesian logistic supervised topic models namely general formulation introducing regularization parameter avoid model imbalance highly efﬁcient gibbs sampling algorithm without restricting assumptions posterior distributions exploring idea data augmentation. algorithm also parallelized. empirical results binary multi-class classiﬁcation demonstrate classiﬁcation. models built random topic assignments. classiﬁcation performance increases fast converges stable optimum burn-in steps. training time increases linearly general using burn-in steps. moreover training time increases linearly increases. previous experiments fig. shows performance gslda+ parallel implementation multi-class classiﬁcation different burn-in steps. number burn-in steps larger performance gslda+ quite stable. again log-log scale since slopes lines fig. close constant training time grows linearly number burn-in steps increases. even burn-in steps training time still competitive compared variational vslda. parallel-gslda+ using graphlab training consistently orders magnitudes faster. meanwhile classiﬁcation performance also comparable gslda+ number burn-in steps larger previous experiments gslda+ parallel-gslda+. signiﬁcant improvements existing logistic supervised topic models. preliminary results graphlab shown promise parallelizing gibbs sampling algorithm. carry e.g. using architectures make sampling algorithm highly scalable deal massive data corpora. moreover data augmentation technique applied deal types response variables count data negative-binomial likelihood work supported national foundation projects tsinghua initiative scientiﬁc research program tsinghua national laboratory information science technology basic research plan young faculties tsinghua university.", "year": 2013}