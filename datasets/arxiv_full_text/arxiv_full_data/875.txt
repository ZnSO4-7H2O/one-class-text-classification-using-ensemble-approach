{"title": "The Variational Gaussian Process", "tag": ["stat.ML", "cs.LG", "cs.NE", "stat.CO"], "abstract": "Variational inference is a powerful tool for approximate inference, and it has been recently applied for representation learning with deep generative models. We develop the variational Gaussian process (VGP), a Bayesian nonparametric variational family, which adapts its shape to match complex posterior distributions. The VGP generates approximate posterior samples by generating latent inputs and warping them through random non-linear mappings; the distribution over random mappings is learned during inference, enabling the transformed outputs to adapt to varying complexity. We prove a universal approximation theorem for the VGP, demonstrating its representative power for learning any model. For inference we present a variational objective inspired by auto-encoders and perform black box inference over a wide class of models. The VGP achieves new state-of-the-art results for unsupervised learning, inferring models such as the deep latent Gaussian model and the recently proposed DRAW.", "text": "variational inference powerful tool approximate inference recently applied representation learning deep generative models. develop variational gaussian process bayesian nonparametric variational family adapts shape match complex posterior distributions. generates approximate posterior samples generating latent inputs warping random non-linear mappings; distribution random mappings learned inference enabling transformed outputs adapt varying complexity. prove universal approximation theorem demonstrating representative power learning model. inference present variational objective inspired auto-encoders perform black inference wide class models. achieves state-of-the-art results unsupervised learning inferring models deep latent gaussian model recently proposed draw. variational inference powerful tool approximate posterior inference. idea posit family distributions latent variables member family closest posterior. originally developed variational inference enjoyed renewed interest around developing scalable optimization large datasets deriving generic strategies easily ﬁtting many models applying neural networks ﬂexible parametric family approximations research particularly successful computing deep bayesian models require inference complex posterior distribution classical variational inference typically uses mean-ﬁeld family latent variable independent governed variational distribution. convenient strong independence limits learning deep representations data. newer research aims toward richer families allow dependencies among latent variables. introduce dependence consider variational family model latent variables variational models naturally extend bayesian hierarchies retain mean-ﬁeld likelihood introduce dependence variational latent variables. paper develop powerful variational model—the variational gaussian process bayesian nonparametric variational model; complexity grows efﬁciently towards distribution adapting inference problem hand. highlight three main contributions work prove universal approximation theorem certain conditions capture continuous posterior distribution—it variational family speciﬁed expressive needed. derive efﬁcient stochastic optimization algorithm variational inference vgp. algorithm used wide class models. inference black variational method study standard benchmarks unsupervised learning applying perform inference deep latent gaussian models draw latent attention model. models report best results date. technical summary. generative models hypothesize distribution observations latent variables variational inference posits family latent variables tries variational parameters closest divergence posterior. variational model might contain variational latent variables; implicitly marginalized variational family ﬂexible variational model. draw inputs simple distribution warps inputs non-linear mapping uses output mapping govern distribution latent variables non-linear mapping random variable constructed gaussian process. inspired ideas gaussian process latent variable model gaussian process regression variational parameters kernel parameters gaussian process variational data input-output pairs. variational data crucial anchors non-linear mappings given inputs outputs. parameters learns complex representations. finally given data stochastic optimization variational parameters minimize divergence model posterior. variational models introduce latent variables variational family providing rich construction posterior approximation introduce variational gaussian process bayesian nonparametric variational model based gaussian process. gaussian process provides class latent variables lets capture downstream distributions varying complexity. variational models denote posterior distribution latent variables conditioned data family distributions parameterized variational inference seeks minimize divergence equivalent maximizing evidence lower bound elbo written expected likelihood data divergence variational distribution prior traditionally variational inference considers tractable family distributions analytic forms also known mean-ﬁeld family. mean-ﬁeld families lead efﬁcient computation limit expressiveness approximation. variational family distributions interpreted model latent variables made richer introducing latent variables. hierarchical variational models consider distributions speciﬁed variational prior mean-ﬁeld parameters factorized figure graphical model variational gaussian process. generates samples latent variables evaluating random non-linear mappings latent inputs drawing mean-ﬁeld samples parameterized mapping. latent variables follow posterior distribution generative model conditioned data governed prior hyperparameters hierarchical variational models richer classical variational families—their expressiveness determined complexity prior many expressive variational approximations viewed construct review gaussian process consider data source-target pairs source covariates paired multidimensional target learn function source-target pairs unknown. function decouple regression estimates functional form placing prior parameters bayesian nonparametric variational model admits arbitrary structures match posterior distributions. generates generating latent inputs warping random non-linear mappings using warped inputs parameters mean-ﬁeld distribution. random mappings drawn conditional variational data variational parameters. show enables samples mean-ﬁeld follow arbitrarily complex posteriors. figure displays graphical model vgp. here represents variational data comprising input-output pairs parameters variational distribution. marginalizing latent inputs non-linear mappings variational model forms inﬁnite ensemble mean-ﬁeld distributions. meanﬁeld distribution given ﬁrst term integrand above. conditional ﬁxed function input outputs mean-ﬁeld’s parameters. form hierarchical variational model places continuous bayesian nonparametric prior mean-ﬁeld parameters. unlike mean-ﬁeld capture correlation latent variables. reason evaluates independent draws latent input induces correlation outputs mean-ﬁeld parameters thus also correlation latent variables. further ﬂexible. complex non-linear mappings drawn allow capture complex discrete continuous posteriors. emphasize needs variational data. unlike typical regression observed data available learn distribution non-linear mappings latent variables thus \"data\" variational parameters appear conditional distribution eq.. anchor random non-linear mappings certain input-ouput pairs. optimizing learned variational data enables ﬁnds distribution latent variables closely follows posterior. understand capacity representing complex posterior distributions analyze role gaussian process. simplicity suppose latent variables real-valued treats output function draws posterior samples. consider optimal function transformation draw calculate resulting distribution posterior distribution. explicit construction exists dimension latent input equal number latent variables. denote inverse posterior standard normal cdf. using techniques common copula literature optimal function imagine generating samples using function. latent input standard normal applies probability integral transform squashes output uniformly distributed inverse posterior transforms uniform random variables follow posterior. function produces exact posterior samples. random function interpolates values variational data optimized minimize divergence. thus inference distribution learns concentrate around optimal function. perspective provides intuition behind following result. theorem denote variational gaussian process. consider posterior distribution ﬁnite number latent variables continuous quantile function exists sequence parameters figure sequence domain mappings inference variational latent variable space posterior latent variable space data space perform variational inference posterior space auxiliary inference variational space. appendix proof. theorem states posterior distribution strictly positive density represented vgp. thus ﬂexible model learning posterior distributions. auxiliary model various versions objective considered literature recently revisited salimans ranganath perform variational inference posterior latent variable space minimizing klp) learn variational model; occur perform auxiliary inference variational latent variable space minimizing klr) learn auxiliary model. figure unlike previous approaches rewrite variational objective connect divergences taken tractable distributions autoencoder parlance maximize expected negative reconstruction error regularized terms expected divergence variational model original model’s prior expected divergence auxiliary model variational model’s prior. simply nested instantiation variational auto-encoder bound divergence inference model prior taken regularizers posterior variational spaces. interpretation justiﬁes previously proposed bound variational models; shall also enables lower variance gradients stochastic optimization. inference network provide ﬂexible parameterization approximating distributions used helmholtz machines deep boltzmann machines variational auto-encoders replaces local variational parameters global parameters coming neural network. latent variables inference network speciﬁes neural network takes input local variational parameters output. amortizes inference deﬁning global parameters. formally output mappings parameters respectively. write output distributions emphasize mappings parameterization variational model auxiliary model local variational parameters variational data auxiliary model speciﬁed fully factorized gaussian local variational parameters sampling variational model evaluating noisy gradients. first reduce variance stochastic gradients analytically deriving tractable expectations. divergence commonly used reduce variance traditional variational auto-encoders analytic deep generative models deep latent gaussian model deep recurrent attentive writer divergence analytic distributions gaussian. difference simply difference gaussian densities. appendix details. derive black gradients ﬁrst reparameterize separating noise generation samples parameters generative process easily enables reparameterization latent inputs transformation kξsk− equivalent evaluating random mapping suppose mean-ﬁeld also reparameterizable function whose output two-level reparameterization equivalent generative process outlined section enables gradients move inside expectations backpropagate nested reparameterization. thus take unbiased stochastic gradients exhibit variance analytic terms reparameterization. gradients derived appendix including case ﬁrst analytically intractable. outline method algorithm massive data apply subsampling gradients model log-likelihood employ convenient differentiation tools stan theano non-differentiable latent variables mean-ﬁeld distributions without efﬁcient reparameterizations apply black gradient estimator ranganath take gradients inner expectation. kernel hyperparameters ﬁxed across data points. note also unique auto-encoder approaches inference network take input avoids explicit speciﬁcation conditional distribution difﬁcult model. idea ﬁrst suggested ranganath computational storage complexity algorithm complexity number latent variables size variational data number layers neural networks average hidden layer size. particular algorithm linear number latent variables competitive variational inference methods. number variational auxiliary parameters complexity; complexity comes storing kernel hyperparameters neural network parameters. unlike literature require rank constraints inducing variables scalable computation variational data serve similar purpose inducing variables reduce rank kernel matrix; variational data directly determine kernel matrix thus kernel matrix ﬁxed. although haven’t found necessary practice appendix scaling size variational data. recently interest applying parametric transformations approximate inference. parametric transformations random variables induce density transformed space jacobian determinant accounts transformation warps unit volumes. kucukelbir consider viewpoint automating inference posit transformation standard normal possibly constrained latent variable space. general however calculating jacobian determinant incurs costly complexity cubic number latent variables. dinh consider volume-preserving transformations avoid calculating jacobian determinants. salimans consider volume-preserving transformations deﬁned markov transition operators. rezende mohamed consider slightly broader class parametric transformations jacobian determinants complexity. instead specifying parametric class mappings posits bayesian nonparametric prior continuous mappings. recover certain class parametric transformations using kernels induce prior class. context inﬁnitely wide feedforward network warps latent inputs mean-ﬁeld parameters. thus offers complete ﬂexibility space mappings—there restrictions invertibility linear complexity—and fully bayesian. further hierarchical variational model using variational prior mean-ﬁeld parameters enables inference discrete continuous latent variable models. addition ﬂexibility parametric methods computationally efﬁcient. parametric methods must consider transformations jacobian determinants complexity. restricts ﬂexibility mapping therefore ﬂexibility variational model comparison distribution outputs using prior require jacobian determinants instead requires auxiliary inference inferring variational latent variables further unlike discrete bayesian nonparametric priors inﬁnite mixture mean-ﬁeld distributions enables black inference lower variance gradients—it applies location-scale transform reparameterization analytically tractable terms. transformations convert samples tractable distribution posterior classic technique bayesian inference. ﬁrst studied monte carlo methods core development methods path sampling annealed importance sampling sequential monte carlo methods recast specifying discretized mapping times draws tractable distribution outputs samples outputs exact samples following posterior. applying sequence various forms transformation bridges tractable distribution posterior. specifying good transformation—termed schedule literature—is crucial efﬁciency methods. rather specify explicitly adaptively learns transformation avoids discretization. limiting various ways recovers well-known probability models variational approximations. speciﬁcally recover discrete mixture mean-ﬁeld distributions also recover form factor analysis variational space. mathematical details appendix following standard benchmarks variational inference deep learning learn generative models images. particular learn deep latent gaussian model layered hierarchy gaussian random variables following neural network architecures recently proposed deep recurrent attentive writer latent attention model iteratively constructs complex images using recurrent architecture sequence variational auto-encoders learning rate apply version rmsprop scale value decaying schedule /t/+\u0001 size variational data across experiments latent input dimension equal number latent variables. binarized mnist data consists pixel images binary-valued outcomes. training dlgm apply stochastic layers random variables random variables respectively in-between stochastic layer deterministic layer units using tanh nonlinearities. apply mean-ﬁeld gaussian distributions stochastic layers bernoulli likelihood. train learn dlgm cases stochastic layer stochastic layers. draw augment mean-ﬁeld gaussian distribution originally used generate latent samples time step places complex variational prior parameters. encoding recurrent neural network outputs variational data well mean-ﬁeld gaussian parameters architecture hyperparameters gregor training evaluate test likelihood lower bounds true value. table reports approximations lower bounds various methods. achieves highest known results log-likelihood using draw reporting value compared original highest also achieves highest known results among class non-structure exploiting models using dlgm value compared previous best reported burda demonstration vgp’s complexity learning representations also examine sketch data consists human sketches equally distributed object categories. partition training examples test examples. architecture draw read window write attention window glimpses—these values selected using coarse grid search choosing lead best training likelihood. inference original auto-encoder version well augmented version vgp. table draw achieves signiﬁcantly better lower bound performing better original version seen state-of-the-art success many computer vision tasks. moreover model inferred using able generate complex images original version—it performs better maintains higher visual ﬁdelity. present variational gaussian process variational model adapts shape match complex posterior distributions. draws samples tractable distribution posits bayesian nonparametric prior transformations tractable distribution meanﬁeld parameters. learns transformations space continuous mappings—it universal approximator ﬁnds good posterior approximations optimization. future work explored application monte carlo methods efﬁcient proposal distribution importance sampling sequential monte carlo. important avenue research also characterize local optima inherent objective function. analysis improve understanding limits optimization procedure thus limits variational inference. thank david duvenaud kucukelbir ryan giordano anonymous reviewers helpful comments. work supported iis- iis- iis- n--- darpa fa--- n--c- facebook adobe amazon seibel john templeton foundations. bergstra james breuleux olivier bastien frédéric lamblin pascal pascanu razvan desjardins guillaume turian joseph warde-farley david bengio yoshua. theano math expression compiler. proceedings python scientiﬁc computing conference bishop christopher lawrence neil jordan michael jaakkola tommi. approximating posterior distributions belief networks using mixtures. neural information processing systems carpenter hoffman matthew brubaker marcus daniel peter betancourt michael. stan math library reverse-mode automatic differentiation c++. arxiv preprint arxiv. cunningham john shenoy krishna sahani maneesh. fast gaussian process methods point process intensity estimation. international conference machine learning. gregor karol danihelka graves alex rezende danilo jimenez wierstra daan. draw recurrent neural network image generation. international conference machine learning rezende danilo jimenez mohamed shakir wierstra daan. stochastic backpropagation approximate inference deep generative models. international conference machine learning analyze special cases limiting generative process various ways recover well-known models. provides intuition behind vgp’s complexity. section show many recently proposed models also viewed special cases vgp. special case mixture mean-ﬁeld distributions without kernel. discrete mixture mean-ﬁeld distributions classically studied variational model dependencies latent variables. instead mapping interpolates inputs variational data suppose simply performs nearest-neighbors latent input ξ—selecting output tied nearest variational input induces multinomial distribution outputs samples variational outputs’ mean-ﬁeld parameters. thus prior interpolates inputs seen kernel density smoothing nearest-neighbor function. special case variational factor analysis linear kernel variational data. maximum likelihood estimate factor analysis maximum posteriori estimate formulation. generally non-linear kernel induces non-linear dependence learning kernel hyperparameters thus learns capturing variation latent embedding proof theorem theorem denote variational gaussian process. consider posterior distribution ﬁnite number latent variables continuous quantile function exists sequence parameters formally given variational input-output pairs nearest-neighbor function deﬁned output’s distribution multinomial probabilities proportional areas partitioned nearest-neighbor space. size latent input equivalent number latent variables furthermore simplicity assume drawn uniformly d-dimensional hypercube. explained section denote inverse posterior cumulative distribution function optimal denoted deﬁne points deﬁne d-dimensional product containing pairs element denote mapping conditioned dataset random mapping satisﬁes noise free prediction property gaussian processes continuity converges rate convergence ﬁnite sizes variational data studied posterior contraction rates random covariates additional assumption using stronger continuity conditions posterior quantile matern covariance functions required theory applicable variational setting. divergence mean-ﬁeld model prior analytically tractable certain popular models. example deep latent gaussian model draw mean-ﬁeld distribution model prior gaussian leading analytic term gaussian random variables dimension consider second term. recall specify auxiliary model fully factorized gaussian rc+d rc+d. further variational priors deﬁned gaussian. therefore also divergence gaussian distributed random variables. similarly simply difference gaussian densities. second expression simple compute backpropagate gradients. assume terms analytically written appendix gradients propagated similarly computational graph. practice need careful expectations gradients functions written taken care automatic differentiation tools. also derive gradients general variational bound eq.—it assumes ﬁrst term measuring divergence prior necessarily tractable. following reparameterizations described section variational objective rewritten massive sizes variational data required e.g. cubic complexity inversion matrix becomes bottleneck computation scale further. consider ﬁxing variational inputs grid. stationary kernels allows exploit toeplitz structure fast matrix inversion. particular embed toeplitz matrix circulant matrix apply conjugate gradient combined fast fourier transforms order compute inverse-matrix vector products computation storage product kernels exploit kronecker structure allow fast matrix inversion operations storage number kernel products kernel speciﬁcally leads complexity linear", "year": 2015}