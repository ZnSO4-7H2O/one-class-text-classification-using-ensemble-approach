{"title": "Faster Coordinate Descent via Adaptive Importance Sampling", "tag": ["cs.LG", "cs.CV", "math.OC", "stat.CO", "stat.ML", "G.1.6"], "abstract": "Coordinate descent methods employ random partial updates of decision variables in order to solve huge-scale convex optimization problems. In this work, we introduce new adaptive rules for the random selection of their updates. By adaptive, we mean that our selection rules are based on the dual residual or the primal-dual gap estimates and can change at each iteration. We theoretically characterize the performance of our selection rules and demonstrate improvements over the state-of-the-art, and extend our theory and algorithms to general convex objectives. Numerical evidence with hinge-loss support vector machines and Lasso confirm that the practice follows the theory.", "text": "coordinate descent methods employ random partial updates decision variables order solve huge-scale convex optimization problems. work introduce adaptive rules random selection updates. adaptive mean selection rules based dual residual primal-dual estimates change iteration. theoretically characterize performance selection rules demonstrate improvements stateof-the-art extend theory algorithms general convex objectives. numerical evidence hinge-loss support vector machines lasso conﬁrm practice follows theory. work show surpass existing convergence rates exploiting adaptive sampling strategies change sampling probability distribution iteration. purpose adopt primal-dual framework d¨unner contast however also handle convex optimization problems general convex regularizers without assuming strong convexity regularizer. particular consider adaptive coordinate-wise duality based sampling. hence work viewed natural continuation work csiba authors introduce adaptive version sdca smoothed hingeloss support vector machine however work generalizes gap-based adaptive criterion nontrivial broader convex optimization template following form coordinate descent methods rely random partial updates decision variables scalability. indeed space computational eﬃciency well ease implementation methods state-of-the-art wide selection standard machine learning signal processing applications basic coordinate descent methods sample active optimization uniformly random stochastic dual coordinate cent variants however recent results suggest employing appropriately deﬁned non-uniform ﬁxed sampling strategy convergence improved theory well practice template problem class includes smoothed hinge-loss also lasso ridge regression original hingeloss logistic regression etc. result theoretical results adaptive sampling also recover existing results ﬁxed non-uniform uniform sampling special cases. outline section provides basic theoretical preliminaries. section describes theoretical results introduces sampling schemes. section discusses application theory machine learning compares computational complexity proposed sampling methods. section provides numerical evidence methods. section discusses contributions light existing work. choosing mapping allows running existing algorithms based alone without algorithm needing take care nevertheless mapping allows express purely terms original variable time deﬁne deﬁnition consider primal-dual setting µi-strongly convex convexity parameter case require bounded support. then given i-th dual residue iteration given t-support set. lemma suppose lipschitz. then |κi| proof. lemma li-lipschitzness implies li-bounded support therefore |αi| writing lipschitzness bounded subgradient |ui| |κi| |αi| |ui| algorithm describes coordinate descent method primal-dual setting method major steps coordinate selection linesearch along chosen coordinate primal-dual parameter updates. standard methods chooses coordinates random ﬁxed distibutions develop adaptive strategies sequel change sampling distribution iteration. steps remain essentially same. subsection introduces lemma characterizes relationship sampling distribution coordinates denoted convergence rate method. purpose build upon relax strong-convexity restrictions gi’s. derive convergence result general convex coordinate-dependent strong convexity constants contrast bounded support. gap-wise uniform sampling compare rates obtained theorem gap-wise sampling theorem uniform sampling. according theorem rate distribution written follows importance sample ﬁxed non-uniform variant adaptive obtained bounding likaik distribution computed once. data normalized sampling variant coincides importance sampling problems easily reformulated primal-dual setting choosing lasso logistic loss classiﬁcation respectively. knowledge importance sampling adaptive sampling techniques setting. according lemma proper convex function bounded support lipschitz continuous. modify λ|αi| restricting support interval radius λkαk). since algorithm monotone choose enough guarantee stay inside ball optimization i.e. algorithm’s iterate sequence aﬀected maxt since process ﬁnding ◦ada rather problematic good found replacing ◦ada upper bound minimizing w.r.t. another option safe choice provide balance distributions. strategy beneﬁts convergence guarantees case unknown ◦ada. applications section latter option call sampling variant ada-uniform sampling. variable update distribution generation computing dual residuals coordinate-wise duality gaps expensive epoch classic method i.e. need operations contrast updating coordinate cheap total cost epoch naive implementation expensive sampling schemes adaptive supportset-uniform ada-uniform ada-gap. completely recompute sampling distribution iteration giving total per-epoch complexity contrast ﬁxed non-uniform sampling scheme importance requires build sampling distribution once epoch framework directly covers original hinge-loss formulation. importance sampling technique applicable original hinge-loss relies smoothed version hinge-loss changing problem. discuss computational costs proposed variants diﬀerent sampling schemes. table states costs detail number non-zero entries data matrix table epoch means consecutive coordinate updates number features lasso number datapoints svm. provide numerical evidence sampling strategies machine learning problems lasso hinge-loss svm. algorithms theory also directly applicable sparse logistic regression others omit experiments space limitations. performance. figures show performance studied variants record suboptimality duality main measures algorithm performance. reported results averaged runs algorithm. methods ﬁxed sampling distributions. three eﬃcient sampling schemes results show importance converges faster uniform datasets lasso however worse uniform svm. mildly adaptive strategy gap-per-epoch based coordinate-wise duality theory computed per-epoch signiﬁcantly outperforms them. observed number epochs well number total vector operations methods adaptive sampling distributions adaptive methods updating probabilities coordinate step figure show importance sampling baseline method measured epoch adaptive methsvms related methods studied since introduction e.g. hsieh ﬁrst propose partially separable primal-dual setting hinge-loss. theoretical convergence rates beyond application hinge-loss found sdca line work primal-dual analog primal-only algorithms. however main limitation sdca applicable strongly convex regularizers requires smoothing techniques applicable general regularizers technique d¨unner extend algorithms well primal-dual analysis problem class interest here using bounded interest iterates instead relying smoothing. convergence rate stochastic methods naturally depends sampling probability distribution coordinates datapoints respectively. virtually existing methods sampling uniformly random recently showed appropriately deﬁned ﬁxed non-uniform sampling distribution work taken nonuniform sampling step towards adaptive sampling. restricted strongly convex regularizers rates provided adasdca algorithm updating probabilities step beat ones uniform importance sampling. diﬀerent approach adapting sampling distribution proposed block coordinate frank-wolfe algorithm enhanced sampling proportional values block-wise duality gaps. adaptive variant studied proposed adaptive sampling scheme dependent past iterations markovian manner without giving explicit convergence rates. adaptive heuristics without proven convergence guarantees include acid general convex regularizers development algorithms includes restricted uniform sampling aware proven convergence rates showing improvements non-uniform even adaptive sampling non-uniform sampling require smoothing modiﬁcation original problem covering adaptive sampling. work investigated conclusion. adaptive rules adjusting sampling probabilities coordinate descent. theoretical results provide improved convergence rates general class algorithm schemes hand optimization problems hand able directly analyze general convex objectives particularly useful problems hinge-loss objectives covered previous schemes. practical experiments conﬁrm strong performance adaptive algorithms conﬁrm behavior predicted theory. finally advocate computationally eﬃcient gap-per-epoch sampling scheme practice. scheme close ones supported theory explicit primal-dual convergence analysis remains future research question. ilya loshchilov marc schoenauer mich`ele sebag. adaptive coordinate descent. natalio krasnogor pier luca lanzi editors genetic evolutionary computation conference pages dublin ireland acm-sigevo. necorara yurii nesterov fran¸cois glineur. eﬃciency randomized coordinate descent methods optimization problems linearly coupled constraints. technical report anton osokin jean-baptiste alayrac isabella lukasewitz puneet dokania simon lacoste-julien. minding gaps block frank-wolfe optimization structured svms. icml proceedings international conference machine learning guillaume papa pascal bianchi st´ephan cl´emen¸con. adaptive sampling incremental optimization using stochastic gradient descent. international conference algorithmic learning theory pages", "year": 2017}