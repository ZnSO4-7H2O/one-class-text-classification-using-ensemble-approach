{"title": "Double Relief with progressive weighting function", "tag": ["cs.LG", "cs.AI"], "abstract": "Feature weighting algorithms try to solve a problem of great importance nowadays in machine learning: The search of a relevance measure for the features of a given domain. This relevance is primarily used for feature selection as feature weighting can be seen as a generalization of it, but it is also useful to better understand a problem's domain or to guide an inductor in its learning process. Relief family of algorithms are proven to be very effective in this task.  On previous work, a new extension was proposed that aimed for improving the algorithm's performance and it was shown that in certain cases it improved the weights' estimation accuracy. However, it also seemed to be sensible to some characteristics of the data. An improvement of that previously presented extension is presented in this work that aims to make it more robust to problem specific characteristics. An experimental design is proposed to test its performance. Results of the tests prove that it indeed increase the robustness of the previously proposed extension.", "text": "feature weighting algorithms solve problem great importance nowadays machine learning search relevance measure features given domain. relevance primarily used feature selection feature weighting seen generalization also useful better understand problem’s domain guide inductor learning process. relief family algorithms proven eﬀective task. previous work extension proposed aimed improving algorithm’s performance shown certain cases improved weights’ estimation accuracy. however also seemed sensible characteristics data. improvement previously presented extension presented work aims make robust problem speciﬁc characteristics. experimental design proposed test performance. results tests prove indeed increase robustness previously proposed extension. feature selection undoubtedly important problems machine learning pattern recognition information retrieval among others. feature selection algorithm computational solution motivated certain deﬁnition relevance. however relevance feature several deﬁnitions depending objective looked after. hand feature weighting algorithms estimate relevance rather binarily deciding whether feature either relevant not. much harder problem also ﬂexible framework inductive learning perspective. kind work review relief popular feature weighting algorithms. original relief variants presented section drawing heavily earlier material. next revisit \"double\" feedback extension algorithm ﬁrstly introduced previous work takes estimations account order improve general performance. finally version algorithm presented section uses estimations progressive manner initially behaves like traditional algorithm gradually increases importance estimates behave \"double\" version. experimental design presenten secion test performance original algorithm versus proposed ones. finally results conclusions presented. relief feature weighting algorithm doesn’t share common characteristic feature selection weighting methods. treat features individually assuming conditional independence features upon class. hand relief takes features care evaluating speciﬁc feature. another interesting characteristic relief aware contextual information able detect local correlations feature values ability discriminate instance diﬀerent class. main idea behind relief assign large weights features contribute separating near instances diﬀerent class joining near instances belonging class. word \"near\" previous sentence crucial importance since mentioned main diﬀerences relief cited methods ability take local context account. relief reward features separate instances diﬀerent classes general features near instances. fig. original algorithm presented kira rendell maintained original notation slightly diﬀers used features labeled detecting whether feature useful discriminate near instances selects nearest neighbors current instance class called nearest diﬀerent class called nearest miss. nearest neighbors increases weight ofthe feature value decreases otherwise. opposite occurs nearest miss relief increases weight feature opposite values decreases otherwise. original deﬁnition heterogeneous distance metric composed overlap metric nominal features normalized euclidean distance linear features called heom. instances features algorithm cost loop instances. instance main loop compute distance instances times complexity calculating drelief easily complexity complexity user deﬁned parameter measure control cost relief algorithm tradeoﬀ accuracy estimation complexity algorithm however never greater |i|. ﬁrst modiﬁcation proposed algorithm make deterministic changing outer loop randomly chosen instances loop instances. obviously increases algorithms computation cost becomes makes experiments small datasets reproducible. kononenko uses simpliﬁed version algorithm paper test extensions original relief. version also used authors given name relieved ﬁnal \"deterministic\". extensions original relief algorithm proposed order overcome limitations couldn’t deal incomplete datasets sensible noisy data could deal multi-class problems splitting problem series -class problems. able relief deal incomplete datasets i.e. contained missing values modiﬁcation function needed. function must capable calculating diﬀerence value feature missing value missing values addition calculation diﬀerence known values. kononenko proposed various modiﬁcations function paper found performed better others version relief called relief-d diﬀerence function used relief-d seen focus giving relief greater robustness noise. robustness achieved increasing number nearest hits misses look mitigates eﬀect choosing neighbor would nearest without eﬀect noise. algorithm user deﬁned parameter controls number nearest neighbors use. choosing tradeoﬀ locality noise robustness. states good choice purposes. last limitation algorithm designed -class problems. straightforward extension multi-class problems would take near miss nearest neighbor belonging diﬀerent class. variant relief so-called relief-e kononenko. later proposes another variant gave better results take nearest neighbor class average contribution keep contributions hits misses symmetric interval gives relief-f algorithm seen fig. neighbors version algorithm called myopic relieff loses context locality property. rewriting removing neighboring condition applying bayes’ rule obtain myopic relieff holds kind normalization multi-valued attributes using factor peqval. solves bias impurity functions towards attributes multiple values. anther improvement compared gini-index gini-index gain values decrease number classes increase. denominator avoids strange behavior. relief’s estimates features’ quality used successfully weights distance calculation could estimation previous iteration compute distance instances searching nearest hits misses. refer version relieff double relieff short drelieff. want begin distance calculation without using weight estimates then relief’s weight estimates become accurate increase importance weights distance calculation. lets distance calculation like would like function that increasing respect continuous function could refer version relieff using distance equation progressively weighted double relief short pdrelieff. control parameter determines steepness ﬁnal value curve described function iteration number another desirable property function would always gives results regardless number iterations. words total number iterations would like value whatever value achieve must make depend also total number iterations decrement steepness function number total iterations increases. posible deﬁnition shown fig. varies inﬂuence diﬀerent weights iterations high values function converges ﬁrst iterations stabilizes value near values it’s value remains near till end. choose value compute area left function. normal relieff particular case maximum area drelieff another particular case minimum area. want choose parameters two. speciﬁcally could choose parameters leave area function. solve possible combination parameters solves equation graphicly seen fig. values make weights’ ponderations stay near half iterations takes values near weights’ values. value chosen experiments. main factor impact performance results problem want solve addition diﬃcult reduce. order eliminate it’s inﬂuence possible problems would tried obviously impossible. another factor clearly impact performance type attributes relief heterogeneous function distance calculation depends whether attributes numeric categoric. reduce eﬀect factors experiments diﬀerent problems three numeric attributes three categoric ones. problems tested artiﬁcial suﬃcient knowledge data make performance weighting dependent performance classiﬁer. ranges factor chosen. least relevant attribute irrelevant order check whether algorithm seems capable distinguishing them start experiments. number irrelevant attributes depend number relevant ones order test percentage irrelevant attributes number relevant attributes. good choice could twice number irrelevant attributes number relevant ones. upper bound number relevant attributes depend number instances generated. interesting test algorithms wide range attributes instances ratios. arbitrarily number instances generated number instances would interesting features ratio attributes instances low. want total features keep number irrelevant attributes twice number relevant ones upper bound number relevant attributes decide possible combinations factors tried experiments. better reduce eliminate contribution experimental error factors would treat blocking factors. create homogeneous blocks factors kept constant target factor takes possible values. blocking possible limited resources random subset block run. ranges described above total diﬀerent factor combinations problem seen number relevant attributes number iterations combination relevant irrelevant attribute numbers. gives total number diﬀerent combinations problem. number combinations combinations run. experimental design full blocking design shown fig. algorithmic way. data generator produces data randomly numeric attributes producing decision list. decision list consists rules. rules form inequality term attribute random value. rule number random instances generated randomly one. class determined ﬁrst rule true current instance. decision list fails classify current instance rule according current instance generated added decision list. irrelevant attributes generated randomly range radial basis functions functions characteristic feature response decreases monotonically distance central point. diﬀerent formulas describe speciﬁc shape function usually parameters control center distance scale. particular case function used gaussian described seen fig. parameters mean standard deviation gaussian monotonically decreases distance center. randomrbf data generated ﬁrst creating random centers class. center randomly assigned weight central point attribute standard deviation. generate instances center chosen random taking weights center consideration. attribute values randomly generated oﬀset center overall vector scaled length equals value sampled randomly gaussian distribution center. particular center chosen determines class instance. randomrbf data contains numeric attributes non-trivial include nominal values. irrelevant attributes generated following gaussian distribution random centers standard deviation. section results described experiments presented. plots presented fig. clearly understand axes represent notation introduced. relevant attributes irrelevant ones order accentuate global diﬀerences three algorithms plots presented accumulated results y-axis. fig. shows results. x-axis keeps deﬁnition y-axis accumulated value separability formula y-axis value point knowing separability deﬁned point axis deﬁnition slope function indicates positive negative separability. function descends point separability negative hand function ascending point separability positive. steepness slope indicates magnitude separability ﬁnally separation curves algorithm tells accumulated diﬀerence separabilities. algorithm another shows accumulated separability greater particular algorithm conclude average algorithm outperforms other. looking results above seen none three algorithms clearly better another chosen problems. looking ﬁrst plots separability x-axis curves three algorithms almost same attributes drelieff seems diﬀerent behavior. anomaly problem random rbfs drelieff clearly worse. fatct except majority problem drelieff always worse algorithm even non-signiﬁcantly better. diﬀerence drelieff algorithms uses calculated weights distance ponderations starting ﬁrst iteration algorithm. certainly cause relieff stuck local minimum found ﬁrst iterations distance function using take account relevant variables. section pdrelieff introduced stated hypothesis using weights estimates since ﬁrst iteration cause decrease performance fact estimations biased ﬁrst instances optimal weights. results help support hypothesis. could also explain drelieff’s behavior diﬀerent others attributes evaluated opposed attributes present. attributes calculate distance with making mistake choosing ponderations makes changes results problems attributes sensible wrong distance calculations cause drelieff either much higher lower performance depending close early weights real optimal weights. ﬁrst instances seen algorithm representative whole example share common characteristic rare among instances weights used biased; hand ﬁrst instances give accurate weight approximates possible drelieff’s worked better rest. also another characteristic results pointed out. second plots diﬀerences among algorithms stand clearer diﬀerences behavior normal version algorithm opposed modiﬁed ones. plots parallel curves separability algorithms indicate performance evolves meanwhile divergent curves indicate performance increases other. mind results show ﬁrst problems numeric attributes performance drelieff decreases quick normal relieff best three pdrelieff close though performance also decreases faster normal relieff’s. results nonmonotonic clear separability particular problem keeps high number attributes three algorithms perform almost identical. modiﬁcations could applied generation problem make diﬃcult relieff discriminate attributes’ relevance compare performance degradation three algorithms. thing contrary happens numeric problems move onto categoric ones algorithm suﬀers least performance decrease drelieff followed pdrelieff. ﬁnal conclusion looking experimental results must although performance three algorithms frequently almost same algorithm pdrelieff introduced seems always middle quite stick better better worse depending problem type maybe depending whether attributes numeric categoric. also drelieff sensible early errors weight approximation relieff must used carefully. future work problems could tested speciﬁc experiments conducted deeper hypothesis diﬀerent versions relieff perform diﬀerent problems numeric categoric attributes. also tests real data done using diﬀerent classiﬁers contrast", "year": 2015}