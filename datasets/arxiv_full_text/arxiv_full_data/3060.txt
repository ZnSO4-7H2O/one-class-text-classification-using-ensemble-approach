{"title": "Learning Nested Sparse Structures in Deep Neural Networks", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Recently, there have been increasing demands to construct compact deep architectures to remove unnecessary redundancy and to improve the inference speed. While many recent works focus on reducing the redundancy by eliminating unneeded weight parameters, it is not possible to apply a single deep architecture for multiple devices with different resources. When a new device or circumstantial condition requires a new deep architecture, it is necessary to construct and train a new network from scratch. In this work, we propose a novel deep learning framework, called a nested sparse network, which exploits an n-in-1-type nested structure in a neural network. A nested sparse network consists of multiple levels of networks with a different sparsity ratio associated with each level, and higher level networks share parameters with lower level networks to enable stable nested learning. The proposed framework realizes a resource-aware versatile architecture as the same network can meet diverse resource requirements. Moreover, the proposed nested network can learn different forms of knowledge in its internal networks at different levels, enabling multiple tasks using a single network, such as coarse-to-fine hierarchical classification. In order to train the proposed nested sparse network, we propose efficient weight connection learning and channel and layer scheduling strategies. We evaluate our network in multiple tasks, including adaptive deep compression, knowledge distillation, and learning class hierarchy, and demonstrate that nested sparse networks perform competitively, but more efficiently, than existing methods.", "text": "figure conceptual illustration nested sparse network nested levels nested sparse network consists internal networks core level full level internal network share parameters higher level internal networks. since nested network produces multiple different outputs leveraged multiple tasks. availability massive labeled data computational power process data. more many studies conducted toward deep dense models achieve performance gain. despite success remarkable progress accomplished expense intensive computational memory requirements limit deep networks practical especially mobile devices computing capability. particular size network architecture designed colossal problematic network achieve mission-critical tasks commercial device requires real-time operation. fortunately well known exists much redundancy deep architectures i.e. number network parameters represent whole deep network substance motivates many researchers exploit redundancy multiple points view. concept sparse representation exploit redundancy representing network small number representative parameters. sparse deep networks prune connecrecently increasing demands construct compact deep architectures remove unnecessary redundancy improve inference speed. many recent works focus reducing redundancy eliminating unneeded weight parameters possible apply single deep architecture multiple devices different resources. device circumstantial condition requires deep architecture necessary construct train network scratch. work propose novel deep learning framework called nested sparse network exploits n-in--type nested structure neural network. nested sparse network consists multiple levels networks different sparsity ratio associated level higher level networks share parameters lower level networks enable stable nested learning. proposed framework realizes resource-aware versatile architecture network meet diverse resource requirements. moreover proposed nested network learn different forms knowledge internal networks different levels enabling multiple tasks using single network coarse-to-ﬁne hierarchical classiﬁcation. order train proposed nested sparse network propose efﬁcient weight connection learning channel layer scheduling strategies. evaluate network multiple tasks including adaptive deep compression knowledge distillation learning class hierarchy demonstrate nested sparse networks perform competitively efﬁciently existing methods. deep neural networks recently become standard architecture signiﬁcant performance improvement traditional machine learning models number ﬁelds image recognition object detection image generation natural language processing successful outcomes derived tions insigniﬁcant contributions prune number channels prune number layers sparse regularization. another regularization strategy exploit network redundancy low-rank approximation approximates weight tensors minimizing reconstruction error original network reduced network weight tensors approximated decomposing tensors pre-speciﬁed sizes minimizing nuclear-norm regularized optimization problem obviously developing compact deep architecture beneﬁcial satisfy speciﬁcation device capacity. however difﬁcult learned compact network adjusted different hardware speciﬁcations since deep neural network normally learns parameters given dataset different task model device different computation power required usually deﬁne deep network manually trial error. likewise different form knowledge required trained network hard keep learned knowledge training using network using network general perform multiple tasks need multiple networks cost considerable computation time memory footprint. work exploit nested structure deep neural architecture realizes n-in- versatile network conduct multiple tasks within single neural network nested structure network parameters assigned multiple sets nested levels level subset parameters higher level set. different sets capture different forms knowledge according type information making possible perform multiple tasks using single network. propose nested sparse network termed nestednet consists multiple levels networks different sparsity ratios internal network lower nested level shares parameters internal networks higher nested levels. thus lower level internal network learn common knowledge higher level internal network learn task-speciﬁc knowledge. nestednet generic network encompasses popular deep architectures alexnet resnet also compressed architectures. unlike existing networks nested sparse network learn different forms knowledge internal networks different levels. hence network applied multiple tasks satisfying different resource requirements reduce efforts train separate existing networks. addition consensus different knowledge nested network improve performance overall network. order exploit nested structure present several pruning strategies including efﬁcient connection optimization method used learn parameters scratch using off-the-shelf deep learning libraries. also provide applications nested structure applied adaptive deep compression knowledge distillation hierarchical classiﬁcation. experimental results demonstrate proposed nested sparse network performs competitively compared popular baseline existing sparse networks. particular results application obtained single nested network making nestednet highly efﬁcient compared currently available approaches. summary main contributions work present efﬁcient connection pruning method learns sparse connections scratch. also provide efﬁcient channel layer pruning scheduling exploit nested structure avoid need train multiple different networks. propose n-in- nested sparse network realize nested structure deep neural network. nested structure enables resource-aware adaptive learning knowledge-aware adaptive learning various tasks possible existing deep architectures. besides consensus multiple knowledge improve prediction nestednet. proposed nested networks performed various applications order demonstrate efﬁciency nested structure comparable performance. na¨ıve approach compress deep neural network prune network connections sparse approximation. proposed iterative prune retrain approach using ll-norm regularization. prunes weight connections weight element predeﬁned threshold retrain network iteratively. zhou proposed forward-backward splitting method solve l-norm regularized optimization problem. learns weights ﬁrst standard back-propagation optimizes using subgradient algorithm. note weight pruning methods non-structured sparsity difﬁcult achieve valid speed-up using standard machines irregular memory access channel pruning approaches proposed structured sparsity regularization using group lasso regression channel selection method using lasso regression since reduce actual number parameters beneﬁts computational memory resources compared weight connection pruning methods. layer pruning another candidate compression parameters associated layer little contributions deep neural network using short-cut connection another line compressing deep networks low-rank approximation weight tensors approximated low-rank tensor decomposition solving nuclear-norm regularized optimization problem save memory storage enable valid speed-up learning inferencing network. low-rank approximation approaches normally require pretrained model optimizing parameters reduce reconstruction error original learned parameters. important note that however learned networks using compression approaches difﬁcult utilized different tasks different compression ratios since learned parameters trained single task compression ratio required train network manual model parameter tuning scratch tune trained network suit demand procedure conducted continually whenever form model changed requiring additional resources efforts. difﬁculty fully addressed using proposed nested sparse network. embody multiple internal networks within network perform different tasks cost learning single network. furthermore since nested sparse network constructed scratch effort learn baseline network needed. studies build compact network given large network called knowledge distillation maintaining knowledge large network shares intention deep network compression approaches utilizes teacher-student paradigm ease training networks since produces separate small student network learned teacher network efﬁciency also limited similar deep compression models. proposed nested structure also related treestructured deep architectures. hierarchical structures deep neural network recently exploited better learning proposed hierarchical architecture outputs coarse-to-ﬁne predictions using different internal networks. proposed structured deep network enable model parallelization compact model compared previous hierarchical deep networks. however since networks nested structure since parameters networks form independent groups hierarchy canbeneﬁt nested learning sharing knowledge obtained coarseﬁne-level sets parameters. limitation discussed section loss function network output ground-truth label regularizer constrains weight parameters weighting factor balancing loss regularizer. outputs according purpose task classiﬁcation regression chain linear nonlinear operations using parameter parameters represented {wl}≤l≤l number layers network rkw×kh×ci×co convolutional weight rci×co fully-connected weight popular deep learning architectures alexnet residual networks here width height convolutional kernel number input output channels respectively. order exploit sparse structure neural network many studies usually enforce constraints sparsity using weight decay low-rank-ness using nuclear-norm tensor factorization however many previous studies utilize pre-trained network prune connections network develop parsimonious network architecture usually requires signiﬁcant additional computation. investigate three pruning approaches sparse deep learning weight connection pruning channel pruning layer pruning used nested sparse network described section achieve sparse weight connection learning pruning strategies proposed reduce learned parameters using pre-deﬁned threshold using subgradient method l-norm regularized optimization problem however require additional pruning steps sparsify learned dense network. alternative propose efﬁcient sparse connection learning approach learns scratch without additional pruning steps using standard optimization tool. problem formulation constructed follows absolute operator activation function encode binary output pre-deﬁned threshold value pruning. since unit-step function makes learning problem standard backpropagation difﬁcult discontinuity present approximated unit-step function using combination existing activation functions relu sigmoid large value mimic slope unit-step function. approximation sensitive initial values parameters applying popular initialization method empirical experiences. note acts implicit mask reveal sparse weight tensor. element becomes corresponding weight longer updated optimization procedure making contribution network. solving construct sparse deep network based off-the-shelf deep learning libraries without additional efforts. l}≤l≤l consists binary weight tensors whose numbers input output channels reduced smaller numbers numbers channels baseline architecture fulﬁll demanded sparsity. words model network single number scheduled channels using optimize network scratch. achieving multiple sparse networks scheduling multiple numbers channels described following section. similar channel pruning implementing layer pruning straight-forward reducing number layers repeated blocks addition pruning approaches combined various nested structures described next section. nested sparse networks goal nested sparse network represent nin- nested structure parameters deep neural network allow nested internal networks shown figure nested structure internal network lower nested level gives higher sparsity parameters higher sparsity means smaller number non-zero entries. addition internal network core level deﬁnes compact network among internal networks internal network full level deﬁnes fully dense network. them internal networks intermediate sparsity ratios. importantly internal network lower nested level shares parameters internal networks higher nested levels. number nested levels. since masks }≤l≤l represents j-th nested level weights binary values pωmj pωmk simple graphical illustration nested parameters fully-connected layers shown figure optimizing build nested sparse network nested levels standard weight decay regularization using regularize weights evenly throughout nested network. order masks apply three pruning approaches described section realizing nested structure entry-wise weight connections masks estimated solving weight connection pruning problem different thresholds iteratively. speciﬁcally mask consisting k-th nested level weights obtained network train network masked weight pωmk using higher value threshold another mask giving higher sparsity procedure conducted iteratively reaching sparsest mask core level. strategy helpful sparse dominant weights network trained using strategy performs better network whose sparse mask obtained randomly sparse networks section nested sparse structure convolutional channels layers schedule masks according type pruning. channel-wise scheduling number channels convolutional layers dimensions fully-connected layers scheduled pre-speciﬁed numbers scheduled layers. scheduled weights learned solving without performing mask estimation phase mathematically represent weights rkw×kh×i×o ﬁrst level weight rkw×kh×ci×co full nested level weight l-th figure illustrates nested sparse network channel scheduling different color represents weights different nested level except sub-level weights. ﬁrst input layer observation data scheduled work unlike nested sparse network weight pruning method holds whole-size network structure nested levels channel scheduling keeps learns parameters corresponding number scheduled channels associated nested level making valid speed-up especially inference phase. likewise schedule number layers corresponding weights repeated network block learn parameters solving note residual network consists layers number residual blocks schedule single nested residual network consists three residual networks size end. among them full level network number parameters conventional residual network size without introducing parameters. provide three applications proposed nested sparse network nestednet adaptive deep compression knowledge distillation hierarchical classiﬁcation. adaptive deep compression. since nested sparse network constructed weight connection learning apply deep compression furthermore nested figure graphical representation channel scheduling using nested parameters l-th convolutional layers nested sparse network layers scheduled. denote additional numbers input output channels k-th level respectively. sparse network realizes adaptive deep compression nested structure makes possible infer adaptively using internal network sparsity level suitable required speciﬁcation. problem apply weight pruning channel scheduling presented section knowledge distillation. knowledge distillation used represent knowledge compactly network here apply channel layer scheduling approaches make small-size sub-networks shown figure train internal networks full-level sub-level networks simultaneously scratch without pre-training full-level network. note nested structure sublevel networks necessarily coincided combination channel layer scheduling according design choice. hierarchical classiﬁcation. hierarchical classiﬁcation problem hierarchy modeled internal network nested level. example model nested network nested levels cifar- dataset super classes class subclasses enables nested network perform coarse inference. apply channel pruning method since handle different output dimensionality. evaluated proposed network nestednet based popular deep architectures resnet-n wrn-n-k number layers scale factor number convolutional channels table deep compression results using resnet- cifar- dataset. denotes compression rate parameters baseline network. baseline results obtained author’s implementation without pruning three applications adaptive deep compression knowledge distillation hierarchical classiﬁcation since difﬁcult compare fairly baseline sparse networks non-nested structure provide one-to-one comparison internal networks nestednet corresponding independent baselines published networks network structure. nestednet performed benchmark datasets cifar- cifar- different numbers object classes. since cifar- consists two-level hierarchy classes networks nested levels applied dataset hierarchical classiﬁcation problem. also provide results using imagenet dataset hierarchical classiﬁcation appendix. test time computed batch size training phase. nestednet variants compared baselines implemented using tensorflow library processed nvidia titan graphics card. implementation details models described appendix. goal adaptive deep compression demonstrate effectiveness nestednet various compression ratios compared baseline networks corresponding internal networks published sparse deep networks. applied weight connection channel pruning approaches described section based resnet compare state-of-the-art network pruning channel pruning approaches implemented iterative network pruning method experimental environment giving baseline accuracy results channel pruning approaches baseline network refereed compare channel pruning approaches nested network constructed nested levels full-level core-level compare network pruning method constructed another nestednet three internal networks setting full-level networks give result baseline network resnet- experiment also provide results two-level nested network learned using random sparse masks whose weight connections randomly pruned weight tensors order verify effectiveness proposed weight connection learning method section table shows classiﬁcation accuracy comchanpared networks cifar- dataset. pruning nestednet gives smaller performance loss baseline recently proposed channel pruning approaches reduced parameters even though baseline performance different implementation strategies. weight connection pruning performs better network pruning average. show accuracy compression gives better accuracy compression. here weight connection pruning approaches outperform channel pruning approaches including channel scheduling based network compression since prune unimportant connections elementwise channel pruning approaches eliminate connections group-wise produce information loss. note random connection pruning gives poor performance conﬁrming beneﬁt proposed connection learning approach learning nested structure. figure represents learned ﬁlters nested network channel pruning approach using resnet- three nested levels size ﬁrst convolutional ﬁlters observable large size figure results nestednet cifar- dataset. learned ﬁlters ﬁrst convolutional layer. activation feature maps train image column represents different layer represents images internal network core-level full-level best viewed color. ﬁlters performance. shown ﬁgure connections core-level internal network dominant upper-level ﬁlters include sublevel ﬁlters lower importance core-level ﬁlters learn side information dataset. figure shows activation feature maps image different layers. likewise core-level network learns dominant information image higher-level internal networks using parameters current levels except lower-level parameters catch complementary contents. better understanding provide additional activation maps train test images appendix. show effectiveness nested structures evaluated nestednets using channel layer scheduling knowledge distillation learned internal networks jointly rather learning distilled network pre-trained model literature proposed network constructed architecture since scales number convolutional channels residual network channel scheduling produce internal networks structure conventional residual networks making convenient comparison baseline network. full-level network wrn-- scale factor channel scheduling resnet-) layer scheduling scale factor combined scheduling channel layer here apply nested structure ﬁrst convolutional layer ﬁnal output layer. applied proposed network cifar- cifar- datasets. figure shows comparison nestednet four internal networks corresponding baseline networks learned independently cifar- dataset. also provide test time every internal network. shown ﬁgure nestednet performs competitively compared baseline networks density ratios. even though total number parameters construct nested sparse network smaller learn independent baseline networks shared knowledge among multiple internal networks give competitive performance baselines. comes test time achieve valid speed-up internal networks different sparsity speed-up even though speed-up linear compression ratio. table shows performance nestednet baseline structure previous example cifar- dataset. problem nestednet comparable corresponding baseline networks average requires similar resource baseline full level. table minor compression nestednet makes little performance loss large compression sacriﬁces large performance still gives competitive performance baseline. evaluated nested sparse network hierarchical classiﬁcation. constructed two-level nested network cifar- dataset channel scheduling applied handle different dimensionality hierarchical structure dataset. compared state-oftable hierarchical classiﬁcation results proposed nested sparse network cifar- dataset. indicates approximated total resource nested network. accuracies consensus computed number classes the-art architecture splitnet address class hierarchy. following practice proposed nestednet constructed wrn-- architecture adopted wrn-- core internal network also provide performances baselines corresponding internal networks nestednet. since number parameters splitnet reduced nearly baseline constructed another nestednet based wrn-- architecture almost number parameters splitnet. table shows performance comparison among compared networks. overall nestednets based different architectures give better performance baselines cases since learn abundant knowledge learning speciﬁc classes learning abstract level knowledge within nested network compared merely learning independent class hierarchy. nestednet also outperforms splitnet architecture architecture number parameters splitnet learns parameters divided independent sets nestednet learns shared knowledge different tasks improve performance combined knowledge obtained multiple internal networks. experiment shows nested structure realize encompassing multiple semantic knowledge single network accelerate learning. note number internal networks increases hierarchy amount resources saved increases. consensus multiple knowledge utilize beneﬁt appended another layer call consensus layer combine outputs nested levels better prediction averaging learning nestednetl simply added fully-connected layer concatenated vector outputs nestednet additionally collected class output core level network hierarchical classiﬁcation problem. appendix details. overhead combining outputs different levels nestednet negligible shown table table consensus approaches nestednet outperform existing structures including nestednet full-level similar number parameters. notably nestednet full-level hierarchical classiﬁcation gives better performance knowledge distillation architecture wrn-- since rich knowledge incorporating coarse class information architecture without introducing additional structures. proposed nested sparse network named nestednet realize n-in- nested structure neural network several networks different sparsity ratios contained single network learned simultaneously. exploit structure novel weight pruning scheduling strategies presented. nestednet efﬁcient architecture incorporate multiple knowledge additional information within neural network. nestednets extensively tested various applications demonstrated performs competitively efﬁciently existing deep architectures.", "year": 2017}