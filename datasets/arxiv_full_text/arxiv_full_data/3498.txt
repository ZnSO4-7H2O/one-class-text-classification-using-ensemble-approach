{"title": "Subspace Clustering using Ensembles of $K$-Subspaces", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We present a novel approach to the subspace clustering problem that leverages ensembles of the $K$-subspaces (KSS) algorithm via the evidence accumulation clustering framework. Our algorithm forms a co-association matrix whose $(i,j)$th entry is the number of times points $i$ and $j$ are clustered together by several runs of KSS with random initializations. We analyze the entries of this co-association matrix and show that a naive version of our algorithm can recover subspaces for points drawn from the same conditions as the Thresholded Subspace Clustering algorithm. We show on synthetic data that our method performs well under subspaces with large intersection, subspaces with small principal angles, and noisy data. Finally, we provide a variant of our algorithm that achieves state-of-the-art performance across several benchmark datasets, including a resulting error for the COIL-20 database that is less than half that achieved by existing algorithms.", "text": "present novel approach subspace clustering problem leverages ensembles k-subspaces algorithm evidence accumulation clustering framework. algorithm forms co-association matrix whose entry number times points clustered together several runs random initializations. analyze entries co-association matrix show naïve version algorithm recover subspaces points drawn conditions thresholded subspace clustering algorithm. show synthetic data method performs well subspaces large intersection subspaces small principal angles noisy data. finally provide variant algorithm achieves state-of-the-art performance across several benchmark datasets including resulting error coil- database less half achieved existing algorithms. modern computer vision problems facial recognition object tracking researchers found success applying union subspaces model data vectors near several subspaces. model goal simultaneously identify underlying subspaces cluster points according nearest subspace. algorithms designed solve problem fall category subspace clustering topic received great deal attention recent years efﬁcacy real-world datasets extended yale face database mnist handwritten digit database earliest approaches solving subspace clustering problem involves iterative method spirit k-means known k-subspaces alternates assigning points clusters estimating subspace basis associated cluster. algorithm guaranteed converge local minimum practice runs many instances algorithm chooses ﬁnal clustering produces minimum cost. although empirical performance limited continues serve benchmark subspace clustering algorithms part computational efﬁciency simplicity. therefore deeper understanding method important contribution area subspace clustering contribution paper. cost function alternating algorithm perhaps natural approach subspace clustering problem known initializations nonzero measure algorithm convergence point global minimizer. observation even initializations commonly give partially-correct clustering behavior combined form accurate clustering algorithm. contributions follows. introduce novel application well-known evidence accumulation clustering framework leverages ensembles algorithm perform subspace clustering. combining results many random initializations algorithm obtains afﬁnity matrix apply spectral clustering. provide theoretical guarantees regarding resulting afﬁnity matrix lead recovery guarantees subspace clustering problem. show method extremely effective synthetic real datasets; show synthetic data method superior performance subspaces extremely close together. further show variant algorithm achieves state-of-the-art performance several real datasets including error coil- image database full yale database better state-of-the-art respectively. finally since method relies multiple independent initializations inherently parallelizable. best knowledge provide ﬁrst theoretical guarantees characterizing co-association matrix resulting evidence accumulation well ﬁrst recovery guarantees variant algorithm. label points unknown union subspaces according nearest subspace. clusters obtained corresponding subspace bases recovered using principal components analysis state-of-the-art approaches subspace clustering rely self-expressiveness property data informally states points model efﬁciently represented points within subspace. methods typically self-expressive data cost function regularized enforce efﬁcient representation follows -norm sparse subspace clustering nuclear norm low-rank representation combination norms. afﬁnity/similarity matrix obtained |z|+|z|t spectral clustering performed. terms considered optimization problem provide robustness noise outliers numerous recent papers follow framework efﬁcient solutions problem based orthogonal matching pursuit elastic-net presented approaches include thresholded subspace clustering afﬁnity matrix formed ﬁnding nearest neighbors points terms spherical distance greedy subspace clustering greedily builds subspaces order form afﬁnity matrix. cases spectral clustering performed ﬁnal step obtain cluster labels. subspace bases. beginning initialization candidate subspace bases proceeds alternating fashion clustering points nearest subspace obtaining subspace bases performing points cluster. algorithm computationally efﬁcient guaranteed converge local minimum k-means output highly dependent initialization. typically applied performing many restarts choosing result minimum cost output. idea extended minimize norm method intelligent initialization also proposed. authors alternating method based perform online subspace clustering case missing data. recently authors propose novel initialization method based ideas perform subspace update step using gradient steps along grassmann manifold. method computationally efﬁcient improves upon previous performance lacks theoretical guarantees. ensemble methods used context general clustering time fall within domain consensus clustering overview beneﬁts techniques given central idea behind methods obtain many clusterings simple base clusterer k-means combine results intelligently. order obtain different clustering results base clustering diversity sort must incorporated. typically done obtaining bootstrap samples data subsampling data reduce computational complexity performing random projections data alternatively authors randomness different initializations k-means obtain diversity approach take subspace clustering. diversity achieved base clustering results must combined. evidence accumulation clustering framework laid results combined voting i.e. creating co-association matrix whose entry equal number times points clustered together. theoretical framework approach laid entries co-association matrix modeled binomial random variables. approach studied clustering problem solved bregman divergence minimization. models result improved clustering performance previous work accompanied theoretical guarantees regard resulting co-association matrix. further speciﬁcally designed consider case data generated model. remainder paper apply ideas consensus clustering subspace clustering problem. describe ensemble algorithm guarantees demonstrate algorithm’s state-of-the-art performance synthetic several real datasets. ekss leverages fact several runs random initialization results partially correct clustering information. therefore several random initializations form co-association matrix using results apply spectral clustering. theoretical results imply even data come generative subspaces arbitrary positioning algorithm outputs perfect clustering long maximum subspace afﬁnity bounded points drawn uniformly true subspaces without noise. noisy data ﬁnal afﬁnity matrix contains false connections points. clustered together entry co-association matrix. threshold co-association matrix taking values row/column. thresholded co-association matrix formed cluster labels obtained using spectral clustering. pseudocode ekss given alg. thresh sets entries row/column zero spectralclustering clusters data points based co-association matrix note number candidates candidate dimension need match number dimension true underlying subspaces. fig. shows progression co-association matrix base clusterings used case noiseless data subspaces dimension ambient space dimension using candidates dimension recovery guarantees still absent despite nearly twenty years since introduction. intelligent initialization methods based probabilistic farthest insertion provided still lack theoretical guarantees. section provide ﬁrst step toward understanding performance well recovery guarantees subspace clustering problem. show combining clusterings result many random initializations subspace candidates entries resulting afﬁnity matrix converge monotonically increasing function absolute value inner product points. corollary fact simpliﬁed version ekss exhibits recovery guarantees best knowledge work ﬁrst provide theoretical guarantees algorithm well ﬁrst characterization co-association matrix context consensus clustering. alternating nature analyzing multiple iterations algorithm remains challenging. instead analyze ﬁrst iteration random candidates drawn points clustered based nearest candidate. further restrict case number candidates candidate dimension finally purposes analysis replace unit norm candidates step gaussian random vectors noting nearly equivalent high dimensions concentration norm refer ekss choice parameters ekss- include explicit pseudocode appendix remarkably show combining results many random instances naïve algorithm leads recovery guarantees turn comparable ssc. analyze case multiple iterations performed iterations guaranteed increase cost function practice running convergence improves clustering performance. fig. co-association matrix ekss base clusterings. data generation parameters data noise-free; algorithm uses candidate subspaces dimension resulting clustering errors limit ekss- delivers correct clustering probability least proof theorem proof hinges following lemma states case points clustered together probability increases monotonically lemma probability points clustered together base clustering ekss- increasing function proof lemma given appendix large numbers entry next afﬁnity matrix approaches probability analyzed lemma hence also increasing function depends relative order lemma lemma order entries thresholded afﬁnity matrix thm. states perfect clustering data guaranteed even case intersecting subspaces long subspaces close directions. clustering condition thm. constants factors. along result recovery guarantees follow lemma indicating ekss- results false connections noisy data missing data outliers. discussion guarantees relation algorithms inverse dependence implies subspace afﬁnity must shrink grows. hand intuitive many points subspace likely points close intersection subspaces potentially misclustered. hand points allows better chance nearest point inner product within subspace. indeed empirical results ekss perform better larger finally note analysis holds case letting guaranteed increase cost function extension thm. case important topic ongoing research. natural heuristic improve clustering performance ekss larger values afﬁnity matrix base clusterings clustering believed accurate smaller values case clustering believed inaccurate. here brieﬂy describe approach. note step ekss equivalent fig. clustering error proposed state-of-the-art subspace clustering algorithms function problem parameters number points subspace true subspace dimension angle subspaces fixed problem parameters warm-start ekss well-known performance alternating methods optimization depends initialization problem parameters reason propose warm-start method improve robustness outliers noise. ﬁrst ekss small number base clusterings then using estimated labels obtained form initial candidate subspace bases performing points cluster. candidate bases used initialize place random candidates ekss. refer variant ekss-ws pseudocode algorithm given appendix parameter selection experiments using ekss take choose best approximation true subspace dimension. assume work good approximating dimension underlying subspace known reasonable several practical applications. example images lambertian object varying illumination known near subspace moving objects video known near afﬁne subspace case unknown subspace dimensions could ekss increasing methods proposed could employed. rather choosing explicitly alg. convergence. ekss algorithm relies appropriate choice number base clusterings thresholding parameter general chosen large computation time allows. experiments real data choose thresholding parameter chosen according data-driven techniques following choice experiments real data select best large range values ekss provide fair comparison. warm-start ekss-ws small comparison requires choice variants require parameters selected. finally implementation spectralclustering section demonstrate performance terms clustering error ekss synthetic real datasets. ﬁrst show performance algorithm function relevant problem parameters verify ekss- exhibits empirical performance tsc. also show ekss recover subspaces either large intersection extremely close. demonstrate real datasets ekss improves previous iterative methods warm-start variant ekss surpasses state-of-the-art results many cases. validate theoretical results draw gaussian candidates rather orthonormal bases ekss-. angles subspaces explicitly speciﬁed assumed subspaces drawn uniformly random d-dimensional subspaces experiments draw points uniformly random unit sphere corresponding subspace show mean error random problem instances. code provided authors ssc. employ admm implementation choose parameters result best performance scenario. misclassiﬁcation rate number points subspace ranges subspace dimension ranges pairs subspaces necessarily intersection intersection dimension grows first ﬁgures demonstrate ekss- achieves roughly performance resulting correct clustering even case subspaces large intersection. second ekss correctly cluster subspace dimensions larger long sufﬁciently many points subspace. large subspace dimensions moderate number points subspace achieves best performance. next explore clustering performance function distance subspaces shown second fig. subspace dimension generate subspaces principal angles values range strikingly ekss able resolve subspaces even smallest separation. stands contrast fails regime subspaces extremely close inner products points different subspaces nearly large within subspace. similarly case points different subspace used regress given point without added cost fails small subspace angles. however long still separation subspaces ekss able correctly cluster points. theory presented capture phenomenon recovery guarantees take account multiple iterations important topic future work. ﬁnal comparison show clustering performance noisy data. fig. shows clustering error function angle subspaces case subspaces dimension points corrupted zero-mean gaussian noise covariance .id. ﬁgure shows ekss- obtain similar performance importantly ekss robust small subspace angles even case noisy data. section show ekss achieves competitive performance variety real datasets commonly used benchmarks subspace clustering literature. consider hopkins- dataset cropped extended yale face database coil- coil- object databases usps dataset provided digits mnist handwritten digit database obtain features using scattering network descriptions datasets relevant problem parameters included appendix hopkins- database report mean median misclassiﬁcation rates common literature compare performance ekss several benchmark algorithms median k-flats ssc-omp elastic subspace clustering online low-rank subspace clustering ekss base clusterings. others code provided authors recommended parameters available otherwise parameters result best performance. fair comparison trials clustering result achieves lowest cost. refer warm-start variant ekss ekss-ws. implementation details including parameter selection data preprocessing found appendix clustering error datasets algorithms shown table lowest errors given bold. first note ekss outperforms cases typically large margin. result emphasizes importance leveraging clustering information base clusterings opposed simply choosing best single clustering. next results show ekss-ws among performers datasets considered. although code method unavailable ekss-ws achieves similar performance reported misclassiﬁcation rate hopkins- dataset. also observe scalable algorithms ssc-omp ensc perform poorly hopkins dataset likely small number points whereas ekss works well small large striking resulting misclassiﬁcation rates yale coil- datasets ekss-ws signiﬁcantly outperforms best existing algorithm. interestingly achieves best performance mnist-k dataset whereas ensc achieves much better performance full -digit database reported memory constraints unable compare performance full mnist database algorithms including ekss. implementing memory-efﬁcient version ekss important topic future work. work presented ﬁrst step toward theoretical understanding algorithm analyzing effect combining multiple clusterings using evidence accumulation clustering framework. showed given choice parameters algorithm provably cluster data union subspaces conditions existing algorithms. demonstrated efﬁcacy approach synthetic real data showed warm-start variant method achieves excellent performance several real datasets. theoretical guarantees presented match existing guarantees literature experiments synthetic data indicate iterative approach provides major improvement robustness small angles subspaces. further results hold case -dimensional candidates observed performance improvement case d-dimensional candidates used. extending analysis general case alg. important next step. finally ekss-ws given parameter choices achieves excellent empirical performance deeper understanding appropriate parameter selection method could lead improved performance robustness across different datasets. section prove lemma section iii-a allows leverage connectivity results theorem convenience include pseudocode explicit ekss- algorithm alg. restate lemma below. lemma probability points clustered together base clustering ekss- increasing function proof. one-dimensional gaussian candidates ekss-. orthogonally invariant i.e. orthogonal matrix furthermore cluster together cluster together. result consider without note disjoint events occur equal probability since identically distributed. remainder proof dedicated showing conditional probability given decreasing function |v|. fact follows probability decreasing function total probability lemma follows equal probability arccos decreasing function convenience random variables i.i.d. gaussian random variables vector scaled rotation i.i.d. gaussian random variables. follows either conditional probability given section prove claim initializations nonzero measure necessarily lead k-subspaces algorithm solution global minimizer simple case one-dimensional subspaces sphere. denote angle vectors i.e. cos− symmetry consider angles rotation invariance candidate subspaces assume sint without loss generality. consider case note event occurs independently probability next note implies case points assigned similarly implies case points assigned thus points clustered candidate subspace long occurs probability initializations converges local global optimum measure least section include implementation details beyond included section iii-b. ﬁrst provide pseudocode thresh ekss-ws algorithms. describe preprocessing steps parameters used experiments real data. section describe real datasets used experiments well preprocessing steps parameters selected algorithms. datasets normalized column lies unit sphere corresponding ambient dimension common literature table gives summary datasets considered. hopkins- dataset consists motion sequences sequences remaining sequence objects moving along different trajectories near afﬁne subspace dimension perform preprocessing steps dataset. perform initial whitening removing ﬁrst singular components dataset project data onto ﬁrst principal components reduce computational complexity methods. whitening resulted worse performance algorithms omitted step. variety rotations. images size datasets whiten removing ﬁrst singular component usps dataset provided contains total handwritten digits size roughly even label distribution. parameters used experiments shown table iii. recommended parameters available choose parameters result best performance cases. olrsc recommended basri jacobs lambertian reﬂectance linear subspaces ieee tpami vol. february tron vidal benchmark comparison motion segmentation algorithms ieee int. conf. comp. vision pattern vidal sastry generalized principal component analysis. springer-verlag georghiades belhumeur kriegman from many illumination cone models face recognition variable lighting pose lecun cortes burges mnist database handwritten digits available yann.lecun.com/exdb/mnist bradley mangasarian k-plane clustering journal global optimization vol. tseng nearest q-ﬂat points journal optimization theory applications vol. agarwal mustafa k-means projective clustering proceedings twenty-third sigmod-sigact-sigart symposium vidal favaro rank subspace clustering pattern recognition letters vol. shen online low-rank subspace clustering basis dictionary pursuit proc. international conference machine learning heckel bölcskei robust subspace clustering thresholding ieee trans. inf. theory vol. park caramanis sanghavi greedy subspace clustering advances neural information processing systems zhang szlam lerman median k-ﬂats hybrid linear modeling many outliers computer vision workshops leisch bagged clustering vienna university economics business tech. rep. minaei-bidgoli topchy punch ensembles partitions data resampling information technology coding computing tumer agogino ensemble clustering voting active clusters pattern recognition letters vol. topchy jain punch clustering ensembles models consensus weak partitions ieee transactions pattern analysis probabilistic consensus clustering using evidence accumulation machine learning vol. weiss jordan spectral clustering analysis algorithm proc. neural information processing systems zhang balzano global convergence grassmannian gradient descent algorithm subspace estimation proceedings international conference artiﬁcial intelligence statistics ser. proceedings machine learning research gretton robert eds. vol. cadiz spain pmlr available http//proceedings.mlr.press/v/zhangb.html tomasi kanade shape motion image streams orthography int’l computer vision vol. heckel agustsson bolcskei neighborhood selection thresholding-based subspace clustering acoustics speech signal nene nayar murase columbia object image library columbia university tech. rep. columbia object image library columbia university tech. rep. huang graph regularized non-negative matrix factorization data representation ieee transactions pattern", "year": 2017}