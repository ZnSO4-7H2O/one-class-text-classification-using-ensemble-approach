{"title": "Neural Programmer: Inducing Latent Programs with Gradient Descent", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "Deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition, speech recognition, and sequence to sequence learning. However, this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, it has been shown that neural networks fail to learn to add two binary numbers reliably. In this work, we propose Neural Programmer, an end-to-end differentiable neural network augmented with a small set of basic arithmetic and logic operations. Neural Programmer can call these augmented operations over several steps, thereby inducing compositional programs that are more complex than the built-in operations. The model learns from a weak supervision signal which is the result of execution of the correct program, hence it does not require expensive annotation of the correct program itself. The decisions of what operations to call, and what data segments to apply to are inferred by Neural Programmer. Such decisions, during training, are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent. We find that training the model is difficult, but it can be greatly improved by adding random noise to the gradient. On a fairly complex synthetic table-comprehension dataset, traditional recurrent networks and attentional models perform poorly while Neural Programmer typically obtains nearly perfect accuracy.", "text": "deep neural networks achieved impressive supervised classiﬁcation performance many tasks including image recognition speech recognition sequence sequence learning. however success translated applications like question answering involve complex arithmetic logic reasoning. major limitation models inability learn even simple arithmetic logic operations. example shown neural networks fail learn binary numbers reliably. work propose neural programmer neural network augmented small basic arithmetic logic operations trained end-to-end using backpropagation. neural programmer call augmented operations several steps thereby inducing compositional programs complex built-in operations. model learns weak supervision signal result execution correct program hence require expensive annotation correct program itself. decisions operations call data segments apply inferred neural programmer. decisions training done differentiable fashion entire network trained jointly gradient descent. training model difﬁcult greatly improved adding random noise gradient. fairly complex synthetic table-comprehension dataset traditional recurrent networks attentional models perform poorly neural programmer typically obtains nearly perfect accuracy. past years seen tremendous success deep neural networks variety supervised classiﬁcation tasks starting image recognition speech recognition dnns ﬁxed-length input output. recently success translated applications involve variable-length sequence input and/or output machine translation image captioning conversational modeling end-to-end end-to-end speech recognition results strongly indicate models capable learning fuzzy underlying patterns data similar impact applications involve crisp reasoning. major limitation models inability learn even simple arithmetic logic operations. example joulin mikolov show recurrent neural networks fail task adding binary numbers even result less bits. makes existing models unsuitable downstream applications require complex reasoning e.g. natural language question answering. example answer question many states border texas? algorithm perform counting table something neural network good fairly common method solving problems program induction goal program correctly solve task. application models semantic parsing task build natural language interface structured database problem often formulated mapping natural language question executable query. drawback existing methods semantic parsing difﬁcult train require great deal human supervision. space programs non-smooth difﬁcult apply simple gradient descent; often gradient descent augmented complex search procedure sampling simplify training algorithmic designers manually supervision signals models form annotation complete program every question domain-speciﬁc grammar example designing grammars contain rules associate lexical items correct operations e.g. word largest operation argmax produce syntactically valid programs e.g. disallow program dog. role hand-crafted grammars crucial semantic parsing also limits general applicability many different domains. recent work wang build semantic parsers domains authors hand engineer separate grammar domain. goal work develop model require substantial human supervision broadly applicable across different domains data sources natural languages. propose neural programmer neural network augmented small basic arithmetic logic operations trained end-to-end using backpropagation. formulation neural network several steps using recurrent neural network. step select segment data source particular operation apply segment. neural network propagates outputs forward every step form ﬁnal complicated output. using target output adjust network select right data segments operations thereby inducing correct program. approach selection process done differentiable fashion whole neural network trained jointly gradient descent. test time replace soft selection hard selection. figure architecture neural programmer neural network augmented arithmetic logic operations. controller selects operation data segment. memory stores output operations applied data segments previous actions taken controller. controller runs several steps thereby inducing compositional programs complex built-in operations. dotted line indicates controller uses information memory make decisions next time step. combining neural network mathematical operations utilize fuzzy pattern matching capabilities deep networks crisp algorithmic power traditional programmable computers. approach using augmented logic arithmetic component reminiscent idea using conventional computer loosely related symbolic numerical processing abilities exhibited intraparietal sulcus area brain work also inspired success soft attention mechanism application learning neural network control additional memory component neural programmer attractive properties. first learns weak supervision signal result execution correct program. require expensive annotation correct program training examples. human supervision effort form question data source answer triples. second neural programmer require additional rules guide program search making general framework. neural programmer algorithmic designer deﬁnes list basic operations requires lesser human effort previous program induction techniques. experiment synthetic table-comprehension dataset consisting questions wide range difﬁculty levels. examples natural language translated queries include print elements column whose ﬁeld column greater ﬁeld column less what difference elements column number rows table?. lstm recurrent networks lstm models attention work well. neural programmer however completely solve task achieve greater accuracy cases inducing required latent program. training model difﬁcult greatly improved injecting random gaussian noise gradient enhances generalization ability neural programmer. even though model quite general paper apply neural programmer task question answering tables task previously attempted neural networks. implementation task neural programmer total time steps chosen advance induce compositional programs operations. model consists four modules figure implementation neural programmer task question answering tables. output model time step obtained applying operations data segments weighted probabilities. ﬁnal output model output time step dotted line indicates input history step question module converts question tokens distributed representation. basic version model simple parameterized question last hidden state used question representation figure question module process input question. denotes question representation used neural programmer. consider input question containing words question module performs following computations represents embedded representation word represents concatenation vectors question rd×d recurrent matrix question tanh element-wise non-linearity function representation question. pre-process question removing numbers storing numbers separate list. along numbers store word appeared left question useful compute pivot values comparison operations described section tasks involve longer questions bidirectional since simple unidirectional trouble remembering beginning question. bidirectional used question representation obtained concatenating last hidden states two-ends bidirectional rnns. question representation denoted selector produces probability distributions every time step probablity distribution operations another probability distribution columns. inputs selector question representation question module output history time step stores information operations columns selected model previous step. operation represented using d-dimensional vector. number operations ro×d matrix storing representations operations. operation selection performed rd×d parameter matrix operation selector produces probability distribution selector also produces probability distribution columns every time step. obtain vector representations column names using parameters question module word embedding phrase embedding. rc×d matrix storing representations column names. data selection performed neural programmer currently supports types outputs scalar output list items selected table ﬁrst type output questions type elements column second type output questions type print elements column greater facilitate this model maintains kinds output variables every step scalar answert lookup answert m×c. output lookup answert stores probability element table part output. ﬁnal output model scalar answert lookup answert depending whichever updated time steps. apart output variables model maintains additional variable selectt updated every time step. variables selectt maintain probability selecting allows model dynamically select subset rows within column. output initialized zero select variable initialized neural programmer built-in operations access outputs model every time step current time step operations access enables model build powerful compositional programs. important design operations work probabilistic column selection model differentiable. table shows list operations built model along deﬁnitions. reset operation selected number times required allows model induce programs whose complexity less steps. deﬁnitions operations fairly straightforward comparison operations greater lesser require pivot value input appears question. numbers appear question. every comparison operation compute pivot value adding numbers question weighted probabilities assigned computed using hidden vector position left number operation’s embedding vector. precisely vector representation operation rn×d matrix storing hidden vectors question positions left occurrence numbers. overloading deﬁnition denote probability assigned selector operation column time step respectively. figure show output selector variables computed. output selector variables step obtained additively combining output individual operations different data segments weighted corresponding probabilities assigned model. figure output selector variables obtained applying operations data segments additively combining outputs weighted using probabilities assigned selector. disscusion concerned tables numeric entries. section describe neural programmer handles text entries input table. assume column contain either numeric text entries. example query what elements column whose ﬁeld column word ﬁeld column word?. words query looking text entries column match speciﬁed words questions. answer queries text match operation updates selector variable appropriately. implementation parameters vector representations column’s text entries shared question module. text match operation uses two-stage soft attention mechanism back forth text entries question module. following explain implementation detail. columns text entries store vector representations text entries. ﬁrst stage question representation coarsely selects appropriate text entries sigmoid operation. concretely coarse selection given sigmoid product vector representations text entries question representation allow different words question matched corresponding columns column name representations obtain column representations make representation also sensitive column name. second stage compute attention hidden states question attention vector column input table. concretely compute product hidden states question obtain scalar values. two-stage mechanism required since experiments simply averaging vector representations fails make representation column speciﬁc enough question. unless otherwise stated experiments input tables whose entries numeric case model contain text match operation. history keeps track previous operations columns selected selector module model induce compositional programs. information encoded hidden vector history time step helps selector module induce probability distributions operations columns taking account previous actions selected model. figure shows details component. input history time step obtained concatenating weighted representations operations column names corresponding probability distribution produced selector step precisely parameters model include parameters question question parameters history history word embeddings operation embeddings operation selector column selector matrices respectively. training depending whether answer scalar lookup table different loss functions. |scalar answer absolute difference predicted true answer huber constant treated model hyper-parameter. experiments using square loss makes training unstable using absolute loss makes optimization difﬁcult near non-differentiable point. answer list items selected table convert answer }m×c indicates whether element part output. case log-loss elements table given number training examples lookup scalar lookup loss example boolean random variable true example’s answer scalar false answer lookup hyper-parameter model allows weight loss functions appropriately. inference time replace three softmax layers model conventional maximum operation ﬁnal output model either scalar answert lookup answert depending whichever among updated time steps. algorithm gives high-level view neural programmer inference. neural programmer faced many challenges speciﬁcally model learn parameters different modules delayed supervision steps? exhibit compositionality generalizing unseen questions? question module handle variability ambiguity natural language? experiments mainly focus answering ﬁrst questions using synthetic data. reason using synthetic data easier understand model synthetic dataset. generate data large quantity whereas biggest real-word semantic parsing datasets know contains training examples small neural network standards. experiments introduce simple word-level variability simulate aspect difﬁculties dealing natural language input. algorithm high-level view neural programmer inference stage input example. input table rm×c question initialize scalar answer lookup answer select history vector preprocessing remove numbers question store list along words compute history vector passing input history operation selection using operation representations data selection table using column representations update scalar answert lookup answert select using selected operation columns respectively. elements table uniformly randomly sampled training test time respectively. number rows sampled randomly training prediction number rows question test unique i.e. generated distinct template. following settings single column ﬁrst perform experiments single column enables different question templates answered using time steps. many columns increase difﬁculty experimenting multiple columns training number columns randomly sampled test time every question maximum number columns used training. variability simulate aspect difﬁculties dealing natural language input consider multiple ways refer operation text match consider cases columns input table contain text entries. small vocabulary words column uniformly randomly sampling them. ﬁrst experiment text entries table always contains columns text numeric entries next experiment example columns containing numeric entries columns containing text entries training. test time examples contain columns numeric entries columns text entries. following benchmark performance neural programmer various versions table-comprehension dataset. slowly increase difﬁculty task changing table properties question properties discuss comparison neural programmer lstm lstm attention. time steps experiments neural programmer trained mini-batch stochastic gradient descent adam optimizer parameters initialized uniformly randomly within range experiments mini-batch size dimensionality initial learning rate momentum hyper-parameters adam default values found extremely useful random gaussian noise gradients every training step. acts regularizer model allows actively explore programs. schedule inspired welling every step sample gaussian mean variance= curr step−.. prevent exploding gradients perform gradient clipping scaling gradient norm exceeds threshold threshold value picked tune hyper-parameter adam huber constant using grid search. performing experiments multiple random restarts performance model stable respect gradient clipping threshold tune different random seeds. table summary performance neural programmer various versions synthetic table-comprehension task. prediction model considered correct equal correct answer ﬁrst decimal place. last column indicates percentage question templates test observed training. unseen question templates generate questions containing sequences words model never seen before. model generalize unseen question templates evident -columns word variability -columns text match columns experiments. indicates neural programmer powerful compositional model since solving unseen question templates requires performing sequence actions never done training. training consists triples experiments. table shows performance neural programmer synthetic data experiments. single column experiments model answers questions correctly manually verify inspecting programs induced model. many columns experiments columns bidirectional columns additionally perform attention question every time step using history vector. model able generalize unseen question templates considerable fraction columns experiment. also seen word variability experiment columns text match experiment columns two-thirds test contains question templates unseen training. indicates neural programmer powerful compositional model since solving unseen question templates requires inducing programs appear training. almost errors made model questions require difference operation used. table shows examples model selects operation column every time step three test questions. apply three-layer sequence-to-sequence lstm recurrent network model lstm model attention explore multiple attention heads cases placing input table question. consider simpler version single column dataset questions scalar answers. number elements column uniformly randomly sampled table example outputs model time steps three questions test set. show synthetically generated question along natural language translation. question model takes steps step selects operation column. pivot numbers comparison operations computed performing steps. show selected columns cases selected operation acts particular column. figure effect adding random noise gradients versus adding experiment columns hyper-parameters same. models trained noise generalizes almost always better. elements sampled best accuracy using models close spite relatively easier questions supplying fresh training examples every step. scale input numbers changed test time accuracy drops neural programmer solves task achieves accuracy using training examples. since hardmax operation used test time answers neural programmer invariant scale numbers length input. program induction studied context semantic parsing natural language processing. pasupat liang develop semantic parser hand engineered grammar question answering tables natural language questions. methods piantadosi eisenstein clarke learn compositional semantic model without hand engineered compositional grammar still requiring hand labeled lexical mapping words operations. poon develop unsupervised method semantic parsing requires many pre-processing steps early work using neural networks learning context free grammar context sensitive grammar small problems. neelakantan learn simple horn clauses large knowledge base using rnns. neural networks also used datasets require complicated arithmetic logic reasoning work augmenting neural networks additional memory aware work augments neural network operations enhance complex reasoning capabilities. work submitted arxiv neural programmer-interpreters method learns induce programs supervision entire program proposed. followed neural enquirer similar work tackles problem synthetic table however method achieves perfect accuracy given supervision entire program. later dynamic neural module network proposed question answering uses syntactic supervision form dependency trees. develop neural programmer neural network model augmented small arithmetic logic operations perform complex arithmetic logic reasoning. model trained end-to-end fashion using backpropagation induce programs requiring much lesser sophisticated human supervision prior work. general model program induction broadly applicable across different domains data sources languages. experiments indicate model capable learning delayed supervision exhibits powerful compositionality. dastjerdi mohammad ozker muge foster brett rangarajan vinitha parvizi josef. numerical processing human parietal cortex experimental natural conditions. nature communications hannun awni case carl casper jared catanzaro bryan diamos greg elsen erich prenger ryan satheesh sanjeev sengupta shubho coates adam andrew deep speech scaling end-to-end speech recognition. arxiv preprint arxiv. hinton geoffrey deng dong dahl george rahman mohamed abdel jaitly navdeep senior andrew vanhoucke vincent nguyen patrick sainath tara kingsbury brian. deep neural networks acoustic modeling speech recognition. signal processing magazine kucian karin loenneker thomas dietrich thomas dosch mengia martin ernst aster michael. impaired neural networks approximate calculation dyscalculic children functional study. behavioral brain functions kumar ankit irsoy ozan jonathan bradbury james english robert pierce brian ondruska peter gulrajani ishaan socher richard. anything dynamic memory networks natural language processing. arxiv kelvin jimmy kiros ryan kyunghyun courville aaron salakhutdinov ruslan zemel richard bengio yoshua. show attend tell neural image caption generation visual attention. icml count print greater lesser greater count lesser count greater print lesser print greater lesser lesser greater greater lesser lesser greater greater lesser count lesser greater count greater lesser count lesser greater count greater lesser print lesser greater print greater lesser print lesser greater print diff count count diff table question templates single column experiment. four categories questions simple aggregation comparison logic arithmetic ﬁrst sample categories uniformly randomly program within category equally likely. word variability experiment columns sampled programs uniformly randomly since greater test questions unseen training using procedure. total total count count many greater greater than bigger bigger than larger larger lesser lesser than smaller smaller than print display show difference difference greater greater total greater total greater greater greater total greater total greater bigger bigger total bigger total bigger bigger bigger total bigger total bigger larger larger total larger total larger larger larger total larger total larger", "year": 2015}