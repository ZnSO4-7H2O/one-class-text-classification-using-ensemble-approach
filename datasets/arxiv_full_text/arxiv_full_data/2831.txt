{"title": "Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical  Care", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Patients in the intensive care unit (ICU) require constant and close supervision. To assist clinical staff in this task, hospitals use monitoring systems that trigger audiovisual alarms if their algorithms indicate that a patient's condition may be worsening. However, current monitoring systems are extremely sensitive to movement artefacts and technical errors. As a result, they typically trigger hundreds to thousands of false alarms per patient per day - drowning the important alarms in noise and adding to the exhaustion of clinical staff. In this setting, data is abundantly available, but obtaining trustworthy annotations by experts is laborious and expensive. We frame the problem of false alarm reduction from multivariate time series as a machine-learning task and address it with a novel multitask network architecture that utilises distant supervision through multiple related auxiliary tasks in order to reduce the number of expensive labels required for training. We show that our approach leads to significant improvements over several state-of-the-art baselines on real-world ICU data and provide new insights on the importance of task selection and architectural choices in distantly supervised multitask learning.", "text": "figure schematic overview described problem setting critical care. alarm brought attention clinical staff alarm classiﬁcation algorithm could analyse recent window full available data streams order identify whether alarm likely caused artefact technical error therefore reported lower degree urgency. intelligent alarm classiﬁcation system could potentially reduce burden large subset false alarms assessing alarms likely caused either artefact technical error reporting alarms lower degree urgency roadblocks prevented adoption machine learning task heterogeneity monitoring systems requirement extremely high speciﬁcity avoid suppressing important alarms prohibitively high cost associated obtaining representative clinically validated labels manifold alarm types monitoring system conﬁgurations hospitals. present semi-supervised approach false alarm reduction automatically identiﬁes incorporates large amount distantly supervised auxiliary tasks order signiﬁcantly reduce number expensive labels required training. demonstrate real-world data approach able correctly classify alarms originating artefacts technical errors better several state-of-the-art methods semi-supervised learning using labelled samples. besides importance clinical practice results highlight power distant multitask supervision ﬂexible effective tool learning unlabelled data readily available shed light semi-supervised learning beyond low-resolution image benchmark datasets. patients intensive care unit require constant close supervision. assist clinical staff task hospitals monitoring systems trigger audiovisual alarms algorithms indicate patient’s condition worsening. however current monitoring systems extremely sensitive movement artefacts technical errors. result typically trigger hundreds thousands false alarms patient drowning important alarms noise adding exhaustion clinical staff. setting data abundantly available obtaining trustworthy annotations experts laborious expensive. frame problem false alarm reduction multivariate time series machine-learning task address novel multitask network architecture utilises distant supervision multiple related auxiliary tasks order reduce number expensive labels required training. show approach leads signiﬁcant improvements several state-of-theart baselines real-world data provide insights importance task selection architectural choices distantly supervised multitask learning. introduction false alarms enormous mental burden clinical staff extremely dangerous patients alarm fatigue desensitisation lead clinically important alarms missed reportedly several hundreds deaths year associated false alarms patient monitoring united states alone institute robotics intelligent systems zurich switzerland neurocritical care unit department neurosurgery university hospital zurich switzerland. correspondence patrick schwab <patrick.schwabhest.ethz.ch>. introduce dsmt-nets novel neural architecture built idea utilising distant supervision multiple auxiliary tasks order better harness unlabelled data. present methodology selecting large related auxiliary tasks time series data training procedure dsmt-nets counteracts adverse gradient interactions auxiliary tasks main task. perform extensive quantitative experiments real-world dataset consisting almost alarms order evaluate relative classiﬁcation performance label efﬁciency dsmt-nets compared several state-of-the-art methods. related work background. driven widespread efforts automate patient monitoring recent surge works applying machine learning vast amounts data generated icus. notable driver mimic dataset made data accessible large number researchers. related works have example explored multivariate data tasks mortality modelling illness assessment forecasting diagnostic support patient state prediction learning weaning policies mechanical ventilation applying machine-learning approaches clinical physiological data challenging heterogenous noisy confounded sparse high temporal resolution long periods time. properties stark contrast many benchmark datasets machinelearning approaches typically developed evaluated several works therefore deal adapting existing machine-learning approaches idiosyncrasies clinical physiological data missingness long-term temporal dependencies noise heterogeneity sparsity build several innovations work. alarm fatigue. physionet challenge false alarm reduction electrocardiography monitoring notable efforts date address issue false alarms physiological monitoring. within challenge researchers introduced several highly effective approaches reducing false alarm rate arrhythmia alerts signals. however clinicians monitor arrhythmias many adverse events using multitude different monitoring systems. typically monitoring systems operate isolation single biosignal trigger distinct sets alarms. previous research shown opportunity data biosignals identify false alarms related waveforms therefore believe comprehensive technical solution alarm fatigue requires holistic approach accounts monitoring setup whole rather targeting speciﬁc systems alarms isolation. distant supervision multitask learning. multitask learning rich history healthcare applications example used risk prediction neonatal intensive care drug discovery prediction clostridium difﬁcile leveraging multitask learning improve label-efﬁciency learn jointly complementary unsupervised auxiliary tasks along supervised main task. existing literature refers concept applying indirect supervision auxiliary tasks label-efﬁciency additional predictive performance weak supervision distant supervision self-supervision particular used distantly supervised multitask learning increase predictive performance computer vision four hand-engineered auxiliary tasks. using auxiliary tasks addition main task also shown promising approach reinforcement adversarial learning recently proposed outputs model different points training varying amounts regularisation additional unsupervised targets main task. contrast existing works present ﬁrst approach distantly supervised multitask learning automatically identiﬁes large related auxiliary tasks multivariate time series jointly learn labelled unlabelled data. addition approach scales hundreds auxiliary tasks end-toend trained neural network. semi-supervised learning. beside distant supervision state-of-the-art approaches semi-supervised learning neural networks include broadly methods based reconstruction objectives variational autoencoders ladder networks adversarial learning however standard benchmarks consisting primarily low-resolution image datasets unclear degree method’s results generalise heterogenous long-term high-resolution time series datasets informative missingness commonly encountered healthcare applications. figure dsmt-net architectures without three auxiliary tasks. number horizontally aligned multitask blocks variable. multitask block hosts auxiliary task additional bypass connection gives head block direct access concatenated hidden states perception blocks perception block operates input data stream model incorporates binary missing indicators perception block handle situations input data streams missing. distantly supervised multitask networks distantly supervised multitask networks end-to-end trained neural networks process heterogenous input data streams order solve multitask learning problem main task auxiliary tasks designed augment main task. conceptually dsmt-net consists following components perception block input data streams variable number multitask blocks single head block block types neural network parameters arbitrary architectures hyperparameters. role perception blocks extract hidden feature representation respective input data streams separate perception blocks input stream order able model dynamic potentially missing input data streams. allow model learn missingness patterns follow accompany perception block missing indicator data stream present missing. additionally perform zero-imputation missing perception blocks’ features hpi. concatenate features extracted perception blocks corresponding missing indicators joint feature representation input data streams follows streams serves input higher level multitask blocks head block. main role multitask blocks host auxiliary tasks multitask blocks aligned parallel order minimise distance gradients propagate joint feature representation head block. output multitask block produces hidden high-level feature representation compared straightforward approach directly appending auxiliary tasks head block positioning multitask blocks head block achieves separation concerns. dsmt-nets head block focuses learning hidden feature representation optimised solely main task rather forced learn joint feature representation performs well multiple possibly competing tasks. head block computes ﬁnal model output processes hidden feature representations multitask blocks combinator function addition hidden feature representations multitask blocks head block retains direct access bypass connection. motivate inclusion bypass connection desire learn hidden feature representations multitask blocks information mathematically formulate head block follows combinator function. dsmt-nets combinator function integrates data ﬂows multitask blocks’ hidden representations well joint feature representation propose combinator function consists single hidden-layer multilayer perceptron dimensionality twice single multitask block’s feature representation. input receives concatenation feature representations integrated important questions distantly supervised learning identify suitable auxiliary tasks. common choice auxiliary task unsemi-supervised learning reconstruction feature and/or hidden representation space. several modern semi-supervised methods take approach reconstruction convenient choice auxiliary task because generically applicable input data neural architecture predictive task. however given recent empirical successes distant supervision speciﬁcally engineered auxiliary tasks reason related tasks might better choice auxiliary task semi-supervised learning reconstruction using multiple diverse auxiliary tasks might effective since predictive feature main task also good auxiliary task learning shared predictive representations follow simple twostep feature selection methodology automatically identify large auxiliary tasks closely related main task extract features large pool manuallydesigned features input time series. large wealth research manual feature engineering exist vast repositories features many data modalities e.g. time series examples features would e.g. autocorrelation different levels power spectral density speciﬁc frequency range. statistically test extracted features importance related main task order rank features estimated predictive potential determine relevance. suitable statistical test example hypothesis test correlation between labels ytrue extracted features using kendall’s using approach able identify large ranked list predictive features suitable target labels auxiliary tasks dsmt-nets. approaches choosing subset features auxiliary targets order feature importance randomly relevant features. main difference between approaches random selection higher expected task diversity similar tasks likely also rank similarly terms importance. arguments higher task diversity. therefore evaluate approaches experiments. problem training neural networks multiple tasks simultaneously using stochastic gradient descent gradients different tasks interfere adversely therefore completely disentangle training unsupervised supervised tasks dsmt-nets. instead training auxiliary tasks jointly main task alternate optimising dsmt-nets auxiliary tasks main task epoch starting auxiliary tasks. computational cost additional pass training two-step training procedure prevents potential adverse intra-step gradient interactions classes tasks. ensure similar convergence rates main auxiliary tasks weight auxiliary tasks total learning rate unsupervised supervised step approximately same i.e. weight auxiliary task auxiliary tasks. similar training schedule generator discriminator networks trained another iteration proposed train generative adversarial networks experiments performed extensive quantitative experiments realworld data using multitude different hyperparameter settings order answer following questions dsmt-nets perform terms predictive performance label efﬁciency multivariate false alarm detection relative state-of-the-art methods semi-supervised learning? answer question systematically evaluated dsmt-nets several baseline models terms area receiver operator curve using varying amounts manually classiﬁed labels nlabels varying amounts auxiliary tasks dsmt-nets. chose label subsets random without stratiﬁcation. comparison models’ performances using different levels labels enable judge label efﬁciency compared models i.e. level predictive performance achieve limited amount labels. also changing amount auxiliary tasks used models additionally able assess relationship number auxiliary tasks label efﬁciency predictive performance answer question performed ablation study using dsmt-nets auxiliary tasks using varying amounts manually classiﬁed labels base models. trained models without two-step training procedure addition evaluated performance deep highway network auxiliary tasks distributed sequentially among layers compare multitask learning depth width. lastly also evaluated multitask network auxiliary tasks placed directly head block process aimed determine relative importance individual design choices introduced section answer question compared predictive performance dsmt-nets using random selection signiﬁcant features determined feature selection methodology dsmt-nets selection order feature importance. dsmt-nets auxiliary tasks additionally assess whether importance auxiliary task selection sensitive number auxiliary tasks. zurich switzerland. data included continuous evenly-sampled waveforms obtained electrocardiography arterial blood pressure pulse oximetry intracranial pressure measurements. study collect make personal demographic clinical data prior diagnoses treatments electronic health records. obtain ground truth assessment alarms provided clinical staff user interface instructions annotating alarms believed caused artefacts technical error technical limitations exporting data biosignal database selected subset monitoring days patients highest amount manually labelled alarms analysis. evaluated dataset encompassed grand total alarms yielding average rate alarms patient day. number line alarm rates reported previous works alarms caused alarm-generating algorithm operating waveform derived either ecg-derived signal heart rate signal. annotations. whole alarms alarms manually labelled clinical staff observed period. used multiple annotators calibrated other’s assessments additionally conducted review annotations order ensure internal consistency annotations whole. review round found total annotations inconsistent. subsequently assigned corrected labels alarms. since label quality paramount model training validation suggest least label review round using majority vote committee labellers clear instructions order maintain sufﬁcient degree label consistency. recent large-scale labelling efforts physiological monitoring arrhythmias suggest even review rounds might necessary obtain gold standard labels. ﬁnal label annotated alarms labelled likely caused artefact technical error. note data collection effort scale input data extracted second window time frame immediately alarm triggered available biosignal. considered signal stream missing given alarm setting last recorded measurement type happened longer seconds ago. reduce computational resources required experiments resampled input data original sampling rate. preliminary evaluation signiﬁcant performance changes using higher sampling rate longer context window. additionally standardised extracted windows stream range using maximum minimum values encountered window. baselines. ensure fair reference used dsmtnets base architecture without horizontal blocks auxiliary tasks supervised baseline supervised baselines auxiliary tasks trained purely supervised manner labelled alarms only. baseline feature selection used automated feature extraction selection approach identify large number relevant time series features multivariate input data. note followed process separately distinct amount labels order avoid information leakage. features random forest classiﬁer consisting trees produce predictions mentioned section used feature selection approach identify suitable auxiliary tasks dsmt-nets. feature baseline therefore serves reference directly using identiﬁed signiﬁcant features make prediction. comparison state-of-the-art reconstructionbased semi-supervised learning evaluated ladder networks dataset. replaced dsmt-net components joint feature representation ladder network order comparable architecture also able model missingness heterogenous input streams. comparison state-of-the-art semi-supervised adversarial learning trained gans using semisupervised objective function feature matching dataset. type shown highly efﬁcacious semi-supervised learning low-resolution image datasets trained generator networks generate context window multiple high-resolution time series input dsmt-net discriminator without auxiliary tasks. terms architecture generator networks used strided upsampling convolutions. hyperparameters. ensure fair comparison used systematic approach hyperparameter selection evaluated neural network. trained model times random choice three variable hyperparameters bound ranges reset random seed value model order make search deterministic across training runs i.e. models evaluated exactly hyperparameter values. note setup guarantee optimality model however respect evaluated hyperparameters guarantees models evaluated fairly given amount scrutiny. train neural network models used learning rate ﬁrst epochs afterwards optimise binary cross-entropy main classiﬁcation output mean squared error auxiliary tasks. additionally used early stopping patience epochs. extra hyperparameters ladder networks noise level ﬁxed every layer denoising loss weight ﬁrst hidden layer every following hidden layer. models used base learning rate discriminator slightly increased learning rate generator counteract faster convergence discriminator networks. trained gans using early stopping patience main loss steps minimum steps. choose extra hyperparameters gans ladder networks followed original author’s published conﬁgurations adjusted slightly ensure converged. architectures. used conceptual architecture figure base architecture dsmt-nets. perception blocks employed resnets -dimensional convolutions time axis input data stream. head block multitask blocks used highway networks head block hosted sigmoid binary output indicated whether proposed alarm likely caused artefact. addition used batch normalisation dsmt-net blocks. table comparison maximum auroc value across distinct models trained using different sets hyperparameters varying amounts labels report auroc best encountered model calculated test alarms. best results column highlighted bold. predictive performance. overall found label limit purely supervised approaches consistently outperformed semi-supervised approaches labels. strongest approach using available labels label subset purely supervised feature baseline. compared methods dsmt-nets label-efﬁcient approach using labels. however feature matching outperformed dsmt-nets using labels. experimental setting best dsmt-nets yielded signiﬁcant improvements auroc reconstruction-based well adversarial state-of-the-art approaches semi-supervised learning low-resolution image benchmarks. relative improvements auroc amounted ladder networks feature matching gans labels respectively. note even na¨ıve multitask networks make adaptions introduced dsmt-nets exception cases outperformed ladder networks feature matching gans suggesting distant supervision general highly efﬁcacious approach semi-supervised learning domain. interestingly evaluated semi-supervised approaches exception na¨ıve multitask networks outperformed purely supervised counterparts lower amounts labels would expect many cases large margin. indeed feature matching gans well ladder networks eclipsed supervised baseline labels. suggests either feature matching gans ladder networks require higher degree hyperparameter optimisation evaluated approaches strengths approaches domain low-resolution images generalise degree domain multivariate high-resolution time series without adaptations. novel ﬁndings given recent evaluations state-of-the-art methods semi-supervised learning conﬁned solely low-resolution image domain. believe that future systematic replication studies presented work necessary evaluate degree methods generalise beyond benchmark datasets often cover many practically important data modalities time series data idiosyncrasies missingness heterogeneity sparsity noise. terms sensitivity speciﬁcity best models would able reduce number false alarms brought attention clinical staff degree urgency true alarms sensitivities using respectively labelled training samples speciﬁcity relative terms dsmtnets therefore labels able realise expected reduction false alarms feature trained labels. ﬁnding conﬁrms modest data collection effort would sufﬁcient achieve considerable improvement false alarm rates critical care. number auxiliary tasks. dsmt-nets auxiliary tasks selected feature importance auxiliary tasks achieved slightly better performances sufﬁcient amounts labels available. reason that head block trained labelled samples only greater number labels necessary effectively orchestrate extra information provided larger number multitask blocks. however behavior dsmt-nets auxiliary tasks selected random. here performances dsmt-nets auxiliary tasks comparable across label levels. importance adaptions. found using dsmtnets trained auxiliary tasks distributed depth performed worse proposed architecture demonstrating parallel alignment multitask blocks superior architectural design choice. similarly dsmt-net- variants without step training procedure consistently failed reach semi-supervised performance counterparts step training procedure enabled labels. shows disentangling training auxiliary main task played integral role strong semi-supervised performance dsmt-nets reinforces prior reports adverse gradient interactions challenge multitask learning neural networks task selection. found random selection cases outperformed selection order feature importance comparing dsmt-net- dsmt-net-r variants. believe result increased task diversity selecting random relevant auxiliary tasks similar features rank close terms feature importance. fact effect less pronounced models auxiliary tasks supports theory larger tasks automatically higher diversity limited amount highly similar features thus decreasing importance accounting diversity selection methodology. therefore conclude task diversity dominant factor selecting related auxiliary tasks distant multitask supervision. limitations false alarms solely technical problem organisational processual aspects must also considered comprehensibly address issue clinical care aspect question best manage alarms ﬂagged false alarm classiﬁcation system. reason that inherent possibility suppressing true alarm sensible approach would report errors lower degree urgency i.e. less pronounced sound rather completely suppressing another limitation work considered detection alarms caused either artefacts technical errors. alarms technically correct clinically require intervention another important source false alarms analyse work. identifying clinically false alarms signiﬁcantly harder caused artefacts technical errors clinical reasoning requires deep knowledge patient’s high-level physiological state well signiﬁcant amount domain knowledge. lastly presented distantly supervised approach semi-supervised learning highly efﬁcacious dataset applicability datasets hinges able determine multiple related auxiliary tasks. evaluated distantly supervised multitask learning time series data large numbers suitable auxiliary tasks readily available automated feature extraction selection hypothesise might trivial large repositories auxiliary tasks suitable distant multitask supervision data types. comparatively small number potential auxiliary tasks reported related works computer vision natural language processing finally experiments yield insights importance auxiliary task selection dsmt-nets theoretical analyses necessary understand exactly types auxiliary task useful degree distantly supervised multitask learning. conclusion present novel approach reducing false alarms using data obtained dynamic multiple heterogenous biosignal monitors. unlabelled data abundantly available obtaining trustworthy expert labels laborious expensive setting. introduce multitask network architecture leverages distant supervision multiple related auxiliary tasks order reduce number expensive labels required training. develop methodology automatically selecting auxiliary tasks multivariate time series well optimised training procedure prevents adverse gradient interactions tasks. using real-world critical care dataset demonstrate approach leads signiﬁcant improvements several stateof-the-art baselines. addition determine task diversity adverse gradient interactions concerns distantly supervised multitask learning. going forward believe approach could applicable wide variety machine-learning tasks healthcare obtaining labelled data major challenge. acknowledgements work partially funded swiss national science foundation project within national research program data swiss commission technology innovation project references aboukhalil anton nielsen larry saeed mohammed mark roger clifford gari reducing false alarm rates critical arrhythmias using arterial blood pressure waveform. journal biomedical informatics zhengping purushotham sanjay kyunghyun sontag david yan. recurrent neural networks multivariate time series missing values. arxiv preprint arxiv. cheng li-fang darnell gregory chivers corey draugelis michael engelhardt barsparse multi-output gaussian processes bara medical time series prediction. arxiv preprint arxiv. choi edward bahadori mohammad taha schuetz andy stewart walter jimeng. doctor predicting clinical events recurrent neural networks. machine learning healthcare conference christ maximilian kempa-liehr andreas feindt michael. distributed parallel time series feature extraction industrial data applications. arxiv preprint arxiv. clifford gari silva ikaro moody benjamin qiao kella danesh shahin abdullah kooistra tristan perry diane mark roger physionet/computing cardiology challenge reducing false arrhythmia alarms icu. computing cardiology ieee zihang yang zhilin yang cohen william salakhutdinov ruslan. good semi-supervised learning requires gan. advances neural information processing systems deriu gonzenbach maurice uzdilli fatih lucchi aurelien luca valeria jaggi martin. swisscheese semeval- task sentiment classiﬁcation using ensemble convolutional neural networks distant supervision. proceedings semeval drew barbara harris patricia z`egre-hemsey jessica mammone tina schindler daniel salas-boni rebeca yong tinoco adelita ding quan xiao. insights problem alarm fatigue physiologic monitor devices comprehensive observational study consecutive intensive care unit patients. plos ghassemi marzyeh naumann tristan doshi-velez finale brimmer nicole joshi rohit rumshisky anna szolovits peter. unfolding physiological state mortality modelling intensive care units. proceedings sigkdd international conference knowledge discovery data mining ghassemi marzyeh pimentel marco naumann tristan brennan thomas clifton david szolovits peter feng mengling. multivariate timeseries modeling approach severity illness assessment forecasting sparse heterogeneous clinical data. proceedings twenty-ninth aaai conference artiﬁcial intelligence clifford chengyu moody benjamin lehman silva ikaro qiao johnson mark roger classiﬁcation short single lead recording physionet computing cardiology challenge computing cardiology goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. advances neural information processing systems kaiming zhang xiangyu shaoqing jian. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition ramsundar bharath kearnes steven riley patrick webster dale konerding david pande vijay. massively multitask networks drug discovery. arxiv preprint arxiv. jaderberg mnih volodymyr czarnecki wojciech marian schaul leibo joel silver david kavukcuoglu koray. reinforcement learning unsupervised auxiliary tasks. arxiv preprint arxiv. kingma diederik mohamed shakir rezende danilo jimenez welling max. semi-supervised learning deep generative models. advances neural information processing systems lasko thomas denny joshua levy computational phenotype discovery using unsupervised feature learning noisy sparse irregular clinical data. plos lipton zachary kale david elkan charles wetzell randall. learning diagnose lstm recurrent neural networks. international conference learning representations lipton zachary kale david wetzel randall. directly modeling missing data sequences rnns improved classiﬁcation clinical time series. machine learning healthcare conference oquab maxime bottou l´eon laptev ivan sivic josef. object localization free? weakly-supervised learning convolutional neural networks. proceedings ieee conference computer vision pattern recognition prasad niranjani cheng li-fang chivers corey draugelis michael engelhardt barbara reinforcement learning approach weaning mechanical ventilation intensive care units. arxiv preprint arxiv. rasmus antti berglund mathias honkala mikko valpola harri raiko tapani. semi-supervised learning ladder networks. advances neural information processing systems saeed mohammed villarroel mauricio reisner andrew clifford gari lehman li-wei moody george heldt thomas kyaw moody benjamin mark roger multiparameter intelligent monitoring intensive care public-access intensive care unit database. critical care medicine salimans goodfellow zaremba wojciech cheung vicki radford alec chen improved techniques training gans. advances neural information processing systems saria suchi rajani anand gould jeffrey koller daphne penn anna integration early physiological responses predicts later illness severity preterm infants. science translational medicine schwab patrick scebba gaetano zhang delai marco karlen walter. beat beat classifying cardiac arrhythmias recurrent neural networks. computing cardiology bapst victor pascanu razvan heess nicolas quan john kirkpatrick james czarnecki wojciech hadsell raia. distral robust multitask reinforcement learning. advances neural information processing systems vincent pascal larochelle hugo bengio yoshua manzagol pierre-antoine. extracting composing robust features denoising autoencoders. proceedings international conference machine learning wiens jenna guttag john horvitz eric. patient risk stratiﬁcation time-varying parameters multitask learning approach. journal machine learning research", "year": 2018}