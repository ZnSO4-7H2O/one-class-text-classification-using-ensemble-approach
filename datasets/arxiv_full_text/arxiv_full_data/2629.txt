{"title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in  Neural Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Traditionally, multi-layer neural networks use dot product between the output vector of previous layer and the incoming weight vector as the input to activation function. The result of dot product is unbounded, thus increases the risk of large variance. Large variance of neuron makes the model sensitive to the change of input distribution, thus results in poor generalization, and aggravates the internal covariate shift which slows down the training. To bound dot product and decrease the variance, we propose to use cosine similarity or centered cosine similarity (Pearson Correlation Coefficient) instead of dot product in neural networks, which we call cosine normalization. We compare cosine normalization with batch, weight and layer normalization in fully-connected neural networks as well as convolutional networks on the data sets of MNIST, 20NEWS GROUP, CIFAR-10/100 and SVHN. Experiments show that cosine normalization achieves better performance than other normalization techniques.", "text": "input activation function incoming weight vector input vector also output vector previous layer indicates product. equation rewritten equation cosine angle euclidean norm vector. result product unbounded thus increases risk large variance. large variance neuron makes model sensitive change input distribution thus results poor generalization. large variance could also aggravate internal covariate shift slows training using small weights alleviate problem. weight decay normalization methods could decrease weights. batch normalization uses statistics calculated mini-batch training examples normalize result product layer normalization uses statistics layer single training case. variance constrained within certain range using batch layer normalization. weight normalization re-parameterizes weight vector dividing norm thus partially bounds result product. traditionally multi-layer neural networks product output vector previous layer incoming weight vector input activation function. result product unbounded thus increases risk large variance. large variance neuron makes model sensitive change input distribution thus results poor generalization aggravates internal covariate shift slows training. bound product decrease variance propose cosine similarity centered cosine similarity instead product neural networks call cosine normalization. compare cosine normalization batch weight layer normalization fully-connected neural networks well convolutional networks data sets mnist news group cifar-/ svhn. experiments show cosine normalization achieves better performance normalization techniques. deep neural networks received great success recent years many areas e.g. image recognition speech processing natural language processing game training deep neural networks nontrivial task. gradient descent commonly used train neural networks. however gradient vanishing problem works badly directly applying deep networks. institute computing technology chinese academy sciences university chinese academy sciences beijing academy frontier science technology. correspondence chunjie <luochunjieict.ac.cn> zhan jianfeng <zhanjianfengict.ac.cn> wang <wanglei ict.ac.cn> yang qiang <yangqiangmail.bafst.com>. methods widely used data mining machine learning particularly cosine similarity commonly used high dimensional spaces. example information retrieval text mining cosine similarity gives useful measure similar documents paper combine cosine similarity neural networks. cosine similarity instead product computing pre-activation. seen normalization procedure call cosine normalization. equation shows cosine normalization. extend centered cosine similarity pearson correlation coefﬁcient instead product. ignoring magnitude input activation function bounded higher learning rate could used training without risk large variance. moreover network cosine normalization trained batch gradient descent stochastic gradient descent since depend statistics batch mini-batch examples. compare cosine normalization batch weight layer normalization fully-connected neural networks mnist news group data sets. additionally convolutional networks different normalization techniques evaluated cifar-/ svhn data sets. brief summary large variance neuron neural network makes model sensitive change input distribution thus results poor generalization. moreover variance could ampliﬁed information moves forward along layers especially deep network. large variance could also aggravate internal covariate shift refers change distribution layer training parameters previous layers change internal covariate shift slows training layers need continuously adapt distribution. traditionally neural networks product compute pre-activation neuron. result product unbounded. result could value whole real space thus increases risk large variance. using small weights alleviate problem since pre-activation equation decreased small. weight decay normalization methods make weights small. weight decay adds extra term cost function penalizes squared value weight separately. normalization puts constraint maximum squared length incoming weight vector neuron. update violates constraint normalization scales vector incoming weights allowed length. objective original optimization problem changed using weight decay moreover bring additional hyper parameters carefully preset. batch normalization uses statistics calculated mini-batch training examples normalize pre-activation. normalized value rescaled re-shifted using additional parameters. since batch normalization uses statistics mini-batch examples effect dependent mini-batch size. overcome problem normalization propagation uses data-independent parametric estimate mean standard deviation layer normalization computes mean standard deviation layer single training case. weight normalization re-parameterizes incoming weight vector dividing norm. decouples length weight vector direction thus partially bounds result product. consider length input vector. methods bring additional parameters learned thus make model complex. important source inspiration work cosine similarity widely used data mining machine learning thoroughly bound product straight-forward idea cosine similarity. combine cosine similarity neural network details described next section. neural network cosine normalization. comparing batch normalization cosine normalization depend statistics batch mini-batch examples model trained batch gradient descent stochastic gradient descent. meanwhile cosine normalization performs computation forward propagation training inference times. procedure back propagation neural network cosine normalization ordinary neural network except derivative netnorm respect netnorm normalized pre-activation incoming weight vector input vector indicates product nonlinear activation function. cosine normalization bounds pre-activation result could even smaller dimension high. result variance controlled within narrow range. figure simple neural network. output hidden unit nonlinear transform product input vector incoming weight vector. computed cosine normalization output hidden unit computed result normalization needs rescaling re-shifting. therefore additional parameter learned hyper-parameter preset. however using activation functions like sigmoid tanh softmax result normalization re-valued fully utilize non-linear regime functions. implementing cosine normalization fullyconnected nets need divide norm incoming weight vector well norm input vector. input vector output vector previous layer. hidden units layer norm input vector. convolutional nets input vector constrained receptive ﬁeld. different receptive ﬁelds different norms. thing noticed cosine similarity measure similarity non-zero vectors since denominator zero. non-zero bias added avoid situation zero vector. adding bias non-zero. pointed centering inputs units help training neural networks. batch layer normalization centers data subtracting mean batch layer mean-only batch normalization enhance performance weight normalization pearson correlation coefﬁcient centered cosine similarity extend cosine normalization re-scaling parameter learned gradient descent. ignoring re-scaling parameter weight normalization could seen partial cosine normalization constrains weights. additionally dividing magnitude cosine normalization bounds pre-activation within narrower range thus makes lower variance neurons. moreover cosine normalization makes model robust different input magnitude. example forward procedure fully-connected network ~xl+ scale factor ~xl+ activation function relu ~xl+ linearly transmitted last layer. last layer softsteep nonlinearity softmax. example input vector softmax output distribution linearly transmitting input vector softmax becomes output distribution becomes supposing want recognize handwritten digit scaling whole digit factor bring valid information. words output distribution changed. using cosine normalization output distribution stable input magnitude varies depends angle input weight. scaling input derivative netnorm respect becomes equation comparing equation equation scaling input also makes gradient scaling weight normalization. cosine normalization shown equation scaling factor offset |λ~x| denominator. however three differences pearson correlation coefﬁcient layer normalization pearson correlation coefﬁcient constrains well layer normalization constrains thus pearson correlation coefﬁcient robust scaling shifting weight input. layer normalization computes mean standard deviation activation product pearson correlation coefﬁcient computes mean standard deviation product after activation. convolutional networks pearson correlation coefﬁcient calculates mean standard deviation receptive ﬁelds layer normalization calculates mean standard deviation whole layer. different receptive ﬁelds different mean standard deviation using pearson correlation coefﬁcient layer mean standard deviation using layer normalization. pointed layer normalization works well hidden units layer make similar contributions assumption similar contributions longer true convolutional networks. pearson correlation coefﬁcient needs assumption similar contributions receptive ﬁelds rather whole layer. reasonable convolutional networks. machine learning data mining lots metrics measure similarity distance different samples. among them cosine similar centered cosine heavily used many ﬁelds e.g. k-nearest neighbors classiﬁcation k-means clustering information retrieval item user based recommendation. also neural networks using similarity metrics output neurons e.g. radial basis function networks self-organizing training networks using back propagation hard build end-to-end deep networks using som. paper argues level abstraction product thus uses multi-layer perceptron learn convolution ﬁlter convolutional networks. since product decent metric directly metrics. know ﬁrst time cosine similarity pearson correlation coefﬁcient basic metric build end-to-end deep network trained back propagation. compare cosine normalization centered cosine normalization batch weight layer normalization fully-connected neural networks mnist news group data sets. additionally convolutional networks different normalization evaluated cifar- cifar- svhn data sets. also test networks without normalization fully-connected convolutional. results much mnist data consists pixel handwritten digit black white images. task classify images digit classes. training images test images mnist data set. scale pixel values range inputting models. original training contains text documents test contains text documents. document classiﬁed topic convenience using mini-batch gradient descent examples training examples test randomly dropped. result training examples test examples experiments. words whose document frequency larger used input features. feature dimensions ﬁnally. then model term frequency-inverse document frequency used transform text documents vectors. that feature re-scaled range cifar- data natural images -classes images training testing. cifar- similar cifar- classes. augment data images cropped pixels centrally evaluation randomly training. then series random distortions applied randomly image left right. randomly distort image brightness. randomly distort image contrast. procedure augmentation cifar- example tensorﬂow street view house numbers dataset includes images testing images. similar mnist goal classify digit centered image. augment data using procedure cifar-/ mentioned above. results test error mnist shown figure converging speeds different normalization techniques close. observation also true data sets present next. cosine normalization accelerate training networks well normalization. also observe centered cosine normalization cosine normalization achieve similar test errors slightly better layer normalization. table shows mean variance test error last epochs. centered cosine normalization achieves lowest mean test error cosine layer normalization achieve respectively. weight normalization highest test error comparing normalization. although batch normalization gets lowest test error point causes large variance test error training continues. large ﬂuctuation batch normalization caused change statistics different mini-batch examples. fully-connected neural network hidden layers used experiments mnist news group. hidden layer units. last layer softmax classiﬁcation layer -class mnist -class news group. evaluated convolutional networks shown table vgg-like architecture weighted layers evaluated experiments cifar-/ svhn. convolutional layer receptive ﬁeld stride pool layer regions stride relu activation function used hidden layers. weights randomly initialized truncated normal distribution mean variance. mini-batch gradient descent used train networks. batch size experiments fully-connected nets convolutional nets. experiments re-scaling re-shifting normalization hidden layers. however last layer re-scale normalized values inputting softmax. tried different learning rate normalization techniques found cosine normalization larger learning rate normalization techniques. learning rate cosine normalization centered cosine normalization batch normalization weight normalization layer normalization respectively experiments. exponential moving average parameters used inference convolutional networks. regularization dropout dynamic learning rate used. train fully-connected nets epochs convolutional nets step since performances improved anymore achieves second lowest test error batch normalization performs poorly task high dimensional text classiﬁcation. achieves test error. weight normalization layer normalization achieve close performances. batch weight normalization larger variance test error normalization. results cifar- shown figure table centered cosine normalization achieves lowest test error cosine normalization achieves second lowest test error layer normalization also achieves good performance better batch normalization experiment. achieves test error. batch normalization achieves test error still larger variance test error normalization. weight normalization achieves highest test error results cifar- shown figure table centered cosine normalization achieves lowest test error cosine normalization batch normalization achieve close performance respectively. batch normalization larger variance test error. weight normalization achieves highest test error", "year": 2017}