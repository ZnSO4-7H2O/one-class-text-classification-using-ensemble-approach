{"title": "Online and Distributed learning of Gaussian mixture models by Bayesian  Moment Matching", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "The Gaussian mixture model is a classic technique for clustering and data modeling that is used in numerous applications. With the rise of big data, there is a need for parameter estimation techniques that can handle streaming data and distribute the computation over several processors. While online variants of the Expectation Maximization (EM) algorithm exist, their data efficiency is reduced by a stochastic approximation of the E-step and it is not clear how to distribute the computation over multiple processors. We propose a Bayesian learning technique that lends itself naturally to online and distributed computation. Since the Bayesian posterior is not tractable, we project it onto a family of tractable distributions after each observation by matching a set of sufficient moments. This Bayesian moment matching technique compares favorably to online EM in terms of time and accuracy on a set of data modeling benchmarks.", "text": "gaussian mixture model classic technique clustering data modeling used numerous applications. rise data need parameter estimation techniques handle streaming data distribute computation several processors. online variants expectation maximization algorithm exist data efﬁciency reduced stochastic approximation e-step clear distribute computation multiple processors. propose bayesian learning technique lends naturally online distributed computation. since bayesian posterior tractable project onto family tractable distributions observation matching sufﬁcient moments. bayesian moment matching technique compares favorably online terms time accuracy data modeling benchmarks. gaussian mixture models simple expressive distributions often used soft clustering generally data modeling. traditionally parameters gmms estimated batch expectation maximization however datasets larger memory continuously streaming several online variants proposed process data sweep updating sufﬁcient statistics constant time observation however update approximate stochastic slows learning rate. furthermore clear distribute computation several processors given sequential nature updates. propose bayesian learning technique lends naturally online distributed computation. pointed bayes’ theorem applied observation update posterior online fashion dataset partitioned subsets processed different processors compute partial posteriors combined single exact posterior corresponds product partial posteriors divided respective priors. main issue bayesian learning posterior tractable compute represent. start prior consists product dirichlet several normal-wisharts parameters posterior becomes mixture products dirichlets normal-wisharts number mixture components grows exponentially number observations. keep computation tractable project posterior onto single product dirichlet normal-wisharts matching moments approximate posterior moments exact posterior. moment matching popular frequentist technique used estimate parameters model matching moments empirical distribution dataset moment matching bayesian setting project complex posterior onto simpler family distributions. instance type bayesian moment matching used expectation propagation however major limitation approach data point number terms posterior given increases factor summation number components. hence after data points posterior consist mixture terms intractable. paper describe bayesian moment matching technique helps circumvent problem. bayesian moment matching algorithm approximates posterior obtained iteration manner prevents exponential growth mixture terms achieved approximating distribution obtained posterior another distribution family distributions prior matching sufﬁcient moments ˜pn. make idea concrete following sections. distributions exists monomials knowing allows calculate parameters example gaussian distribution sufﬁcient moments means knowing allows estimate parameters characterize distribution. concept called method moments algorithm. eral passes data converging therefore restricted single pass necessarily incurs loss accuracy bayesian moment matching converges single pass. approximation moment matching also induces loss accuracy empirical results suggest less important loss incurred online finally lends naturally distributed computation case online rest paper structured follows. section discusses problem statement motivation online bayesian moment matching algorithm. section give brief background moment methods describe family distributions dirichlet normalwishart normal-gamma used priors work. review online algorithm online used parameter estimation gaussian mixture models. section presents bayesian moment matching algorithm approximate bayesian learning using moment matching. section demonstrates effectiveness online online distributed moment matching online empirical results synthetic real data sets. finally section concludes paper talks future work. given data instances data instance assumed sampled independently identically gaussian mixture model want estimate parameters gaussian mixture model online setting. precisely data points data point sampled gaussian mixture model components. parameters underlying gaussian mixture model denoted .... tuple ..... weight mean covariance matrix component gaussian mixture model. expressed tion based evaluation empirical moments dataset. previously used estimate parameters latent dirichlet allocation mixture models hidden markov models method moments moment matching technique also used bayesian setting computing subset moments intractable posterior distribution given subsequently another tractable distribution family distributions matches moments selected approximation intractable posterior distribution. gaussian mixture models dirichlet prior weights mixture normal-wishart distribution prior gaussian component. next give details dirichlet normal-wishart distributions including sufﬁcient moments. bayesian moment matching project posterior onto tractable family distribution matching sufﬁcient moments. ensure scalability desirable start family distributions conjugate prior pair multinomial distribution gaussian distribution unknown mean covariance matrix. product dirichlet distribution weights normal-wishart distribution mean covariance matrix gaussian component ensures posterior mixture products dirichlet normal-wishart distributions. subsequently approximate mixture posterior single product dirichlet normal-wishart distributions using moment matching. explain greater detail section ﬁrst describe brieﬂy normalwishart dirichlet distributions along sets sufﬁcient moments. normal-wishart distribution multivariate distribution four parameters. conjugate prior multivariate gaussian distribution unknown mean covariance matrix makes normalwishart distribution natural choice prior unknown mean precision matrix case. section deﬁned sufﬁcient moments characterize distribution. case normalwishart distribution would require least four different moments estimate four parameters characterize sufﬁcient moments case element matrix expressions sufﬁcient moments given dirichlet distribution family multivariate continuous probability distributions interval conjugate prior probability distribution multinomial distribution hence natural choice prior weights gaussian mixture model. sufﬁcient moments dirichlet distribution .... parameters dirichlet distribution batch expectation maximization often used practice learn parameters underlying distribution given data assumed derived. ﬁrst online variant proposed later modiﬁed improved several variants closer original batch algorithm. online updated parameter estimate produced observing data instance done replacing expectation step stochastic approximation maximization step left unchanged. limit online converges estimate batch allowed several iterations data. hence loss accuracy incurred restricted single pass data required streaming setting. components. independence assumption helps simplify expressions posterior. hence prior chosen product dirichlet distribution weights normal-gamma distributions precisely tuple discuss detail bayesian moment matching algorithm. approximates posterior observation fewer terms order prevent number terms grow exponentially. algorithm ﬁrst describe generic procedure approximate posterior observation simpler distribution moment matching. precisely moments sufﬁcient deﬁne matched moments exact posterior every iteration ﬁrst calculate exact posterior then compute moments sufﬁcient deﬁne distribution family next compute parameter vector based sufﬁcient moments. determines speciﬁc distribution family approximate note moments sufﬁcient approximate posterior exact posterior. however moments outside sufﬁcient moments necessarily same. next section illustrate algorithm learning parameters univariate gaussian mixture model. subsequently give algorithm general multivariate gaussian mixture models. section illustrate bayesian moment matching algorithm gaussian mixture models. dataset data points derived univariate gaussian mixture model density function given ﬁrst step choose appropriate family distributions prior conjugate prior probability distribution pair likelihood would desirable family distributions. make assumption every component independent sian mixture model section brieﬂy discuss general case multivariate gaussian mixture model. family distributions prior case becomes algorithm works manner shown before. however update equations would change accordingly. section presented expressions sufﬁcient moments normal-wishart distribution. using expressions approximate mixture products dirichlet normal-wishart distributions posterior single product dirichlet normal-wishart distributions previous section. finally estimate obtained observing data algorithm give algorithm bayesian moment matching gaussian mixture models. major advantages bayes’ theorem computation posterior distributed several machines processes subset data. also possible compute posterior distributed manner using bayesian moment matching algorithm. example assume machines data data points. machine compute approximate posterior product distributions product component summation form family distributions prior evident terms posterior grow factor iteration problematic. next step approximate mixture single product dirichlet normal-gamma distributions matching sufﬁcient moments i.e. using equations given approximate exact posterior posterior prior next iteration keep following steps iteratively ﬁnally distribution observing stream data estimate returned. here assumed number components known. practice however case. problem addressed taking large enough value learning model. although approach might lead overﬁtting maximum likelihood techniques online case reasonable approach since bayesian learning fairly robust overﬁtting. evaluated performance obmm sets real datasets moderate-small size datasets large datasets available publicly online machine learning repository function approximation repository. datasets span diverse domains. number attributes range order evaluate performance obmm compare oem. measure quality algorithms terms average log-likelihood scores held-out test datasets scalability terms running time. wilcoxon signed ranked test compute p-value report statistical signiﬁcance p-value less test statistical signiﬁcance results. computed parameters algorithm range components varying analysis report model log-likelihood test data stabilized showed signiﬁcant improvement obmm. step size stochastic approximation e-step number observations. evaluate performance online distributed moment matching dividing training datasets smaller data sets processing small datasets different machine. output machine collected combined give single estimate parameters model learned. table.log-likelihood scores data sets. best results among obmm highlighted bold font. indicates method signiﬁcantly better loglikelihoods online bayesian moment matching under wilcoxon signed rank test pvalue subsequently estimate obtained whole data set. therefore bayesian moment matching algorithm perform bayesian learning online distributed fashion. show section distributed bayesian moment matching performs favorably terms accuracy results huge speed-up running time. performed experiments synthetic real datasets evaluate performance online bayesian moment matching algorithm used synthetic datasets verify whether obmm converged true model given enough data. subsequently compared performance obmm online expectation maximization algorithm described compared obmm version oemsince shown perform best among various variants discuss experiments kinds datasets detail. evaluate performance obmm different synthetic data sets. data sets generated gaussian mixture model different number components lying range components having different number attributes range dimensions. data sampled data points. divided data training data instances testing instances. evaluate performance obmm calculated average log-likelihood model learned obmm data instance observed. figure shows plots performance obmm true model. subplot average log-likelihood vertical axis number observation horizontal axis. clear plots obmm converges true model likelihood nine cases given large enough data set. table shows average log-likelihood test sets obmm oem. obmm outperforms datasets. results show datasets obmm signiﬁcantly better log-likelihoods oem. table figure.performance analysis online bayesian moment matching algorithm gaussian mixture models synthetic datasets training instances testing instances. plot shows convergence log-likelihood model learned number observed data instances. plot clearly shows convergence true model. table show log-likelihood scores running times algorithm large datasets. terms loglikelihood scores obmm outperforms odmm datasets. while performance odmm expected worse obmm noticed performance odmm signiﬁcantly worse. encouraging light huge gains terms running time odmm obmm. table shows performance algorithm terms running times. odmm outperforms algorithms signiﬁcantly. also worth noting obmm performed better datasets. cial spanning diverse areas like physics molecular biology social networks health care trading markets name few. therefore become imperative develop algorithms process large data sets minimum time online fashion. paper explored online algorithms learn parameters gaussian mixture models. proposed online bayesian moment matching algorithm parameter learning demonstrated used distributed manner leading substantial gains running time. showed empirical analysis online bayesian moment matching algorithm converges true model outperforms online terms accuracy running time. also demonstrated distributing algorithm several machines results faster running times without signiﬁcantly compromising accuracy particularly advantageous running time major bottleneck. future would like develop online bayesian moment matching algorithm learn number components mixture model online fashion. work already done direction dirichlet process mixtures would desirable explore algorithm adapted learn number components. further proposed online gaussian mixture models extend work learn sum-product network continuous variables online manner.", "year": 2016}