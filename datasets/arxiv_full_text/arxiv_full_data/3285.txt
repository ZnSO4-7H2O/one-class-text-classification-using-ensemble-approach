{"title": "Caffe con Troll: Shallow Ideas to Speed Up Deep Learning", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "We present Caffe con Troll (CcT), a fully compatible end-to-end version of the popular framework Caffe with rebuilt internals. We built CcT to examine the performance characteristics of training and deploying general-purpose convolutional neural networks across different hardware architectures. We find that, by employing standard batching optimizations for CPU training, we achieve a 4.5x throughput improvement over Caffe on popular networks like CaffeNet. Moreover, with these improvements, the end-to-end training time for CNNs is directly proportional to the FLOPS delivered by the CPU, which enables us to efficiently train hybrid CPU-GPU systems for CNNs.", "text": "intel’s current haswell achieve flops single chip. moreover simd parallelism doubled last four intel generations likely continue. users cannot control footprint data center another issue amazon’s provides gpus neither azure google compute motivates study cnn-based systems across different architectures. conduct study forked caﬀe popular open-source system rebuilt internals produce system call caﬀe troll fully compatible end-to-end version caﬀe matches caﬀe’s output layer unit computation. reported literature conﬁrmed experiments bottleneck layers so-called convolutional layers consume execution time. although optimize layers using essentially techniques focus tradeoﬀ space convolutional layer cpus gpus. convolutional layer operates batches tensors. currently studies method performing convolution called lowering remaps high-dimensional input tensors series standard matrix multiplications. turn matrix multiplications executed using blas-compatible library openblas intel’s mkl. lowering used many state-of-the-art systems including caﬀe cudnn. previous approaches picked single lowering least three different ways matrices lowering operation. study reveals optimal strategy depends ratio input output channels convolution means lowering usually dominates others oﬀer experimental evidence fact propose simple automatic optimizer pick best lowering tradeoﬀ space automatically. popular networks optimal lowering contributes around execution time single layer performance improvement end-to-end execution. signiﬁcantly standard batching optimizations employed systems study reveals systems much faster often reported literature. using simple batching strategy present caﬀe troll fully compatible endto-end version popular framework caﬀe rebuilt internals. built examine performance characteristics training deploying general-purpose convolutional neural networks across diﬀerent hardware architectures. that employing standard batching optiimprovement caﬀe popular networks like caﬀenet. moreover improvements end-to-end training time cnns directly proportional flops delivered enables eﬃciently train hybrid cpu-gpu systems cnns. deep learning using convolution neural networks topic machine learning research basis staggering number consumer-facing data-driven applications including based object recognition voice recognition search deep learning likely major workload future data analytics applications. given recent resurgence cnns studies cnns data-systems perspective. database systems role here eﬃciency runtime cost chief concerns owners systems. contrast many analytics memory-bound calculations often compute-bound. thus processor technology plays role systems. gpus popular choice support cnns modern gpus offer tflops tflops however gpus connected host memory slow pci-e interconnect. hand microsoft’s project adam argues cpus deliver cost-eﬀective performance debate going interesting next generation gpus promise high-speed interconnection host mempermission make digital hard copies part work personal classroom granted without provided copies made distributed proﬁt commercial advantage copies bear notice full citation ﬁrst page. copy otherwise republish post servers redistribute lists requires prior speciﬁc permission and/or fee. copyright x-xxxxx-xx-x/xx/xx ..... figure three logical steps lowering process lowering transform tensors matrices multiply multiply result lifting transform back tensor representation build proportionality devices create hybrid cpu-gpu system. typically systems either gpu-based cpu-based–but both. debate reached almost religious levels. using argue cpus gpus simultaneously. ﬁrst hybrid system uses cpus gpus single layer. show instance even underpowered older core achieve higher throughput single convolutional layer. thus hybrid solutions become eﬀective homogeneous systems open questions provisioning systems. finally newly announced amazon instance gpus also show end-to-end speedups ﬁrst describe deﬁnition convolution operation technique called lowering popular implement convolution operation. describe three diﬀerent lowering techniques. standard image d-convolution many kernels indexed third index like kernels straightforward implementation operation suboptimal. transform tensor problem highlyoptimized matrix multiplication kernels. convolution type balanced. lowerings type represent extremes spectrum blowup either lowering phase lifting phase. natural middle point spectrum balances expense lowering lowering lifting take time space sits squarely approaches. expected matrix multiplication intermediate cost. study tradeoﬀs empirically appendix fusion. conceptually straightforward fuse three steps avoid materialization cost lowering; requires rewriting blas kernels. developed kernel preliminary experiments indicate improve performance paper report numbers without fusion discuss optimization further. section discusses partitioning batch partitions processing batch partitions parallel leads signiﬁcant speedups cpu. accomplish convolution matrix create lowering phase times larger images processed time. first study memory footprint performance related large batch execute matrix multiplication caﬀe uses batch size convolutions. means image lowering gemm done sequentially. smallest possible memory footprint needs maintain lowered matrix single memory; hand batch size takes times memory. shown figure convolutional layers diﬀerence memory footprint directly proportional devices limited memory gpus might favor large batch sizes. computationally however suﬀers lower hardware eﬃciency. figure shows speedup w.r.t. number cores diﬀerent batch sizes. batch size large shown figure machine physical cores observe almost linear speedup cores. vary batch size figure plot speedup smaller batch size lower speedup. compared using core. underlying reason lowered data matrix ‘thinner’ higher batch sizes. thinner matrices mean possible partition sizes underlying algorithm smaller kernel unable optimize example caches cannot ﬁlled blocking optimizations. result likely memory-bandwidth-bound higher batch sizes. phenomenon likely severe gemm kernel executed multiple threads. hence advocate simple strategy batch much possible note could mean processing entire batch threads used gemm partitioning batch partitions size threads used gemm. equivalent exactly blas parallelizes gemm partitioning partition columns batch partitioning strategy equivalent terms gemm coarse-grained perform lowering parallel similar batch partitioning employed parallelize layers. figure shows impact batch partitioning full end-to-end caﬀenet c.xlarge instance physical cores. batch size used images horizontal axis represents many parallel partitions partitioned images. none indicates default caﬀe implementation convolutions image processed serially layers full batch indicates images processed together number parallel partitions images equally split partitions layers processed partition parallel gemm performed parallel partition threads gemm. example point indicates partitions size convolutions lowering gemm done parallel partitions. currently consider data parallelism within layer decision fraction input send device. simple heuristic device takes fraction input fraction total flops device contributes. tflops tflops send figure end-to-end performance comparison across diﬀerent machines caﬀenet. numbers normalized speedup running caﬀe’s version g.xlarge instance evaluate compare caﬀe popular libraries cnns. systems neural network architectures caﬀenet default architecture benchmarking. compile caﬀe gcc-.. nvcc-.. openblas versions cublas shipped cuda versions. caﬀe imagenet datasets caffenet diverse machines illustrated figure systems take input network conﬁguration caﬀe provides. given random seed caﬀe generate output layer within small tolerance. thus concentrate throughput. caﬀe iterations compare output model layer. systems produce output within relative error. thus focus remaining experiments runtime performance. mostly caﬀe lowering single images time lowers batching. similar results obtained two-socket instance caﬀe lowering type observed type becomes faster type ratio input/output channels increases true conv diﬀerence small ning cores slightly slower running cores. instance provides peak ability tflops single-socket instance provides tflops. diﬀerence peak ﬂoating point operations corresponds performance diﬀerence caﬀe cct. stance given diﬀerence performance fact instance slightly cheaper instance. however number smaller order magnitude typically associated cpu-based deep learning. suggests that cloud services without instances e.g. microsoft azure google compute train deep learning workload pure version using cct. validate using cpus instance accelerate purely training. ﬁrst focus speed running convolution operation. implement version hybrid version that batch images runs subset others cpu. systems instance bridge cores report number figure system ﬁrst convolutional layer caﬀenet grouping finally figure presents end-to-end alexnet execution time g.xlarge instance gpus. caﬀe execution time iteration. adding gives speedup although expect number increase optimizations. gpus currently give simplex program national science foundation career award iis- oﬃce naval research awards national institutes health grant awarded national institute biomedical imaging bioengineering funds provided trans-nih data knowledge initiative sloan research fellowship moore foundation american family insurance google toshiba. opinions ﬁndings conclusions recommendations expressed material authors necessarily reﬂect views darpa afrl u.s. government. brieﬂy describe previous studies also focus improving eﬃciency deep learning primitives. although contributions paper leverage decades work high-performance computing omit discussion space constraints. cnns computationally expensive optimizing performance become well-studied problem recent years. popular libraries include caﬀe theano cudaconvnet cudnn compute convolutions many frameworks lowering idea proposed chellapilla takes advantage highly-optimized blas libraries. work follows line research instead explore tradeoﬀs diﬀerent types lowerings previously studied. anapproach computing convolutions recently gained attention fast fourier transform work also demonstrated interesting performance tradeoﬀs based size input hope incorporate additional optimizations future work. computing convolutions across series inputs. example chetlur demonstrate performance convolution operation parameterized dimensions; thus optimizing computation diﬃcult task. paper analyze sophisticated tradeoﬀ space detail; single ratio used characterize three lowering techniques. recently theano library embraced idea building so-called meta-optimizer code release. metaoptimizer would treat various approaches computing convolutions black-box solvers would select optimal approach given input. idea similar notion automatic optimizer; however intention understand tradeoﬀ space within particular strategy rather relying existing approaches. distributed deep learning. distributed systems deep learning popular topic including singa google’s distbelief microsoft’s project adam eﬀorts concentrate core challenges scheduling across diﬀerent nodes distributing model parameters across diﬀerent nodes. technique used approaches hogwild designed single node since extended distributed setting spirit work focuses improving performance context single node. future work also plan study training distributed setting believe eﬀorts single-node case lead performance gains distributed settings. model. figure vary respectively dimensions ﬁxed. strategy performs diﬀerently vary neither dominates other. would expect number output channels decreases lowering type outperforms lowering type vice versa. diﬀerence eﬃciency approaches order magnitude. relative performance diﬀerent lowering strategies determined ratio number input channels number output channels. figure demonstrates relative performance lowering type lowering type w.r.t. ratio between input channels output channels dimensions ﬁxed. ratio increases type outperforms type vice versa. allows choose strategy optimally current cnns ratio within narrow band. hence lowering major impact performance. scheduling results estimating fraction total flops device contributes. follow experiment protocol section vary ratio shown figure here denotes fraction jobs gpu. figure large small speedup cross-device scheduling less essence ﬁnishes early. empirically optimal achieved also label estimated using simple heuristic theoretical peak tflops device could deliver within optimal scheduling plan. also tried estimate using empirical tflops device gets result similar; speedup still within optimal summarize tradeoﬀ space analytically figure empirically figures matrix multiplication report cost openblas cubic input dimension. simplicity notation focus analyzing case large enough observation figure lowering type largest input size lowered data smallest output size matrix multiplication. lowering type between. constant lowering type involves blowup data size number input channels lowering type involves blowup data size number output channels. relative performance strategies depends ratio", "year": 2015}