{"title": "Hierarchically-Attentive RNN for Album Summarization and Storytelling", "tag": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "abstract": "We address the problem of end-to-end visual storytelling. Given a photo album, our model first selects the most representative (summary) photos, and then composes a natural language story for the album. For this task, we make use of the Visual Storytelling dataset and a model composed of three hierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album photos, select representative (summary) photos, and compose the story. Automatic and human evaluations show our model achieves better performance on selection, generation, and retrieval than baselines.", "text": "previous visual storytelling works classiﬁed types vision-based languagebased image language stories constructed respectively. among vision-based approaches unsupervised learning commonly applied e.g. learns latent temporal dynamics given large amount albums formulate photo selection sparse time-varying directed graph. however visual summaries tend difﬁcult evaluate selected photos agree human selections. languagebased approaches sequence natural language sentences generated describe photos. drive work collected dataset mined blog posts. however kind data often contains contextual information loosely related language. direct dataset recently released multi-sentence stories collected describing photo albums amazon mechanical turk. paper make visual storytelling dataset authors provide seqseq baseline deal task generating stories given representative photos hand-selected people album. instead focus challenging realistic problem end-toend generation stories entire albums. requires either generate story album’s photos learn selection mechanisms identify representative photos generate stories summary photos. evaluate type approach. address problem end-to-end visual storytelling. given photo album model ﬁrst selects representative photos composes natural language story album. task make visual storytelling dataset model composed three hierarchically-attentive recurrent neural nets encode album photos select representative photos compose story. automatic human evaluations show model achieves better performance selection generation retrieval baselines. since ﬁrst developed language humans always told stories. fashioning good story creativity developing algorithms replicate long running challenge. adding pictures input provide information guiding story construction offering visual illustrations storyline. related task image captioning methods generate descriptions individual images short videos depicting single activity. recently datasets introduced extend task longer temporal sequences movies photo albums type data consider paper provides input illustrations story generation form photo albums sampled minutes days time. type data generating textual descriptions involves telling temporally consistent story depicted visual information stories must coherent take account temporal context imconsisting three stages. ﬁrst encodes whole album context photo’s content second provides weights photo selection third takes weighted representation decodes resulting sentences. note training given full input albums output stories model needs learn summary photo selections latently. show model achieves better performance baselines automatic metrics human evaluations. side product show latent photo selection also reasonably mimics human selections. additionally propose album retrieval task reliably pick correct photo album given sequence sentences model also outperforms baselines task. related work recent years witnessed explosion interest vision language tasks reviewed below. visual captioning recent approaches image captioning used cnn-lstm structures generate descriptions. captioning video movie content sequence-to-sequence models widely applied ﬁrst sequence encodes video frames second sequence decodes description. attention techniques commonly incorporated tasks localize salient temporal spatial information. video summarization similar documentation summarization extracts sentences words video summarization selects frames shots. approaches unsupervised learning intuitive criteria pick salient frames recent models learn human-created summaries recently better exploit semantics proposed textually customized summaries. visual storytelling visual storytelling tries tell coherent visual textual story image set. previous works include storyline graph modeling unsupervised mining blog-photo alignment language retelling collects data mining blog posts collects stories using mechanical turk providing directly relevant stories. model model composed three modules album encoder photo selector story generator jointly learned training. album encoder given album composed photos bi-directional encode local album context photo. ﬁrst extract -dimensional visual representation photo using resnet bi-directional applied encode full album. following choose gated recurrent unit unit encode photo sequence. sequence output time step encodes local album context photo fused visual representation followed relu ﬁnal photo representation photo selector photo selector identiﬁes representative photos summarize album’s content. discussed assume given ground-truth album summaries training instead regarding selection latent variable end-to-end learning. inspired pointer networks another gru-rnn perform task figure model album encoder bi-directional gru-rnn encodes album photos; photo selector computes probability photo album-summary photo; ﬁnally story generator outputs sequence sentences combine tell story album. summarization step takes previous previous hidden state input outputs next hidden state ¯ht. fused photo representation compute test photo’s attention time simply pick photo highest probability summary photo step story generator generate album’s story given album representation matrix photo summary probabilities ﬁrst modules compute visual summary representation weighted album representations i.e. embeddings used decode story sentences respectively shown blue part fig. given story {st} t-th summary sentence. following donahue l-th word probability t-th sentence word embedding. takes joint input visual summarization previous word embedding previous hidden state outputs next hidden state. generation loss negative likelihoods correct words straint order sequence sentences within story story randomly shufﬂe sentences generate negative story instances apply max-margin ranking loss encourage correctly-ordered stories lrank) max)+log ﬁnal loss combination generation ranking losses visual storytelling dataset consisting albums photos. album contains photos taken within -hour span annotations album summarizations selected representative photos stories describing selected photos. task generate -sentence story describing album. compare model sequence-to-sequence baselines encoderdecoder model sequence album photos encoded last hidden state decoder story generation encoder-attention-decoder model weights computed using soft-attention mechanism. decoding time step weighted hidden states encoder decoded. fair comparison test variants model trained without ranking regularization controlling loss function denoted h-attn h-attn-rank evaluations model shown table h-attn outperforms baselines h-attnrank achieves best performance metrics. note beam-search beam size= generation reasonable performancespeed trade-off test performance optimal image selection ground-truth human-selected -photo-sets oracle hard-code photo selection denoted h-attn-rank. achieves slightly higher meteor compared end-to-end model. additionally also human evaluations forced-choice task people choose stories generated different methods. evaluation select albums evaluated turkers. results shown table experiments signiﬁcant preference model baselines. simple turing test also compare results human written stories indicating room improvement methods. also compute p-value meteor samples bootstrap test meteor better agreement human judgments bleu/rouge h-attn-rank model strong statistical signiﬁcance enc-dec enc-attn-dec models human-selected -photo stories). comparison evaluate enc-attn-dec task aggregating predicted attention selecting photos highest accumulated attention. additionally also dpp-based video summarization using album features. models higher performance compared baselines shown table output example analysis fig. fig. shows several output examples joint album summarization storytelling generation. compare full model h-attnrank baseline enc-attn-dec models able album summarization story generation tasks jointly. fig. fig. blue dashed indicate album summarization models respectively. reference also show groundtruth album summaries randomly selecting human album summaries highlighted green box. album generated stories. album retrieval given human-written story introduce task retrieve album described story. randomly select albums groundtruth story evaluation. using generation loss compute likelihood album given query story retrieve album highest generation likelihood argmaxamp. recallk median rank evaluation. shown table models outperform baselines ranking term eqn. improve performance signiﬁcantly. figure examples album summarization storytelling enc-attn-dec h-attn-rank ground-truth randomly select human album summaries ground-truth here. figure examples album summarization storytelling enc-attn-dec h-attn-rank ground-truth randomly select human album summaries ground-truth here. references harsh agrawal arjun chandrasekaran dhruv batra devi parikh mohit bansal. sort story sorting jumbled images captions stories. emnlp. jeffrey donahue lisa anne hendricks sergio guadarrama marcus rohrbach subhashini venugopalan kate saenko trevor darrell. long-term recurrent convolutional networks visual recognition description. cvpr. ting-hao kenneth huang francis ferraro nasrin mostafazadeh ishan misra aishwarya agrawal jacob devlin ross girshick xiaodong pushmeet kohli dhruv batra visual storytelling. naccl. kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhutdinov richard zemel yoshua bengio. show attend tell neural image caption generation visual attention. icml.", "year": 2017}