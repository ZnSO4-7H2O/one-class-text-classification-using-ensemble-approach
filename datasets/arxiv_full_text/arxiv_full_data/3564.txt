{"title": "On the Effectiveness of Least Squares Generative Adversarial Networks", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\\chi^2$ divergence. We also present a theoretical analysis about the properties of LSGANs and $\\chi^2$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. For evaluating the image quality, we train LSGANs on several datasets including LSUN and a cat dataset, and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. Furthermore, we evaluate the stability of LSGANs in two groups. One is to compare between LSGANs and regular GANs without gradient penalty. We conduct three experiments, including Gaussian mixture distribution, difficult architectures, and a new proposed method --- datasets with small variance, to illustrate the stability of LSGANs. The other one is to compare between LSGANs with gradient penalty and WGANs with gradient penalty (WGANs-GP). The experimental results show that LSGANs with gradient penalty succeed in training for all the difficult architectures used in WGANs-GP, including 101-layer ResNet.", "text": "abstract—unsupervised learning generative adversarial networks proven hugely successful. regular gans hypothesize discriminator classiﬁer sigmoid cross entropy loss function. however found loss function lead vanishing gradients problem learning process. overcome problem propose paper least squares generative adversarial networks adopt least squares loss function discriminator. show minimizing objective function lsgan yields minimizing pearson divergence. also present theoretical analysis properties lsgans divergence. beneﬁts lsgans regular gans. first lsgans able generate higher quality images regular gans. second lsgans perform stable learning process. evaluating image quality train lsgans several datasets including lsun dataset experimental results show images generated lsgans better quality ones generated regular gans. furthermore evaluate stability lsgans groups. compare lsgans regular gans without gradient penalty. conduct three experiments including gaussian mixture distribution difﬁcult architectures proposed method datasets small variance illustrate stability lsgans. compare lsgans gradient penalty wgans gradient penalty experimental results show lsgans gradient penalty succeed training difﬁcult architectures used wgans-gp including -layer resnet. even applied many real-world tasks image classiﬁcation object detection segmentation tasks obviously fall scope supervised learning means labeled data provided learning processes. compared supervised learning however unsupervised learning tasks generative models obtain limited impact deep learning. although deep generative models e.g. proposed models face difﬁculty intractable functions difﬁculty intractable inference turn restricts effectiveness models. recently generative adversarial networks demonstrated impressive performance unsupervised learning tasks. unlike deep generative models usually adopt approximation methods intractable functions inference gans require approximation trained endto-end differentiable networks. basic idea gans simultaneously train discriminator generator discriminator aims distinguish real samples kong hong kong. wang center optical imagery analysis learning school mechanical engineering northwestern polytechnical university xian china. smolley codehatch corp. edmonton alberta canada. generated samples; generator tries generate fake samples real possible making discriminator believe fake samples real data. plenty works shown gans play signiﬁcant role various tasks image generation image super-resolution semisupervised learning spite great progress gans image generation quality generated images gans still limited realistic tasks. regular gans adopt sigmoid cross entropy loss function discriminator argue loss function however lead problem vanishing gradients updating generator using fake samples correct side decision boundary still real data. figure shows fake samples update generator making discriminator believe real data cause almost error correct side i.e. real data side decision boundary. however samples still real data want pull close real data. based observation propose least squares generative adversarial networks adopt least squares loss function discriminator. idea simple powerful least squares loss function able move fake samples toward decision boundary least squares loss function penalizes samples long correct side decision boundary. figure shows least squares loss function penalize fake samples pull toward decision boundary even though correctly classiﬁed. based property lsgans able generate samples closer real data. fig. illustration different behaviors loss functions. decision boundaries loss functions. note decision boundary across real data distribution successful gans learning. otherwise learning process saturated. decision boundary sigmoid cross entropy loss function. orange area side real samples blue area side fake samples. gets small errors fake samples updating correct side decision boundary. decision boundary least squares loss function. penalizes fake samples result forces generator generate samples toward decision boundary. learning process. generally speaking training gans difﬁcult issue practice instability gans learning recently several papers pointed instability gans learning partially caused objective function speciﬁcally minimizing objective function regular suffers vanishing gradients makes hard update generator. lsgans relieve problem lsgans penalize samples based distances decision boundary generates gradients update generator. proposed method evaluate stability gans using difﬁcult architectures. however practice always select stable architectures tasks. inspired motivation propose difﬁcult datasets stable architectures evaluate stability gans. speciﬁcally create synthetic digit datasets small variance rendering digits using kind font. datasets small variance difﬁcult gans learn since discriminator distinguish real samples easily datasets small variance. gradient penalty proved effective improving stability gans training gradient penalty also able improve stability lsgans. adding gradient penalty lsgans able train successfully difﬁcult architectures used propose method evaluating stability gans. synthetic digit datasets small variance created published. train lsgans regular gans stable architecture synthetic datasets. lsgans succeed generating high quality digits regular gans suffer severe mode collapse problem. another comparison experiments also conducted prove stability lsgans. evaluate lsgans gradient penalty difﬁcult architectures used wgans-gp. experimental results show lsgans gradient penalty succeed training architectures including -layer resnet. paper extends previous conference work number ways. first present theoretical analysis properties lsgans divergence. second provide results dataset also shows lsgans able generate higher quality images regular gans. third propose method evaluating stability gans. addition using difﬁcult architectures propose difﬁcult datasets stable architectures evaluate stability gans. experiment also shows lsgans perform stable regular gans. finally present comparison experiment lsgans gradient penalty wgans gradient penalty results show lsgans gradient penalty succeed training architectures including -layer resnet stability lsgans gradient penalty comparable wgans-gp. related work generative adversarial networks proposed goodfellow explained theory gans learning based game theoretic scenario. showing powerful capability unsupervised tasks gans applied many speciﬁc tasks like image generation image superresolution text image synthesis image image translation combining traditional content loss adversarial loss super-resolution generative adversarial networks achieve state-of-the-art performance task image super-resolution. reed proposed model synthesize images given text descriptions based conditional gans isola also used conditional gans transfer images representation another. addition unsupervised learning tasks gans also show potential semi-supervised learning tasks. salimans proposed gan-based framework semi-supervised learning discriminator outputs probability input image real data also outputs probabilities belonging class. beneﬁts lsgans derived aspects. first unlike regular gans cause almost loss samples long correct side decision boundary lsgans penalize samples even though correctly classiﬁed update generator parameters discriminator ﬁxed i.e. decision boundary ﬁxed. result penalization make generator generate samples toward decision boundary. hand decision boundary across manifold real data successful gans learning. otherwise learning process saturated. thus moving generated samples toward decision boundary leads making closer manifold real data. second penalizing samples lying long decision boundary generate gradients updating generator turn relieves problem vanishing gradients. allows lsgans perform stable learning process. beneﬁt also derived another perspective shown figure least squares loss function point sigmoid cross entropy loss function saturate relatively large. furthermore provide theoretical analysis stability lsgans section despite great successes gans achieved improving quality generated images still challenge. works proposed improve quality images gans. radford ﬁrst introduced convolutional layers gans architecture proposed network architecture called deep convolutional generative adversarial networks denton proposed another framework called laplacian pyramid generative adversarial networks constructed laplacian pyramid generate high-resolution images starting low-resolution images. further salimans proposed technique called feature matching better convergence. idea make generated samples match statistics real data minimizing mean square error intermediate layer discriminator. another critical issue gans stability learning process. many works proposed address problem analyzing objective functions gans viewing discriminator energy function used auto-encoder architecture improve stability gans learning. make generator discriminator balanced metz created unrolled objective function enhance generator. incorporated reconstruction module distance real samples reconstructed samples regularizer stable gradients. nowozin pointed objective original related jensen-shannon divergence special case divergence estimation generalized arbitrary f-divergences arjovsky extended analyzing properties four different divergences distances distributions concluded wasserstein distance nicer jensen-shannon divergence. proposed losssensitive whose loss function based assumption real samples smaller losses fake samples proved loss function non-vanishing gradient almost everywhere. method generative adversarial networks learning process gans train discriminator generator simultaneously. target learn distribution data starts sampling input variables uniform gaussian distribution maps input variables data space differentiable network. hand classiﬁer aims recognize whether image training data minimax objective gans formulated follows least squares generative adversarial networks viewing discriminator classiﬁer regular gans adopt sigmoid cross entropy loss function. stated section updating generator loss function cause problem vanishing gradients samples correct side decision boundary still real data. remedy problem propose least squares generative adversarial networks suppose coding scheme discriminator labels fake equation corresponds also found optimizing equation tends perform similarly klpd). klpd) widely used variational inference convenient evidence lower bound however optimizing klpd) problem mode-seeking behavior underdispersed approximations problem also appears learning known mode collapse problem. deﬁnition klpd) given follows recently divergence drawn researchers’ attentaion variational inference since divergence able produce over-dispersed approximations objective function equation become inﬁnite happen since pearsonpg) zero-forcing property. thus makes lsgan less mode-seeking relieves mode collapse problem. parameters selection method determine values equation satisfy conditions minimizing equation yields minimizing pearson divergence example setting following objective functions implementation details implementation proposed models based public implementation dcgans using tensorflow learning rate except lsun-scenes whose learning rate following dcgans adam optimizer implementation available https//github.com/xudonmao/improved lsgan. image quality qualitative evaluation scenes generation train lsgans dcgans network architecture resolution lsun-bedroom dataset. generated images methods presented figure compared images generated dcgans texture detail images generated lsgans exquisite images generated lsgans look sharper. also train lsgans four scene datasets including church dining room kitchen conference room. generated results shown figure cats generation also evaluate lsgans dataset ﬁrst preprocess methods public project head images whose resolution bigger resize images resolution network fig. model architecture. conv/deconv stride denotes convolutional/deconvolutional layer kernel output ﬁlters stride layer means layer followed batch normalization layer. denotes fully-connected layer output nodes. activation layers omitted. generator. discriminator. section ﬁrst present details implementation. next present results qualitative evaluation quantitative evaluation lsgans. compare stability lsgans dcgans without gradient penalty three comparison experiments. finally evaluate stability lsgans gradient penalty. architecture used task consists four transposed convolutional layers four convolutional layers generator discriminator respectively. following evaluation protocol comparing performance lsgans dcgans. first train lsgans dcgans using architecture dataset. training save checkpoint model batch generated images every iterations. second select best models lsgans dcgans checking quality saved images every iterations. finally selected best models randomly generate images compare quality generated images. selected models lsgans dcgans available https//github.com/xudonmao/improved lsgan. figure shows generated images lsgans dcgans results found appendix. observe lsgans generate cats sharper exquisite furs faces ones generated dcgans. checking samples appendix using saved models generate samples also observe overall quality generated images lsgans better dcgans. randomly generate images calculating inception scores evaluated inception scores lsgans dcgans shown table observe inception scores vary different trained models reported inception scores table averaged different trained models lsgans dcgans. quantitative evaluation inception score lsgans show comparable performance dcgans. human subjective study evaluate performance lsgans conduct human subjective study using generated bedroom images lsgans dcgans network architectures. randomly construct image pairs image lsgans dcgans. amazon mechanical turk fig. generated images datasets. lsgans generate cats sharper exquisite furs faces ones generated dcgans. overall quality generated images lsgans better dcgans. training stability section evaluate stability proposed lsgans compare baselines including dcgans wgansgp. gradient penalty proved effective improving stability gans training gradient penalty also able improve stability lsgans. also obvious disadvantages additional computational cost memory cost. thus evaluate stability lsgans groups. compare model without gradient penalty dcgans. compare model gradient penalty wgans-gp. ﬁrst compare lsgans dcgans without gradient penalty. three comparison experiments conducted learning gaussian mixture distribution; learning difﬁcult architectures; learning datasets small variance. gaussian mixture distribution learning gaussian mixture distribution evaluate stability proposed literature model generate samples around mode mode-seeking behavior. train lsgans dcgans mixture eight gaussian mixture distribution using simple network architecture generator discriminator contain three fully-connected layers. figure shows dynamic results gaussian kernel density estimation. dcgans suffer mode collapse starting difﬁcult architectures another experiment train gans difﬁcult architectures proposed model generate similar images suffers mode collapse problem. based network architecture presented architectures designed compare stability. ﬁrst exclude batch normalization generator second exclude batch normalization generator discriminator pointed selection optimizer critical model performance. thus evaluate architectures optimizers adam rmsprop summary following four training settings adam rmsprop bngd adam bngd rmsprop. train models lsun-bedroom dataset using lsgans dcgans separately following four major observations. first adam chance lsgans generate relatively good quality images. test times succeed generate relatively good quality images. dcgans never observe successful learning. dcgans suffer severe degree mode collapse. generated images lsgans dcgans shown figure second bngd rmsprop figure shows lsgans generate higher quality images dcgans slight degree mode collapse. third lsgans dcgans similar performances rmsprop bngd adam. speciﬁcally rmsprop lsgans dcgans able generate relatively good images. bngd adam slight degree mode collapse. last rmsprop performs stable adam dcgans since dcgans succeed generating relatively good images rmsprop fail learn adam. datasets small variance using difﬁcult architectures effective evaluate stability gans. however practice always select stable architectures tasks. difﬁculty practical task task itself. inspired motivation propose difﬁcult datasets stable architectures evaluate stability gans. datasets small variance difﬁcult gans learn since discriminator distinguish real samples easily datasets small variance. speciﬁcally construct datasets rendering digits using timesnew-roman font. datasets created applied random horizontal shift; applied random horizontal shift random rotation degree. category contains thousand samples datasets. note second dataset larger variance ﬁrst one. examples synthetic datasets shown fig. comparison experiments lsgans-gp wgans-gp using difﬁcult architectures images generated wgans-gp duplicated lsgans-gp succeed architectures. ﬁrst column figure stable architecture digits generation follow suggestions discriminator similar lenet generator contains three transposed convolutional layers. train dcgans lsgans datasets generated images shown figure major observations. first lsgans succeed generating digits datasets dcgans suffer severe mode collapse problem. second generated image quality second dataset lsgans better ﬁrst one. implies increasing variance dataset able improve generated image quality relieve mode collapse problem. based observation applying data augmentation shifting cropping rotation effective improve learning. evaluation gradient penalty gradient penalty proved effective improving stability training compare wgans gradient penalty state-of-theart model stability adopt gradient penalty lsgans hyper-parameters respectively. experiment implementation based ofﬁcial implementation wgans-gp. follow evaluation method wgans-gp train difﬁcult architectures including normalization constant number ﬁlters generator; -layer -dimension relu generator; normalization either generator discriminator; gated multiplicative nonlinearities generator discriminator; tanh nonlinearities generator discriminator; -layer resnet generator discriminator. results presented figure generated images wgans-gp duplicated paper. following major observations. first like wgans-gp lsgans gradient penalty also suc. suggestions practice based experiments following suggestions practical tasks. first suggest lsgans without gradient penalty works. using gradient penalty introduce additional computational cost memory cost inﬂuence image quality. second observe quality generated images lsgans shift good training process. thus suggest keep record generated images every thousand hundred iterations select model manually checking image quality. third lsgans without gradient penalty work suggest gradient penalty hyper-parameters according suggestions literature experiments hyper-parameters setting works tasks. conclusions future work paper proposed least squares generative adversarial networks experimental results show lsgans generate higher quality images regular gans. three comparison experiments evaluating stability also conducted results demonstrate lsgans perform stable regular gans. also compare stability lsgans gradient penalty wgans-gp. train lsgans gradient penalty lsun-bedroom using difﬁcult architectures. lsgans gradient penalty able train successfully architectures. based present ﬁndings hope extend lsgans complex datasets imagenet future. instead pulling generated samples toward decision boundary designing method pull generated samples toward real data directly also worth investigation. zhang deep residual learning image recognition computer vision pattern recognition girshick faster r-cnn towards real-time object detection region proposal networks advances neural information processing systems long shelhamer darrell fully convolutional networks semantic segmentation computer vision pattern recognition hinton salakhutdinov reducing dimensionality data neural networks science vol. salakhutdinov hinton deep boltzmann machines proceedings international conference artiﬁcial intelligence statistics vol. international conference learning representations goodfellow pouget-abadie mirza warde-farley ozair courville bengio generative adversarial nets advances neural information processing systems ledig theis huszar caballero cunningham acosta aitken tejani totz wang photo-realistic single image super-resolution using generative adversarial network arxiv. radford metz chintala unsupervised representation learning deep convolutional generative adversarial networks international conference learning representations chen duan houthooft schulman sutskever abbeel infogan interpretable representation learning information maximizing generative adversarial nets advances neural information processing systems denton chintala szlam fergus deep generative image models using laplacian pyramid adversarial networks advances neural information processing systems jacob bengio mode regularized nguyen wainwright jordan estimating divergence functionals likelihood ratio convex risk minimization ieee transactions information theory vol.", "year": 2017}