{"title": "Unsupervised feature learning by augmenting single images", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "When deep learning is applied to visual object recognition, data augmentation is often used to generate additional training data without extra labeling cost. It helps to reduce overfitting and increase the performance of the algorithm. In this paper we investigate if it is possible to use data augmentation as the main component of an unsupervised feature learning architecture. To that end we sample a set of random image patches and declare each of them to be a separate single-image surrogate class. We then extend these trivial one-element classes by applying a variety of transformations to the initial 'seed' patches. Finally we train a convolutional neural network to discriminate between these surrogate classes. The feature representation learned by the network can then be used in various vision tasks. We find that this simple feature learning algorithm is surprisingly successful, achieving competitive classification results on several popular vision datasets (STL-10, CIFAR-10, Caltech-101).", "text": "deep learning applied visual object recognition data augmentation often used generate additional training data without extra labeling cost. helps reduce overﬁtting increase performance algorithm. paper investigate possible data augmentation main component unsupervised feature learning architecture. sample random image patches declare separate single-image surrogate class. extend trivial one-element classes applying variety transformations initial ’seed’ patches. finally train convolutional neural network discriminate surrogate classes. feature representation learned network used various vision tasks. simple feature learning algorithm surprisingly successful achieving competitive classiﬁcation results several popular vision datasets deep convolutional neural networks trained backpropagation recently shown perform well image classiﬁcation tasks containing millions images thousands categories deep convolutional neural networks known yield good results supervised image classiﬁcation tasks mnist long time recent successes made possible optimized implementations efﬁcient model averaging data augmentation techniques feature representation learned networks achieves state performance classiﬁcation task network trained also various computer vision tasks example classiﬁcation caltech- caltech- caltech-ucsd birds dataset sun- scene recognition database detection pascal dataset capability generalize datasets indicates supervised discriminative learning currently best known algorithm visual feature learning. downside approach need expensive labeling amount required labels grows quickly larger model gets. reason unsupervised learning although currently underperforming remains appealing paradigm since make unlabeled images videos readily available virtually inﬁnite amounts. work combine power discriminative supervised learning simplicity unsupervised data acquisition. main novelty approach obtain training data convolutional network unsupervised manner. standard supervised setting exists large labeled images augmented small translations rotations color variations generate even training data. contrast method require labeled data augmentation step alone create surrogate training data unlabeled images. start trivial surrogate classes consisting random image patch each augment data applying random transformations patch. train convolutional neural network classify surrogate classes. feature representation learned network construction discriminative time invariant typical data transformations. nevertheless immediately clear would feature representation learned surrogate task perform well general image classiﬁcation problems? experiments show that indeed simple unsupervised feature learning algorithm achieves competitive state results several benchmarks. performing image augmentation provide prior knowledge natural image distribution training algorithm. precisely assigning label transformed versions image patch force learned feature representation invariant transformations applied. seen indirect form supervision algorithm needs expert knowledge transformations features invariant however similar expert knowledge used unsupervised feature learning algorithms. features usually learned small image patches assumes translational invariance. turning images grayscale assumes invariance color changes. whitening contrast normalization assumes invariance contrast changes largely color variations. approach related large body work unsupervised learning convolutional neural networks. contrast method unsupervised learning approaches e.g. rely modeling input distribution explicitly often reconstruction error term rather training discriminative model thus cannot used jointly train multiple layers deep neural network straightforward manner. among unsupervised methods similar approach several studies learning invariant representations transformed input samples example proposed method related work metric learning example however instead enforcing metric feature representation directly implicitly force representation transformed images mapped close together introduced surrogate labels. enables discriminative training learning feature representation performs well classiﬁcation tasks. learning invariant features discriminative objective previously considered early work tangent propagation aims learn features invariant small predeﬁned transformations directly penalizing derivative network output respect parameters transformation. contrast work algorithm rely labeled data less dependent small magnitude applied transformations. tangent propagation successfully combined unsupervised feature learning algorithm build classiﬁer exploiting information manifold structure learned representation. this however comes disadvantages reconstruction-based training. loosely related work research using unlabeled data regularizing supervised algorithms example self-training entropy regularization contrast semisupervised methods training procedure mentioned before make labeled data. finally idea creating pseudo-task improve performance supervised algorithm used describe detail feature learning pipeline. main stages approach generating surrogate training data training convolutional neural network using data. figure random patches sampled stl- unlabeled dataset later augmented various transformation obtain surrogate classes neural network training. input algorithm unlabeled images come roughly distribution images later classify. randomly sample random patches size pixels different images varying positions scales. sample regions considerable gradient energy avoid getting uniformly colored patches. apply random transformations sampled patches. random transformations composition four random ’elementary’ transformations following list apply preprocessing obtained patches subtracting mean pixel whole training dataset. examples patches sampled stl- unlabeled dataset shown fig. examples transformed versions patch shown fig. training result procedure described above patch initially sampled patches apply transformations transformed versions tixi ti}. declare sets class assigning label class train convolutional neural network discriminate surrogate classes. formally minimize following loss function denotes function computing values output layer neural network given input data standard basis vector. training network implementation based fast convolutional neural network code modiﬁed support dropout. ﬁxed network architecture experiments convolutional layers ﬁlters size followed fully connected layer neurons dropout softmax layer top. perform max-pooling convolutional layers perform contrast normalization layers. start learning rate gradually decrease learning rate training. train improvement validation error decrease learning rate factor repeat procedure several times signiﬁcant improvement validation error. experiments number surrogate classes large relative number training samples surrogate class observed training process training error signiﬁcantly decrease compared initial chance level. alleviate problem training network whole surrogate dataset pre-train subset fewer surrogate classes typically stop pre-training soon training error starts falling indicating optimization found direction towards good local minimum. weights learned pre-training phase initialization training whole surrogate dataset. training procedure ﬁnished apply learned feature representation classiﬁcation tasks ’real’ datasets consisting images differ size surrogate training images. extract features images convolutionally compute responses network layers except softmax form -layer spatial pyramid them. train linear support vector machine features. select hyperparameters crossvalidation. report classiﬁcation results stl- cifar- caltech- datasets approaching exceeding state unsupervised algorithms them. also evaluate effects number surrogate classes number training samples surrogate class training data. training network experiments generate surrogate dataset using patches extracted stl- unlabeled dataset. stl- usual testing protocol averaging results pre-deﬁned folds training data report mean standard deviation. cifar- report results ’cifar-’ means training whole cifar- training ’cifar--reduced’ means average random selections training samples class. caltech- follow usual protocol selecting random samples class training training samples class testing repeated times. table compare classiﬁcation results recent work. network trained surrogate dataset surrogate classes containing samples each. remind extracting features test time ﬁrst layers network ﬁlters respectively. feature representation hence considerably compact competing approaches. list results supervised methods cifar- since directly comparable unsupervised feature learning method. k-means multi-way local pooling slowness videos receptive ﬁeld learning hierarchical matching pursuit multipath sum-product networks view-invariant k-means paper there ways compute accuracy caltech- simply averaging accuracy whole test calculating accuracy class separately averaging values. methods differ many classes less test samples available. seems researchers machine learning ﬁeld ﬁrst method report table. using second method performance drops pipeline lets easily vary number surrogate classes training data number training samples surrogate class. measure effect factors quality resulting features. vary number surrogate classes number training samples surrogate class results shown fig. fig. also show baseline classiﬁcation performance random ﬁlters initializing random ﬁlters require training data hence seen using samples surrogate class. error bars fig. show standard deviations computed testing folds stl- dataset. apparent trend fig. increasing number surrogate classes results increase classiﬁcation accuracy reaches optimum around surrogate classes. number surrogate classes increased classiﬁcation results change slightly decrease. explanation behavior larger number surrogate classes becomes classes overlap. result overlap classiﬁcation problem becomes difﬁcult adapting network surrogate task longer succeeds. check validity explanation also plot fig. classiﬁcation error validation computed training network. rapidly grows number surrogate classes increases supporting claim task quickly becomes difﬁcult number surrogate classes increases. fig. shows classiﬁcation accuracy increases increasing number samples surrogate class saturates around samples. also seen training small numbers samples surrogate class clear indication classes lead better performance. hypothesize reason training samples class surrogate classiﬁcation problem simple hence network severely overﬁt results poor unstable generalization real classiﬁcation tasks. however starting around samples surrogate class surrogate task gets sufﬁciently complicated networks diverse training data perform consistently better. figure dependence classiﬁcation accuracy stl- number surrogate classes training data. reference error validation surrogate data also shown. note different scales graphs. proposed simple unsupervised feature learning approach based data augmentation shows good results variety classiﬁcation tasks. approach sets state stl- remains seen whether success translated consistently better performance datasets. performance method saturates number surrogate classes increases. probable reason surrogate task relatively simple allow network learn complex invariances viewpoint invariance inter-instance invariance. hypothesize unsupervised feature learning method could learn powerful higherlevel features surrogate data similar real-world labeled datasets. could achieved using extra weak supervision provided example video data small number labeled samples. another possible obtaining richer surrogate training data would merging similar surrogate classes. interesting directions future work.", "year": 2013}