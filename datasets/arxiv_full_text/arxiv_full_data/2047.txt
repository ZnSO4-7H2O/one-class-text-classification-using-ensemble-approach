{"title": "Discovering Structure in High-Dimensional Data Through Correlation  Explanation", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We introduce a method to learn a hierarchy of successively more abstract representations of complex data based on optimizing an information-theoretic objective. Intuitively, the optimization searches for a set of latent factors that best explain the correlations in the data as measured by multivariate mutual information. The method is unsupervised, requires no model assumptions, and scales linearly with the number of variables which makes it an attractive approach for very high dimensional systems. We demonstrate that Correlation Explanation (CorEx) automatically discovers meaningful structure for data from diverse sources including personality tests, DNA, and human language.", "text": "introduce method learn hierarchy successively abstract representations complex data based optimizing information-theoretic objective. intuitively optimization searches latent factors best explain correlations data measured multivariate mutual information. method unsupervised requires model assumptions scales linearly number variables makes attractive approach high dimensional systems. demonstrate correlation explanation automatically discovers meaningful structure data diverse sources including personality tests human language. without prior knowledge automatically learned high-dimensional data? variables uncorrelated system really high-dimensional viewed collection unrelated univariate systems. correlations exist however common cause causes must responsible generating them. without assuming particular model hidden common causes still possible reconstruct them? propose informationtheoretic principle refer correlation explanation codiﬁes problem model-free mathematically principled way. essentially searching latent factors that conditioned factors correlations data minimized words look simplest explanation accounts correlations data. bonus building information-based foundation leads naturally innovative paradigm learning hierarchical representations tractable bayesian structure learning provides richer insights neural network inspired approaches introducing principle correlation explanation sec. show efﬁciently implemented sec. demonstrate power approach begin sec. simple synthetic example show standard learning techniques fail detect highdimensional structure corex succeeds. sec. show corex perfectly reverse engineers personality types survey data approaches fail sec. corex automatically discovers nearly perfect predictors independent signals relating gender geography ethnicity. sec. apply corex text recover stylistic features hierarchical topic representations. brieﬂy considering intriguing theoretical connections sec. conclude future directions sec. using standard notation capital denotes discrete random variable whose instances written lowercase. probability distribution random variable shortened unless ambiguity arises. cardinality values random variable take always ﬁnite denoted |x|. random variables subset indices corresponding subset random variables entropy deﬁned usual higherorder entropies constructed various ways standard deﬁnition. instance mutual information random variables written following measure mutual information among many variables ﬁrst introduced total correlation also called multi-information multivariate mutual information semicolons reminder symmetric arguments unlike mutual information. zero maximized) distribution conditioned factorizes. would case common cause xi’s case explains correlation also seen encoding local markov properties among group variables therefore specifying quantity appeared measure redundant information xi’s carry connections discussed sec. optimizing seen search latent factor explains correlations make concrete letting discrete random variable take possible values searching probabilistic functions solution optimization given special case sec. total correlation functional joint distribution optimization implicitly depends data typically small number samples drawn make matters worse optimizing involves least variables. surprisingly despite difﬁculties show next section optimization carried efﬁciently. maximum achievable value objective occurs ﬁnite implies data perfectly described naive bayes model parent children. generally expect correlations data result several different factors. therefore extend optimization include different factors simultaneously search subsets variables variables explain correlations group. necessary make optimization tractable impose additional condition variable single group associated single parent reason restriction shown value objective interpreted lower bound note objective valid meaningful regardless details data-generating process. assume given samples output procedure gives yj’s probabilistic functions iteratively apply optimization resulting probability distribution searching explain correlations hierarchy variables forms tree. show optimization carried efﬁciently even high-dimensional spaces small numbers samples. αi¯j note also dropped subscript second term effect solutions must satisfy show. ﬁxed straightforward solution lagrangian optimization problem solution self-consistent equations. details derivation found sec. note kronecker delta depends non-zero. remarkably yj’s dependence written terms linear number parameters marginals approximate δ¯xx/n. approximation allows estimate marginals ﬁxed accuracy using constant number samples true distribution. sec. show deﬁnes soft labeling seen linear function followed non-linear threshold reminiscent neural networks. also note normalization constant calculated easily summing |yj| values. ﬁxed values parameters integer linear program made easy however leads rough optimization space. solution valid arbitrary values relax optimization accordingly. step optimization uniformly random step make pick small update direction solution. second term converges true solution limit leads smooth optimization good choices intuitive arguments described sec. rules update increase value objective simply iterate achieve convergence. guarantee global optimum objective upper bounded lower bounded pseudo-code approach described algorithm additional details provided sec. source code available online. overall complexity linear number variables. bound complexity terms number samples always minibatches ﬁxed size estimate marginals common problem representation learning pick number latent variables describe data. consider limit representation would need exactly variable group therefore whole objective suggests maximum value objective must achieved value practice means high subset latent variables used solution demonstrate fig. words high enough optimization result number clusters optimal respect objective. representations different numbers layers different different compared according tight lower bound provide test corex’s ability recover latent structure data begin generating synthetic data according latent tree model depicted fig. variables hidden except leaf nodes. difﬁcult part reconstructing tree clustering leaf nodes. clustering method latent variables reconstructed cluster easily using consider many different clustering methods typically several variations technique details described sec. adjusted rand index measure accuracy inferred clusters recover ground truth. generated samples model fig. varied number leaves branch. xi’s depend yj’s binary erasure channel erasure probability capacity reﬂect intuition signal parent node weakly distributed across children generated samples. example yj’s weakly correlated root node binary symmetric channel probability fig. shows small medium number variables techniques recover structure fairly well dimensionality increases corex continues hierarchical clustering compete second place. corex also perfectly recovers values latent factors example. latent tree models recovery latent factors gives global optimum objective even though corex guaranteed local optima example correctly converges global optimum range problem sizes. note growing literature latent tree learning attempts reconstruct latent trees theoretical guarantees principle compare techniques scale table method recent survey latent tree learning methods techniques able largest dataset considered table datasets paper orders magnitude larger one. fig. visualizes structure learning process. example similar includes uncorrelated random variables show treated corex. clusters variables used hidden variables. iteration show hidden variables connected input variables connectivity matrix mutual information shown bottom. beginning started full connectivity nothing learned time hidden units compete group xi’s explain correlations. iterations overall structure appears iterations exactly described. uncorrelated random variables hidden variables explained correlations easily distinguished discarded psychological theory suggests traits largely reﬂect differences personality types extraversion neuroticism agreeableness conscientiousness openness experience. psychologists designed various instruments intended measure whether individuals exhibit traits. consider survey subjects rate ﬁfty statements life party point scale disagree slightly disagree neutral slightly agree agree. data consist answers questions rand index counts percentage pairs whose relative classiﬁcation matches clusterings. adds correction random clustering give score zero corresponds perfect match. thousand test-takers. test designed intention question belong cluster according personality trait question gauges. true factors strongly predict answers questions? corex learned two-level hierarchical representation applied data ﬁrst level corex automatically determined questions cluster groups. surprisingly clusters exactly correspond personality traits labeled test designers. unusual recover ground truth perfect accuracy unsupervised learning problem tried number standard clustering methods could reproduce result. display results using confusion matrices fig. details techniques used described sec. advantage corex since required specify correct number clusters. none techniques able recover personality types exactly. interestingly independent component analysis method comes close. intuition behind linear transformation input minimizes multi-information among outputs contrast corex searches yj’s multiinformation among xi’s minimized conditioning assumes signals give rise data independent corex not. case personality traits like extraversion agreeableness correlated violating independence assumption. figure confusion matrix comparing predicted clusters true clusters questions big- personality test. hierarchical model constructed samples corex. next consider data taken individuals diverse geographic ethnic backgrounds data consist variables describing different snps corex learn hierarchical representation depicted fig. evaluate quality representation adjusted rand index compare clusters induced latent variable hierarchical representation different demographic variables data. latent variables substantially match demographic variables labeled fig. representation learned ﬁrst layer contains perfect match oceania nearly perfect matches america subsaharan africa gender. second layer three variables correspond closely broad geographic regions subsaharan africa east eurasia. twenty newsgroups dataset consists documents taken twenty different topical message boards thousand posts analyzing unstructured text typical feature engineering approaches heuristically separate signals like style sentiment topics. principle three signals manifest terms subtle correlations word usage. recent attempts learning large-scale unsupervised hierarchical representations text produced interesting results though validation difﬁcult quantitative measures representation quality often correlate well human judgment focus linguistic signals removed meta-data like headers footers replies even though give strong signals supervised newsgroup classiﬁcation. considered thousand frequent tokens constructed words representation. used corex learn level representation data latent variables ﬁrst layer. details described sec. portions ﬁrst three levels tree keeping nodes highest normalized mutual information parents shown fig. fig. figure portions hierarchical representation learned twenty newsgroups dataset. label latent variables overlap signiﬁcantly known structure. newsgroup names abbreviations broad groupings shown right. provide quantitative benchmark results test extent learned representations related known structure data. post labeled newsgroup belongs according broad categories author. learned binary variables active around posts report fraction activations coincide known label fig. variables clearly represent sub-topics newsgroup topics expect high recall. small portion tree shown fig. reﬂects intuitive relationships contain hierarchies related sub-topics well clusters function words again several learned variables perfectly captured known structure data. users sent images text using encoded format. feature matched image posts correlated presence unusual short tokens. also perfect matches three frequent authors banks medin beauchaine. note learned variables trigger names appeared text posts authored. authors elaborate signatures long identiﬁable quotes evaded preprocessing created strongly correlated signal. another variable perfect precision forsale newsgroup labeled comic book sales nearly perfect predictors described extensive discussions armenia/turkey talk.politics.mideast specialized unix jargon match sci.crypt precision recall. ranked latent factors according normalized version examples showed deﬁne interesting less obvious connections. instance optimization similar recently introduced measure common information objective appears exactly bound ancestral information instance steudel show objective positive least variables share common ancestor describing them. provides extra rationale relaxing original optimization include non-binary values αij. similar learning approach presented information bottleneck extension multivariate information bottleneck motivation behind information bottleneck compress data smaller representation information relevance term maintained. second term analogous compression term. instead maximizing relevance term maximizing information individual sub-systems redundant information data preferentially stored uncorrelated random variables completely ignored. broad problem transforming complex data simpler meaningful forms goes rubric representation learning shares many goals dimensionality reduction subspace clustering. insofar approach learns hierarchy representations superﬁcially resembles deep approaches like neural nets autoencoders approaches scalable common critique involve many heuristics discovered trial-and-error difﬁcult justify. hand rich literature learning latent tree models excellent theoretical properties scale well. basing method information-theoretic optimization nevertheless performed quite efﬁciently hope preserve best worlds. conclusion challenging open problems today involve high-dimensional data diverse sources including human behavior language biology. complexity underlying systems makes modeling difﬁcult. demonstrated model-free approach learn successfully coarsegrained representations complex data efﬁciently optimizing information-theoretic objective. principle explaining much correlation data possible provides intuitive fully data-driven discover previously inaccessible structure high-dimensional systems. seem surprising corex perfectly recover structure diverse domains without using labeled data prior knowledge. hand patterns discovered low-hanging fruit right point view. intelligent systems able learn robust general patterns face rich inputs even absence labels deﬁne important. information redundant high-dimensional data provides good starting point. several fruitful directions stand out. first promising preliminary results invite in-depth investigations related problems. computational point view main work algorithm involves matrix multiplication followed element-wise non-linear transform. true neural networks scaled large data using e.g. gpus. theoretical side generalizing approach allow non-tree representations appears feasible desirable thank virgil grifﬁth shuyang hsuan-yi shirley pepke bilal shaw jose-luis ambite nathan hodas helpful conversations. research supported part afosr grant fa--- darpa grant wnf---.", "year": 2014}