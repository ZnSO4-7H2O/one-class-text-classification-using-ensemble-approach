{"title": "Mean teachers are better role models: Weight-averaged consistency  targets improve semi-supervised deep learning results", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels from 35.24% to 9.11%.", "text": "recently proposed temporal ensembling achieved state-of-the-art results several semi-supervised learning benchmarks. maintains exponential moving average label predictions training example penalizes predictions inconsistent target. however targets change epoch temporal ensembling becomes unwieldy learning large datasets. overcome problem propose mean teacher method averages model weights instead label predictions. additional beneﬁt mean teacher improves test accuracy enables training fewer labels temporal ensembling. without changing network architecture mean teacher achieves error rate svhn labels outperforming temporal ensembling trained labels. also show good network architecture crucial performance. combining mean teacher residual networks improve state cifar- labels imagenet labels deep learning seen tremendous success areas image speech recognition. order learn useful abstractions deep learning models require large number parameters thus making prone over-ﬁtting moreover adding high-quality labels training data manually often expensive. therefore desirable regularization methods exploit unlabeled data effectively reduce over-ﬁtting semi-supervised learning. percept changed slightly human typically still considers object. correspondingly classiﬁcation model favor functions give consistent output similar data points. approach achieving noise input model. enable model learn abstract invariances noise added intermediate representations insight motivated many regularization techniques dropout rather minimizing classiﬁcation cost zero-dimensional data points input space regularized model minimizes cost manifold around data point thus pushing decision boundaries away labeled data points since classiﬁcation cost undeﬁned unlabeled examples noise regularization semi-supervised learning. overcome this model evaluates data point without noise applies consistency cost predictions. case model assumes dual role teacher student. student learns before; teacher generates targets used student learning. since model generates targets well incorrect. much weight given generated targets cost inconsistency outweighs misclassiﬁcation preventing learning figure sketch binary classiﬁcation task labeled examples unlabeled example demonstrating choice unlabeled target affects ﬁtted function model regularization free function predicts labeled training examples well. model trained noisy labeled data learns give consistent predictions around labeled data points. consistency noise around unlabeled examples provides additional smoothing. clarity illustration teacher model ﬁrst ﬁtted labeled examples left unchanged training student model. also clarity omit small dots ﬁgures noise teacher model reduces bias targets without additional training. expected direction stochastic gradient descent towards mean individual noisy targets ensemble models gives even better expected target. temporal ensembling mean teacher method approach. information. effect model suffers conﬁrmation bias hazard mitigated improving quality targets. least ways improve target quality. approach choose perturbation representations carefully instead barely applying additive multiplicative noise. another approach choose teacher model carefully instead barely replicating student model. concurrently research miyato taken ﬁrst approach shown virtual adversarial training yield impressive results. take second approach show provides signiﬁcant beneﬁts. understanding approaches compatible combination produce even better outcomes. however analysis combined effects outside scope paper. goal then form better teacher model student model without additional training. ﬁrst step consider softmax output model usually provide accurate predictions outside training data. partly alleviated adding noise model inference time consequently noisy teacher yield accurate targets approach used pseudo-ensemble agreement lately shown work well semi-supervised image classiﬁcation laine aila named method model; name version basis experiments. model improved temporal ensembling maintains exponential moving average prediction training examples. training step predictions examples minibatch updated based predictions. consequently prediction example formed ensemble model’s current version earlier versions evaluated example. ensembling improves quality predictions using teacher predictions improves results. however since target updated epoch learned information incorporated training process slow pace. larger dataset longer span updates case on-line learning unclear temporal ensembling used all. evaluations epoch number training examples.) overcome limitations temporal ensembling propose averaging model weights instead predictions. since teacher model average consecutive student models call mean teacher method averaging model weights training steps tends produce figure mean teacher method. ﬁgure depicts training batch single labeled example. student teacher model evaluate input applying noise within computation. softmax output student model compared one-hot label using classiﬁcation cost teacher output using consistency cost. weights student model updated gradient descent teacher model weights updated exponential moving average student weights. model outputs used prediction training teacher prediction likely correct. training step unlabeled example would similar except classiﬁcation cost would applied. accurate model using ﬁnal weights directly take advantage training construct better targets. instead sharing weights student model teacher model uses weights student model. aggregate information every step instead every epoch. addition since weight averages improve layer outputs output target model better intermediate representations. aspects lead practical advantages temporal ensembling first accurate target labels lead faster feedback loop student teacher models resulting better test accuracy. second approach scales large datasets on-line learning. formally deﬁne consistency cost expected distance prediction student model prediction teacher model noise difference model temporal ensembling mean teacher teacher predictions generated. whereas model uses temporal ensembling approximates weighted average successive predictions deﬁne training step successive weights smoothing coefﬁcient hyperparameter. additional difference three algorithms model applies training whereas temporal ensembling mean teacher treat constant regards optimization. approximate consistency cost function sampling noise training step stochastic gradient descent. following laine aila mean squared error consistency cost experiments. table error rate percentage svhn runs exponential moving average weights evaluation models. methods similar -layer convnet architecture. table appendix results without input augmentation. test hypotheses ﬁrst replicated model tensorflow baseline. modiﬁed baseline model weight-averaged consistency targets. model architecture -layer convolutional neural network three types noise random translations horizontal ﬂips input images gaussian noise input layer dropout applied within network. mean squared error consistency cost ramp weight ﬁnal value ﬁrst epochs. details model training procedure described appendix experiments using street view house numbers cifar- benchmarks datasets contain pixel images belonging different classes. svhn example close-up house number class represents identity digit center image. cifar- example natural image belonging class horses cats cars airplanes. svhn contains training samples test samples. cifar- consists training samples test samples. tables compare results recent state-of-the-art methods. methods comparison similar -layer convnet architecture. mean teacher improves test accuracy model temporal ensembling semi-supervised svhn tasks. mean teacher also improves results cifar- baseline model. recently published version virtual adversarial training miyato performs even better mean teacher -label svhn -label cifar-. discussed introduction mean teacher complimentary approaches. combination yield better accuracy either alone investigation beyond scope paper. figure smoothened classiﬁcation cost classiﬁcation error mean teacher baseline model svhn ﬁrst training steps. upper training classiﬁcation costs measured using labeled data. above suggested mean teacher scales well large datasets on-line learning. addition svhn cifar- results indicate uses unlabeled examples efﬁciently. therefore wanted test whether reached limits approach. besides primary training data svhn includes also extra dataset examples. picked samples primary training labeled training examples. used rest primary training together extra training unlabeled examples. experiments mean teacher baseline model used either extra examples. table shows results. training curves figure help understand effects using mean teacher. expected ema-weighted models give accurate predictions bare student models initial period. using ema-weighted model teacher improves results semi-supervised settings. appears virtuous feedback cycle teacher improving student consistency cost student improving teacher exponential moving averaging. feedback cycle detached learning slower model starts overﬁt earlier mean teacher helps labels scarce. using labels mean teacher learns faster continues training model stops improving. hand all-labeled case mean teacher model behave virtually identically. figure validation error -label svhn four runs hyperparameter setting means. experiment varied hyperparameter used evaluation hyperparameters table rest. hyperparameter settings used evaluation runs marked bolded font weight. text details. mean teacher uses unlabeled training data efﬁciently model seen middle column. hand extra unlabeled examples model keeps improving longer. mean teacher learns faster eventually converges better result sheer amount data appears offset model’s worse predictions. assess importance various aspects model experiments svhn labels varying hyperparameters time keeping others ﬁxed. removal noise introduction figure presented hypothesis model produces better predictions adding noise model sides. addition mean teacher noise still needed? yes. either input augmentation dropout necessary passable performance. hand input noise help augmentation use. dropout teacher side provides marginal beneﬁt student side least input augmentation use. sensitivity decay consistency weight essential hyperparameters mean teacher algorithm consistency cost weight decay sensitive algorithm values? case good values span roughly order magnitude outside ranges performance degrades quickly. note decay makes model variation model although somewhat inefﬁcient gradients propagated student path. note also evaluation runs used decay ramp-up phase rest training. chose strategy student improves quickly early training thus teacher forget inaccurate student weights quickly. later student improvement slows teacher beneﬁts longer memory. decoupling classiﬁcation consistency consistency teacher predictions necessarily good proxy classiﬁcation task especially early training. model strongly coupled tasks using output both. would decoupling tasks change performance algorithm? investigate changed model layers produce outputs. trained outputs classiﬁcation consistency. also added mean squared error cost output logits varied weight cost allowing control strength coupling. looking results strongly coupled version performs well loosely coupled versions not. hand moderate decoupling seems beneﬁt making consistency ramp-up redundant. changing kl-divergence following laine aila mean squared error consistency cost function kl-divergence would seem natural choice. works better? experiments instances cost function family ranging kl-divergence found setting performs better cost functions. appendix details cost function family intuition performs well. experiments above used traditional -layer convolutional architecture beneﬁt making comparisons earlier work easy. order explore effect model architecture experiments using -block residual network shake-shake regularization cifar-. details model training procedure described appendix shown table results improve remarkably better network architecture. test whether methods scales natural images experiments imagenet dataset using labels. used -block resnext architecture clear improvement state art. test publicly available measured results using validation set. noise regularization neural networks proposed sietsma recently several types perturbations shown regularize intermediate representations effectively deep learning. adversarial training changes input slightly give predictions different possible original predictions. dropout zeroes random dimensions layer outputs. dropconnect generalizes dropout zeroing individual weights instead activations. stochastic depth drops entire layers residual networks swapout generalizes dropout stochastic depth. shake-shake regularization duplicates residual paths samples linear combination outputs independently forward backward passes. several semi-supervised methods based training model predictions consistent perturbation. denoising source separation framework uses denoising latent variables learn likelihood estimate. variant ladder network implements deep learning model classiﬁcation tasks. produces noisy student predictions clean teacher predictions applies denoising layer predict teacher predictions student predictions. model improves model removing explicit denoising layer applying noise also teacher predictions. similar methods proposed already earlier linear models deep learning virtual adversarial training similar model uses adversarial perturbation instead independent noise. idea teacher model training student related model compression distillation knowledge complicated model transferred simpler model training simpler model softmax outputs complicated model. softmax outputs contain information task one-hot outputs requirement representing knowledge regularizes simpler model. besides model compression distillation used harden trained models adversarial attacks difference distillation consistency regularization distillation performed training whereas consistency regularization performed training time. consistency regularization seen form label propagation training samples resemble likely belong class. label propagation takes advantage assumption pushing label information example examples near according metric. label propagation also applied deep learning models however ordinary label propagation requires predeﬁned distance metric input space. contrast consistency targets employ learned distance metric implied abstract representations model. model learns features distance metric changes accommodate features. therefore consistency targets guide learning ways. hand spread labels according current distance metric hand network learn better distance metric. temporal ensembling virtual adversarial training forms consistency regularization recently shown strength semi-supervised learning. paper propose mean teacher method averages model weights form target-generating teacher model. unlike temporal ensembling mean teacher works large datasets on-line learning. experiments suggest improves speed learning classiﬁcation accuracy trained network. addition scales well state-of-the-art architectures large image sizes. success consistency regularization depends quality teacher-generated targets. targets improved mean teacher virtual adversarial training represent ways exploiting principle. combination yield even better targets. probably additional methods uncovered improve targets trained models even further. thank samuli laine timo aila fruitful discussions work phil bachman colin raffel corrections pre-print version paper. also thank everyone curious company help encouragement ideas. references abadi martín agarwal ashish barham paul brevdo eugene chen zhifeng citro craig corrado greg davis andy dean jeffrey devin matthieu ghemawat sanjay goodfellow harp andrew irving geoffrey isard michael yangqing jozefowicz rafal kaiser lukasz kudlur manjunath levenberg josh mané monga rajat moore sherry murray derek olah chris schuster mike shlens jonathon steiner benoit sutskever ilya talwar kunal tucker paul vanhoucke vincent vasudevan vijay viégas fernanda vinyals oriol warden pete wattenberg martin wicke martin yuan zheng xiaoqiang. tensorflow large-scale machine learning heterogeneous systems. buciluˇa cristian caruana rich niculescu-mizil alexandru. model compression. proceedings sigkdd international conference knowledge discovery data mining yarin ghahramani zoubin. dropout bayesian approximation representing model uncertainty deep learning. proceedings international conference machine learning miyato takeru maeda shin-ichi koyama masanori ishii shin. virtual adversarial training regularization method supervised semi-supervised learning. arxiv. april arxiv netzer yuval wang coates adam bissacco alessandro andrew reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning papernot nicolas mcdaniel patrick somesh swami ananthram. distillation defense adversarial perturbations deep neural networks. arxiv. november arxiv yunchen henao ricardo yuan chunyuan stevens andrew carin lawrence. variational autoencoder deep learning images labels captions. arxiv. september arxiv rasmus antti berglund mathias honkala mikko valpola harri raiko tapani. semisupervised learning ladder networks. cortes lawrence sugiyama garnett advances neural information processing systems curran associates inc. russakovsky olga deng krause jonathan satheesh sanjeev sean huang zhiheng karpathy andrej khosla aditya bernstein michael berg alexander feifei imagenet large scale visual recognition challenge. arxiv. september arxiv sajjadi mehdi javanmardi mehran tasdizen tolga. regularization stochastic transformations perturbations deep semi-supervised learning. sugiyama luxburg guyon garnett advances neural information processing systems curran associates inc. salimans kingma diederik weight normalization simple reparameterization accelerate training deep neural networks. advances neural information processing systems salimans goodfellow zaremba wojciech cheung vicki radford alec chen improved techniques training gans. advances neural information processing systems srivastava nitish hinton geoffrey krizhevsky alex sutskever ilya salakhutdinov ruslan. dropout simple prevent neural networks overﬁtting. mach. learn. res. january issn table error rate percentage svhn cifar- runs including results without input augmentation. exponential moving average weights evaluation models. comparison methods -layer convnet architecture similar augmentation similar ours expect augmentation. replicated model laine aila tensorflow added support mean teacher training. modiﬁed model slightly match requirements experiments described subsections b... difference original model described laine aila baseline model thus depends experiment. difference layer input translation horizontal ﬂipa randomly gaussian noise convolutional convolutional convolutional pooling dropout convolutional convolutional convolutional pooling dropout convolutional convolutional convolutional pooling softmax ﬁlters padding ﬁlters padding ﬁlters padding maxpool ﬁlters padding ﬁlters padding ﬁlters padding maxpool ﬁlters valid padding ﬁlters padding ﬁlters padding average pool fully connected baseline model mean teacher model whether teacher weights identical student weights student weights. addition models backpropagate gradients sides model whereas mean teacher applies student side. table describes architecture convolutional network. applied mean-only batch normalization weight normalization convolutional softmax layers. used leaky relu nonlinearity convolutional layers. used cross-entropy student softmax output one-hot label classiﬁcation cost mean square error student teacher softmax outputs consistency cost. total cost weighted costs weight classiﬁcation cost expected number labeled examples minibatch subject ramp-ups described below. trained network minibatches size used adam optimizer training learning rate parameters baseline model applied gradients teacher student sides network. mean teacher model teacher model parameters updated training step using hyperparameters subject ramp-ups ramp-downs described below. applied ramp-up period training steps beginning training. consistency cost coefﬁcient learning rate ramped maximum values using sigmoid-shaped function used different training settings different experiments. cifar- experiment matched settings laine aila closely possible. svhn experiments diverged laine aila accommodate sparsity labeled data. table summarizes differences experiments. sampling minibatches labeled unlabeled examples treated equally thus number labeled examples varied minibatch minibatch. applied ramp-down last training steps. learning rate coefﬁcient ramped maximum value. adam ramped maximum value. ramp-downs performed using sigmoid-shaped function e−.x ramp-downs improve results used stay close possible settings laine aila normalized input images zero mean unit variance. semi-supervised training used labeled example unlabeled examples mini-batch. important speed training using extra unlabeled data. labeled examples used shufﬂed reused. similarly unlabeled examples used shufﬂed reused. applied different values adam decay rate ramp-up period rest training. values ﬁrst steps afterwards. helped -label case converge reliably. trained network steps using extra unlabeled examples steps using extra unlabeled examples steps using extra unlabeled examples. training supervised-only model baselines used hyperparameters training mean teacher except stopped training earlier prevent over-ﬁtting. supervisedruns include unlabeled examples apply consistency cost. trained supervised-only model cifar- steps using images steps using images steps using images steps using images. trained svhn steps using labels steps using labels. trained model cifar- steps using labels steps using labels steps using labels labels. trained svhn steps using labels steps using labels. cifar- replicated shake-shake regularized architecture described consisting residual blocks. trained network gpus using minibatches images labeled. sampled images described svhn experiments above. augmented input images random translations random horizontal ﬂips. used larger translation size earlier experiments.) normalized images channel-wise zero mean unit variance training data. trained network using stochastic gradient descent initial learning rate nesterov momentum trained epochs decaying learning rate cosine annealing would reached zero epochs. deﬁne epoch pass unlabeled examples labeled example included many times epoch. used total cost function consisting classiﬁcation cost three costs used dual output trick described subsection figure cost logits coefﬁcient simpliﬁed hyperparameter choices improved results. used consistency cost coefﬁcient ramping ﬁrst epochs using sigmoid ramp-up shape experiments above. also used weight decay coefﬁcient used decay value imagenet evaluation runs used -layer resnext architecture consisting residual blocks groups channels ﬁrst block. trained network gpus using minibatches images labeled. sampled images described svhn experiments above. following randomly augmented images using degree rotation crop aspect ratio resized pixels random horizontal color jitter. normalized images channel-wise zero mean unit variance training data. trained network using stochastic gradient descent maximum learning rate nesterov momentum ramped learning rate linearly ﬁrst epochs trained epochs decaying learning rate cosine annealing would reached zero epochs. used total cost function consisting classiﬁcation cost three costs used dual output trick described subsection figure cost logits coefﬁcient used kl-divergence consistency cost coefﬁcient ramping ﬁrst epochs using sigmoid ramp-up shape experiments above. also used weight decay coefﬁcient used decay value figure copy figure main text. validation error -label svhn four runs mean varying consistency cost shape hyperparameter mean squared error kl-divergence development phase work cifar- svhn datasets separated training data validation set. removed randomly labels remaining training data retaining equal number labels class. used different labels evaluation runs. retained labels validation enable exploration results. ﬁnal evaluation phase used entire training including validation labels removed. real-world case would possess large fully-labeled validation set. however setup useful research setting since enables thorough analysis results. best knowledge common practice carrying research semi-supervised learning. retaining hyperparameters previous work possible decreased chance over-ﬁtting results validation labels. imagenet experiments removed randomly labels training retaining equal number labels class. validation used given validation without modiﬁcations. used different training labels evaluation runs evaluated results validation set. exact reason performs better kl-divergence remains unclear form help explain modern neural network architectures tend produce accurate overly conﬁdent predictions assume true labels accurate discount conﬁdence teacher predictions. classiﬁcation cost consistency cost. discount conﬁdence approximations keeps gradients large enough provide useful training signal. however perform experiments validate explanation.", "year": 2017}