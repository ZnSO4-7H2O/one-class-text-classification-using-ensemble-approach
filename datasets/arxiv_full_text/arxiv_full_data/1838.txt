{"title": "Counterfactual Learning from Bandit Feedback under Deterministic  Logging: A Case Study in Statistical Machine Translation", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "The goal of counterfactual learning for statistical machine translation (SMT) is to optimize a target SMT system from logged data that consist of user feedback to translations that were predicted by another, historic SMT system. A challenge arises by the fact that risk-averse commercial SMT systems deterministically log the most probable translation. The lack of sufficient exploration of the SMT output space seemingly contradicts the theoretical requirements for counterfactual learning. We show that counterfactual learning from deterministic bandit logs is possible nevertheless by smoothing out deterministic components in learning. This can be achieved by additive and multiplicative control variates that avoid degenerate behavior in empirical risk minimization. Our simulation experiments show improvements of up to 2 BLEU points by counterfactual learning from deterministic bandit feedback.", "text": "goal counterfactual learning statistical machine translation optimize target system logged data consist user feedback translations predicted another historic system. challenge arises fact riskaverse commercial systems deterministically probable translation. lack sufﬁcient exploration output space seemingly contradicts theoretical requirements counterfactual learning. show counterfactual learning deterministic bandit logs possible nevertheless smoothing deterministic components learning. achieved additive multiplicative control variates avoid degenerate behavior empirical risk minimization. simulation experiments show improvements bleu points counterfactual learning deterministic bandit feedback. interaction user translated content. indirect feedback form user clicks displayed shown valuable feedback signal response prediction display advertising similar computational advertising scenario could imagine scenario systems optimized partial information form user feedback predicted translations instead manually created reference translations. learning scenario investigated areas bandit learning reinforcement learning figure illustrates learning protocol using terminology bandit structured prediction round system makes prediction receives reward used update system. commercial systems allow record large amounts interaction data cost. logs typically contain record source translation predicted system user feedback. latter gathered directly explicit user quality ratings translations supported inferred indirectly counterfactual learning attempts reuse existing interaction data predictions made historic system different target system. enables ofﬂine batch learning logged data important online experiments deploy target system risky and/or expensive. counterfactual learning tasks include policy evaluation i.e. estimating target policy would performed control choosing predictions rewards logged policy optimization i.e. optimizing parameters target policy given logged data historic system. tasks called counterfactual off-policy terms since target policy actually control logging. figure shows learning protocol off-policy learning partial feedback. crucial trick obtain unbiased estimators evaluate optimize off-policy system correct sampling bias logging policy. done importance sampling estimate corrected inverse propensity score historical algorithm mitigating problem predictions favored historical system over-represented logs. shown langford strehl sufﬁcient exploration output space logging system prerequisite counterfactual learning. logging policy acts stochastically predicting outputs condition satisﬁed inverse propensity scoring applied correct sampling bias. however commercial systems usually avoid risk probable translation. effectively results deterministic logging policies making theory practice off-policy methods inapplicable counterfactual learning smt. paper presents case study counterfactual learning shows policy optimization deterministic bandit logs possible despite seemingly contradictory theoretical requirements. formalize learning problem empirical risk minimization logged data. simple empirical risk minimizer show degenerate behavior objective minimized avoiding over-representing training samples thus suffering decreased generalization ability show control variates remedy problem. techniques doubly-robust policy evaluation learning weighted importance sampling interpreted additive multiplicative control variates serve variance reduction estimation. observe effect techniques smoothing deterministic components taking whole output space account. furthermore conjecture outputs logged deterministically stochastic selection inputs serves sufﬁcient exploration parameter optimization joint feature representation inputs outputs. present experiments using simulated bandit feedback different tasks showing improvements bleu domain adaptation deterministically logged bandit feedback. result together comparison standard case policy learning stochastically logged simulated bandit feedback conﬁrms effectiveness proposed techniques. reward function quantifying quality structured outputs. given data triples outputs inputs generated logging system loss values observed generated data points. case stochastic logging probability inverse propensity scoring approach uses importance sampling achieve unbiased estimate expected loss parametric target policy equation assumes deterministically logged outputs propensity historical system. call objective deterministic propensity matching objective since matches deterministic outputs logging system outputs n-best list target system. optimization deterministic logging sampling bias unavoidable since objective correct importance sampling. furthermore estimator show degenerate behavior learning. problem remedied control variates discuss section learning principle doubly controlled empirical risk minimization. ﬁrst modiﬁcation equation originally motivated weighted importance sampling inverse propensity scoring dudik inter alia) reinforcement learning precup jiang thomas brunskill inter alia) structured prediction inter alia). idea behind approaches ﬁrst perform policy evaluation policy optimization under assumption better evaluation leads better optimization. work puts focus policy optimization empirical risk minimization framework deterministically logged data. simulation study compare deterministic case standard scenario policy optimization evaluation stochastic logging. variance reduction additive control variates implicitly used doubly robust techniques however connection monte carlo techniques made explicit thomas brunskill control variate technique optimizing variance reduction adjusting linear interpolation scalar applied off-policy learning. similarly technique weighted importance sampling used variance reduction technique off-policy learning connection multiplicative control variates made explicit swaminathan joachims knowledge analysis control variate techniques perspective avoiding degenerate behavior learning deterministically logged data novel. problem deﬁnition. problem counterfactual learning bandit structured prediction described follows structured input space possible output structures input modiﬁcation equation motivated incorporation direct reward estimation method inverse propensity scorer proposed doubly-robust estimator regression-based reward model trained logged data scalar allows optimize estimator minimal variance deﬁne doubly controlled empirical risk minimization objective ˆrˆc follows perspective monte carlo simulation doubly robust estimator seen variance reduction additive control variates random variables. expectation equation rewritten e¯πw+ variance term x−ˆc var+ˆcvar− cov. chap. shows variance estimator reduced variable representing reward function variable representing regression-based reward model positively correlated. optimal scalar parameter perspective monte carlo simulation advantage modiﬁcation explained viewing reweighting multiplicative control variate joachims random varit= ables approximately written follows cov). shows positive correlation variable representing target model probability variable representing target model scaled task loss function reduce variance estimator. since exponentially many outputs choose input logging variance reduction useful counterfactual learning even deterministic case. under stochastic logging policy similar modiﬁcation done objective reweighting ratio reweighted objective called ips+r comparison experiments stochastically logged data. case stochastic logging reweighted target probability replaced reweighted ratio ¯ρt. reweighted models original doubly robust model without optimal called experiments stochastic logging. learning algorithms. applying stochastic gradient descent update rule objective functions deﬁned leads variety algorithms. gradients objectives derived using score function gradient estimator shown table stochastic gradient descent algorithms apply differentiable policy thus methods applied variety systems including linear non-linear models. since previous work off-policy methods contextual bandits done area linear classiﬁcation start adaptation off-policy methods linear models work. assume gibbs model setup. experiments simulate following scenario assume possible divert small fraction user interaction trafﬁc purpose policy evaluation perform stochastic logging small data set. main trafﬁc assumed logged deterministically following conservative regime one-best translations used system change frequently time. since experiments simulation studies additionally perform stochastic logging compare policy learning case deterministic logging case stochastic logging. deterministic-based policy learning experiments evaluate empirical risk minimization algorithms derived objectives doubly controlled objective employ variants first second calculate described equation algorithms used policy evaluation stochastic-based policy learning variants objectives replace yield estimators ips+r expected loss. objectives employed domain adaptation scenario machine translation. system trained out-of-domain data used collect feedback in-domain data. data serve logged data learning experiments. conduct tasks hypergraph re-decoding ﬁrst german-to-english trained using concatenation europarl corpus common crawl corpus news commentary corpus goal adapt trained system domain transcribed talks using parallel corpus second task uses french-to-english europarl data goal domain adaptation news articles news commentary corpus split parts corpus used validation test data learning experiments. validation data news commentary corpus splits provided shared task namely nc-devtest validation data nc-test test data. overview data statistics seen table baseline out-of-domain system built using scfg framework cdec dense features tokenizing lowercasing training data data word aligned using cdec’s fast align. -gram language model build target languages out-of-domain data using kenlm news additionally assume access in-domain target language text train another in-domain language model data increasing number features news. framework uses standard linear gibbs model whose distribution peaked using parameter higher value shift probability one-best translation closer others closer using training promote learn models optimal outputting one-best translation. experiments found work well validation data. additionally tune system using cdec’s mert implementation indomain data references. fullinformation in-domain system conveys best possible improvement using given training data. thus seen oracle system systems learnt using input-side training data bandit feedback available learning signal. systems evaluated using corpus-level bleu metric in-domain training data corpora using original out-of-domain systems logging one-best translation. stochastic experiments translations sampled model distribution. feedback logged translation simulated using reference sentence-level bleu direct reward estimation. creating logged data also record feature vectors translations train direct reward estimate needed using feature vector input per-sentence bleu output value train regressionbased random forest trees using scikitlearn measure performance perform -fold cross-validation measure macro average estimated rewards true rewards micro average quantiﬁes expect model random sample model used experiments trained full training data. cross-validation results regression-based direct reward model found table policy evaluation. policy evaluation aims logged data estimate performance target system small logged data deval diverted policy evaluation created translating sentences in-domain training data out-ofdomain system sample translations according model probability. record sentence-level bleu feedback. reference translations also exist order multiplicative control variate effective learning procedure utilize mini-batches. mini-batch size chosen small estimates control variates reliable. test mini-batch sizes examples whereas news means perform batch training since mini-batch spans entire training set. minibatch size early stopping point selected choosing setup iteration achieved highest bleu score one-best translations validation data. learning rate selected whereas possible values alternatively adadelta sets learning rate per-feature basis. results validation test reported table statistical signiﬁcance outof-domain system compared systems measured using approximate randomization testing deterministic case general dpm+r shows lowest increase still signiﬁcantly outperform baseline. explanation dpm+r cannot improve further addressed separately below. yields improvements bleu points obtains improvements bleu points out-of-domain baseline. detail data close nearly bleu half out-of-domain full-information indomain system. improve bleu signiﬁcant improvement also note that takes iterations reach best result validation data already outperforms stopping iteration point better bleu validation continues increase stopping iteration. ﬁnal results falls bleu behind oracle system references available learning process. conk sentences used measure ground truth bleu value translations using fullinformation in-domain system. goal evaluation achieve value ips+r deval close possible ground truth bleu value. able measure variance create folds deval differing random seeds. report average difference ground truth bleu score value log-based policy evaluation well standard deviation table ips+r underestimates bleu value news. overestimates instead. achieves closest estimate overestimating true value less bleu. policy evaluation results overestimates. variants overestimation result explained random forests’ tendency overestimate. optimal correct this always sufﬁcient way. policy learning. learning experiments learning starts weights outof-domain model. system produced logged data ﬁrst iteration translations one-best position. iterations however translation logged ﬁrst position more. case n-best list searched correct translation. speed reasons scores translation system normalized probabilities using ﬁrst unique entries n-best list rather using table bleu increases learning out-of-domain baseline validation test set. outof-domain baseline starting system in-domain oracle system tuned in-domain data references. deterministic case results statistically signiﬁcant regards baseline. stochastic case results statistically signiﬁcant regards baseline except ips+r news corpus. sidering substantial difference information systems available remarkable. improvements news corpus show similar tendencies. nearly bleu close improvement bleu points achieve notable result. able improve successfully case corpus. analyzing actual values calculated experiments allows gain insight case average case news however maximum value thus stays quite close would equate using thus surprising signiﬁcant difference comparison stochastic case. even realistic commercial applications simulation study allows stochastically large amounts data order compare learning deterministic logs standard case. shown table relations between algorithms even absolute improvements similar stochastic deterministic logging. signiﬁcance tests deterministic/stochastic experiment pair show signiﬁcant difference case dc/dr data. however result still signiﬁcantly outperform best deterministic objective values experiment pairs conclude indeed acceptable practice deterministically. langford show counterfactual learning impossible unless logging system sufﬁciently explores output space. condition seemingly satisﬁed logging systems acts according deterministic policy. furthermore since techniques exploration time applicable commercial systems frequently changed time case counterfactual learning seems hopeless. however experiments present evidence contrary. following present analysis aims explain apparent contradiction. implicit exploration. experimental comparison stochastic deterministic logging bandit learning computational advertising chapelle observed varying contexts induces enough exploration selection learning becomes possible. similar implicit exploration also attributed case identical input word phrase lead depending words phrases input sentence different output words phrases. moreover identical output word phrase appear different output sentences. across entire implicitly performs exploration phrase translations seems missing ﬁrst glance. smoothing multiplicative control variates. estimator show degenerate behavior objective minimized simply setting probability every logged data point over-represents logged data received rewards undesired. furthermore systems optimized objective cannot properly discriminate between translations output space. seen case translation invariance objective previously noted swaminathan joachims adding small constant probability every data point increases overall value objective without improving discriminative power high-reward low-reward translations. dpm+r solves degeneracy deﬁning probability distribution logged data reweighting multiplicative control variate. reweighting objective value decrease probability low-reward translation increased takes away probability mass other higher reward samples. trade-off balancing probabilities low-reward high-reward samples becomes important desired. smoothing additive control variates. despite reweighting dpm+r still show degenerate behavior setting probabilities highest-reward samples avoiding logged data points. clearly hampers generalization ability model since inputs avoided training receive proper ranking translations. additive control variate solve problem using reward estimate takes full output space account. objective increased probability translations high estimated reward increased even seen training. shift probability mass unseen data high estimated-reward thus improve generalization ability model. paper showed off-policy learning deterministic bandit logs possible smoothing techniques based control variates used. techniques avoid degenerate behavior learning improve generalization empirical risk minimization logged data. furthermore showed standard off-policy evaluation applicable stochastic logging policies. knowledge ﬁrst application counterfactual learning complex structured prediction problem like smt. since objectives agnostic choice underlying model also possible transfer techniques non-linear models neural machine translation. desideratum future work.", "year": 2017}