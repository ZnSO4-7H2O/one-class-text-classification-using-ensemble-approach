{"title": "Graph-Structured Representations for Visual Question Answering", "tag": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "This paper proposes to improve visual question answering (VQA) with structured representations of both scene contents and questions. A key challenge in VQA is to require joint reasoning over the visual and text domains. The predominant CNN/LSTM-based approach to VQA is limited by monolithic vector representations that largely ignore structure in the scene and in the form of the question. CNN feature vectors cannot effectively capture situations as simple as multiple object instances, and LSTMs process questions as series of words, which does not reflect the true complexity of language structure. We instead propose to build graphs over the scene objects and over the question words, and we describe a deep neural network that exploits the structure in these representations. This shows significant benefit over the sequential processing of LSTMs. The overall efficacy of our approach is demonstrated by significant improvements over the state-of-the-art, from 71.2% to 74.4% in accuracy on the \"abstract scenes\" multiple-choice benchmark, and from 34.7% to 39.1% in accuracy over pairs of \"balanced\" scenes, i.e. images with fine-grained differences and opposite yes/no answers to a same question.", "text": "figure encode input scene graph representing objects spatial arrangement input question graph representing words syntactic dependencies. neural network trained reason representations produce suitable answer prediction output vocabulary. scenes experiments focus dataset clip scenes allow focus semantic reasoning vision-language interactions isolation performance visual recognition also allow manipulation image data better illuminate algorithm performance. particularly attractive dataset introduced selecting questions binary answers pairing image minimally-different complementary version elicits opposite answer strongly contrasts datasets real images correct answer often obvious withlooking image relying systematic regularities frequent questions answers performance improvements reported datasets difﬁcult interpret actual progress scene understanding reasoning might similarly taken represent better modeling language prior dataset. hampers best obscures progress toward greater goal general vqa. view despite obvious limitations synthetic images improvements aforementioned balanced dataset constitute illuminating measure progress scene-understanding language paper proposes improve visual question answering structured representations scene contents questions. challenge require joint reasoning visual text domains. predominant cnn/lstm-based approach limited monolithic vector representations largely ignore structure scene question. feature vectors cannot effectively capture situations simple multiple object instances lstms process questions series words reﬂect true complexity language structure. instead propose build graphs scene objects question words describe deep neural network exploits structure representations. show approach achieves signiﬁcant improvements state-of-the-art increasing accuracy abstract scenes multiple-choice benchmark challenging balanced scenes i.e. image pairs ﬁne-grained differences opposite yes/no answers question. task visual question answering received growing interest recent years example). interesting aspects problem combines computer vision natural language processing artiﬁcial intelligence. open-ended form question provided text natural language together image correct answer must predicted typically form single word short phrase. multiple-choice variant answer selected provided candidates alleviating evaluation issues related synonyms paraphrasing. challenges questions clip-art dataset vary greatly complexity. directly answered observations visual elements e.g. room weather good others require relating multiple facts understanding complex actions e.g. going catch ball? winter?. additional challenge affects datasets sparsity training data. even large number training questions cannot possibly cover combinatorial diversity possible objects concepts. adding challenge methods process question recurrent neural network trained scratch solely training questions. language representation reasons motivate take advantage extensive existing work natural language community processing questions. first identify syntactic structure question using dependency parser produces graph representation question node represents word edge particular type dependency second associate word vector embedding pretrained large corpora text data embedding maps words space distances semantically meaningful. consequently essentially regularizes remainder network share learned concepts among related words synonyms. particularly helps dealing rare words also allows questions include words absent training questions/answers. note pretraining processing language part mimics practice common image part visual features usually obtained ﬁxed pretrained larger dataset different objective. scene representation object scene corresponds node scene graph associated feature vector describing appearance. graph fully connected edge representing relative position objects image. semantic relationships elements contrasts typical approach representing image activations processing words question serially graph representation ignores order elements processed instead represents relationships different elements using different edge types. network uses multiple layers iterate features associated every node ultimately identiﬁes soft matching nodes graphs. matching reﬂects correspondences words question objects image. features matched nodes feed classiﬁer infer answer question main contributions paper four-fold. describe graph representations scene question neural network capable processing representations infer answer. show make off-the-shelf language parsing tool generating graph representation text captures grammatical relationships making information accessible model. representation uses pre-trained word embedding form node features encodes syntactic dependencies between words edge features. train proposed model abstract scenes benchmark demonstrate efﬁcacy raising state-of-the-art accuracy multiple-choice setting. balanced version dataset raise accuracy hardest setting evaluate uncertainty model presenting ﬁrst time task precision/recall curves predicted answers. curves provide insight single accuracy metric show uncertainty estimated model predictions correlates ambiguity human-provided ground truth. applying neural networks graphs graph representations feed deep neural network describe section advantage approach textscene-graphs rather typical representations graphs capture relationships between words objects semantic signiﬁcance. enables exploit unordered nature scene elements task visual question answering received increasing interest since seminal paper antol recent methods based idea joint embedding image question using deep neural network. image passed convolutional neural network pretrained image classiﬁcation intermediate features extracted describe image. question typically passed refigure architecture proposed neural network. input provided description scene parsed question scene-graph contains node feature vector object edge features represent spatial relationships. question-graph reﬂects parse tree question word embedding node vector embedding types syntactic dependencies edges. recurrent unit associated node graphs. multiple iterations updates representation node integrates context neighbours within graph. features objects words combined pairwise weighted form attention. effectively matches elements question scene. weighted features passed ﬁnal classiﬁer predicts scores ﬁxed candidate answers. current neural network lstm produces ﬁxed-size vector representing sequence words. representations mapped joint space several non-linear layers. classiﬁer output vocabulary predicting ﬁnal answer. recent papers propose improvements variations basic idea. consult survey. major improvement basic method attention mechanism models interactions speciﬁc parts inputs depending actual contents. visual input typically represented spatial feature instead holistic image-wide features. feature used question determine spatial weights reﬂect relevant regions image. approach uses similar weighting operation which graph representation equate subgraph matching. graph nodes representing question words associated graph nodes representing scene objects vice versa. similarly co-attention model determines attention weights image regions question words. best-performing approach proceeds sequential manner starting question-guided visual attention followed image-guided question attention. case found joint one-pass version performs better. major contribution model structured representations input scene question. contrasts typical models limited spatial feature maps sequences words respectively. dynamic memory networks applied also maintain set-like representation input. model models interactions between different parts input. method additionally take input features characterizing arbitrary relations parts input speciﬁcally allows making syntactic dependencies words pre-parsing question. systems trained end-to-end questions images answers exception visual feature extractor typically pretrained image classiﬁcation. language processing part methods address semantic aspect word embeddings pretrained language modeling task syntactic relationships words question typically overlooked however. hand-designed rules serve identify primary secondary objects questions. neural module networks question processed dependency parser fragments parse selected ﬁxed rules associated modules assembled full neural network. contrast method trained make direct output syntactic parser. neural networks graphs received signiﬁcant attention recently approach similar gated graph sequence neural network associate gated recurrent unit node updates feature vector node iteratively passing messages neighbours. also related work vinyals embedding ﬁxed-size vector invariant order elements. feeding entire recurrent unit multiple times. iteration uses attention mechanism focus different parts set. formulation similarly incorporates information neighbours node feature multiple iterations advantage using attention mechanism within tions input data training test instance question parameterized description contents scene. question processed stanford dependency parser outputs following. words constitute nodes question graph. word represented index input vocabulary token dataset provides following information image objects constitute nodes scene graph. node represented vector visual features please refer supplementary material implementation details. experiments carried datasets clip scenes descriptions scenes provided form lists objects visual features. method equally applicable real images object list replaced candidate object detections. experiments clip allows effect proposed method isolated performance object detector. please refer supplementary material implementation details. features nodes edges projected vector space common dimension question nodes edges vector embeddings implemented look-up tables scene nodes edges afﬁne projections graphs representing question scene processed independently recurrent architecture. drop exponents paragraph procedure applies graphs. node associated gated recurrent unit processed ﬁxed number iterations square brackets semicolon represent concatenation vectors hadamard product. ﬁnal state used representation nodes pool operation transforms features variable number neighbours ﬁxed-size representation. commutative operation used implementation found best performance average function taking care averaging variable number connected neighbours. intuitive interpretation recurrent processing progressively integrate context information connected neighbours node’s representation. node corresponding word ’ball’ instance might thus incorporate fact associated adjective ’red’. formulation similar slightly different gated graph networks propagation information model limited ﬁrst order. note graphs typically densely connected. introduce form attention model constitutes essential part model. motivation two-fold identify parts input data relevant produce answer align speciﬁc words question particular elements scene. practically estimate relevance possible pairwise combination words objects. precisely compute scalar matching weights weights comparanode sets attention weights models therefore learned weights biases logistic function introduces nonlinearity bounds weights formulation similar cosine similarity learned weights feature dimensions. note weights computed using initial embedding node features apply scalar weights corresponding pairwise combinations question scene features thereby focusing giving importance matched pairs weighted features scene elements question learned weights biases relu softmax logistic function summations scene elements question elements form pooling brings variable number features ﬁxed-size output. ﬁnal output vector contains scores possible answers number dimensions equal binary questions balanced dataset number candidate answers abstract scenes dataset. candidate answers appearing least times training evaluation datasets evaluation uses datasets original abstract scenes antol balanced extension contain scenes created humans drag-and-drop interface arranging clip objects ﬁgures. original dataset contains k/k/k scenes k/k/k questions humanprovided ground-truth answers. questions categorized based type correct answer yes/no number other method used categories type test questions unknown. balanced version dataset contains subset questions binary answers addition complementary scenes created elicit opposite answer question. signiﬁcant guessing modal answer training succeed half time give accuracy complementary pairs. contrasts datasets blind guessing effective. pairs complementary scenes also typically differ objects displaced removed slightly modiﬁed makes questions challenging requiring take account subtle details scenes. metrics main metric average score soft accuracy takes account variability ground truth answers multiple human annotators. refer test question index possible answer output vocabulary index ground truth score answer provided annotators. otherwise method outputs predicted score question answer overall accuracy average ground truth score highest prediction question i.e. argued balanced dataset better evaluate method’s level visual understanding datasets less susceptible language priors dataset regularities initial experiments conﬁrmed performances various algorithms balanced dataset indeed better separated used ablative analysis. also focus hardest evaluation setting measures accuracy pairs complementary scenes. metric blind models obtain null accuracy. setting also consider pairs test scenes deemed ambiguous disagreement annotators. test scene still evaluated independently however model unable increase performance forcing opposite answers pairs questions. metric standard hard accuracy i.e. ground truth scores please refer supplementary material additional details. compare method three models proposed ensemble models exploiting either lstm processing question elaborate hand-designed rules identify objects focus question. visual features three models respectively empty global focused objects identiﬁed question. models speciﬁcally designed binary questions whereas generally applicable. nevertheless obtain signiﬁcantly better accuracy three differences performance mostly visible pairs setting believe reliable discards ambiguous test questions human annotators disagreed. training take care keep pairs complementary scenes together forming mini-batches. signiﬁcant positive effect stability optimization. interestingly notice tendency toward overﬁtting training balanced scenes. hypothesize pairs complementary scenes strong regularizing effect force learned model focus relevant details scenes. fig. visualize matching weights question words scene objects expected tend larger semantically related figure precision/recall abstract scenes balanced datasets scores assigned model predicted answers reliable measure certainty strict threshold ﬁlters incorrect answers produces high precision. abstract scenes dataset slight advantage brought training soft target scores capture ambiguities human-provided ground truth. best performance still absolute terms understandable wide range concepts involved questions seems unlikely concepts could learned training question/answers alone suggest signiﬁcant improvement performance require external sources information training and/or test time. ablative evaluation evaluated variants model measure impact various design choices question side evaluate graph approach without syntactic parsing building question graphs types edges previous/next linking consecutive nodes. shows advantage using graph method together syntactic parsing. optimizing word embeddings scratch rather pretrained glove vectors produces signiﬁcant drop performance. scene side removed edge features setting conﬁrms model makes spatial relations objects encoded edges graph. rows disabled recurrent graph processing either question scene both. ﬁnally tested model uniform matching weights expected performed poorly. weights similarly attention mechanisms models observations conﬁrm mechanisms crucial good performance. global image features attention-based image features graph question parsing question word embedding pretrained scene edge features graph processing disabled question graph processing disabled scene graph processing disabled question/scene graph processing iteration question graph processing iteration scene graph processing iteration question/scene uniform matching weights candidate answers almost reported results consist single accuracy metric. provide insight produce precision/recall curves predicted answers. precision/recall point obtained setting threshgraphs similarities simple concatenation words would reveal trained model limited subsets training data unsurprisingly performance grows steadily amount training data suggests larger datasets would improve performance. opinion however seems unlikely sufﬁcient data covering possible concepts could collected form question/answer examples. data however brought sources information supervision. parsing word embeddings small step direction. techniques clearly improve generalization effect particularly visible case relatively small number training examples unclear whether huge datasets could ultimately negate advantage. future experiments larger datasets answer question. report results original abstract scenes dataset table evaluation performed automated server allow extensive ablative analysis. anecdotally performance validation corroborates ﬁndings presented above particular strong beneﬁt pre-parsing pretrained word embeddings graph processing gru. time submission method occupies place leader board open-ended multiple choice settings. advantage existing method pronounced binary counting questions. refer fig. supplementary visualizations results. presented deep neural network visual question answering processes graph-structured representations scenes questions. enables leveraging existing natural language processing tools particular pretrained word embeddings syntactic parsing. latter showed signiﬁcant advantage traditional sequential processing questions e.g. lstms. opinion systems unlikely learn everything question/answer examples alone. believe significant improvement performance require additional sources information supervision. explicit processing language part small step direction. clearly shown improve generalization without resting entirely vqa-speciﬁc annotations. applied method datasets clip scenes. direct extension real images addressed future work replacing nodes input scene graph proposals pretrained object detectors. figure impact amount training data performance language preprocessing always improve generalization pre-parsing pretrained word embeddings positive impact individually effects complementary other. indicator function. plot precision/recall curves fig. datasets. predicted score proves reliable indicator model conﬁdence threshold achieve near-perfect accuracy ﬁltering harder and/or ambiguous test cases. compare models trained either softmax sigmoid ﬁnal non-linearity common practice train softmax hard classiﬁcation objective using cross-entropy loss answer highest ground truth score target. attempt make better multiple human-provided answers propose soft ground truth scores target logarithmic loss. shows advantage abstract scenes dataset dataset soft target scores reﬂect frequent ambiguities questions scenes synonyms constitute multiple acceptable answers. cases avoid potential confusion induced hard classiﬁcation speciﬁc answer. balanced dataset nature contains almost ambiguities signiﬁcant difference different training objectives effect training size motivation introducing language parsing pretrained word embeddings better generalize concepts learned limited training examples. words representing semantically close concepts ideally assigned close word embeddings. similarly paraphrases similar questions produce parse figure qualitative results abstract scenes dataset balanced pairs show input scene question predicted answer correct answer prediction erroneous. also visualize matrices matching weights question words scene objects matching weights also visualized objects scene summation words giving indication estimated relevance. ground truth object labels reference only used training inference.", "year": 2016}