{"title": "Deep Learning with Data Dependent Implicit Activation Function", "tag": ["cs.LG", "cs.CV", "stat.ML", "68Txx"], "abstract": "Though deep neural networks (DNNs) achieve remarkable performances in many artificial intelligence tasks, the lack of training instances remains a notorious challenge. As the network goes deeper, the generalization accuracy decays rapidly in the situation of lacking massive amounts of training data. In this paper, we propose novel deep neural network structures that can be inherited from all existing DNNs with almost the same level of complexity, and develop simple training algorithms. We show our paradigm successfully resolves the lack of data issue. Tests on the CIFAR10 and CIFAR100 image recognition datasets show that the new paradigm leads to 20$\\%$ to $30\\%$ relative error rate reduction compared to their base DNNs. The intuition of our algorithms for deep residual network stems from theories of the partial differential equation (PDE) control problems. Code will be made available.", "text": "though deep neural networks achieve remarkable performances many artiﬁcial intelligence tasks lack training instances remains notorious challenge. network goes deeper generalization accuracy decays rapidly situation lacking massive amounts training data. paper propose novel deep neural network structures inherited existing dnns almost level complexity develop simple training algorithms. show paradigm successfully resolves lack data issue. tests cifar cifar image recognition datasets show paradigm leads relative error rate reduction compared base dnns. intuition algorithms deep residual network stems theories partial differential equation control problems. code made available. last decades many initialization optimization regularization many techniques invented make deep neural networks easily applicable solving challenging artiﬁcial intelligence tasks nevertheless classical dnns like networks problem degradation i.e. network goes deeper training testing errors increase even sufﬁcient training data deep residual networks especially pre-activated ones proposed employ shortcut connections learn residuals keep clean information path efﬁciently solve aforementioned degradation problem. furthermore deep department mathematics ucla angeles california department mathematical sciences mathematical sciences center tsinghua university beijing china department mathematics duke university durham north carolina usa. correspondence wang <wangbaonjgmail.com>. many advances made emergence deep resnets. include theoretical analysis algorithmic development. original work pre-activated resnets formulated discrete dynamical systems. dynamical system point view leads elegant analysis optimality identity resnets. hardt matrix factorization techniques analyze landscape linear resnets representation power. considers deep resnets control problem class continuous dynamical systems network structures development thriving theoretical analysis. instead using single shortcut connect consecutive residual blocks densely connect convolutional networks employ shortcut connections connect distinct blocks wide residual networks increase width layers original resnets. dense nets wider resnets certain amount improvement compared resnets. dual path networks another family interesting improvement resnets dense nets accuracy externally depends massive amounts training data. lack sufﬁcient training data typically leads another degradation problem. signiﬁcant accuracy reduction tends occur network goes deeper demonstrated paper. many regularization techniques explored attempt tackle challenge satisfactory results rare. existing strategies classiﬁed either loss function regularization network structure regularization. none considered speciﬁcity data critical importance data analysis. paper solve issue lacking enough training data using information data prior train dnns. ﬁrst build connection between deep resnets partial differential equation control problems. well-posedness theories control problems suggest take data dependent activations dnns. make data dependent activation trainable propose surgeries existing dnns construct data dependent implicitly activated dnns. efﬁcient algorithms train test model investigated. numerical results cifar dataset quite limited randomly selected instances show great success paradigm solving challenge insufﬁcient data. another successful achievement framework regarding generalization error reduction. cifar cifar datasets receive error reduction respectively compared base dnns includes resnet pre-activated resnet families. method provides alternative towards model compression important applications mobile devices. paper structured follows section present connection control problems deep resnets improvements deep resnets motivated theory. data interpolation general manifold harmonic extension manner reviewed section surgeries along training/testing procedures presented section validate algorithms large variety numerical results demonstrated section summary work future directions discussed section deep resnets especially pre-activated resnets realized adding shortcut connections connect consecutive residual blocks classical convolutional neural networks also regarded cascade residual block shown fig. followed ﬁnal ﬂatten activation layers. mathematically residual block formulated corresponds weight layers residual block relu step forward euler discretization equivalent advancing layer deep resnets numerical solution transport equation given order guarantee representability wnll labeled data cover types instances data pool. this give necessary condition theorem theorem suppose data pool formed classes data uniformly number instances class sufﬁciently large. want classes data sampled least once average distinct types instances already sampled. follows probability instance different type n−i+ essentially obtain i-th distinct type random variable follows geometric distribution n−i+ n−i+. thus deep resnets enable hierarchical representation learning leads fabulous performance many artiﬁcial intelligence tasks. wnll harmonic extension approach manifold extension analytically adaptable even labeled instances extremely scarce. illustrated connection deep resnets control problems data dependent terminal value control problem i.e. data dependent activation function deep resnets better activation functions e.g. softmax linear activations. section denotes training label instance control problem uniquely determined terminal value velocity ﬁeld conventionally corresponding terminal value transport equation selected softmax activation function shown control problem point view softmax function good terminal condition since pre-determined maybe real value ideal terminal function smooth function close labeled value training based observation weighted nonlocal laplacian seems provide good choice terminal function. section brieﬂy discuss smooth interpolation general smooth manifold give sufﬁcient condition number samples needed make sure interpolation enough representation diversity. consider following interpolation problem p··· points manifold s··· subset suppose labels data want extend label function entire dataset harmonic extension natural approach minimizes following dirichlet energy functional discuss efﬁciently perform label extension harmonic extension manner wnll deep resnets. framework following beneﬁts deep resnet learn optimal representations wnll interpolation; simultaneously wnll activation layer learned deep representations better utilized classical activation functions. show on-the-ﬂy coupling information feedforward error back propagation solves lack data issue achieves great accuracy improvement compared existing dnns. ﬁrst discuss numerical approach solve wnll interpolation given eq.. numerical approach straight forward computational burdens involved ﬁnding weights solving resulting linear system. pairwise weights need perform nearest neighbor searching. brute-force approach quadratic scaling however many fast algorithms sub-linear scaling purpose e.g. kd-tree ball-tree etc. adopt approximate nearest neighbor searching algorithm scalable extremely large scale high dimensional data. resulted linear system sparse positive deﬁnite efﬁciently solved conjugate gradient method work. worth emphasizing order guarantee wnll interpolation suitable represent classes instances labeled instances least around number classes. important component algorithm wnll activation layer dnns design efﬁcient algorithms information feed-forward propagation error back-propagation. information error propagation paths demonstrated fig.. standard e.g. resnet plotted fig. ’dnn’ block represents layers except last softmax activation function. naive approach place wnll simply replace softmax function wnll. however case though information feed-forwarded error cannot back-propagated since wnll implicitly deﬁnes activation function learned representation gradient explicitly available. efﬁciently train network wnll activation introduce structure inherited standard depicted chart fig.. structure quite ﬂexible inherited existing dnn. equip blocks standard buffer block wnll activation function buffer block simply chosen composition fully connected layer preserves dimension input tensor followed relu function. buffer block made complicated. buffer block tensor passed activations step training epochs network freeze linear activation blocks tune ’buffer block’. order back-propagate error ground-truth wnll interpolated results feed data pre-trained linear activation function corresponding computational graph perform error back-propagation. trained network generalization step wnll activation ﬁnal inference. algorithm designed greedy fashion. following numerical results validate efﬁciency training algorithm superiority network structure. training testing procedures proposed network summarized algorithms respectively. remark back-propagation computational graph linear function approximate wnll function since linear functions simplest nontrivial harmonic functions. mixed gaussian seems appealing approximation since compatible wnll. continue explore better approximations subsequent work. validate accuracy efﬁciency robustness proposed model present numerical results different tests cifar cifar mnist svhn dataset. generally believe difﬁculty datasets ranked cifar followed cifar svhn easiest mnist. numerical experiments take standard data augmentation widely used cifar datasets mnist svhn data without data augmentation. order computational graph linear function approximate wnll need dynamical computational graph. purpose implement algorithm pytorch platform automatic differentiation used figure illustration architectures. panel depicts standard block represents layers except last activation layer network. panel plots standard network last layer replaced wnll layer. panel shows network structure used work detailed explanation presented paper. diving performance dnns ﬁrst compare performance wnll interpolation shallow classiﬁers different datasets. table lists performance k-nearest neighbors support vector machine kernel softmax regression wnll interpolation function. wnll interpolation order speed computation keep nearest neighbors neighbor’s distance used normalize weight matrix. wnll regarded nonparametric approaches. wnll outperforms methods except svm. general better softmax regression demonstrates importance manifold structure data. results show potential using wnll instead softmax activation function dnns. epochs training vanilla i.e. standard initial learning rate halved every epochs cifar cifar datasets. also train epochs wnll i.e. wnll activated dnn. since stage already well trained smaller learning rate cifar datasets hyper-parameters chosen based cross validation. keep alternating three steps learning rate ﬁfth previous stages. batch size training vanilla experiments. svhn experiments hyperparameters reported batch size training wnll dnn. theorem number enough sample types instances even cifar classes images. optimizations carried simple solver default nesterov momentum acceleration. sufﬁcient training data generalization accuracy typically decays network goes deeper. phenomenon illustrated fig.. left right panels plot cases ﬁrst data training cifar involved training vanilla wnll dnns. believe increase generalization error data sufﬁcient parametrize deep networks. suitable regularization techniques deep networks better parametrized small amount training data. wnll activation involves information data’s geometric structures regularizers. wnll activation generalization error rate decays persistently network goes deeper. generalization accuracy vanilla wnll differ percent within testing regime. even though built connection deep resnets control problems also test performance surgeries algorithms base dnns e.g. networks. table list generalization error rates different dnns resnet pre-activated resnet families entire ﬁrst instances cifar training set. easy wnll activated dnns typically much accuracy improvement resnets pretrain linear activation starting previous iteration. ﬁrst step default initialized one. denote temporary model dnn. split training validation parts model. partition nbatch nbatch mini-batches denoted nbatch nbatch given integers. nbatch voting back-propagate loss using computational graph linear activation function update dnn. generally classiﬁcation tasks loss selected cross entropy exact predicted labels. input testing training features labels instances trained surgeries denoted dnns. output predicted labels test partition nbatch nbatch number mini-batches denoted {ti}nbatch nbatch nbatch given integers. nbatch around epochs accuracies vanilla dnns plateaued cannot improve more. however stage wnll activation jump generalization accuracy; stage even though initially accuracy reduction training continuing accuracy keeps climbing while. generalization accuracy increases ﬁnally wnll activation function. figure evolution generation accuracy training procedure. charts accuracy plots resnet training data plots epoch v.s. accuracy vanilla wnll activated dnn. panels correspond case training data preactresnet. tests done cifar dataset. figure taming degeneration problem vanilla wnll activated dnn. panels plot generation error training data used train vanilla wnll activated respectively. plot test three different networks preactresnet preactresnet preactresnet. easy vanilla network becomes deeper generation error decayed wnll activation resolves degeneracy. tests done cifar dataset. next present superiority deep network terms generalization accuracy compared base network. figure. plots generalization accuracy evolution training procedure. panels plot cases resnet wnll activated resnet ﬁrst cifar training data utilized. charts cases ﬁrst cifar training instances used train vanilla pre-activated resnet wnll activated version. afstreet view house number recognition task simply test performance full training data used. test performance resnets pre-activated resnets. relatively error rate reduction dnns. relatively improvement pre-activated resnets resnets consistent basic control problem ansatz. tables list error rate different vanilla networks wnll activated networks. cifar wnll activated dnns outperformed vanilla ones around absolute relative error rate reduction. reproduced results vanilla dnns datasets. results consistent original reports researchers’ reproductions interestingly task become harder improvement becomes signiﬁcant. builds conﬁdent trying harder tasks future. reducing sizes models important direction make applicable generalize purpose e.g. auto-drive mobile intelligence etc. successful attempt weights quantization. approach direction reducing size model achieve level accuracy compared vanilla networks model’s size tens times smaller. motivated connection deep resnets control problems propose novel structure inherited existing dnn. end-to-end greedy styled multi-staged training algorithm proposed train novel networks. order efﬁciently back propagate errors utilized computational graph linear function dynamically approximate manifold interpolation function. hand framework resolves issue lack training data hand provides great accuracy improvement compared base dnns. improvement consistent networks different depths. utilizing structure easy near state-of-the-art results small model great potential mobile device applications. nevertheless many directions improvement current manifold interpolation still computational bottlenecks according representability theorem data many classes. instance batch size need large imagenet dataset poses memory challenges. another important issue approximation gradient wnll activation function. linear function option optimal. believe better harmonic function approximation lift model’s performance. material based part upon work supported u.s. department energy ofﬁce science national science foundation national science foundation china grant numbers doe-sc dms- nsfc netzer yuval wang coates adam bissacco alessandro andrew reading digits natural images unsupervised features learning. nips workshop deep learning unsupervised feature learning paszke adam gross chintala soumith chanan gregory yang edward devito zachary zeming desmaison alban antiga luca lerer adam. automatic differentiation pytorch.", "year": 2018}