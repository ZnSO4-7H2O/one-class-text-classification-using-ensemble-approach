{"title": "Feature Control as Intrinsic Motivation for Hierarchical Reinforcement  Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "The problem of sparse rewards is one of the hardest challenges in contemporary reinforcement learning. Hierarchical reinforcement learning (HRL) tackles this problem by using a set of temporally-extended actions, or options, each of which has its own subgoal. These subgoals are normally handcrafted for specific tasks. Here, though, we introduce a generic class of subgoals with broad applicability in the visual domain. Underlying our approach (in common with work using \"auxiliary tasks\") is the hypothesis that the ability to control aspects of the environment is an inherently useful skill to have. We incorporate such subgoals in an end-to-end hierarchical reinforcement learning system and test two variants of our algorithm on a number of games from the Atari suite. We highlight the advantage of our approach in one of the hardest games -- Montezuma's revenge -- for which the ability to handle sparse rewards is key. Our agent learns several times faster than the current state-of-the-art HRL agent in this game, reaching a similar level of performance. UPDATE 22/11/17: We found that a standard A3C agent with a simple shaped reward, i.e. extrinsic reward + feature control intrinsic reward, has comparable performance to our agent in Montezuma Revenge. In light of the new experiments performed, the advantage of our HRL approach can be attributed more to its ability to learn useful features from intrinsic rewards rather than its ability to explore and reuse abstracted skills with hierarchical components. This has led us to a new conclusion about the result.", "text": "problem sparse rewards hardest challenges contemporary reinforcement learning. hierarchical reinforcement learning tackles problem using temporally-extended actions options subgoal. subgoals normally handcrafted speciﬁc tasks. here though introduce generic class subgoals broad applicability visual domain. underlying approach hypothesis ability control aspects environment inherently useful skill have. incorporate subgoals end-to-end hierarchical reinforcement learning system test variants algorithm number games atari suite. highlight advantage approach hardest games montezuma’s revenge ability handle sparse rewards key. agent learns several times faster current state-of-the-art agent game reaching similar level performance. update found standard agent simple shaped reward i.e. extrinsic reward feature control intrinsic reward comparable performance agent montezuma revenge. light experiments performed advantage approach attributed ability learn useful features intrinsic rewards rather ability explore reuse abstracted skills hierarchical components. conclusion result. reinforcement learning methods often struggle environments rewards sparsely encountered acquisition requires coordination temporally extended sequences actions. types environments archetypally exempliﬁed atari game montezuma’s revenge dearth feedback agent receives environment makes difﬁcult learn long sequences actions particularly timescale exploration strategy short. hierarchical reinforcement learning approach aims deal reward sparsity problem equipping agent temporally extended macro-actions also known options skills abstract sequences primitive actions. useful options established long sequences primitive actions expressed much shorter sequences options easier learn agent employ temporally extended exploration option space. however learning useful options difﬁcult task itself; possibility incorporate prior knowledge task construction limit generalisability algorithm tasks. paper constrain prior knowledge hypothesis ability control features environment inherently useful skill agent succeeding wide variety tasks. applying concept deep setting design agent intrinsically motivated control aspects environment options. architecture agent inspired feudal reinforcement learning hierarchical deep reinforcement learning framework whereby meta-controller provides embedded subgoals sub-controller interacts directly environment agent metacontroller learns maximise extrinsic reward environment tells sub-controller feature environment control; sub-controller receives intrinsic rewards successfully changing chosen feature well extrinsic rewards environment. show that guided form intrinsic motivation agent learns perform better tasks featuring sparse rewards. main contribution design discrete sets subgoals available meta-controller choose corresponding intrinsic reward. taking existing idea feature control incorporating subgoal design introduce hierarchical agent generically useful learnable options empirically evaluate atari domain. idea embodying agent form intrinsic motivation case desire able control aspects environment explored several works. klyubin introduced empowerment information theoretic measure degrees freedom agent environment concept empowerment recently gained interest context intrinsically motivated reinforcement learning lines work intrinsic motivation deﬁned form curiosity measured model-learning progress information gain jaderberg introduced idea off-policy training auxiliary control tasks pixel control feature control signiﬁcantly speed learning main task rationale learning auxiliary tasks gives agent features useful manipulating environment. drawing idea apply idea pixel feature control framework options constructed explicit motive altering given features patches pixels. agent equipped temporally-extended options used on-policy explore environment temporally-extended manner thereby helping address problem sparse reward. architecture also takes inspiration recent work kulkarni vezhnevets works outline hierarchical architectures comprise subgoal-selecting metacontroller sub-controller tries achieve subgoal. main feature sets model apart design subgoals. kulkarni pre-deﬁne discrete subgoals speciﬁc tasks hand vezhnevets construct subgoals large continuous embedded states. construct discrete sets subgoals discussed detail section ﬁxed designed generically applicable visual domains; automatically learned subgoals useful solving task hand. large number works subgoal discovery based ﬁnding bottleneck states. since ﬁnding bottleneck states requires global statistics environment ﬁnding difﬁcult hard scale. work line contemporary e.g. option-critic moved towards end-to-end training options subgoals automatically emerge optimisation system carefully designed architectures objective functions. consider standard reinforcement learning setting agent interacts environment observing state environment taking action every discrete time step figure depiction action-perception loop system. diagram architecture model used baseline. lstm components depicted circles encoding layers depicted trapezium bold horizontal line depicts concatenation incoming vectors. diagram proposed model. notable difference additional lstm component parameterises meta-controller’s policy value function. dotted line represents slower time scale meta-controller operates. environment provides extrinsic reward agent rext transitions next state st+. goal agent maximise accumulated extrinsic rewards ﬁnite horizon length episode. speciﬁcally consider hierarchical agent components meta-controller subcontroller. sub-controller responsible choosing agent’s actions directly interacts environment. meta-controller operates longer time scale time steps inﬂuences behaviour sub-controller subgoal argument inﬂuence imposed giving input sub-controller addition importantly meta-controller also gives intrinsic reward sub-controller rint successfully completing subgoal thus learning associate rint sub-controller’s behaviour biased complete subgoals. time meta-controller learns select sequences sub-controller trajectory maximises accumulated extrinsic reward. detail variants algorithm corresponding ways deliver subgoal pixel-control agent feature-control agent. agents architecture discussed section main difference calculation intrinsic reward crucial manipulation behaviour sub-controller. pixel control following jaderberg study basic form controlling ability visual domain ability control given subset pixels visual input divide pre-processed input image pixel patches size deﬁne intrinsic reward squared difference consecutive frames pixels patch normalized squared difference whole image. sub-controller thus encouraged maximise change values pixels given patch relative entire screen. written formally hhhk binary ﬁlter matrix entries equal apart pixel patch entries equal applying ﬁlter element-wise multiplication changes relevant part screen taken account. scaling factor controls magnitude intrinsic reward time step. choose gives reasonable value accumulated intrinsic reward episode start training. leave tuning parameter future work. believe tuning parameter important important parameter relative weight extrinsic intrinsic reward feature control jaderberg introduced notion feature control deﬁned ability control activations speciﬁc neurons. similarly bengio introduced notion feature selectivity measures much feature controlled independently features deﬁne intrinsic reward bengio al.’s feature selectivity measure second convolutional layer network. measure selectivity feature take difference mean activation selected feature consecutive time steps normalize feature maps. written summation feature maps. contrast pixel-control agent allowing meta-controller select convolutional feature endows agent ﬂexible abstract control environment. instruction meta-controller abstract since feature represent complex function sensory inputs ﬂexible feature maps shaped learning encode aspects environment useful control completion main task. shaped reward addition intrinsic reward also give extrinsic reward sub-controller enabling learn ﬁne-grained behaviour. adjust ratio intrinsic extrinsic reward parameter results shaped reward sub-controller strictly follows order meta-controller hand meta-controller little direct inﬂuence sub-controller. case subcontroller still receives subgoal argument input receive rewards attaining subgoal. baseline model variant asynchronous advantage actor-critic algorithm adapted openai’s implementation follow architecture speciﬁed wang model consists parts encoding module long-short term memory layer encoding module consists convolution layers fully connected layer. ﬁrst convolution ﬁlters stride length second layer ﬁlters stride length second convolution followed fully connected layer units. output fully connected layer concatenated previous action previous reward lstm layer. lstm cells whose output linearly projects policy value networks. hierarchical model extends baseline model follows additional lstm layer added parameterise meta-controller’s value policy function. input meta-controller’s lstm includes previous subgoal argument extrinsic rewards accumulated previous meta-step. sub-controller’s lstm also additional input consisting current subgoal argument. meta-controller operates every time steps. subgoal argument one-hot vector speciﬁes index subgoal selected meta-controller. pixel-control agent subgoals corresponding patches pixel plus no-op gives intrinsic reward. feature-control agent discrete subgoals corresponding feature second convolutional layer. value policy networks optimised using loss function asynchronous agents advantage estimated generalized advantage estimator using adam optimizer learning rate experiments. experiments backpropagation time trajectory length either time steps sub-controller baseline agent meta-steps meta-controller. finally gradients scaled l-norms exceed figure performance pixel-control agent feature-control agent using different values vertical axis shows average score episode evaluated every episodes. learning curve average four runs shaded areas showing difference maximum minimum scores four runs. evaluated model atari games openai environment toolkit comparing reinforcement learning algorithms wraps arcade learning environment number modiﬁcations. experiments make comparisons feudal network option-critic architectures evaluated ale. environment provides state pixels. pre-process state reshaping matrix retaining channels. also clip extrinsic reward range used setting games e.g. montezumarevenge-v montezuma’s revenge. evaluate effectiveness meta-controller agent different relative weights extrinsic intrinsic reward compared performance baseline agent. used bptt sub-controller. first found agent’s performance similar baseline. result demonstrates signiﬁcant gain decline performance using values attributed intrinsic reward. however important note directly comparable. openai’s adds stochasticity random frame-skips deterministic environment. standard evaluation protocol random number no-op actions start episode achieve stochasticity. figure performance feature-control agent montezuma’s revenge frostbite bptt bptt= signiﬁcant performance gain montezuma’s revenge bptt increased frostbite results drop performance level baseline. shown figure feature-control agent outperforms agents montezuma’s revenge frostbite competitive agents q*bert private eye. result suggests introducing certain proportion intrinsic reward sub-controller positive effect sparse reward environments without degrading performance dense reward environments agents perform poorly expected. since sub-controller follow limited number subgoals meta-controller behaviours also limited case. giving extrinsic reward sub-controller allow ﬁne-grained behaviours important maximising extrinsic reward. interestingly agents also perform worse baseline q*bert. result shows much inﬂuence meta-controller negative effect dense reward environments. interestingly best value consistent across four games. observe pixel-control agent learns quickly feature-control agent. however feature-control agent generally achieves better scores million frames training. likely fact feature-control agent needs time learn useful features inﬂuence meta-controller becomes meaningful. features learned subgoals obtained higher quality hard-coded ones pixel-control agent. initial experiments observed instability training curve feature-control agent comes form catastrophic drops performance. alleviate problem tried increasing bptt roll-out steps sub-controller. reasoned longer unrolled sequence bptt could contribute training stability following ways updates less frequent give agent stable features crucial component calculation intrinsic reward allows gradient backpropagated past potentially reduces bias update. figure montezuma’s revenge agent attains much higher score bptt bptt frostbite however observe opposite effect. could gradient already stable bptt increasing bptt length yield positive effect; contrary result lowering frequency updates result slower learning experiment evaluated feature-control agent pac-man asterix zaxxon montezuma’s revenge. show method applicable broad range games compare system state-of-the-art end-to-end systems namely option-critic architectures. results shown figure note following pac-man asterix zaxxon achieve better maximum scores option-critic network worse maximum scores network; montezuma’s revenge agent reaches approximately maximum score network learns much quickly reaching level performance fewer ﬁfth number observations. anticipate able improve agent’s performance broader parameter search. example discount parameter shown signiﬁcant impact performances different atari games compare state-of-the-art results montezuma’s revenge obtained unreal agent dqn-cts dqn-pixelcnn since competing methods advantageous features could easily integrated agent. inﬂuence intrinsic motivation provided meta-controller agent’s behaviour easily visualised pixel-control agent. figure sequence screenshots montezuma’s revenge shown sub-controller moving character patch selected meta-controller causing jump around patch order generate intrinsic reward. figure shows another sequence meta-controller selects patch ladder must climbed collect key. character moves patch meta-controller changes location patch sub-controller ignores instead proceeds collect results extrinsic reward. example highlights importance motivating sub-controller extrinsic well intrinsic reward allowing agent ﬂexible completely mercy meta-controller. interpreting intrinsic motivation feature-control agent much difﬁcult since involves understanding encoded selected convolutional feature map. attempt visualise this upsampled selected feature overlaid state input. feature-control agent learn strategies maximally change activations feature gain intrinsic reward. present scenarios feature-control agent figure indicate different types features evolve form useful options agent. figure shows agent collecting ﬁrst room montezuma’s revenge. scenario feature activated front agent path towards key. implicitly encourages agent move towards attempts maximally alter activations feature map. figure shows agent collecting sword another room. scenario feature activated agent completes apparent sub-task opposed ﬁrst scenario entire path completing sub-task highlighted. paper presented approach tackling reward sparsity problem form two-module deep hierarchical agent. montezuma’s revenge atari game particularly figure examples screenshot sequences illustrating intrinsic motivations pixelcontrol agent feature-control agent. white square represents pixel patch chosen meta-controller white cloud represents upsampled activations chosen feature map. sparse rewards agent learns several times faster current state-of-the-art agents reaching similar ﬁnal level performance. also show subgoal designs generically applicable across visual tasks evaluating agent several different games. agent almost always performs better baseline agent suggests ability control aspects environment generically useful subgoal. argue part performance gain comes ability perform temporally-abstracted exploration. visualising trajectories pixel-control agent observe successfully learns move towards patch selected meta-controller order maximise intrinsic reward; acquisition skill allows meta-controller motivate agent explore environment broad temporally extended manner. feature-control agent options learned shaping convolutional features features harder interpret pixel patches evidence visualisations activated completion intuitive subgoals collection sword montezuma’s revenge. important result experiments best performances achieved subcontroller motivated combination intrinsic extrinsic reward. leaking extrinsic reward sub-controller frees restriction subgoals need complete carefully designed lead brittle sub-optimal solutions long subgoals useful exploration agent equipped skills learn faster still maintaining ability ﬁne-tune behaviour maximise extrinsic reward. order give agent ﬂexibility would interesting future work incorporate termination condition options would allow instruction meta-controller variable length thus temporally precise. additionally would interesting quantify extent agents learned control environment perhaps using measure empowerment update later found agent trained shaped reward according equation perform well feature-control agent montezuma revenge. result line supports claim additional auxiliary rewards loss signals beneﬁcial dealing sparse reward environments even though reward possibly skew deﬁnition task. importantly raises question beneﬁt hierarchical elements proposed paper. appears decisions made meta-controller signiﬁcantly contribute success feature-control agent. would like thank marc deisenroth providing azure credits microsoft azure sponsorship teaching research. would also like thank kyriacos nikiforou hugh salimbeni arulkumaran fruitful discussions. n.d. supported dpst scholarship thai government. references abadi agarwal barham brevdo chen citro corrado davis dean devin ghemawat goodfellow harp irving isard jozefowicz kaiser kudlur levenberg mané monga moore murray olah schuster shlens steiner sutskever talwar tucker vanhoucke vasudevan viégas vinyals warden wattenberg wicke zheng. tensorflow large-scale machine learning heterogeneous systems software available tensorﬂow.org. bellemare srinivasan ostrovski schaul saxton munos. unifying count-based exploration intrinsic motivation. advances neural information processing systems pages dayan hinton. feudal reinforcement learning. proceedings international conference neural information processing systems pages morgan kaufmann publishers inc. houthooft chen duan schulman turck abbeel. vime variational information maximizing exploration. advances neural information processing systems pages jaderberg mnih czarnecki schaul leibo silver kavukcuoglu. reinforcement learning unsupervised auxiliary tasks. international conference learning representations konidaris barto. skill discovery continuous reinforcement learning domains using skill chaining. advances neural information processing systems pages kulkarni narasimhan saeedi tenenbaum. hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation. advances neural information processing systems pages mcgovern barto. automatic discovery subgoals reinforcement learning using diverse density. proceedings eighteenth international conference machine learning pages morgan kaufmann publishers inc. mohamed rezende. variational information maximisation intrinsically motivated reinforcement learning. advances neural information processing systems pages ¸sim¸sek barto. using relative novelty identify useful temporal abstractions reinforcement learning. proceedings twenty-ﬁrst international conference machine learning page kiros courville salakhudinov zemel bengio. show attend tell neural image caption generation visual attention. international conference machine learning pages", "year": 2017}