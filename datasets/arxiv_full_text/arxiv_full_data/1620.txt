{"title": "Wav2Letter: an End-to-End ConvNet-based Speech Recognition System", "tag": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "abstract": "This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform.", "text": "paper presents simple end-to-end model speech recognition combining convolutional network based acoustic model graph decoding. trained output letters transcribed speech without need force alignment phonemes. introduce automatic segmentation criterion training sequence annotation without alignment simpler. show competitive results word error rate librispeech corpus mfcc features promising results waveform. present end-to-end system speech recognition going speech signal power spectrum waveform) transcription. acoustic model trained using letters directly take need intermediate phonetic transcription. indeed classical pipeline build state systems speech recognition consists ﬁrst training hmm/gmm model force align units ﬁnal acoustic model operates approach takes roots hmm/gmm training improvements brought deep neural networks convolutional neural networks acoustic modeling extend training pipeline. current state librispeech uses approach additional step speaker adaptation recently proposed gmm-free training approach still requires generate force alignment. approach ties hmm/gmm pipeline train recurrent neural network phoneme transcription. competitive end-to-end approaches acoustic models toppled rnns layers trained sequence criterion however models computationally expensive thus take long time train. compared classical approaches need phonetic annotation propose train model end-to-end using graphemes directly. compared sequence criterion based approaches train directly speech signal graphemes propose simple architecture based convolutional networks acoustic model toppled graph transformer network trained simpler sequence criterion. word-error-rate clean speech slightly better slightly worse particular factoring train hours train available librispeech’s train set. finally models also trained waveform rest paper structured follows next section presents convolutional networks used acoustic modeling along automatic segmentation criterion. following section shows experimental results comparing different features criterion current best word error rates librispeech. speech recognition system standard convolutional neural network various different features trained alternative connectionist temporal classiﬁcation coupled simple beam search decoder. following sub-sections detail components. consider three types input features model mfccs power-spectrum wave. mfccs carefully designed speech-speciﬁc features often found classical hmm/gmm speech systems dimensionality compression power-spectrum features found recent deep learning acoustic modeling features wave somewhat explored recent work convnets advantage ﬂexible enough used either input feature types. acoustic models output letter scores convnet acoustic model acoustic models considered paper based standard convolutional neural networks convnets interleave convolution operations pointwise non-linearity operations. often convnets also embark pooling layers type layers allow network larger context without increasing number parameters locally aggregating previous convolution operation output. instead networks leverage striding convolutions. given t=...tx input sequence frames dimensional vectors convolution kernel width stride frame size output computes following lution pointwise non-linear layers added convolutional layers. experience surprisingly found using hyperbolic tangents piecewise linear counterpart hardtanh relu units lead similar results. slight variations architectures depending input features. mfcc-based networks need less striding standard mfcc ﬁlters applied large strides input sequence. power spectrum-based wave-based networks observed overall stride network important convolution strides placed. found thus preferrable strided convolutions near ﬁrst input layers network leads fastest architectures power spectrum features wave input sequences long ﬁrst convolutions thus expensive ones. figure neural network architecture wave. layers convolutions strides. last layers convolutions equivalent fully connected layers. power spectrum mfcc based networks ﬁrst layer. last layer convolutional network outputs score letter letter dictionary architecture wave shown figure inspired architectures power spectrum mfcc features include ﬁrst layer. full network seen non-linear convolution kernel width size stride equal given sample rate data label scores produced using window steps large labeled speech databases provide text transcription audio ﬁle. classiﬁcation framework would need segmentation letter transcription train properly model. unfortunately manually labeling segmentation letter would tedious. several solutions explored speech community alleviate issue hmm/gmm models iterative procedure estimation step best segmentation inferred according current model maximizing joint probability letter transcription input sequence. maximization step model optimized minimizing frame-level criterion based inferred segmentation. approach also often used boostrap training neural network-based acoustic models. alternatives explored context hybrid hmm/nn systems criterion maximizes mutual information acoustic sequence word sequences minimum bayse risk criterion recently standalone neural network architectures trained using criterions jointly infer segmentation transcription increase overall score right transcription popular certainly connectionist temporal classiﬁcation criterion core baidu’s deep speech architecture assumes network output probability scores normalized frame level. considers possible sequence letters lead given transcription. also allow special blank state optionally inserted letters. rational behind blank state twofolds modeling garbage frames might occur letter identifying separation identical consecutive letters transcription. figure shows example sequences accepted given transcription. practice graph unfolded shown figure available frames output acoustic model. denote gctc unfolded graph frames given transcription gctc path graph representing sequence letters transcription. time step node graph assigned corresponding log-probability letter output acoustic model. aims maximizing overall score paths gctc; purpose minimizes forward score logadd operation also often called log-sum-exp deﬁned logadd log). overall score efﬁciently computed forward algorithm. things perspective would replace logadd would maximize score best path according model belief. logadd seen smooth version paths similar scores attributed weight overall score paths much larger score much overall weight paths scores. practice using logadd works much better max. also worth noting maximizing diverge acoustic model assumed output normalized scores paper explore alternative three differences blank labels un-normalized scores nodes global normalization instead per-frame normalization figure criterion graph. graph represents acceptable sequences letters transcription cat. shows graph unfolded frames. transitions scores. time step nodes assigned conditional probability output neural network acoustic model. possible garbage frames letters. modeling letter repetitions easily replaced repetition character labels example caterpillar could written caterpilar label represent repetition previous letter. blank labels also simpliﬁes decoder. easily plug external language model would insert transition scores edges graph. could particularly useful future work wanted model representations high-level letters. respect avoiding normalized transitions important alleviate problem label bias work limited transition scalars learned together acoustic model. following name criterion auto segmentation criterion considering notations unfolded graph gasg frames given transcription well fully connected graph frames aims minimizing transition score model jump label label left-hand part promotes sequences letters leading right transcription right-hand part demotes sequences letters. parts efﬁciently computed forward algorithm. derivatives respect obtained applying chain rule forward recursion. wrote one-pass decoder performs simple beam-search beam threholding histogram pruning language model smearing kept decoder simple possible implement sort model adaptation decoding word graph rescoring. decoder relies kenlm language modeling part. also accepts un-normalized acoustic scores input. decoder attempts maximize following figure criterion graph. graph represents acceptable sequences letters transcription cat. shows graph unfolded frames. shows corresponding fully connected graph describe possible sequences letter; graph used normalization purposes. un-normalized transitions scores possible edges. time step nodes assigned conditional un-normalized score output neural network acoustic model. implemented everything using torch. criterion well decoder implemented consider benchmark librispeech large speech database freely available download librispeech comes train validation test sets. except speciﬁed used available data training validating models. original sampling rate. vocabulary contains graphemes standard english alphabet plus apostrophe silence special repetition graphemes encode duplication previous letter architecture hyper-parameters well decoder ones tuned using validation set. following either report letter-error-rates word-error-rates wers obtained using decoder standard -gram language model provided librispeech. mfcc features computed coefﬁcients sliding window stride. included ﬁrst second order derivatives. power spectrum features computed window stride components. features normalized input sequence. table reports comparison terms speed. criterion implemented leveraging instructions possible. batching done openmp parallel for. picked criterion implementation provided baidu. criteria lead ler. comparing speed report performance sequence sizes reported initially baidu also longer sequence sizes corresponds average table asg. baidu’s implementation. implemented reports performance ler. timings small sequences long sequences reported respectively. timings include forward backward passes. implementations threads. figure valid v.s. training size compares mfcc-based power spectrum-based architectures. experiments include data augmentation. provide baidu deep speech numbers librispeech comparison case. appears faster long sequences even though running only. baidu’s implementation seems aimed larger vocabularies also investigated impact training size dataset well effect simple data augmentation procedure shifts introduced input frames well stretching. purpose tuned size architectures avoid over-ﬁtting. figure shows augmentation helps small training size. however enough training data effect data augmentation vanishes type features appear perform similarly. figure reports respect available training data size. observe compare well deep speech trained much data finally report table best results system trained speech type features. overall stride architectures produces label every found could squeeze performance reﬁning precision output. efﬁciently achieved shifting input sequence feeding network several times. results table obtained single extra shift power spectrum features performing slightly worse mfccs. could expect however enough data would vanish. introduced simple end-to-end automatic speech recognition system combines standard convolutional neural network sequence criterion infer segmentation simple beam-search decoder. decoding results competitive librispeech corpus mfcc features promising power spectrum speech showed autosegcriterion faster accurate approach breaks free hmm/gmm pre-training force-alignment well computationally intensive rnn-based approaches references amodei anubhai battenberg case casper catanzaro chen chrzanowski coates diamos deep speech end-to-end speech recognition english mandarin. arxiv preprint arxiv. bahl brown souza mercer maximum mutual information estimation hidden markov model parameters speech recognition. acoustics speech signal processing ieee international conference ieee bottou bengio global training document processing systems using graph transformer networks. computer vision pattern recognition proceedings. ieee computer society conference ieee graves fernández gomez schmidhuber connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks. proceedings international conference machine learning hannun case casper catanzaro diamos elsen prenger satheesh sengupta coates deep speech scaling end-to-end speech recognition. arxiv preprint arxiv. hinton deng dahl mohamed a.-r. jaitly senior vanhoucke nguyen sainath deep neural networks acoustic modeling speech recognition shared views four research groups. signal processing magazine ieee lafferty mccallum pereira conditional random ﬁelds probabilistic models segmenting labeling sequence data. eighteenth international conference machine learning icml palaz collobert doss estimating phoneme class conditional probabilities speech signal using convolutional neural networks. arxiv preprint arxiv. palaz magimai-doss collobert joint phoneme segmentation inference classiﬁcation using crfs. signal information processing ieee global conference ieee panayotov chen povey khudanpur librispeech corpus based public domain audio books. acoustics speech signal processing ieee international conference ieee peddinti chen manohar povey khudanpur aspire system robust lvcsr tdnns i-vector adaptation rnn-lms. proceedings ieee automatic speech recognition understanding workshop", "year": 2016}