{"title": "Feasibility of random basis function approximators for modeling and  control", "tag": ["cs.NE", "cs.AI"], "abstract": "We discuss the role of random basis function approximators in modeling and control. We analyze the published work on random basis function approximators and demonstrate that their favorable error rate of convergence O(1/n) is guaranteed only with very substantial computational resources. We also discuss implications of our analysis for applications of neural networks in modeling and control.", "text": "though parameters trained principle typically linear weights network trained. locations widths gaussians usually uniform nonuniform grid covering operating domain system. popularity approximators approximation capabilities homogenous structure also efﬁciency approximators high dimensions. particular allowed vary rate parameters convergence approximation error target function function shown independent input dimension furthermore achievable rate convergence l-norm shown order despite advantageous features approximators viz. favorable independence convergence rates input dimension function approximated issue achieve convergence rate order practice. even though offer constructive procedure optimal selection basis functions step procedures involves nonlinear optimization routine searching best possible values also shown linear parameters adjusted approximation error cannot made smaller /n/d uniformly functions satisfying smoothness constraints. necessity adjust nonlinear parameters restricts practical application models problems optimization feasible. adaptive control nonlinearly parameterized models remains challenging issue; e.g. though published quite paper igelnik recently received numerous citations variety intelligent control publications; e.g. paper advocates random basis networks. nonlinear parameters randomly initialization rather training. trainable parameters enter network equation linearly paper provides mathematical justiﬁcation linear-in-parameters function approximators modeling control crucially simplifying analysis properties closed-loop control system featuring approximators. analysis simpliﬁcation attractive entails number issues important consider whenever abstract— discuss role random basis function approximators modeling control. analyze published work random basis function approximators demonstrate favorable error rate convergence guaranteed substantial computational resources. also discuss implications analysis applications neural networks modeling control. efﬁcient modeling control complex systems presence uncertainties important modern engineering. especially true domain intelligent systems designed operate uncertain environments. uncertainties systems usually quantitative relations measured signals physical models relations always available quite common mathematical substitutes e.g. superpositions functions capable approximating a-priori unknown required precision. thus successful modeling control domain intelligent systems critically dependent availability adequate efﬁcient function approximators take care various uncertainties system. domain modeling control intelligent systems multilayer perceptrons radial basis functions networks popular function approximators uses basis form sigmoids global support. one-hidden layer output determined ivan tyukin department mathematics university leicester department automation control processes petersburg state university electrical engineering russia perceptual dynamics riken japan i.tyukinleicester.ac.uk planning apply random basis function approximators practice. show rate convergence order approximators achievable large probabilistic nature. latter feature require introduction supervisory mechanism control system re-initialize network required accuracy met. paper organized follows. section analyze reasoning compare results show that although results seem inconsistent linear-in-parameter approximator rate convergence order independent derived different asymptotics large different convergence criteria statistical implications analysis illustrated section simple example followed discussion section section concludes paper. section review compare results function approximation neural networks. ﬁrst result so-called greedy approximation upon famous barron’s construction based framework function approximated sequence linear combinations basis functions. basis function satisfy certain optimality condition result overall rate convergence optimized well. second result random basis function approximator also known random vector functional-link network basis functions randomly chosen linear parameters optimized. results enjoy convergence rates depend input dimension target functions. however differences important practical results. first show number practically required approximation elements guarantees given approximation quality differ substantially. second quality criteria also different framework greedy approximation merely l-norm deterministic functional whereas rvfl framework criterion statistical. detailed description. function induces parameterized basis. indeed take integral quadratures sufﬁciently large values would express following sums parameterized approximation error statistical whereas approximation error deterministic. means fnωω guaranteed close every randomly chosen length however conclude sufﬁciently small probability fnωω close approaches monte-carlo based scheme converge needs ensure bounded. this however conﬂicts requirement hence class functions scheme applies restricted. order mitigate restriction proposed consider functions compact support class functions dimensionindependent rate convergence guaranteed. order illustrate main difference greedy rvfl approximators consider following example simple function approximated methods greedy approximation approximation based summation taken points weighting coefﬁcients. variables play different roles approximations schemes. value ensure approximation error decreasing every iteration stands scaling factor random sampling. advantage monte-carlo integration hence approximation techniques based upon method order convergence large known bounded variance estimate bounded pointwise above shown fig. observed ﬁgure even though values form monotonically decreasing sequence least behavior across trials consistent least networks smaller elements indicated signiﬁcant spread among curves. fig. practical speed convergence function approximators greedy algorithm monte-carlo based random choice basis functions target function shown panel. small searching solution utilize speciﬁc optimization routine. sampled space parameters randomly picked ﬁrst values satisfy integral evaluated quadratures uniform grid points trials shown fig. trial consisted iterations thus leading networks elements step. observe values monotonically decrease behavior approximation procedure consistent across trials. second implemented approximator based monte-carlo integration. step approximation procedure pick randomly element element selected current pool basis functions fig. plots convergence rates function approximators greedy algorithm monte-carlo random choice basis functions middle panel corresponds case basis functions leading ill-conditioning discarded. bottom panel shows performance trained method effective counteracting ill-conditioning adjusting linear weights only. curve shows upper bound calculated accordance duplicated average performance greedy algorithm middle bottom panels convenience comparison. farther system basis functions orthogonal time needed adaptive system converge desired domain; e.g. fact example section observed values hidden layer weights large error function number elements network; blue boxes contain data points trials; whiskers delimit areas containing data crosses show remaining part data. plots random basis function approximators rvfl networks mostly match performance greedy approximators networks reasonable size. perhaps employing integration methods variance minimization could improve performance. this however would amount using prior knowledge target function making difﬁcult apply rvfl networks problems function uncertain. demonstrate performance trained approximate target function. trained gradient based method described ﬁrst full network training carried several network sizes input samples randomly drawn values network sizes suggests training performance much smaller networks examined. networks trained resulting respectively averaged trials network size. next train linear weights ﬁxing nonlinear weights random values. results averaged trials shown fig. bottom panel remarkably results random basis network worse full network training. results indicate greedy monte-carlo approximation results shown fig. quite conservative. furthermore best i.e. greedy approximation’s dramatically improved practical gradient based training. analyzed theoretically illustrated simple example happen basis function approximation chosen random. wish discuss recent result presented regarding random basis function approximators. choose work representative recent trend neural network control literature exempliﬁed trend purpose several neural networks implementing random basis account unknown bounded modeling nonlinearity. tracking errors bounded asymptotically main theorem proof imply performance improvement. instead proof attempts relate design parameters magnitudes disturbances weights neural networks. though disturbance magnitude indeed known priori assume sufﬁciently small bounds values weights weights need large order compensate residual modeling errors randomly assigned basis functions. furthermore larger weights however large bounds weights force control system designer decrease design parameters which turn results increase region uniform ultimate boundedness ironically region shrink zero even ideal case zero disturbances. depending uncontrollable quantities weights makes impossible provide practically valuable guarantees closed-loop system performance. work demonstrate that despite increasing popularity random basis function networks control literature especially domain intelligent/adaptive control needs special attention practical aspects affect performance systems applications. first analyzed section showed example although rate convergence random basis function approximator qualitatively similar greedy approximator rate random basis function approximator achievable number elements network sufﬁciently large. second approximators motivated monte-carlo integration method offer statistical measure approximation quality. words small approximation errors guaranteed probability. means that practical adaptive control rvfl networks model compensate system uncertainties employment re-initialization supervisory mechanism monitoring quality rvfl network necessary. unlike network training methods adjust linear nonlinear weights network mechanism made robust numerical problems often occurs monte-carlo method. conclusion random basis function approximators also consistent following intuition. approximating elements chosen random subsequently trained usually placed accordance density input data. though computationally easier nonlinear parameters training linear parameters becomes ineffective reducing errors inherited nonlinear part approximator. thus order improve effectiveness random basis function approximators could combine unsupervised placement network nodes according input data density subsequent supervised reinforcement learning values linear parameters approximator. however combination methods not-trivial adaptive control modeling often able allocate approximation resources adaptively full network training seems natural handle adaptation. authors grateful prof. a.n. gorban useful comments numerous technical discussions preparation work. ﬁrst author’s research supported royal society international joint project grant partially supported rfbr grant ---a. kuljaca tesnjak koroman. performance analysis adaptive neural network frequency controller thermal power systems. proceedings wseas international conference automatic control modeling simulation istanbul turkey sarangapani venayagamoorthy wunsch crow cartes. decentralized neural network-based excitation control large-scale power systems. international journal control automation systems vol. october jones. simple lemma greedy approximation hilbert space convergence rates projection pursuit regression neural network training. annals statistics ai-poh a.m. annaswamy f.p. skantze. adaptation presence general nonlinear parameterization error model approach. ieee trans. automatic control park sandberg. approximation radial basis function d.v. prokhorov v.a. terekhov i.yu. tyukin. applicability conditions algorithms adaptive control nonconvex problems. automation remote control i.yu. tyukin d.v. prokhorov leeuwen. adaptation parameter estimation systems unstable target dynamics nonlinear parametrization. ieee trans. automatic control feldkamp prokhorov eagen yuan. enhanced multi-stream kalman ﬁlter training recurrent networks. suykens vandewalle nonlinear modeling advanced black-box techniques kluwer academic publishers", "year": 2009}