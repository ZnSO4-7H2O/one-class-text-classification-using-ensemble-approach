{"title": "PAC-Bayes Analysis of Multi-view Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "This paper presents eight PAC-Bayes bounds to analyze the generalization performance of multi-view classifiers. These bounds adopt data dependent Gaussian priors which emphasize classifiers with high view agreements. The center of the prior for the first two bounds is the origin, while the center of the prior for the third and fourth bounds is given by a data dependent vector. An important technique to obtain these bounds is two derived logarithmic determinant inequalities whose difference lies in whether the dimensionality of data is involved. The centers of the fifth and sixth bounds are calculated on a separate subset of the training set. The last two bounds use unlabeled data to represent view agreements and are thus applicable to semi-supervised multi-view learning. We evaluate all the presented multi-view PAC-Bayes bounds on benchmark data and compare them with previous single-view PAC-Bayes bounds. The usefulness and performance of the multi-view bounds are discussed.", "text": "shiliang shanghai laboratory multidimensional information processing department computer science technology east china normal university dongchuan road shanghai china liang shanghai laboratory multidimensional information processing department computer science technology east china normal university dongchuan road shanghai china paper presents eight pac-bayes bounds analyze generalization performance multi-view classiﬁers. bounds adopt data dependent gaussian priors emphasize classiﬁers high view agreements. center prior ﬁrst bounds origin center prior third fourth bounds given data dependent vector. important technique obtain bounds derived logarithmic determinant inequalities whose diﬀerence lies whether dimensionality data involved. centers ﬁfth sixth bounds calculated separate subset training set. last bounds unlabeled data represent view agreements thus applicable semi-supervised multi-view learning. evaluate presented multi-view pac-bayes bounds benchmark data compare previous single-view pac-bayes bounds. usefulness performance multi-view bounds discussed. multi-view learning promising research direction prevalent applicability instance multimedia content understanding multimedia segments described video audio signals video audio signals regarded views. learning data relies collecting data contain suﬃcient signal encoding prior knowledge increasingly sophisticated regularization schemes enable signal extracted. certain co-regularization schemes multi-view learning performs well various learning tasks. statistical learning theory provides general framework analyze generalization performance machine learning algorithms. theoretical outcomes used motivate algorithm design select models give insights eﬀects behaviors interesting quantities. example well-known large margin principle support vector machines well supported various bounds diﬀerent early bounds often rely complexity measures considered function classes recent pac-bayes bounds give tightest predictions generalization performance prior posterior distributions learners involved learning setting beyond common supervised learning pac-bayes analysis also applied tasks e.g. density estimation reinforcement learning although ﬁeld multi-view learning enjoyed great success algorithms applications provided theoretical results pac-bayes analysis multi-view learning still absent. paper attempt developments theory practice proposing pac-bayes bounds multi-view learning. earlier attempt analyze generalization two-view learning made using rademacher complexity bound relied estimating empirical rademacher complexity class pairs functions views matched expectation data generating distribution. hence approach also implicitly relied data generating distribution deﬁne function class current paper makes deﬁnition prior terms data generating distribution explicit pac-bayes framework provides several bounds. however main advantage deﬁnes framework makes explicit deﬁnition prior terms data generating distribution setting template related approaches encoding complex prior knowledge relies data generating distribution. kakade foster characterized expected regret semi-supervised multiview regression algorithm. results given sridharan kakade take information theoretic approach involves number assumptions diﬃcult check practice. assumptions theoretical results including pac-style analysis bound expected losses given involve bayes optimal predictor cannot provide computable classiﬁcation error bounds since data generating distribution usually unknown. results therefore represent related distinct approach. adopt pac-bayes analysis encode assumptions priors deﬁned terms data generating distribution. priors studied catoni name localized priors recently lever data distribution dependent priors. papers considered schemes placing prior classiﬁers deﬁned true generalization errors. contrast prior consider mainly used encode assumption relationship views data generating distribution. data distribution dependent priors cannot subjected traditional bayesian analysis since explicit form pac-bayes theorem bounds true error distribution classiﬁers terms term sample complexity divergence posterior prior distributions classiﬁers. technical innovations paper enable bounding divergence term terms empirical quantities despite involving priors cannot computed. approach adopted parrado-hern´andez simple priors gaussian centered current paper treats signiﬁcantly sophisticated case priors encode expectation good weight vectors found give similar outputs views. speciﬁcally ﬁrst provide four pac-bayes bounds using priors reﬂect well views agree average examples. ﬁrst bounds gaussian prior centered origin third fourth ones adopt diﬀerent prior whose center origin. however formulations priors involve mathematical expectations respect unknown data distributions. manage bound expectation related terms empirical estimations ﬁnite sample data. then provide pac-bayes bounds using part training data determine priors pac-bayes bounds semi-supervised multi-view learning unlabeled data involved deﬁnition priors. natural feature split exist multi-view learning could still obtain performance improvements manufactured splits provided views contains enough information learning task itself knowledge views have. therefore important people split features views satisfying assumptions. however data split still open question beyond scope paper. rest paper organized follows. brieﬂy reviewing pac-bayes bound svms section give derive four multi-view pac-bayes bounds involving empirical quantities section section give bounds whose centers calculated separate subset training data section that present semi-supervised multi-view pac-bayes bounds section optimization formulations related single-view multi-view svms well semi-supervised multi-view svms given section evaluating usefulness performance bounds section give concluding remarks section sample including examples indicator function. distribution deﬁne average true error ec∼qed average empirical error ˆeqs ec∼qˆes. following lemma provides pac-bayes bound current context binary classiﬁcation. theorem data distribution prior classiﬁer suppose training examples learn classiﬁer represented sign) projection original feature certain feature space induced kernel function. deﬁne prior posterior classiﬁer propose data dependent prior pac-bayes analysis multi-view learning. particular take distribution concatenation weight vectors individual product weight manner associated well weights agree averagely examples. prior concatenated weight vector views concatenated concatenation maps kernel-induced feature spaces. note indicate features example views respectively. simplicity original features derive results though kernel maps implicitly employed well. dimensionality independent bounds work even dimension kernelized feature space goes inﬁnity. problem expression contains expectations input distribution unable compute. deﬁned prior distribution terms input distribution function. priors referred localized catoni work considered speciﬁc examples priors satisfy certain optimality conditions deﬁnition consider encoding natural prior assumptions link input distribution classiﬁcation function namely simple representation views. example luckiness generalization estimated making assumptions proven true lead tighter bounds example case large margin classiﬁer. develop methods estimate relevant quantities empirical data additional empirical estimations involved ﬁnal bounds besides usual empirical error. note that although formulation involves outer product feature vectors actually represented inner product obvious following determinant equality attempt improve bounds using separate training data determine priors inspired ambroladze parrado-hern´andez consider spherical gaussian whose center calculated subset training theorem consider classiﬁer prior given classiﬁer posterior given given data distribution positive positive probability least following multi-view pac-bayes bound holds although bounds look similar essentially diﬀerent priors determined diﬀerently. experimental results also perform diﬀerently applied experiments. theorem consider classiﬁer prior given deﬁned classiﬁer posterior given unlabeled {˜xj}m+u j=m+. data distribution probability least following inequality holds theorem consider classiﬁer prior given deﬁned classiﬁer posterior given unlabeled {˜xj}m+u j=m+. data distribution positive positive probability least following provide optimization formulations single-view multi-view svms well semi-supervised multi-view svms adopted train classiﬁers calculate pac-bayes bounds. note augmented vector representation used appending scalar feature representations order formulate classiﬁer simple form without explicit bias term. scalar controls balance margin empirical loss. problem diﬀerentiable convex problem aﬃne constraints. constraint qualiﬁcation satisﬁed reﬁned slater’s condition. denote classiﬁer weights views assumed unit vectors moment. inspired semi-supervised multi-view svms objective function multi-view svms given bounds evaluated synthetic three real-world multi-view data sets learning task binary classiﬁcation. ﬁrst introduce used data experimental settings. report test errors involved variants algorithms evaluate usefulness relative performance pac-bayes bounds. synthetic data include examples half belong positive class. dimensionality views ﬁrst generate random direction vectors view view sample points make inner products direction feature vector half points positive inner products half points negative. point dutch utility maps. consists examples ﬁrst view fourier coeﬃcients second view karhunen-lo`eve coeﬃcients image. binary classiﬁcation digits used experiments. data used classifying images non-ads data consists examples ads. binary attributes used classiﬁcation whose values attributes divided views view describes image view contains features information views features respectively. course data consists two-view pages collected computer science department sites four universities cornell university university washington university wisconsin university texas. course pages noncourse pages. views words occurring page words appearing links pointing page document vectors normalized -idf features principal component analysis used perform dimensionality reduction. dimensions views respectively. experiments include algorithm test error evaluation pac-bayes bound evaluation single-view learning multi-view learning supervised learning semi-supervised learning. single-view learning svms trained separately views third view providing three supervised classiﬁers called svm- svm- svm- respectively. evaluating performance third view interesting compare single-view multi-view learning methods since single-view learning third view exploit data usual multi-view learning algorithms. mvsvms smvsvms supervised multi-view learning semi-supervised multi-view learning algorithms respectively. linear kernel used algorithms. labeled training respectively rest forms test set. supervised algorithms unlabeled training data. multi-view pac-bayes bound labeled training calculate prior evaluate bounds remaining training set. setting involves random partitions subsets. reported performance average test error standard deviation random partitions. bayes bounds evaluated conﬁdence normalize posterior calculate bounds. multi-view pac-bayes bounds ﬁxed equal clear augmented feature representation data normalization preprocessing prediction performances svms mvsvms smvsvms four experimental settings reported table table table table respectively. data best performance indicated boldface numbers. results mvsvms smvsvms best overall performance sometimes single-view svms best performances. smvsvms often perform better mvsvms since additional unlabeled examples used especially labeled training data small. moreover expected labeled training data prediction performance algorithms usually increase. table table table table show values various pac-bayes bounds diﬀerent settings data best bound indicated bold best multi-view bound indicated underline. bound results best single-view bound usually tighter best multi-view bound expect synthetic data set. possible explanation that synthetic data ideal accordance assumptions multi-view learning encoded prior real world data sets not. also indicates much space possibility developments multi-view pac-bayes analysis. addition labeled training data corresponding bound usually become tighter. last least among eight presented multi-view pacbayes bounds real world data sets tightest often ﬁrst semi-supervised multi-view bound exploits unlabeled data calculate function needs relaxation. results also show second multi-view pac-bayes bound sometimes good. paper lays foundation theoretical practical framework deﬁning priors encode non-trivial interactions data distributions classiﬁers translating sophisticated regularization schemes associated generalization bounds. speciﬁcally presented eight multi-view pac-bayes bounds integrate view agreement measure modulate prior distributions classiﬁers. extensions pac-bayes analysis multi-view learning scenario proposed theoretical results promising developments theory practice multi-view learning also possible serve underpinnings explain eﬀectiveness multi-view learning. validated theoretical superiority multiview learning ideal case synthetic data though evident real world data well meet assumptions priors multi-view learning. usefulness proposed bounds shown. although often current bounds tightest indeed open possibility applying pac-bayes analysis multi-view learning. think bounds could tightened future adopting techniques. also possible study algorithms whose co-regularization term pushes towards minimization multi-view pac-bayes bounds. addition work paper motivate pac-bayes analysis learning tasks multi-task learning domain adaptation since tasks closely related current multi-view learning. work supported national natural science foundation china project scientiﬁc research foundation returned overseas chinese scholars state education ministry shanghai knowledge service platform project obtain lagrangian dual function minimized respect primal variables eliminate variables compute corresponding partial derivatives obtaining following conditions optimize ﬁrst derive lagrange dual function following line optimization derivations mvsvms. although derivations similar mvsvms completeness include them. obtain lagrangian dual function minimized respect primal variables eliminate variables setting corresponding partial derivatives results following conditions", "year": 2014}