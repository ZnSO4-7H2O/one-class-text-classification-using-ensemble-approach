{"title": "Empirical Study on Deep Learning Models for Question Answering", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "In this paper we explore deep learning models with memory component or attention mechanism for question answering task. We combine and compare three models, Neural Machine Translation, Neural Turing Machine, and Memory Networks for a simulated QA data set. This paper is the first one that uses Neural Machine Translation and Neural Turing Machines for solving QA tasks. Our results suggest that the combination of attention and memory have potential to solve certain QA problem.", "text": "paper explore deep learning models memory component attention mechanism question answering task. combine compare three models neural machine translation neural turing machine memory networks simulated data paper ﬁrst uses neural machine translation neural turing machines solving tasks. results suggest combination attention memory potential solve certain problem. question answering natural language processing task requires deep understanding semantic abstraction reasoning facts relevant question many different approaches constructing pipeline component separately trained assembled building large knowledge bases reasoning facts therein machine reading approach comprehend question documents answers contained. recently various deep learning models proposed different learning problems. models usually differentiable gradient descent. require neither hand craft features separately tuned components. thus think important study models addressing problem. implicitly explicitly solving problem divided steps. ﬁrst step locates information relevant question e.g. sentences text document facts knowledge graph. call search step. second step call generation step extracts generates answer relevant pieces information detected search step. paper focuses reading comprehension type search generation sometimes coupled. focus neural machine translation memory network neural turing machine models representative state-of-the-art model architectures categories fall conducted empirical studies work better understand strength places improve model solving experiment settings follow -step framework. brieﬂy describe memnn next section. memnn memnns applied shown promising results different input transformation model changes. strength mainly lies reasoning inference components combined long-term memory component learning jointly. general memnn four modules input converts incoming input internal feature representation output produces output generalize updates memories given input response converts output response format desired. memory network described memorizes fact memory slot uses supporting facts given question labeled training data learn search facts. sainbayar described another version memnn trained end-to-end without knowing supporting facts. using machine translation technique generating answer given question regarded generating target text given source text words answer translation question. several previous works used translation models determine answers brings approaches machine translation example recurrent neural network models proposed following previous success applying think important study could help best knowledge done study yet. traditional translation systems usually phrase-based small sub-components tuned separately improves phrase-based systems using neural network encode sentence source language decode sentence target language end-to-end system. however main constraint encoder-decoder approach model needs compress information source sentence ﬁxed-length vector difﬁcult long sentences passages reading comprehension style order address issue bahdanau introduced extension encoder-decoder model uses bi-directional learns align translate jointly. model shown figure input includes passage question delimited marker. ﬁgure rnns read word word input different directions. time model generates answer word searches multiple positions passage relevant information concentrated. model predicts answer word based context vectors associated positions previous generated answer words. formally i-th answer word conditioned words answer word passage conditional probability modeled nonlinear function depends previous answer word hidden state time context context vector weighted sequence annotations. annotation information complete passage focus parts around i-th word. resembles turing machines could learn arbitrary procedure theory. believe problem solved programs paper would examine well performs reading comprehension tasks. ntms essentially rnns turn turing-complete capable encoding computer program theory always practical. end-to-end trainable gradient descent fact every component differentiable enabled converting hard read write ‘blurry’ operations interact greater lesser degree. read write heads well memory component recurrently updated time matter controller recurrent not. paper ﬁrst examine neural turing machines problems. implementation internally uses single-layer lstm network controller. inputs word distributed representations word embedding directed lstm controller within output generated softmax layer output corresponds answer. regard problem multi-class classiﬁcation problem task supporting fact supporting facts supporting facts arg. relations three arg. relations yes/no questions counting lists/sets simple negation indef. knowledge basic coreference conjunction comp. coreference time reasoning basic deduction basic induction pos. reasoning size reasoning path finding agents motivations mean performance ai-complete data synthetic dataset designed tasks. task small large training sets questions respectively test questions. supervision training given true answers questions supporting facts answering given question. tasks clean human could achieve accuracy generated simulation. data every statement passage number question associated true answer supporting facts. memnn shown promising results ai-complete data. ﬁrst want learn different memnn components contribute choose memnn adaptive memory baseline divide virtual steps -step framework. believe searching supporting facts important memnn memnn response module room improve attention mechanism focus words retrieved facts. train memnn-s using complete passage input supporting facts prediction train memnn-r using true supporting facts input ﬁnal answer prediction. column table conﬁrm hypothesis accuracy searching supporting facts memnn-s much better accuracy predicting ﬁnal answer words memnn-r. following ﬁndings ﬁrst experiment want could better memnn-r. using setting testing memnn-r show almost perfect results analyzed think main reason nmt’s attention machanism ntm’s memory component help. wonder supporting facts input perfect example search step mark facts supporting facts high probability. therefore experiment input complete passage including non-supporting facts markers annotate begin supporting facts passage. including nonsupporting facts input brings noise requires model memorize relevant information ignore non-relevant ones. results group perform good little expected drop using supporting facts only. shown better ntm’s explicit memory component content addressing mechanism could directly target relevant information stored. although showed good capability solving generation step experiments above. supporting facts still need identiﬁed models could applied. analyzed above memnn-s good searching supporting facts. thus memnn-s generate facts models apply based fact-searching results. combination improves average performance baseline proves novel major advantage applying models solve learned/tuned end-toend. therefore combination models separate training less advantageous single model elements. analyzed nmt’s architecture essentially attention mechanism bidirectional memory functionality. thus wonder single model would perform compared architectural combination shown above. group experiments model end-to-end without medium step ﬁnding supporting facts compare previous architectural combination. said supporting facts marked training testing reduced information model learn. requires attention supporting facts need learned implicit byproduct attention mechanism. small large training sets. experiment results show without tuning specialization problem. result small training within reasonable previous architectural combination considering model supporting facts all. furthermore training data sufﬁcient accuracy even comparable state-of-the-art accuracy specialized features added tuned speciﬁcally data. think potential address certain problem memory supported attention memory. studied several state-of-the-art models proposed different learning tasks solving problem. experiment results suggest good agent need remember forget facts necessary external memory choice. also convinced generate answer appropriate attention mechanism well. therefore believe model combining memory attention mechanism great potential handling problem.", "year": 2015}