{"title": "Distribution of the search of evolutionary product unit neural networks  for classification", "tag": ["cs.NE", "cs.AI", "cs.CV"], "abstract": "This paper deals with the distributed processing in the search for an optimum classification model using evolutionary product unit neural networks. For this distributed search we used a cluster of computers. Our objective is to obtain a more efficient design than those net architectures which do not use a distributed process and which thus result in simpler designs. In order to get the best classification models we use evolutionary algorithms to train and design neural networks, which require a very time consuming computation. The reasons behind the need for this distribution are various. It is complicated to train this type of nets because of the difficulty entailed in determining their architecture due to the complex error surface. On the other hand, the use of evolutionary algorithms involves running a great number of tests with different seeds and parameters, thus resulting in a high computational cost", "text": "abstract paper deals distributed processing search optimum classification model using evolutionary product unit neural networks. distributed search used cluster computers. objective obtain efficient design architectures distributed process thus result simpler designs. order best classification models evolutionary algorithms train design neural networks require time consuming computation. reasons behind need distribution various. complicated train type nets difficulty entailed determining architecture complex error surface. hand evolutionary algorithms involves running great number tests different seeds parameters thus resulting high computational cost. research distribution processing involved search best product-unit neural network models using evolutionary algorithms eas. cluster computers used carry distribution processing. many different types neural network architectures used popular single-hidden-layer feedforward network. amongst numerous approaches neural networks classification problems focus attention evolutionary artificial neural networks eanns research area past decade providing improved platform optimizing network performance architecture simultaneously. proposed evolutionary computation good candidate searching space architectures fitness function associated space complex noisy non-differentiable multi-modal deceptive. since then many evolutionary programming methods developed evolve artificial neural networks instance objective paper design neural network architecture suitable classification; involves achieving proper balance explorative/exploitative activities performed learning design search good eann models usually based trial error strategy follows blind search using small group possible configurations. implies high computational cost order solve medium-sized problem well search process makes task practically overwhelming even well-equipped computer. case distribute certain network parameters throughout cluster carry several experiments time previously would needed one. ifferent reasons make distribution necessary training type nets complicated difficulty entailed determining architecture models complexity error surface; demands high number runs using different seeds parameters giving rise high computational cost. fundamental goal work twofold improve efficiency distribution different values main parameters different nodes cluster also improve speedup computation different runs searching models. rest paper organized follows. section describe proposed methodology. section continues distributed computation design neural networks analysis results obtained. finally last section present concluding remarks. proposed methodology consists tool learning architecture weights punn class multiplicative neural networks comprises types sigma-pi networks product unit networks. advantages punns increased information capacity ability form higher-order combinations inputs besides that possible obtain upper bounds dimension product-unit neural networks similar obtained sigmoidal neural networks finally straightforward consequence stone-weierstrass theorem prove product-unit neural networks universal approximators despite advantages product-unit based networks major drawback. networks based product units local minima probability becoming trapped main reason difficulty small variations exponents cause large changes total error surface. several efforts made carry learning methods pus. developed genetic algorithm evolving weights network based predefined architecture. major problem kind algorithm obtain optimal architecture hand disadvantage using training punn processing time could great respect dimension characteristics space number classes considered concrete problem. system created allow configuration parameter distribution throughout different nodes cluster computers. design structure learn weights punns. search begins random initial population iteration population updated using population-update algorithm. population subjected operations replication mutation. crossover used potential disadvantages evolving artificial networks. using algorithm classification problems consider construction aptitude function changes involved mutation operators. probability pattern belongs class taking account function cross-entropy error used evaluate individuals network reflected following expression ince want minimise chosen error function fitness function form parametric mutation consists simulated annealing algorithm. severity mutation individual dictated temperature represents one-dimensional normally distributed random variable mean standard deviation pointed modification exponents different modification coefficients structural mutation implies modification function structure allows exploration different regions search space helping keep diversity population. four different structural mutations similar ones gnarl model node addition node deletion connection addition connection deletion. mutations made sequentially given order probability generation network. probability select mutation mutations chosen random applied network. details case since classification problem evolution process must short. evolution mechanism must selected parameters converges toward optimum values quickly. success rule rechenberg first case parameters distributed throughout nodes cluster. number neurons hidden layer maximum number generations optionally also value output-variance consider base configuration modified processing nodes. therefore specific mission always tune parameters. changes defined relative depend base configuration. modifications made nodes experiments configuration. nodes case distributing parameters take different values case parameters different values values. distribution processing used following datasets balance cancer pima repository validation technique parameters used considered cross validation experimental design called hold-out consists splitting data sets train test. case size train test number patterns problem. results experimentation proposal obtained following common parameters features configuration. results section present results obtained validation groups described configurations. called base configuration first group. amongst configurations topology parameters changed amongst others topology changed. best configuration seen bold. show mean values generalization best model obtained standard deviation best worst value. topologies shown format number inputs hidden layer node number output layer node number. statistical analysis performed statistical analysis compare base configuration best first three above-mentioned databases. comparison used number connections obtained generalization process. balance algorithm times using default parameters using obtained best results present following considerations using kolmogorov-smirnov test conclude distributions validation number connections best network model distributed normal distribution asintotic signification levels greater standard value normality hypothesis variance equality contrasts means equality studentâ€™s tests done considering runs base best algorithm independent. results show significant differences variances mean values addition results number connections best network models reveal significant differences variances however differences respect connection medium number thus models base parameters less significant shorter. results cancer show test conclude distributions test number connections follows normal distribution asintotic signification levels greater value hand hypothesis normal independent distributions deduce significant differences variances mean values likewise results number connections best models show significant differences variance altough differences respect average number connections obtained models best parameters better regarding mean. results conclude preferable considering cancer algorithm parameters ones best configuration. results pima again show test distribution test number connections follow normal distribution level asintotic signification greater hand hypothesis normal independent distributions infer significant differences variances however differences mean values results number connections best network models show significant differences variances instead differences mean number connections model obtained best parameters significantly better regarding greater significant values mean though respect number connections significant mean number greater models base parameters. previous results conclude preferable pima consider algorithm parameters ones best configuration. comparison previous results although generalization values seem good compare obtained previously standard apply distribution models. table appears generalization mean value best model obtained best configuration using standard distributed pus. best results obtained distribution written bold. average results obtained best distributed models overcomes cases results achieved standard pus. balance improvement; variance reduction produced well indicates greater homogeneity solutions. cancer generalization increment variance decreases slightly. pima significant improvement however variance increases slightly. hyphotyroid generalization increases waveform improvements description case divide execution number amongst nodes. objective obtain measure optimal number nodes considering previous databases. used best configurations first three databases previous proposal. performance analysis obtain time performance measure best configurations first three databases previous proposal. since cluster parallel distributed architecture performance evaluation measures parallel algorithms like speedup efficiency. speedup defined ratio execution time processor parallel time equal processors. definition applied case consideration refer nodes processors. general practice speedup saturated number processors increases. obtain number processors saturation takes place. speedup considers execution time always best measure evaluate algorithm throughput varies number processors. necessary normalize amounts time compare throughput using different nodes. establish normalization efficiency defined ratio speedup number processors. means efficiency obtain optimal value regarding number nodes must carry experiments. consider runs best configuration above-mentioned. seen speedup increase linearly saturates itself. case saturation takes place nodes plotted efficiency values establish comparison values scale. easier obtain optimal number computing nodes; necessary verify number efficiency value greatest. optimal number indicated speedup graph. obtained results different experiments conclude following proposals implemented distribute different features topology experimentation process; tests performed mention efficiency efficacy proposal acceptable; carried experiments medium-size classification databases show model basic behaviour; means model distribution overcome previous results. bishop neural networks pattern recognition. oxford university press. united kingdom. buyya high performance cluster computing architectures systems vol. prentice hall haykin neural networks. comprehensive foundation. macmillan college publishing york. garcÃ­a-pedrajas multiobjetive cooperative coevolution artificial neural networks. neural networks", "year": 2012}