{"title": "Training Feedforward Neural Networks with Standard Logistic Activations  is Feasible", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Training feedforward neural networks with standard logistic activations is considered difficult because of the intrinsic properties of these sigmoidal functions. This work aims at showing that these networks can be trained to achieve generalization performance comparable to those based on hyperbolic tangent activations. The solution consists on applying a set of conditions in parameter initialization, which have been derived from the study of the properties of a single neuron from an information-theoretic perspective. The proposed initialization is validated through an extensive experimental analysis.", "text": "abstract—training feedforward neural networks standard logistic activations considered difﬁcult intrinsic properties sigmoidal functions. work aims showing networks trained achieve generalization performance comparable based hyperbolic tangent activations. solution consists applying conditions parameter initialization derived study properties single neuron information-theoretic perspective. proposed initialization validated extensive experimental analysis. decade impressive performance achieved numerous computer vision tasks including object detection human action recognition image restoration image classiﬁcation natural language tasks including language modelling parsing machine translation speech-to-text translation success deep learning capability transforming input data representations increasingly abstract depth resembles brain structure primates recent theoretical analysis provides partial conﬁrmation experimental ﬁndings obtained deep learning models show exponential advantage terms complexity functions computed deep architectures shallow ones deep learning models impactful many real-world applications transfer technology society created emerging issues like need model interpretability general data protection regulation approved european parliament effective concrete example need provide human understandable justiﬁcations decisions taken automated data-processing systems research could probably inspired literature neural networks better explanations dynamics deep learning provide human interpretable solutions. example process found standard logistic activation functions studied extensively past tend substituted activation functions modern neural networks. understand case important recall unique properties logistic function therefore analyze reasons introduced neural networks. firstly standard logistic function biologically plausible. fact best differentiable approximations leaky integrate-and-ﬁre model used neuroscience model spiking behaviour biological neurons biological plausibility essential driving deep learning research towards uncovering human learning dynamics also providing explanations effectively interpreted humans secondly theoretical work showing family neural networks provided standard logistic activations equivalently converted fuzzy rule-based systems thus raising possibility perform reasoning using fuzzy logic potentially extract human interpretable explanations predictions made deep learning models standard logistic function used extensively activation shallow neural networks receives less attention deep learning. common justiﬁcation supporting fact training deep neural networks challenging intrinsic properties standard logistic function like non-zero mean output value derivative score zero bounded alternative standard logistic activation hyperbolic tangent allows easier training. nevertheless function biological plausible relation fuzzy logic. work aims showing training deep feedforward neural networks standard logistic activations feasible careful initialization. particular derive conditions using information theory used principled criteria initialization. show extensive experimental analysis conditions guarantee better propagation information whole network training vanishing gradients observed thus boosting convergence speed optimizer. proposed initialization outperforms existing strategies also terms generalization performance contribute bridge networks standard logistic activations networks hyperbolic tangents. rest paper organized follows. section provides preliminary discussion statistical properties single neuron standard logistic activation function. section studies neuron informationtheoretic perspective derives initialization conditions parameters. section relates proposed conditions rumelhart propose backpropagation algorithm train feedforward neural network. seminal work authors random weight initialization break symmetry parameters allow perform credit assignment training namely knowing compute weight contribution ﬁnal error. nevertheless initial choice parameters plays important role determining generalization performance ﬁnal trained network demonstrated subsequent works empirical comparison among main works period updated summary related work period research initialization focused mainly shallow architectures motivated fact shallow neural networks universal function approximators deep networks difﬁcult train shallow counterparts problem vanishing exploding gradients. authors probably among propose initialization strategies deep learning. particular random weight initialization combination hand-crafted activations hidden neurons. experimental section proposed initialization strategy particularly suited standard logistic activations strongly affected vanishing/ exploding gradients. ﬁrst effective strategy learn deep models appears consists splitting learning process stages called unsupervised pre-training ﬁnetuning respectively. ﬁrst stage unsupervised algorithm applied layerwise learn increasingly complex representations input features. second stage network parameters updated/ﬁne-tuned using supervised criterion gradient-based optimization. explanation success appears later authors show experimentally unsupervised pretraining regarded effective initialization strategy subsequent optimization stage. words pre-training guides learning towards basins attraction minima support better generalization training data unsupervised pre-training extremely expensive computational perspective alternatives proposed overcome problem vanishing/exploding gradients. particular solutions modify structure neural networks skip connections hidden layers normalizing layers order guarantee continuous information network. approaches study properties loss surface develop training algorithms able better minima. particular optimization algorithms based accelerated gradients like momentum adam combine information past current gradients order dampen oscillations loss surface observed training thus converging faster ﬁnal solution second-order optimization strategies like hessian-free natural gradient methods looks efﬁcient approximations hessian using gaussnewton fisher information matrices respectively. plethora works studying optimization neural networks therefore invite interested reader recent work provides theorical comparative analysis different accelerated gradient-based algorithms survey presents general overview optimization strategies machine learning. inﬂuential studies initialization last decade work particular authors observes logistic sigmoid activation unsuited deep networks random initialization mean value drive especially hidden layer saturation. recent works example conﬁrm fact standard logistic activation difﬁcult train activations proper rescaling making logistic function similar hyperbolic tangent required successful training. alternative activations therefore proposed literature. rectiﬁer linear function appealing solutions unbounded nature allow gradient vanish. principled criteria based orthogonality normalization weights used combination random weight initialization better starting solutions training networks. recent theoretical analysis properties random inizialization analysis rectiﬁer linear functions general theory validated also hyperbolic tangents reveals exists range values variance weights suited propagation gradients thus improving trainability networks. work proposes study difﬁcult problem training standard logistic activations initialization perspective. furthermore shed light general criterion derive initializating conditions explicitly maximizes amount information propasection following question region parameter space guarantees maximum amount information propagated activation function address question formulating problem optimization. logit output modelled continuous random variables distributed according respectively. choose entropy viz. objective maximization problem discard example mutual information since deﬁned particular case. therefore objective written following ez{·} entropy expected value derivative computed variable respectively. note second line obtained ﬁrst simple change variable namely thus information coming neuron proportial information logit shape activation function. error function function visualized fig. deﬁnes lower upper bounds therefore used surrogate objective maximization interval mass fully concentrates characterized input vector weight vector bias standard logistic activation function fig. provides graphical interpretation computational unit used many neural networks. consider logit given neuron namely wixi modelling inputs independent random variables densities/distributions characterized ﬁnite means ﬁnite variances possible exploit lyapunov theorem model gaussian random variable mean variance ar{xi} e{·} ar{·} expected value variance operators respectively. interesting note mean value gaussian density associated mainly dominated parameter variance inﬂuenced weight vector therefore subsequent considerations valid valid also respectively. nonlinearity activation function produces output different statistical properties ones associated logit fact density associated output expressed following relation namely gaussian density seen fig. important mention controls mean variance therefore also amount information propagated neuron. fact large small values information propagation since activation function saturated output variance tends zero happens small values case behaves similarly dirac delta centered shown that extension central limit theorem relaxes assumptions random variables require random variables independent necessarily identically distributed. section study implications maximizing mutual information neuron problem vanishing gradients particular show conditions established theorem ensure selected starting point lies away critical point implies occurrence vanishing gradients. although proposed theoretical analysis imply condition cannot reached later stage learning effectiveness proposed initialization also supported extensive testing presented experimental section. study problem vanishing gradients adopting methodological analysis focus recurrent neural networks seen deepest version feedforward neural nets. obtained results therefore valid traditional feedforward neural networks layer deep feedforward neural net. input hidden state output vectors network respectively. rh×d rh×h wout ro×h weight matrices associated connections neurons bout bias vectors element-wise operator applies sigmoid function incoming vector. recurrent neural networks usually trained mint imizing objective sums loss contributions incurred time horizon duration namely training requires computing gradient objective respect parameters namely optimality result given theorem interpreted condition maximum amount information propagated sigmoid activation. hand condition implies wie{xi} meaning expected value associated logit must central part sigmoid away saturating regions. hand condition ar{xi} note that variances inputs equal constant namely ar{xi} means maximum amount information propagated thorugh obtained vector lies hypersphere appendix show deal general case input variances different other. practice condition ar{xi} rewritten diag. positive deﬁnite quadratic equation characterizes multidimensional ellipsoid. vector must therefore geometric locus points guarantee maximal information propagation. argue obtained results useful deﬁne initial conditions learning parameters neurons standard logistic activations cannot enforced training limit expressivity neural network. fact impose neuron given real-world applications usually require hundreds/thousands hidden neurons layer. therefore even cannot conclude whole training exempt vanishing gradients problems inizialize process sufﬁcient margin prevent problem initial phase typically critical. experimental results provide evidence this shown later paper. experimental results section evaluate performance proposed initialization theory several benchmarks. start analyzing shallow networks consider case deep networks ﬁnally extend analysis recurrent neural networks compare initialization several competitors. hereunder provide summary strategies lecun initialize weights according experiments lecun glorot ortho lsuv identify results achieved respectively. random+ep ortho+ep instead used identify versions initialization procedure. case acronym stands ellyptical projection case shallow network experiment generate samples four dimensional gaussians. classes assigned create ar{x} ar{x} computed numerically using optimality particular ﬁrst inequality obtained using property frobenius norm. equality second line follows directly fact operator- norm diagonal matrix equal maximum diagonal entries whereas last inequality fact derivative sigmoid function cannot larger vanishing gradient problem refers decay number time instants equivalently fact bound tends number layers becomes larger. sufﬁcient condition occurrence problem given condition zero i-th matrix corresponding weights i-th neuron. furthermore conditions derived section viz. number hidden neurons less quantity implies less hcritic vanishing gradient problem guaranteed occur. case refers output variance sigmoid activation less equal therefore hcritic means sufﬁcient condition occurrence vanishing gradient problem number hidden neurons less practical cases limit overcome large margin conﬁguration. case neural network hidden layer containing hidden units. smallest network learn correctly problem gradient descent momentum equal learning rate equal applied minimization cross-entropy objective function. fig. shows results repeated experiments clear experiments proposed initialization allows achieve best solution efﬁcient way. also lsuv able achieve comparable performance method. it’s interesting note ortho obtains moreover worst performance fact test error rate viz. t.e. meaning initialization particularly suited non-linear separable scenario. mnist shallow network section compare different initialization methods shallow network hidden units characterized sigmoid activation functions softmax output layer mnist bechmark dataset mini-batch gradient descent momentum equal learning rate equal applied minimization cross-entropy objective function. size mini-batch consists training samples training algorithm iterations. results including learning curves averaged different random initializations. data normalized mean-centered range plot training curves objective computed training validation sets fig. proposed initialization scheme allows converge faster even case non-deep models. furthermore strategy quite robust initial generation weights. previous attempts shown random initializations produces solutions different performance thus requiring expensive pre-processing strategies reduce variance generalization estimates like unsupervised pre-training best knowledge ﬁrst time random initialization without unsupervised pre-training allows converge solutions reduced variance performance table show test errors related training times. model selection performed based minimization validation objective. performance terms generalization pretty similar methods thus main advantage strategy consists faster convergence. apply also dropout compare state results reported obtained network architecture hand-crafted activation functions. particular authors squashed hyperbolic tangent activations deﬁned atanh function gain close nominal region computed gradients therefore less attenuated compared sigmoids. table summarizes results. best knowledge ﬁrst experimental trial demonstrating sigmoids achieve similar performance hyperbolic tangents fig. learning curves training validation objectives mnist results shallow network results deep network results deep network data augmentation. repeat experiments previous subsection setup deeper network. particular architecture consisting layers number neurons layer chosen according pyramidal structure namely neurons respectively. fig. fig. observe immediately initialization provides gain terms convergence speed quality obtained solution. also interesting competitors observe slow learning early stages training probably fact network parameters initialized regions training objective gradients therefore vanishing. also analyze evolution statistics layer ﬁrst iterations fig. shows behaviour network different initialization strategies. almost competitors activities last hidden layer tend biased towards zero signiﬁcant variation appreciated output layer. symptomatic behaviour already observed networks standard logistic activations. interesting mention problem visible lsuv proposed initializations. probably fact impose conditions variance weights neuron. particular lsuv imposes unit variance conditions require larger value based information-theoretically criterion. also beneﬁcial impact generalization performance shown table therefore weight initialization sufﬁcient guarantee better propagation gradients layers whole training process allowing achieve faster convergence better solutions. apply also data augmentation compare state results reported squashed hyperbolic tangent activations. particular training data augmented following methodology suggested elastic distortion emulate uncontrolled oscillations fig. temporal evolution statistics activations layer different initialization methods ﬁrst iterations. data augmentation applied improve performance. section even deeper rnns case study compare different initialization strategies. well known models difﬁculties remember information inputs long time intervals. fact training rnns affected problem vanishing/exploding gradients literature contains plethora proposed solutions. particular authors propose strategy called echo-state networks consist carefully initializing recurrent weights training output parameters. practice recurrent weight matrix initialized spectral radius close inputs echo long time. represents drastic solution doesn’t exploit full potential rnns. authors show long-term dependencies learnt using hessian-free optimization provided information curvature loss surface therefore able deal vanishing gradients. authors propose solution train rnns regularizer speciﬁcally designed cope vanishing gradients applied loss objective simple momentum-based optimizer used combination gradient clipping ensures gradients explode. experiments strategies focus analyzing impact initialization problem. also important mention recent line research rnns studying unitary recurrent weight matrices example nevertheless work want study general case feasible consists whole parameter space. order study capability models learning long-term dependencies consider similar pathological task introduced ﬁrst time called copy memory problem. task given dictionary input length characters viz. {ai} sequence containing characters dictionary. specifically ﬁrst three characters sequence drawn uniformly independently replacement represent sequence memorized. subset {ai} next characters represent dummy sequence. next character represent trigger inform model start predict memorized sequence. last three characters represent dummy sequence. ground truth output length sequence ﬁrst entries last three characters copy ﬁrst characters input sequence. therefore task consists memorizing input sequence time instants output memorized sequence. experiments generate training test datasets consisting samples each. compare rnns using different initialization strategies report also results baselines. ﬁrst baseline consists outputting ﬁrst entries last randomly sampling subset {ai} three characters. equivalent memoryless strategy. second baseline consists predicting output using lstm model widely used fig. general architecture used experiments recurrent neural networks. input output vectors respectively representing charater word using one-hot encoding represents distributed embedding input vector. data augmentation plays important role achieving better generalization performance partially overcome problems incurred using wrong initialization. nevertheless advantages initialization still visible. fig. shows temporal evolution statistics layer emphasizing fact almost competitors subject problem saturation last hidden layer early stages training table summarizes quantitative results. initialization strategy allows achieve test error rate close performance obtained using hand-crafted hyperbolic tangent provides evidence deep networks standard logistic activations perform similarly networks hyperbolic tangents training made feasible simple conditions initialization. alternative rnns able learn long-term dependencies. performance measured terms perplexity quantity also used training objective. note perplexity ﬁrst baseline analytically network architecture used experiments shown fig. input output vectors represent one-hot encoding character dictionary. size hidden layers recurrent model depends whether using lstm. particular lstm hidden layers size respectively equivalent roughly parameters model. adam optimizer learning rate equal used training algorithm iterations. results averaged different random initializations. figure shows learning curves different initialization strategies well learning curves baselines. possible eventually approaches perform better memoryless strategy meaning networks effectively learn memorize information. furthemore initialization outperforms approaches ﬁrst iterations including lstm. phase iterations ortho achieves best performance. discussed orthogonal initialization guarantee recurrent weight matrix remains orthogonal whole training process even case nonlinear activation functions. particularly useful copying memory problem since family orthogonal matrices contain solution allows learn identity function namely copy exact input sequence output. note iterations performance ortho degrades problem exploding gradients. interesting note convergence achieve best performance among initializations able closer results lstm. language modelling recurrent neural network section conduct experiments real-world task namely language modelling problem using penn tree bank dataset dataset consists training words validation words test words vocabulary words. dataset downloaded tomas mikolov’s webpage. compare rnns using different initialization strategies report also results baseline using lstm experiments show conditions allow improve generalization performance rnns. initialization able achieve comparable performance obtained lstm. network architecture used copying memory problem different size hidden layers. particular neurons rnns neurons lstm. corresponds roughly parameters model. results averaged different random initializations shown table lsuv ortho+ep achieve performance closer ones lstm random+ep obtains apparently performance. argue generation initial matrices random+ep allows sample much larger space matrices consequently adam chances overﬁt. note problem overﬁtting adam optimizer known discussed theoretical perspective validate claim potentially reduce problem overﬁtting also experiments dropout following procedure dropping probability results shown table conﬁrm initial claim. random+ep outperforms initialization able achieve performance comparable lstm. note rnns offer clear advantage terms computational complexity lstm demonstrated training times table initialization parameters sufﬁcient successfully train feedforward neural networks standard logistic activations. initialization based conditions derived studying properties single neuron information theory. study corroborated numerous experiments different known benchmarks different networks. appendix derivation bounds note ﬁrst equations always negative third equation zero therefore hessian matrix obtained stationary point negativedeﬁnite. q.e.d. appendix general case different input variances suppose given initial weight vector general satisy optimality condition given theorem namely ar{xi} need closest vector satisﬁes condition order maximum amount information propagation. formulated following optimization problem diag solution obtained using existing iterative non-convex optimization procedures fact feasible convex. acknowledgments gratefully acknowledge nvidia corporation donation titan pascal machine support research. thank massimo zanetti insightful discussion. references girshick donahue darrell malik region-based convolutional networks accurate object detection segmentation tpami vol. yang convolutional neural networks human action recognition tpami vol. graves jaitly towards end-to-end speech recognition recurrent neural networks icml kruger janssen kalkan lappe leonardis piater rodriguez-sanchez wiskott deep hierarchies primate visual cortex learn computer vision? tpami vol. raghu poole kleinberg ganguli sohl-dickstein expressive power deep neural networks icml poole lahiri raghu sohl-dickstein ganguli exponential expressivity deep neural networks transient chaos nips trends machine learning vol. doshi-velez towards rigorous science interpretable machine learning arxiv preprint arxiv. goodman flaxman european union regulations algorithmic decision-making ’right explanation’ arxiv preprint arxiv.", "year": 2017}