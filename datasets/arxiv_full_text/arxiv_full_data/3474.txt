{"title": "Active Learning for Convolutional Neural Networks: A Core-Set Approach", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (ie. active learning).  Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, ie. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.", "text": "convolutional neural networks successfully applied many recognition learning tasks using universal recipe; training deep model large dataset supervised examples. however approach rather restrictive practice since collecting large labeled images expensive. ease problem coming smart ways choosing images labelled large collection empirical study suggests many active learning heuristics literature effective applied cnns batch setting. inspired limitations deﬁne problem active learning core-set selection i.e. choosing points model learned selected subset competitive remaining data points. present theoretical result characterizing performance selected subset using geometry datapoints. active learning algorithm choose subset expected yield best result according characterization. experiments show proposed method signiﬁcantly outperforms existing approaches image classiﬁcation experiments large margin. deep convolutional neural networks shown unprecedented success many areas research computer vision pattern recognition image classiﬁcation object detection scene segmentation. although cnns universally successful many tasks major drawback; need large amount labeled data able learn large number parameters. importantly almost always better data since accuracy cnns often saturated increasing dataset size. hence constant desire collect data. although desired behavior algorithmic perspective labeling dataset time consuming expensive task. practical considerations raise critical question what optimal choose data points label highest accuracy obtained given ﬁxed labeling budget. active learning common paradigms address question. goal active learning effective ways choose data points label pool unlabeled data points order maximize accuracy. although possible obtain universally good active learning strategy exist many heuristics proven effective practice. active learning typically iterative process model learned iteration points chosen labelled pool unlabelled points using aforementioned heuristics. experiment many heuristics paper effective applied cnns. argue main factor behind ineffectiveness correlation caused batch acquisition/sampling. classical setting active learning algorithms typically choose single point iteration; however feasible cnns since single point likely statistically signiﬁcant impact accuracy local optimization methods iteration requires full training convergence makes intractable query labels one-by-one. hence necessary query order tailor active learning method batch sampling case decided deﬁne active learning core-set selection problem. core-set selection problem aims small subset given large labeled dataset model learned small subset competitive whole dataset. since labels available perform core-set selection without using labels. order attack unlabeled core-set problem cnns provide rigorous bound average loss given subset dataset remaining data points geometry data points. active learning algorithm choose subset bound minimized. moreover minimization bound turns equivalent k-center problem adopt efﬁcient approximate solution combinatorial optimization problem. study behavior proposed algorithm empirically problem image classiﬁcation using three different datasets. empirical analysis demonstrates state-of-the-art performance large margin. discuss related work following categories separately. brieﬂy work different existing approaches deﬁnes active learning problem core-set selection consider fully supervised weakly supervised cases iii) rigorously address core-set selection problem directly cnns extra assumption. active learning active learning widely studied early work found classical survey settles covers acquisition functions information theoretical methods ensemble approaches uncertainty based methods bayesian active learning methods typically non-parametric model like gaussian process estimate expected improvement query expected error queries approaches directly applicable large cnns since scale large-scale datasets. recent approach ghahramani shows equivalence dropout approximate bayesian inference enabling application bayesian methods deep learning. although bayesian active learning shown effective small datasets empirical analysis suggests scale large-scale datasets batch sampling. important class uncertainty based methods hard examples using heuristics like highest entropy geometric distance decision boundaries empirical analysis effective cnns. recent optimization based approaches trade-off uncertainty diversity obtain diverse hard examples batch mode active learning setting. elhamifar yang design discrete optimization problem purpose convex surrogate. similarly cast similar problem matrix partitioning. however optimization algorithms proposed papers variables number data points. hence scale large datasets. also many pool based active learning algorithms designed speciﬁc class machine learning algorithms like k-nearest neighbors naive bayes logistic regression schuurmans linear regression gaussian noise even algorithm agnostic case design set-cover algorithm cover hypothesis space using sub-modularity hand demir uses heuristic ﬁrst ﬁlter pool based uncertainty choose point label using diversity. algorithm considered class; however uncertainty information. algorithm also ﬁrst applied cnns. similar joshiy uses similar optimization problem. however offer theoretical justiﬁcation analysis. wang proposes empirical risk minimization like however minimize difference distributions instead recently discrete optimization based method similar presented k-nn type algorithms domain shift setting. although theoretical analysis borrows techniques them results valid k-nns. active learning algorithms cnns also recently presented wang propose heuristic based algorithm directly assigns labels data points high conﬁdence queries labels ones conﬁdence. moreover stark speciﬁcally targets recognizing captcha images. although results promising captcha recognition method effective image classiﬁcation. discuss limitations approaches section theoretical side shown greedy active learning possible algorithm data agnostic case however data dependent results showing indeed possible obtain query strategy better sample complexity querying points. results either assumptions data-dependent realizability hypothesis space like data dependent measure concept space called disagreement coefﬁcient also possible perform active learning batch setting using greedy algorithm importance sampling although aforementioned algorithms enjoy theoretical guarantees apply large-scale problems. core-set selection closest literature work problem core-set selection since deﬁne active learning core-set selection problem. problem considers fully labeled dataset tries choose subset model trained selected subset perform closely possible model trained entire dataset. speciﬁc learning algorithms methods like core-sets core-sets k-means k-medians however aware method cnns. similar algorithm unsupervised subset selection algorithm uses facility location problem diverse cover dataset. algorithm differs uses slightly different formulation facility location problem. instead min-sum minimax form. importantly apply algorithm ﬁrst time problem active learning provide theoretical guarantees cnns. weakly-supervised deep learning paper also related semi-supervised deep learning since experiment active learning fully-supervised weakly-supervised scheme. early weakly-supervised convolutional neural network algorithms ladder networks recently seen adversarial methods learn data distribution result two-player non-cooperative game methods extended feature learning ladder networks experiments; however method agnostic weakly-supervised learning algorithm choice utilize model. section formally deﬁne problem active learning batch setting notation rest paper. interested class classiﬁcation problem deﬁned compact space label space also consider loss function parametrized hypothesis class e.g. parameters deep learning algorithm. assume class-speciﬁc regression functions λη-lipschitz continuous consider large collection data points sampled i.i.d. space yi}i∈ consider initial pool data-points chosen uniformly random }j∈. active learning algorithm access {xi}i∈ {ys}j∈. words labels points initial sub-sampled pool. also given budget queries words active learning algorithm choose extra points labelled oracle minimize future expected loss. differences formulation classical deﬁnition active learning. classical methods consider case budget single point negligible effect deep learning regime hence consider batch case. also common consider multiple rounds game. also follow multiple round formulation myopic approach solving single round labelling iteration active learning algorithm stages identifying data-points presenting oracle labelled training classiﬁer using previously labeled data-points. second stage done fully weakly-supervised manner. fully-supervised case training classiﬁer done using labeled data-points. weakly-supervised case training also utilizes points labelled yet. although existing literature focuses active learning fully-supervised models consider cases experiment both. classical active learning setting algorithm acquires labels querying oracle unfortunately feasible training cnns since single point statistically signiﬁcant impact model local optimization algorithms. infeasible train many models number points since many practical problem interest large-scale. hence focus batch active learning problem active learning algorithm choose moderately large points labelled oracle iteration. order design active learning strategy effective batch setting consider following upper bound active learning loss formally deﬁned quantity interested population risk model learned using small labelled subset population risk controlled training error model labelled subset generalization error full dataset term deﬁne core-set loss. core-set loss simply difference average empirical loss points labels average empirical loss entire dataset including unlabelled points. empirically widely observed cnns highly expressive leading training error typically generalize well various visual problems. moreover generalization error cnns also theoretically studied shown bounded mannor hence critical part active learning core-set loss. following observation re-deﬁne active learning problem informally given initial labelled budget trying points query labels learn model performance model labelled subset whole dataset close possible. optimization objective deﬁne directly computable since access labels unlabelled). hence section give upper bound objective function optimize. start presenting bound loss function lipschitz ﬁxed true label parameters show loss functions cnns relu non-linearities satisfy property. also rely zero training error assumption. although zero training error entirely realistic assumption experiments suggest resulting upper bound effective. state following theorem; theorem given i.i.d. samples drawn yi}i∈ points loss function λl-lipschitz continuous bounded regression function λη-lipschitz cover yi}i∈ probability least since assume zero training error core-set core-set loss equal average error entire dataset state theorem form consistent visualize theorem figure defer proof appendix. theorem cover means balls radius centered member cover entire informally theorem suggests bound core-set loss covering radius term goes zero rate depends solely interesting result since bound depend number labelled points. words provided label help core-set loss unless decreases covering radius. order show bound applies cnns prove lipschitz-continuity loss function respect input image ﬁxed true label following lemma max-pool restricted linear units non-linearities loss deﬁned distance desired class probabilities soft-max outputs. cnns typically used cross-entropy loss classiﬁcation problems literature. indeed also perform experiments using cross-entropy loss although loss theoretical study. although theoretical study extend cross-entropy loss experiments suggest resulting algorithm effective cross-entropy loss. lemma loss function deﬁned -norm class probabilities softmax output convolutional neural network convolutional fully connected layers deﬁned classes -lipschitz function input ﬁxed class probabilities network parameters. here maximum input weights neuron although general unbounded made arbitrarily small without changing loss function behavior defer proof appendix conclude cnns enjoy bound presented theorem order computationally perform active learning upper bound. words practical problem interest becomes mins|s≤b| δs∪s. problem equivalent k-center problem next section explain solve k-center problem practice using greedy approximation. provided upper bound loss function core-set selection problem showed minimizing equivalent k-center problem intuitively deﬁned follows; choose center points largest distance data point nearest center minimized. formally trying solve unfortunately problem np-hard however possible obtain solution efﬁciently using greedy approach shown algorithm mins maxi minj∈s∪s greedy algorithm shown algorithm proven solution that; maxi minj∈s∪s although greedy algorithm gives good initialization practice improve solution iteratively querying upper bounds optimal value. words design algorithm decides order deﬁne mixed integer program parametrized feasibility indicates mins maxi minj∈s∪s straight-forward algorithm would sub-routine performing binary search result greedy algorithm half since optimal solution guaranteed included range. constructing also handle weaknesses k-center algorithm namely robustness. make k-center problem robust assume upper limit number outliers algorithm choose cover unsupervised data points. mixed integer program written critical design choices distance metric distance activations ﬁnal fully-connected layer distance. weakly-supervised learning used ladder networks experiments used vgg- architecture. initialized convolutional ﬁlters according optimized models using rmsprop learning rate using tensorﬂow train cnns scratch iteration. tested algorithm problem classiﬁcation using three different datasets. performed experiments cifar caltech- datasets image classiﬁcation svhn dataset digit classiﬁcation. cifar dataset tasks; coarse-grained classes ﬁne-grained classes. performed experiments both. compare method following baselines i)random choosing points labelled uniformly random unlabelled pool. ii)best empirical uncertainty following empirical setup perform active learning using max-entropy bald variation ratios treating soft-max outputs probabilities. report best performing dataset since perform similar other. iii) deep bayesian active learning perform monte carlo dropout obtain improved uncertainty measures report best performing acquisition function among max-entropy bald variation ratios dataset. best oracle uncertainty also report best performing oracle algorithm uses label information entire dataset. replace uncertainty unlabelled examples. sample queries normalized form function setting probability choosing point queried v)k-median choosing points labelled cluster centers k-median algorithm. vi)batch mode discriminative-representative active learning based approach uses uncertainty minimizes iid. samples dataset actively chosen points. vii)ceal ceal weakly-supervised active learning method proposed speciﬁcally cnns. include weakly-supervised analysis. conducted experiments active learning fully-supervised models well active learning weakly-supervised models. experiments start small images sampled uniformly random dataset initial pool. weakly-supervised model access labeled examples well unlabelled examples. fully-supervised model access labeled data points. experiments random initializations initial pool labeled points average classiﬁcation accuracy metric. plot accuracy number labeled points. also plot error bars standard deviations. query algorithm iteratively; words solve discrete optimization problem minsk+|sk+|≤b exy∼pz point accuracy number labelled examples graph. present results figures figures suggests algorithm outperforms baselines experiments; case weakly-supervised models large margin. believe effectiveness approach weakly-supervised case better feature learning. weakly-supervised models provide better feature spaces resulting accurate geometries. since method geometric performs signiﬁcantly better better feature spaces. also observed algorithm less effective cifar- caltech- compared cifar- svhn. easily explained using theoretical analysis. bound core-set loss scales number classes hence better fewer classes. interesting observation fact state-of-the-art batch mode active learning baseline necessarily perform better greedy ones. believe fact still uses uncertainty information soft-max probabilities good proxy uncertainty. method uncertainty. incorporating uncertainty method principled open problem fruitful future research direction. hand pure clustering based batch active learning baseline also effective. believe rather intuitive since cluster sentences likely points well covered initial iid. samples. hence clustering based method fails sample tails data distribution. results suggest oracle uncertainty information bayesian estimation uncertainty helpful since improve empirical uncertainty baseline; however still effective batch setting since random sampling outperforms them. believe correlation queried labels consequence active learning batch setting. investigate qualitative analysis tsne embeddings. compute embeddings points using features learned using labelled examples visualize points sampled method well oracle uncertainty. visualization suggests correlation among samples uncertainty based methods fail cover large portion space conﬁrming hypothesis. optimality k-center solution proposed method uses greedy -opt solution k-center problem initialization checks feasibility mixed integer program figure tsne embeddings cifar dataset behavior uncertainty oracle well method. methods initial labeled pool images shown blue images chosen labeled green remaining ones red. algorithm results queries evenly covering space. hand samples chosen uncertainty oracle fails cover large portion space. lp-relaxation deﬁned branch-and-bound obtain integer solutions. utility obtained solving expensive investigated. compare average run-time run-time -opt solution table also compare accuracy obtained optimal k-center solution -opt solution figure cifar- dataset. shown table although run-time polynomial worst-case practice converges tractable amount time dataset images. hence algorithm easily applied practice. figure suggests small signiﬁcant drop accuracy -opt solution used. hence conclude unless scale dataset restrictive using proposed optimal solver desired. even accuracy drop active learning strategy using -opt solution still outperforms baselines. hence conclude algorithm scale dataset size small accuracy drop even solving feasible. study active learning problem cnns. empirical analysis showed classical uncertainty based methods limited applicability cnns correlations caused batch sampling. re-formulate active learning problem core-set selection study core-set problem cnns. validated algorithm using extensive empirical study. empirical results three datasets showed state-of-the-art performance large margin. references martin abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv. beg¨um demir claudio persello lorenzo bruzzone. batch-mode active-learning methods interactive classiﬁcation remote sensing images. ieee transactions geoscience remote sensing daniel golovin andreas krause. adaptive submodularity theory applications active learning stochastic optimization. journal artiﬁcial intelligence research kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition steven rong jianke michael lyu. batch mode active learning application medical image classiﬁcation. proceedings international conference machine learning joshiy porikli papanikolopoulos. multi-class batch-mode active learning image classiﬁcation. ieee international conference robotics automation ./robot... yuval netzer wang adam coates alessandro bissacco andrew reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning volume yang zhigang feiping xiaojun chang alexander hauptmann. multi-class active learning uncertainty sampling diversity maximization. international journal computer vision", "year": 2017}