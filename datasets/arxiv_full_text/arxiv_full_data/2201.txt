{"title": "Learning Causal Structures Using Regression Invariance", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We study causal inference in a multi-environment setting, in which the functional relations for producing the variables from their direct causes remain the same across environments, while the distribution of exogenous noises may vary. We introduce the idea of using the invariance of the functional relations of the variables to their causes across a set of environments. We define a notion of completeness for a causal inference algorithm in this setting and prove the existence of such algorithm by proposing the baseline algorithm. Additionally, we present an alternate algorithm that has significantly improved computational and sample complexity compared to the baseline algorithm. The experiment results show that the proposed algorithm outperforms the other existing algorithms.", "text": "study causal inference multi-environment setting functional relations producing variables direct causes remain across environments distribution exogenous noises vary. introduce idea using invariance functional relations variables causes across environments. deﬁne notion completeness causal inference algorithm setting prove existence algorithm proposing baseline algorithm. additionally present alternate algorithm signiﬁcantly improved computational sample complexity compared baseline algorithm. experiment results show proposed algorithm outperforms existing algorithms. causal inference fundamental problem machine learning applications several ﬁelds biology economics epidemiology computer science etc. performing interventions system possible main approach identifying direction inﬂuences learning causal structure perform statistical tests based conditional dependency variables data case complete conditional independence based algorithm allows learning causal structure extent possible complete mean algorithm capable distinguishing orientations markov equivalence. algorithms perform conditional independence test along meek rules introduced algorithms well known examples. within framework structural equation models adding assumptions model non-gaussianity nonlinearity equal noise variances even possible identify exact causal structure. experimenter capable intervening system effect varying variable variables system causal structure could exactly learned. setting common identiﬁcation procedure assumes variables whose distributions varied descendants intervened variable hence causal structure reconstructed performing interventions different variables system take different approach traditional interventional setting considering multienvironment setting functional relations producing variables parents remain across environments distribution exogenous noises vary. different customary interventional setting model experimenter control location changes system seen figure prevent ordinary interventional approaches working. multi-environment setting also studied work perspective relationship related work below. focus linear additive noise underlying data generating model note model problematic models literature causal inference noises gaussian distribution many structures none existing observational approaches identify underlying causal structure uniquely. main idea proposed approach utilize change regression coefﬁcients resulting changes across environments distinguish causes effects. approach able identify causal structures previously identiﬁable using existing approaches. figure shows simple examples illustrate point. ﬁgure directed edge form variable implies direct cause change exogenous noise across environments denoted ﬂash sign. consider structure figure equations independent mean-zero gaussian exogenous noises. suppose interested ﬁnding variable cause effect. given environments across exogenous noise varied. denoting regression coefﬁcient resulting regressing case therefore except pathological cases values variance exogenous noises environments regression coefﬁcient resulting regressing cause variable effect variable varies environments regression coefﬁcient regressing effect variable cause variable remains same. hence cause distinguishable effect. note structures markov equivalence class hence distinguishable using merely conditional independence tests. also since exogenous noises variables changed commonly used interventional tests also capable distinguishing structures moreover shortly explained exogenous noise target variable changed invariant prediction method cannot discern correct structure either. another example consider structure figure suppose exogenous noise varied across environments. similar previous example shown varies across environments remains same. implies edge former later. similarly varies across environments remains same. implies parent therefore structure figure distinguishable using proposed identiﬁcation approach. note invariant prediction method cannot identify relation conditional independence tests also able distinguish structure. related work. best known algorithms causal inference observational setup algorithms. purely observational approaches reconstruct causal graph markov equivalence classes. thus directions edges remain unresolved. studies attempt identify exact causal structure restricting model class work consider independent noise. lingam method potent approach capable structure learning linear model additive noise long distribution noise gaussian. authors showed nonlinearities play role similar non-gaussianity. interventional approach causal structure learning experimenter picks speciﬁc variables attempts learn relation variables observing effect perturbing variables distribution others. recent work bounds required number interventions complete discovery causal relationships well passive adaptive algorithms minimize number experiments derived work assume functional relations variables direct causes across environments invariant. similar assumptions considered work speciﬁcally studies ﬁnding causal relation variables related invertible function assumes distribution cause function mapping cause effect independent since correspond independent mechanisms nature. little work multi-environment setup authors analyze classes structures equivalent relative stream distributions present algorithms output graphical representations equivalence classes. assume changing distribution variable varies marginal distribution descendants. naturally also assumes access enough samples test variable marginal distribution change. approach cannot identify causal relations among variables affected environment changes way. closely related work approach invariant prediction method utilizes different environments estimate predictors target variable. work assumed exogenous noise target variable vary among environments. fact method crucially relies assumption adds variables estimated predictors necessary keep distribution target variable’s noise ﬁxed. besides high computational complexity invariant prediction framework result contain parents target variable. additionally optimal predictor necessarily unique. show many cases proposed approach overcome issues. recently authors considered setting changes mechanism variables prevents ordinary conditional independence based algorithms discovering correct structure. authors modeled changes multiple environments proposed general solution non-parametric model ﬁrst detects variables whose mechanism changed ﬁnds causal relations among variables using conditional independence tests. generality model method requires high number samples. contribution. propose novel causal structure learning framework capable uniquely identifying structures identiﬁable using existing methods. main contribution work introduce idea using invariance functional relations variables direct causes across environments. would imply invariance coefﬁcients special case linear distinguishing causes effects. deﬁne notion completeness causal inference algorithm setting prove existence algorithm proposing baseline algorithm algorithm ﬁrst ﬁnds variables distributions noises varied across environments uses information identify causal structure. additionally present alternate algorithm signiﬁcantly improved computational sample complexity compared baseline algorithm. regression-based causal structure learning deﬁnition consider directed graph vertex directed edges ﬁnite graph directed cycles. called causal vertices represent random variables directed edges indicates variable direct cause variable consider linear underlying data generating model. model value variable determined linear combination values causal parents plus additive exogenous noise nj’s jointly independent follows implies variable written linear combination exogenous noises system. assume model variables observable. also ease representation focus zero-mean gaussian exogenous noise; otherwise results could easily extended arbitrary distribution exogenous noise system. following deﬁnitions used throughout paper. deﬁnition graph union mixed graphs skeleton mixed graph skeleton members contains directed edge rest edges remain undirected. deﬁnition causal dags markov equivalent every distribution compatible graphs also compatible other. markov equivalence equivalence relationship graphs graph union dags markov equivalence class called essential graph denoted ess. consider multi-environment setting consisting environments em}. structure causal functional relations producing variables parents remains across environments exogenous noises vary though. pair environments variables whose exogenous noise changed environments. given consistent essential graph obtained conditional independence test deﬁne regression invariance follows regression coefﬁcients regressing variable environments respectively. words variables contains subsets \\{x} regress regression coefﬁcients change across deﬁnition given variables whose exogenous noise changed environments dags called i-distinguishable make following assumption distributions exogenous noises. purpose assumption rule pathological cases values variance exogenous noises environments make special regression relations. instance example variances exogenous noise environments respectively. note special relation assumption given structure perturbing variance distributions exogenous noises small value change regression invariance give following examples applications approach. example consider dags calculating regression coefﬁcients explained section hence i-distinguishable. mentioned section structures distinguishable using ordinary conditional independence tests. also case invariant prediction approach ordinary interventional tests experimenter expects change distribution effect would perturb marginal distribution cause variable capable distinguishing structures either. example consider figure {x}. consider alternative compared directed edge replaced compared directed edge replaced since pair pair belongs structure also distinguishable using proposed identiﬁcation approach. note distinguishable using conditional independence tests. also invariant prediction method cannot identify relation since keep variance noise ﬁxed setting predictor empty intersection. example consider structure figure {x}. among possible triangle dags i-distinguishable structure hence environments differing exogenous noise triangle could identiﬁed. note triangle dags markov equivalent class hence using information environment alone observation setting cannot lead identiﬁcation. structure figure i-distinguishable triangle direction edge ﬂipped. dags also distinguishable using usual intervention analysis invariant prediction method. structure ground truth structure. deﬁne dags i-distinguishable using form mixed graph graph union members deﬁnition algorithm gets essential graph regression invariance input returns mixed graph regression invariance complete directed graph words algorithm regression invariance complete given correct essential graph regression invariance able return appropriate mixed graph. section introduce structure learning algorithm complete sense deﬁnition existence complete algorithms section show existence complete algorithm learning causal structure among variables whose dynamics satisfy sense deﬁnition pseudo-code algorithm presented algorithm suppose ground truth structure. algorithm ﬁrst performs conditional independence test followed applying meek rules obtain essential graph ess. pair environments ﬁrst algorithm calculates regression coefﬁ cients forms regression invariance contains pairs regression coefﬁcients change next using function changefinder discover variables whose exogenous noises varied environments using function consistantfinder possible dags consistent rij. taking union graphs form graph mixed graph containing causal relations distinguishable given regression information environments. clearly since searching dags baseline algorithm complete sense deﬁnition obtaining pairs environments algorithm forms mixed graph taking graph union mij’s. perform meek rules extra orientations output obtaining part given signiﬁcance level show obtained correctly probability least given environments deﬁne null hypothesis input joint distribution environments {ei}m obtain performing complete conditional independence test. pair environments obtain changef inder. consistentf inder iij). lemma given environments variable neighbors variance exogenous noise changed environments. otherwise variance ﬁxed. appendix proof. based lemma variance residual xsβs remains ﬁxed environments. check whether variance exogenous noise changed environments testing following null hypothesis order test null hypothesis compute variance residuals less reject null hypothesis maximum degree causal graph. reject hypothesis tests iij. function consistentfinder directed paths variable variable deﬁne weight directed path π∈dbvu coefﬁcients deﬁnition seen entry matrix thus entries matrix multivariate polynomials entries rows corresponding matrix respectively matrix diagonal matrix discussion know entries matrix multivariate polynomials entries equation implies entries vector rational functions entries therefore entries jacobian matrix respect diagonal entries also rational expression parameters. function consistentfinder select directed graph consistent order check whether initially then parametrically compute jacobian matrix noted above entries jacobian matrix obtained rational expressions entries columns jacobian matrix corresponding elements zero changing varying variances exogenous noises iij). checking consider graph rij. algorithm baseline algorithm section presented prove existence complete algorithms practical high computational sample complexity. section present local regression examiner algorithm alternative much efﬁcient algorithm learning causal structure among variables pseudo-code algorithm presented algorithm make following result algorithm. lemma consider adjacent variables causal structure pair environments parent appendix proof. algorithm consists three stages. ﬁrst stage similar baseline algorithm performs complete conditional independence test obtain essential graph. variable forms discovered parents discovered children leaves remaining neighbors unknown second stage goal variable relation neighbors based invariance regression neighbors across pair environments. pair environments changing look auxiliary among neighbors minimum number elements s∪{x}. found implies child otherwise required regressors coefﬁcient parents otherwise required regressors coefﬁcient although still parents make decisions regarding relation adding discovered relationships initial mixed graph third stage perform meek rules resulting mixed graph extra possible orientations output analysis reﬁned algorithm. hypothesis testing test whether equal p-value vectors less reject null hypothesis output algorithm correct probability least regarding computational complexity since pair environments worse case perform hypothesis tests experiments evaluate performance algorithm testing synthetic real data. seen pseudo-code algorithm three stages ﬁrst stage complete conditional independence performed. order acceptable time complexity simulations used algorithm known complexity order applied graph order degree bound synthetic data. generated dags order ﬁrst selecting causal order variables connecting pair variables probability generated data linear gaussian coefﬁcients drawn uniformly random variance exogenous noise drawn uniformly random variable structure samples generated. simulation consider scenario environments second environment exogenous noise variables varied. perturbed variables chosen uniformly random. figure shows error ratio undirected edges ratio stage corresponds algorithm ﬁnal output algorithm. deﬁne error ratio calculated follows link directed undirected edge. ratio count number undirected edges among correctly detected links i.e. ratio seen figure change second environment reduces ratio percent compared algorithm. also main source error algorithm results application algorithm. also compared error ratio algorithm invariant prediction lingam lingam combined data environments input. therefore distribution exogenous noise variables guassian anymore. seen figure error ratio increases size increases. mainly fact approach assumed distribution exogenous noise target variable change violated increasing |i|. result simulations shows error ratio lingam approximately twice real data. considered dataset educational attainment teenagers dataset collected pupils high school attributes including gender race base year composite test score family income whether parent attended college county unemployment rate. split dataset parts ﬁrst part includes data pupils live closer miles -year college. experiment tried identify potential causes inﬂuence years education pupils received. algorithm parts data environments signiﬁcance level obtained following attributes possible parents target variable base year composite test score whether father college graduate race whether school urban area. method also showed ﬁrst attributes signiﬁcant effects target variable. janzing mooij zhang lemeire zscheischler daniušis steudel schölkopf. information-geometric approach inferring causal directions. artiﬁcial intelligence peters bühlmann meinshausen. causal inference using invariant prediction identiﬁcation conﬁdence intervals. journal royal statistical society series tian pearl. causal discovery changes. proceedings seventeenth conference uncertainty artiﬁcial intelligence pages morgan kaufmann publishers inc. verma pearl. algorithm deciding observed independencies causal explanation. proceedings eighth international conference uncertainty artiﬁcial intelligence pages morgan kaufmann publishers inc. zhang hyvärinen. distinguishing causes effects using nonlinear acyclic causal models. journal machine learning research workshop conference proceedings volume pages variance changed clearly choice second summation vanishes ﬁrst summation therefore variance residual remains unvaried. otherwise variance varies change cancel speciﬁc values variances exogenous noises according similar reasoning assumption ignore expression ﬁrst summation contains exogenous noises numerator second summation contains terms related variance orthogonal exogenous noises. therefore assumption remains unchanged. case also note always remain unchanged exogenous noise variables affect", "year": 2017}