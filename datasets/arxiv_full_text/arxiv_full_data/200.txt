{"title": "Lightweight Neural Networks", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "abstract": "Most of the weights in a Lightweight Neural Network have a value of zero, while the remaining ones are either +1 or -1. These universal approximators require approximately 1.1 bits/weight of storage, posses a quick forward pass and achieve classification accuracies similar to conventional continuous-weight networks. Their training regimen focuses on error reduction initially, but later emphasizes discretization of weights. They ignore insignificant inputs, remove unnecessary weights, and drop unneeded hidden neurons. We have successfully tested them on the MNIST, credit card fraud, and credit card defaults data sets using networks having 2 to 16 hidden layers and up to 4.4 million weights.", "text": "weights lightweight neural network value zero remaining ones either universal approximators require approximately bits/weight storage posses quick forward pass achieve classiﬁcation accuracies similar conventional continuous-weight networks. training regimen focuses error reduction initially later emphasizes discretization weights. ignore insigniﬁcant inputs remove unnecessary weights drop unneeded hidden neurons. successfully tested mnist credit card fraud credit card defaults data sets using networks hidden layers million weights. lightweight neural networks subset conventional continuous-weight networks call lightweight trained lwns weights require approximately bits/weight storage forward-passes require ﬂoating-point multiplications. characteristic lwns sparsity weight matrices. moreover non-zero weights matrices limited values networks ﬁrst introduced multiplier-free networks used training heuristics proposed recent interest similar networks present results highlighting sparsity networks natural inclination towards forming tight receptive ﬁelds universal approximation capability. would like highlight aspects neurons make similar structure function biological neurons compared neurons. consider axon source biological neuron connecting dendrite target neuron synapse. types neurotransmitter chemicals released axon’s side synapse whenever source neuron activated chemicals bind receptors dendrite-side synapse resulting increase decrease electrical potential membrane target neuron. electrical potential membrane contributions ﬁrings neurons connected synapses target neuron. membrane’s electrical potential reaches threshold value target neuron ﬁres. highlight narrative absence multiplication operations presence synaptic values excitatory inhibitory. figure neuron connected neurons preceding layer whereas neurons limited receptive ﬁelds. weights varying values positive negative weights values receptive ﬁeld biological neuron fan-in neuron. studies processing visual cortex animals show varies among diﬀerent types neurons similar structure neuron specialist neuron specializes particular subset inputs. conventional ﬁxed every neuron every layer equal number neurons preceding layer. much smaller varies number neurons layer. training inspired synaptic pruning process biological brain start plenty; prune excess later. natural phenomenon prunes example synapses among humans approach adulthood reduce number neurons. training process prunes initial count synapses which many cases results elimination neurons well. universal approximation although weights restricted lwns’ thresholds constraints. activation functions bounded conﬁned hyperbolic tangent discussion. help ingredients create one-dimensional bump arbitrarily small height arbitrary location x-axis using expressions form summations bumps used approximate arbitrary one-dimensional functions accuracy. discuss extension one-dimensional construction denseness start theorem khan weightless neural network hand simpler unit-valued inputoutput-layer weights hand complex types hidden neurons. types diﬀering irrational multiplication factor activation functions. allowing inputoutput-layer weights assume additional values harm density result result networks train quicker compact. choice multiplication factor arbitrary chosen close one. simulated digital computer choice become exactly limited precision computer network single type hidden neurons. simpliﬁed conﬁguration inputs hidden neurons written expression depicts network solitary layer hidden neurons. form multi-hidden-layer networks layers comprising neurons identical ﬁrst hidden-layer employed. moreover simulations used output neurons identical hidden neurons. several approaches proposed training neural networks discrete weights. hwang sung take trained discretize weights ternary values retrain using backpropagation. also restricted signals depth three bits. mellempudi also start trained ﬁne-grained ternarization method exploits local correlations dynamic-range parameters minimize impact discretization accuracy network. stochastic gradient descent method train ternary-weight networks. ternary-weights forward backward propagations weight updates. gradually discretize weights zero powers minimizing euclidean distance conventional weights closest discrete value backpropagation. figure plot training epochs mean-squared error training data ratio non-discrete weights weights test data miss-classiﬁcations. bottom plot magniﬁed y-axis calculated desired output vectors value individual weight weight discretization function. diﬀerentiable zeros {±}. main point original heuristics sequential application error-reduction weightdiscretization steps network every training epoch. based steepest-descent weight-discretization supplemented additional mechanism take care weight-update paralysis. paralysis caused opposing weight-updates calculated error-reduction weight-discretization steps. additional mechanism black-hole mechanism forced nearlydiscrete weights discrete values. rate weight-discretization radius black-hole grew error output network shrank. black-hole mechanism worked well shallow networks comprising hundreds weights discussed failed overcome weight-update paralysis deeper lwns thousands millions weights discussed paper. networks propose additional mechanism comes play almost weights discrete. stage weights rounded nearest discrete value {±}. results network acceptable test-data accuracy training concluded. otherwise rounding step rolled back normal training resumed pre-rounding weights. resulting error plots typical shown figure mnist well-known data images handwritten digits used version available tensorflow machine learning library version includes images training test set. images normalized centered transformed matrix -bit ﬂoating-point numbers ranging zero according gray-scale value associated pixel. matrices ﬂattened -element vectors. image labels -element vectors single element vector equal rest value zero. used data help network conﬁguration training process information original nature image vector. interest best possible result look learns simplest possible conﬁguration. simplest conﬁguration neurons layers exactly layers fully-connected layer preceding them. factors varied number hidden-layers number neurons layers. summary results two-hidden-layer lwns shown table conﬁguration results top-two lwns best test-data accuracies shown. test-data accuracies range want balance accuracy size choose lwn. achieves accuracy million weights compares favorably much larger extra neurons million additional weights. result similar reported reported much epochs required achieve reasonable accuracy number much dependent training parameters. make eﬀort optimize parameters purpose. figure shows mean-squared error training data ratio misclassiﬁcations total number test data examples ratio non-discrete weights total number weights function training epochs. ﬁgure clearly reﬂects deliberate slowness weight-discretization processes compared error minimization process. intention continuous-weight error minimum ﬁrst look discrete-weight minimum immediate vicinity. lack smoothness training error test misclassiﬁcation curves weight-discretization corrections. smoothness non-discrete weight curve indicates slowness discretization. zero-valued weights sparsity two-hidden-layer lwns ranged larger networks tending sparser weight matrices. relationship dimensions hidden layers number non-zero weights shown figure log-log-log scale. seems mainly dependent all-zero-rows column table indicates ignores inputs number neurons ﬁrst hidden layer inadequate. mnist data minimum layer size seems number consistent minimum reported lecun lwns ignored pixels bottom three rows four left-most three right-most columns. whereas larger lwns ignored pixels bottom three left-most right-most columns. indicates normalized mnist images slightly oﬀ-center lwns ignoring nearly-white pixels around digits information value. fully-connected structure weights-to-neurons ratio increases number neurons. restricts rise complexity limiting neuron narrow receptive ﬁeld. figure shows weights-to-neurons ratio function number neurons network. function size hidden layer shown figure here excluded smaller networks neurons hidden-layer linear-regression model. reduces burden processing neuron reducing added ﬁrst hidden-layer. emphasize knowing mnist data comprises images. neither architecture include custom-designed small-receptive-ﬁeld convolution layers convolution neural network lecun however tends drop non-crucial inputs receptive-ﬁeld neurons setting corresponding weight zero. behavior true almost neurons layers network. contrast size receptive ﬁeld varies neuron neuron parameter training process arrived naturally consequence training process. receptiveﬁelds clearly present biological systems processing visual aural touch possibly stimuli. design example inspired systems. dataprocessing situations receptive-ﬁelds obvious. automatically discover leverage even situations. hidden neurons weights zero consequence training regimen. number dropped neurons indicated number all-zero rows weight matrices related mismatch number neurons consecutive layers. table makes clear greater mismatch greater number dropped hidden neurons. tables also shows that cases hidden-layer neuron connected output neurons. indicates eﬃcient distribution image recognition task among hidden neurons. task distributed neuron distinct focused responsibility. size hidden-layer seem much factor. extreme case dropped neurons second hidden layer. remaining connected single output neuron connected output neurons. average figure relationship average size receptive ﬁelds average number neurons ﬁrst hidden-layer mnist data set. regression model excludes ﬁrst four points. number approaches sparsity increases. table shows comparison storage requirements various weight depths excludes eﬀect thresholds storage requirements. -bit thresholds conﬁguration consume respectively. training -bit thresholds expect work results reported vanhoucke lwns posses eﬃcient forward pass multiplications neurons trivial nature weights restricted {±}. moreover high sparsity weight matrices even trivial operation necessary time. thresholds -bit ﬂoating-point numbers never part multiplication operation. therefore require ﬂoating-point multiplication operation all. training deeper networks trained several lwns hidden-layers varying sizes training heuristics suﬀered vanishing gradient problem results shown table training runs using training parameter values -hidden-layer lwns. focus obtaining best performing lwns able train theses deeper lwns all. surprisingly extensive simulations mnist data looked credit card fraud default data validate viability lwn. characteristic data sets class imbalance. issue addressed undersampling majority class oversampling minority class. tried oversampling diﬀerent ways simple repetition minority class examples synthetic minority oversampling technique simulations normalized continuous features zero-mean unit-variance clipped range binary features mapped split data training testing sets using stratiﬁed sampling. assigned separate outputs classes. higher outputs considered winning class testing. methodology avoids work required suitable output threshold classiﬁcation switches class other. credit card fraud data contained features consisted transactions fraudulent. best performing network score oversampled training data. pozzolo reported g-mean scores data using logit boost random forests support vector machine respectively diﬃcult compare results g-mean score among things diﬀerences data pre-processing training/testing data splits classiﬁcation thresholds. credit card defaults data contained features consisted cases defaults. replaced categorical features separate features category. three sampling techniques case resulted similar scores relatively lower imbalance among classes. reported accuracy using neural network classiﬁer data set. possible compare result result lack adequate information training testing processes. lwns weights conventional sense excitatory inhibitory connections unit strength. approximate continuous function accuracy. require modest storage multiplication-free forward pass rendering suitable deployment inexpensive hardware. sparse weight matrices loosen coupling among layers making tolerant failure individual neurons. learned information distributed weights. picture less fuzzy localized nature computation much obvious presence large number zero-valued weights. image processing scale well increases resolution images cwn’s fully-connected structure. hand scales much better sparsity weight matrices. small magnitude weights result smooth mappings small number non-zero weights result generalization error learning process automatically drops insigniﬁcant inputs unnecessary weights unneeded hidden-neurons. process relatively complex slow results networks almost accurate cwns much lower information complexity. conjectured accuracies matched using bigger networks. understood considering limited number angles neuron draw classiﬁcation boundaries opposed neuron draw boundaries arbitrary angle. superposition neurons however approximate arbitrary boundaries added complexity justify minuscule improvement approximation accuracy. time neurons unrestricted thresholds. exploring magnitude thresholds restricted arbitrary value e.g. lead eﬃcient storage. current form training process applies training operation every weight irrespective value weight. trying training process made eﬃcient sometimes ignoring weights deviated zero value several epochs.", "year": 2017}