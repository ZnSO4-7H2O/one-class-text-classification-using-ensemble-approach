{"title": "Modeling The Intensity Function Of Point Process Via Recurrent Neural  Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Event sequence, asynchronously generated with random timestamp, is ubiquitous among applications. The precise and arbitrary timestamp can carry important clues about the underlying dynamics, and has lent the event data fundamentally different from the time-series whereby series is indexed with fixed and equal time interval. One expressive mathematical tool for modeling event is point process. The intensity functions of many point processes involve two components: the background and the effect by the history. Due to its inherent spontaneousness, the background can be treated as a time series while the other need to handle the history events. In this paper, we model the background by a Recurrent Neural Network (RNN) with its units aligned with time series indexes while the history effect is modeled by another RNN whose units are aligned with asynchronous events to capture the long-range dynamics. The whole model with event type and timestamp prediction output layers can be trained end-to-end. Our approach takes an RNN perspective to point process, and models its background and history effect. For utility, our method allows a black-box treatment for modeling the intensity which is often a pre-defined parametric form in point processes. Meanwhile end-to-end training opens the venue for reusing existing rich techniques in deep network for point process modeling. We apply our model to the predictive maintenance problem using a log dataset by more than 1000 ATMs from a global bank headquartered in North America.", "text": "event sequence asynchronously generated random timestamp ubiquitous among applications. precise arbitrary timestamp carry important clues underlying dynamics lent event data fundamentally different time-series whereby series indexed ﬁxed equal time interval. expressive mathematical tool modeling event point process. intensity functions many point processes involve components background effect history. inherent spontaneousness background treated time series need handle history events. paper model background recurrent neural network units aligned time series indexes history effect modeled another whose units aligned asynchronous events capture long-range dynamics. whole model event type timestamp prediction output layers trained end-to-end. approach takes perspective point process models background history effect. utility method allows black-box treatment modeling intensity often pre-deﬁned parametric form point processes. meanwhile end-to-end training opens venue reusing existing rich techniques deep network point process modeling. apply model predictive maintenance problem using dataset atms global bank headquartered north america. event sequence becoming increasingly available variety applications e-commerce transactions social network activities conﬂicts equipment failures etc. event data carry rich information event attribute also timestamp ti}n indicating event occurs. treated random variable event stochastically generated asynchronous manner timestamp makes correspondence author junchi yan. research partially supported national research development program china nsfc stcsm china postdoctoral science foundation funded project program copyright association advancement artiﬁcial intelligence rights reserved. event sequence fundamentally different time series equal ﬁxed time interval whereby time point serves role index {yt}t major line research devoted study event sequence especially exploring timestamp information model underlying dynamics system whereby point process powerful compact framework direction. recently many machine learning based models scalable point process modeling. attribute progressions direction part smart mathematical reformulations optimization techniques e.g. well novel parametric forms conditional intensity function carefully designed researchers’ prior knowledge capture character dataset study. however major limitation parametric forms point process specialized restricted expression capability arbitrary distributed event data trends oversimpliﬁed even infeasible capturing problem complexity real applications. moreover runs risk model underﬁtting misjudgement model choice. recent works e.g. start turn non-parametric form structure point process method hawkes process formulationwhile formulation runs risk model mischoice. paper view conditional intensity point process nonlinear mapping predicted transient occurrence intensity events different types model input information event participators event proﬁle system history. nonlinear mapping expected complex ﬂexible enough model various characters real event data application utility. fact deep learning models convolutional neural networks recurrent neural networks attracted wide attention recent vision speech language communities many dominated competing results perceptual benchmark tasks e.g. particular turn rnns terpret instantiate conditional intensity function fused time series event sequence rnns. opens room connecting neural network techniques traditional point process emphasizes speciﬁc model driven domain knowledge. importantly introduction full treatment lessen efforts design parametric point process model complex learning algorithms often call special tricks prohibiting wide practitioners. contrast neural networks speciﬁcally becoming off-the-shelf tools getting widely used recently. model simple general end-to-end trained. target predictive maintenance problem. data global bank headquartered north america consisting decades thousands event logs large number automated teller machines stateof-the-art performance failure type timestamp prediction corroborates suitability real-world applications. view related concepts work section mainly focused recurrent neural networks applications time series sequences data respectively. give point view existing point process methods connection rnns. observations indeed motivate work paper. recurrent neural network building blocks model recurrent neural networks modern variant long short-term memory units rnns dynamical systems whose next state output depend present network state input general models feed-forward networks. rnns long explored perceptual applications many decades however difﬁcult train rnns learn long-range dynamics perhaps part vanishing exploding gradients problem. lstms provide solution incorporating memory units allow network learn forget previous hidden states update hidden states given information. recently rnns lstms successfully applied large-scale vision speech language problems. rnns series data application perspective view rnns works scenarios particularly considered paper rnns synchronized series evenly spaced interval e.g. time series indexed sequence pure order information e.g. language; asynchronous sequence random timestamp e.g. event data. synchronized series rnns long time natural tool standard time series modeling prediction whereby indexed series data point input rnn. broader sense video frames also treated time series widely used recent visual analytics works speech idea highlights model interprets conditional intensity function point process nonlinear mapping synergetically established composite neural network rnns building blocks. illustrated fig. time series event sequence distinct time series suitable carry synchronously regularly updated constant proﬁle features event sequence compactly catch event driven abrupt information affect condition intensity function longer time period. specifically highlights paper ﬁrst make observation many conditional intensity functions viewed integration effects spontaneous background component inherently affected internal attributes individual event type; effects history events. meanwhile information real world also covered continuously updated features like temperature asynchronous event data clinical records failures. motivates devise general approach. whose units aligned time points time series units whose units aligned events. time series timely track spontaneous background event sequence used efﬁciently capture long-range dependency history arbitrary time intervals. allows arbitrary dynamics point process otherwise difﬁcult often impossible speciﬁed parameterized model certain assumptions. contrast event sequence timestamp occurrence asynchronously randomly distributed continuous time space another typical input type rnns differentiation ﬁrst scenario timestamp time duration events taken input rnns. event dependency effectively encoded. point process point process principled framework modeling event data dynamics point process well captured conditional intensity function whose deﬁnition brieﬂy reviewed here short time window represents rate occurrence event conditioned history ti|ti e|ht) expectation number events happened interval given historical observations conditional intensity function played central role point processes many popular processes vary parameterized. poisson process homogeneous poisson process simple form intensity function poisson process time-varying generalization assumed independent history. reinforced poisson processes model captures ‘rich-get-richer’ mechanism characterized compact intensity function recently used popularity prediction hawkes process hawkes process received wide attention recently social network analysis viral diffusion criminology etc. explicitly uses triggering term model excitation effect history events originally motivated analyze earthquake aftershocks. predictive maintenance predictive maintenance sound testbed model refers practice involves equipment risk prediction allow proactive scheduling corrective maintenance. early identiﬁcation potential concerns helps deploy limited resources cost effectively reduce operations costs model poisson process reinforced poisson process ti<t hawkes process ti<t reactive point process ti<t self-correcting process ti<t noteδ dirac function time-decaying kernel maximize equipment uptime predictive maintenance adopted wide variety applications inspection data center electrical grid management. practical importance different scenarios relative rich event data modeling target model real-world dataset automated teller machines global bank headquartered north america. network structure end-to-end learning taking sequence {x}t input generates hidden states {h}t outputs sequence speciﬁcally implement long short term memory popularity well-known capability efﬁcient longrange dependency learning. fact variant e.g. gated recurrent units also alternative choice. reiterate formulation lstm consider types input continuously evenly distributed time-series data e.g. temperature; event data whose occurrence time interval random. network comprised rnns using evenly spaced time series {yt}t model background intensity events occurrence event sequence ti}n capture long-range event dependency. result have figure network trained end-to-end. time series event sequence rnns connected embedding mapping layer fuses information lstms. three prediction layers used output predicted main type subtype events associated timestamp. cross-entropy time penalty loss square loss respectively used event type timestamp prediction. event type main types ‘ticket’ ‘error’ sep. mar. statistics presented table moreover ‘error’ divided subtypes regarding component error occurs printer cash dispenser module internet data center communication part printer monitor miscellaneous e.g. card module following importance weighting methodology skewed data model training weight parameters main-type subtype inverse sample number ratio type total size samples order weight classes fewer training samples. loss independent main-type subtype prediction shown fig. weight parameter zero respectively. adopt rmsprop gradients shown work well training deep networks learn parameters. failure prediction predictive atms maintenance typical example event based point process modeling. prior knowledge dynamics complex system task involve arbitrarily working schedules heterogeneous conditions. takes much cost even impractical devise specialized models. problem real data description maintenance support services device fails equipment owner raises maintenance service ticket technician assigned repair failure. fact history relevant proﬁle information equipment indicative signals coming failures. studied dataset comprised event logs involving error reporting failure tickets originally collected large number atms owned anonymous global bank headquartered north america. bank also customer technical support service department fortune company. number training samples indexed history information. underlying rationale third term encourage correct classiﬁcation coming event type also reinforce corresponding timestamp event shall close ground truth. adopt gaussian penalty function ﬁxed time series extract features including inventory information models location etc; event statistics including tickets events maintenance records errors system log. occurrence frequencies used features. concatenation categories features serves features sub-window i.e. time series point. event sequence event type time interval events. model setting single layer lstm size sigmoid gate activations tanh activation hidden representation. embedding layer fully connected uses tanh activation outputs dimensional vector. one-hot embedding used event type representation. large number types embedding representation compact efﬁcient. time series length sub-window days number sub-window observation length days time series. event-dependency length event sequence arbitrarily long. take also test degraded versions model follows time series input event sequence removed. note design spirit similar many lstm models used video analytics whereby frame sequence treated time series input lstms. event whose input time series removed; intensity fused shown fig.. three methods output layer directly ﬁne-grained subtype events hierarchical structure shown left part fig.) dark green. also term three ‘hierarchical’ versions whose hierarchical prediction layers fig. used time series hrnn event hrnn intensity hrnn. addition compare three major peer methods. logistic model input concatenation feature vectors active time series sub-windows rmtpp hawkes process train model event sequences associated information. fact rmtpp process event data similar input information event rnn. recurrent marked temporal point processes uses neural network model event dependency ﬂexibly. method sample transient time series features event happens partially parametric form base intensity. hawkes process enable multi-type event prediction multi-dimensional hawkes process. similar also sparsity regularization term mutual infection matrix lowrank assumption removed subtypes. evaluation metrics several popular prediction metrics performance evaluation. coming event type prediction adopt precision recall score confusion matrix main types well confusion matrix subtypes ‘error’. note metrics computed type averaged types. event time prediction mean absolute error measures absolute difference predicted time point actual one. settings similar evaluate type timestamp prediction jointly devise strict metrics. type prediction narrow test samples whose timestamp prediction error mae< days compute score+. timestamp recompute mae+ samples whose coming event correctly predicted. averaged performance table shows averaged performance among various types events. shown fig. test architectures event type prediction layer i.e. hierarchical predictor independent predictors main type includes ‘ticket’ ‘error’ subtype include ‘ticket’ subtypes ‘error’ describe earlier paper. confusion matrix confusion matrix subtypes ‘error’ event well main types ‘ticket’ ‘error’ shown fig. various methods. make observations analysis based results shown main-type architecture directly predicts main types outperforms hierarchical different settings input well varying evaluation metrics. explained loss function focuses main-type misclassiﬁcation only. subtype prediction hierarchical layer performs better since fuses output main-type prediction layer embedding layer shown fig.. surprisingly event type timestamp prediction main approach i.e. intensity fuses rnns outperforms counterparts time series event sequence notable margin. event also often performs better time series counterpart. suggests least studied dataset history event effects important future event occurrence. simple general learned end-to-end standard backpropagation opens possibilities borrowing advances neural network learning area point process modeling applications. representative study paper clearly suggests high potential real-world problems even domain knowledge problem hand. contrast existing point process models assumption dynamics often need speciﬁed beforehand. table ablation test method peer methods i.e. multi-dimensional hawkes process recurrent hawkes process logistic classiﬁcation regression numbers averaged types. time series event sequence intensity hawkes process logistic prediction rmtpp time series event sequence intensity hawkes process logistic prediction rmtpp time series event sequence intensity hawkes process logistic prediction rmtpp time series event sequence intensity hawkes process logistic prediction rmtpp time series event sequence intensity hawkes process logistic prediction rmtpp time series event sequence intensity hawkes process logistic prediction rmtpp diction task whereby logistic classiﬁcation model performs better. however challenging tasks i.e. subtype prediction event timestamp prediction method signiﬁcantly outperforms especially subtype prediction task. interestingly point process based models obtain better results task suggests point process models promising compared classical classiﬁcation models. indeed methodology provides endto-end learning mechanism without pre-assumption modeling point process. empirical results realworld tasks suggest efﬁcacy approach. fig. conclude position model development modeling intensity function point process. fact hawkes process uses full explicit parametric model rmtpp misses dense time series features model time-varying base intensity assumes partially parametric form make step full implicit mapping model. model", "year": 2017}