{"title": "Learning Abstract Classes using Deep Learning", "tag": ["cs.CV", "cs.AI"], "abstract": "Humans are generally good at learning abstract concepts about objects and scenes (e.g.\\ spatial orientation, relative sizes, etc.). Over the last years convolutional neural networks have achieved almost human performance in recognizing concrete classes (i.e.\\ specific object categories). This paper tests the performance of a current CNN (GoogLeNet) on the task of differentiating between abstract classes which are trivially differentiable for humans. We trained and tested the CNN on the two abstract classes of horizontal and vertical orientation and determined how well the network is able to transfer the learned classes to other, previously unseen objects.", "text": "convolutional neural networks consist multiple layers nodes also called neurons. important layer type convolutional layer networks obtain name. convolutional layer responses nodes depend convolution region input image kernel. additional layers introduce non-linearities rectiﬁcation pooling etc. goal training lies optimizing network weights using image-label pairs best reconstruct correct label given image. testing network confronted novel images expected generate correct label. network trained gradient descent. gradient calculated backpropagation labeling errors. general idea cnns automatically learn features needed distinguish classes generate increasingly abstract features information moves layers. since cnns popular moment perceived parts computer vision community obtaining human like performance wanted test applicability visual tasks slightly outside mainstream still trivially solvable humans. chose learn simple abstract classes using standard assume perform better tasks other possibly much simpler methods want gain insights cnns perform tasks solved trivially humans. mainly give insight amount training images needed well classiﬁer generalizes previously unseen shapes representing abstract concepts. classes used training testing convolutional neural networks concrete notable exception work g¨ul¸cehre trained recognize whether multiple presented shapes same. essence training abstract classes. problems presented bongard inspired research presented paper. foundalis gives good introduction problems presents system intended solve computationally. used methods deep learning though. previously fleuret compared human performance classical machine learning methods classiﬁcation tasks. classes used similar spirit bongard problems problems similar nature also humans generally good learning abstract concepts objects scenes last years convolutional neural networks achieved almost human performance recognizing concrete classes paper tests performance current task diﬀerentiating abstract classes trivially diﬀerentiable humans. trained tested abstract classes horizontal vertical orientation determined well network able transfer learned classes other previously unseen objects. deep learning methods gained interest machine learning computer vision research community last years methods provide exceptional performance classiﬁcation tasks. especially convolutional neural networks ﬁrst introduced lecun become popular object classiﬁcation. cnns widely used deep krizhevsky outperformed state methods ilsvrc wide margin decided classes horizontal vertical classiﬁcation experiments since abstract visually unambiguous easy diﬀerentiate humans easily transfer diﬀerent shapes. goal learn classiﬁer distinguish horizontally vertically oriented structures transfer knowledge previously unseen objects shapes. ﬁrst experiments exploring representational capabilities current deep learning systems regarding abstract classes. paper used convolutional neural network googlenet presented szegedy number categories ilsvrc. slightly adapted implementation provided caﬀe deep learning framework task experiments started initial learning rate adagrad adapt learning rate time. trained iterations. point loss small meaningful improvement possible. trained times sets diﬀerent randomly generated images judge mean accuracy well variance diﬀerent numbers training images. graphs paper show mean accuracy blue dots measurements fall within shaded area test contained images class. reported accuracy proportion correctly classiﬁed images test set. test well googlenet generalize abstract classes diﬀerent shapes diﬀerent renderings following procedure train googlenet without pre-training dataset consisting classes horizontal vertical. test performance test containing classes represented diﬀerent shapes rendered diﬀerently randomly generated vertically horizontally oriented ﬁlled rectangles white background training network. tested randomly generated vertically horizontally oriented ﬁlled ellipses figure shows accuracy iterations relation number training images used class. able learn generalize classes less perfectly training images class. surprise even images class result mean accuracy expected variance higher fewer training images. assume images better representations classes others. since images randomly generated quality whole dataset vary fewer images. test sensitive network regarding diﬀerent representations shape trained network horizontal vertical classes outlines rectangles used ﬁlled rectangles figure testing. able learn abstract concepts reason adding another shape outline training improve accuracy. force learn abstract concept. test hypothesis added horizontally vertically oriented ellipse outlines training images tested accuracy ﬁlled rectangles. figure addition ellipse outlines improved performance ﬁlled rectangle test slightly. figure shows network trained random shape outlines even performs better detecting orientation ﬁlled rectangles network trained similar data sets ﬁnal experiment looked well random shapes generalize other textured random shapes. used likely diﬃcult test texture orientation orthogonal orientation shape. figure shows examples class. results indicate training examples lead extreme variance results. performance network varies perfect accuracy pure guessing. addition nothing training phase indicates well network perform test showed state convolutional neural network able learn abstract classes transfer information other previously unseen shapes. also apparent current networks sensitive used training testing data regarding well transfer knowledge works. russakovsky deng krause satheesh huang karpathy khosla bernstein berg fei-fei. imagenet large scale visual recognition challenge. international journal computer vision pages april gles testing ﬁlled ellipses comparison training rectangle outlines testing ﬁlled rectangles. unclear experiments googlenet perform much better ﬁrst task second one. humans general much less aﬀected representational diﬀerences perform well highly variable data sets seen problem sets used measuring non-verbal abstract reasoning bongard problems. course humans well animals already pre-training encountering tasks. pre-training form previously learned concepts well optimization occurred evolution manifests organization brain. missing information. think therefore pre-training networks right data paramount increase performance abstract tasks. results g¨ul¸cehre also inception module small layer network repeated many times. used main building block googlenet. figure shows graphical representation inception module. googlenet consist stack nine inception modules. three points softmax used calculate loss network. nine inception modules third sixth inception module. reasoning behind using middle layers calculate error function promote better discrimination lower layers calculate better gradient signal. needed since dealing deep network layers. refer figure paper szegedy detailed description layer structure googlenet.", "year": 2016}