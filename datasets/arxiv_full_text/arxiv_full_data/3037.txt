{"title": "Robust Emotion Recognition from Low Quality and Low Bit Rate Video: A  Deep Learning Approach", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Emotion recognition from facial expressions is tremendously useful, especially when coupled with smart devices and wireless multimedia applications. However, the inadequate network bandwidth often limits the spatial resolution of the transmitted video, which will heavily degrade the recognition reliability. We develop a novel framework to achieve robust emotion recognition from low bit rate video. While video frames are downsampled at the encoder side, the decoder is embedded with a deep network model for joint super-resolution (SR) and recognition. Notably, we propose a novel max-mix training strategy, leading to a single \"One-for-All\" model that is remarkably robust to a vast range of downsampling factors. That makes our framework well adapted for the varied bandwidths in real transmission scenarios, without hampering scalability or efficiency. The proposed framework is evaluated on the AVEC 2016 benchmark, and demonstrates significantly improved stand-alone recognition performance, as well as rate-distortion (R-D) performance, than either directly recognizing from LR frames, or separating SR and recognition.", "text": "department computer science electrical engineering university missouri kansas city snap department industrial systems engineering university washington abstract—emotion recognition facial expressions tremendously useful especially coupled smart devices wireless multimedia applications. however inadequate network bandwidth often limits spatial resolution transmitted video heavily degrade recognition reliability. develop novel framework achieve robust emotion recognition rate video. video frames downsampled encoder side decoder embedded deep network model joint super-resolution recognition. notably propose novel max-mix training strategy leading single one-for-all model remarkably robust vast range downsampling factors. makes framework well adapted varied bandwidths real transmission scenarios without hampering scalability efﬁciency. proposed framework evaluated avec benchmark demonstrates signiﬁcantly improved stand-alone recognition performance well ratedistortion performance either directly recognizing frames separating recognition. emotion recognition facial expressions mostly relies data collected highly controlled environment high resolution frontal faces. coupled widespread smart wearable devices emotion recognition techniques demonstrated tremendous application value tracking human mental status detecting mental illness less obtrusive traditional mental healthcare monitoring approaches however ever-growing wireless multimedia applications available network bandwidth often inadequate stream video. transmit video contents limited bandwidth networks encoder often compromises spatial resolution video frames reducing rates adaptive downsampling video resolution prior compression yields improved performance coding original full-size video expense degrading quality. particular facial images decompression constitutes severe challenge facial expression analysis figure displays figure face image detected frame avec dataset downsampled versions different downsampling factors note proposed approach substantially improve emotion recognition performance paper presents novel framework achieve robust reliable emotion recognition keeping communication load low. encoder side video frames adaptively downsampled compression transmission order meet bandwidth requirements. core innovation proposed framework jointly optimized scheme super resolution recognition models based deep learning decoding. important ﬁnding develop novel max-mix training strategy obtain single deep model veriﬁed robust vast range downsampling factors. onefor-all model well adapted varied bandwidths practical transmission. model demonstrates signiﬁcantly superior recognition rate-distortion performance either directly recognizing frames two-stage pipeline restoration recognition separate. finally point directions towards framework improved. related work emotion recognition facial expressions recognizing human emotion depend upon gesture pose facial expression speech behaviors even brain signals paper mainly discuss emotion recognition videos record facial expressions. seminal work recognized ﬁne-grained changes facial expression proposing facial action coding system large portion research efforts tried formulate emotion recognition multi-class classiﬁcation problem. famous categorization system scheme universal atom emotions anger disgust fear happiness sadness surprise. many feature engineering feature learning approaches proposed six-emotion classiﬁcation problem e.g. regression formulation another promising alternative model inﬁnite space possible emotions person’s emotions found described low-dimensional representation. simple common choice decompose emotion orthogonal real-valued dimensions arousal valence arousal measures engaged apathetic subject appears valence measures positive negative subject appears. arousal-valence representation describes larger continuous space emotions six-emotion scheme roughly partitions emotion space regions. moreover regression formulation allows time-continuous real-valued outputs realistic modeling temporal emotion dynamics video. several benchmarks constructed task automatic emotion recognition extended cohn-kanade dataset facial expression database following many recent works develop emotion recognition model based avec dataset whose data originally recola corpus multimodal signals including audio video physiological signals synchronously recorded subjects. continuous-time continuous-valued ratings arousal valence given human raters. paper focus video data only choose valence value regression goal simplicity stateof-the-arts dataset). proposed method integrate data modalities easily extended predict arousal valence values jointly. rate video transmission adaptive downsampling variety computer vision tasks processing server needs communicate remotely deployed visual sensors communication costs prohibitive especially applications like city-scale visual surveillance networks thousands high resolution cameras connected. reduce communication cost distributed vision system important research issue. reduce operating cost rate upscaling/super-resolution visually beat video compressed directly using standard codecs number bits insufﬁcient rates addition video downsampling also common practice pre-processing high-level computer vision tasks detection tracking order meet computational complexity and/or latency requirements especially mobile devices limited processing power decoder side techniques often adopted post-processing enhancing display quality ﬁxed downsampling ratio encoding known models obtained various example-based training approaches however practical bandwidth might varied network load congestion bottleneck situations. pointed achieve overall optimal performance downsampling ratio encoder adaptively determined. distortions caused downsampling reduces number pixels transmitted coding introduces quantization noises pixels transmitted could balanced. result post-processing decoder side effectively cope varied downsampling factors. straightforward expensive solution utilize ensemble models trained dedicatedly downsampling factor. cost-effective option seek single onefor-all model whose performance keeps robust useful range resolutions. best knowledge viability examined yet. low-resolution visual recognition empirical studies face recognition proved minimum face resolution required stand-alone recognition algorithms whose performance would much degraded applied even lower resolutions emotion recognition literature existing methods assumed availability frontal faces. ﬁrst investigated effects different image resolutions facial expression analysis. author concluded performance difference negligible head region resolution higher recognition turned growingly unreliable head region resolution lower thus desirable obtain robust features images low-intensity expressions dealing subjects traditional twostage pipeline tried ﬁrst apply algorithms perform recognition tasks. recently performance noticeably improved deep network models however recovered images inevitably over-smoothened details. importantly straightforward approach yields sub-optimal performance artifacts introduced reconstruction process undermine ﬁnal recognition. presented close-theloop approach image restoration recognition based assumption degraded image correctly restored also good identiﬁability. advanced mostly inherits cnn+d structure target video frames. convolutional layers consisting ﬁlters respectively size ﬁrst layers followed pooling third layer followed quadrant pooling. followed fully-connected layer hidden units regularized dropout probability relu neuron adopted all. linear regression layer estimates valence values mean squared error loss function. pointed training cnn-based recognition model images usually robust prone overﬁtting severe information loss. hand trained images also witness degraded performance tested images domain mismatch. main intuition regularize enhance feature extraction pre-training ﬁrst several convolutional layers using sub-model reconstructs images counterparts. -layer fully convolutional network ﬁrst constructed figure ﬁrst three layers conﬁgured ﬁrst three layers target fourth layer reconstructs input image output feature maps third layer. sr-fcn trained unsupervised reconstruct frames inputs loss well. note different target regresses frames valence values. that ﬁrst three layers exported initialize ﬁrst layers target cnn. starting sr-based partial initialization jointly tuned emotion recognition task end. max-mix training one-for-all model almost data-driven approaches well latest low-resolution recognition works assume identical downsampling factor training testing. model dedicated coping downsampling factor. desirable train one-for-all model since robust vast range downsampling factors caused varied transmission bandwidths without incurring scalability efﬁciency issue. given range possible downsampling factors propose max-mix training ﬁrst pre-training sr-fcn lr-hr pairs generated maximum downsammethodology using deep network trained observed possibility robust object recognition even region interests smaller pixels. however remains open issue much performance degradation remedied emotion recognition. pipeline proposed framework illustrated figure assume face detection cropping accomplished encoder side pre-processing. cropped faces downsampled compressed transmitted decoder side decoding joint emotion recognition module simultaneously enhances spatial resolution predicts per-frame valence value using end-to-end deep network detailed next section. system outputs time series predicted valence values. discuss adaptively control downsampling factors communication needs well studied previous video coding wireless communication literature instead make decoder robust wide range varied downsampling factors encoder might adopt. joint emotion recognition pling factor followed ﬁne-tuning model mixture frames generated frames using range downsampling factors. veriﬁed experiments resulting able achieve even better performance dedicatedly trained models speciﬁc downsampling factor. favor lower rmse well higher ccc. note reliable measure among three thus used choose avec competition winners. performance evaluation analysis lowresolution emotion recognition avec video data ﬁrst convert color frames gray-scale crop face video frame using given bounding box. face regions normalized pixels treated subjects downsampled compressed transmitted. generate frames using range downsampling factors range intentionally vast causes mild degradations leads facial regions whose expressions unlikely identiﬁed even human viewers. cnns trained using stochastic gradient descent batch size momentum weight decay apply mean subtraction contrast normalization prior passing face image cnn. train sr-scn iterations using constant learning rate used ﬁne-tune target learning rate used ﬁrst three pre-trained layers remaining layers initialized randomly trained learning rate learning rates divided observe validation performance stops improve. avec development sequences testing set. three metrics measured emotion recognition performance root mean square error pearson correlation coefﬁcient concordance correlation coefﬁcient combines rmse mean compared time series. good recognition result likely experiments mixing frames lead optimal performance. conjecture thatbad values lead un-recognizable samples perturb training. instead reasonable range samples ﬁne-tuning. default obtain joint-oa model veriﬁed better ﬁne-tuning single non-joint-s sr-fcn ﬁrst trained up-scale frames separate fully-connected neural network trained regress predict valence values up-scaled images. emotion recognition modules jointly tuned. joint-s joint emotion recognition model described section trained dedicatedly speciﬁc downsampling factor joint-oa joint emotion recognition model training max-mix strategy. fair comparison carefully ensure models amount parameters. table presents overall rmse comparison results avec development comparison lr-s certiﬁes notable impact resolution emotion recognition. non-joint methods achieve best almost cases however rmse results display little consistency cc/cccs implying rmse reliable measure. little improvement seems attainable baselines since recognizable information almost lost resolutions results fairly consistent recognition beneﬁts joint training cases. more joint-oa model consistently outperforms joints largest margins surprise notice joint-oa results even slightly surpass terms ccc. questions arise naturally joint training help; distracted joint-oa model beat dedicated joint-s models? question hallucinated details help discover subtle features otherwise prone overlooked frames however restoration-driven pre-training non-selectively enhances visual details also include artifacts hamper recognition. joint tuning step introduces extra information reinforce learning task-related features suppressing unrelated components. question conjecture pre-training sr-fcn maximum helps lowlevel ﬁlters capture robust mappings boosting feature enhancement. further mixture ﬁnetuning correspond re-scaling training data popular type data augmentation classiﬁcation tasks helps learn scale-invariant features. bandwidth constrained applications achieving robust facial expression recognition rate video attractive many security surveillance applications. problem coupled resolution sensor problem peculiar challenges. addition loss pixels resolution limitations video coding also introduce quantization errors affect emotion recognition performance. indeed compression visual features visual recognition active research topic many interesting results point feature compressions experiment encode actively downsampled testing video different quality-rate levels mimic real world transmissions video usually coded subject rate constraint. decoded videos joint recognition models calculate results. observations figure mostly consistent uncompressed case showing models’ robustness coding qualities. joint-oa gains advantages larger quantization parameters joint-oa outperforms methods rates. rate-distortion operating range good excellent visual quality loss recognition performance negligible. coding-introduced distortion becomes pronounced larger recognition starts suffer. starts saturate smaller operate approximately bits pixel loss coding efﬁciency compared ﬁxed overhead video coding headers structures shared among pixels. efﬁciency decreases number pixels reduced. notice pixels recognition algorithm -bit. compression indeed effective active downsampling conserving bandwidth. summary actively downsampling reduces number pixels transmitted coding larger enforces heavier quantization pixels remaining. contribute saving bandwidth exists interesting tradeoff in-between. conclusion discussion paper presents novel framework robust emotion recognition rate video demonstrates promising performance well strong robustness pixel reduction pixel quantization. apparent room performance improvement. system perspective expect incorporate building blocks joint optimization scheme make pipeline figure end-to-end. model perspective utilized temporal information videobased recognition. previous work exploited recurrent neural networks capture temporal coherence obtained additional performance gains. since adjusting temporal resolution also common means reduce video rates future work also extend adaptive temporal downsampling followed temporal-spatial joint video recognition. finally observe cc/ccc evidently better evaluation metrics rmse noteworthy option train emotion recognition model cc/ccc-based loss functions rather current loss. acknowledgments bowen cheng ding thomas huang’s research works supported part army research ofﬁce grant wnf---. authors sincerely acknowledge valuable efforts avec challenge organizers authors would also like acknowledge helpful discussions pooya khorrami thomas paine.", "year": 2017}