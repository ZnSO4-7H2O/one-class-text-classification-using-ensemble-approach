{"title": "Flexible Deep Neural Network Processing", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "The recent success of Deep Neural Networks (DNNs) has drastically improved the state of the art for many application domains. While achieving high accuracy performance, deploying state-of-the-art DNNs is a challenge since they typically require billions of expensive arithmetic computations. In addition, DNNs are typically deployed in ensemble to boost accuracy performance, which further exacerbates the system requirements. This computational overhead is an issue for many platforms, e.g. data centers and embedded systems, with tight latency and energy budgets. In this article, we introduce flexible DNNs ensemble processing technique, which achieves large reduction in average inference latency while incurring small to negligible accuracy drop. Our technique is flexible in that it allows for dynamic adaptation between quality of results (QoR) and execution runtime. We demonstrate the effectiveness of the technique on AlexNet and ResNet-50 using the ImageNet dataset. This technique can also easily handle other types of networks.", "text": "recent success deep neural networks drastically improved state many application domains. achieving high accuracy performance deploying state-of-the-art dnns challenge since typically require billions expensive arithmetic computations. addition dnns typically deployed ensemble boost accuracy performance exacerbates system requirements. computational overhead issue many platforms e.g. data centers embedded systems tight latency energy budgets. article introduce ﬂexible dnns ensemble processing technique achieves large reduction average inference latency incurring small negligible accuracy drop. technique ﬂexible allows dynamic adaptation quality results execution runtime. demonstrate effectiveness technique alexnet resnet- using imagenet dataset. technique also easily handle types networks. increased availability computational power recent years deep neural networks shown generate state-of-the-art performance many complex machine learning computer vision problems. performance leap widespread dnns across industries data centers embedded devices explosive increase academic research area. currently dnns power major image voice recognition systems including apple google many applications emerging. industries health care ﬁnance also started adopting technology. recently dnns shown outperform radiologists spotting pneumonia industry-wide adoption dnns proliferated deployment wide range systems battery-operated embedded platforms massive data centers presents signiﬁcant runtime challenges. addition dnns dramatically improve accuracy performance many machine learning problems developing network architectures learning methods improves state-of-the-art tedious process. reason popular method boost inference accuracy performance apply ensemble learning input evaluated single model using multiple independently trained dnns. exacerbates deployment challenges time-pressured systems user-oriented services require strict latency enforcement systems constrained computational resources autonomous vehicles embedded platforms low-latency energy inference desirable runtime performance lifetime systems. foundational works targeting general resource-constrained deployment machine learning models introduce ﬂexible computation methodologies partial results accepted exchange reduction allocations costly resources time memory since inference output time-dependent utility waiting increasingly accurate results could negative impact. costs delayed actions increase computations outweighs beneﬁts accurate inference. adopting ﬂexible computing approach large computation latency reductions made incurring small drop quality results constraint ﬂexibility overall utility systems must decrease. deployment ensembles additional model evaluation linearly increases overlatency required computations systems. platforms real-time delay/energy constraints processing every input using models ensemble always efﬁcient even possible. address issue propose ﬂexible ensemble processing form ﬂexible computation input data evaluated using additional model based metareasoner. metareasoner computes likelihood increase utility additional model evaluations decides whether continue output current results. ﬂexible computation model achieve large reduction average runtime input still maintaining beneﬁts ensemble learning. contribution introduce ﬂexible ensemble processing methodology offering large runtime energy reduction small inference accuracy drop compared normal ensemble execution. demonstrate technique well-known dnns namely alexnet resnet-. many machine learning problems order reduce generalization error simple effective technique often deploy ensemble models. problems small models datasets techniques bootstrap aggregating often used. hand larger models dnns normally reach large variety solutions simply using different initializations takes away need special data partitioning reason dnns ensembles formed using single model architecture independently training different initializations. zheng demonstrates practical application ensembles dnns show better prediction capability software-reliability single model. effectiveness model ensembles explained dietterich consequence three fundamental reasons statistics computations representations. ensembles dramatically improve many machine learning problems deploying large models incurs longer latency requires massive resource. horvitz rutledge demonstrate examples increases inference latency system resource allocations could nullify additional utility gains extra computations. fact continuing wait higher decrease overall utility systems. addressing problem horvitz introduces ﬂexible computation methodologies knowledge model utility used control trade offs additional computations acting partial results. work propose methodology enable ﬂexible ensemble processing order minimize overall systems latency trading small inference accuracy loss. deploying ensemble dnns proven simple reliable method boost inference accuracy. independently training multiple dnns architecture input evaluated using networks. ﬁnal output model computed combining outputs dnns ensemble. combination process weighted unweighted voting. study unweighted outputs averaging. instance suppose networks ensemble input produce output logit vectors ﬁnal output vector computed case classiﬁcation predicted class simply maximum element figure inference accuracy versus average runtime input alexnet resnet- ensembles imagenet validation set. data label shows number networks ensemble. runtime results based system nvidia titan gpu. figure shows top- accuracy imagenet validation versus average runtime input alexnet resnet- ensembles different number networks. shown dnns inference accuracy improves ensembles increasing number dnns. based figure improvement seems logarithmic number models ensembles. accuracy saturates ensemble models alexnet continues improve models resnet-. although accuracy improvement diminishes saturates larger ensembles still represent signiﬁcant boost compared single network performance. goal ensemble execution improve overall inference accuracy model. however executing networks ensemble every input incurs high costs valuable resources time memory. shown figure increasing ensemble size linearly increases processing latency logarithmically improves performance accuracy. proposed methodology preserve accuracy improvement signiﬁcantly reducing average latency input. introducing metareasoner determines unnecessary execute additional networks. instance ensemble dnns instead running input dnns ﬁrst process input using dnn. then based metareasoner evaluate using additional dnns necessary. similar fashion horvitz metareasoner reasons probability utility increase additional latency resource allocations. utility thought value additional computations performed. positive value computation increases utility systems. case deﬁne value computation positive situations highly probable current inference output incorrect additional model evaluation change that. decision model introduces processing ﬂexibility every input evaluated using number models ensembles considered optimal. order compute probability positive value computation analyze score margins current inference output. score margin deﬁned absolute difference scores output. instance suppose prepost-softmax output vector score margin deﬁned observed exists strong correlation score margins prediction accuracy observation also observed models shown figure here show histogram plots score margins inference correct wrong based true data labels single dnn. ﬁgure generated using random subset images training data. correlation value computation estimated comparing score margin threshold. figure score margins histograms correct wrong top- inference alexnet. x-axis shows score margin y-axis shows number samples score margin bin. figure illustrates ﬂexible ensemble processing. input evaluated though score margin computed passed metareasoner compared threshold. post-softmax output score margin always range margin higher threshold highly probable current already correct additional model processing likely produce negative value computation. reason processing halted current prediction output ﬁnal inference result. otherwise execute additional model average prediction. process repeated until ﬁnish executing ensemble. since number dnns ensemble execution changes different thresholds ensemble size. empirically choose thresholds based training data minimize latency loss. objective function inference latency relative increase inference error compared normal ensemble execution. determines relative importance error rate increase latency improvement. since score margin range perform grid search ensemble size different threshold values output value minimizes experiments measure deployment runtime ﬂexible ensemble execution using system intel core nvidia titan gpu. setup allows analyze runtime beneﬁt smaller scale systems dnns ensemble executed serially. note saving reported system also observed smaller embedded platforms executed time. accuracy results based imagenet datasets. employ well-known architectures namely alexnet resnet experiments based caffe decision model score margin threshold choices directly affect inference latency versus accuracy trade off. lower threshold values would mean input likely less number dnns results shorter average latency. however would also mean accuracy figure inference accuracy versus average runtime input alexnet resnet- normal ﬂexible ensemble execution. runtime results based system nvidia titan gpu. figure shows inference latency accuracy execution modes normal proposed ﬂexible ensemble executions. alexnet figure shows ensemble dnns achieves accuracy gain compared dnns. reason show results ensemble dnns figure dnns presented ﬂexible ensemble processing retains majority inference accuracy normal ensembles offering large reduction average latency. instance alexnet case ensemble dnns average runtimes normal ﬂexible executions respectively accuracies respectively. close latency improvement inference accuracy drop. using methodology drop traded latency improvement adjusting score margin thresholds. extreme case accuracy drop tolerable score margin thresholds high equivalent normal ensemble execution would result relative accuracy loss. bounded-resource inference long pressing issue machine learning problems. flexible computing introduces alternative inference strategies gracefully traded beneﬁts lower computational costs toward goal presented ﬂexible execution methodology lessens ensemble computation latency overheads still maintaining much inference accuracy. technique allows large degree freedoms inference accuracy versus latency trade readily combined types approximations. addition approach easily extended handle types neural networks kinds machine learning models.", "year": 2018}