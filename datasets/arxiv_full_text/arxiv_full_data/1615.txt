{"title": "Lifted Rule Injection for Relation Embeddings", "tag": ["cs.LG", "cs.AI", "cs.CL"], "abstract": "Methods based on representation learning currently hold the state-of-the-art in many natural language processing and knowledge base inference tasks. Yet, a major challenge is how to efficiently incorporate commonsense knowledge into such models. A recent approach regularizes relation and entity representations by propositionalization of first-order logic rules. However, propositionalization does not scale beyond domains with only few entities and rules. In this paper we present a highly efficient method for incorporating implication rules into distributed representations for automated knowledge base construction. We map entity-tuple embeddings into an approximately Boolean space and encourage a partial ordering over relation embeddings based on implication rules mined from WordNet. Surprisingly, we find that the strong restriction of the entity-tuple embedding space does not hurt the expressiveness of the model and even acts as a regularizer that improves generalization. By incorporating few commonsense rules, we achieve an increase of 2 percentage points mean average precision over a matrix factorization baseline, while observing a negligible increase in runtime.", "text": "verga mccallum although models able learn robust representations large amounts data often lack commonsense knowledge. knowledge rarely explicitly stated texts found resources like ppdb wordnet combining neural methods symbolic commonsense knowledge instance form implication rules focus current research recent approach regularizes entity-tuple relation embeddings ﬁrst-order logic rules. every ﬁrst-order rule propositionalized based observed entity-tuples differentiable loss term added every propositional rule. approach scale beyond entity-tuples rules. example propositionalizing rule isman ismortal would result large number loss terms large database. paper present method incorporate simple rules maintaining computational efﬁciency modeling training facts. achieved minimizing upper bound loss encourages implication relations hold entirely independent number entity pairs. involves representations relations mentioned rules well general rule-independent constraint entity-tuple embedding space. example given above require every component methods based representation learning currently hold state-of-the-art many natural language processing knowledge base inference tasks. major challenge efﬁciently incorporate commonsense knowledge models. recent approach regularizes relation entity representations propositionalization ﬁrst-order logic rules. however propositionalization scale beyond domains entities rules. paper present highly efﬁcient method incorporating implication rules distributed representations automated knowledge base construction. entity-tuple embeddings approximately boolean space encourage partial ordering relation embeddings based implication rules mined wordnet. surprisingly strong restriction entity-tuple embedding space hurt expressiveness model even acts regularizer improves generalization. incorporating commonsense rules achieve increase percentage points mean average precision matrix factorization baseline observing negligible increase runtime. current successful methods automated knowledge base construction tasks heavily rely learned distributed vector representations deals groups random variables ﬁrst-order level. sense approach lifted form rule injection. allows imposing large numbers rules learning distributed representations relations entity-tuples. besides drastically lower computation time important advantage method rockt¨aschel constraints satisﬁed injected rules always hold even unseen inferred facts. method presented deals implications general ﬁrst-order rules rely assumption independence relations hence generally applicable. contributions fourfold develop efﬁcient regularizing relation representations incorporate ﬁrst-order logic implications reveal that expectation mapping entity-tuple embeddings non-negative space hurt instead improves generalization ability model show improvements knowledge base completion task injecting mined commonsense rules wordnet ﬁnally give qualitative analysis results demonstrating implication constraints indeed satisﬁed asymmetric result substantially increased structuring relation embedding space section revisit matrix factorization relation extraction model riedel introduce notation used throughout paper. choose matrix factorization model simplicity base develop implication injection. dimensional latent representation particular relation instance fact combination relation tuple entities engaged relation written write input facts available training. furthermore every entity-tuple represented latent vector model riedel measures compatibility relation entitytuple using product respective vector representations. training representations learned valid facts receive high scores whereas negative ones receive scores. typically negative evidence available training time therefore bayesian personalized ranking objective used. given pair facts objective requires embeddings trained minimizing convex loss function penalizes violations requirement iterating training set. practice positive training fact compared randomly sampled unobserved fact relation. overall loss hence written measures well observed valid facts ranked unobserved facts thus reconstructing ranking training data. henceforth call reconstruction loss make distinction implication loss introduce later. riedel logistic loss denotes sigmoid function. order avoid overﬁtting regularization term embeddings added reconstruction loss. overall objective minimize hence bound others conceivable too. practice rescale hyper-parameter control impact upper bound overloss. call lifted loss longer depends entity-tuples; grounded unit tuples instead. implication thus imposed minimizing lifted loss note minimizing model encouraged satisfy constraint relation embeddings denotes component-wise comparison. fact sufﬁcient condition hold approximately boolean entity tuples order impose implications minimizing lifted loss tuple-embedding space needs restricted rk+. chosen restrict tuple space even required namely hypercube approximately boolean embeddings tuple imposed independently entity-tuples. simplicity abbreviate implications grounded loss formulation implication rule imposed requiring every tuple least compatible relation written terms latent representations therefore becomes true fact high score fact even higher score must also true vice versa. therefore inject implication rule minimizing loss term separate contribution every adding total loss corresponding inequality satisﬁed. order make contribution every tuple loss independent magnitude tuple embedding divide sides inequality implication loss rule written appropriate convex loss function similarly practice summation reduced tuples occur combination training data. still propositionalization terms training facts leads heavy computational cost imposing single implication similar technique introduced rockt¨aschel moreover simpliﬁcation guarantee implication relations would generalize towards inferred facts seen training. lifted loss formulation problems mentioned avoided instead tuple-independent upper bound minimized. bound constructed provided components restricted nonnegative embedding space i.e. rk+. small positive margin ensure gradient disappear inequality actually satisﬁed. experiments. main advantage presented approach earlier methods impose rules grounded computational efﬁciency imposing lifted loss. evaluating gradient implication rule comparable evaluating reconstruction loss pair training facts. typical applications much fewer rules training facts extra computation time needed inject rules therefore negligible. recent research combining rules learned vector representations important developments ﬁeld knowledge base completion. rockt¨aschel rockt¨aschel provided framework jointly maximize probability observed facts propositionalized ﬁrst-order logic rules. wang demonstrated different types rules incorporated using integer linear programming approach. wang cohen learned embeddings facts ﬁrst-order logic rules using matrix factorization. approaches ground rules training data limiting scalability towards large rule sets many entities. argued introduction forms important motivation lifted rule injection model forward work construction suffer limitation. proposed alternative strategy tackle scalability problem reasoning ﬁltered subset grounded facts. proposed path ranking approach capturing long-range interactions between entities extra loss term besides loss models pairwise relations. model differs substantially approach consider tuples instead separate entities inject given rules. creminimizing loss gradients hence computed respect regularization applied components instead choices ensuring restriction possible found approach works better practice also observed unit tuples implication loss grounded form special case approximately boolean embeddings. convex implication loss logistic loss suited imposing implications inequality satisﬁed components need separated further. however would continue happen small reconstruction loss non-zero gradient. desirable effect separates scores positive negative examples. however implication imposed relations almost equivalent according ating partial ordering relation embeddings result injecting implication rules model also capture interactions beyond direct relations. demonstrated injecting rules surface patterns still measuring improvement predictions structured freebase relations. combining logic distributed representations also active ﬁeld research outside automated knowledge base completion. recent advances include work faruqui injected ontological knowledge wordnet word representations. furthermore vendrov proposed enforce partial ordering embeddings space images phrases. method related order embeddings since deﬁne partial ordering relation embeddings. however ensure implications hold entity-tuples also need restriction entitytuple embedding space derive bounds loss. another important contribution recent work proposed framework injecting rules general neural network architectures jointly training actual targets rule-regularized predictions provided teacher network. although quite different ﬁrst sight work could offer model various neural network architectures integrating proposed lifted loss teacher network. paper builds upon previous workshop paper work tested different tuple embedding transformations ad-hoc manner. used approximately boolean representations relations instead entity-tuples strongly reducing model’s degrees freedom. derive model carefully considered mathematical transformation grounded loss. model restricts tuple embedding space whereby relation vectors remain real valued. furthermore previous experiments performed small-scale artiﬁcial datasets whereas test real-world relation extraction benchmark. finally explicitly discuss main differences respect strongly related work rockt¨aschel method general cover wide range ﬁrst-order logic rules whereas discuss implications. lifted scalability proposed model lifted rule injection scales according number implication rules instead number rules times number observed facts every relation present rule. generalizability injected implications hold even facts seen training validity depends order relation imposed relation representations. guaranteed training rules grounded training facts rockt¨aschel training flexibility method trained various loss functions including rank-based loss used riedel possible model rockt¨aschel already leads improved accuracy seen zero-shot learning experiment independence assumption rockt¨aschel implication form ground atoms modeled logical equivalence probability approximated terms elementary probaassumes independence atoms hold practice. approach rely assumption also works cases statistical dependence. example independence assumption hold trivial case relations atoms equivalent whereas model constraints would simply reduce present experimental results. start describing experimental setup hyperparameters. turning injection rules compare model model show restricting tuple embedding space regularization effect rather limiting expressiveness model demonstrate model capable zero-shot learning show injecting high-quality wordnet rules person/company location/containedby person/nationality author/works written person/place birth parent/child person/place death neighborhood/neighborhood person/parents company/founders sports team/league team owner/teams owned team/arena stadium ﬁlm/directed broadcast/area served structure/architect composer/compositions person/religion ﬁlm/produced table weighted mean average precision reimplementation matrix factorization model compared restricting entity-pair space injecting wordnet rules model results riedel denoted r-f. leads improved precision proceed visual illustration relation embeddings without injected rules provide details time efﬁciency lifted rule injection method show correctly captures asymmetry implication rules implemented tensorflow hyperparameters riedel hidden dimensions weight regularization loss. adam optimization initial learning rate mini-batch size embeddings initialized sampling uniformly implication loss throughout experiments. restricted embedding space incorporating external commonsense knowledge relation representations curious much lose restricting entity-tuple space approximately boolean embeddings. evaluate models york times dataset introduced riedel surprisingly expressiveness model suffer strong restriction. table restricting tuple-embedding space seems perform slightly better opposed realvalued tuple-embedding space suggesting restriction regularization effect improves generalization. also provide original results model riedel comparison. different implementation optimization procedure results model identical. inspecting relations sampled dimension embedding space reveals relation space model closely resembles clusters model hypothesize might caused approximately boolean entity-tuple representations model resulting attribute-like entity-tuple vectors capture relation clusters belong zero-shot learning zero-shot learning experiment performed rockt¨aschel leads important ﬁnding injecting implications right-hand sides freebase relations limited training facts available model able infer validity freebase facts relations based rules correlations textual surface patterns. inject hand-picked relations used rockt¨aschel removing freebase training facts. lifted rule injection reaches weighted comparable joint model rockt¨aschel note experiment initialized freebase relations implied rules negative random vectors reason without negative training facts relations components implication loss want values high optimization. figure shows relation extraction performance improves freebase relation training facts added. efﬁctively measures well proposed models matrix factorization propositionalized rule injection model make provided rules correlations textual surface form patnsubj<-represent->dobj appos->member->prep->of->pobj->team->nn nn<-return->prep->to->pobj nsubj<-die->dobj nsubj<-speak->prep->about->pobj appos->champion->poss relation extraction. wordnet hypernyms generate rules dataset. iterate surface form patterns replace words dataset attempt pattern hypernyms. resulting pattern contained dataset generate corresponding rule. instance generate rule appos->diplomat->amod appos->official->amod since patterns contained dataset know wordnet diplomat ofﬁcial. leads rules wordnet subsequently annotate manually obtain high-quality rules. note none rules directly imply freebase relation. although test relations originate freebase still hope improvements transitive effects i.e. better surface form representations turn help predict freebase facts. show results obtained injecting wordnet rules table weighted measure increases respect model compared reimplementation matrix factorization model demonstrates imposing partial ordering based implication rules used incorporate logical commonsense knowledge increase quality information extraction systems. note evaluation setting guarantees indirect effects rules measured i.e. rules directly implying test relations. shows injecting rules inﬂuences relation embedding space beyond relations explicitly stated rules. example injecting rule appos<-father->appos poss<-parent->appos contribute improved predictions relation parent/child. terns increased fractions freebase training facts. although starts lower performance r-joint freebase training facts present outperforms r-joint plain matrix factorization model substantial margin provided freebase training facts. indicates that addition much faster r-joint make better provided rules training facts. attribute bayesian personalized ranking loss instead logistic loss used rockt¨aschel former compatible ruleinjection method approach maximizing expectation propositional rules used r-joint. injecting high-quality wordnet rules unﬁltered rules respectively. increasing amount injected rules leads increase computation time even though setup rule losses used every training batch. conﬁrms high efﬁciency lifted rule injection method. asymmetric character implications order demonstrate injecting implications conserves asymmetric nature perform following experiment. incorporating highquality wordnet rules model select tuples occur relation training fact matching relation result high values scores implication holds. however tuples selected training facts matched relation scores much lower inverse implication hold table lists averaged results example rules average relations wordnet rules case injected rules without rules easier comparison scores mapped unit interval sigmoid function. quantity often interpreted probability corresponding fact holds because bpr-based training differences scores play role here. injecting rules average scores facts inferred rules model fsl) always higher facts inferred inverse rules model fsl). fourth example inverse rule leads high scores well fact daily newspaper relations less equivalent components much last example asymmetry implication maintained although absolute scores rather relations. provide visual inspection structure relation embedding space changes rules imposed. select relations involved wordnet rules gather columns single matrix sorted increasing norm figures show difference model values embeddings model polarized i.e. observe stronger negative positive components model furthermore also reveals clearer difference left right-most embeddings results imposing order relation injecting implications. order idea time efﬁciency injecting rules measure time epoch restricting program execution single .ghz core. measure average epoch without rules rule appos->organization->amod appos->party->amod poss<-parent->appos poss<-father->appos appos->lawyer->nn appos->prosecutor->nn appos->newspaper->amod appos->daily->amod appos->ambassador->amod appos->diplomat->amod tween relations asymmetric character implications. example purely based training data appears likely parent relation implies father relation vice versa. demonstrates importance added value injecting external rules capturing commonsense knowledge. presented novel fast approach incorporating ﬁrst-order implication rules distributed representations relations. termed approach ‘lifted rule injection’ avoids costly grounding ﬁrst-order implication rules thus independent size domain entities. construction rules satisﬁed observed unobserved fact. presented approach requires restriction entity-tuple embedding space. however experiments real-world dataset show impair expressiveness learned representations. contrary appears beneﬁcial regularization effect. incorporating rules generated wordnet hypernyms model improved matrix factorization baseline knowledge base completion. especially domains annotation costly small amounts training facts available approach provides leverage external knowledge sources inferring facts. future work want extend proposed ideas beyond implications towards general ﬁrstorder logic rules. believe supporting conjunctions disjunctions negations would enable debug improve representation learning based knowledge base completion. furthermore want integrate ideas neural methods beyond matrix factorization approaches. work supported research foundation flanders ghent university iminds microsoft research scholarship programme allen distinguished investigator award marie curie career integration award.", "year": 2016}