{"title": "MARGIN: Uncovering Deep Neural Networks using Graph Signal Analysis", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Interpretability has emerged as a crucial aspect of machine learning, aimed at providing insights into the working of complex neural networks. However, existing solutions vary vastly based on the nature of the interpretability task, with each use case requiring substantial time and effort. This paper introduces MARGIN, a simple yet general approach to address a large set of interpretability tasks ranging from identifying prototypes to explaining image predictions. MARGIN exploits ideas rooted in graph signal analysis to determine influential nodes in a graph, which are defined as those nodes that maximally describe a function defined on the graph. By carefully defining task-specific graphs and functions, we demonstrate that MARGIN outperforms existing approaches in a number of disparate interpretability challenges.", "text": "interpretability emerged crucial aspect machine learning aimed providing insights working complex neural networks. however existing solutions vary vastly based nature interpretability task case requiring substantial time effort. paper introduces margin simple general approach address large interpretability tasks ranging identifying prototypes explaining image predictions. margin exploits ideas rooted graph signal analysis determine inﬂuential nodes graph deﬁned nodes maximally describe function deﬁned graph. carefully deﬁning taskspeciﬁc graphs functions demonstrate margin outperforms existing approaches number disparate interpretability challenges. widespread adoption deep learning solutions science engineering obtaining posteriori interpretations learned models emerged crucial research direction. driven community-wide effort develop meta-techniques able provide insights complex neural network systems explain training predictions. despite identiﬁed research direction exists well-accepted definition interpretability. instead different contexts refer variety tasks ranging debugging models determining anomalies training data recent efforts provide formal deﬁnition interpretability generating interpretable rules focus instance-level explanations i.e. understanding network arrived particular decision single instance. arating hyperplanes classiﬁers combating noisy labels training detecting adversarial attacks generating saliency maps image classiﬁcation. discussed below solutions problems proposed using custom tailored task-speciﬁc approaches. example variety tools explain parts image responsible prediction. however cannot easily repurposed identify samples dataset helpful harmful train classiﬁer. instead introduce margin framework directly applies wide variety interpretability tasks. margin poses task hypothesis test derives measure inﬂuence indicates parts data/model maximally support hypothesis. speciﬁcally task construct graph whose nodes represent entities interest deﬁne function graph encodes hypothesis. example task determine labels need corrected dataset corrupted labels domain samples function local label agreement measures many neighbors label current node. using graph signal processing identify samples important describe label agreement function turn faulty labels introduce signiﬁcant local variations function. similarly deﬁne graphs functions address number different tasks using procedure. generic formulation extremely simple implementation provides powerful protocol realize several meta-learning techniques allowing user incorporate rich semantic information straightforward manner. nutshell proposed protocol comprised following steps identifying domain interpretability constructing neighborhood graph model domain deﬁning explanation function nodes graph performing graph signal analysis estimate inﬂuence structure domain creating interpretations based estimated inﬂuence structure. figure illustrates steps involved margin posteriori interpretability. overview using different choices graph construction figure margin overview proposed protocol posteriori interpretability tasks. illustration consider problem identifying incorrectly labeled samples given dataset. margin identiﬁes important samples need corrected ﬁxing lead improved predictive models. explanation function design present case studies demonstrate broad applicability margin posteriori interpretability. first section study unsupervised problem identifying samples well characterize underlying data distribution referred prototypes criticisms respectively show margin better identifying candidates state-of-the-art techniques. section obtain localized image saliency pixel level using margin clearly explaining predictions black-box pre-trained model show strongly agree techniques even access entire model. section identify label corruptions training data show margin able identify samples effectively recently proposed approaches also able explain results intuitively. section analyze decision surfaces pre-trained classiﬁers determining samples confusing model. finally section extend recently proposed statistical techniques detect adversarial examples harmless examples demonstrate incorporating inside margin improves discriminative power signiﬁcantly. outline recent works closely related central framework themes around margin. papers pertinent individual case studies identiﬁed respective sections. goal paper identify core framework capable repurposed several interpretability tasks. related recent works fong propose perturb images repurposed several tasks interpretability one. authors proposed strategy select inﬂuential samples extending ideas robust statistics shown applicable variety scenarios. approaches reasonably general proposed framework leverages generality graph structures along ability include arbitrary semantically rich functions deﬁned node. best knowledge work ﬁrst propose formulation. central idea margin graph signal processing identify high frequency regions graph signals. relatively recent area broad classes approaches builds spectral graph theory using graph laplacian matrix based algebraic signal processing builds upon graph shift operator applicable framework adopt latter formulation. approach relies deﬁning measure inﬂuence node related sampling graph signals. active research area several works generalizing ideas sampling interpolation domain graphs many cases signal assumed known contributions identify right function given interpretability task. addition hypothesis analyzing high frequency content function conceptually similar efﬁcient without requiring need solve sophisticated optimization. section provide overview different steps margin describe proposed inﬂuence estimation technique next section. domain design graph construction domain deﬁnition step crucial generalization margin across different scenarios. order enable instance-level interpretations single instance data possibly along perturbed variants form domain; whereas holistic understanding model obtained deﬁning entire dataset domain. regardless choice domain propose model using neighborhood graphs enables concise representation relationships samples. speciﬁcally given samples {xi} construct k-nearest neighbor domain graph captures local geometry data samples. metric graph construction arise prior knowledge domain designed based latent representations pre-trained models. example latent features alexnet resulting graph respects distance metric inferred alexnet image classiﬁcation. though difﬁculty choosing appropriate designing robust graphs well known designing better graphs beyond scope paper. experiments results sensitive choice formally undirected weighted graph represented triplet denotes nodes denotes edges adjacency matrix speciﬁes weights edges corresponds edge weight nodes {m|wnm deﬁne neighborhood node i.e. nodes connected normalized graph laplacian constructed d−/wd−/ degree matrix denotes identity matrix. explanation function deﬁnition component margin construct explanation function measures well node graph supports presented hypothesis. illustrate process example order create saliency maps image classiﬁcation build graph node corresponds potential explanation edges measure likely explanations produce similar predictions. scenario hypothesize ideal explanation sparse terms number pixels since interpretable. consequently size explanation used function. table shows domain design graph construction function deﬁnition choices made different cases. section present detailed discussion. inﬂuence estimation central analysis step margin obtaining inﬂuence estimates nodes reveal nodes maximally describe variations chosen explanation function. implicitly step viewed soft-sample selection strategy respect structure induced domain graph. propose perform estimation using tools graph signal analysis. section describes proposed algorithm inﬂuence estimation. inﬂuence interpretation depending hypothesis chosen posteriori analysis step requires design appropriate strategy transferring estimated inﬂuences interpretable explanation. proposed inﬂuence estimation given neighborhood graph along explanation function propose employ graph signal analysis estimate node inﬂuence scores. describe algorithm present brief overview preliminaries. deﬁnitions notation terminology deﬁning operator analogous time-shift delay operator classical signal processing. graph shift operation function node replaced weighted linear combination neighbors graph shift operator simplest non-trivial graph ﬁlter. commonly used choices include adjacency matrix transition matrix graph laplacian eigenvectors graph shift operator referred graph fourier basis uλut rn×n fourier transform signal deﬁned ordered eigenvalues corresponding eigenvectors represent frequencies signal representing smallest largest frequencies. notion frequency graph corresponds rate change function across nodes neighborhood. higher change corresponds high frequency smooth variation corresponds frequency. context graph ﬁltering using graph shift operator corresponds low-pass ﬁlter dispenses high frequency components function. similarly simple high-pass ﬁlter easily designed algorithm overall procedure obtain inﬂuence algorithm inﬂuence estimation input domain graph explanation function deﬁned nodes output inﬂuence estimate node construct graph shift operator foreach scores nodes found algorithm intuitively design high-pass ﬁlter eliminates frequency content retains signal energy nodes characterize extreme variations function. following high-pass ﬁltering step inﬂuence score node estimated magnitude ﬁltered function value node corresponds high-pass ﬁltered version interestingly analyzing high frequency components explanation function often leads sparse inﬂuence structure indicating presence multiple local optima corroborate hypothesis. conversely inﬂuence structure obtained frequency components typically dense hence requires additional processing qualify regions disagreement. commonly encountered problem interpretability identify samples prototypical dataset statistically different prototypes together provide holistic understanding underlying data distribution. even cases access label information seek hypothesis pick samples representatives local neighborhood emphasizing statistically anomalous samples. function recently utilized deﬁne prototypes criticisms based maximum mean discrepancy formulation following general protocol figure domain deﬁned complete dataset along labels available. since analysis rely pretrained models construct neighborhood graph based conventional metrics e.g. euclidean distance. inspired deﬁne following explanation function sample remove chosen sample connected neighbors graph construct estimate function node mmd. cases labeled datasets kernel density estimates computation obtained using samples belonging class. refer cases global local respectively. hypothesis regions criticisms tend produce highly varying scores thereby producing high frequency content hence associated high margin scores. conversely samples margin scores correspond prototypes since regions strong agreement scores. speciﬁcally consider samples margin scores prototypes rank actual function values. contrast greedy inference approach estimates prototypes criticisms separately inferred jointly case. experiment setup results evaluate effectiveness chosen samples predictive modeling experiments. usps handwritten digits data experiment consists images belonging classes. standard train/test split dataset training samples rest testing. fair comparisons simple -nearest neighbor classiﬁer. described earlier consider unsupervised supervised variants explanation function sample selection. expect prototypical samples helpful predictive modeling i.e. good generalization. figure observe prototypes margin perform competitively comparison baseline technique. importantly margin particularly superior global case access label information. hand criticisms expected least helpful generalization since often comprise boundary cases outliers under-sampled regions space. hence evaluate test error using criticisms training data. interestingly shown figure criticisms margin achieve signiﬁcantly higher test errors comparison samples identiﬁed using mmd-critic based optimization furthermore examples selected prototypes criticisms margin included figure generating explanations predictions crucial debugging black-box models eventually building trust. given model deep neural network designed classify image classes plausible explanation test prediction quantify importance different image regions overall prediction i.e. produce saliency map. posit perturbing salient regions result maximal changes prediction. figure using margin sample prototypes criticisms. experiment study generalization behavior models trained solely using prototypes criticisms. addition expect sparse explanations interpretable. section describe margin applied achieve objectives. formulation since interested producing explanations instance-level predictions using margin domain corresponds possible explanations image. note that space explanations combinatorially large hence adopt following greedy approach construct domain. slic algorithm varying number superpixels deﬁne domain union superpixels independent runs. setup superpixels plausible explanation become nodes assuming test image assigned class softmax probability explanations mask pixels image pre-trained model obtain softmax probability measure saliency ˆpi|. using estimates obtain pixel-level saliency weighted combination saliency different superpixels dense saliency similar previous approaches note that saliency estimation process impose sparsity requirement. hence margin obtain inﬂuence scores based sparseness. construct neighborhoods explanations based impact predictions i.e. edges computed based ˆpi| values. explanation function node deﬁned ratio size superpixel corresponding node size largest superpixel graph. intuitively margin ﬁnds sparsest explanation different level sets saliency function ˆpi|. subsequently compute pixellevel inﬂuence scores weighted combination inﬂuences different superpixels. overall saliency obtained inal refers hadamard product. experiment setup results using images imagenet database alexnet model demonstrate margin effectively produce explanations classiﬁcation. figure illustrates process obtaining ﬁnal saliency image tabby class. interestingly mouth whiskers highlighted salient regions prediction. figure shows saliency maps margin several cases. comparison show results grad-cam white-box approach accesses gradients network. that using black-box approach margin produces explanations strongly corroborate grad-cam cases produces interpretable explanations. example case cream image margin identiﬁes cream spoon salient regions grad-cam highlights cream quite background regions salient. similarly case fountain image margin highlights fountain grad-cam highlights background slightly fountain itself readily interpretible. increasingly important problem real-world applications concerned quality labels supervisory tasks. since presence noisy labels impact model learning recent approaches attempt compensate perturbing labels samples determined highrisk corrupted possible annotators check labels high-risk samples. section propose employ margin recover incorrectly labeled samples. particular consider binary classiﬁcation task assume labels randomly ﬂipped class. order identify samples incorrectly labeled select samples highest margin score followed simulating human user correcting labels samples. ideally would figure show entire process constructing saliency particular image imagenet. left right original image saliency sparsity ﬁnally explanation margin inal. figure approach identiﬁes salient regions different classes image classiﬁcation using alexnet. bottom original image margin’s explanation overlaid image grad-cam’s explanation. note approach yields highly speciﬁc sparse explanations different regions image given class. like number samples checked user small possible. formulation similar case study entire dataset used deﬁne domain user-deﬁned metric used construct graph. since expect ﬂips random hypothesize occur regions labels corrupted samples different neighbors. instead directly using label node explanation function believe smoothly varying function allow extract regions high frequency changes robustly. result propose measure level distrust given node measuring many neighbors disagree label iments enron spam classiﬁcation dataset containing training examples imbalanced class split around following standard practice randomly corrupt labels samples. enron spam dataset extracted bag-of-words features dimensions corresponding frequently occurring words. features used construct k-nn graph number neighbors ﬁxed report average results repetitions experiment. compare approach three baselines inﬂuence functions obtain inﬂuential samples using inﬂuence func random sampling tions oracle best case scenario number labels corrected equal number samples observed. following vary percentage inﬂuential samples chosen compute recall measure corresponds fraction label ﬂips recovered chosen subset samples. percentage points better state-of-the-art inﬂuence functions achieving recall nearly observing samples. figure study margin scores incorrectly labeled samples. y-axis show percentage neighbors agree original label proxy measure identify samples closer classiﬁcation boundary ones farther away. x-axis shows margin score clear trend indicates strong preference samples farther away classiﬁcation boundary. words corresponds strongly correcting least number samples lead gain validation performance using trained model. studying black-box models crucial obtain holistic understanding strengths importantly weaknesses. conventionally carried characterizing decision surfaces resulting classiﬁers. experiment demonstrate margin utilized identify samples confusing model. formulation order adopt margin analyzing speciﬁc model construct graph using latent representations inferred model. since decision surface characterization similar case study local label agreement measure explanation function. experiment setup results perform experiment -class datasets extracted imagenet mnist. speciﬁcally case imagenet perform decision surface characterization classes tabby great dane. used features pretrained alexnet’s penultimate layer construct graph. mnist dataset considered data samples digits used latent space produced using convolutional neural network analysis. selected subset samples characterizing decision surfaces datasets shown figure imagenet clear model gets confused whenever animal’s face visible contorted position occluded. similarly mnist dataset examples shown depict atypical ways digits written. application examine problem quantifying statistical properties adversarial examples using margin. adversarial samples refer examples specially crafted particular trained model ‘tricked’ misclassifying them. done typically perturbing sample sometimes ways imperceptible humans maximizing misclassiﬁcation rates. order better understand behaviour adversarial examples studies past show adversarial examples statistically different normal test examples. example score distributions proposed kernel density estimator however measures global provide little insight individual samples. propose margin develop statistical measures sample level study individual adversarial samples differ regular samples. formulation case studies margin constructs graph node corresponds example either adversarial harmless edges constructed using neighbors latent space model adversarial examples designed. consider kinds functions experiment global similar score between whole without particular sample neighbors. provides capture statistically rarer samples dataset; also figure comparison statistical scores identify adversarial samples without incorporating graph structure. including structure results much better separation adversarial harmless examples. addition regions overlap easily explained. sample proposed measure discrepancy sample training samples predicted class. measures illustrative useful functions determine inﬂuences within margin. experiment setup results perform experiments randomly sampled test images mnist dataset adversarially perturb images. measure margin scores using global popular attacks fast gradient sign method attack l-attack setup including network architecture mnist. resulting margin score determined using algorithm discriminative seen figure noted measures effective stronger attacks l-attack. reﬂected much lower degree even approach small overlap distributions. proposed generic framework called margin able provide explanations popular interpretability tasks machine learning. range identifying prototypical samples dataset might helpful training explaining salient regions image classiﬁcation. regard margin exploits ideas rooted graph signal processing identify inﬂuential nodes graph nodes maximally affect graph function. framework extremely simple highly general allows practitioner include rich semantic information easily three crucial ways deﬁning domain edges ﬁnally function deﬁned node. graph based analysis easily scales sparse graphs tens thousands references achanta radhakrishna shaji appu smith kevin lucchi aurelien pascal susstrunk sabine. slic superpixels compared state-of-the-art superpixel methods. ieee transactions pattern analysis machine intelligence biggio battista corona igino maiorca davide nelson blaine ˇsrndi´c nedim laskov pavel giacinto giorgio roli fabio. evasion attacks machine learning test time. joint european conference machine learning knowledge discovery databases springer carlini nicholas wagner david. adversarial examples easily detected bypassing detection methods. proceedings workshop artiﬁcial intelligence security chen siheng varma rohan sandryhaila aliaksei kovaˇcevi´c jelena. discrete signal processing graphs sampling theory. ieee transactions signal processing been khanna rajiv koyejo oluwasanmi examples enough learn criticize criticism interpretability. advances neural information processing systems. krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems ribeiro marco tulio singh sameer guestrin carlos. trust you? explaining predictions classiﬁer. proceedings sigkdd international conference knowledge discovery data mining russakovsky olga deng krause jonathan satheesh sanjeev sean huang zhiheng karpathy andrej khosla aditya bernstein michael imagenet large scale visual recognition challenge. international journal computer vision selvaraju ramprasaath cogswell michael abhishek vedantam ramakrishna parikh devi batra dhruv. grad-cam visual explanations deep networks gradient-based localization. ieee international conference computer vision shuman david narang sunil frossard pascal ortega antonio vandergheynst pierre. emerging ﬁeld signal processing graphs extending highdimensional data analysis networks irregular domains. ieee signal processing magazine szegedy christian zaremba wojciech sutskever ilya bruna joan erhan dumitru goodfellow fergus rob. intriguing properties neural networks. arxiv preprint arxiv.", "year": 2017}