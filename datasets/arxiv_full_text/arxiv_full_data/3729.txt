{"title": "Statistical-mechanical analysis of pre-training and fine tuning in deep  learning", "tag": ["stat.ML", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI", "cs.LG"], "abstract": "In this paper, we present a statistical-mechanical analysis of deep learning. We elucidate some of the essential components of deep learning---pre-training by unsupervised learning and fine tuning by supervised learning. We formulate the extraction of features from the training data as a margin criterion in a high-dimensional feature-vector space. The self-organized classifier is then supplied with small amounts of labelled data, as in deep learning. Although we employ a simple single-layer perceptron model, rather than directly analyzing a multi-layer neural network, we find a nontrivial phase transition that is dependent on the number of unlabelled data in the generalization error of the resultant classifier. In this sense, we evaluate the efficacy of the unsupervised learning component of deep learning. The analysis is performed by the replica method, which is a sophisticated tool in statistical mechanics. We validate our result in the manner of deep learning, using a simple iterative algorithm to learn the weight vector on the basis of belief propagation.", "text": "procedure updating weight vector basis gradient method i.e. back propagation takes relatively long time) regularization norm greedy algorithm.–) many local minima found optimization dnn. labelled data generated joint probability number labelled data. unlabelled data number unlabelled data follow marginal probability pgp. following assume large-n limit huge number data well symmetric distribution label exchange order operations thermodynamic limit assume replica number temporarily natural number evaluation introduce following constraints simplify calculation dependent expectation taken distributionqn introduce auxiliary parameters give integral representation kronecker’s delta. denote average respect -multivariate gaussian random variables {ua} vanishing mean covariance qab. generalization error exhibit nontrivial behaviour. however nonzero nontrivial curves give multiple solutions plane. remarkable result combination unsupervised supervised learning. nontrivial curves imply existence metastable state similar several classical spin models.) decreases spinodal point moves larger values decreasing leads diﬃculties next consider eﬀect tuning context deep learning. plot saddle-point solutions plane multiple solutions appear certain region. increasing number unlabelled data leads improvement nsam maximum iteration number tuning fig. plot average generalization error nsam independent runs starting randomized initial conditions. theoretically predicted results conﬁrm water-falling phenomena", "year": 2015}