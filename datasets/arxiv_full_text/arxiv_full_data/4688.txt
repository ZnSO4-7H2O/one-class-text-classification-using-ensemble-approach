{"title": "Exploring Parallelism in Learning Belief Networks", "tag": ["cs.AI", "cs.LG"], "abstract": "It has been shown that a class of probabilistic domain models cannot be learned correctly by several existing algorithms which employ a single-link look ahead search. When a multi-link look ahead search is used, the computational complexity of the learning algorithm increases. We study how to use parallelism to tackle the increased complexity in learning such models and to speed up learning in large domains. An algorithm is proposed to decompose the learning task for parallel processing. A further task decomposition is used to balance load among processors and to increase the speed-up and efficiency. For learning from very large datasets, we present a regrouping of the available processors such that slow data access through file can be replaced by fast memory access. Our implementation in a parallel computer demonstrates the effectiveness of the algorithm.", "text": "shown class probabilistic domain models cannot learned correctly algorithms employ several existing single-link lookahead search. multi­ link lookahead search used computa­ tional complexity increases. study parallelism tackle increased complexity learn­ models speed learning large domains. algorithm proposed decompose learning task parallel pro­ task decomposition cessing. used balance load among processors crease speed-up efficiency. learn­ large datasets grouping available slow data access file replaced fast memory access. implementation parallel fectiveness algorithms monly adopted efficiency. single-link adopted differ search consecutive link. however shown exists class domain models termed pseudo­ sikora realize works par­ allel learning belief networks. study focuses learning decomposable although result generalized bayesian networks. study parallelism message passing mimd parallel applicability strated different domains many effective infer­ ence techniques networks domain experts elicita­ tion becomes bottleneck. knowledge acquisition tively investigated data sepset i-map etc. junc­ tion forest chordal graph component paper organized follows troduce models multi-link section propose parallel belief networks. also analyze problems load balancing local memory solutions. results. shown exists class probability sets collectively marginal parity problems several algorithms works shown underlying simple model four variables examples suppose ture resultant graph non-chordal. complexity chordality number variables number edges graph amount com­ putation dashed links graph chordal. since complexity cross entropy improve lookah links total number explorers total number servers threshold partition sets send distinct server broadcast last explorers; coefficient whose value imental isfy idel maximum local memory available .sec. learning idel o.obmb idml .mb. learning performed large dataset marginal suppose choose idel obtain message passing message. sage passing number links sors. important number oflinks involved minimized. communication ager explorers mesh ternary size dataset memory processor marginal servers. comparison using different servers work. figure shows speed-up figure axis labelled horizontal table lists experimental result four models mentioned above. triple-link head search used learning respectively. learning number explorers tion time speed efficiency second column. result models employed. plorers ficiency models in-between. increase larger allocation quence message less significant compared hence efficiency parallel studied increased networks learning proposed multiple ditional parallel method handle variation searching overcome parallel allows dataset much larger processor.", "year": 2013}