{"title": "Fast k-means based on KNN Graph", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "In the era of big data, k-means clustering has been widely adopted as a basic processing tool in various contexts. However, its computational cost could be prohibitively high as the data size and the cluster number are large. It is well known that the processing bottleneck of k-means lies in the operation of seeking closest centroid in each iteration. In this paper, a novel solution towards the scalability issue of k-means is presented. In the proposal, k-means is supported by an approximate k-nearest neighbors graph. In the k-means iteration, each data sample is only compared to clusters that its nearest neighbors reside. Since the number of nearest neighbors we consider is much less than k, the processing cost in this step becomes minor and irrelevant to k. The processing bottleneck is therefore overcome. The most interesting thing is that k-nearest neighbor graph is constructed by iteratively calling the fast $k$-means itself. Comparing with existing fast k-means variants, the proposed algorithm achieves hundreds to thousands times speed-up while maintaining high clustering quality. As it is tested on 10 million 512-dimensional data, it takes only 5.2 hours to produce 1 million clusters. In contrast, to fulfill the same scale of clustering, it would take 3 years for traditional k-means.", "text": "abstract—in data k-means clustering widely adopted basic processing tool various contexts. however computational cost could prohibitively high data size cluster number large. well known processing bottleneck k-means lies operation seeking closest centroid iteration. paper novel solution towards scalability issue k-means presented. proposal k-means supported approximate k-nearest neighbors graph. k-means iteration data sample compared clusters nearest neighbors reside. since number nearest neighbors consider much less processing cost step becomes minor irrelevant processing bottleneck therefore overcome. interesting thing k-nearest neighbor graph constructed iteratively calling fast k-means itself. comparing existing fast k-means variants proposed algorithm achieves hundreds thousands times speed-up maintaining high clustering quality. tested million -dimensional data takes hours produce million clusters. contrast fulﬁll scale clustering would take years traditional k-means. clustering problems arise wide variety applications knowledge discovery data compression large-scale image linking visual vocabulary construction since general k-means algorithm proposed continuous efforts made search better solution issue. various algorithms proposed last decades mean shift db-scan spectral clustering rankorder birch clusterdp etc. among algorithms k-means remains popular simplicity efﬁciency moderate stable performance different contexts. known popular algorithms data mining traditional k-means given data samples real d-dimensional space integer clustering modeled distortion minimization process. clustering process partitions samples sets minimize mean squared distance sample nearest cluster centroid. could formularized centroid cluster eqn. function returns closest centroid sample general major steps k-means iteration. ﬁrst step sample assigned closest centroid. second step centroid updated taking average assigned samples. steps repeated distortion change consecutive iterations. fujian laboratory sensing computing smart city school information science engineering xiamen university xiamen china. wan-lei zhao corresponding author. e-mail wlzhaoxmu.edu.cn. although k-means remains popular actually suffers major issues. firstly well-known k-means converges local optima. recent researches working improving clustering quality thanks introduction incremental optimization strategy k-means able converge considerably lower distortion. second issue mainly scalability. although complexity k-means linear size input data clustering cost could prohibitively high given size data expected number clusters large. moreover according worst case running time k-means could exponential size input samples. steady growth data volume various forms daily basis scalability issue traditional algorithm becomes imminent. k-means iteration intensive operation assigning samples closest centroid. result scalability issue principally heavy cost computing nearest centroid sample recent years continuous efforts devoted looking effective solutions still workable webscale data. representative works however k-means variants achieve high speed efﬁciency sacriﬁcing clustering quality. algorithm presented demonstrates faster speed maintains relatively high quality. unfortunately extra memory required. speciﬁcally memory complexity quadratic turns unsuitable case large. paper efﬁcient k-means variant proposed k-means clustering process supported approximate k-nearest neighbor graph approximate graph built pre-processing step fast k-means graph construction process jointly undertaken. interestingly constructed graph applied approximate nearest neighbor search tasks. moreover comparing graph construction algorithms speciﬁcally designed approximate nearest neighbor search algorithm requires much lower computational cost. remainder paper organized follows. reviews representative works improving performance traditional k-means presented section section important k-means variants reviewed facilitates discussion algorithm section extensive experiment studies proposed clustering method presented section section concludes paper. related works k-means variants versatility simplicity k-means widely adopted different contexts. big-data k-means used basic tool process large-scale data various forms. unfortunately discussed section computational cost could prohibitively high scale data increases extraordinarily large i.e. billion level. recently several k-means variants proposed either enhance clustering quality scalability. terms clustering quality important work comes vassilvitskii motivation based observation k-means converges better local optima initial clustering centroids carefully selected. according k-means iteration also converges faster careful selection initial cluster centroids. however order adapt initial centroids data distribution rounds scanning data necessary. although number scanning rounds reduced extra computational cost still inevitable. recently variant called boost k-means proposed egg-chicken loop k-means simpliﬁed stochastic optimization process also known incremental k-means indicated extensive experiments able converge considerably better local optima involving extra cost. superior performance incremental optimization scheme adopted design. order facilitate discussion detailed review boost k-means given section k-means iteration processing bottleneck operation assigning sample closest centroid. iteration becomes unbearably slow size dimension data large. noticed nearest neighbor search problem kanungo proposed index dataset tree speed-up sample-to-centroid nearest neighbor search. unfortunately feasible dimension data tens. similar scheme adopted however curse dimensionality method becomes ineffective dimension data grows hundreds. recent work takes similar speed-up nearest neighbor search indexing dataset inverted structure. iteration fig. stastitics co-occurrence rate sample nearest neighbor cluster. experiments conducted siftk traditional k-means two-means tree experiment size cluster ﬁxed note probability randomly selected samples fall cluster fig. shows co-occurrence rate sample κ-th nearest neighbor cluster. k-means variants two-means tree tested siftk trend observed cases. samples closer probability appear cluster higher. probability much higher probability random collision. observation learn sample nearest neighbors arranged cluster. hand indicates sample neighbors temporarily cluster reasonable compare sample clusters neighbors reside. among clusters probably true neighboring samples live together. hand viewpoint k-nearest neighbor graph construction order build list sample sufﬁcient compare sample samples reside cluster since neighbors likely reside cluster. based analysis scalability issue kmeans clustering addressed steps paper. firstly fast k-means called build approximate graph itself. secondly fast k-means clustering undertaken support constructed graph. fast k-means iteration sample compare clusters top-κ nearest neighbors reside. usually number nearest neighbors consider considerably smaller clustering number number clusters actually visit even fewer. consequence signiﬁcant speed-up expected. moreover revealed later clustering quality drops little kind speed-up scheme. best knowledge ﬁrst piece work graph used speed-up k-means clustering. addition comparing graph construction algorithms algorithm computationally efﬁcient leads lowest clustering distortion. furthermore applied anns problem shows satisfactory performance across different datasets. order facilitate discussions later sections important k-means variants reviewed namely boost k-means means tree shown later speed-up scheme built upon boost kmeans instead traditional k-means former always produces clusters higher quality. means tree used produce initial clusters high efﬁciency. extension incremental k-means boost k-means allows optimization iteration feasible whole space. different k-means variants boost kmeans iteration driven explicit objective function. given clusters sr=···k composite vector cluster objective function boost directly derived eqn. objective function traditional k-means clustering revised stochastic optimization procedure. time sample randomly selected searches better re-allocation leads highest increase namely variation function value incurred possible movement given optimization process seeks movement highest positive. particular movement made soon movement appropriate. according able converge much better local optima comparison k-means variants. cost checking best movement boost k-means equivalent seeking closest centroid traditional k-means. result boost k-means complexity level traditional kmeans. centroid queried indexed data. attributing efﬁciency inverted structure orders magnitude speed-up observed. however inverted indexing structure effective sparse vectors. alternatively scalability issue k-means addressed subsampling dataset k-means iteration. namely methods pick small portion whole dataset update clustering centroids time. sake speed efﬁciency number iterations empirically small value. therefore possible clustering terminates without single pass whole dataset leads higher speed also higher clustering distortion. even though coping high dimensional data size speed-up achieved methods still limited. apart methods speed-up could achieved reducing comparisons samples centroids. active points samples located cluster boundaries considered swapped clusters. comparing kmeans variants makes good trade-off efﬁciency clustering quality whereas considerable quality degradation still inevitable. another easy reduce number comparisons samples centroids conduct clustering top-down hierarchical manner speciﬁcally clustering solution obtained sequence repeated bisections. clustering complexity k-means reduced o·n·d) particularly signiﬁcant large. however poor clustering performance achieved usual case breaks lloyd’s condition k-nearest neighbor graph construction graph primarily built support nearest neighbor search also data structure manifold learning machine learning basically tries top-κ nearest neighbors data point. built brute-force time complexity could large. result computationally expensive build exact graph. reason recent works search approximate efﬁcient solution. approximate graph built efﬁciently divide-and-conquer strategy. algorithm original dataset partitioned thousands small subsets trees. list built exhaustive comparison within subset. however recall graph turns low. recent works could viewed improvements work. successful graph construction algorithm called descent/kgraph proposed. algorithm proposed based observation neighbor neighbor also likely neighbor. according empirical time complexity unfortunately according observation recall drops dramatically scale data increases large i.e. algorithm presented faces similar problem. means tree means tree variant hierarchical bisecting k-means. adopted graph construction high speed efﬁciency alg. shows general procedure means tree. similar bisecting k-means samples partitioned recursively clusters time clusters produced. different bisecting k-means step taken bisecting. resulting clusters adjusted equal size. complexity bisecting kmeans namely even faster round k-means iteration. paper means tree adopted generate initial k-means partition. order enhance performance aforementioned boost kmeans integrated bisecting operation algorithm twomeans order facilitate operations later steps mapping line converts cluster labels samples cluster means tree clustering cluster mapped back cluster label representation line graph based -means section solution scalability issue kmeans presented. firstly general procedure boost k-means undertaken support graph given. support fast clustering process graph construction sufﬁciently fast otherwise becomes another processing bottleneck. overcome problem novel light-weight graph construction procedure also introduced. motivation illustrated fig. strong correlation closeness data samples membership living cluster. correlation could interpreted either side k-means clustering side graph construction. clustering side list sample known clustering process arranging close neighbors cluster. result given sample clustering needs check clusters κ-nearest neighbors live seeks approriate cluster move therefore need check clusters. consequence processing bottleneck overcomed. graph construction side data samples already partitioned small clusters graph construction undertaken within cluster exhaustive pair-wise comparison. consequence graph construction pulled efﬁcient manner. based ﬁrst piece interpretation work fast k-means algorithm. similarly based second piece interpretation graph construction algorithm conceived. fast k-means driven graph given graph ready boost k-means procedure presented revised alg. beginning clustering tree called produce clusters. initial clusters incrementally optimized later steps. step optimization iteration sample randomly selected. thereafter clusters neighbors reside collected. selected sample therefore checked clusters seek best move. iteration terminates convergence condition reached. algorithm gk-means comparing procedure presented boost kmeans basically major modiﬁcations. firstly initial clusters initialized means tree whose complexity o·d) considerably faster traditional k-means initialization. secondly shown line clusters keep ﬁrst neighbors visited number much smaller furthermore possible several neighbors live cluster. consequence number clusters sample visits even smaller that line alg. modiﬁed seeking closest centroid collected clusters. section performance alternative conﬁguration presented. revealed similar speed-up achieved whereas shows inferior clustering quality comparison built upon boost k-means. graph construction fast k-means discussed section samples live cluster likely neighbors. graph construction clue could fully exploited. namely search nearest neighbors sample undertaken within cluster resides. based principle fast graph construction conceived. firstly fast k-means clustering called produce ﬁxed number clusters. thereafter exhaustive comparisons conducted within cluster. closer point pairs used update graph order control complexity graph construction level cluster size ﬁxed small i.e. given cluster size ﬁxed constant constant easy cluster number according alg. graph required input parameter. design random graph supplied beginning. since graph randomly initialized would expect good cluster partitions returned alg. beginning. however iteration continues quality graph enhanced incrementally. accordingly structure cluster partitions returned alg. becomes better. result structures graph cluster structures evolve alternatively. fig. illustrates intertwined evolving process. iteration parameter controls ﬁnal quality graph. larger leads preciser graph taking higher time cost. algorithm graph construction fig. shows curves average recall graph clustering distortion functions shown ﬁgure beginning procedure quality clustering quality graph poor. clustering results nearly random. alg. speciﬁcation graph construction algorithm adopted. result graph supplied construction algorithms achieve similar speed-up. however revealed later graph algorithm presented alg. produces best clustering quality. moreover comparing graph construction algorithms alg. takes much less memory. extra memory takes keep graph. furthermore least times faster descent small world graph construction computational cost alg. also adopted construct graph approximate nearest neighbor search. according observation although quality graph usually lower descent able achieve similar even better performance methods presented instance takes less fulﬁll query million sifts recall full discussion anns graph beyond focus paper. summary proposed fast k-means consists major steps. ﬁrst step fast k-means called build approximate graph itself. second step fast k-means performed produce clusters support approximate graph. since graph built based intermediate clustering results ﬁrst step information samples organized clusters kept graph. clustering second step therefore guided kind prior knowledge. since algorithm based graph called graph based k-means discussion parameters alg. alg. besides cluster number additionally three parameters involved. parameter alg. controls quality graph. according observation sufﬁcient clustering task. alg. called produce graph anns task could up-to i.e. parameter controls size cluster used graph construction. larger leads better graph quality whereas also induces number pair-wise comparisons. reason trade-off made. according observation recommended range parameter controls number neighbors sample consider fast k-means clustering. turn determines number clusters sample visits. neighbors considered chance miss true cluster high. hand many neighbors considered comparison comparisons required. speedtraditional k-means becomes less signiﬁcant. trade-off made. according empirical study clustering quality becomes stable larger implementation ﬁxed respectively. complexity analysis section complexity alg. alg. analyzed. shown above gk-means comprised major parts namely means initialization fast k-means clustering. ﬁrst part complexity tree initialization second part since sample visits clusters iteration cost clustering d·n·κ iteration. result overall complexity o+t·d·n·κ) number iterations. analysis clear cluster number minor impact clustering complexity. graph construcion consists major steps namely fast k-means clustering graph reﬁnement. clustering step according analysis complexity result complexity graph construction o+·d·n·κ+d·n·ξ) small constants. overall complexity whole procedure level. boost k-means closure k-means mini-batch considered inferior performance closure k-means reported comparisons k-means variants performance gk-means studied different conﬁgurations. namely well gk-means performed built upon traditional kmeans instead boost k-means. additionally gk-means also tested approximate graph supplied descent that want search best conﬁguration currently gk-means. mainly study clustering quality scalability four large-scale datasets summarized tab. type data covers image local features image global features vectorized text word features. dimension data varies dimensions. scale datasets million level. methods considered paper implemented compiled simulations conducted single thread .ghz xeon memory setup. evaluation protocol similar average distortion adopted evaluate clustering quality. basically average distance samples cluster centroid given eqn. seen equation nothing taking average k-means objective function eqn. lower distortion value better quality clustering result. measure within-cluster squared distortions order study relation quality graph quality clustering result average recall graph also considered. evaluation recall top- nearest neighbor measured. siftm dataset ground-truth graph produced brute-force search takes hours. vladm dataset costly produce ground-truth whole recall therefore estimated considering nearest neighbors randomly selected samples. conﬁguration test section different conﬁgurations alg. tested. discussed section alg. could supported graph construction algorithm. addition also pointed similar speed-up scheme alg. feasible traditional k-means. section three shown fig. methods except mini-batch clustering distortion changes little iterations. boost k-means always demonstrates best performance terms clustering quality. cases gk-means shows slightly lower clustering quality boost k-means. siftm gistm even outperforms traditional k-means. kgraph+gk-means achieves similar performance gkmeans across datasets. however around times slower since costly construct graph descent. gk-means shows highest efﬁciency tests. overall gk-means offers much better trade-off efﬁciency clustering quality among existing kmeans variants. different conﬁgurations tested. ﬁrst alg. supplied graph descent denoted kgraph+gk-means run. second alg. modiﬁed built upon traditional k-means denoted gk-means− run. standard setup gk-means clustering built upon boost kmeans. gk-means− gk-means graph supplied alg. experments conducted siftm dataset. runs cluster number ﬁxed fig. shows distortion trend three conﬁgurations graphs different qualities supplied. basically conﬁgurations higher graph quality leads steadily lower clustering distortion. graphs recall level gk-means built upon boost k-means shows much lower clustering distortion. furthermore gk-means converges slightly lower distortion graph supplied alg. comparing graph supplied descent graph alg. carries information samples roughly organized clusters since list built alg. based clustering results. following superior performance gk-means selected standard conﬁguration alg. comparison. clustering quality section clustering quality gk-means studied comparison k-means boost k-means closure kmeans mini-batch. quality fast k-means clustering measured studying trend clustering distortion number iterations. experiments conducted datasets siftm glovem gistm. cluster number ﬁxed experiments. fig. show trend clustering distortion function iteration datasets siftm glove gistm respectively. fig. show trend clustering distortion function clustering time algorithms make relatively good trade-off efﬁciency quality. performance k-means boost k-means mini-batch presented ﬁrst experiment clustering methods tested scale input images varies data different scales clustered ﬁxed number clusters i.e. time costs methods presented fig. accordingly average distortion methods presented fig. shown fig. gk-means constantly faster closure k-means least times faster kmeans boost k-means. mean time shown fig. clustering quality gk-means close boost kmeans across different scales input data. contrast although mini-batch demonstrates fastest speed test clustering quality turns poor different settings shown fig. addition scalability clustering methods tested number clusters varies scale input data ﬁxed million. fig. shows time cost methods. accordingly average distortion methods given fig. shown ﬁgure k-means boost k-means mini-batch clustering methods time cost increases linearly number clusters increases. mini-batch longer efﬁcient increases. contrast time cost closure k-means gk-means remains nearly constant across different cluster numbers. terms clustering quality seen fig. gk-means demonstrates similar quality boost k-means considerably better closure k-means mini-batch k-means. clear trend observed fig. methods based boost k-means shows increasingly higher performance rest grows. overall clustering driven proposed optimization process shows higher speed better quality. highest speed achieved gk-means minutes required cluster million -dimensional data clusters. another challenging scalability test also conducted vladm partitioned million clusters. workable algorithms case namely closure k-means gk-means tested. gk-means besides standard conﬁguration gk-means graph supplied descent also tested denoted kgraph+gk-means. performance shown tab. gk-means kgraph+gk-means recall level approximate graph also reported. shown table compared closure kmeans runs gk-means show signiﬁcantly lower clustering distortion. particular gk-means standard conﬁguration shows lowest clustering distortion. similar experiments section gk-means shows better performance graph supplied alg. graph provided alg. keeps information intermediate clustering structures. kind information transferred clustering process. therefore able produce better quality even though recall graph lower descent. gkmeans also achieves highest speed efﬁciency challenging test. according estimation would take years fulﬁll task traditional kmeans. conclusion paper presented solution scalability issue k-means. show fast k-means clustering achievable support approximate graph. speciﬁcally k-means iteration sample needs compare clusters nearest neighbors live clustering complexity therefore irrelevant clustering number. shown paper hundreds thousands times speed-up achieved particular case large. addition since fast k-means built upon boost k-means also shows high clustering quality. overall proposed gkmeans shows considerably better trade-off clustering quality efﬁciency existing solutions. moreover beauty algorithm also lies design fast graph construction process. graph built calling gk-means intertwined evolving process. process graph k-means clustering incrementally optimized. intertwined self-evolving process could generalized unsupervised learning framework future research work. gong pawlowski yang brandy bourdev fergus scale photo hash clustering single machine cvpr sivic zisserman video google text retrieval approach object matching videos iccv oct. lloyd least squares quantization ieee trans. information theory vol. mar. macqueen james some methods classiﬁcation analysis multivariate observations proceedings ﬁfth berkeley symposium mathematical statistics probability vol. ester peter kriegel sander density-based algorithm discovering clusters large spatial databases noise ieee transactions knowledge data engineering zhang ramakrishnan livny birch efﬁcient data clustering method large databases proceedings sigmod international conference management data vol. jun. sculley web-scale k-means clustering proceedings international conference world wide bahmani moseley vattani kumar vassilvitskii scalable k-means++ proceedings vldb endowment vol. broder garcia-pueyo josifovski vassilvitskii venkatesan scalable k-means ranked retrieval proceedings international conference search data mining avrithis quantize conquer dimensionality-recursive solution clustering vector quantization image retrieval proceedings ieee international conference computer vision dec. verma kpotufe dasgupta which spatial partition trees adaptive intrinsic dimension? proceedings twenty-fifth conference uncertainty artiﬁcial intelligence jun. pelleg moore accelerating exact k-means algorithms geometric reasoning proceedings fifth sigkdd international conference knowledge discovery data mining aug. broder garcia-pueyo josifovski vassilvitskii venkatesan scalable k-means ranked retrieval proceedings international conference search data mining feb. jain dubes algorithms clustering data. zhao karypis hierarchical clustering algorithms document datasets data mining knowledge discovery vol. mar. chen fang yousef fast approximate graph construction high dimensional data recursive lanczos bisection journal machine learning research vol. dec.", "year": 2017}