{"title": "Semantic Compositional Networks for Visual Captioning", "tag": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "A Semantic Compositional Network (SCN) is developed for image captioning, in which semantic concepts (i.e., tags) are detected from the image, and the probability of each tag is used to compose the parameters in a long short-term memory (LSTM) network. The SCN extends each weight matrix of the LSTM to an ensemble of tag-dependent weight matrices. The degree to which each member of the ensemble is used to generate an image caption is tied to the image-dependent probability of the corresponding tag. In addition to captioning images, we also extend the SCN to generate captions for video clips. We qualitatively analyze semantic composition in SCNs, and quantitatively evaluate the algorithm on three benchmark datasets: COCO, Flickr30k, and Youtube2Text. Experimental results show that the proposed method significantly outperforms prior state-of-the-art approaches, across multiple evaluation metrics.", "text": "semantic compositional network developed image captioning semantic concepts detected image probability used compose parameters long short-term memory network. extends weight matrix lstm ensemble tag-dependent weight matrices. degree member ensemble used generate image caption tied image-dependent probability corresponding tag. addition captioning images also extend generate captions video clips. qualitatively analyze semantic composition scns quantitatively evaluate algorithm three benchmark datasets coco flickrk youtubetext. experimental results show proposed method signiﬁcantly outperforms prior state-of-the-art approaches across multiple evaluation metrics. recent surge interest developing models generate captions images videos termed visual captioning. approaches learn probabilistic model caption conditioned image video inspired successful encoder-decoder framework employed machine translation recent work visual captioning employs convolutional neural network encoder obtaining ﬁxed-length vector representation given image video. recurrent neural network typically implemented long short-term memory units employed decoder generate caption. recent work shows adding explicit high-level semantic concepts input image/video improve visual captioning. shown detecting explicit semantic concepts encoded image adding figure model architecture illustration semantic composition. triangle symbol represents ensemble tag-dependent weight matrices. number next semantic concept probability corresponding semantic concept presented input image. high-level semantic information cnn-lstm framework improved performance signiﬁcantly. specifically feeds semantic concepts initialization step lstm decoder. model semantic attention proposed selectively attends semantic concepts soft attention mechanism hand although signiﬁcant performance improvements achieved integration semantic concepts lstm-based caption generation process constrained methods; e.g. soft attention initializapaper propose semantic compositional network effectively assemble meanings individual tags generate caption describes overall meaning image illustrated figure similar conventional cnn-lstm-based image captioning framework used extract visual feature vector lstm generating image caption however unlike conventional lstm extends weight matrix conventional lstm ensemble tag-dependent weight matrices subject probabilities tags present image. tag-dependent weight matrices form weight tensor large number parameters. order make learning feasible factorize tensor three-way matrix product dramatically reduces number free parameters learned also yielding excellent performance. main contributions paper follows propose effectively compose individual semantic concepts image captioning. perform comprehensive evaluations image captioning benchmarks demonstrating proposed method outperforms previous state-of-the-art approaches substantial margin. example reported coco ofﬁcial test server achieve bleu- improvement points current published state-of-the-art extend proposed framework image captioning video captioning demonstrating versatility proposed model. also perform detailed analysis study showing model adjust caption smoothly modifying tags. focus recent neural-network-based literature caption generation relevant work. models typically extract visual feature vector send vector language model caption generation. representative works include image captioning video captioning. differences various methods mainly types architectures language models. example vanilla used lstm used visual feature vector ﬁrst time step used time step recently utilized attention-based mechanism learn focus image caption generation. work followed introduced review module improve attention mechanism proposed method improve correctness visual attention. moreover variational autoencoder developed image captioning. related work includes video captioning composing sentences describe novel objects. another class models uses semantic information caption generation. speciﬁcally applied retrieved sentences additional semantic information guide lstm generating captions applied semantic-concept-detection process generating sentences. addition also proposes deep multimodal similarity model project visual features captions joint embedding space. line methods represents current state image captioning. proposed model also lies category; however distinct aforementioned approaches model uses weight tensors lstm units. allows learning ensemble semantic-concept-dependent weight matrices generating caption. related distinct hierarchical composition recursive neural network model carries implicit composition concepts hierarchical relationship among concepts. figure illustrates semantic composition manifested model. specifically semantic concepts baby holding toothbrush mouth detected high probabilities. semantic concept turned model generate description covering part input image shown sentences figure however assembling semantic concepts able generate comprehensive description baby holding toothbrush mouth. interestingly shown sentences figure also great ﬂexibility adjust generation caption changing certain semantic concepts. tensor factorization method used make compact simplify learning. similar ideas exploited authors also brieﬂy discussed using tensor factorization method image captioning. speciﬁcally visual features extracted cnns utilized inferred scene vector used tensor factorization. contrast works semanticconcept vector formed probabilities tags weight basis lstm weight matrices ensemble. semantic-concept vector powerful visual-feature vector scene vector terms providing explicit semantic information image hence leading signiﬁcantly better performance shown quantitative evaluation. addition usage semantic concepts also makes proposed interpretable shown qualitative analysis since unit semantic-concept vector corresponds explicit tag. figure comparison proposed model conventional recurrent neural network caption generation. denote visual feature semantic feature respectively. represents special start-of-the-sentence token represents caption denotes hidden states. triangle symbol represents ensemble tagdependent weight matrices. extends weight matrix conventional ensemble tag-dependent weight matrices subjective probabilities tags present image. speciﬁcally scn-rnn computes hidden states follows ensembles tag-dependent weight matrices subjective probabilities tags present image according semantic-concept vector given deﬁne weight tensors rnh×nx×k rnh×nh×k number consider image associated caption ﬁrst extract feature vector often top-layer features pretrained cnn. henceforth simplicity omit explicit dependence represent visual feature vector length-t caption represented -of-v encoding vector size vocabulary. length typically varies among different captions. t-th word caption linearly embedded nx-dimensional real-valued vector wext rnx×v word embedding matrix i.e. column chosen one-hot probability caption given image feature vector deﬁned deﬁned special start-of-the-sentence token. words caption sequentially generated using end-of-the-sentence symbol generated. speciﬁcally conditional speciﬁed softmax recursively updated deﬁned zero vector weight matrix connecting rnn’s hidden state used computing distribution words. bias terms omitted simplicity throughout paper. without loss generality begin discussing simple transition function generalized section lstm. speciﬁcally deﬁned logistic sigmoid function represents indicator function. feature vector beginning i.e. deﬁned input matrix termed recurrent matrix. model illustrated figure developed based detection semantic concepts i.e. tags image test. order detect image ﬁrst select tags caption text training set. following common words training captions determine vocabulary tags includes frequent nouns verbs adjectives. order predict semantic concepts given test image motivated treat problem multi-label classiﬁcation task. suppose training examples label vector semantic concept. efﬁciently done sharing composing member ensemble. weight k-th slice tensor contributes parameters given image dependent respective probability k-th semantic concept inferred associated image number parameters basic model number parameters scn-rnn model experiments therefore additional number parameters increased model complexity also indicates increased training/testing time. scn-lstm rnns lstm units emerged popular architecture representational power effectiveness capturing long-term dependencies. generalize scn-rnn model using lstm units. speciﬁcally deﬁne tanh summary distinct previous image-captioning methods model unique utilize combine visual feature semantic-concept vector extracted image lstm initialize ﬁrst step expected provide lstm overview image content. lstm state initialized overall visual context ensemble sets lstm parameters utilized decoding weighted semantic-concept vector generate caption. k-th element denote k-th slice respectively. probability k-th semantic concept associated pair weight matrices implicitly specifying rnns total. consequently training model deﬁned interpreted jointly training ensemble rnns. shared among captions effectively capturing common linguistic patterns; diagonal term diag accounts semantic aspects image test captured analysis also holds true uabc. factorized model weight matrices correspond semantic concept share structure. factorized model illustrated figure similar decomposition manifested matrix wa·diag·wc interpreted k-th slice weight tensor slice corresponding semantic concepts hence decomposition effectively learn ensemble sets parameters framework readily extended task video captioning order effectively represent spatiotemporal visual content video two-dimensional threedimensional extract visual features video frames/clips. perform mean pooling process features features generate feature vectors representation video produced concatenating features. similarly also obtain semantic-concept vector running semantic-concept detector based video representation obtained employ model proposed directly video-caption generation described figure results three benchmark datasets coco flickrk youtubetext coco flickrk image captioning containing images respectively. image annotated least captions. pre-deﬁned splits datasets flickrk images validation test rest training; coco images used validation testing. tested model ofﬁcial coco test consisting images evaluated model coco evaluation server. also follow publicly available code preprocess captions yielding vocabulary sizes coco flickrk youtubetext used video captioning contains youtube clips video annotated around sentences. splits provided videos training videos validation videos testing. convert captions lower case remove punctuation yielding vocabulary size youtubetext. image representation take output pool layer resnet- pretrained imagenet dataset video representation addition using resnet- extract features video frame also utilize extract features video. pretrained sports-m video dataset take output -way layer video representation. consider frames videos input frames second. video frame resized resnet- feature extractor respectively. feature extractor applied video clips length frames overlap frames. procedure described section semantic concept detection. semantic-concept vocabulary size determined reﬂect complexity dataset coco flickrk youtubetext respectively. since youtubetext relatively small dataset found difﬁcult train reliable semantic-concept detector using youtubetext dataset alone limited amount data. experiments utilize additional training data coco. model training parameters scn-lstm initialized uniform distribution bias terms initialized zero. word embedding vectors initialized publicly available wordvec vectors embedding vectors words present table comparison published state-of-the-art image captioning models blind test reported coco test server. scn-lstm model. refers refers oriolvinyals refers captivator pretrained initialzied randomly. number hidden units number factors scn-lstm mini-batches size maximum number epochs three datasets gradients clipped norm parameter vector exceeds perform dataset-speciﬁc tuning regularization dropout early stopping validation sets. adam algorithm learning rate utilized optimization. experiments implemented theano testing beam search caption generation selects top-k best sentences time step considers candidates generate top-k best sentences next time step. beam size experiments. evaluation widely used bleu meteor rougel cider-d metrics reported quantitative evaluation performance proposed model baselines literature. metrics computed using code released coco evaluation server coco flickrk datasets besides comparing results reported previous work also reimplemented strong baselines comparison. results image captioning presented table models implemented follows. lstm-r lstm-t lstm-rt denotes using different features. speciﬁcally denotes resnet visual feature vector denotes tags denotes concatenation features standard lstm decoder initial time step. particular lstm-t model proposed lstm-rt resnet feature vector sent standard lstm decoder ﬁrst time step vector sent lstm decoder every time step addition input word. model similar without using semantic attention. model closest ours provides direct comparison proposed model. video captioning experiments notation. example lstm-c means leverage feature caption generation. quantitative results performance coco flickrk ﬁrst present results task image captioning summarized table tags provides better performance leveraging visual features alone combining tags visual features enhances performance expected. compared feeding tags lstm initial time step lstm-rt yields better results since takes input feature time step. further direct comparison lstm-rt scn-lstm demonstrates advantage proposed model indicating approach better method fuse semantic concepts lstm. also report results averaging ensemble identical scn-lstm models trained different initializations common strategy adopted widely obtain state-of-the-art results coco flickrk datasets. remarkably improve state-of-the-art bleu- score points coco. performance coco test server also evaluate proposed scn-lstm model uploading results onfigure detected tags sentence generation results coco. output captions generated scn-lstm scn-lstm-t scn-lstm model without visual feature inputs i.e. inputs. line coco test server. table shows comparison published state-of-the-art image captioning models blind test reported coco test server. include models published perform top- table. compared methods proposed scn-lstm model achieves best performance across evaluation metrics testing sets. performance youtubetext results video captioning presented table scn-lstm achieves signiﬁcantly better results competing methods metrics especially cider-d. self-comparison also worth noting model improves lstmcrt substantial margin. again using overaching ensemble enhances performance. figure shows three examples illustrate semantic composition caption generation. model properly describes image content using correctly detected tags. manually replacing speciﬁc tags model adcaption smoothly. example left image replacing grass model imagines laying bed. model also able generate novel captions highly unlikely occur real life. instance middle image replacing road street ocean model imagines driving ocean; right image replacing ﬁeld snow model dreams group zebras standing snow. picks tags well also selects right functional words different concepts form syntactically correct caption. illustrated sentence figure replacing baby girl generated captions changes baby little girl importantly changes mouth mouth. addition also infers underlying semantic relatedness different tags. illustrated sentence figure switching mouth generated caption becomes toothbrush indicating semantic closeness mouth toothbrush. switching baby generate detailed description baby brushing teeth. analysis shows importance tags generating captions. however generates captions using semantic concepts global visual feature vector. language model learns assemble semantic concepts consideration global visual information coherent meaningful sentence captures overall meaning image. order demonstrate importance visual feature vectors train anscn-lstm-t model scn-lstm model without visual feature inputs i.e. inputs shown ﬁrst example figure image tagger detects high probability. using inputs scn-lstm-t generate wrong caption laying stuffed animal. additional visual feature inputs scn-lstm model correctly replaces teddy bear present examples generated captions coco various methods figure along detected tags. seen model often generates reasonable captions lstm-r high-level semantic concepts. example ﬁrst image lstm-r outputs irrelevant caption image detection table library helps model generate sensible caption. further although model lstm-rt utilize detected tags caption generation model often depicts image content comprehensively; lstm-rt larger potential miss important details image. instance image appears caption generated model missed lstm-rt. observation might fact provides better approach fuse information process caption generation. similiar observations also found video captioning experiments demonstrated figure presented semantic compositional network framework effectively compose individual semantic meaning tags visual captioning. extends weight matrix conventional lstm three-way matrix product matrices dependent inferred tags. consequently viewed ensemble tag-dependent lstm bases contribution lstm basis unit proportional likelihood present image. experiments conducted three visual captioning datasets validate superiority proposed approach.", "year": 2016}