{"title": "Domain-Adversarial Neural Networks", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We introduce a new representation learning algorithm suited to the context of domain adaptation, in which data at training and test time come from similar but different distributions. Our algorithm is directly inspired by theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on a data representation that cannot discriminate between the training (source) and test (target) domains. We propose a training objective that implements this idea in the context of a neural network, whose hidden layer is trained to be predictive of the classification task, but uninformative as to the domain of the input. Our experiments on a sentiment analysis classification benchmark, where the target domain data available at training time is unlabeled, show that our neural network for domain adaption algorithm has better performance than either a standard neural network or an SVM, even if trained on input features extracted with the state-of-the-art marginalized stacked denoising autoencoders of Chen et al. (2012).", "text": "hana ajakan pascal germain hugo larochelle franc¸ois laviolette mario marchand d´epartement d’informatique g´enie logiciel universit´e laval qu´ebec canada d´epartement d’informatique universit´e sherbrooke qu´ebec canada authors contributed equally work. introduce representation learning algorithm suited context domain adaptation data training test time come similar different distributions. algorithm directly inspired theory domain adaptation suggesting that effective domain transfer achieved predictions must made based data representation cannot discriminate training test domains. propose training objective implements idea context neural network whose hidden layer trained predictive classiﬁcation task uninformative domain input. experiments sentiment analysis classiﬁcation benchmark target domain data available training time unlabeled show neural network domain adaption algorithm better performance either standard neural network even trained input features extracted state-of-theart marginalized stacked denoising autoencoders chen cost generating labeled data learning task often obstacle applying machine learning methods. thus great incentive develop ways exploiting data problem generalizes another. domain adaptation focuses situation data generated different somehow similar distributions. example context sentiment analysis written reviews might want distinguish positive negative ones. might labeled data reviews type products might want able generalize reviews products domain adaptation tries achieve transfer exploiting extra unlabeled training data problem wish generalize main approach achieve transfer learn classiﬁer representation favor transfer. large body work exists training classiﬁer representation linear however recent research shown non-linear neural networks also successful speciﬁcally variant denoising autoencoder known marginalized stacked denoising autoencoders demonstrated state-of-the-art performance problem. learning representation robust input corruption noise able learn representation also stable across changes domain thus allow cross-domain transfer. paper propose control stability representation domains explicitly neural network learning algorithm. approach motivated theory domain adaptation suggests good representation cross-domain transfer algorithm cannot learn identify domain origin input observation. show principle implemented neural network learning objective includes term network’s hidden layer working adversarially towards output connections predicting domain membership. neural network simply trained gradient descent objective. success domain-adversarial neural network conﬁrmed extensive experiments real world datasets. particular show dann achieves better performances regular neuben-david suggested that even generally hard compute exactly easily approximate running learning algorithm problem discriminating source target examples. construct dataset thus given test error problem discriminating source target examples proxy a-distance given experiments section paper compute value following approach glorot chen i.e. train linear subset dataset obtained classiﬁer error subset value equation work ben-david also showed h-divergence upper bounded empirical estimate plus constant complexity term depends dimension size samples combining result similar bound source risk following theorem obtained. theorem hypothesis class dimension probability choice samples every network sentiment analysis classiﬁcation benchmark. moreover show dann reach state-of-the-art performance taking input representation learned msda conﬁrming minimizing domain discriminability explicitly improves relying representation robust noise. domain adaptation consider binary classiﬁcation tasks input space label set. moreover different distributions called source domain target domain domain adaptation learning algorithm provided labeled source sample drawn i.i.d. unlabeled target sample drawn i.i.d. marginal distribution paper focus h-divergence used ben-david based earlier work kifer deﬁnition kifer given domain distributions hypothesis class h-divergence between proved that symmetric hypothesis class compute empirical h-divergence consider class hyperplanes representation space. inspired proxy a-distance suggest estimating part equation logistic regressor model probability given input source domain line theorem optimization problem implements trade-off minimization source risk divergence ˆdh. hyperparameter used tune trade-off quantities learning process. previous result tells term i.e. exists classiﬁer achieve risk distributions. also tells that classiﬁer small given class ﬁxed dimension learning algorithm minimize trade-off source risk empirical h-divergence pointed-out ben-david strategy control h-divergence representation examples source target domain indistinguishable possible. representation hypothesis source risk will according theorem perform well target data. paper present algorithm directly exploits idea. originality approach explicitly implement idea exhibited theorem neural network classiﬁer. learn model generalize well domain another ensure internal representation neural network contains discriminative information origin input preserving risk source examples. note component denotes conditional probability neural network assigns class given training source sample recently non-linear representations become increasingly studied including neural network representations notably state-of-theart msda literature mostly focused exploiting principle robust representations based denoising autoencoder paradigm contribution work show domain discriminability another principle complimentary robustness improve crossdomain adaptation. equation involves maximization operation. hence neural network domain regressor competing other adversarial term. obtained domain adversarial neural network illustrated figure dann hidden layer maps example representation output layer accurately classiﬁes source sample domain regressor unable detect example belongs source sample target sample. optimize equation option would follow hard-em approach would alternate between optimizing convergence adversarial parameters regular neural network parameters however we’ve found simpler stochastic gradient descent approach sufﬁcient works well practice. here approach consists sampling pair source target example updating gradient step update parameters dann. crucially update regular parameters follows usual opposite direction gradient adversarial parameters step must follow gradient’s direction algorithm detailed algorithm pseudocode represent one-hot vector consisting except position also element-wise product. experiment described paper used early stopping stopping criteria split source labeled sample training remaining validation stop learning process risk minimal. distinguishes work domain adaptation literature dann’s inspiration theoretical work ben-david indeed dann directly optimizes notion h-divergence. note work huang yates representations learned word tagging using posterior regularizer also inspired ben-david al.’s work. addition tasks different would argue dann learning objective closely optimizes hdivergence huang yates relying cruder approximations efﬁciency reasons. idea learning representations indiscriminate auxiliary label also explored contexts. instance zemel proposes notion fair representations indiscriminate whether example belongs identiﬁed groups. resulting algorithm different dann directly derived h-divergence. finally mention related research using adversarial formulation learn model classiﬁer neural network data. work learning linear classiﬁers robust changes input distribution based minimax formulation work however assumes good feature representation input linear classiﬁer available doesn’t address problem learning also note work goodfellow propose generative adversarial networks learn good generative model true data distribution. work shares dann adversarial objective applying instead unsupervised problem generative modeling. ﬁrst experiment study behavior proposed dann algorithm variant inter-twinning moons problem target distribution rotation source distribution. source sample generate lower moon upper moon labeled respectively containing examples. target sample obtained generating sample rotating example thus contains unlabeled examples. figure examples represented +and examples represented black dots. size neurons. even train using procedure dann. keep updating domain regressor component using target sample disable adversarial back-propagation hidden layer. execute algorithm omitting lines numbered obtain learning algorithm based source risk minimization equation simultaneously train domain regressor equation discriminate source target domains. using experiment ﬁrst illustrate dann adapts decision boundary compared moreover also illustrate representation given hidden layer less adapted domain task dann results illustrated figure graphs part relate standard graphs part relate dann. looking corresponding graphs column compare dann four different perspectives described detail below. label classiﬁcation. ﬁrst column figure shows decision boundaries dann problem predicting labels source target examples. expected accurately classiﬁes classes source sample fully adapted target sample contrary decision boundary dann perfectly classiﬁes examples source target samples. dann clearly adapts target distribution. representation pca. analyze domain adaptation regularizer affects representation provided hidden layer second column figure presents principal component analysis representations source target data points i.e. thus given trained network every point mapped -dimensional feature space hidden layer projected back two-dimensional plane deﬁned ﬁrst principal components. dann-pca representation observe target points homogeneously spread among source points. nn-pca representation clusters target points containing source points clearly visible. hence task labeling target points seems easier perform dann-pca representation. push analysis further four crucial data points identiﬁed graphs ﬁrst column represented graphs second column. observe points close nn-pca representation clearly belong different classes. happens points conversely four points located opposite corners dann-pca representation. note also target point difﬁcult classify original space located +cluster dann-pca representation. therefore representation promoted dann suited domain adaptation task. domain classiﬁcation. third column figure shows decision boundary domain classiﬁcation problem given domain regressor equation precisely classiﬁed source example classiﬁed domain example otherwise. remember that learning process dann regressor struggles discriminate source target domains hidden representation adversarially updated prevent succeed. explained above trained domain regressor learning process without allowing inﬂuence learned representation hand dann domain regressor utterly fails discriminate source target distributions. hand domain regressor shows better discriminant. corroborates dann representation doesn’t allow discriminating domains. hidden neurons. plot last column figure lines show decision surfaces hidden layer neurons words ﬁfteen plot line corresponds points component equals observe neurons grouped three clusters allowing generate straight line part curved decision boundary label classiﬁcation problem. however neurons also able capture rotation angle domain classiﬁcation problem. hence observe adaptation regularizer dann prevents kinds neurons produced. indeed striking predominant patterns neurons absent among dann neurons. section compare performance proposed dann algorithm standard neural network hidden layer described equation support vector machine linear kernel. select hyper-parameters algorithms grid search small validation consists labeled examples target domain. finally select classiﬁers lowest target validation risk. source name target name dann books books electronics books kitchen books electronics kitchen electronics books electronics electronics kitchen kitchen books kitchen kitchen electronics perform twelve domain adaptation tasks. example books corresponds task books source domain disks target one. learning algorithms given labeled source examples unlabeled target examples. then evaluate separate target test sets note don’t unlabeled target sample learning. details procedure used learning algorithms. dann. adaptation parameter chosen among values logarithmic scale. hidden layer size either finally learning rate ﬁxed exactly hyper-parameters training procedure dann above except don’t need adaptation parameter. note train using dann implementation svm. hyper-parameter chosen among values logarithmic scale. range values used chen experiments. original data part table shows target test risk algorithms table reports probability algorithm signiﬁcantly better anaccording poisson binomial test note dann signiﬁcantly better performance respective probabilities difference dann wonder whether dann algorithm improve representation learned state-of-theart marginalized stacked denoising autoencoders proposed chen brief msda unsupervised algorithm learns robust feature representation training samples. takes unlabeled parts source target samples learn feature input space representation space. denoising autoencoder ﬁnds feature representation reconstruct original features example noisy counterpart. chen showed using msda linear classiﬁer gives state-of-the-art performance amazon reviews datasets. alternative propose apply dann algorithm representations generated msda note that even msda dann representation learning approaches optimize different objectives complementary. perform experiment amazon reviews dataset described previous subsection. pair source-target generate msda representations using corruption probability number layers execute three learning algorithms representations. precisely following experimental procedure chen concatenation output layers original input representation. thus example encoded vector figure proxy a-distances note values msda representations symmetric swapping source target samples. also dann results used msda values lower doesn’t appear figure dimensions. note grid search subsection learning rate dann results msda representation columns table conﬁrm combining msda dann sound approach. indeed poisson binomial test shows dann better performance probabilities respectively reported table theoretical foundation dann domain adaptation theory ben-david claimed dann ﬁnds representation source target example hardly distinguishable. experiment section already points evidences want conﬁrm real data. compare proxy a-distance various representations amazon reviews dataset. representations obtained running either dann msda msda dann combined. recall described section metric estimating similarity source target representations. precisely obtain value following procedure construct dataset equation using source target representations training samples; randomly split subsets equal size; train linear svms ﬁrst subset using large range values; compute error obtained classiﬁers second subset lowest error compute value equation sentations standard representations. inﬂuenced hidden layer size size neurons algorithms. also adaptation parameter dann value selected time preceding experiments amazon reviews dataset. again dann clearly leading lowest values. lastly figure presents sets results related section experiments. hand reproduce results chen noticed msda representations gave greater values obtained original data. although msda approach clearly helps adapt target task seems contradict theory ben-david al.. hand observe that running dann msda obtained representations much lower values. observations might explain improvements provided dann combined msda procedure. paper proposed neural network algorithm named dann strongly inspired domain adaptation theory ben-david main idea behind dann encourage network’s hidden layer learn representation predictive source example labels uninformative domain input extensive experiments inter-twinning moons problem amazon reviews sentiment analysis dataset shown effectiveness strategy. notably achieved state-of-the-art performances combining dann msda autoencoders chen turned goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. nips huang yates alexander. biased representation learning domain adaptation. joint conference empirical methods natural language processing computational natural language learning believe domain adaptation regularizer develop dann algorithm incorporated many learning algorithms. natural extensions work would deeper network architectures multi-source adaptation problems learning tasks beyond basic binary classiﬁcation setting. also intend meld dann approach denoising autoencoders potentially improve steps procedure section baktashmotlagh mahsa harandi mehrtash lovell brian salzmann mathieu. unsupervised domain adaptation domain invariant projection. proceedings ieee international conference computer vision ben-david shai blitzer john crammer koby kulesza alex pereira fernando vaughan jennifer wortman. theory learning different domains. machine learning blitzer john mcdonald ryan pereira fernando. domain adaptation structural corresponconference empirical methods dence learning. natural language processing bruzzone lorenzo marconcini mattia. domain adaptation problems dasvm classiﬁcation technique circular validation strategy. transaction pattern analysis machine intelligence germain pascal habrard amaury laviolette franc¸ois morvant emilie. pac-bayesian approach domain adaptation specialization linear classiﬁers. icml", "year": 2014}