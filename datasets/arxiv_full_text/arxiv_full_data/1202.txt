{"title": "Massively Deep Artificial Neural Networks for Handwritten Digit  Recognition", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Greedy Restrictive Boltzmann Machines yield an fairly low 0.72% error rate on the famous MNIST database of handwritten digits. All that was required to achieve this result was a high number of hidden layers consisting of many neurons, and a graphics card to greatly speed up the rate of learning.", "text": "however desktop graphics processing units become faster large amount on-board memory allows possibility training large rbms quickly efﬁciently. mnist database contains digits examples shown figure standard mnist dataset comprised sets training testing common split data sets images used training images kept validation. network trained standard mnist digits. pixel intensities standard mnist data range pixels mnist images mapped real input values pixelintensity layer artiﬁcial neural network. training done simplistic restrictive boltzmann machine containing hidden layers shifting numbers hidden units. number hidden units layer typically shrank toward output layer. momentum dropout. learning rate vary epoch starting leading downwards weights initially uniform random distribution decay weights neuron’s activation function scaled hyperbolic tangent abstract—greedy restrictive boltzmann machines yield fairly error rate famous mnist database handwritten digits. required achieve result high number hidden layers consisting many neurons graphics card greatly speed rate learning. task recognising handwritten digits great interest academic commercial application. existing contemporary algorithms already adept learning recognise handwritten digits used within post ofﬁces automated letter sorting. mnist database handwritten digits understood popular benchmark form pattern recognition task. years class artiﬁcial neural networks called restrictive boltzmann machines amongst ﬁrst initial classiﬁers tested mnist data rbms variant standard boltzmann machines restriction neurons must form bipartite graph; pair visible hidden units used respectively. restricted boltzmann machines commonly used formulate deep neural networks. deep belief networks formed stacking rbm’s ﬁnetuning network using gradient descent optimisation algorithm backpropagation. ﬁrst documented convolutional neural networks achieved world-record error rate given task classifying mnist digits. recently better results obtained pre-training hidden layer unsupervised manner achieving incredibly error rate extremely resource heavy time consuming. online backpropagation thousands epochs large rbms could take months even newest standard off-the-shelf desktop microprocessors. option could parallelise workload across computing cluster latency issues individual computers prove difﬁcult overcome. multi-threading multi-core processor difﬁcult simard steinkraus platt. best practices convolutional neural networks applied visual document analysis. international conference document analysis recognition volume pages ieee computer society parallel distributed processing explorations microstructure cognition vol. chapter information processing dynamical systems foundations harmony theory pages press cambridge previously program create graphical operations using technologies directx opengl. despite limitations people still able hard code implement number gpu-based artiﬁcial neural networks. added complexity networks typically shallow. noticeable modest speedup observed gpus. nvidia announce ﬁrst foray scientiﬁc computing cuda c-like programming language scientiﬁc use. gpus greater amount pure processing speed memory bandwidth compared microprocessors; allows quick effective implementations. tests computer intel .ghz processor nvidia graphics card gddr memory. used accelerate performance forward propagation backpropagation routines. trained lowest validation error selected used evaluate performance mnist test set. results summarised table best neural network error rate investigation proved majority misclassiﬁed digits feature main attributes meaning even human perception difﬁcult correctly identify. best test error particular even lower identiﬁed maximum capacity network. obvious performance increases greatly adding hidden layers units hidden layer. example network table greatly push forward boundaries machine learning techniques. modern-day gpus already times faster standard general purpose multiprocessors faced task training deep artiﬁcial neural networks. extremely difﬁcult mnist handwritten benchmark standard shelf gpu-based neural networks surpassed previously reported results including scores obtained using complex specialised architectures. course approach limited task classifying handwritten digits holds great promise pattern recognition tasks. work begun course authors undergraduate dissertation. would like thank project supervisor chuan guidance adam gibson providing deeplearningj deep learning framework.", "year": 2015}