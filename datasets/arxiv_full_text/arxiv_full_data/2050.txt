{"title": "Learning Discriminative Metrics via Generative Models and Kernel  Learning", "tag": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "abstract": "Metrics specifying distances between data points can be learned in a discriminative manner or from generative models. In this paper, we show how to unify generative and discriminative learning of metrics via a kernel learning framework. Specifically, we learn local metrics optimized from parametric generative models. These are then used as base kernels to construct a global kernel that minimizes a discriminative training criterion. We consider both linear and nonlinear combinations of local metric kernels. Our empirical results show that these combinations significantly improve performance on classification tasks. The proposed learning algorithm is also very efficient, achieving order of magnitude speedup in training time compared to previous discriminative baseline methods.", "text": "metrics specifying distances data points learned discriminative manner generative models. paper show unify generative discriminative learning metrics kernel learning framework. speciﬁcally learn local metrics optimized parametric generative models. used base kernels construct global kernel minimizes discriminative training criterion. consider linear nonlinear combinations local metric kernels. empirical results show combinations signiﬁcantly improve performance classiﬁcation tasks. proposed learning algorithm also efﬁcient achieving order magnitude speedup training time compared previous discriminative baseline methods. metric learning learning specify distances data points topic much interest machine learning recently. example discriminative techniques metric learning improve performance classiﬁer k-nearest neighbor classiﬁer training set. general strategy techniques reduce distances data points belonging class increasing distances data points different classes framework mahalanobis metric parameterized positive deﬁnite matrix metric learning performed using semi-deﬁnite programming involving constraints pairs triplets data points training asymptotic limit performance nearest neighbor classiﬁers approach theoretical limit bounded twice bayes optimum error rate independent underlying metric used ﬁnite sampling case performance nearest neighbor classiﬁer depend upon choice metric showed bias term estimated using simple class-conditional generative models data. generative local metric optimized minimize bias term. however local metric learning algorithm several shortcomings. first local metric needs computed every point difﬁcult calculate geodesic distance pairs distant points. also unclear correlate choice generative models discriminative classiﬁer performance. paper address issues combining learned local metrics global discriminative kernel thus reducing computational costs classifying points. approach viewed using metric learning deﬁne base kernels combined discriminately base kernels derived parametric generative models thus reaping beneﬁts generative discriminative models show simple linear nonlinear combinations result highly discriminative global kernel outperforms competing methods signiﬁcantly number machine leaning datasets. moreover show approach also computationally efﬁcient methods often achieving orders magnitude speedup training time. section review previous discriminative generative metric learning techniques. describe approach combining local metrics trained generative models section present extensive empirical studies section followed discussion method future direction section brieﬂy review techniques learning metrics. start discriminative metric learning using large margin nearest neighbor classiﬁer illustrative example next examine learning generative local metric exploits information parametric generative models explicity attempt minimize classiﬁcation errors. discriminative learning metric consider nearest neighbor classiﬁer labels d-dimensional data point label nearest neighbor supervised training order identify nearest neighbors distances data points need determined. special case general mahalanobis distance mahalanobis metric rd×d equal d-dimensional identity matrix. paper follow popular terminology metric learning literature calling squared distance distance. general positive semideﬁnite matrix factor ltl. implies general arguably performance nearest neighbor classiﬁers depends critically metric good intuitively pull data points class closer push data points different classes away. general criteria discriminative methods metric learning example large margin nearest neighbor classiﬁer casts learning convex optimization problem. point training differentiates sets neighboring data whose labels points target points different lmnn identiﬁes optimal solution objective function balances forces pulling targets towards pushing impostors away distance impostor greater distance target minimum margin using slack variables ξijl. note formulation lmnn makes assumptions data distributed. additionally optimization criterion directly related learned metric used classiﬁcation. approach contrasts sharply generative model approach describe next. consider binary classiﬁcation problem labels assume training data points drawn class conditional distributions asymptotic limit error rate nearest neighbor classiﬁer given terms class conditional distributions asymptotic error easily shown invariant linear transformation variables implies learning different metric effect error rate asymptotic limit. solution apparent paradox described showed number training points ﬁnite error rate nearest neighbor classiﬁer deviates asymptotic error rate ﬁnite bias term constant factor tends zero approaches inﬁnity scalar laplacian trace hessian ∇∇p. bias term depend upon choice underlying metric linear transformation bias term given integral generative local metric algorithm optimizes local metric minimize local bias term ﬁnite sample error rate approaches asymptotic error rate ﬁrst-order approximation. resulting optimization semideﬁnite constraints easily solved data point using spectral decomposition. optimum positive semideﬁnite matrix whose eigenvectors φi’s. diagonal matrix composed φi’s positive eigenvalues corresponding diagonal matrix negative proportionality constant determined scaling determinant learning algorithm attempt reduce nearest neighbor classiﬁcation error rate explicitly. prior empirical studies shown generative learning metric performs competitively even compared discriminative methods large margin nearest neighbor classiﬁers however several shortcomings. every data point classiﬁed optimization needs solved. resulting metric depends distances training data points need computed speciﬁc metric. specialized data structures exploited speed process identifying nearest neighbors structures usually require ﬁxed metric cannot easily adapted metric signiﬁcantly increase computational cost testing time. secondly performance depends speciﬁc form class conditional distributions used generative modeling. initial studies suggested even simplistic models gaussian distributions attains robust competitive performance. nevertheless rigorously quantifying relationship assumed generative models classiﬁcation performance lacking. particular unclear choice generative models adapted order improve classiﬁcation performance nearest neighbor classiﬁer learned metric. address issues viewing problem learning metrics learning kernels. investigate improve classiﬁcation performance using kernels. consider schemes learning kernels discriminatively linear nonlinear combination metrics. note learned locally neighborhood treat biased estimate global kernel function space training examples. arrive unbiased estimator global metric intuition made clear linearly combining local kernels learned training samples constrained nonnegative guaranteeing combination coefﬁcients {αi}n resulting kernel positive semideﬁnite. global metric simply convex combination local metrics. consider simplest convex combination uniform averaging computed space proportionally identity matrix. thus average euclidean distance based nearest neighbor classiﬁcation perform well space. formally theorem assume class conditional distribution gaussian every class. local metric computed minimizing bias term space then uniform convex combination metric induces linear transformation furthermore denote local metric computed space class conditional distribution have note combined kernel highly nonlinear albeit convex function local metrics. well-known positive semideﬁnite kernels including represented distances corresponding reproducing kerner hilbert space however opposed global metric cannot represent distance closed-form function {mi}n typical applications often chooses gaussian kernels identity covariance matrices exp. difﬁculty properly choosing non-identity covariance matrices base kernels especially high-dimensional problems. formulation overcomes challenge using non-euclidean metrics computed generative modeling. reﬁne combination optimizing {αil} discriminatively. speciﬁcally coefﬁcients {αil} adjusted kernel achieves lowest empirical risk used kernel based classiﬁers support vector machines aspect formulation reaps beneﬁts generative modeling discriminative training. wonder framework multiple kernel learning used nonlinear combination metrics section used discriminatively optimize convex combination coefﬁcients preliminary results indicate general performs well. consistent previous extensive work combining kernels linearly discriminative learning combinations reliably outperform simpler strategies combinations including uniform combination present experimental details including forms convex combinations appendix found computationally appealing empirically effective. computational complexity algorithms dominated calculation local metrics main calculation involved diagonalizing matrix d-dimensional space computational cost since local metrics computed every training sample total computational cost computing adds little overhead. contrast discriminative techniques lmnn learning single global metric require iterative numerical optimization. lmnn optimization needs examine roughly number approaches greatly outperform constraints. large small moderate lmnn speed demonstrated later section compare methods discriminative kernel learning generative local metrics competitive methods metric learning. report results applying simple linear nonlinear combinations classiﬁcation. comprehensive details included appendix datasets used datasets -normal iris wine heart vehicle ionosphere image german mnist letters. ﬁrst datasets small-scale data points dimensionality ranging number labelled classes range mnist letters datasets substantially larger mnist deskewed images classes letters examples labelled classes. -normal synthetic containing mixture gaussians. datasets downloaded machine learning repository benchmark repository data small-scale datasets preprocessed feature vector components range supervised learning tasks classiﬁcation dataset randomly split times training validation testing sets. mnist images resolution pixels preprocessed reducing dimensionality respectively save training time prevent overﬁtting. perform random splits samples training validation testing. letters dataset perform random splits samples training validation testing. letters-scaled features scaled within range also provide experimental results unscaled version training time lmnn sensitive scaling dataset. euclidean. k-nearest neighbor classiﬁer using euclidean distances. lmnn. classiﬁer using metric large margin nearest neighbor glmint. classiﬁer using generative local metrics gaussian distrituning parameters methods optimized validation sets overall performance reported test sets. tunable parameters include number nearest neighbors interpolation parameter glmint margins used energy-based classiﬁcation. used lmnn implementation reported performance classiﬁcation tasks table displays averaged misclassiﬁcation error rates small-scale data sets. standard errors reported appendix last column averaged ranking performance smaller number better performance average. glmint outperforms lmnn euclidean sets though performance surpassed lmnne. however best performers simple strategy uniformly combining generative local metrics. table displays averaged error rates largescale datasets. standard errors reported appendix mnist dataset report results several pca-preprocessed dimensionality. letters report results cases using scaled unscaled features. lmnne perform better methods dimensionality however larger dimensionality lmnn lmnne outperform methods including approaches. possible explanation that increased dimensionality generative modeling used glmint approaches data properly. hand discriminative training might able overcome problem better regularization. summary observe methods computationally efﬁcient achieving orders magnitude speedup training time. example mnist- lmnn takes minutes learn ﬁnal metric algorithm takes minutes. also report results nonlinear combination generative local metrics using framework discriminative kernel learning described section baseline system learns kernel following bandwidth kernel. method energy point assigned class deﬁned differences quantities distances point nearest neighbor class distances point nearest neighbor classes point assigned class label lowest energy. energy-based classiﬁcation improve performance signiﬁcantly classiﬁcation discriminative kernel learning replaces euclidean distance conventional gaussian kernel generative local metrics many local metrics number training examples. thus implementation regional metrics speciﬁcally partition training data parts. average local metrics data points part obtain speciﬁc case uniform linearly regional metrics {mp}p combined metric. baseline method approach kernel learning combination coefﬁcients optimized simplemkl algorithm table displays averaged misclassiﬁcation error rates small-scale datasets. experiments datasets ongoing. experiment different aggregate results reporting best performing table details method’s performance different provided appendix section datasets nonlinear combination metrics clearly outperforms baseline signiﬁcant improvement datasets -normal iris vehicle; however method performs poorly ionosphere dataset. note table local metrics used alone attain better error rate best nonlinear kernel method thus analysis still needed understand effective methods nonlinear combinations local metrics. many unsupervised learning problems clustering dimensionality reduction also rely upon proper metric calculate distances. also investigated apply algorithms learn metrics unsupervised problems. crucial step extract discriminative information unlabeled data algorithms compute better metrics. address issue developed em-like procedure iterate inferring labels computing local global metrics. details presented appendix applied procedure number unsupervised learning problems. achieve signiﬁcantly better performance standard approaches clustering. additionally exploit learned metric dimensionality reduction instance learning nonlinear manifold structure data. illustrate example show beneﬁts using global metric algorithm isomap particular compare isomap embedding results computed euclidean metric results metric mnist dataset. selected random samples different digits resized images fig. plots different low-dimensional embeddings data samples colored according digit identities. clearly shows learning global metric helps discover better embedding exhibits clear clustering structure among different class identities. context metric learning proposed several approaches reap beneﬁts discriminative training generative modeling. method builds upon connection kernel learning framework using learned positive semideﬁnite metrics generative models base kernels. empirical studies validate algorithms improving classiﬁcation performance across variety datasets well computational efﬁciency implementation. ongoing work includes investigations effective approaches training nonlinear combinations learned local metrics discriminative manner.", "year": 2011}