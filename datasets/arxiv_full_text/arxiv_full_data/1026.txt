{"title": "A Cheap Linear Attention Mechanism with Fast Lookups and Fixed-Size  Representations", "tag": ["cs.LG", "cs.IR", "cs.NE", "stat.ML"], "abstract": "The softmax content-based attention mechanism has proven to be very beneficial in many applications of recurrent neural networks. Nevertheless it suffers from two major computational limitations. First, its computations for an attention lookup scale linearly in the size of the attended sequence. Second, it does not encode the sequence into a fixed-size representation but instead requires to memorize all the hidden states. These two limitations restrict the use of the softmax attention mechanism to relatively small-scale applications with short sequences and few lookups per sequence. In this work we introduce a family of linear attention mechanisms designed to overcome the two limitations listed above. We show that removing the softmax non-linearity from the traditional attention formulation yields constant-time attention lookups and fixed-size representations of the attended sequences. These properties make these linear attention mechanisms particularly suitable for large-scale applications with extreme query loads, real-time requirements and memory constraints. Early experiments on a question answering task show that these linear mechanisms yield significantly better accuracy results than no attention, but obviously worse than their softmax alternative.", "text": "softmax content-based attention mechanism proven beneﬁcial many applications recurrent neural networks. nevertheless suffers major computational limitations. first computations attention lookup scale linearly size attended sequence. second encode sequence ﬁxed-size representation instead requires memorize hidden states. limitations restrict softmax attention mechanism relatively small-scale applications short sequences lookups sequence. work introduce family linear attention mechanisms designed overcome limitations listed above. show removing softmax non-linearity traditional attention formulation yields constant-time attention lookups ﬁxed-size representations attended sequences. properties make linear attention mechanisms particularly suitable large-scale applications extreme query loads realtime requirements memory constraints. early experiments question answering task show linear mechanisms yield signiﬁcantly better accuracy results attention obviously worse softmax alternative. many large-scale applications particular among information retrieval tasks require efﬁcient algorithms compress documents query them. example test time systems process millions queries simultaneously real-time. content-based attention mechanism recently introduced architecture allows system focus particular parts document depending query. proven beneﬁcial many applications deep learning expensive computations often prevent used large-scale applications. work introduce family linear attention mechanisms overcome limitations still offer extent beneﬁts traditional attention mechanism. notations represent document sequence tokens consider queries document. represent queries encoded column vector representation document processed recurrent neural network which timestep computes hidden state size matrix composed hidden states document stacked vertically i.e. whose mechanism involves matrix multiplications result overall complexity single query lookup. instead considering single query would like process queries complexity would large complexity prohibitive restricts scale potential applications. furthermore classic softmax attention mechanism allow store ﬁxed-size representation document instead hidden states network stored resulting variable-size representation requires memory space. also prohibitive large. machine translation document would source sentence translated composed words. translated sentence generated iteratively timestep attention lookup performed. number words translated sequence corresponds number required attention lookups. thus generated word attention lookup performed. signiﬁcantly slow translation long sentences prevent real-time translations. question answering document usually text document words. query question document might questions document. practice undeﬁned. cost current softmax attention mechanisms prevent real-time question answering many users. information retrieval tasks document represent long sequence query could single question fact implicitly contained documents classic softmax attention mechanism would require scanning words every document searched query. network architectures external memory represents memory queried. current attention mechanism limit size memory number queries. seems particularly important develop efﬁcient memory mechanisms. possibly would memory architecture whose memory size scale linearly number facts stored. another would linear size memory sublinear query algorithm. note form found memory networks forms common particular introduced bahdanau present particular form similar cheap mechanism introduce next section. test time computational complexity independent document size opposition complexity current attention mechanisms. cheap attention would little overhead compared recurrent model attention linear attention mechanism introduce next section satisﬁes requirements allowing potentially tackle problems much larger scale. expected early experiments show computational gains come price slightly worse accuracy softmax attention mechanism deﬁnitively better attention. section introduce simplest version linear attention mechanism; sophisticated additions described next section. linear attention mechanism results removal softmax leading following linear attention mechanism square matrix dimension represents non-centered covariance matrix hidden states computed complexity. importantly depends document implies computed once attention lookup cost i.e. complexity independent length document sequence. queries resulting attention complexity would i.e. speedup compared classic softmax attention mechanism furthermore document summarized matrix ﬁxed-size representation size instead matrix hidden states required softmax attention. note memory improvement case suitable store rather singular matrix rank notice seen non-centered covariance matrix hidden states. although complexity computing still linear size sequence computation done single time document contrasts classic attention mechanism scan document query table summarizes computational memory beneﬁts using linear attention mechanisms compared original softmax attention. forward encoding pass slightly expensive linear attention mechanism perform outer product timestep update matrix table comparison traditional softmax mechanism linear mechanism computational cost attention lookup memory requirements store encoded document computational cost encoding document backpropagation requires know intermediate values timestep. instead storing forward pass would prohibitive memory-wise incrementally recompute starting ﬁnal matrix invert successive transformations. theano implementations backward pass code experiments available github repository. experiments below particular instance general model above call gated linear attention. deﬁned sigmoid element-wise product. words network capacity control information adds matrix full mechanism written figure comparison validation accuracies obtained different attention mechanisms question-answering dataset. observe expected softmax attention mechanism yields best accuracy linear mechanisms signiﬁcantly better attention gated linear attention signiﬁcantly better basic linear attention models attention faster converge probably skip connections introduced attention mechanism. cheap linear attention mechanism designed large-scale applications large number queries document real-time. research datasets really suitable highlight computational efﬁciency practice. therefore focus comparing accuracy results. evaluated basic gated versions linear attention mechanism question answering task. used dataset released hermann composed cloze style questions documents words average. questions document. reach state results simply compare different versions attention. such used simple architecture requires hours train. ﬁxed architecture experiments models differ attention part. precisely common architecture composed single-layer network encode query separate single-layer network encode document. used adam train networks. networks chose small hidden size word embeddings size test time optimized implementation yield speedup n∗k∗m attention lookup. however stage interested accuracy results comparison rather speed. speedup would better illustrated applications large number queries document relatively long documents public datasets still rare. early experiments question-answering suggest linear mechanisms gated extensions signiﬁcantly improve models attention. expected accuracy results softmax attention better reduced adding non-linear gates basic linear mechanism. believe sophisticated extensions could improve results. terms memory linear attention mechanisms seen trade-off no-attention models classic softmax models. compress document sequence representations store information k-length vector last hidden state classic recurrent network obviously less stored hidden states softmax attention mechanism. probably suitable tasks relatively long sequences extremely high number lookups. nevertheless extremely long sequences believe ﬁxed-size representations capture enough information research focus sublinear adaptative depending much information contained sequence) representations. representation store information k-length vector also acts skip connections past hidden states output. result observed capture longer term dependencies training optimization easier less prone vanishing gradient problem. potential extension cheap mechanism interleave updates create ﬂavor recurrent unit uses second order information past hidden states seen non-centered covariance matrix). recurrent unit would take input previous hidden state current input also product evaluates extent much already stored introduced family attention mechanisms called linear attention mechanisms which little computational overhead yield better easier optimize models compared standard recurrent networks attention. constant attention lookup complexity memory requirements make appealing alternatives build large-scale information retrieval systems computational costs traditional softmax attention mechanisms prohibitive. precisely believe linear attention mechanisms would suitable large-scale tasks three properties note baseline model without attention hermann concatenated question document. despite improving performance approach allow compute representation document independent query therefore encoded query document independent networks. many attention lookups traditional softmax attention mechanisms would slow. particularly important real-time systems process extremely large loads queries simultaneously hermann karl moritz koˇcisk´y tom´aˇs grefenstette edward espeholt lasse will suleyadvances mustafa blunsom phil. teaching machines read comprehend. neural information processing systems http//arxiv.org/abs/ sukhbaatar sainbayar szlam arthur weston jason fergus rob. end-to-end memory networks. cortes lawrence sugiyama garnett advances neural information processing systems curran associates inc. http//papers.nips.cc/paper/-end-to-end-memory-networks. pdf.", "year": 2016}