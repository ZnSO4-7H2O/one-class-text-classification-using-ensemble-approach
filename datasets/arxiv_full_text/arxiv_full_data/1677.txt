{"title": "Inference by Minimizing Size, Divergence, or their Sum", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "We speed up marginal inference by ignoring factors that do not significantly contribute to overall accuracy. In order to pick a suitable subset of factors to ignore, we propose three schemes: minimizing the number of model factors under a bound on the KL divergence between pruned and full models; minimizing the KL divergence under a bound on factor count; and minimizing the weighted sum of KL divergence and factor count. All three problems are solved using an approximation of the KL divergence than can be calculated in terms of marginals computed on a simple seed graph. Applied to synthetic image denoising and to three different types of NLP parsing models, this technique performs marginal inference up to 11 times faster than loopy BP, with graph sizes reduced up to 98%-at comparable error in marginals and parsing accuracy. We also show that minimizing the weighted sum of divergence and size is substantially faster than minimizing either of the other objectives based on the approximation to divergence presented here.", "text": "speed marginal inference ignoring factors signiﬁcantly contribute overall accuracy. order pick suitable subset factors ignore propose three schemes minimizing number model factors bound divergence pruned full models; minimizing divergence bound factor count; minimizing weighted divergence factor count. three problems solved using approximation divergence calculated terms marginals computed simple seed graph. applied synthetic image denoising three diﬀerent types parsing models technique performs marginal inference times faster loopy graph sizes reduced %—at comparable error marginals parsing accuracy. also show minimizing weighted divergence size substantially faster minimizing either objectives based approximation divergence presented here. computational resources naturally scarce; hence practice often replace generic graphical models machinery restricted classes distributions. example dependency parsing natural language processing formulated inference markov network approach allows great modelling ﬂexibility principled integration several modules pipeline practical applications parsing web-scale data usually apply local classiﬁcation apshow generic approaches based graphical models much competitive restrict subgraph network suﬃciently approximates full graph. choi darwiche show graph minimizing divergence full partial model; however require inference full graph—the operation want avoid. then eﬃciently suﬃcient subgraph without inference full network? show how—and when—the divergence approximated using marginals simple proxy seed graph subgraph unary factors. represent divergence three terms isolated per-factor divergences based marginals seed graph factors target subgraph; term measures error introduce using seed graph instead subgraph calculating divergence; term measures error introduce considering remaining factors isolation. informally error terms disappear increasing independence potentials remaining factors proxy distribution. based representation approximate divergence isolated per-factor gains. approximation eﬃciently computed work poorly unused potentials highly correlated distribution proxy graph. approximation decide factors ignore? show three distinct ways answer question principled manner. first minimize size graph bound approximate divergence; second minimize approximate divergence bound number factors; ﬁnally minimize note expectations variable formulated expectation unary feature variable. also often need expectations covariances potentials products abbreviated assume already means inference model hand—at least small versions could among others loopy belief propagation gibbs sampler naive structured mean ﬁeld approach developed optimized version above. many approximations however still slow practical purposes scale best linearly network size. consider factor sets grow super-exponentially least high polynomial inference soon becomes bottleneck application. networks need large ignore factors speed-up inference still achieve suﬃcient accuracy? noted above least three ways tackle question ﬁnding optimal subset factors approximate full network. ﬁrst phrased follows among distributions within bounded divergence full graph smallest terms factor count. formally need weighted approximate divergence factor count. ﬁrst second objectives minimized time number factors proxy graph. third objective requires time. empirically evaluate approaches state-of-theart graphical models dependency parsing several languages. models eﬃcient dynamic programs exist. others available belief propagation shown signiﬁcantly faster providing comparable accuracy show three objectives speed inference order magnitude loss application accuracy achieved using fraction factors original graph. also show jointly minimizing size divergence fastest option accuracy comparable objectives. following ﬁrst introduce notation present optimization problems trying solve upper bounds divergence make possible relate approach existing work evaluate synthetic image denoising real-world data conclude appendix give proof sketches approximations bounds. using markov networks often need calculate expectations variables features model. used calculating gradient maximum likelihood learning providing conﬁdence values predictions. also often used posterior decoding state variable chosen maximize marginal probability possibly subject constraints. case explicitly searching trade-oﬀ eﬃciency accuracy need commit speciﬁc bound size divergence. crucially turns approximations divergence presented here choosing problem asymptotically empirically faster problems three problems combinatorial—the space models search exponential |f|. obviously cannot search exhaustively because space large also evaluating divergence generally requires inference divergence proxies cannot every possible subset factors perform inference calculate divergence instead small initial seed graph perform inference results approximate divergences several clearly eﬀectiveness approach hinges choice seed graph previous work relaxation cutting plane methods shown simple local approximations long initial formulations inference problems following qualify quantify approximation works well. cornerstone approach formulation divergence factors isolated per-factor divergences calculated using beliefs proxy graph following assume def= def= derived proxy adding factors full graph removing remaining factors adding error terms measures correlation product potentials remaining product measures correlation remaining individual potentials leads following factorized proxy-based approximation divergence approximation consider minimizing three objectives presented earlier. clearly accuracy depend correlation features factors measured particular easy show features remaining factors independent other independent approximation exact. finally note riedel smith show expectation feature eﬃciently calculate gain factorized proxy-based divergence allows approximation problem simply minimizing number factors constraint approximate divergence given proxy need additional factors according riedel smith give looser version bound without term also bound tightest possible given potential expectations still quite loose many cases. fact allows potentials maximally correlated practice rarely case. link work relaxation approaches inference violated factors/constraints graphical model weighted finite state machine integer linear program incrementally added partial graph. work diﬀers tackle marginal instead inference frame problem choosing factors optimization problem. sontag jaakkola compute marginal probabilities using cutting plane approach starts local polytope optimizes approximation partition function. hence ignore constraints instead factors. approach tackle marginalization focusing improving accuracy. particular optimization problems solve iteration fact larger problem want compress paper partly seen theoretical justiﬁcation work riedel smith heuristically threshold gain function iteration order choose factors add. prove eﬀectively optimizing weighted proxy-based factorized divergence graph size present moreover present tighter bound divergence—in fact algorithm summarizes method three basic variations investigate here. start inference initial graph using blackbox method case graph always fully factorized local factors. choose additional factors either discarding discarded factor gains exceeds picking factors highest gain picking factors gain higher upper bound divergence found performed inference close really full graph terms divergence. could used decide whether continue inference give conﬁdence intervals marginals return. note require proxy graph; instead directly evaluate using inference result algorithm. also note bounds canapplied cannot easily evaluate constants depend second order moments. test claim controlled environment existing synthetic image denoising dataset consists original images versions added noise. factor graph apply ising model binary grid nodes edges deﬁned follows observed pixel value added noise normalized mean. parameter controls conﬁdence give unary factors observations have. hence choose unary factors seed graph hope large algorithms factors still produce accurate results. note model uses edge weight experiments varying weights observed qualitative trends. choose free parameters different optimization strategies setting reference point tune parameter yield factor graphs original factors. allows directly compare diﬀerent speed-ups errors well methods qualitative behaviour range values. finally inference sub-graphs full graph tree-reweighted figure shows relative size ﬁnal factor graph compared full graph three objectives. figure presents corresponding average error marginals respect marginals full graph finally ﬁgure shows relative speed-ups compared inference full graph. ﬁgures show clear picture conﬁdence seed graph better methods perform. case mindivergence achieve increasingly accurate results number factors. minsize minjoint require fewer factors achieve equivalent levels accuracies. moreover reduction size leads direct improvements inference speed despite inference twice. tightest bound possible ﬁrst order moments available. also provide experiments analysis. previous work trainingtime feature induction mrfs also applied greedy iterative divergence minimization would interesting apply methods adding multiple features training time. approach also closely related traditional variational methods naive structured mean ﬁeld approaches assume structure ﬁxed search good parametrization/weights keep parametrization ﬁxed search good structure. advantage approach dynamically adapt complexity model. areas graph modelled well local factors alone others easiest capture structure full graph. areas usually depends observed evidence hence cannot identify ﬁxed structure advance. moreover case pick particular family distributions advance; instead allow grow needed. finally approach related edge deletion bayesian networks remove edges bayesian network order close approximation full network useful inference-related tasks obvious diﬀerence bayesian networks instead markov networks. also factors remove full graph instead partial graph. requires inference full model—the operation want avoid. moreover method choosing edges remove corresponds mindivergence therefore requires sorting edges. hence minjoint approach asymptotically faster. approach inspired following observation made practice often models consists conﬁdent local graph long distance factors serve correct mistakes local model makes. argue cases three objectives choose small subset factors suﬃcient synthetic nature experiments helped describe basic properties minsize minjoint mindivergence clear whether useful methods practice. show utility real world also evaluate three second order dependency parsing models presented smith eisner models yield state-of-the-art performance many languages relevant practice. factor graphs computationally challenging employ number variables quadratic sentence length represent candidate dependency links words cubic number pairwise factors them. factor graphs english example average nodes factors sentence. models consider also contain single combinatorial factor connects variables ensures distribution models spanning trees tokens sentence. since exponential number valid trees naively computing factor’s messages intractable; instead follow smith eisner calculated messages combinatorial optimization. crucially approach treats machinery calculate initial ﬁnal beliefs black box. evaluate accuracy inference ways. first measure error marginals relative marginals calculated full sentence node highest error average maximal errors corpus. absolute errors marginals give full picture. measure eﬀective accuracy practice posterior decoding tree maximizes calculated marginal beliefs. allows compare gold standard tree. results terms average dependency accuracy speed ﬁnal factor graph size seen table compare full three setups present method language eﬃcient parameters lead dependency accuracy full graph possible reasonable amount time. closer look. table shows that across board signiﬁcant reduction size possible without loss dependency accuracy. moreover case italian calculate marginals exactly hence measure true error approximations. note fact nearly identical. clearly reduced size direct eﬀect speed. example minjoint algorithm observe times times times speed-up. minjoint outperforms approaches comparable accuracy higher speed. table averaged results dependency accuracy; f=percentage binary edges added; t=time e=averaged max. error marginals; a=accuracy gold standard; np=non-projective; p=projective; gp=grandparent edges; sib=sibling edges. *e=calculated w.r.t. true marginals calculated sub-models used initial proxy graphs however think many cases local clues enough determine state variable hence incorporated powerful local sub-graphs. frequently observed local features fact often hard beat also crucial success factor reduction methods inference tasks entity resolution semantic role labeling. want extend work directions. first want ignore factors also values. believe many cases models almost deterministic belief state certain variables cases gain signiﬁcant eﬃciencies ignoring states entirely. second want investigate reparametrization remaining factors spirit choi darwiche figure shows number added factors versus number nodes graph italian. expected graph size stays constant mindivergence inference means allocate many factors small problems want allocate enough large ones. contrast factor count required minsize minjoint adapt problem size. figure shows runtime also scales better graph size. fact minjoint minsize mindiverence observe linear behaviour number nodes akin observations riedel relaxed/cutting plane inference. asymptotically however algorithm still scales number factors since scan remaining factors candidates. scheme chosen pick factors? previous sections suggest minjoint better approach faster. show observation based better tuning parameter show method performs vary free parameters. figure presents effectiveness scheme several values parameter draw average time spent average accuracy. shown substantially speed marginal inference ignoring factors signiﬁcantly contribute overall accuracy. surely cannot expect always work well require factor graphs contain small highly conﬁdent log`eig´+log second term rewritp log`eig´ hence write divergence deﬁnition covariance replacing log`eig´ d`pg||pg∪{i}´ leads desired representation. egˆlog`ψf\\h´˜ work supported part center intelligent information retrieval part international subcontract arfl prime contract fa--c- part army prime contract number wnf--- university pennsylvania subaward number part upenn medium iis-. opinions ﬁndings conclusions recommendations expressed material authors’ necessarily reﬂect sponsor. references anguelov koller srinivasan thrun pang h.-c. davis correlated correspondence algorithm unsupervised registration nonrigid surfaces. nips", "year": 2012}