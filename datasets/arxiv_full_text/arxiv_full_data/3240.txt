{"title": "Random Binary Mappings for Kernel Learning and Efficient SVM", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Support Vector Machines (SVMs) are powerful learners that have led to state-of-the-art results in various computer vision problems. SVMs suffer from various drawbacks in terms of selecting the right kernel, which depends on the image descriptors, as well as computational and memory efficiency. This paper introduces a novel kernel, which serves such issues well. The kernel is learned by exploiting a large amount of low-complex, randomized binary mappings of the input feature. This leads to an efficient SVM, while also alleviating the task of kernel selection. We demonstrate the capabilities of our kernel on 6 standard vision benchmarks, in which we combine several common image descriptors, namely histograms (Flowers17 and Daimler), attribute-like descriptors (UCI, OSR, and a-VOC08), and Sparse Quantization (ImageNet). Results show that our kernel learning adapts well to the different descriptors types, achieving the performance of the kernels specifically tuned for each image descriptor, and with similar evaluation cost as efficient SVM methods.", "text": "abstract. propose learn kernel weighted large number simple randomized binary stumps. stump takes extracted features input. leads efﬁcient fast also alleviating task kernel selection. demonstrate capabilities kernel standard vision benchmarks combine several common image descriptors namely histograms attributelike descriptors sparse quantization results show kernel learning adapts well different feature types achieving performance kernels speciﬁcally tuned each evaluation cost similar efﬁcient methods. success support vector machines e.g. object recognition stems well-studied optimization kernels solve non-linear classiﬁcation problems.designing right kernel combination appropriate image descriptors crucial. joint design leads chicken-and-egg problem right kernel depends image descriptors image descriptors designed familiar kernels. multiple kernel learning eases kernel selection automatically learning combination given base kernels. although successful various vision tasks might lead complex inefﬁcient kernels. recently bazavan introduced approach avoids explicit computation kernel. efﬁciently approximates non-linear mapping hand-selected kernels thus delivering impressive speed-ups. propose another around kernel learning also allows efﬁcient svms. instead combining ﬁxed base kernels investigate random binary mappings coin approach multiple binary kernel learning given methods based binary decisions random forests boosting decision stumps performed equally well image classiﬁcation benchmarks kernel svms important show mbkl does. mbkl alleviate task selecting right kernel resulting kernel efﬁcient compute scale large datasets. histogram-based daimler attribute-based a-pascal detection sparse quantization demonstrate ﬁrst time classiﬁer based achieve performances comparable hand-selected kernels speciﬁc descriptor. moreover fast fastest kernel approximations without need interactively selecting kernel. efﬁcient svm. denote parameters model non-linear mapping higher-dimensional space. classiﬁcation score feature vector aims minimizing hinge loss. implementation typically applies kernel trick lagrange multipliers {αi} classiﬁcation score becomes optimal multipliers tend sparse select relatively ‘support vectors’ many training samples. kernel trick bypasses computation non-linear mapping directly computing inner products strength svms yield max-margin classiﬁers. test time computational cost number support vectors times cost computing kernel. problem latter quite expensive. also training complexity computing kernel matrix grows quadratically number training images renders intractable large datasets. several authors tried speed kernel-based classiﬁcation. ideas include limiting number support vectors creating low-rank approximations kernel matrix methods effective scale well large datasets require kernel distances training set. rahimi recht introduced random fourier features thereby circumventing approximation explicit feature techniques explored kernels used common image descriptors rb-χ kernels intersection kernel approaches kernel linearize image descriptors sparse feature embeddings recently introduced power mean kernel generalizes intersection kernels among others achieves remarkably efﬁcient scalable optimization. multiple kernel learning aims jointly learning linear combination given base kernels. hope committee base kernels powerful kernel. denoting base kernels ﬁnal kernel takes form recent years many advances made improve efﬁciency various optimization techniques introduced e.g. semi-deﬁnite programming semi-inﬁnite linear programming gradient-based methods scalability large datasets remains issue methods explicitly compute base kernel matrices. therefore bazavan exploit random fourier features approximate non-linear mapping kernel allow scale large datasets. approach related latter also aims efﬁcient scalable kernel learning. mbkl’s base kernels hand-selected. instead approximating distance coming pre-selected kernel explore random learn distance classiﬁcation. indeed large-scale image retrieval increasing body evidence suggests effective evaluate distances e.g. next section introduce formulation kernel learning built bms. this turn yield kernel efﬁcient learn evaluate moreover kernel adapt image descriptors since learned input data binary base kernels. used speed-up distance computations large-scale image retrieval e.g. methods input feature transformed binary vector preserves locality original feature space. context classiﬁcation enforce kernel separates image classes well. adopt formulation starting point kernel since aims jointly learning classiﬁer kernel distance incorporate restrict base kernels take binary values. binary base kernels deﬁned indicator function returns input true otherwise input feature base kernel built upon single need linear adapted problem desired. sequel explore different possibilities kernel restricted them. cases divides feature space sets indicator function returns whether input samples fall part feature space not. ﬁnal kernel linear combination binary kernels σk]. note restricted binary though base kernels are. supplementary material show ‘multiple binary kernel’ valid mercer kernel. appropriate choice important arrive good classiﬁcations. instance class problem better separate classes better kernel might learned constants correspond underlying rameters classiﬁer. mapping recovers mbkl kernel form thus evaluate mbkl test time need evaluate kernel access non-linear mapping beneﬁts kernel learning. mbkl generalizes input features. easily seen ﬁxing learning rather ﬁxing several advantages. recall kernel distance depend image class evaluating. equal contribute ﬁnal kernel distance hence discarded image classes. crucial arrive competitive computational complexity. moreover mbkl aims learning kernel distance adapted image descriptors used tasks classiﬁcation. found deﬁning simple random decision stumps achieves excellent results image descriptors commonly used literature. decision stumps select component feature vector threshold randomly select component fig. kernel learning. flowers training comparison distance mbkl distance point represents distance images training mbkl kernel mbkl kernel learned images sorted class label semantic clusters seen around diagonal. input feature vector using uniform probability distribution feature length. then calculated applying threshold threshold value. again threshold generated uniform probability distribution interval values observed training component note generate randomly without using labeled data. contrast supervised learning kernel labeled data appropriately combine need thousands random arrive desired level performance. since decision stumps cost computational complexity evaluating mbkl test time grows linearly number bms. experiments show order magnitude feature length order higher. allows achieve competitive computational cost compared methods report experimental section. intuitively random decision stumps seem quantize image descriptor crudely. might affect structure feature space deteriorate performance. classiﬁcation decision stumps known allow good generalization illustration fig. compares distance mbkl decision stumps. experimental setup flowers best performing kernel kernel distances datasets paper yield conclusions. mbkl uses random decision stumps time simply i.e. distances produced methods highly correlated. decision stumps change structure feature space keep largely intact. since kernel distance parametrized modulates contribution mbkl adjust kernel distance objective. fig. shows ﬁnal mbkl kernel fig. mbkl θ-adjusted kernel learned section observe high kernel values images different classes non-learned kernel smoothed learned kernel. parameters individually using labeled data progressively adds ﬁnal classiﬁer. mbkl generates once random parameters without using labeled data. section introduce formulation learning kernel classiﬁer parameters random already generated. mbkl pursues minimizing objective. rather jointly optimizing kernel classiﬁer feasible practice thousands binary kernels decompose learning stages make tractable. stages optimize objective either classiﬁer parameters kept ﬁxed. firstly classiﬁer parameters initial guess learn kernel secondly classiﬁer trained learned kernel. could extend algorithm iteratively re-learn classiﬁer parameters would obviously raise computational cost observe increase performance. also note resample discarding some. next describe steps learning detail. actual optimization starts initialize classiﬁer describe prior step step learns kernel parameters step learns actual classiﬁer. show complex optimizations solved off-the-shelf solvers primal form. summarize steps learning algorithm step efﬁcient initial guess classiﬁer. recall binary kernel parameters associated order efﬁciently initial guess parameters learn pair parameters individually without taking account kernels. downside form learning rather myopic blind information coming kernels. however alleviated global learning kernel weights classiﬁer steps algorithm. using proportion samples responded determine sign number samples class learning classiﬁer number samples rest classes many samples class test value table shows cases otherwise sign. case discard since contribute ﬁnal classiﬁcation score. rules deduced fulﬁlling max-margin objective. multi-class problem one-vs-rest strategy initialize parameters using initial guess previously learned learn minimizes loss. order write loss directly depending reorder obtain ¯σk. note known depends already guessed algebra objective pursuing becomes observe regularizer becomes either discarded square. thus learn -class linear input features since off-the-shelf optimizers constrain might happen kernels fulﬁlled. case directly positive note necessary yield valid mercer kernel. multi-class problem kernel parameters i.e. {σk} class denoted classes whereas speciﬁc using multi-class heuristic one-vs-all. responses corresponding class classes still twoclass take positive samples evaluated true class others negative. yield large negative training found practice sufﬁces reduced subset examples. practice generate subset negative samples randomly extracting examples whose object class different target class taking account amount examples object class balanced. number samples dataset detailed experiments section. step learning classiﬁer. finally learned kernel train standard classiﬁer thus replacing initial guess classiﬁer. discard learning contribute ﬁnal kernel. deduce optimizing objective remaining parameters done primal form training simple rules table besides since learned independently parallelized. step optimized svms primal form. note practical cases computational cost step bottleneck algorithm. feature length step equal initial number larger number used step section report experimental results benchmarks order evaluate mbkl context variety vision tasks image descriptors. introducing relevant implementation details discuss results. experimental setup experiments cpus intel i.ghz. chose parameter among cross-validation training computing times. liblinear library using linear libsvm library using kernel. table summarizes datasets used well characteristics features used. case features attributes normalize logistic function interval normalization methods. dataset standard evaluation procedures described literature used. details provided supplementary material. evaluate mbkl’s efﬁciency datasets except computational cost little methods. daimler two-class benchmark consisting disjoints sets containing pedestrian samples nonpedestrian examples. splits training testing. testing done sets separately yielding total testing results. descriptor used. report results two-class problems namely liver sonar using cross-validations. attribute-based descriptors provided length respectively. flowers consists different kinds ﬂowers images class divided splits. describe images using features provided sift opponent sift wsift color attention building bag-of-words histogram computed using spatial pyramids. contains images categories used training rest testing. -dimensional gist descriptor relative attributes provided authors a-pascal consists images objects divided train validation sets. objects cropped original images voc. different categories examples class except people features provided dataset local texture color descriptors. image also attributes given. reported attributes obtained asking users mechanical turk semantic attributes object class dataset. used improve classiﬁcation accuracy. features attributes. fig. analysis mbkl flowers. accuracy training time testing time amount selected varying amount initially randomly generated bms. imagenet create dataset taking subset images. subset contains images different classes overlap synset. randomly split subset images testing rest training maintaining proportion images class. evaluation report average classiﬁcation accuracy across classes. nested sparse quantization descriptors provided using setup. codebook entries max-pooling spatial pyramids regions many state-of-the-art descriptors image classiﬁcation better accuracy achieved linear kernel svm. additionally test second descriptor imagenet. setup create standard bag-of-words replacing max-pooling average pooling. descriptor performs better kernel achieves lower performance max-pooling linear svm. investigate impact different mbkl parameters. results given flowers. conducted analysis rest tested datasets could extract similar conclusions them. conduct analysis following baselines mbkl binarized projections test mbkl replace decision stumps random projections. generate random projections using explicit feature approximate obtain threshold using decision stumps. random projections achieve high performance flowers computational cost order random projection. mbkl order analyze impact learn directly equivalent using input features linear svm. input features -svm input features regularizer allows discarding bms. note contrast mbkl selection different class. impact bms. fig. mbkl different baselines report accuracy training time testing time ﬁnal amount varying amount initial bms. comparing random projections decision stumps observe obtain similar performance. also note random projection computational complexity order feature length decision stumps noticeable test time training since learning algorithm much expensive computing bms. observe increasing number accuracy saturates degrade. mbkl suffer over-ﬁtting including large amount bms. believe generated without labeled data used kernel properly regularized. impact fig. also show results ﬁxing yields performance close mbkl parameters compensate lack learning note ﬁxing lowers training time since kernel need learned. learning kernel justiﬁed allows discard classes together yields faster testing time. number diminishes learning kernel. also case framework efﬁcient evaluate degrades performance. interestingly observe original feature length using initial amount performance already competitive. also note number saturates around redundant feature descriptors non-informative feature pooling regions mbkl learns relevant classiﬁcation. observed drastic reduction feature length flowers multiple descriptors. computational cost kernel learning. table shows impact parameters computational cost learning algorithm. also report datasets mbkl parameters rest experiments. recall parameters step initial number number training samples learn two-class svm. parameters strike good balance accuracy efﬁciency. proportion negative positive training samples table learning parameters mbkl. report amount randomly generated number selected proportion positive negative training samples amount samples training times split. except cases yields insufﬁcient training data. observe initial number usually times length image descriptor. consequence learning become computational bottleneck. fortunately accuracy step ﬂattens small number selected training samples. learning kernel number times length image descriptor. thus learning ﬁnal one-vs-all usually computationally cheaper learning though cost increases number classes. methods based binary decision compare random forest using trees depth except trees depth also compare adaboost implementation using iterations. parameters best found. approx. intersection kernel maji following suggestion authors bins quantization. observe signiﬁcative change accuracy increasing number bins quantization. power mean approximation intersection approximation default parameters. features scaled following author’s recommendation. also analyzed kernel approximation raginsky lazebnik approach combines random projections rahimi recht contrast mbkl method learns kernel distance preserve locality original descriptor space. resulting kernel linear found performs poorly note method designed preserve locality useful criterion image retrieval less image classiﬁcation. moreover based random projections approximate kernel might adequate image descriptors use. report accuracy performance method large-scale data bazavan uses approximations predeﬁned kernels already report accuracy probably comparable approximations parameters crossvalidation. performance accuracy. results reported table observe mbkl method benchmark achieves accuracy similar best performing method benchmark. note kernels approximations perform equally well descriptor types. performance degrade descriptors attribute-based features also descriptors already linearly separable s-o-a descriptors large scale datasets. cases approximations predeﬁned kernel perform similarly actual kernel. observed performance pmsvm lower feature length small. conclude mbkl outperforms methods based binary decisions including random forest boosting. tested datasets accuracy mbkl comparable s-o-a normally achieved using different methods different datasets. even outperform s-o-a well daimler benchmark since fig. testing time accuracy datasets. computational cost normalized respect mbkl. score means times slower mbkl. accuracy respect mbkl difference accuracy competing method mbkl. score means method performs better mbkl. points area indicate method perform better mbkl speed accuracy. found better parameters features. imagenet mbkl outperforms using descriptors achieving good compromise accuracy efﬁciency test time. fig. compares testing time mbkl efﬁcient methods. report testing times relative test time mbkl well accuracy relative mbkl. mbkl achieves competitive levels efﬁciency. mbkl’s computation speed depends number flowers mbkl faster linear fewer ﬁnal original feature components. note also ﬁnal length feature mbkl approximate mbkl faster decision stumps faster calculate projections approximate pmsvm achieves better accuracy speed mbkl cases note rest cases opposite. note datasets fig. table pmsvm performs poorly compared mbkl since descriptors attribute-based. training time. learning mbkl optimizations done off-the-shelf linear svm. thus computational complexity depends mainly optimizer. liblinear could efﬁcient optimizer. comparing different optimizers scope paper. compare training time mbkl methods predeﬁned kernels methods computational overhead learning kernel. paper introduced kernel learned combining large amount simple randomized bms. derived form non-linear mapping kernel allows similar levels efﬁciency reached fast kernel approximations. experiments show learned kernel adapt common image descriptors achieving performance comparable kernels speciﬁcally selected image descriptor. expect generalization capabilities kernel exploited design unexplored image descriptors.", "year": 2013}