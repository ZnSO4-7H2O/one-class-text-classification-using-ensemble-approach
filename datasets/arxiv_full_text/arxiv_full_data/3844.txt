{"title": "Attention Strategies for Multi-Source Sequence-to-Sequence Learning", "tag": ["cs.CL", "cs.NE", "68T50", "I.2.7"], "abstract": "Modeling attention in neural multi-source sequence-to-sequence learning remains a relatively unexplored area, despite its usefulness in tasks that incorporate multiple source languages or modalities. We propose two novel approaches to combine the outputs of attention mechanisms over each source sequence, flat and hierarchical. We compare the proposed methods with existing techniques and present results of systematic evaluation of those methods on the WMT16 Multimodal Translation and Automatic Post-editing tasks. We show that the proposed methods achieve competitive results on both tasks.", "text": "modeling attention neural multi-source sequence-to-sequence learning remains relatively unexplored area despite usefulness tasks incorporate multiple source languages modalities. propose novel approaches combine outputs attention mechanisms source sequence hierarchical. compare proposed methods existing techniques present results systematic evaluation methods multimodal translation automatic post-editing tasks. show proposed methods achieve competitive results tasks. sequence-to-sequence learning attention mechanism recently became successful paradigm state-of-the-art results machine translation image captioning text summarization tasks. applications learning make single encoder. depending modality either recurrent neural network textual input data convolutional network images. work focus special case learning multiple input sequences possibly different modalities single output-generating recurrent decoder. explore various strategies decoder employ attend hidden states individual encoders. multimodal image caption input might expect caption primary source information whereas image would play role output disambiguation. automatic post-editing sentence source language automatically generated translation input might want attend source text case model decides error translation. section review attention mechanism single-source learning. section introduces attention combination strategies. section evaluate proposed models tasks. summarize related work section conclude section attention mechanism learning allows decoder directly access information input time emits symbol. inspired content-based addressing neural turing machines attention mechanism estimates probability distribution encoder hidden states decoding step. distribution used computing context vector—the weighted average encoder hidden states—as additional input decoder. trainable parameters projection matrices transform decoder encoder states common vector space weight vector dimensions space. denotes length input sequence. sake clarity bias terms omitted. widely adopted technique combining multiple attention models decoder concatenation context vectors mentioned section setting forces model attend encoder independently lets attention combination resolved implicitly subsequent network layers. section propose alternative strategies combining attentions multiple encoders. either decoder learn distribution jointly encoder hidden states factorize distribution individual encoders flat attention combination projects hidden states encoders shared space computes arbitrary distribution projections. difference concatenation context vectors attention combination coefﬁcients computed jointly encoders length input sequence n-th encoder attention energy j-th state k-th encoder i-th decoding step. attention energies computed equation parameters shared among encoders different encoder serves encoder-speciﬁc projection hidden states common vector space. states individual encoders occupy different vector spaces different dimensionality therefore context vector cannot computed weighted sum. project projection matrices weight vector ψisi sentinel vector. note sentinel energy term depend hidden state encoder. sentinel vector projected vector space encoder state equation term added extra attention energy term equation sentinel vector used corresponding vector summation equation technique allow decoder choose whether attend encoder focus state like language model. beneﬁcial encoder contain much relevant information current decoding step. figure learning curves validation data context vector concatenation hierarchical attention combination withsentinel without sharing projection matrices. models implemented using neural monkey sequence-to-sequence learning toolkit setups process textual input bidirectional network units hidden state direction units embeddings. attention projection space hidden units. optimize network minimize output cross-entropy using adam algorithm learning rate model visual input processed pre-trained network without ﬁne-tuning. attention distribution visual input computed last convolutional layer network. project hidden states common vector space. raises question whether space projected energy computation using matrices equation i.e. whether experiments explore options. also adding adding context vecsentinel tor. hierarchical attention combination hierarchical attention combination model computes every context vector independently similarly concatenation approach. instead concatenation second attention mechanism constructed context vectors. divide computation attention distribution steps first compute context vector encoder independently using equation second project context vectors common space compute another distribution projected context vectors corresponding weighted average context vector k-th encoder additional trainable parameters shared encoders encoder-speciﬁc projection matrices equal shared similarly case attention combination. table results experiments test sets multik dataset dataset. column ‘share’ denotes whether projection matrix shared energies context vector computation ‘sent.’ indicates whether sentinel vector used not. transforming output reference. technique prevent model paraphrasing input sentences. decoder network hidden units. unlike setup conditional prone overﬁtting small dataset work with. attempts models relatively rare niehues concatenate inputs long sequence forces encoder able work source target language. attention similar combination strategy; however used sequential data. best system wmt’ competition trains separate models translating output post-edited targets second source sentences post-edited targets. decoders average output distributions similarly decoder ensembling. biggest source improvement state-of-theart posteditor came additional training data generation rather changes network architecture. results experiments multimodal shown table achieved best results using hierarchical attention combination without sentinel mechanism also showed fastest convergence. combination strategy achieves similar results eventually. sharing projections energy context vector computation improve concatenation baseline slows training alprohibitively. multimodal models able surpass textual baseline using conditional units brought improvement bleu points average exception concatenation scenario performance dropped almost bleu points. hypothesize caused fact model learn implicit attention combination multiple places output projection three times inside conditional unit thus report scores introduced attention combination techniques trained conditional units compare concatenation baseline trained plain units. used data task consists training validation test sentence triplets domain. triplet contains english source sentence automatically generated german translation source sentence manually post-edited german sentence reference. case dataset outputs almost perfect little effort required post-edit sentences. results evaluated using humantargeted error rate bleu score conducted experiments proposed strategies multimodal translation automatic post-editing tasks showed hierarchical attention combination applied tasks maintaining competitive score previously used techniques. unlike simple context vector concatenation introduced combination strategies used conditional units decoder. that hierarchical combination strategy exhibits faster learning strategies. research funded czech science foundation grant p//g grant h-ict--- charles university grant project work using language resources developed and/or stored and/or distributed lindat-clarin project ministry education czech republic", "year": 2017}