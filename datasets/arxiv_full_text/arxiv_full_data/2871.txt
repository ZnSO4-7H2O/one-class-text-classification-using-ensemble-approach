{"title": "Natural Gradient Deep Q-learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "This paper presents findings for training a Q-learning reinforcement learning agent using natural gradient techniques. We compare the original deep Q-network (DQN) algorithm to its natural gradient counterpart (NGDQN), measuring NGDQN and DQN performance on classic controls environments without target networks. We find that NGDQN performs favorably relative to DQN, converging to significantly better policies faster and more frequently. These results indicate that natural gradient could be used for value function optimization in reinforcement learning to accelerate and stabilize training.", "text": "paper presents ﬁndings training q-learning reinforcement learning agent using natural gradient techniques. compare original deep q-network algorithm natural gradient counterpart measuring ngdqn performance classic controls environments without target networks. ngdqn performs favorably relative converging signiﬁcantly better policies faster frequently. results indicate natural gradient could used value function optimization reinforcement learning accelerate stabilize training. natural gradient originally proposed amari method accelerate gradient descent rather exclusively using loss gradient including second-order information natural gradient uses information found parameter space model. natural gradient successfully applied several deep learning domains used accelerate training reinforcement learning systems motivate approach hoped using natural gradient would accelerate training reinforcement learning systems making system sample-efﬁcient thereby addressing major problems reinforcement learning. also hoped since natural gradient stabilizes training ngdqn could able achieve good results without target network converge good solutions stability. experiments observed effects. training without target network ngdqn converged much faster frequently baseline ngdqn training appeared much stable. paper inspired requests research list published openai listed application natural gradient techniques q-learning since june paper presents ﬁrst successful attempt knowledge method accelerate training q-networks using natural gradient. reinforcement learning agent trained interact environment maximize cumulative reward. agent interacts environment observing state performing action receiving state reward often environment modeled markov decision process deﬁnes states actions expected reward given q-learning model-free reinforcement learning algorithm works gradually learning expectation cumulative reward. bellman equation deﬁnes optimal q-value function optimized value iteration deﬁnes update rule a)|s additionally optimal policy deﬁned argmaxaq∗ train q-learning agent often \u0001-greedy policy. initially training agent acts nearly randomly order explore potentially successful strategies. agent learns acts randomly less mathematically probability choosing random action gradually annealed course training. q-learning recently used play atari games using convolutional networks mnih state action learning rate function estimates future reward updated follows environments deﬁned endings ﬁnal timestep deﬁned since future reward. deep q-learning mapping learned neural network. neural network described parametric function approximator uses layers units containing weights biases activation functions called neurons. layer’s output next layer loss backpropagated layer’s weights order adjust parameters according effect loss. deep q-learning neural network parameterized takes state outputs predicted future reward possible action linear activation ﬁnal layer. loss network deﬁned follows given environment notice take mean-squared-error expected q-value actual q-value. neural network optimized course numerous iterations form gradient descent. original paper adaptive gradient method used train network deep q-networks experience replay train q-value estimator randomly sampled batch previous experiences experience replay makes training samples independent identically distributed unlike highly correlated consecutive samples encountered interaction environment prerequisite many convergence theorems. additionally \u0001-greedy policy baseline ngdqn. combine approaches using natural gradient optimize arbitrary neural network q-learning architectures. gradient descent optimizes parameters model respect loss function descending loss manifold. this take gradient loss respect parameters move opposite direction gradient mathematically gradient descent proposes point given learning rate parameters α∇θl. commonly used variant gradient descent stochastic gradient descent instead calculating entire gradient time uses mini-batch training samples α∇θl. baselines adam adaptive gradient optimizer modiﬁcation however approach gradient descent number issues. gradient descent often become slow plateaus magnitude gradient close zero. also gradient descent takes uniform steps parameter space necessarily correspond uniform steps output distribution. natural gradient attempts issues incorporating inverse fisher information matrix concept statistical learning theory essentially core problem euclidean distances parameter space give enough information distances corresponding outputs strong enough relationship kullback leibler deﬁne expressive distributionwise measure follows perform gradient descent manifold functions given model fisher information metric riemannian manifold. since symmetric divergence behaves like distance measure inﬁnitesimal form riemannian metric derived hessian divergence symmetric divergence give pascanu bengio’s deﬁnition assume probability point sampled network gaussian network’s output mean ﬁxed variance. given probability density function input vector parameters using deﬁnition solving lagrange multiplier minimizing loss parameters updated constraint constant symmetric divergence derive approximation constant symmetric divergence using information matrix. taking second-order taylor expansion gets output probability distribution dependent ﬁnal layer activation pascanu bengio give following representation layer linear activation adapted q-learning deﬁned standard deviation formulation since information dependent ﬁnal layer’s activation different activations hidden layers without changing fisher information. pascanu bengio fisher information derived corresponds jacobian output vector respect parameters follows flinear βes∼dπ borrow heavily approach pascanu bengio using natural gradient deep neural networks formalization implementation method. next look work different method natural gradient descent desjardins paper algorithm called projected natural gradient descent proposed also considers fisher information matrix derivation. paper explore approach could area future research prong shown converge better multiple data-sets cifar- additional methods applying natural gradient reinforcement learning using reinforcement learning algorithms policy gradient actor-critic explored kakade peters works natural variants respective algorithms shown perform favorably compared non-natural counterparts. details theory implementation results respective papers. insights mathematics optimization using natural conjugate gradient techniques provided work honkela methods allow efﬁcient optimization high dimensions nonlinear contexts. natural temporal difference learning algorithm applies natural gradient reinforcement learning systems based bellman error although q-learning explored authors natural gradient residual gradient minimizes bellman error apply natural gradient sarsa on-policy learning algorithm. empirical experiments show natural gradient outperforms standard methods tested environments. finally knowledge published publicly available version natural q-learning created work authors re-implemented prong veriﬁed efﬁcacy mnist. however authors attempted apply q-learning negative results change cartpole worse results gridworld. experiments standard method q-learning environment. lasagne theano agentnet complete brunt computational work. implementation natural gradient adapted pascanu bengio originally mapping directly back-propagated loss modify training procedure target value change similar described also decay learning rate multiplying constant factor every iteration. output layer q-network linear activation function parameterization fisher information matrix linear activations determines natural gradient. this refer equation approximated every batch. minres-qlp krylov subspace descent algorithm calculates change parameters according fisher information matrix pascanu bengio efﬁciently solving system linear equations relating desired change parameters gradients loss implementation runs openai platform provides several classic control environments ones shown here well environments atari current algorithm takes continuous space maps discrete actions. algorithm adapt mnih al.’s algorithm pascanu bengio’s algorithm environments require preprocessing omitted preprocessing step however easily re-added. experiments chosen somewhat arbitrarily selected according grid-search according grid search either leave damping value unchanged adjust according levenberg-marquardt heuristic used pascanu bengio martens algorithm natural gradient deep q-learning experience replay require initial learning rate require learning rate decay require function update damping probability select random action otherwise select action maxa execute action emulator observe reward state store transition memory sample random minibatch transitions update damping gnorm deﬁne baseline openai’s open-source baselines library allows reliable testing tuned reinforcement learning architectures. deﬁned performance measured taking best -episode reward course running. grid search parameter spaces speciﬁed hyperparameters section measuring performance possible combinations. certain parameters like exploration fraction used implementation ngdqn grid search parameters well. wish compare vanilla ngdqn vanilla target networks model saving features prioritized experience replay. following grid search take best result performance environment ngdqn conﬁguration times recording moving -episode average best -episode average course number runs. experiments reveal natural gradient compares favorably standard adaptive gradient techniques. however increase stability speed comes trade-off additional computation natural gradient takes longer train compared adaptive methods adam optimizer details found pascanu bengio’s work below describe environments summarizing data taken https//github.com/ openai/gym information provided wiki https//github.com/openai/gym/wiki. classic control task cartpole involves balancing pole controllable sliding cart frictionless rail timesteps. agent solves environment average reward episodes equal greater however sake consistency measure performance taking best -episode average reward. agent assigned reward timestep pole angle less cart position less units center. agent given continuous -dimensional space describing environment respond returning values pushing cart either right left. cartpole-v challenging environment requires agent balance pole cart timesteps rather agent solves environment gets average reward course timesteps. however sake consistency measure performance taking best -episode average reward. environment essentially behaves identically cartpole-v except cart balance timesteps instead acrobot environment agent given rewards swinging double-jointed pendulum stationary position. agent actuate second joint returning three actions performance shown worse performance might comparing implementations. discrepancy often used target networks whereas case target networks either ngdqn dqn. reason want compare ngdqn original algorithm presented algorithm mnih order test natural gradient stabilize deep q-networks place target networks. plan expand analysis include target networks future work. corresponding left right torque. agent given dimensional vector describing environments angles velocities. episode ends second pole length pole base. timestep agent reach state given reward. finally lunarlander environment agent attempts land lander particular location simulated world. lander hits ground going fast lander explode lander runs fuel lander plummet toward surface. agent given continuous vector describing state turn engine off. landing placed center screen lander lands given reward. agent also receives variable amount reward coming rest contacting ground leg. agent loses small amount reward ﬁring engine loses large amount reward crashes. although environment also deﬁnes solve point metric measure performance. ngdqn four experiments achieve following results summarized figure hyperparameters found hyperparameters section code project found code section. environment number episodes standards best episode performance taken. experiments natural gradient converges faster signiﬁcantly consistently benchmark indicating robustness task compared standard adaptive gradient optimizer used baseline library success across tests indicates natural gradient generalizes well diverse control tasks simpler tasks like cartpole complex tasks like lunarlander. paper natural gradient methods shown accelerate stabilize training common control tasks. could indicate q-learning’s instability diminished naturally optimizing also natural gradient could applied areas reinforcement learning order address important problems sample efﬁciency. here brief contributor statement provided recommended sculley primary author research wrote ﬁrst draft paper edited later stages paper wrote adapted code project. additionally experiments primary author. secondary author veriﬁed code correctness edited paper. secondary author also important derivation understanding divergence calculation natural gradient reaching fellow academics. thanks selby providing valuable insight support reviewing paper offering suggestions course writing process. thanks leonard instruction advice generous encouragement especially applied math ﬁrst part project took place. also huge thanks kavosh asadi providing valuable feedback direction helping navigate research scene. alex barron todor markov zack swafford. deep q-learning natural gradients https//github.com/todor-markov/natural-q-learning/blob/master/ writeup.pdf. greg brockman vicki cheung ludwig pettersson jonas schneider john schulman tang wojciech zaremba. openai gym. arxiv e-prints abs/. http //arxiv.org/abs/.. sou-cheng choi christopher paige michael saunders. minres-qlp krylov subspace method indeﬁnite singular symmetric systems. siam journal scientiﬁc computing http//web.stanford. edu/group/sol/software/minresqlp/minresqlp-sisc-.pdf. guillaume desjardins karen simonyan razvan pascanu koray kavukcuoglu. natural neural cortes lawrence sugiyama garnett editors networks. advances neural information processing systems pages curran associates inc. http//papers.nips.cc/paper/-natural-neural-networks.pdf. prafulla dhariwal christopher hesse oleg klimov alex nichol matthias plappert alec radford john schulman szymon sidor yuhuai openai baselines. https//github.com/ openai/baselines sander dieleman schl¨uter colin raffel eben olson søren kaae sønderby daniel nouri daniel maturana martin thoma eric battenberg jack kelly jeffrey fauw michael heilman diogo moitinho almeida brian mcfee hendrik weideman g´abor tak´acs peter rivaz crall gregory sanders kashif rasul cong geoffrey french jonas degrave. lasagne first release august http//dx.doi.org/./zenodo.. hester vecerik pietquin lanctot schaul piot horgan quan sendonaris dulac-arnold osband agapiou leibo gruslys. deep q-learning demonstrations. arxiv e-prints april antti honkela matti tornio tapani raiko juha karhunen. natural conjugate gradient variational inference. international conference neural information processing https//www.hiit.fi/u/ahonkela/papers/honkelaiconip.pdf. sham kakade. natural policy gradient. thomas dietterich suzanna becker zoubin ghahramani editors advances neural information processing systems pages press http//books.nips.cc/papers/files/nips/cn. pdf. volodymyr mnih koray kavukcuoglu david silver alex graves ioannis antonoglou daan wierstra martin riedmiller. playing atari deep reinforcement learning. arxiv e-prints abs/. http//arxiv.org/abs/.. peters sethu vijayakumar stefan schaal. natural actor-critic pages springer berlin heidelberg berlin heidelberg isbn ----. https//doi.org/./_. rummery niranjan. on-line q-learning using connectionist systems. technical report cambridge university engineering department september ftp//svr-ftp. eng.cam.ac.uk/reports/rummery_tr.ps.z. john schulman sergey levine pieter abbeel michael jordan philipp moritz. trust region policy optimization. francis bach david blei editors proceedings international conference machine learning volume proceedings machine learning research pages lille france pmlr. http//proceedings.mlr.press/ v/schulman.html. christopher john cornish hellaby watkins. learning delayed rewards. thesis king’s college cambridge http//www.cs.rhul.ac.uk/~chrisw/new_thesis. pdf. ngdqn minimum epsilon ngdqn model tested using initial learning rate ngdqn epsilon decay since wasn’t equivalent value baselines library grid search baselines included exploration fraction either likewise give baselines best chance beating ngdqn also searched wide range learning rates given below. batch running time given sherlock. ngdqn lunarlander-v partition supplied either nvidia titan black nvidia tesla gpu. environments normal partition. additional details natural gradient computation time found pascanu bengio", "year": 2018}