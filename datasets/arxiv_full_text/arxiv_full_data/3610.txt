{"title": "i-RevNet: Deep Invertible Networks", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "It is widely believed that the success of deep convolutional networks is based on progressively discarding uninformative variability about the input with respect to the problem at hand. This is supported empirically by the difficulty of recovering images from their hidden representations, in most commonly used network architectures. In this paper we show via a one-to-one mapping that this loss of information is not a necessary condition to learn representations that generalize well on complicated problems, such as ImageNet. Via a cascade of homeomorphic layers, we build the i-RevNet, a network that can be fully inverted up to the final projection onto the classes, i.e. no information is discarded. Building an invertible architecture is difficult, for one, because the local inversion is ill-conditioned, we overcome this by providing an explicit inverse. An analysis of i-RevNets learned representations suggests an alternative explanation for the success of deep networks by a progressive contraction and linear separation with depth. To shed light on the nature of the model learned by the i-RevNet we reconstruct linear interpolations between natural image representations.", "text": "widely believed success deep convolutional networks based progressively discarding uninformative variability input respect problem hand. supported empirically difﬁculty recovering images hidden representations commonly used network architectures. paper show one-to-one mapping loss information necessary condition learn representations generalize well complicated problems imagenet. cascade homeomorphic layers build i-revnet network fully inverted ﬁnal projection onto classes i.e. information discarded. building invertible architecture difﬁcult local inversion ill-conditioned overcome providing explicit inverse. analysis i-revnets learned representations suggests alternative explanation success deep networks progressive contraction linear separation depth. shed light nature model learned i-revnet reconstruct linear interpolations natural image representations. effective classifying images sorts cascade linear nonlinear operators reveals little contribution internal representation classiﬁcation. learning process characterized steady reduction large amounts uninformative variability images simultaneously revealing essence visual class. widely believed process based progressively discarding uninformative variability input respect problem hand however extent information discarded lost somewhere intermediate nonlinear processing steps. paper provide insight variability reduction process proposing invertible convolutional network discard information input. difﬁculty recover images hidden representations found many commonly used network architectures poses question substantial loss information necessary successful classiﬁcation. show information discarded. using homeomorphic layers invariance built last layer projection. shwartz-ziv tishby minimal sufﬁcient statistics proposed candidate explain reduction variability. tishby zaslavsky introduces information bottleneck principle states optimal representation must reduce mutual information input representation reduce much uninformative variability possible. time network maximize mutual information desired output representation effectively preserve class collapsing onto classes. effect information bottleneck demonstrated small datasets shwartz-ziv tishby achille soatto however work show necessary condition build cascade homeomorphic layers preserves mutual information input hidden representation shows loss information occur ﬁnal layer. demonstrate loss information avoided maintaining discriminability even large-scale problems like imagenet. reduce variability progressive contraction respect meaningful metric intermediate representations. several works observed phenomenon progressive separation contraction non-invertible networks limited datasets. progressive improvements interpreted creation progressively stronger invariants classiﬁcation. ideally contraction brutal avoid removing important information intermediate signal. shows good trade-off discriminability invariance progressively built. paper extend ﬁndings zeiler fergus oyallon imagenet importantly show loss information necessary observing progressive contraction. duality invariance separation classes discussed mallat here intra-class variabilities modeled groups processed performing parallel transport along symmetries. filters adapted learning speciﬁc bias dataset avoid contract along discriminative directions. however using groups beyond euclidean case image classiﬁcation hard. mainly groups associated abstract variabilities difﬁcult estimate high-dimensional nature well appropriate degree invariance required. illustration framework euclidean group given scattering transform builds invariance small translations recoverable certain extent. work introduce network cannot discard information except ﬁnal classiﬁcation stage demonstrate numerically progressive contraction separation signal classes. introduce i-revnet invertible deep network. i-revnets retain information input signal intermediate representations last layer. architecture builds upon recently introduced revnet replace non-invertible components original revnets invertible ones. i-revnets achieve performance imagenet compared similar non-invertible revnet resnet architectures shed light mechanism underlying generalization-ability learned representation show i-revnets progressively separate contract signals depth. results evidence effective reduction variability contraction recoverable input obtained series one-to-one mappings. several recent works show signiﬁcant information input images lost depth successful imagenet classiﬁcation cnns understand loss information references propose invert representations means learned hand-engineered priors. approximate inversions indicate increased geometric photometric invariance depth. multiple works report progressive properties deep networks linked discarded information representations well linearization linear separability contraction low-dimensional embeddings however clear observations loss information necessity observed progressive phenomena. work show progressive separation contraction obtained time allowing exact reconstruction signal. multiple frameworks introduced permit learn invertible representations certain conditions. parseval networks introduced increase robustness learned representations respect adversarial attacks. framework spectrum convolutional operators constrained norm learning. linear operator thus injective. consequence input parseval networks recovered built-in nonlinearities invertible well typically case. bruna derive conditions pooling representations method directly overcomes issue. scattering transform example predeﬁned deep representation approximately invariant translations reconstructed degree invariance speciﬁed small. requires gradient descent optimization guarantee convergences known. summary references make clear invertibility requires special care designing architecture special care designing optimization procedure. paper introduce network overcomes issues exact inverse construction. main inspiration work recent reversible residual network introduced gomez revnets turn closely related nice real-nvp architectures make constrained jacobian determinants generative modeling. architectures similar lifting scheme feistel cipher diagrams show. revnets illustrate build invertible resnet-type blocks avoid storing intermediate activations necessary backward pass. however revnets still employ multiple non-invertible operators like max-pooling downsampling operators part network. such revnets invertible construction. paper show build invertible type revnet architecture performs competitively revnets imagenet call i-revnet invertible revnet. section introduces general framework i-revnet architecture explains explicitly build inverse left-inverse i-revnet. practical implementation discussed demonstrate competitive numerical results. figure main component i-revnet inverse. revnet blocks interleaved convolutional bottlenecks reshufﬂing operations ensure invertibility architecture computational efﬁciency. input processed splitting operator output merged observe inverse network obtained minimal adaptations. equal size thanks splitting operator paper choose split channel dimension done revnets. operator linear injective reduces spatial resolution coefﬁcients potentially increase layer size wider layers usually improve classiﬁcation performance thus build pseudo inverse used inversion. recall invertible ˜s−. number coefﬁcients next block maintained depth representation decoupled variables play interlaced roles. strategy implemented i-revnet consists alternation additions nonlinear operators progressively down-sampling signal thanks operators here consists convolutions non-linearity ˜xj. pair ﬁnal layer concatenated merging operator omit sake simplicity necessary. figure describes blocks i-revnet. design similar feistel cipher diagrams lifting scheme invertible efﬁcient implementations complex transforms like second generation wavelets. avoid non-invertible modules revnet necessary train reasonable time designed build invariance w.r.t. translation variability. method shows replace linear invertible modules reduce spatial resolution maintaining layer’s size increasing number channels. keep computational cost manageable tightly coupling downsampling increase width network. reducing spatial resolution undesirable potentially identity. refer networks i-revnets. leads following equations invertible mapping. principle invertible downsampling operation like e.g. dilated convolutions considered here. inverse operation described illustrated figure since preserves roughly spatial ordering thus permits avoid mixing different neighborhoods next convolution. similar also linearly increases channel dimensionality example concatenating ﬁnal layer averaged along spatial dimension followed relu non-linearity ﬁnally linear projection class probes supervised training algorithm. given i-revnet possible deﬁne left-inverse i.e. φ+φx even inverse i.e. φ−φx φ−φx invertible. cases convolutional sections well i-revnets. i-revnet dual inverse −fj) depth apply sense requires replace subsection discuss inverse suffer signiﬁcant round-off errors however sensitive small variations input large subspace shown subsection subsection describe models trained injective i-revnet bijective i-revnet fewer parameters. hyper-parameters selected either close resnet revnet baselines terms number layers parameters keeping performance competitive. reasons gomez scheme also allows avoiding storing intermediate activations training time making memory consumption deep i-revnets issue practice. compare implementation revnet layers corresponding parameters provided open source release gomez standard resnet layers parameters block bottleneck block consists succession convolutional operators preceded batchnormalization relu non-linearity. second layer four times fewer channels corresponding kernel sizes respectively ﬁnal representation spatially averaged projected onto classes relu nonlinearity. discuss progressively decrease spatial resolution increasing number channels layer operators ﬁrst describe model consists layers optimized match performances revnet resnet approximatively number layers. particular explain progressively decrease spatial resolution increasing number channels block operators splitting operator consists linear injective embedding downsamples factor spatial resolution increasing number output channels simply adding latter permits increase initial layer size consequently size next layers performed gomez thus bijective injective i-revnet. depth allows reduce number computations maintaining good classiﬁcation performance. correspond downsampling operator respectively depth similar normal revnet. spatial resolution layers reduced factor increasing number channels factor respectively furthermore means corresponding spatial resolutions input size respectively total number coefﬁcients layer remaining blocks kept identity explained section above. architecture bijective consists layers whose total numbers parameters optimized match revnet layers. initially input split corresponds invertible spatial downsampling increases number channels thus keeps dimension constant permits building bijective i-revnet. then depth spatial resolution reduced contrary architecture dimensionality layer constantly equal ﬁnal layer channel sizes networks training imagenet follows setup gomez train momentum regularized model weight decay batch normalization. dataset processed iterations batch size distributed gpus. initial learning rate dropped factor every iterations. dataset augmented according gomez images values mapped following geometric transformations applied random scaling random horizontal ﬂipping random cropping size ﬁnally color distortions. regularizations incorporated classiﬁcation pipeline. test time rescale image size perform center crop size report training loss curves figure i-revnet resnet baseline displayed moving average iterations. observe decrease training-losses similar indicates constraint invertibility interfere negatively learning process. however observed third longer wall-clock times i-revnets compared plain revnets channel size becomes larger. table reports performances i-revnets comparable revnet resnet. first compare i-revnet revnet resnet. indeed cnns number layers i-revnet increases channel width initial layer done gomez drawback technique kernel sizes larger subsequent layers. i-revnet times parameters revnet resnet leads similar accuracy validation imagenet. contrary i-revnet designed roughly number parameters revnet resnet bijective. accuracy decreases absolute percent imagenet compared revnet baseline surprising number channels drastically increased earlier layers done baselines explore wide ranges hyper-parameters thus likely reduced additional engineering. analyze representation built bijective neural network i-revnet inverse trained ilsvrc-. ﬁrst explain obtaining challenging even locally. discuss reconstruction displaying image space linear interpolations representations. previous section described i-revnet architecture permits deﬁning deep network explicit inverse. explain normally difﬁcult studying local inversion. study local stability network inverse w.r.t. input means quantify locally variations network inverse w.r.t. small variations input. differentiable equivalent perform study analyze singular values differential point close following holds ideally well-conditioned operator singular values constant equal instance achieved isometric operators cisse numerical application image corresponds large matrix whose computations expensive. figure corresponds singular values differential decreasing order given natural image imagenet. example plot typical behavior observe fast decay numerically ﬁrst singular values responsible respectively cumulated energy indicates linearizes space locally considerably smaller space comparison original input dimension. however dimensionality still quite large thus infer lays locally low-dimensional manifold. also proves inversing difﬁcult ill-conditioned problem. thus obtaining implicitly inverse would challenging task avoided thanks formal reconstruction algorithm provided subsection visualizing understanding important directions representation inner layers particular ﬁnal layer complex typically cascade either invertible unstable. approach reconstruct output layer consists ﬁnding input image matches activation gradient descent. however technique leads partial informal reconstruction another method consists embedding representation lower dimensional space comparing common attributes nearest neighbors also possible train reconstruct representation methods require priori knowledge order appropriate embeddings training sets. discuss improvements achieved i-revnet. main claim local inversion ill-conditioned inverse computations involve signiﬁcant round-off errors. forward pass network seem suffer signiﬁcant instabilities thus seems coherent assume hold well. example adding constraints beyond vanishing moments case lifting scheme difﬁcult weakness method. validate claim computing empirical relative error several subsets data evaluate measure subset independent uniform noises validation imagenet. report respectively close machine error indicates inversion suffer signiﬁcant round-off errors. given pair images propose study linear interpolations pair representations feature domain. interpolations correspond existing images exact inverse. reconstruct convex path input points; means discretized adapt step size manually reconstruct sequence xtk}. results displayed figure selected images basel face dataset describable texture dataset imagenet. interpret results. first observe linear interpolation feature space linear interpolation image space intermediary images noisy even small deformations mostly remain recognizable. however geometric transformations d-rotation seem linearized suggested aubry russell next section thus investigate linear separation progresses depth. section study bijective i-revnet. ﬁrst show localized linear classiﬁer progressively improves depth. then describe linear subspace spanned namely feature space showing classiﬁcation performed much smaller subspace built pca. show resnet i-revnet build progressively linearly separable contracted representation measured oyallon observe property holds irevnet despite fact discard information. investigate properties block following experimental protocol. reduce computational burden used subset randomly selected imagenet classes consist images keep subset following experiments. depth extract features {φjxn}n≤n training average along spatial variable standardize order avoid ill-conditioning effects. used nearest neighbor classiﬁer linear svm. former localized classiﬁer indicates metric progressively important classiﬁcation linear measures linear separation different classes. parameters linear cross-validated small subset training prior training classes. evaluate classiﬁers model validation imagenet report top- accuracy figure observe classiﬁers progressively improve similarly depth model linear performing slightly better nearest neighbor classiﬁer robust discriminative classiﬁer two. case i-revnet classiﬁcation performed leads linear performs slightly better ﬁne-tune model classes. observe intense jump performance last layers seems indicate former layers prepared representation contracted linearly separated ﬁnal layers. results suggest low-dimensional embedding data difﬁcult validate estimating local dimensionality high dimensions open problem. however next section compute dimension discriminative part representation built i-revnet. section investigate reﬁne dimensionality informative variabilities ﬁnal layer i-revnet. indeed cascade convolutional operators trained training separate different classes homeomorphism feature space. thus dimensionality feature space potentially large. shown previous subsection ﬁnal layer progressively prepared projected ﬁnal probes corresponding classes. indicates non-informative variabilities classiﬁcation removed linear projection ﬁnal layer space dimension most. however projection built supervision still retain directions contracted thus selected algorithm pca. show fact retains necessary information classiﬁcation small subspace. build linear projectors subspace ﬁrst principal components propose measure classiﬁcation power projected representation supervised classiﬁer e.g. nearest neighbor linear previous class task. again feature representation {φxn}n≤n spatially averaged remove translation variability standardized training set. apply classiﬁers report classiﬁcation accuracy {πdφxn}n≤n w.r.t. figure linear projection removes information recovered linear classiﬁer therefore observe classiﬁcation accuracy decreases signiﬁcantly shows signal indeed lies subspace much lower dimensional original feature dimensions extracted simply considers directions largest variances illustrating successful contraction representation. invertible representations relationship loss information agenda deep learning time. understanding transformations feature space related corresponding input important step towards interpretable deep networks invertible deep networks play important role analysis since example could potentially back-track property feature space input space. best knowledge work provides ﬁrst empirical evidence learning invertible representations discard information input large-scale supervised problems possible. achieve introduce i-revnet class fully invertible permits exactly recover input last convolutional layer. i-revnets achieve classiﬁcation accuracy classiﬁcation complex datasets illustrated ilsvrc- compared revnet resnet architectures similar number layers. furthermore inverse network obtained free training i-revnet requiring minimal adaption recover inputs hidden representations. absence loss information surprising given wide believe discarding information essential learning representations generalize well unseen data. show case propose explain generalization property empirical evidence progressive separation contraction depth imagenet. j¨orn-henrik jacobsen partially funded perspective program imagene. edouard oyallon partially funded grant invariantclass grant students conseil r´egional dile-de-france postdoctoral grant dpei inria collaboration cwi. thank berkay kicanaoglu basel face data mathieu andreux eugene belilovsky amal rannen kyriacos shiarlies feedback drafts paper. mathieu aubry bryan russell. understanding deep features computer-generated imagery. proceedings ieee international conference computer vision mircea cimpoi subhransu maji iasonas kokkinos sammy mohamed andrea vedaldi. describing textures wild. proceedings ieee conference computer vision pattern recognition moustapha cisse piotr bojanowski edouard grave yann dauphin nicolas usunier. parseval networks improving robustness adversarial examples. international conference machine learning alexey dosovitskiy thomas brox. inverting visual representations convolutional networks. proceedings ieee conference computer vision pattern recognition kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. international conference machine learning alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems aravindh mahendran andrea vedaldi. understanding deep image representations inverting them. proceedings ieee conference computer vision pattern recognition pascal paysan reinhard knothe brian amberg sami romdhani thomas vetter. face model pose illumination invariant face recognition. advanced video signal based surveillance avss’. sixth ieee international conference ieee olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein imagenet large scale visual recognition challenge. international journal computer vision wenzhe jose caballero ferenc husz´ar johannes totz andrew aitken bishop daniel rueckert zehan wang. real-time single image video super-resolution using efﬁcient sub-pixel convolutional neural network. proceedings ieee conference computer vision pattern recognition christian szegedy wojciech zaremba ilya sutskever joan bruna dumitru erhan goodfellow fergus. intriguing properties neural networks. arxiv preprint arxiv.", "year": 2018}